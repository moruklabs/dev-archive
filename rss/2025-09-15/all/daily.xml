<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Sun, 14 Sep 2025 01:30:21 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>CodebuffAI/codebuff</title>
      <link>https://github.com/CodebuffAI/codebuff</link>
      <description>&lt;p&gt;Generate code from the terminal!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Codebuff&lt;/h1&gt; 
&lt;p&gt;Codebuff is an &lt;strong&gt;open-source AI coding assistant&lt;/strong&gt; that edits your codebase through natural language instructions. Instead of using one model for everything, it coordinates specialized agents that work together to understand your project and make precise changes.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/assets/codebuff-vs-claude-code.png" alt="Codebuff vs Claude Code" width="400" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Codebuff beats Claude Code at 61% vs 53% on &lt;a href="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/evals/README.md"&gt;our evals&lt;/a&gt; across 175+ coding tasks over multiple open-source repos that simulate real-world tasks.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/assets/demo.gif" alt="Codebuff Demo" /&gt;&lt;/p&gt; 
&lt;h2&gt;How it works&lt;/h2&gt; 
&lt;p&gt;When you ask Codebuff to "add authentication to my API," it might invoke:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A &lt;strong&gt;File Explorer Agent&lt;/strong&gt; to scan your codebase to understand the architecture and find relevant files&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;Planner Agent&lt;/strong&gt; to plan which files need changes and in what order&lt;/li&gt; 
 &lt;li&gt;An &lt;strong&gt;Editor Agent&lt;/strong&gt; to make precise edits&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;Reviewer Agent&lt;/strong&gt; to validate changes&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/assets/multi-agents.png" alt="Codebuff Multi-Agents" width="250" /&gt; 
&lt;/div&gt; 
&lt;p&gt;This multi-agent approach gives you better context understanding, more accurate edits, and fewer errors compared to single-model tools.&lt;/p&gt; 
&lt;h2&gt;CLI: Install and start coding&lt;/h2&gt; 
&lt;p&gt;Install:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g codebuff
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd your-project
codebuff
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then just tell Codebuff what you want and it handles the rest:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Fix the SQL injection vulnerability in user registration"&lt;/li&gt; 
 &lt;li&gt;"Add rate limiting to all API endpoints"&lt;/li&gt; 
 &lt;li&gt;"Refactor the database connection code for better performance"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Codebuff will find the right files, makes changes across your codebase, and runs tests to make sure nothing breaks.&lt;/p&gt; 
&lt;h2&gt;Create custom agents&lt;/h2&gt; 
&lt;p&gt;To get started building your own agents, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;codebuff init-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can write agent definition files that give you maximum control over agent behavior.&lt;/p&gt; 
&lt;p&gt;Implement your workflows by specifying tools, which agents can be spawned, and prompts. We even have TypeScript generators for more programmatic control.&lt;/p&gt; 
&lt;p&gt;For example, here's a &lt;code&gt;git-committer&lt;/code&gt; agent that creates git commits based on the current git state. Notice that it runs &lt;code&gt;git diff&lt;/code&gt; and &lt;code&gt;git log&lt;/code&gt; to analyze changes, but then hands control over to the LLM to craft a meaningful commit message and perform the actual commit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;export default {
  id: 'git-committer',
  displayName: 'Git Committer',
  model: 'openai/gpt-5-nano',
  toolNames: ['read_files', 'run_terminal_command', 'end_turn'],

  instructionsPrompt:
    'You create meaningful git commits by analyzing changes, reading relevant files for context, and crafting clear commit messages that explain the "why" behind changes.',

  async *handleSteps() {
    // Analyze what changed
    yield { tool: 'run_terminal_command', command: 'git diff' }
    yield { tool: 'run_terminal_command', command: 'git log --oneline -5' }

    // Stage files and create commit with good message
    yield 'STEP_ALL'
  },
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;SDK: Run agents in production&lt;/h2&gt; 
&lt;p&gt;Install the &lt;a href="https://www.npmjs.com/package/@codebuff/sdk"&gt;SDK package&lt;/a&gt; -- note this is different than the CLI codebuff package.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install @codebuff/sdk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Import the client and run agents!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;import { CodebuffClient } from '@codebuff/sdk'

// 1. Initialize the client
const client = new CodebuffClient({
  apiKey: 'your-api-key',
  cwd: '/path/to/your/project',
  onError: (error) =&amp;gt; console.error('Codebuff error:', error.message),
})

// 2. Do a coding task...
const result = await client.run({
  agent: 'base', // Codebuff's base coding agent
  prompt: 'Add comprehensive error handling to all API endpoints',
  handleEvent: (event) =&amp;gt; {
    console.log('Progress', event)
  },
})

// 3. Or, run a custom agent!
const myCustomAgent: AgentDefinition = {
  id: 'greeter',
  displayName: 'Greeter',
  model: 'openai/gpt-5',
  instructionsPrompt: 'Say hello!',
}
await client.run({
  agent: 'greeter',
  agentDefinitions: [myCustomAgent],
  prompt: 'My name is Bob.',
  customToolDefinitions: [], // Add custom tools too!
  handleEvent: (event) =&amp;gt; {
    console.log('Progress', event)
  },
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more about the SDK &lt;a href="https://www.npmjs.com/package/@codebuff/sdk"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Why choose Codebuff&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Deep customizability&lt;/strong&gt;: Create sophisticated agent workflows with TypeScript generators that mix AI generation with programmatic control. Define custom agents that spawn subagents, implement conditional logic, and orchestrate complex multi-step processes that adapt to your specific use cases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Any model on OpenRouter&lt;/strong&gt;: Unlike Claude Code which locks you into Anthropic's models, Codebuff supports any model available on &lt;a href="https://openrouter.ai/models"&gt;OpenRouter&lt;/a&gt; - from Claude and GPT to specialized models like Qwen, DeepSeek, and others. Switch models for different tasks or use the latest releases without waiting for platform updates.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Reuse any published agent&lt;/strong&gt;: Compose existing &lt;a href="https://www.codebuff.com/agents"&gt;published agents&lt;/a&gt; to get a leg up. Codebuff agents are the new MCP!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fully customizable SDK&lt;/strong&gt;: Build Codebuff's capabilities directly into your applications with a complete TypeScript SDK. Create custom tools, integrate with your CI/CD pipeline, build AI-powered development environments, or embed intelligent coding assistance into your products.&lt;/p&gt; 
&lt;h2&gt;Contributing to Codebuff&lt;/h2&gt; 
&lt;p&gt;We ‚ù§Ô∏è contributions from the community - whether you're fixing bugs, tweaking our agents, or improving documentation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Want to contribute?&lt;/strong&gt; Check out our &lt;a href="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;Some ways you can help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Fix bugs&lt;/strong&gt; or add features&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Create specialized agents&lt;/strong&gt; and publish them to the Agent Store&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Improve documentation&lt;/strong&gt; or write tutorials&lt;/li&gt; 
 &lt;li&gt;üí° &lt;strong&gt;Share ideas&lt;/strong&gt; in our &lt;a href="https://github.com/CodebuffAI/codebuff/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;CLI&lt;/strong&gt;: &lt;code&gt;npm install -g codebuff&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SDK&lt;/strong&gt;: &lt;code&gt;npm install @codebuff/sdk&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Resources&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href="https://codebuff.com/docs"&gt;codebuff.com/docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community&lt;/strong&gt;: &lt;a href="https://codebuff.com/discord"&gt;Discord&lt;/a&gt; - Join our friendly community&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Issues &amp;amp; Ideas&lt;/strong&gt;: &lt;a href="https://github.com/CodebuffAI/codebuff/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Contributing&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/CodebuffAI/codebuff/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; - Start here to contribute!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href="mailto:support@codebuff.com"&gt;support@codebuff.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#CodebuffAI/codebuff&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=CodebuffAI/codebuff&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MotiaDev/motia</title>
      <link>https://github.com/MotiaDev/motia</link>
      <description>&lt;p&gt;Modern Backend Framework that unifies APIs, background jobs, workflows, and AI Agents into a single core primitive with built-in observability and state management.&lt;/p&gt;&lt;hr&gt;&lt;a href="https://motia.dev"&gt; &lt;img src="https://raw.githubusercontent.com/MotiaDev/motia/main/assets/github-readme-banner.png" alt="Motia Banner" width="100%" /&gt; &lt;/a&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14032" style="margin-right:8px;"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/14032" alt="Motia" style="width: 250px; height: 55px; margin-right:8px;" width="250" height="55" /&gt; &lt;/a&gt; &lt;a href="https://vercel.com/blog/summer-2025-oss-program#motia" target="_blank" style="margin-left:8px;"&gt; &lt;img alt="Vercel OSS Program" src="https://vercel.com/oss/program-badge.svg?sanitize=true" style="width: 250px; height: 55px; margin-left:8px;" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;üî• The Unified Backend Framework That Eliminates Runtime Fragmentation üî•&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;APIs, background jobs, workflows, and AI agents in one system. JavaScript, TypeScript, Python, and more in one codebase.&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.npmjs.com/package/motia"&gt; &lt;img src="https://img.shields.io/npm/v/motia?style=flat&amp;amp;logo=npm&amp;amp;logoColor=white&amp;amp;color=CB3837&amp;amp;labelColor=000000" alt="npm version" /&gt; &lt;/a&gt; &lt;a href="https://github.com/MotiaDev/motia/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/badge/license-MIT-green?style=flat&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white&amp;amp;labelColor=000000" alt="license" /&gt; &lt;/a&gt; &lt;a href="https://github.com/MotiaDev/motia"&gt; &lt;img src="https://img.shields.io/github/stars/MotiaDev/motia?style=flat&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;color=yellow&amp;amp;labelColor=000000" alt="GitHub stars" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/motiadev" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Follow-@motiadev-1DA1F2?style=flat&amp;amp;logo=twitter&amp;amp;logoColor=white&amp;amp;labelColor=000000" alt="Twitter Follow" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/motia" target="_blank"&gt; &lt;img src="https://img.shields.io/discord/1322278831184281721?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;color=5865F2&amp;amp;label=Discord&amp;amp;labelColor=000000" alt="Discord" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.motia.dev/manifesto"&gt;üí° Motia Manifesto&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.motia.dev/docs/getting-started/quick-start"&gt;üöÄ Quick Start&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.motia.dev/docs/concepts/steps/defining-steps"&gt;üìã Defining Steps&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.motia.dev/docs"&gt;üìö Docs&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ What is Motia?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Motia solves backend fragmentation.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Modern software engineering is disjointed ‚Äì APIs live in one framework, background jobs in another, queues have their own tooling, and AI agents are springing up in yet more isolated runtimes. &lt;strong&gt;This fragmentation demands a unified system.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Motia unifies APIs, background jobs, workflows, and AI agents into a &lt;strong&gt;single coherent system&lt;/strong&gt; with shared observability and developer experience. Similar to how React simplified frontend development, where everything is a component, &lt;strong&gt;Motia simplifies backend development, where everything is a Step&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Write &lt;strong&gt;JavaScript, TypeScript, Python, and more&lt;/strong&gt; in the same workflow. &lt;strong&gt;What used to take 5 frameworks to build now comes out of the box with Motia.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://motia.dev"&gt;&lt;img src="https://raw.githubusercontent.com/MotiaDev/motia/main/assets/github-readme-banner.gif" alt="Motia combines APIs, background queues, and AI agents into one system" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Quickstart&lt;/h2&gt; 
&lt;p&gt;Get Motia project up and running in &lt;strong&gt;under 60 seconds&lt;/strong&gt;:&lt;/p&gt; 
&lt;h3&gt;1. Bootstrap a New Motia Project&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx motia@latest create   # runs the interactive terminal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the prompts to pick a template, project name, and language. &lt;img src="https://raw.githubusercontent.com/MotiaDev/motia/main/assets/motia-terminal.gif" alt="motia-terminal" /&gt;&lt;/p&gt; 
&lt;h3&gt;2. Start the Workbench&lt;/h3&gt; 
&lt;p&gt;Inside your new project folder, launch the dev server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx motia dev # ‚ûú http://localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; You have:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ REST APIs with validation&lt;/li&gt; 
 &lt;li&gt;‚úÖ Visual debugger &amp;amp; tracing&lt;/li&gt; 
 &lt;li&gt;‚úÖ Multi-language support&lt;/li&gt; 
 &lt;li&gt;‚úÖ Event-driven architecture&lt;/li&gt; 
 &lt;li&gt;‚úÖ Zero configuration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MotiaDev/motia/main/assets/new-workbench.png" alt="new-workbench" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://motia.dev/docs/getting-started/quick-start"&gt;Full tutorial in our docs ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üéØ Step Types&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Trigger&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;code&gt;api&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HTTP Request&lt;/td&gt; 
   &lt;td&gt;REST endpoints&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;code&gt;event&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Topic subscription&lt;/td&gt; 
   &lt;td&gt;Background processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;code&gt;cron&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Schedule&lt;/td&gt; 
   &lt;td&gt;Recurring jobs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;code&gt;noop&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Manual&lt;/td&gt; 
   &lt;td&gt;External processes&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://motia.dev/docs/concepts/steps"&gt;Learn more about Steps ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ Examples&lt;/h2&gt; 
&lt;h3&gt;üèÜ &lt;strong&gt;&lt;a href="https://chessarena.ai"&gt;ChessArena.ai&lt;/a&gt;&lt;/strong&gt; - Full-Featured Production App&lt;/h3&gt; 
&lt;p&gt;A complete chess platform benchmarking LLM performance with real-time evaluation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://chessarena.ai"&gt;Live Website ‚Üí&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://github.com/MotiaDev/chessarena-ai"&gt;Source Code ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;img src="https://github.com/MotiaDev/chessarena-ai/raw/main/public/images/chessarena.gif?raw=true" alt="ChessArena.ai in action (raw GIF)" /&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Built from scratch to production deployment, featuring:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîê &lt;strong&gt;Authentication &amp;amp; user management&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Multi-agent LLM evaluation&lt;/strong&gt; (OpenAI, Claude, Gemini, Grok)&lt;/li&gt; 
 &lt;li&gt;üêç &lt;strong&gt;Python engine integration&lt;/strong&gt; (Stockfish chess evaluation)&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Real-time streaming&lt;/strong&gt; with live move updates and scoring&lt;/li&gt; 
 &lt;li&gt;üé® &lt;strong&gt;Modern React UI&lt;/strong&gt; with interactive chess boards&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Event-driven workflows&lt;/strong&gt; connecting TypeScript APIs to Python processors&lt;/li&gt; 
 &lt;li&gt;üìà &lt;strong&gt;Live leaderboards&lt;/strong&gt; with move-by-move quality scoring&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Production deployment&lt;/strong&gt; on Motia Cloud&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìö &lt;strong&gt;More Examples&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/MotiaDev/motia-examples"&gt;View all 20+ examples ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/MotiaDev/motia-examples/tree/main/examples/ai-deep-research-agent"&gt;AI Research Agent&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web research with iterative analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/MotiaDev/motia-examples/tree/main/examples/streaming-ai-chatbot"&gt;Streaming Chatbot&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time AI responses&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/MotiaDev/motia-examples/tree/main/examples/gmail-workflow"&gt;Gmail Automation&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart email processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/MotiaDev/motia-examples/tree/main/examples/github-integration-workflow"&gt;GitHub PR Manager&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automated PR workflows&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/MotiaDev/motia-examples/tree/main/examples/finance-agent"&gt;Finance Agent&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time market analysis&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Features demonstrated:&lt;/strong&gt; Multi-language workflows ‚Ä¢ Real-time streaming ‚Ä¢ AI integration ‚Ä¢ Production deployment&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåê Language Support&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;JavaScript&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ Stable&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ Stable&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ Stable&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ruby&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üöß Beta&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Go&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üîÑ Soon&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üìö Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://motia.dev/docs"&gt;üìñ Documentation&lt;/a&gt;&lt;/strong&gt; - Complete guides and API reference&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://discord.gg/motia"&gt;üí¨ Discord&lt;/a&gt;&lt;/strong&gt; - Community support and discussions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/MotiaDev/motia/issues"&gt;üêõ GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/orgs/MotiaDev/projects/2"&gt;üó∫Ô∏è Roadmap&lt;/a&gt;&lt;/strong&gt; - Upcoming features and progress&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöß Roadmap&lt;/h2&gt; 
&lt;p&gt;We have a public roadmap for Motia, you can view it &lt;a href="https://github.com/orgs/MotiaDev/projects/2/views/4"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Feel free to add comments to the issues, or create a new issue if you have a feature request.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python Types&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/485"&gt;#485&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Add support for Python types&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Streams: RBAC&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/495"&gt;#495&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Add support for RBAC&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Streams: Workbench UI&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/497"&gt;#497&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Add support for Workbench UI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Queue Strategies&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/476"&gt;#476&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Add support for Queue Strategies&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reactive Steps&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/477"&gt;#477&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Add support for Reactive Steps&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Point in time triggers&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/480"&gt;#480&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Add support for Point in time triggers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Workbench plugins&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/481"&gt;#481&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Add support for Workbench plugins&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rewrite our Core in either Go or Rust&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/482"&gt;#482&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Rewrite our Core in either Go or Rust&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Decrease deployment time&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/483"&gt;#483&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Decrease deployment time&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Built-in database support&lt;/td&gt; 
   &lt;td&gt;Planned&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MotiaDev/motia/issues/484"&gt;#484&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Add support for built-in database&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! Check our &lt;strong&gt;&lt;a href="https://github.com/MotiaDev/motia/raw/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; to get started.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://motia.dev"&gt;üöÄ Get Started&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://motia.dev/docs"&gt;üìñ Docs&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://discord.gg/motia"&gt;üí¨ Discord&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#motiadev/motia&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=motiadev/motia&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;sub&gt;‚≠ê &lt;strong&gt;Star us if you find Motia useful!&lt;/strong&gt;&lt;/sub&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>simdjson/simdjson</title>
      <link>https://github.com/simdjson/simdjson</link>
      <description>&lt;p&gt;Parsing gigabytes of JSON per second : used by Facebook/Meta Velox, the Node.js runtime, ClickHouse, WatermelonDB, Apache Doris, Milvus, StarRocks&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202-blue.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/LICENSE-MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://simdjson.github.io/simdjson/"&gt;&lt;img src="https://img.shields.io/badge/docs-doxygen-green.svg?sanitize=true" alt="Doxygen Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;simdjson : Parsing gigabytes of JSON per second&lt;/h1&gt; 
&lt;img src="https://raw.githubusercontent.com/simdjson/simdjson/master/images/logo.png" width="10%" style="float: right" /&gt; JSON is everywhere on the Internet. Servers spend a *lot* of time parsing it. We need a fresh approach. The simdjson library uses commonly available SIMD instructions and microparallel algorithms to parse JSON 4x faster than RapidJSON and 25x faster than JSON for Modern C++. 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast:&lt;/strong&gt; Over 4x faster than commonly used production-grade JSON parsers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Record Breaking Features:&lt;/strong&gt; Minify JSON at 6 GB/s, validate UTF-8 at 13 GB/s, NDJSON at 3.5 GB/s.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy:&lt;/strong&gt; First-class, easy to use and carefully documented APIs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strict:&lt;/strong&gt; Full JSON and UTF-8 validation, lossless parsing. Performance with no compromises.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic:&lt;/strong&gt; Selects a CPU-tailored parser at runtime. No configuration needed.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable:&lt;/strong&gt; From memory allocation to error handling, simdjson's design avoids surprises.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Peer Reviewed:&lt;/strong&gt; Our research appears in venues like VLDB Journal, Software: Practice and Experience.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library is part of the &lt;a href="https://awesomecpp.com"&gt;Awesome Modern C++&lt;/a&gt; list.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#real-world-usage"&gt;Real-world usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#godbolt"&gt;Godbolt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#performance-results"&gt;Performance results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#packages"&gt;Packages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#bindings-and-ports-of-simdjson"&gt;Bindings and Ports of simdjson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#about-simdjson"&gt;About simdjson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#funding"&gt;Funding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#contributing-to-simdjson"&gt;Contributing to simdjson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Real-world usage&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/"&gt;Node.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ClickHouse/ClickHouse"&gt;ClickHouse&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://velox-lib.io"&gt;Meta Velox&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/paxml"&gt;Google Pax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/milvus-io/milvus"&gt;milvus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://questdb.io/blog/questdb-release-8-0-3/"&gt;QuestDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aras-p/ClangBuildAnalyzer"&gt;Clang Build Analyzer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Shopify/heap-profiler"&gt;Shopify HeapProfiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/StarRocks/starrocks"&gt;StarRocks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/FishStore"&gt;Microsoft FishStore&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/intel/pcm"&gt;Intel PCM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Nozbe/WatermelonDB"&gt;WatermelonDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/doris"&gt;Apache Doris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dgraph-io/dgraph"&gt;Dgraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unum-cloud/ujrpc"&gt;UJRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/spnda/fastgltf"&gt;fastgltf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tenzir/vast"&gt;vast&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ada-url/ada"&gt;ada-url&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/adamritter/fastgron"&gt;fastgron&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://wasmedge.org"&gt;WasmEdge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/logicalclocks/rondb"&gt;RonDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GreptimeTeam/greptimedb"&gt;GreptimeDB&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are planning to use simdjson in a product, please work from one of our releases.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;The simdjson library is easily consumable with a single .h and .cpp file.&lt;/p&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt; &lt;p&gt;Prerequisites: &lt;code&gt;g++&lt;/code&gt; (version 7 or better) or &lt;code&gt;clang++&lt;/code&gt; (version 6 or better), and a 64-bit system with a command-line shell (e.g., Linux, macOS, freeBSD). We also support programming environments like Visual Studio and Xcode, but different steps are needed. Users of clang++ may need to specify the C++ version (e.g., &lt;code&gt;c++ -std=c++17&lt;/code&gt;) since clang++ tends to default on C++98.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Pull &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.h"&gt;simdjson.h&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.cpp"&gt;simdjson.cpp&lt;/a&gt; into a directory, along with the sample file &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/jsonexamples/twitter.json"&gt;twitter.json&lt;/a&gt;. You can download them with the &lt;code&gt;wget&lt;/code&gt; utility:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.h https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.cpp https://raw.githubusercontent.com/simdjson/simdjson/master/jsonexamples/twitter.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create &lt;code&gt;quickstart.cpp&lt;/code&gt;:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-c++"&gt;#include &amp;lt;iostream&amp;gt;
#include "simdjson.h"
using namespace simdjson;
int main(void) {
    ondemand::parser parser;
    padded_string json = padded_string::load("twitter.json");
    ondemand::document tweets = parser.iterate(json);
    std::cout &amp;lt;&amp;lt; uint64_t(tweets["search_metadata"]["count"]) &amp;lt;&amp;lt; " results." &amp;lt;&amp;lt; std::endl;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;code&gt;c++ -o quickstart quickstart.cpp simdjson.cpp&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;./quickstart&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt; 100 results.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Usage documentation is available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/basics.md"&gt;Basics&lt;/a&gt; is an overview of how to use simdjson and its APIs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/builder.md"&gt;Builder&lt;/a&gt; is an overview of how to efficiently write JSON strings using simdjson.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/performance.md"&gt;Performance&lt;/a&gt; shows some more advanced scenarios and how to tune for them.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/implementation-selection.md"&gt;Implementation Selection&lt;/a&gt; describes runtime CPU detection and how you can work with it.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://simdjson.github.io/simdjson/"&gt;API&lt;/a&gt; contains the automatically generated API documentation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Godbolt&lt;/h2&gt; 
&lt;p&gt;Some users may want to browse code along with the compiled assembly. You want to check out the following lists of examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://godbolt.org/z/7G5qE4sr9"&gt;simdjson examples with errors handled through exceptions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://godbolt.org/z/e9dWb9E4v"&gt;simdjson examples with errors without exceptions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://godbolt.org/z/xK5TGKdPb"&gt;C++26 reflection example&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance results&lt;/h2&gt; 
&lt;p&gt;The simdjson library uses three-quarters less instructions than state-of-the-art parser &lt;a href="https://rapidjson.org"&gt;RapidJSON&lt;/a&gt;. To our knowledge, simdjson is the first fully-validating JSON parser to run at &lt;a href="https://en.wikipedia.org/wiki/Gigabyte"&gt;gigabytes per second&lt;/a&gt; (GB/s) on commodity processors. It can parse millions of JSON documents per second on a single core.&lt;/p&gt; 
&lt;p&gt;The following figure represents parsing speed in GB/s for parsing various files on an Intel Skylake processor (3.4 GHz) using the GNU GCC 10 compiler (with the -O3 flag). We compare against the best and fastest C++ libraries on benchmarks that load and process the data. The simdjson library offers full unicode (&lt;a href="https://en.wikipedia.org/wiki/UTF-8"&gt;UTF-8&lt;/a&gt;) validation and exact number parsing.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/rome.png" width="60%" /&gt; 
&lt;p&gt;The simdjson library offers high speed whether it processes tiny files (e.g., 300 bytes) or larger files (e.g., 3MB). The following plot presents parsing speed for &lt;a href="https://github.com/simdjson/simdjson_experiments_vldb2019/raw/master/experiments/growing/gen.py"&gt;synthetic files over various sizes generated with a script&lt;/a&gt; on a 3.4 GHz Skylake processor (GNU GCC 9, -O3).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/growing.png" width="60%" /&gt; 
&lt;p&gt;&lt;a href="https://github.com/simdjson/simdjson_experiments_vldb2019"&gt;All our experiments are reproducible&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For NDJSON files, we can exceed 3 GB/s with &lt;a href="https://github.com/simdjson/simdjson/raw/master/doc/parse_many.md"&gt;our multithreaded parsing functions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Packages&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/simdjson/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/simdjson.svg?sanitize=true" alt="Packaging status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Bindings and Ports of simdjson&lt;/h2&gt; 
&lt;p&gt;We distinguish between "bindings" (which just wrap the C++ code) and a port to another programming language (which reimplements everything).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/michaeleisel/zippyjson"&gt;ZippyJSON&lt;/a&gt;: Swift bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gerrymanoim/libpy_simdjson/"&gt;libpy_simdjson&lt;/a&gt;: high-speed Python bindings for simdjson using &lt;a href="https://github.com/quantopian/libpy"&gt;libpy&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TkTech/pysimdjson"&gt;pysimdjson&lt;/a&gt;: Python bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TeskaLabs/cysimdjson"&gt;cysimdjson&lt;/a&gt;: high-speed Python bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/simd-lite"&gt;simdjson-rs&lt;/a&gt;: Rust port.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SunDoge/simdjson-rust"&gt;simdjson-rust&lt;/a&gt;: Rust wrapper (bindings).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EgorBo/SimdJsonSharp"&gt;SimdJsonSharp&lt;/a&gt;: C# version for .NET Core (bindings and full port).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/luizperes/simdjson_nodejs"&gt;simdjson_nodejs&lt;/a&gt;: Node.js bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crazyxman/simdjson_php"&gt;simdjson_php&lt;/a&gt;: PHP bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/saka1/simdjson_ruby"&gt;simdjson_ruby&lt;/a&gt;: Ruby bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anilmaurya/fast_jsonparser"&gt;fast_jsonparser&lt;/a&gt;: Ruby bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/minio/simdjson-go"&gt;simdjson-go&lt;/a&gt;: Go port using Golang assembly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eddelbuettel/rcppsimdjson"&gt;rcppsimdjson&lt;/a&gt;: R bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ChomperT/simdjson_erlang"&gt;simdjson_erlang&lt;/a&gt;: erlang bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/saleyn/simdjsone"&gt;simdjsone&lt;/a&gt;: erlang bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FourierTransformer/lua-simdjson"&gt;lua-simdjson&lt;/a&gt;: lua bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hackage.haskell.org/package/hermes-json"&gt;hermes-json&lt;/a&gt;: haskell bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EzequielRamis/zimdjson"&gt;zimdjson&lt;/a&gt;: Zig port.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/travisstaloch/simdjzon"&gt;simdjzon&lt;/a&gt;: Zig port.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rawleyfowler/JSON-simd"&gt;JSON-Simd&lt;/a&gt;: Raku bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://metacpan.org/pod/JSON::SIMD"&gt;JSON::SIMD&lt;/a&gt;: Perl bindings; fully-featured JSON module that uses simdjson for decoding.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sainttttt/gemmaJSON"&gt;gemmaJSON&lt;/a&gt;: Nim JSON parser based on simdjson bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/simdjson/simdjson-java"&gt;simdjson-java&lt;/a&gt;: Java port.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About simdjson&lt;/h2&gt; 
&lt;p&gt;The simdjson library takes advantage of modern microarchitectures, parallelizing with SIMD vector instructions, reducing branch misprediction, and reducing data dependency to take advantage of each CPU's multiple execution cores.&lt;/p&gt; 
&lt;p&gt;Our default front-end is called On-Demand, and we wrote a paper about it:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Keiser, Daniel Lemire, &lt;a href="http://arxiv.org/abs/2312.17149"&gt;On-Demand JSON: A Better Way to Parse Documents?&lt;/a&gt;, Software: Practice and Experience 54 (6), 2024.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Some people &lt;a href="https://arxiv.org/abs/1902.08318"&gt;enjoy reading the first (2019) simdjson paper&lt;/a&gt;: A description of the design and implementation of simdjson is in our research article:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Geoff Langdale, Daniel Lemire, &lt;a href="https://arxiv.org/abs/1902.08318"&gt;Parsing Gigabytes of JSON per Second&lt;/a&gt;, VLDB Journal 28 (6), 2019.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We have an in-depth paper focused on the UTF-8 validation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Keiser, Daniel Lemire, &lt;a href="https://arxiv.org/abs/2010.03090"&gt;Validating UTF-8 In Less Than One Instruction Per Byte&lt;/a&gt;, Software: Practice &amp;amp; Experience 51 (5), 2021.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also have an informal &lt;a href="https://branchfree.org/2019/02/25/paper-parsing-gigabytes-of-json-per-second/"&gt;blog post providing some background and context&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For the video inclined, &lt;br /&gt; &lt;a href="http://www.youtube.com/watch?v=wlvKAT7SZIQ"&gt;&lt;img src="http://img.youtube.com/vi/wlvKAT7SZIQ/0.jpg" alt="simdjson at QCon San Francisco 2019" /&gt;&lt;/a&gt;&lt;br /&gt; (It was the best voted talk, we're kinda proud of it.)&lt;/p&gt; 
&lt;h2&gt;Funding&lt;/h2&gt; 
&lt;p&gt;The work is supported by the Natural Sciences and Engineering Research Council of Canada under grants RGPIN-2017-03910 and RGPIN-2024-03787.&lt;/p&gt; 
&lt;h2&gt;Contributing to simdjson&lt;/h2&gt; 
&lt;p&gt;Head over to &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for information on contributing to simdjson, and &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/HACKING.md"&gt;HACKING.md&lt;/a&gt; for information on source, building, and architecture/design.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This code is made available under the &lt;a href="https://www.apache.org/licenses/LICENSE-2.0.html"&gt;Apache License 2.0&lt;/a&gt; as well as under the MIT License. As a user, you can pick the license you prefer.&lt;/p&gt; 
&lt;p&gt;Under Windows, we build some tools using the windows/dirent_portable.h file (which is outside our library code): it is under the liberal (business-friendly) MIT license.&lt;/p&gt; 
&lt;p&gt;For compilers that do not support &lt;a href="https://en.wikipedia.org/wiki/C%2B%2B17"&gt;C++17&lt;/a&gt;, we bundle the string-view library which is published under the &lt;a href="http://www.boost.org/LICENSE_1_0.txt"&gt;Boost license&lt;/a&gt;. Like the Apache license, the Boost license is a permissive license allowing commercial redistribution.&lt;/p&gt; 
&lt;p&gt;For efficient number serialization, we bundle Florian Loitsch's implementation of the Grisu2 algorithm for binary to decimal floating-point numbers. The implementation was slightly modified by JSON for Modern C++ library. Both Florian Loitsch's implementation and JSON for Modern C++ are provided under the MIT license.&lt;/p&gt; 
&lt;p&gt;For runtime dispatching, we use some code from the PyTorch project licensed under 3-clause BSD.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sentient-agi/ROMA</title>
      <link>https://github.com/sentient-agi/ROMA</link>
      <description>&lt;p&gt;Recursive-Open-Meta-Agent v0.1 (Beta). A meta-agent framework to build high-performance multi-agent systems.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/sentient-logo.png" alt="alt text" width="60%" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;ROMA: Recursive Open Meta-Agents&lt;/h1&gt; 
 &lt;p align="center"&gt; &lt;strong&gt;Building hierarchical high-performance multi-agent systems made easy! (Beta) &lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14848" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14848" alt="sentient-agi%2FROMA | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://sentient.xyz/" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Homepage" src="https://img.shields.io/badge/Sentient-Homepage-%23EAEAEA?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%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%2BPC9zdmc%2B&amp;amp;link=https%3A%2F%2Fhuggingface.co%2FSentientagi" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://github.com/sentient-agi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="GitHub" src="https://img.shields.io/badge/Github-sentient_agi-181717?logo=github" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://huggingface.co/Sentientagi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SentientAGI-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;/div&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://discord.gg/sentientfoundation" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord-SentientAGI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;a href="https://x.com/SentientAGI" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/badge/-SentientAGI-grey?logo=x&amp;amp;link=https%3A%2F%2Fx.com%2FSentientAGI%2F" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.sentient.xyz/blog/recursive-open-meta-agent"&gt;Technical Blog&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/"&gt;Paper (Coming soon)&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.sentient.xyz/"&gt;Build Agents for $$$&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt;  
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/INTRODUCTION.md"&gt;üöÄ Introduction&lt;/a&gt;&lt;/strong&gt; - Understand the vision and architecture behind ROMA&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;üì¶ Setup&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;ü§ñ Agents Guide&lt;/a&gt;&lt;/strong&gt; - Learn how to create and customize your own agents&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/CONFIGURATION.md"&gt;‚öôÔ∏è Configuration&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/ROADMAP.md"&gt;üó∫Ô∏è Roadmap&lt;/a&gt;&lt;/strong&gt; - See what's coming next for ROMA&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéØ What is ROMA?&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/roma_run.gif" alt="alt text" width="80%" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; is a &lt;strong&gt;meta-agent framework&lt;/strong&gt; that uses recursive hierarchical structures to solve complex problems. By breaking down tasks into parallelizable components, ROMA enables agents to tackle sophisticated reasoning challenges while maintaining transparency that makes context-engineering and iteration straightforward. The framework offers &lt;strong&gt;parallel problem solving&lt;/strong&gt; where agents work simultaneously on different parts of complex tasks, &lt;strong&gt;transparent development&lt;/strong&gt; with a clear structure for easy debugging, and &lt;strong&gt;proven performance&lt;/strong&gt; demonstrated through our search agent's strong benchmark results. We've shown the framework's effectiveness, but this is just the beginning. As an &lt;strong&gt;open-source and extensible&lt;/strong&gt; platform, ROMA is designed for community-driven development, allowing you to build and customize agents for your specific needs while benefiting from the collective improvements of the community.&lt;/p&gt; 
&lt;h2&gt;üèóÔ∏è How It Works&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; framework processes tasks through a recursive &lt;strong&gt;plan‚Äìexecute loop&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def solve(task):
    if is_atomic(task):                 # Step 1: Atomizer
        return execute(task)            # Step 2: Executor
    else:
        subtasks = plan(task)           # Step 2: Planner
        results = []
        for subtask in subtasks:
            results.append(solve(subtask))  # Recursive call
        return aggregate(results)       # Step 3: Aggregator

# Entry point:
answer = solve(initial_request)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Atomizer&lt;/strong&gt; ‚Äì Decides whether a request is &lt;strong&gt;atomic&lt;/strong&gt; (directly executable) or requires &lt;strong&gt;planning&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Planner&lt;/strong&gt; ‚Äì If planning is needed, the task is broken into smaller &lt;strong&gt;subtasks&lt;/strong&gt;. Each subtask is fed back into the &lt;strong&gt;Atomizer&lt;/strong&gt;, making the process recursive.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Executors&lt;/strong&gt; ‚Äì Handle atomic tasks. Executors can be &lt;strong&gt;LLMs, APIs, or even other agents&lt;/strong&gt; ‚Äî as long as they implement an &lt;code&gt;agent.execute()&lt;/code&gt; interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aggregator&lt;/strong&gt; ‚Äì Collects and integrates results from subtasks. Importantly, the Aggregator produces the &lt;strong&gt;answer to the original parent task&lt;/strong&gt;, not just raw child outputs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;üìê Information Flow&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Top-down:&lt;/strong&gt; Tasks are decomposed into subtasks recursively.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bottom-up:&lt;/strong&gt; Subtask results are aggregated upwards into solutions for parent tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Left-to-right:&lt;/strong&gt; If a subtask depends on the output of a previous one, it waits until that subtask completes before execution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This structure makes the system flexible, recursive, and dependency-aware ‚Äî capable of decomposing complex problems into smaller steps while ensuring results are integrated coherently.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view the system flow diagram&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TB
    A[Your Request] --&amp;gt; B{Atomizer}
    B --&amp;gt;|Plan Needed| C[Planner]
    B --&amp;gt;|Atomic Task| D[Executor]

    %% Planner spawns subtasks
    C --&amp;gt; E[Subtasks]
    E --&amp;gt; G[Aggregator]

    %% Recursion
    E -.-&amp;gt; B  

    %% Execution + Aggregation
    D --&amp;gt; F[Final Result]
    G --&amp;gt; F

    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#d1c4e9
    style G fill:#c5cae9

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt;
&lt;br /&gt; 
&lt;h3&gt;üöÄ 30-Second Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sentient-agi/ROMA.git
cd ROMA

# Run the automated setup
./setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Choose between:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Setup&lt;/strong&gt; (Recommended) - One-command setup with isolation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native Setup&lt;/strong&gt; - Direct installation for development&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üõ†Ô∏è Technical Stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: Built on &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/your/agnoagents%5D(https://github.com/agno-agi/agno)"&gt;AgnoAgents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Python 3.12+ with FastAPI/Flask&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + TypeScript with real-time WebSocket&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Support&lt;/strong&gt;: Any provider via LiteLLM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Persistence&lt;/strong&gt;: Enterprise S3 mounting with security validation 
  &lt;ul&gt; 
   &lt;li&gt;üîí &lt;strong&gt;goofys FUSE mounting&lt;/strong&gt; for zero-latency file access&lt;/li&gt; 
   &lt;li&gt;üõ°Ô∏è &lt;strong&gt;Path injection protection&lt;/strong&gt; with comprehensive validation&lt;/li&gt; 
   &lt;li&gt;üîê &lt;strong&gt;AWS credentials verification&lt;/strong&gt; before operations&lt;/li&gt; 
   &lt;li&gt;üìÅ &lt;strong&gt;Dynamic Docker Compose&lt;/strong&gt; with secure volume mounting&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Execution&lt;/strong&gt;: E2B sandboxes with unified S3 integration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Production-grade validation and error handling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt;: Multi-modal, tools, MCP, hooks, caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì¶ Installation Options&lt;/h2&gt; 
&lt;h3&gt;Quick Start (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Main setup (choose Docker or Native)
./setup.sh

# Optional: Setup E2B sandbox integration
./setup.sh --e2b

# Test E2B integration  
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Command Line Options&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./setup.sh --docker     # Run Docker setup directly
./setup.sh --docker-from-scratch  # Rebuild Docker images/containers from scratch (down -v, no cache)
./setup.sh --native     # Run native setup directly (macOS/Ubuntu/Debian)
./setup.sh --e2b        # Setup E2B template (requires E2B_API_KEY + AWS creds)
./setup.sh --test-e2b   # Test E2B template integration
./setup.sh --help       # Show all available options
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual Installation&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;setup docs&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h3&gt;üèóÔ∏è Optional: E2B Sandbox Integration&lt;/h3&gt; 
&lt;p&gt;For secure code execution capabilities, optionally set up E2B sandboxes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# After main setup, configure E2B (requires E2B_API_KEY and AWS credentials in .env)
./setup.sh --e2b

# Test E2B integration
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;E2B Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Secure Code Execution&lt;/strong&gt; - Run untrusted code in isolated sandboxes&lt;/li&gt; 
 &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;S3 Integration&lt;/strong&gt; - Automatic data sync between local and sandbox environments&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;goofys Mounting&lt;/strong&gt; - High-performance S3 filesystem mounting&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;AWS Credentials&lt;/strong&gt; - Passed securely via Docker build arguments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ñ Pre-built Agents&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; These agents are demonstrations built using ROMA's framework through simple vibe-prompting and minimal manual tuning. They showcase how easily you can create high-performance agents with ROMA, rather than being production-final solutions. Our mission is to empower the community to build, share, and get rewarded for creating innovative agent recipes and use-cases.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ROMA comes with example agents that demonstrate the framework's capabilities:&lt;/p&gt; 
&lt;h3&gt;üîç General Task Solver&lt;/h3&gt; 
&lt;p&gt;A versatile agent powered by ChatGPT Search Preview for handling diverse tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Search&lt;/strong&gt;: Leverages OpenAI's latest search capabilities for real-time information&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Planning&lt;/strong&gt;: Adapts task decomposition based on query complexity&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Domain&lt;/strong&gt;: Handles everything from technical questions to creative projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quick Prototyping&lt;/strong&gt;: Perfect for testing ROMA's capabilities without domain-specific setup&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: General research, fact-checking, exploratory analysis, quick information gathering&lt;/p&gt; 
&lt;h3&gt;üî¨ Deep Research Agent&lt;/h3&gt; 
&lt;p&gt;A comprehensive research system that breaks down complex research questions into manageable sub-tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Task Decomposition&lt;/strong&gt;: Automatically splits research topics into search, analysis, and synthesis phases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Information Gathering&lt;/strong&gt;: Executes multiple searches simultaneously for faster results&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Source Integration&lt;/strong&gt;: Combines results from web search, Wikipedia, and specialized APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Synthesis&lt;/strong&gt;: Aggregates findings into coherent, well-structured reports&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Academic research, market analysis, competitive intelligence, technical documentation&lt;/p&gt; 
&lt;h3&gt;üíπ Crypto Analytics Agent&lt;/h3&gt; 
&lt;p&gt;Specialized financial analysis agent with deep blockchain and DeFi expertise:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Market Data&lt;/strong&gt;: Integrates with Binance, CoinGecko, and DefiLlama APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-Chain Analytics&lt;/strong&gt;: Access to Arkham Intelligence for wallet tracking and token flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technical Analysis&lt;/strong&gt;: Advanced charting with OHLC data and market indicators&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeFi Metrics&lt;/strong&gt;: TVL tracking, yield analysis, protocol comparisons&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Execution&lt;/strong&gt;: Runs analysis in E2B sandboxes with data persistence&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Token research, portfolio analysis, DeFi protocol evaluation, market trend analysis&lt;/p&gt; 
&lt;p&gt;All three agents demonstrate ROMA's recursive architecture in action, showing how complex queries that would overwhelm single-pass systems can be elegantly decomposed and solved. They serve as templates and inspiration for building your own specialized agents.&lt;/p&gt; 
&lt;h3&gt;Your First Agent in 5 Minutes&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;./setup.sh  # Automated setup with Docker or native installation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access all the pre-defined agents through the frontend on &lt;code&gt;localhost:3000&lt;/code&gt; after setting up the backend on &lt;code&gt;localhost:5000&lt;/code&gt;. Please checkout &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;Setup&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;Agents guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/agent_customization.png" alt="alt text" width="60%" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Your first agent in 3 lines
from sentientresearchagent import SentientAgent

agent = SentientAgent.create()
result = await agent.run("Create a podcast about AI safety")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìä Benchmarks&lt;/h2&gt; 
&lt;p&gt;We evaluate our simple implementation of a search system using ROMA, called ROMA-Search across three benchmarks: &lt;strong&gt;SEAL-0&lt;/strong&gt;, &lt;strong&gt;FRAMES&lt;/strong&gt;, and &lt;strong&gt;SimpleQA&lt;/strong&gt;.&lt;br /&gt; Below are the performance graphs for each benchmark.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/vtllms/sealqa"&gt;SEAL-0&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;SealQA is a new challenging benchmark for evaluating Search-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/seal-0-full.001.jpeg" alt="SEAL-0 Results" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/google/frames-benchmark"&gt;FRAMES&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;A comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/FRAMES-full.001.jpeg" alt="FRAMES Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://openai.com/index/introducing-simpleqa/"&gt;SimpleQA&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;Factuality benchmark that measures the ability for language models to answer short, fact-seeking questions.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/simpleQAFull.001.jpeg" alt="SimpleQA Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîÑ &lt;strong&gt;Recursive Task Decomposition&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Automatically breaks down complex tasks into manageable subtasks with intelligent dependency management. Runs independent sub-tasks in &lt;strong&gt;parallel&lt;/strong&gt;.&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ü§ñ &lt;strong&gt;Agent Agnostic&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Works with any provider (OpenAI, Anthropic, Google, local models) through unified interface, as long as it has an &lt;code&gt;agent.run()&lt;/code&gt; command, then you can use it!&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîç &lt;strong&gt;Complete Transparency&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Stage tracing shows exactly what happens at each step - debug and optimize with full visibility&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîå Connect Any Tool&lt;/h3&gt; &lt;p&gt;Seamlessly integrate external tools and protocols with configurable intervention points. Already includes production-grade connectors such as E2B, file-read-write, and more.&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;This framework would not have been possible if it wasn't for these amazing open-source contributions!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inspired by the hierarchical planning approach described in &lt;a href="https://arxiv.org/abs/2503.08275"&gt;"Beyond Outlining: Heterogeneous Recursive Planning"&lt;/a&gt; by Xiong et al.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt; - Data validation using Python type annotations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/agno-ai/agno%5D(https://github.com/agno-agi/agno)"&gt;Agno&lt;/a&gt; - Framework for building AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/e2b-dev/e2b"&gt;E2B&lt;/a&gt; - Cloud runtime for AI agents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìö Citation&lt;/h2&gt; 
&lt;p&gt;If you use the ROMA repo in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{al_zubi_2025_17052592,
  author       = {Al-Zubi, Salah and
                  Nama, Baran and
                  Kaz, Arda and
                  Oh, Sewoong},
  title        = {SentientResearchAgent: A Hierarchical AI Agent
                   Framework for Research and Analysis
                  },
  month        = sep,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {ROMA},
  doi          = {10.5281/zenodo.17052592},
  url          = {https://doi.org/10.5281/zenodo.17052592},
  swhid        = {swh:1:dir:69cd1552103e0333dd0c39fc4f53cb03196017ce
                   ;origin=https://doi.org/10.5281/zenodo.17052591;vi
                   sit=swh:1:snp:f50bf99634f9876adb80c027361aec9dff97
                   3433;anchor=swh:1:rel:afa7caa843ce1279f5b4b29b5d3d
                   5e3fe85edc95;path=salzubi401-ROMA-b31c382
                  },
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PowerShell/PowerShell</title>
      <link>https://github.com/PowerShell/PowerShell</link>
      <description>&lt;p&gt;PowerShell for every system!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/PowerShell/PowerShell/master/assets/ps_black_64.svg?sanitize=true" alt="logo" /&gt; PowerShell&lt;/h1&gt; 
&lt;p&gt;Welcome to the PowerShell GitHub Community! &lt;a href="https://learn.microsoft.com/powershell/scripting/overview"&gt;PowerShell&lt;/a&gt; is a cross-platform (Windows, Linux, and macOS) automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language, and a framework for processing cmdlets.&lt;/p&gt; 
&lt;h2&gt;Windows PowerShell vs. PowerShell 7+&lt;/h2&gt; 
&lt;p&gt;Although this repository started as a fork of the Windows PowerShell codebase, changes made in this repository are not ported back to Windows PowerShell 5.1. This also means that &lt;a href="https://github.com/PowerShell/PowerShell/issues"&gt;issues tracked here&lt;/a&gt; are only for PowerShell 7.x and higher. Windows PowerShell specific issues should be reported with the &lt;a href="https://support.microsoft.com/windows/send-feedback-to-microsoft-with-the-feedback-hub-app-f59187f8-8739-22d6-ba93-f66612949332"&gt;Feedback Hub app&lt;/a&gt;, by choosing "Apps &amp;gt; PowerShell" in the category.&lt;/p&gt; 
&lt;h2&gt;New to PowerShell?&lt;/h2&gt; 
&lt;p&gt;If you are new to PowerShell and want to learn more, we recommend reviewing the &lt;a href="https://learn.microsoft.com/powershell/scripting/learn/more-powershell-learning"&gt;getting started&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;h2&gt;Get PowerShell&lt;/h2&gt; 
&lt;p&gt;PowerShell is supported on Windows, macOS, and a variety of Linux platforms. For more information, see &lt;a href="https://learn.microsoft.com/powershell/scripting/install/installing-powershell"&gt;Installing PowerShell&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Upgrading PowerShell&lt;/h2&gt; 
&lt;p&gt;For best results when upgrading, you should use the same install method you used when you first installed PowerShell. The update method is different for each platform and install method.&lt;/p&gt; 
&lt;h2&gt;Community Dashboard&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aka.ms/PSPublicDashboard"&gt;Dashboard&lt;/a&gt; with visualizations for community contributions and project status using PowerShell, Azure, and PowerBI.&lt;/p&gt; 
&lt;p&gt;For more information on how and why we built this dashboard, check out this &lt;a href="https://devblogs.microsoft.com/powershell/powershell-open-source-community-dashboard/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Discussions&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://docs.github.com/discussions/quickstart"&gt;GitHub Discussions&lt;/a&gt; is a feature to enable free and open discussions within the community for topics that are not related to code, unlike issues.&lt;/p&gt; 
&lt;p&gt;This is an experiment we are trying in our repositories, to see if it helps move discussions out of issues so that issues remain actionable by the team or members of the community. There should be no expectation that PowerShell team members are regular participants in these discussions. Individual PowerShell team members may choose to participate in discussions, but the expectation is that community members help drive discussions so that team members can focus on issues.&lt;/p&gt; 
&lt;p&gt;Create or join a &lt;a href="https://github.com/PowerShell/PowerShell/discussions"&gt;discussion&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Chat&lt;/h2&gt; 
&lt;p&gt;Want to chat with other members of the PowerShell community?&lt;/p&gt; 
&lt;p&gt;There are dozens of topic-specific channels on our community-driven PowerShell Virtual User Group, which you can join on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://gitter.im/PowerShell/PowerShell"&gt;Gitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/PowerShell"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://web.libera.chat/#powershell"&gt;IRC&lt;/a&gt; on Libera.Chat&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/psslack"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build status of nightly builds&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Azure CI (Windows)&lt;/th&gt; 
   &lt;th align="left"&gt;Azure CI (Linux)&lt;/th&gt; 
   &lt;th align="left"&gt;Azure CI (macOS)&lt;/th&gt; 
   &lt;th align="left"&gt;CodeFactor Grade&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://powershell.visualstudio.com/PowerShell/_build?definitionId=32"&gt;&lt;img src="https://powershell.visualstudio.com/PowerShell/_apis/build/status/PowerShell-CI-Windows-daily" alt="windows-nightly-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://powershell.visualstudio.com/PowerShell/_build?definitionId=23"&gt;&lt;img src="https://powershell.visualstudio.com/PowerShell/_apis/build/status/PowerShell-CI-linux-daily?branchName=master" alt="linux-nightly-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://powershell.visualstudio.com/PowerShell/_build?definitionId=24"&gt;&lt;img src="https://powershell.visualstudio.com/PowerShell/_apis/build/status/PowerShell-CI-macos-daily?branchName=master" alt="macOS-nightly-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.codefactor.io/repository/github/powershell/powershell"&gt;&lt;img src="https://www.codefactor.io/repository/github/powershell/powershell/badge" alt="cf-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Developing and Contributing&lt;/h2&gt; 
&lt;p&gt;Want to contribute to PowerShell? Please start with the &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; to learn how to develop and contribute.&lt;/p&gt; 
&lt;p&gt;If you are developing .NET Core C# applications targeting PowerShell Core, &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md#where-do-i-get-the-powershell-core-sdk-package"&gt;check out our FAQ&lt;/a&gt; to learn more about the PowerShell SDK NuGet package.&lt;/p&gt; 
&lt;p&gt;Also, make sure to check out our &lt;a href="https://github.com/powershell/powershell-rfc"&gt;PowerShell-RFC repository&lt;/a&gt; for request-for-comments (RFC) documents to submit and give comments on proposed and future designs.&lt;/p&gt; 
&lt;h2&gt;Building PowerShell&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Linux&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;macOS&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/building/linux.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/building/windows-core.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/building/macos.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;If you have any problems building PowerShell, please start by consulting the developer &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Downloading the Source Code&lt;/h2&gt; 
&lt;p&gt;You can clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/PowerShell/PowerShell.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information, see &lt;a href="https://github.com/PowerShell/PowerShell/tree/master/docs/git"&gt;working with the PowerShell repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;For support, see the &lt;a href="https://github.com/PowerShell/PowerShell/tree/master/.github/SUPPORT.md"&gt;Support Section&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Legal and Licensing&lt;/h2&gt; 
&lt;p&gt;PowerShell is licensed under the &lt;a href="https://github.com/PowerShell/PowerShell/tree/master/LICENSE.txt"&gt;MIT license&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker Containers&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] The PowerShell container images are now &lt;a href="https://github.com/PowerShell/Announcements/issues/75"&gt;maintained by the .NET team&lt;/a&gt;. The containers at &lt;code&gt;mcr.microsoft.com/powershell&lt;/code&gt; are currently not maintained.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;License: By requesting and using the Container OS Image for Windows containers, you acknowledge, understand, and consent to the Supplemental License Terms available on &lt;a href="https://mcr.microsoft.com/en-us/product/powershell/tags"&gt;Microsoft Artifact Registry&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Telemetry&lt;/h3&gt; 
&lt;p&gt;Please visit our &lt;a href="https://learn.microsoft.com/powershell/module/microsoft.powershell.core/about/about_telemetry"&gt;about_Telemetry&lt;/a&gt; topic to read details about telemetry gathered by PowerShell.&lt;/p&gt; 
&lt;h2&gt;Governance&lt;/h2&gt; 
&lt;p&gt;The governance policy for the PowerShell project is described the &lt;a href="https://github.com/PowerShell/PowerShell/raw/master/docs/community/governance.md"&gt;PowerShell Governance&lt;/a&gt; document.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Please see our &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; before participating in this project.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/SECURITY.md"&gt;Security Policy&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;For any security issues, please see our &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/SECURITY.md"&gt;Security Policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Physical-Intelligence/openpi</title>
      <link>https://github.com/Physical-Intelligence/openpi</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openpi&lt;/h1&gt; 
&lt;p&gt;openpi holds open-source models and packages for robotics, published by the &lt;a href="https://www.physicalintelligence.company/"&gt;Physical Intelligence team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, this repo contains three types of models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;œÄ‚ÇÄ model&lt;/a&gt;, a flow-based vision-language-action model (VLA).&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;œÄ‚ÇÄ-FAST model&lt;/a&gt;, an autoregressive VLA, based on the FAST action tokenizer.&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;œÄ‚ÇÄ.‚ÇÖ model&lt;/a&gt;, an upgraded version of œÄ‚ÇÄ with better open-world generalization trained with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;. Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For all models, we provide &lt;em&gt;base model&lt;/em&gt; checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.&lt;/p&gt; 
&lt;p&gt;This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://droid-dataset.github.io/"&gt;DROID&lt;/a&gt;, and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Sept 2025] We released PyTorch support in openpi.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025]: We have added an &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md#data-filtering"&gt;improved idle filter&lt;/a&gt; for DROID training.&lt;/li&gt; 
 &lt;li&gt;[Jun 2025]: We have added &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md"&gt;instructions&lt;/a&gt; for using &lt;code&gt;openpi&lt;/code&gt; to train VLAs on the full &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;. This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring &lt;code&gt;fsdp_devices&lt;/code&gt; in the training config. Please also note that the current training script does not yet support multi-node training.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mode&lt;/th&gt; 
   &lt;th&gt;Memory Required&lt;/th&gt; 
   &lt;th&gt;Example GPU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 8 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (LoRA)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 22.5 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (Full)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 70 GB&lt;/td&gt; 
   &lt;td&gt;A100 (80GB) / H100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;When cloning this repo, make sure to update submodules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage Python dependencies. See the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installation instructions&lt;/a&gt; to set it up. Once uv is installed, run the following to set up the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: &lt;code&gt;GIT_LFS_SKIP_SMUDGE=1&lt;/code&gt; is needed to pull LeRobot as a dependency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/docker.md"&gt;Docker Setup&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Model Checkpoints&lt;/h2&gt; 
&lt;h3&gt;Base Models&lt;/h3&gt; 
&lt;p&gt;We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;œÄ‚ÇÄ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base autoregressive &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;œÄ‚ÇÄ-FAST model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;œÄ‚ÇÄ.‚ÇÖ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-Tuned Models&lt;/h3&gt; 
&lt;p&gt;We also provide "expert" checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST-DROID&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$-FAST model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-DROID&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-towel&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can fold diverse towels 0-shot on ALOHA robot platforms&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_towel&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-tupperware&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can unpack food from a tupperware container&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_tupperware&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-pen-uncap&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on public &lt;a href="https://dit-policy.github.io/"&gt;ALOHA&lt;/a&gt; data: can uncap a pen&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-LIBERO&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned for the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO&lt;/a&gt; benchmark: gets state-of-the-art performance (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_libero&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-DROID&lt;/td&gt; 
   &lt;td&gt;Inference / Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt; with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;: fast inference and good language-following&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, checkpoints are automatically downloaded from &lt;code&gt;gs://openpi-assets&lt;/code&gt; and are cached in &lt;code&gt;~/.cache/openpi&lt;/code&gt; when needed. You can overwrite the download path by setting the &lt;code&gt;OPENPI_DATA_HOME&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Running Inference for a Pre-Trained Model&lt;/h2&gt; 
&lt;p&gt;Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = download.maybe_download("gs://openpi-assets/checkpoints/pi05_droid")

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    "observation/exterior_image_1_left": ...,
    "observation/wrist_image_left": ...,
    ...
    "prompt": "pick up the fork"
}
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also test this out in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/inference.ipynb"&gt;example notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README.md"&gt;DROID&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real/README.md"&gt;ALOHA&lt;/a&gt; robots.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Remote Inference&lt;/strong&gt;: We provide &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;examples and code&lt;/a&gt; for running inference of our models &lt;strong&gt;remotely&lt;/strong&gt;: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test inference without a robot&lt;/strong&gt;: We provide a &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;script&lt;/a&gt; for testing inference without a robot. This script will generate a random observation and run inference with the model. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fine-Tuning Base Models on Your Own Data&lt;/h2&gt; 
&lt;p&gt;We will fine-tune the $\pi_{0.5}$ model on the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO dataset&lt;/a&gt; as a running example for how to fine-tune a base model on your own data. We will explain three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Convert your data to a LeRobot dataset (which we use for training)&lt;/li&gt; 
 &lt;li&gt;Defining training configs and running training&lt;/li&gt; 
 &lt;li&gt;Spinning up a policy server and running inference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. Convert your data to a LeRobot dataset&lt;/h3&gt; 
&lt;p&gt;We provide a minimal example script for converting LIBERO data to a LeRobot dataset in &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/convert_libero_data_to_lerobot.py"&gt;&lt;code&gt;examples/libero/convert_libero_data_to_lerobot.py&lt;/code&gt;&lt;/a&gt;. You can easily modify it to convert your own data! You can download the raw LIBERO dataset from &lt;a href="https://huggingface.co/datasets/openvla/modified_libero_rlds"&gt;here&lt;/a&gt;, and run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.&lt;/p&gt; 
&lt;h3&gt;2. Defining training configs and running training&lt;/h3&gt; 
&lt;p&gt;To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/policies/libero_policy.py"&gt;&lt;code&gt;LiberoInputs&lt;/code&gt; and &lt;code&gt;LiberoOutputs&lt;/code&gt;&lt;/a&gt;: Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;LeRobotLiberoDataConfig&lt;/code&gt;&lt;/a&gt;: Defines how to process raw LIBERO data from LeRobot dataset for training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;TrainConfig&lt;/code&gt;&lt;/a&gt;: Defines fine-tuning hyperparameters, data config, and weight loader.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We provide example fine-tuning configs for &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;œÄ‚ÇÄ&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;œÄ‚ÇÄ-FAST&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;œÄ‚ÇÄ.‚ÇÖ&lt;/a&gt; on LIBERO data.&lt;/p&gt; 
&lt;p&gt;Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/compute_norm_stats.py --config-name pi05_libero
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now we can kick off training with the following command (the &lt;code&gt;--overwrite&lt;/code&gt; flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command will log training progress to the console and save checkpoints to the &lt;code&gt;checkpoints&lt;/code&gt; directory. You can also monitor training progress on the Weights &amp;amp; Biases dashboard. For maximally using the GPU memory, set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We provide functionality for &lt;em&gt;reloading&lt;/em&gt; normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/norm_stats.md"&gt;norm_stats.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;3. Spinning up a policy server and running inference&lt;/h3&gt; 
&lt;p&gt;Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.&lt;/p&gt; 
&lt;p&gt;For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;remote inference docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;More Examples&lt;/h3&gt; 
&lt;p&gt;We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_sim"&gt;ALOHA Simulator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real"&gt;ALOHA Real&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/ur5"&gt;UR5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PyTorch Support&lt;/h2&gt; 
&lt;p&gt;openpi now provides PyTorch implementations of œÄ‚ÇÄ and œÄ‚ÇÄ.‚ÇÖ models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The œÄ‚ÇÄ-FAST model&lt;/li&gt; 
 &lt;li&gt;Mixed precision training&lt;/li&gt; 
 &lt;li&gt;FSDP (fully-sharded data parallelism) training&lt;/li&gt; 
 &lt;li&gt;LoRA (low-rank adaptation) training&lt;/li&gt; 
 &lt;li&gt;EMA (exponential moving average) weights during training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have the latest version of all dependencies installed: &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double check that you have transformers 4.53.2 installed: &lt;code&gt;uv pip show transformers&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Apply the transformers library patches:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run &lt;code&gt;uv cache clean transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Converting JAX Models to PyTorch&lt;/h3&gt; 
&lt;p&gt;To convert a JAX model checkpoint to PyTorch format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &amp;lt;config name&amp;gt; \
    --output_path /path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Inference with PyTorch&lt;/h3&gt; 
&lt;p&gt;The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = "/path/to/converted/pytorch/checkpoint"

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Policy Server with PyTorch&lt;/h3&gt; 
&lt;p&gt;The policy server works identically with PyTorch models - just point to the converted checkpoint directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Finetuning with PyTorch&lt;/h3&gt; 
&lt;p&gt;To finetune a model in PyTorch:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Convert the JAX base model to PyTorch format:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --config_name &amp;lt;config name&amp;gt; \
    --checkpoint_dir /path/to/jax/base/model \
    --output_path /path/to/pytorch/base/model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the converted PyTorch model path in your config using &lt;code&gt;pytorch_weight_path&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch training using one of these modes:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training:
uv run scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&amp;lt;num_gpus&amp;gt; scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&amp;lt;num_nodes&amp;gt; \
    --nproc_per_node=&amp;lt;gpus_per_node&amp;gt; \
    --node_rank=&amp;lt;rank_of_node&amp;gt; \
    --master_addr=&amp;lt;master_ip&amp;gt; \
    --master_port=&amp;lt;port&amp;gt; \
    scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name=&amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Precision Settings&lt;/h3&gt; 
&lt;p&gt;JAX and PyTorch implementations handle precision as follows:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JAX:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: most weights and computations in bfloat16, with a few computations in float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting &lt;code&gt;dtype&lt;/code&gt; to float32 in the config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: supports either full bfloat16 (default) or full float32. You can change it by setting &lt;code&gt;pytorch_training_precision&lt;/code&gt; in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With torch.compile, inference speed is comparable between JAX and PyTorch.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can't find a solution, please file an issue on the repo (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/CONTRIBUTING.md"&gt;here&lt;/a&gt; for guidelines).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Issue&lt;/th&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uv sync&lt;/code&gt; fails with dependency conflicts&lt;/td&gt; 
   &lt;td&gt;Try removing the virtual environment directory (&lt;code&gt;rm -rf .venv&lt;/code&gt;) and running &lt;code&gt;uv sync&lt;/code&gt; again. If issues persist, check that you have the latest version of &lt;code&gt;uv&lt;/code&gt; installed (&lt;code&gt;uv self update&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training runs out of GPU memory&lt;/td&gt; 
   &lt;td&gt;Make sure you set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; (or higher) before running training to allow JAX to use more GPU memory. You can also use &lt;code&gt;--fsdp-devices &amp;lt;n&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;n&amp;gt;&lt;/code&gt; is your number of GPUs, to enable &lt;a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/"&gt;fully-sharded data parallelism&lt;/a&gt;, which reduces memory usage in exchange for slower training (the amount of slowdown depends on your particular setup). If you are still running out of memory, you may way to consider disabling EMA.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Policy server connection errors&lt;/td&gt; 
   &lt;td&gt;Check that the server is running and listening on the expected port. Verify network connectivity and firewall settings between client and server.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Missing norm stats error when training&lt;/td&gt; 
   &lt;td&gt;Run &lt;code&gt;scripts/compute_norm_stats.py&lt;/code&gt; with your config name before starting training.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset download fails&lt;/td&gt; 
   &lt;td&gt;Check your internet connection. For HuggingFace datasets, ensure you're logged in (&lt;code&gt;huggingface-cli login&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA/GPU errors&lt;/td&gt; 
   &lt;td&gt;Verify NVIDIA drivers are installed correctly. For Docker, ensure nvidia-container-toolkit is installed. Check GPU compatibility. You do NOT need CUDA libraries installed at a system level --- they will be installed via uv. You may even want to try &lt;em&gt;uninstalling&lt;/em&gt; system CUDA libraries if you run into CUDA issues, since system libraries can sometimes cause conflicts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Import errors when running examples&lt;/td&gt; 
   &lt;td&gt;Make sure you've installed all dependencies with &lt;code&gt;uv sync&lt;/code&gt;. Some examples may have additional requirements listed in their READMEs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Action dimensions mismatch&lt;/td&gt; 
   &lt;td&gt;Verify your data processing transforms match the expected input/output dimensions of your robot. Check the action space definitions in your policy classes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diverging training loss&lt;/td&gt; 
   &lt;td&gt;Check the &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, and &lt;code&gt;std&lt;/code&gt; values in &lt;code&gt;norm_stats.json&lt;/code&gt; for your dataset. Certain dimensions that are rarely used can end up with very small &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; values, leading to huge states and actions after normalization. You can manually adjust the norm stats as a workaround.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>huggingface/transformers</title>
      <link>https://github.com/huggingface/transformers</link>
      <description>&lt;p&gt;ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" /&gt; 
  &lt;img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg?sanitize=true" width="352" height="59" style="max-width: 100%;" /&gt; 
 &lt;/picture&gt; &lt;br /&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://huggingface.com/models"&gt;&lt;img alt="Checkpoints on Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&amp;amp;color=brightgreen" /&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/huggingface/transformers"&gt;&lt;img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/transformers/raw/main/LICENSE"&gt;&lt;img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/docs/transformers/index"&gt;&lt;img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&amp;amp;down_message=offline&amp;amp;up_message=online" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/transformers/releases"&gt;&lt;img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/transformers/raw/main/CODE_OF_CONDUCT.md"&gt;&lt;img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://zenodo.org/badge/latestdoi/155220641"&gt;&lt;img src="https://zenodo.org/badge/155220641.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;p&gt; &lt;b&gt;English&lt;/b&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_zh-hans.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_zh-hant.md"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_hd.md"&gt;‡§π‡§ø‡§®‡•ç‡§¶‡•Ä&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ru.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_pt-br.md"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_te.md"&gt;‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_fr.md"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_de.md"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_vi.md"&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ar.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ur.md"&gt;ÿßÿ±ÿØŸà&lt;/a&gt; | &lt;/p&gt; &lt;/h4&gt; 
&lt;h3 align="center"&gt; &lt;p&gt;State-of-the-art pretrained models for inference and training&lt;/p&gt; &lt;/h3&gt; 
&lt;h3 align="center"&gt; &lt;img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png" /&gt; &lt;/h3&gt; 
&lt;p&gt;Transformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer vision, audio, video, and multimodal model, for both inference and training.&lt;/p&gt; 
&lt;p&gt;It centralizes the model definition so that this definition is agreed upon across the ecosystem. &lt;code&gt;transformers&lt;/code&gt; is the pivot across frameworks: if a model definition is supported, it will be compatible with the majority of training frameworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...), and adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;We pledge to help support new state-of-the-art models and democratize their usage by having their model definition be simple, customizable, and efficient.&lt;/p&gt; 
&lt;p&gt;There are over 1M+ Transformers &lt;a href="https://huggingface.co/models?library=transformers&amp;amp;sort=trending"&gt;model checkpoints&lt;/a&gt; on the &lt;a href="https://huggingface.com/models"&gt;Hugging Face Hub&lt;/a&gt; you can use.&lt;/p&gt; 
&lt;p&gt;Explore the &lt;a href="https://huggingface.com/"&gt;Hub&lt;/a&gt; today to find a model and use Transformers to help you get started right away.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Transformers works with Python 3.9+ &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt; 2.1+, &lt;a href="https://www.tensorflow.org/install/pip"&gt;TensorFlow&lt;/a&gt; 2.6+, and &lt;a href="https://flax.readthedocs.io/en/latest/"&gt;Flax&lt;/a&gt; 0.4.1+.&lt;/p&gt; 
&lt;p&gt;Create and activate a virtual environment with &lt;a href="https://docs.python.org/3/library/venv.html"&gt;venv&lt;/a&gt; or &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Rust-based Python package and project manager.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;# venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install Transformers in your virtual environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;# pip
pip install "transformers[torch]"

# uv
uv pip install "transformers[torch]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install Transformers from source if you want the latest changes in the library or are interested in contributing. However, the &lt;em&gt;latest&lt;/em&gt; version may not be stable. Feel free to open an &lt;a href="https://github.com/huggingface/transformers/issues"&gt;issue&lt;/a&gt; if you encounter an error.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/huggingface/transformers.git
cd transformers

# pip
pip install .[torch]

# uv
uv pip install .[torch]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Get started with Transformers right away with the &lt;a href="https://huggingface.co/docs/transformers/pipeline_tutorial"&gt;Pipeline&lt;/a&gt; API. The &lt;code&gt;Pipeline&lt;/code&gt; is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.&lt;/p&gt; 
&lt;p&gt;Instantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from transformers import pipeline

pipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")
pipeline("the secret to baking a really good cake is ")
[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to &lt;code&gt;Pipeline&lt;/code&gt;) between you and the system.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] You can also chat with a model directly from the command line.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;transformers chat Qwen/Qwen2.5-0.5B-Instruct
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from transformers import pipeline

chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Expand the examples below to see how &lt;code&gt;Pipeline&lt;/code&gt; works for different modalities and tasks.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Automatic speech recognition&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;from transformers import pipeline

pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Image classification&lt;/summary&gt; 
 &lt;h3 align="center"&gt; &lt;a&gt;&lt;img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png" /&gt;&lt;/a&gt; &lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;from transformers import pipeline

pipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")
pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorikeet', 'score': 0.00018523589824326336},
 {'label': 'African grey, African gray, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'quail', 'score': 5.502637941390276e-05}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Visual question answering&lt;/summary&gt; 
 &lt;h3 align="center"&gt; &lt;a&gt;&lt;img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg" /&gt;&lt;/a&gt; &lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;from transformers import pipeline

pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(
    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",
    question="What is in the image?",
)
[{'answer': 'statue of liberty'}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Why should I use Transformers?&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Easy-to-use state-of-the-art models:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;High performance on natural language understanding &amp;amp; generation, computer vision, audio, video, and multimodal tasks.&lt;/li&gt; 
   &lt;li&gt;Low barrier to entry for researchers, engineers, and developers.&lt;/li&gt; 
   &lt;li&gt;Few user-facing abstractions with just three classes to learn.&lt;/li&gt; 
   &lt;li&gt;A unified API for using all our pretrained models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Lower compute costs, smaller carbon footprint:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Share trained models instead of training from scratch.&lt;/li&gt; 
   &lt;li&gt;Reduce compute time and production costs.&lt;/li&gt; 
   &lt;li&gt;Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Choose the right framework for every part of a models lifetime:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Train state-of-the-art models in 3 lines of code.&lt;/li&gt; 
   &lt;li&gt;Move a single model between PyTorch/JAX/TF2.0 frameworks at will.&lt;/li&gt; 
   &lt;li&gt;Pick the right framework for training, evaluation, and production.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Easily customize a model or an example to your needs:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;We provide examples for each architecture to reproduce the results published by its original authors.&lt;/li&gt; 
   &lt;li&gt;Model internals are exposed as consistently as possible.&lt;/li&gt; 
   &lt;li&gt;Model files can be used independently of the library for quick experiments.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;a target="_blank" href="https://huggingface.co/enterprise"&gt; &lt;img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925" /&gt; &lt;/a&gt;
&lt;br /&gt; 
&lt;h2&gt;Why shouldn't I use Transformers?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.&lt;/li&gt; 
 &lt;li&gt;The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like &lt;a href="https://huggingface.co/docs/accelerate"&gt;Accelerate&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;The &lt;a href="https://github.com/huggingface/transformers/tree/main/examples"&gt;example scripts&lt;/a&gt; are only &lt;em&gt;examples&lt;/em&gt;. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;100 projects using Transformers&lt;/h2&gt; 
&lt;p&gt;Transformers is more than a toolkit to use pretrained models, it's a community of projects built around it and the Hugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone else to build their dream projects.&lt;/p&gt; 
&lt;p&gt;In order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the community with the &lt;a href="https://raw.githubusercontent.com/huggingface/transformers/main/awesome-transformers.md"&gt;awesome-transformers&lt;/a&gt; page which lists 100 incredible projects built with Transformers.&lt;/p&gt; 
&lt;p&gt;If you own or use a project that you believe should be part of the list, please open a PR to add it!&lt;/p&gt; 
&lt;h2&gt;Example models&lt;/h2&gt; 
&lt;p&gt;You can test most of our models directly on their &lt;a href="https://huggingface.co/models"&gt;Hub model pages&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Expand each modality below to see a few example models for various use cases.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Audio&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Audio classification with &lt;a href="https://huggingface.co/openai/whisper-large-v3-turbo"&gt;Whisper&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Automatic speech recognition with &lt;a href="https://huggingface.co/UsefulSensors/moonshine"&gt;Moonshine&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Keyword spotting with &lt;a href="https://huggingface.co/superb/wav2vec2-base-superb-ks"&gt;Wav2Vec2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Speech to speech generation with &lt;a href="https://huggingface.co/kyutai/moshiko-pytorch-bf16"&gt;Moshi&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Text to audio with &lt;a href="https://huggingface.co/facebook/musicgen-large"&gt;MusicGen&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Text to speech with &lt;a href="https://huggingface.co/suno/bark"&gt;Bark&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Computer vision&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Automatic mask generation with &lt;a href="https://huggingface.co/facebook/sam-vit-base"&gt;SAM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Depth estimation with &lt;a href="https://huggingface.co/apple/DepthPro-hf"&gt;DepthPro&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Image classification with &lt;a href="https://huggingface.co/facebook/dinov2-base"&gt;DINO v2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Keypoint detection with &lt;a href="https://huggingface.co/magic-leap-community/superpoint"&gt;SuperPoint&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Keypoint matching with &lt;a href="https://huggingface.co/magic-leap-community/superglue_outdoor"&gt;SuperGlue&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Object detection with &lt;a href="https://huggingface.co/PekingU/rtdetr_v2_r50vd"&gt;RT-DETRv2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Pose Estimation with &lt;a href="https://huggingface.co/usyd-community/vitpose-base-simple"&gt;VitPose&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Universal segmentation with &lt;a href="https://huggingface.co/shi-labs/oneformer_ade20k_swin_large"&gt;OneFormer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Video classification with &lt;a href="https://huggingface.co/MCG-NJU/videomae-large"&gt;VideoMAE&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multimodal&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Audio or text to text with &lt;a href="https://huggingface.co/Qwen/Qwen2-Audio-7B"&gt;Qwen2-Audio&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Document question answering with &lt;a href="https://huggingface.co/microsoft/layoutlmv3-base"&gt;LayoutLMv3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Image or text to text with &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct"&gt;Qwen-VL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Image captioning &lt;a href="https://huggingface.co/Salesforce/blip2-opt-2.7b"&gt;BLIP-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;OCR-based document understanding with &lt;a href="https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf"&gt;GOT-OCR2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Table question answering with &lt;a href="https://huggingface.co/google/tapas-base"&gt;TAPAS&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Unified multimodal understanding and generation with &lt;a href="https://huggingface.co/BAAI/Emu3-Gen"&gt;Emu3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Vision to text with &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf"&gt;Llava-OneVision&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Visual question answering with &lt;a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf"&gt;Llava&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Visual referring expression segmentation with &lt;a href="https://huggingface.co/microsoft/kosmos-2-patch14-224"&gt;Kosmos-2&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;NLP&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Masked word completion with &lt;a href="https://huggingface.co/answerdotai/ModernBERT-base"&gt;ModernBERT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Named entity recognition with &lt;a href="https://huggingface.co/google/gemma-2-2b"&gt;Gemma&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Question answering with &lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"&gt;Mixtral&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Summarization with &lt;a href="https://huggingface.co/facebook/bart-large-cnn"&gt;BART&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Translation with &lt;a href="https://huggingface.co/google-t5/t5-base"&gt;T5&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Text generation with &lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Text classification with &lt;a href="https://huggingface.co/Qwen/Qwen2.5-0.5B"&gt;Qwen&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We now have a &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-demos.6/"&gt;paper&lt;/a&gt; you can cite for the ü§ó Transformers library:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>protocolbuffers/protobuf</title>
      <link>https://github.com/protocolbuffers/protobuf</link>
      <description>&lt;p&gt;Protocol Buffers - Google's data interchange format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protocol Buffers - Google's data interchange format&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://securityscorecards.dev/viewer/?uri=github.com/protocolbuffers/protobuf"&gt;&lt;img src="https://api.securityscorecards.dev/projects/github.com/protocolbuffers/protobuf/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright 2023 Google LLC&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can learn more about it in &lt;a href="https://protobuf.dev"&gt;protobuf's documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.&lt;/p&gt; 
&lt;h2&gt;Working With Protobuf Source Code&lt;/h2&gt; 
&lt;p&gt;Most users will find working from &lt;a href="https://github.com/protocolbuffers/protobuf/releases"&gt;supported releases&lt;/a&gt; to be the easiest path.&lt;/p&gt; 
&lt;p&gt;If you choose to work from the head revision of the main branch your build will occasionally be broken by source-incompatible changes and insufficiently-tested (and therefore broken) behavior.&lt;/p&gt; 
&lt;p&gt;If you are using C++ or otherwise need to build protobuf from source as a part of your project, you should pin to a release commit on a release branch.&lt;/p&gt; 
&lt;p&gt;This is because even release branches can experience some instability in between release commits.&lt;/p&gt; 
&lt;h3&gt;Bazel with Bzlmod&lt;/h3&gt; 
&lt;p&gt;Protobuf supports &lt;a href="https://bazel.build/external/module"&gt;Bzlmod&lt;/a&gt; with Bazel 7 +. Users should specify a dependency on protobuf in their MODULE.bazel file as follows.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bazel_dep(name = "protobuf", version = &amp;lt;VERSION&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Users can optionally override the repo name, such as for compatibility with WORKSPACE.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bazel_dep(name = "protobuf", version = &amp;lt;VERSION&amp;gt;, repo_name = "com_google_protobuf")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Bazel with WORKSPACE&lt;/h3&gt; 
&lt;p&gt;Users can also add the following to their legacy &lt;a href="https://bazel.build/external/overview#workspace-system"&gt;WORKSPACE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Note that with the release of 30.x there are a few more load statements to properly set up rules_java and rules_python.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;http_archive(
    name = "com_google_protobuf",
    strip_prefix = "protobuf-VERSION",
    sha256 = ...,
    url = ...,
)

load("@com_google_protobuf//:protobuf_deps.bzl", "protobuf_deps")

protobuf_deps()

load("@rules_java//java:rules_java_deps.bzl", "rules_java_dependencies")

rules_java_dependencies()

load("@rules_java//java:repositories.bzl", "rules_java_toolchains")

rules_java_toolchains()

load("@rules_python//python:repositories.bzl", "py_repositories")

py_repositories()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Protobuf Compiler Installation&lt;/h2&gt; 
&lt;p&gt;The protobuf compiler is written in C++. If you are using C++, please follow the &lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md"&gt;C++ Installation Instructions&lt;/a&gt; to install protoc along with the C++ runtime.&lt;/p&gt; 
&lt;p&gt;For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our &lt;a href="https://github.com/protocolbuffers/protobuf/releases"&gt;GitHub release page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In the downloads section of each release, you can find pre-built binaries in zip packages: &lt;code&gt;protoc-$VERSION-$PLATFORM.zip&lt;/code&gt;. It contains the protoc binary as well as a set of standard &lt;code&gt;.proto&lt;/code&gt; files distributed along with protobuf.&lt;/p&gt; 
&lt;p&gt;If you are looking for an old version that is not available in the release page, check out the &lt;a href="https://repo1.maven.org/maven2/com/google/protobuf/protoc/"&gt;Maven repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it's recommended to build your own protoc binary from source.&lt;/p&gt; 
&lt;p&gt;If you would like to build protoc binary from source, see the &lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md"&gt;C++ Installation Instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Protobuf Runtime Installation&lt;/h2&gt; 
&lt;p&gt;Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C++ (include C++ runtime and protoc)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src"&gt;src&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Java&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/java"&gt;java&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python"&gt;python&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Objective-C&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/objectivec"&gt;objectivec&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C#&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/csharp"&gt;csharp&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ruby&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/ruby"&gt;ruby&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Go&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/protocolbuffers/protobuf-go"&gt;protocolbuffers/protobuf-go&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PHP&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/php"&gt;php&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dart&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/dart-lang/protobuf"&gt;dart-lang/protobuf&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JavaScript&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/protocolbuffers/protobuf-javascript"&gt;protocolbuffers/protobuf-javascript&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;The best way to learn how to use protobuf is to follow the &lt;a href="https://protobuf.dev/getting-started"&gt;tutorials in our developer guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to learn from code examples, take a look at the examples in the &lt;a href="https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples"&gt;examples&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The complete documentation is available at the &lt;a href="https://protobuf.dev"&gt;Protocol Buffers doc site&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support Policy&lt;/h2&gt; 
&lt;p&gt;Read about our &lt;a href="https://protobuf.dev/version-support/"&gt;version support policy&lt;/a&gt; to stay current on support timeframes for the language libraries.&lt;/p&gt; 
&lt;h2&gt;Developer Community&lt;/h2&gt; 
&lt;p&gt;To be alerted to upcoming changes in Protocol Buffers and connect with protobuf developers and users, &lt;a href="https://groups.google.com/g/protobuf"&gt;join the Google Group&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/garak</title>
      <link>https://github.com/NVIDIA/garak</link>
      <description>&lt;p&gt;the LLM vulnerability scanner&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;garak, LLM vulnerability scanner&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Generative AI Red-teaming &amp;amp; Assessment Kit&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; checks if an LLM can be made to fail in a way we don't want. &lt;code&gt;garak&lt;/code&gt; probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know &lt;code&gt;nmap&lt;/code&gt; or &lt;code&gt;msf&lt;/code&gt; / Metasploit Framework, garak does somewhat similar things to them, but for LLMs.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; focuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt;'s a free tool. We love developing it and are always interested in adding functionality to support applications.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg?sanitize=true" alt="Tests/Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg?sanitize=true" alt="Tests/Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg?sanitize=true" alt="Tests/OSX" /&gt;&lt;/a&gt; &lt;a href="http://garak.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/garak/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.11036"&gt;&lt;img src="https://img.shields.io/badge/cs.CL-arXiv%3A2406.11036-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/uVch4puUCs"&gt;&lt;img src="https://img.shields.io/badge/chat-on%20discord-yellow.svg?sanitize=true" alt="discord-img" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/garak"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/garak" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/garak"&gt;&lt;img src="https://badge.fury.io/py/garak.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak/month" alt="Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;h3&gt;&amp;gt; See our user guide! &lt;a href="https://docs.garak.ai/"&gt;docs.garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Join our &lt;a href="https://discord.gg/uVch4puUCs"&gt;Discord&lt;/a&gt;!&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Project links &amp;amp; home: &lt;a href="https://garak.ai/"&gt;garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Twitter: &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; DEF CON &lt;a href="https://garak.ai/garak_aiv_slides.pdf"&gt;slides&lt;/a&gt;!&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;h2&gt;LLM support&lt;/h2&gt; 
&lt;p&gt;currently supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/models"&gt;hugging face hub&lt;/a&gt; generative models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/"&gt;replicate&lt;/a&gt; text models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/docs/introduction"&gt;openai api&lt;/a&gt; chat &amp;amp; continuation models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;litellm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;pretty much anything accessible via REST&lt;/li&gt; 
 &lt;li&gt;gguf models like &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; version &amp;gt;= 1046&lt;/li&gt; 
 &lt;li&gt;.. and many more LLMs!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install:&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; is a command-line tool. It's developed in Linux and OSX.&lt;/p&gt; 
&lt;h3&gt;Standard install with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Just grab it from PyPI and you should be good to go:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U garak
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install development version with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;The standard pip version of &lt;code&gt;garak&lt;/code&gt; is updated periodically. To get a fresher version from GitHub, try:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Clone from source&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; has its own dependencies. You can to install &lt;code&gt;garak&lt;/code&gt; in its own Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda create --name garak "python&amp;gt;=3.10,&amp;lt;=3.12"
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OK, if that went fine, you're probably good to go!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you cloned before the move to the &lt;code&gt;NVIDIA&lt;/code&gt; GitHub organisation, but you're reading this at the &lt;code&gt;github.com/NVIDIA&lt;/code&gt; URI, please update your remotes as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git remote set-url origin https://github.com/NVIDIA/garak.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The general syntax is:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak &amp;lt;options&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak --list_probes&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;To specify a generator, use the &lt;code&gt;--model_type&lt;/code&gt; and, optionally, the &lt;code&gt;--model_name&lt;/code&gt; options. Model type specifies a model family/interface; model name specifies the exact model to be used. The "Intro to generators" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set &lt;code&gt;--model_type&lt;/code&gt; to &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;--model_name&lt;/code&gt; to the model's name on Hub (e.g. &lt;code&gt;"RWKV/rwkv-4-169m-pile"&lt;/code&gt;). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; runs all the probes by default, but you can be specific about that too. &lt;code&gt;--probes promptinject&lt;/code&gt; will use only the &lt;a href="https://github.com/agencyenterprise/promptinject"&gt;PromptInject&lt;/a&gt; framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a &lt;code&gt;.&lt;/code&gt;; for example, &lt;code&gt;--probes lmrc.SlurUsage&lt;/code&gt; will use an implementation of checking for models generating slurs based on the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; framework.&lt;/p&gt; 
&lt;p&gt;For help and inspiration, find us on &lt;a href="https://twitter.com/garak_llm"&gt;Twitter&lt;/a&gt; or &lt;a href="https://discord.gg/uVch4puUCs"&gt;discord&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY="sk-123XXXXXXXXXXXX"
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reading the results&lt;/h2&gt; 
&lt;p&gt;For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.&lt;/p&gt; 
&lt;p&gt;Here are the results with the &lt;code&gt;encoding&lt;/code&gt; module on a GPT-3 variant: &lt;img src="https://i.imgur.com/8Dxf45N.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;And the same results for ChatGPT: &lt;img src="https://i.imgur.com/VKAF5if.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections. The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.&lt;/p&gt; 
&lt;p&gt;Errors go in &lt;code&gt;garak.log&lt;/code&gt;; the run is logged in detail in a &lt;code&gt;.jsonl&lt;/code&gt; file specified at analysis start &amp;amp; end. There's a basic analysis script in &lt;code&gt;analyse/analyse_log.py&lt;/code&gt; which will output the probes and prompts that led to the most hits.&lt;/p&gt; 
&lt;p&gt;Send PRs &amp;amp; open issues. Happy hunting!&lt;/p&gt; 
&lt;h2&gt;Intro to generators&lt;/h2&gt; 
&lt;h3&gt;Hugging Face&lt;/h3&gt; 
&lt;p&gt;Using the Pipeline API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface&lt;/code&gt; (for transformers models to run locally)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using the Inference API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface.InferenceAPI&lt;/code&gt; (for API-based model access)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the model name from Hub, e.g. &lt;code&gt;"mosaicml/mpt-7b-instruct"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using private endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type huggingface.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_name&lt;/code&gt; - the endpoint URL, e.g. &lt;code&gt;https://xxx.us-east-1.aws.endpoints.huggingface.cloud&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(optional) set the &lt;code&gt;HF_INFERENCE_TOKEN&lt;/code&gt; environment variable to a Hugging Face API token with the "read" role; see &lt;a href="https://huggingface.co/settings/tokens"&gt;https://huggingface.co/settings/tokens&lt;/a&gt; when logged in&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenAI&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the OpenAI model you'd like to use. &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt; is fast and fine for testing.&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your OpenAI API key (e.g. "sk-19763ASDF87q6657"); see &lt;a href="https://platform.openai.com/account/api-keys"&gt;https://platform.openai.com/account/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.&lt;/p&gt; 
&lt;h3&gt;Replicate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;REPLICATE_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see &lt;a href="https://replicate.com/account/api-tokens"&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Public Replicate models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the Replicate model name and hash, e.g. &lt;code&gt;"stability-ai/stablelm-tuned-alpha-7b:c49dae36"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Private Replicate endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - username/model-name slug from the deployed endpoint, e.g. &lt;code&gt;elim/elims-llama2-7b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cohere&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type cohere&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; (optional, &lt;code&gt;command&lt;/code&gt; by default) - The specific Cohere model you'd like to test&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;COHERE_API_KEY&lt;/code&gt; environment variable to your Cohere API key, e.g. "aBcDeFgHiJ123456789"; see &lt;a href="https://dashboard.cohere.ai/api-keys"&gt;https://dashboard.cohere.ai/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Groq&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type groq&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The name of the model to access via the Groq API&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; environment variable to your Groq API key, see &lt;a href="https://console.groq.com/docs/quickstart"&gt;https://console.groq.com/docs/quickstart&lt;/a&gt; for details on creating an API key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ggml&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type ggml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The path to the ggml model you'd like to load, e.g. &lt;code&gt;/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GGML_MAIN_PATH&lt;/code&gt; environment variable to the path to your ggml &lt;code&gt;main&lt;/code&gt; executable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;REST&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;rest.RestGenerator&lt;/code&gt; is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See &lt;a href="https://reference.garak.ai/en/latest/garak.generators.rest.html"&gt;https://reference.garak.ai/en/latest/garak.generators.rest.html&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h3&gt;NIM&lt;/h3&gt; 
&lt;p&gt;Use models from &lt;a href="https://build.nvidia.com/"&gt;https://build.nvidia.com/&lt;/a&gt; or other NIM endpoints.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;NIM_API_KEY&lt;/code&gt; environment variable to your authentication API token, or specify it in the config YAML&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For chat models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;meta/llama-3.1-8b-instruct&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For completion models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim.NVOpenAICompletion&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;bigcode/starcoder2-15b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Test&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(alternatively) &lt;code&gt;--model_name test.Blank&lt;/code&gt; For testing. This always generates the empty string, using the &lt;code&gt;test.Blank&lt;/code&gt; generator. Will be marked as failing for any tests that &lt;em&gt;require&lt;/em&gt; an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test.Repeat&lt;/code&gt; For testing. This generator repeats back the prompt it received.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to probes&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Probe&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;blank&lt;/td&gt; 
   &lt;td&gt;A simple probe that always sends an empty prompt.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;atkgen&lt;/td&gt; 
   &lt;td&gt;Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 &lt;a href="https://huggingface.co/garak-llm/artgpt2tox"&gt;fine-tuned&lt;/a&gt; on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;av_spam_scanning&lt;/td&gt; 
   &lt;td&gt;Probes that attempt to make the model output malicious content signatures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;continuation&lt;/td&gt; 
   &lt;td&gt;Probes that test if the model will continue a probably undesirable word&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dan&lt;/td&gt; 
   &lt;td&gt;Various &lt;a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html"&gt;DAN&lt;/a&gt; and DAN-like attacks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;donotanswer&lt;/td&gt; 
   &lt;td&gt;Prompts to which responsible language models should not answer.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;encoding&lt;/td&gt; 
   &lt;td&gt;Prompt injection through text encoding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gcg&lt;/td&gt; 
   &lt;td&gt;Disrupt a system prompt by appending an adversarial suffix.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;glitch&lt;/td&gt; 
   &lt;td&gt;Probe model for glitch tokens that provoke unusual behavior.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;grandma&lt;/td&gt; 
   &lt;td&gt;Appeal to be reminded of one's grandmother.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;goodside&lt;/td&gt; 
   &lt;td&gt;Implementations of Riley Goodside attacks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;leakreplay&lt;/td&gt; 
   &lt;td&gt;Evaluate if a model will replay training data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lmrc&lt;/td&gt; 
   &lt;td&gt;Subsample of the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; probes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;malwaregen&lt;/td&gt; 
   &lt;td&gt;Attempts to have the model generate code for building malware&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;misleading&lt;/td&gt; 
   &lt;td&gt;Attempts to make a model support misleading and false claims&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;packagehallucination&lt;/td&gt; 
   &lt;td&gt;Trying to get code generations that specify non-existent (and therefore insecure) packages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;promptinject&lt;/td&gt; 
   &lt;td&gt;Implementation of the Agency Enterprise &lt;a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject"&gt;PromptInject&lt;/a&gt; work (best paper awards @ NeurIPS ML Safety Workshop 2022)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;realtoxicityprompts&lt;/td&gt; 
   &lt;td&gt;Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;snowball&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ofir.io/snowballed_hallucination.pdf"&gt;Snowballed Hallucination&lt;/a&gt; probes designed to make a model give a wrong answer to questions too complex for it to process&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xss&lt;/td&gt; 
   &lt;td&gt;Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Logging&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; generates multiple kinds of log:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A log file, &lt;code&gt;garak.log&lt;/code&gt;. This includes debugging information from &lt;code&gt;garak&lt;/code&gt; and its plugins, and is continued across runs.&lt;/li&gt; 
 &lt;li&gt;A report of the current run, structured as JSONL. A new report file is created every time &lt;code&gt;garak&lt;/code&gt; runs. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's &lt;code&gt;status&lt;/code&gt; attribute takes a constant from &lt;code&gt;garak.attempts&lt;/code&gt; to describe what stage it was made at.&lt;/li&gt; 
 &lt;li&gt;A hit log, detailing attempts that yielded a vulnerability (a 'hit')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How is the code structured?&lt;/h2&gt; 
&lt;p&gt;Check out the &lt;a href="https://reference.garak.ai/"&gt;reference docs&lt;/a&gt; for an authoritative guide to &lt;code&gt;garak&lt;/code&gt; code structure.&lt;/p&gt; 
&lt;p&gt;In a typical run, &lt;code&gt;garak&lt;/code&gt; will read a model type (and optionally model name) from the command line, then determine which &lt;code&gt;probe&lt;/code&gt;s and &lt;code&gt;detector&lt;/code&gt;s to run, start up a &lt;code&gt;generator&lt;/code&gt;, and then pass these to a &lt;code&gt;harness&lt;/code&gt; to do the probing; an &lt;code&gt;evaluator&lt;/code&gt; deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;garak/probes/&lt;/code&gt; - classes for generating interactions with LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/detectors/&lt;/code&gt; - classes for detecting an LLM is exhibiting a given failure mode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/evaluators/&lt;/code&gt; - assessment reporting schemes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/generators/&lt;/code&gt; - plugins for LLMs to be probed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/harnesses/&lt;/code&gt; - classes for structuring testing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resources/&lt;/code&gt; - ancillary items required by plugins&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The default operating mode is to use the &lt;code&gt;probewise&lt;/code&gt; harness. Given a list of probe module names and probe plugin names, the &lt;code&gt;probewise&lt;/code&gt; harness instantiates each probe, then for each probe reads its &lt;code&gt;recommended_detectors&lt;/code&gt; attribute to get a list of &lt;code&gt;detector&lt;/code&gt;s to run on the output.&lt;/p&gt; 
&lt;p&gt;Each plugin category (&lt;code&gt;probes&lt;/code&gt;, &lt;code&gt;detectors&lt;/code&gt;, &lt;code&gt;evaluators&lt;/code&gt;, &lt;code&gt;generators&lt;/code&gt;, &lt;code&gt;harnesses&lt;/code&gt;) includes a &lt;code&gt;base.py&lt;/code&gt; which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, &lt;code&gt;garak.generators.openai.OpenAIGenerator&lt;/code&gt; descends from &lt;code&gt;garak.generators.base.Generator&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using &lt;code&gt;garak&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Developing your own plugin&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Take a look at how other plugins do it&lt;/li&gt; 
 &lt;li&gt;Inherit from one of the base classes, e.g. &lt;code&gt;garak.probes.base.TextProbe&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Override as little as possible&lt;/li&gt; 
 &lt;li&gt;You can test the new code in at least two ways: 
  &lt;ul&gt; 
   &lt;li&gt;Start an interactive Python session 
    &lt;ul&gt; 
     &lt;li&gt;Import the model, e.g. &lt;code&gt;import garak.probes.mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;Instantiate the plugin, e.g. &lt;code&gt;p = garak.probes.mymodule.MyProbe()&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Run a scan with test plugins 
    &lt;ul&gt; 
     &lt;li&gt;For probes, try a blank generator and always.Pass detector: &lt;code&gt;python3 -m garak -m test.Blank -p mymodule -d always.Pass&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For detectors, try a blank generator and a blank probe: &lt;code&gt;python3 -m garak -m test.Blank -p test.Blank -d mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For generators, try a blank probe and always.Pass detector: &lt;code&gt;python3 -m garak -m mymodule -p test.Blank -d always.Pass&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Get &lt;code&gt;garak&lt;/code&gt; to list all the plugins of the type you're writing, with &lt;code&gt;--list_probes&lt;/code&gt;, &lt;code&gt;--list_detectors&lt;/code&gt;, or &lt;code&gt;--list_generators&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;We have an FAQ &lt;a href="https://github.com/NVIDIA/garak/raw/main/FAQ.md"&gt;here&lt;/a&gt;. Reach out if you have any more questions! &lt;a href="mailto:garak@nvidia.com"&gt;garak@nvidia.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Code reference documentation is at &lt;a href="https://garak.readthedocs.io/en/latest/"&gt;garak.readthedocs.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing garak&lt;/h2&gt; 
&lt;p&gt;You can read the &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/garak-paper.pdf"&gt;garak preprint paper&lt;/a&gt;. If you use garak, please cite us.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly"&lt;/em&gt; - Elim&lt;/p&gt; 
&lt;p&gt;For updates and news see &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;¬© 2023- Leon Derczynski; Apache license v2, see &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/happy-llm</title>
      <link>https://github.com/datawhalechina/happy-llm</link>
      <description>&lt;p&gt;üìö ‰ªéÈõ∂ÂºÄÂßãÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/head.jpg" alt="alt text" width="100%" /&gt; 
 &lt;h1&gt;Happy-LLM&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/github/stars/datawhalechina/happy-llm?style=flat&amp;amp;logo=github" alt="GitHub stars" /&gt; 
 &lt;img src="https://img.shields.io/github/forks/datawhalechina/happy-llm?style=flat&amp;amp;logo=github" alt="GitHub forks" /&gt; 
 &lt;img src="https://img.shields.io/badge/language-Chinese-brightgreen?style=flat" alt="Language" /&gt; 
 &lt;a href="https://github.com/datawhalechina/happy-llm"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github" alt="GitHub Project" /&gt;&lt;/a&gt; 
 &lt;a href="https://swanlab.cn/@kmno4/Happy-LLM/overview"&gt;&lt;img src="https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg?sanitize=true" alt="SwanLab" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14175" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14175" alt="datawhalechina%2Fhappy-llm | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://datawhalechina.github.io/happy-llm/"&gt;üìö Âú®Á∫øÈòÖËØªÂú∞ÂùÄ&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;üìö ‰ªéÈõ∂ÂºÄÂßãÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã&lt;/h3&gt; 
 &lt;p&gt;&lt;em&gt;Ê∑±ÂÖ•ÁêÜËß£ LLM Ê†∏ÂøÉÂéüÁêÜÔºåÂä®ÊâãÂÆûÁé∞‰Ω†ÁöÑÁ¨¨‰∏Ä‰∏™Â§ßÊ®°Âûã&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ È°πÁõÆ‰ªãÁªç&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;ÂæàÂ§öÂ∞è‰ºô‰º¥Âú®ÁúãÂÆå DatawhaleÂºÄÊ∫êÈ°πÁõÆÔºö &lt;a href="https://github.com/datawhalechina/self-llm"&gt;self-llm ÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó&lt;/a&gt; ÂêéÔºåÊÑüËßâÊÑèÁäπÊú™Â∞ΩÔºåÊÉ≥Ë¶ÅÊ∑±ÂÖ•‰∫ÜËß£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂéüÁêÜÂíåËÆ≠ÁªÉËøáÁ®ã„ÄÇ‰∫éÊòØÊàë‰ª¨ÔºàDatawhaleÔºâÂÜ≥ÂÆöÊé®Âá∫„ÄäHappy-LLM„ÄãÈ°πÁõÆÔºåÊó®Âú®Â∏ÆÂä©Â§ßÂÆ∂Ê∑±ÂÖ•ÁêÜËß£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂéüÁêÜÂíåËÆ≠ÁªÉËøáÁ®ã„ÄÇ&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÊòØ‰∏Ä‰∏™&lt;strong&gt;Á≥ªÁªüÊÄßÁöÑ LLM Â≠¶‰π†ÊïôÁ®ã&lt;/strong&gt;ÔºåÂ∞Ü‰ªé NLP ÁöÑÂü∫Êú¨Á†îÁ©∂ÊñπÊ≥ïÂá∫ÂèëÔºåÊ†πÊçÆ LLM ÁöÑÊÄùË∑ØÂèäÂéüÁêÜÈÄêÂ±ÇÊ∑±ÂÖ•Ôºå‰æùÊ¨°‰∏∫ËØªËÄÖÂâñÊûê LLM ÁöÑÊû∂ÊûÑÂü∫Á°ÄÂíåËÆ≠ÁªÉËøáÁ®ã„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨‰ºöÁªìÂêàÁõÆÂâç LLM È¢ÜÂüüÊúÄ‰∏ªÊµÅÁöÑ‰ª£Á†ÅÊ°ÜÊû∂ÔºåÊºîÁªÉÂ¶Ç‰Ωï‰∫≤ÊâãÊê≠Âª∫„ÄÅËÆ≠ÁªÉ‰∏Ä‰∏™ LLMÔºåÊúü‰ª•ÂÆûÁé∞Êéà‰πã‰ª•È±ºÔºåÊõ¥Êéà‰πã‰ª•Ê∏î„ÄÇÂ∏åÊúõÂ§ßÂÆ∂ËÉΩ‰ªéËøôÊú¨‰π¶ÂºÄÂßãËµ∞ÂÖ• LLM ÁöÑÊµ©ÁÄö‰∏ñÁïåÔºåÊé¢Á¥¢ LLM ÁöÑÊó†Â∞ΩÂèØËÉΩ„ÄÇ&lt;/p&gt; 
&lt;h3&gt;‚ú® ‰Ω†Â∞ÜÊî∂Ëé∑‰ªÄ‰πàÔºü&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Datawhale ÂºÄÊ∫êÂÖçË¥π&lt;/strong&gt; ÂÆåÂÖ®ÂÖçË¥πÁöÑÂ≠¶‰π†Êú¨È°πÁõÆÊâÄÊúâÂÜÖÂÆπ&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Ê∑±ÂÖ•ÁêÜËß£&lt;/strong&gt; Transformer Êû∂ÊûÑÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;ÊéåÊè°&lt;/strong&gt; È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÂü∫Êú¨ÂéüÁêÜ&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;‰∫ÜËß£&lt;/strong&gt; Áé∞ÊúâÂ§ßÊ®°ÂûãÁöÑÂü∫Êú¨ÁªìÊûÑ&lt;/li&gt; 
 &lt;li&gt;üèóÔ∏è &lt;strong&gt;Âä®ÊâãÂÆûÁé∞&lt;/strong&gt; ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ LLaMA2 Ê®°Âûã&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;ÊéåÊè°ËÆ≠ÁªÉ&lt;/strong&gt; ‰ªéÈ¢ÑËÆ≠ÁªÉÂà∞ÂæÆË∞ÉÁöÑÂÖ®ÊµÅÁ®ã&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;ÂÆûÊàòÂ∫îÁî®&lt;/strong&gt; RAG„ÄÅAgent Á≠âÂâçÊ≤øÊäÄÊúØ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìñ ÂÜÖÂÆπÂØºËà™&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Á´†ËäÇ&lt;/th&gt; 
   &lt;th&gt;ÂÖ≥ÈîÆÂÜÖÂÆπ&lt;/th&gt; 
   &lt;th&gt;Áä∂ÊÄÅ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/%E5%89%8D%E8%A8%80.md"&gt;ÂâçË®Ä&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Êú¨È°πÁõÆÁöÑÁºòËµ∑„ÄÅËÉåÊôØÂèäËØªËÄÖÂª∫ËÆÆ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20NLP%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5.md"&gt;Á¨¨‰∏ÄÁ´† NLP Âü∫Á°ÄÊ¶ÇÂøµ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰ªÄ‰πàÊòØ NLP„ÄÅÂèëÂ±ïÂéÜÁ®ã„ÄÅ‰ªªÂä°ÂàÜÁ±ª„ÄÅÊñáÊú¨Ë°®Á§∫ÊºîËøõ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20Transformer%E6%9E%B6%E6%9E%84.md"&gt;Á¨¨‰∫åÁ´† Transformer Êû∂ÊûÑ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ê≥®ÊÑèÂäõÊú∫Âà∂„ÄÅEncoder-Decoder„ÄÅÊâãÊääÊâãÊê≠Âª∫ Transformer&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md"&gt;Á¨¨‰∏âÁ´† È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Encoder-only„ÄÅEncoder-Decoder„ÄÅDecoder-Only Ê®°ÂûãÂØπÊØî&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md"&gt;Á¨¨ÂõõÁ´† Â§ßËØ≠Ë®ÄÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;LLM ÂÆö‰πâ„ÄÅËÆ≠ÁªÉÁ≠ñÁï•„ÄÅÊ∂åÁé∞ËÉΩÂäõÂàÜÊûê&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B.md"&gt;Á¨¨‰∫îÁ´† Âä®ÊâãÊê≠Âª∫Â§ßÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ÂÆûÁé∞ LLaMA2„ÄÅËÆ≠ÁªÉ Tokenizer„ÄÅÈ¢ÑËÆ≠ÁªÉÂ∞èÂûã LLM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E5%AE%9E%E8%B7%B5.md"&gt;Á¨¨ÂÖ≠Á´† Â§ßÊ®°ÂûãËÆ≠ÁªÉÂÆûË∑µ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;È¢ÑËÆ≠ÁªÉ„ÄÅÊúâÁõëÁù£ÂæÆË∞É„ÄÅLoRA/QLoRA È´òÊïàÂæÆË∞É&lt;/td&gt; 
   &lt;td&gt;üöß&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8.md"&gt;Á¨¨‰∏ÉÁ´† Â§ßÊ®°ÂûãÂ∫îÁî®&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ê®°ÂûãËØÑÊµã„ÄÅRAG Ê£ÄÁ¥¢Â¢ûÂº∫„ÄÅAgent Êô∫ËÉΩ‰Ωì&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/"&gt;Extra Chapter LLM Blog&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰ºòÁßÄÁöÑÂ§ßÊ®°Âûã Â≠¶‰π†Á¨îËÆ∞/Blog ÔºåÊ¨¢ËøéÂ§ßÂÆ∂Êù• PR ÔºÅ&lt;/td&gt; 
   &lt;td&gt;üöß&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Extra Chapter LLM Blog&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/why-fine-tune-small-large-language-models/readme.md"&gt;Â§ßÊ®°ÂûãÈÉΩËøô‰πàÂéâÂÆ≥‰∫ÜÔºåÂæÆË∞É0.6BÁöÑÂ∞èÊ®°ÂûãÊúâ‰ªÄ‰πàÊÑè‰πâÔºü&lt;/a&gt; @&lt;a href="https://github.com/KMnO4-zx"&gt;‰∏çË¶ÅËë±ÂßúËíú&lt;/a&gt; 2025-7-11&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/transformer-architecture/"&gt;Transformer Êï¥‰ΩìÊ®°ÂùóËÆæËÆ°Ëß£ËØª&lt;/a&gt; @&lt;a href="https://github.com/ditingdapeng"&gt;ditingdapeng&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/text-data-processing/readme.md"&gt;ÊñáÊú¨Êï∞ÊçÆÂ§ÑÁêÜËØ¶Ëß£&lt;/a&gt; @&lt;a href="https://github.com/xinala-781"&gt;Ëî°ÈãÜÊç∑&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/vlm-concatenation-finetune/README.md"&gt;Qwen3-"VL"‚Äî‚ÄîË∂ÖÂ∞è‰∏≠ÊñáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑ‚ÄúÊãºÊé•ÂæÆË∞É‚Äù‰πãË∑Ø&lt;/a&gt; @&lt;a href="https://github.com/ShaohonChen"&gt;ShaohonChen&lt;/a&gt; 2025-7-30&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/s1-vllm-thinking-budget/readme.md"&gt;S1: Thinking Budget with vLLM&lt;/a&gt; @&lt;a href="https://github.com/kmno4-zx"&gt;kmno4-zx&lt;/a&gt; 2025-8-03&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/CDDRS/readme.md"&gt;CDDRS: ‰ΩøÁî®ÁªÜÁ≤íÂ∫¶ËØ≠‰πâ‰ø°ÊÅØÊåáÂØºÂ¢ûÂº∫ÁöÑRAGÊ£ÄÁ¥¢ÊñπÊ≥ï&lt;/a&gt; @&lt;a href="https://github.com/Hongru0306"&gt;Hongru0306&lt;/a&gt; 2025-8-21&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;Â¶ÇÊûúÂ§ßÂÆ∂Âú®Â≠¶‰π† Happy-LLM È°πÁõÆÊàñ LLM Áõ∏ÂÖ≥Áü•ËØÜ‰∏≠ÊúâËá™Â∑±Áã¨Âà∞ÁöÑËßÅËß£„ÄÅËÆ§Áü•„ÄÅÂÆûË∑µÔºåÊ¨¢ËøéÂ§ßÂÆ∂ PR Âú® &lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/"&gt;Extra Chapter LLM Blog&lt;/a&gt; ‰∏≠„ÄÇËØ∑ÈÅµÂÆà Extra Chapter LLM Blog ÁöÑ &lt;a href="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/Readme.md"&gt;PR ËßÑËåÉ&lt;/a&gt;ÔºåÊàë‰ª¨‰ºöËßÜ PR ÂÜÖÂÆπÁöÑË¥®ÈáèÂíå‰ª∑ÂÄºÊù•ÂÜ≥ÂÆöÊòØÂê¶ÂêàÂπ∂ÊàñË°•ÂÖÖÂà∞ Happy-LLM Ê≠£Êñá‰∏≠Êù•„ÄÇ&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ê®°ÂûãÂêçÁß∞&lt;/th&gt; 
   &lt;th&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Happy-LLM-Chapter5-Base-215M&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-base"&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Happy-LLM-Chapter5-SFT-215M&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-sft"&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;ModelScope ÂàõÁ©∫Èó¥‰ΩìÈ™åÂú∞ÂùÄÔºö&lt;a href="https://www.modelscope.cn/studios/kmno4zx/happy_llm_215M_sft"&gt;ü§ñ ÂàõÁ©∫Èó¥&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;PDF ÁâàÊú¨‰∏ãËΩΩ&lt;/h3&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;&lt;strong&gt;Êú¨ Happy-LLM PDF ÊïôÁ®ãÂÆåÂÖ®ÂºÄÊ∫êÂÖçË¥π„ÄÇ‰∏∫Èò≤Ê≠¢ÂêÑÁ±ªËê•ÈîÄÂè∑Âä†Ê∞¥Âç∞ÂêéË¥©ÂçñÁªôÂ§ßÊ®°ÂûãÂàùÂ≠¶ËÄÖÔºåÊàë‰ª¨ÁâπÂú∞Âú® PDF Êñá‰ª∂‰∏≠È¢ÑÂÖàÊ∑ªÂä†‰∫Ü‰∏çÂΩ±ÂìçÈòÖËØªÁöÑ Datawhale ÂºÄÊ∫êÊ†áÂøóÊ∞¥Âç∞ÔºåÊï¨ËØ∑Ë∞ÖËß£ÔΩû&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Happy-LLM PDF : &lt;a href="https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1"&gt;https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üí° Â¶Ç‰ΩïÂ≠¶‰π†&lt;/h2&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÈÄÇÂêàÂ§ßÂ≠¶Áîü„ÄÅÁ†îÁ©∂‰∫∫Âëò„ÄÅLLM Áà±Â•ΩËÄÖ„ÄÇÂú®Â≠¶‰π†Êú¨È°πÁõÆ‰πãÂâçÔºåÂª∫ËÆÆÂÖ∑Â§á‰∏ÄÂÆöÁöÑÁºñÁ®ãÁªèÈ™åÔºåÂ∞§ÂÖ∂ÊòØË¶ÅÂØπ Python ÁºñÁ®ãËØ≠Ë®ÄÊúâ‰∏ÄÂÆöÁöÑ‰∫ÜËß£„ÄÇÊúÄÂ•ΩÂÖ∑Â§áÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁõ∏ÂÖ≥Áü•ËØÜÔºåÂπ∂‰∫ÜËß£ NLP È¢ÜÂüüÁöÑÁõ∏ÂÖ≥Ê¶ÇÂøµÂíåÊúØËØ≠Ôºå‰ª•‰æøÊõ¥ËΩªÊùæÂú∞Â≠¶‰π†Êú¨È°πÁõÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÂàÜ‰∏∫‰∏§ÈÉ®ÂàÜ‚Äî‚ÄîÂü∫Á°ÄÁü•ËØÜ‰∏éÂÆûÊàòÂ∫îÁî®„ÄÇÁ¨¨1Á´†ÔΩûÁ¨¨4Á´†ÊòØÂü∫Á°ÄÁü•ËØÜÈÉ®ÂàÜÔºå‰ªéÊµÖÂÖ•Ê∑±‰ªãÁªç LLM ÁöÑÂü∫Êú¨ÂéüÁêÜ„ÄÇÂÖ∂‰∏≠ÔºåÁ¨¨1Á´†ÁÆÄÂçï‰ªãÁªç NLP ÁöÑÂü∫Êú¨‰ªªÂä°ÂíåÂèëÂ±ïÔºå‰∏∫Èùû NLP È¢ÜÂüüÁ†îÁ©∂ËÄÖÊèê‰æõÂèÇËÄÉÔºõÁ¨¨2Á´†‰ªãÁªç LLM ÁöÑÂü∫Êú¨Êû∂ÊûÑ‚Äî‚ÄîTransformerÔºåÂåÖÊã¨ÂéüÁêÜ‰ªãÁªçÂèä‰ª£Á†ÅÂÆûÁé∞Ôºå‰Ωú‰∏∫ LLM ÊúÄÈáçË¶ÅÁöÑÁêÜËÆ∫Âü∫Á°ÄÔºõÁ¨¨3Á´†Êï¥‰Ωì‰ªãÁªçÁªèÂÖ∏ÁöÑ PLMÔºåÂåÖÊã¨ Encoder-Only„ÄÅEncoder-Decoder Âíå Decoder-Only ‰∏âÁßçÊû∂ÊûÑÔºå‰πüÂêåÊó∂‰ªãÁªç‰∫ÜÂΩìÂâç‰∏Ä‰∫õ‰∏ªÊµÅ LLM ÁöÑÊû∂ÊûÑÂíåÊÄùÊÉ≥ÔºõÁ¨¨4Á´†ÂàôÊ≠£ÂºèËøõÂÖ• LLM ÈÉ®ÂàÜÔºåËØ¶ÁªÜ‰ªãÁªç LLM ÁöÑÁâπÁÇπ„ÄÅËÉΩÂäõÂíåÊï¥‰ΩìËÆ≠ÁªÉËøáÁ®ã„ÄÇÁ¨¨5Á´†ÔΩûÁ¨¨7Á´†ÊòØÂÆûÊàòÂ∫îÁî®ÈÉ®ÂàÜÔºåÂ∞ÜÈÄêÊ≠•Â∏¶È¢ÜÂ§ßÂÆ∂Ê∑±ÂÖ• LLM ÁöÑÂ∫ïÂ±ÇÁªÜËäÇ„ÄÇÂÖ∂‰∏≠ÔºåÁ¨¨5Á´†Â∞ÜÂ∏¶È¢ÜÂ§ßÂÆ∂ËÄÖÂü∫‰∫é PyTorch Â±Ç‰∫≤ÊâãÊê≠Âª∫‰∏Ä‰∏™ LLMÔºåÂπ∂ÂÆûÁé∞È¢ÑËÆ≠ÁªÉ„ÄÅÊúâÁõëÁù£ÂæÆË∞ÉÁöÑÂÖ®ÊµÅÁ®ãÔºõÁ¨¨6Á´†Â∞ÜÂºïÂÖ•ÁõÆÂâç‰∏öÁïå‰∏ªÊµÅÁöÑ LLM ËÆ≠ÁªÉÊ°ÜÊû∂ TransformersÔºåÂ∏¶È¢ÜÂ≠¶‰π†ËÄÖÂü∫‰∫éËØ•Ê°ÜÊû∂Âø´ÈÄü„ÄÅÈ´òÊïàÂú∞ÂÆûÁé∞ LLM ËÆ≠ÁªÉËøáÁ®ãÔºõÁ¨¨7Á´†ÂàôÂ∞Ü‰ªãÁªç Âü∫‰∫é LLM ÁöÑÂêÑÁßçÂ∫îÁî®ÔºåË°•ÂÖ®Â≠¶‰π†ËÄÖÂØπ LLM ‰ΩìÁ≥ªÁöÑËÆ§Áü•ÔºåÂåÖÊã¨ LLM ÁöÑËØÑÊµã„ÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRetrieval-Augmented GenerationÔºåRAGÔºâ„ÄÅÊô∫ËÉΩ‰ΩìÔºàAgentÔºâÁöÑÊÄùÊÉ≥ÂíåÁÆÄÂçïÂÆûÁé∞„ÄÇ‰Ω†ÂèØ‰ª•Ê†πÊçÆ‰∏™‰∫∫ÂÖ¥Ë∂£ÂíåÈúÄÊ±ÇÔºåÈÄâÊã©ÊÄßÂú∞ÈòÖËØªÁõ∏ÂÖ≥Á´†ËäÇ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÂú®ÈòÖËØªÊú¨‰π¶ÁöÑËøáÁ®ã‰∏≠ÔºåÂª∫ËÆÆ‰Ω†Â∞ÜÁêÜËÆ∫ÂíåÂÆûÈôÖÁõ∏ÁªìÂêà„ÄÇLLM ÊòØ‰∏Ä‰∏™Âø´ÈÄüÂèëÂ±ï„ÄÅÊ≥®ÈáçÂÆûË∑µÁöÑÈ¢ÜÂüüÔºåÊàë‰ª¨Âª∫ËÆÆ‰Ω†Â§öÊäïÂÖ•ÂÆûÊàòÔºåÂ§çÁé∞Êú¨‰π¶Êèê‰æõÁöÑÂêÑÁßç‰ª£Á†ÅÔºåÂêåÊó∂ÁßØÊûÅÂèÇÂä† LLM Áõ∏ÂÖ≥ÁöÑÈ°πÁõÆ‰∏éÊØîËµõÔºåÁúüÊ≠£ÊäïÂÖ•Âà∞ LLM ÂºÄÂèëÁöÑÊµ™ÊΩÆ‰∏≠„ÄÇÊàë‰ª¨ÈºìÂä±‰Ω†ÂÖ≥Ê≥® Datawhale ÂèäÂÖ∂‰ªñ LLM Áõ∏ÂÖ≥ÂºÄÊ∫êÁ§æÂå∫ÔºåÂΩìÈÅáÂà∞ÈóÆÈ¢òÊó∂Ôºå‰Ω†ÂèØ‰ª•ÈöèÊó∂Âú®Êú¨È°πÁõÆÁöÑ issue Âå∫ÊèêÈóÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊúÄÂêéÔºåÊ¨¢ËøéÊØè‰∏Ä‰ΩçËØªËÄÖÂú®Â≠¶‰π†ÂÆåÊú¨È°πÁõÆÂêéÂä†ÂÖ•Âà∞ LLM ÂºÄÂèëËÄÖÁöÑË°åÂàó„ÄÇ‰Ωú‰∏∫ÂõΩÂÜÖ AI ÂºÄÊ∫êÁ§æÂå∫ÔºåÊàë‰ª¨Â∏åÊúõÂÖÖÂàÜËÅöÈõÜÂÖ±ÂàõËÄÖÔºå‰∏ÄËµ∑‰∏∞ÂØåËøô‰∏™ÂºÄÊ∫ê LLM ÁöÑ‰∏ñÁïåÔºåÊâìÈÄ†Êõ¥Â§ö„ÄÅÊõ¥ÂÖ®Èù¢ÁâπËâ≤ LLM ÁöÑÊïôÁ®ã„ÄÇÊòüÁÅ´ÁÇπÁÇπÔºåÊ±áËÅöÊàêÊµ∑„ÄÇÊàë‰ª¨Â∏åÊúõÊàê‰∏∫ LLM ‰∏éÊôÆÁΩóÂ§ß‰ºóÁöÑÈò∂Ê¢ØÔºå‰ª•Ëá™Áî±„ÄÅÂπ≥Á≠âÁöÑÂºÄÊ∫êÁ≤æÁ•ûÔºåÊã•Êä±Êõ¥ÊÅ¢ÂºòËÄåËæΩÈòîÁöÑ LLM ‰∏ñÁïå„ÄÇ&lt;/p&gt; 
&lt;h2&gt;ü§ù Â¶Ç‰ΩïË¥°ÁåÆ&lt;/h2&gt; 
&lt;p&gt;Êàë‰ª¨Ê¨¢Ëøé‰ªª‰ΩïÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºÅ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Êä•Âëä Bug&lt;/strong&gt; - ÂèëÁé∞ÈóÆÈ¢òËØ∑Êèê‰∫§ Issue&lt;/li&gt; 
 &lt;li&gt;üí° &lt;strong&gt;ÂäüËÉΩÂª∫ËÆÆ&lt;/strong&gt; - ÊúâÂ•ΩÊÉ≥Ê≥ïÂ∞±ÂëäËØâÊàë‰ª¨&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;ÂÜÖÂÆπÂÆåÂñÑ&lt;/strong&gt; - Â∏ÆÂä©ÊîπËøõÊïôÁ®ãÂÜÖÂÆπ&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;‰ª£Á†Å‰ºòÂåñ&lt;/strong&gt; - Êèê‰∫§ Pull Request&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Ëá¥Ë∞¢&lt;/h2&gt; 
&lt;h3&gt;Ê†∏ÂøÉË¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KMnO4-zx"&gt;ÂÆãÂøóÂ≠¶-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; (DatawhaleÊàêÂëò-‰∏≠ÂõΩÁüø‰∏öÂ§ßÂ≠¶(Âåó‰∫¨))&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/logan-zou"&gt;ÈÇπÈõ®Ë°°-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; (DatawhaleÊàêÂëò-ÂØπÂ§ñÁªèÊµéË¥∏ÊòìÂ§ßÂ≠¶)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xinzhongzhu.github.io/"&gt;Êú±‰ø°Âø†-ÊåáÂØº‰∏ìÂÆ∂&lt;/a&gt;ÔºàDatawhaleÈ¶ñÂ∏≠ÁßëÂ≠¶ÂÆ∂-ÊµôÊ±üÂ∏àËåÉÂ§ßÂ≠¶Êù≠Â∑û‰∫∫Â∑•Êô∫ËÉΩÁ†îÁ©∂Èô¢ÊïôÊéàÔºâ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extra-Chapter Ë¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ditingdapeng"&gt;ditingdapeng&lt;/a&gt;ÔºàÂÜÖÂÆπË¥°ÁåÆËÄÖ-‰∫ëÂéüÁîüÂü∫Á°ÄÊû∂ÊûÑÂ∑•Á®ãÂ∏àÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xinala-781"&gt;Ëî°ÈãÜÊç∑&lt;/a&gt;ÔºàÂÜÖÂÆπË¥°ÁåÆËÄÖ-Á¶èÂ∑ûÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ShaohonChen"&gt;ShaohonChen&lt;/a&gt; ÔºàÊÉÖÊÑüÊú∫Âô®ÂÆûÈ™åÂÆ§Á†îÁ©∂Âëò-Ë•øÂÆâÁîµÂ≠êÁßëÊäÄÂ§ßÂ≠¶Âú®ËØªÁ°ïÂ£´Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Hongru0306"&gt;ËÇñÈ∏øÂÑí, Â∫ÑÂÅ•Áê®&lt;/a&gt; (ÂÜÖÂÆπË¥°ÁåÆËÄÖ-ÂêåÊµéÂ§ßÂ≠¶)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÁâπÂà´ÊÑüË∞¢&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÊÑüË∞¢ &lt;a href="https://github.com/Sm1les"&gt;@Sm1les&lt;/a&gt; ÂØπÊú¨È°πÁõÆÁöÑÂ∏ÆÂä©‰∏éÊîØÊåÅ&lt;/li&gt; 
 &lt;li&gt;ÊÑüË∞¢ÊâÄÊúâ‰∏∫Êú¨È°πÁõÆÂÅöÂá∫Ë¥°ÁåÆÁöÑÂºÄÂèëËÄÖ‰ª¨ ‚ù§Ô∏è&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center" style="margin-top: 30px;"&gt; 
 &lt;a href="https://github.com/datawhalechina/happy-llm/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=datawhalechina/happy-llm" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/star-history-2025710.png" alt="Datawhale" width="90%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‚≠ê Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨‰∏Ä‰∏™ StarÔºÅ&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ÂÖ≥‰∫é Datawhale&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/datawhale.png" alt="Datawhale" width="30%" /&gt; 
 &lt;p&gt;Êâ´Êèè‰∫åÁª¥Á†ÅÂÖ≥Ê≥® Datawhale ÂÖ¨‰ºóÂè∑ÔºåËé∑ÂèñÊõ¥Â§ö‰ºòË¥®ÂºÄÊ∫êÂÜÖÂÆπ&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìú ÂºÄÊ∫êÂçèËÆÆ&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ΩúÂìÅÈááÁî®&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;Áü•ËØÜÂÖ±‰∫´ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî®-Áõ∏ÂêåÊñπÂºèÂÖ±‰∫´ 4.0 ÂõΩÈôÖËÆ∏ÂèØÂçèËÆÆ&lt;/a&gt;ËøõË°åËÆ∏ÂèØ„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebook/folly</title>
      <link>https://github.com/facebook/folly</link>
      <description>&lt;p&gt;An open-source C++ library developed and used at Facebook.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Folly: Facebook Open-source Library&lt;/h1&gt; 
&lt;a href="https://opensource.facebook.com/support-ukraine"&gt; &lt;img src="https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB" alt="Support Ukraine - Help Provide Humanitarian Aid to Ukraine." /&gt; &lt;/a&gt; 
&lt;h1&gt;What is &lt;code&gt;folly&lt;/code&gt;?&lt;/h1&gt; 
&lt;img src="https://raw.githubusercontent.com/facebook/folly/main/static/logo.svg?sanitize=true" alt="Logo Folly" width="15%" align="right" /&gt; 
&lt;p&gt;Folly (acronymed loosely after Facebook Open Source Library) is a library of C++17 components designed with practicality and efficiency in mind. &lt;strong&gt;Folly contains a variety of core library components used extensively at Facebook&lt;/strong&gt;. In particular, it's often a dependency of Facebook's other open source C++ efforts and place where those projects can share code.&lt;/p&gt; 
&lt;p&gt;It complements (as opposed to competing against) offerings such as Boost and of course &lt;code&gt;std&lt;/code&gt;. In fact, we embark on defining our own component only when something we need is either not available, or does not meet the needed performance profile. We endeavor to remove things from folly if or when &lt;code&gt;std&lt;/code&gt; or Boost obsoletes them.&lt;/p&gt; 
&lt;p&gt;Performance concerns permeate much of Folly, sometimes leading to designs that are more idiosyncratic than they would otherwise be (see e.g. &lt;code&gt;PackedSyncPtr.h&lt;/code&gt;, &lt;code&gt;SmallLocks.h&lt;/code&gt;). Good performance at large scale is a unifying theme in all of Folly.&lt;/p&gt; 
&lt;h2&gt;Check it out in the intro video&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Wr_IfOICYSs"&gt;&lt;img src="https://img.youtube.com/vi/Wr_IfOICYSs/0.jpg" alt="Explain Like I‚Äôm 5: Folly" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Logical Design&lt;/h1&gt; 
&lt;p&gt;Folly is a collection of relatively independent components, some as simple as a few symbols. There is no restriction on internal dependencies, meaning that a given folly module may use any other folly components.&lt;/p&gt; 
&lt;p&gt;All symbols are defined in the top-level namespace &lt;code&gt;folly&lt;/code&gt;, except of course macros. Macro names are ALL_UPPERCASE and should be prefixed with &lt;code&gt;FOLLY_&lt;/code&gt;. Namespace &lt;code&gt;folly&lt;/code&gt; defines other internal namespaces such as &lt;code&gt;internal&lt;/code&gt; or &lt;code&gt;detail&lt;/code&gt;. User code should not depend on symbols in those namespaces.&lt;/p&gt; 
&lt;h1&gt;Physical Design&lt;/h1&gt; 
&lt;p&gt;At the top level Folly uses the classic "stuttering" scheme &lt;code&gt;folly/folly&lt;/code&gt; used by Boost and others. The first directory serves as an installation root of the library (with possible versioning a la &lt;code&gt;folly-1.0/&lt;/code&gt;), and the second is to distinguish the library when including files, e.g. &lt;code&gt;#include &amp;lt;folly/FBString.h&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The directory structure is flat (mimicking the namespace structure), i.e. we don't have an elaborate directory hierarchy (it is possible this will change in future versions). The subdirectory &lt;code&gt;experimental&lt;/code&gt; contains files that are used inside folly and possibly at Facebook but not considered stable enough for client use. Your code should not use files in &lt;code&gt;folly/experimental&lt;/code&gt; lest it may break when you update Folly.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;folly/folly/test&lt;/code&gt; subdirectory includes the unittests for all components, usually named &lt;code&gt;ComponentXyzTest.cpp&lt;/code&gt; for each &lt;code&gt;ComponentXyz.*&lt;/code&gt;. The &lt;code&gt;folly/folly/docs&lt;/code&gt; directory contains documentation.&lt;/p&gt; 
&lt;h1&gt;What's in it?&lt;/h1&gt; 
&lt;p&gt;Because of folly's fairly flat structure, the best way to see what's in it is to look at the headers in &lt;a href="https://github.com/facebook/folly/tree/main/folly"&gt;top level &lt;code&gt;folly/&lt;/code&gt; directory&lt;/a&gt;. You can also check the &lt;a href="https://raw.githubusercontent.com/facebook/folly/main/folly/docs"&gt;&lt;code&gt;docs&lt;/code&gt; folder&lt;/a&gt; for documentation, starting with the &lt;a href="https://raw.githubusercontent.com/facebook/folly/main/folly/docs/Overview.md"&gt;overview&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Folly is published on GitHub at &lt;a href="https://github.com/facebook/folly"&gt;https://github.com/facebook/folly&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Build Notes&lt;/h1&gt; 
&lt;p&gt;Because folly does not provide any ABI compatibility guarantees from commit to commit, we generally recommend building folly as a static library.&lt;/p&gt; 
&lt;p&gt;folly supports gcc (5.1+), clang, or MSVC. It should run on Linux (x86-32, x86-64, and ARM), iOS, macOS, and Windows (x86-64). The CMake build is only tested on some of these platforms; at a minimum, we aim to support macOS and Linux (on the latest Ubuntu LTS release or newer.)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;getdeps.py&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;This script is used by many of Meta's OSS tools. It will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; 
&lt;p&gt;It's written in python so you'll need python3.6 or later on your PATH. It works on Linux, macOS and Windows.&lt;/p&gt; 
&lt;p&gt;The settings for folly's cmake build are held in its getdeps manifest &lt;code&gt;build/fbcode_builder/manifests/folly&lt;/code&gt;, which you can edit locally if desired.&lt;/p&gt; 
&lt;h3&gt;Dependencies&lt;/h3&gt; 
&lt;p&gt;If on Linux or MacOS (with homebrew installed) you can install system dependencies to save building them:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Clone the repo
git clone https://github.com/facebook/folly
# Install dependencies
cd folly
sudo ./build/fbcode_builder/getdeps.py install-system-deps --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you'd like to see the packages before installing them:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./build/fbcode_builder/getdeps.py install-system-deps --dry-run --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On other platforms or if on Linux and without system dependencies &lt;code&gt;getdeps.py&lt;/code&gt; will mostly download and build them for you during the build step.&lt;/p&gt; 
&lt;p&gt;Some of the dependencies &lt;code&gt;getdeps.py&lt;/code&gt; uses and installs are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a version of boost compiled with C++14 support.&lt;/li&gt; 
 &lt;li&gt;googletest is required to build and run folly's tests.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;p&gt;This script will download and build all of the necessary dependencies first, and will then invoke cmake etc to build folly. This will help ensure that you build with relevant versions of all of the dependent libraries, taking into account what versions are installed locally on your system.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; currently requires python 3.6+ to be on your path.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; will invoke cmake etc.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Clone the repo
git clone https://github.com/facebook/folly
cd folly
# Build, using system dependencies if available
python3 ./build/fbcode_builder/getdeps.py --allow-system-packages build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It puts output in its scratch area:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;installed/folly/lib/libfolly.a&lt;/code&gt;: Library&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--scratch-path&lt;/code&gt; argument to control the location of the scratch directory used for the build. You can find the default scratch install location from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-inst-dir&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;There are also &lt;code&gt;--install-dir&lt;/code&gt; and &lt;code&gt;--install-prefix&lt;/code&gt; arguments to provide some more fine-grained control of the installation directories. However, given that folly provides no compatibility guarantees between commits we generally recommend building and installing the libraries to a temporary location, and then pointing your project's build at this temporary location, rather than installing folly in the traditional system installation directories. e.g., if you are building with CMake you can use the &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; variable to allow CMake to find folly in this temporary installation directory when building your project.&lt;/p&gt; 
&lt;p&gt;If you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Run tests&lt;/h3&gt; 
&lt;p&gt;By default &lt;code&gt;getdeps.py&lt;/code&gt; will build the tests for folly. To run them:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd folly
python3 ./build/fbcode_builder/getdeps.py --allow-system-packages test
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;build.sh&lt;/code&gt;/&lt;code&gt;build.bat&lt;/code&gt; wrapper&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;build.sh&lt;/code&gt; can be used on Linux and MacOS, on Windows use the &lt;code&gt;build.bat&lt;/code&gt; script instead. Its a wrapper around &lt;code&gt;getdeps.py&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Build with cmake directly&lt;/h2&gt; 
&lt;p&gt;If you don't want to let getdeps invoke cmake for you then by default, building the tests is disabled as part of the CMake &lt;code&gt;all&lt;/code&gt; target. To build the tests, specify &lt;code&gt;-DBUILD_TESTS=ON&lt;/code&gt; to CMake at configure time.&lt;/p&gt; 
&lt;p&gt;NB if you want to invoke &lt;code&gt;cmake&lt;/code&gt; again to iterate on a &lt;code&gt;getdeps.py&lt;/code&gt; build, there is a helpful &lt;code&gt;run_cmake.py&lt;/code&gt; script output in the scratch-path build directory. You can find the scratch build directory from logs or with &lt;code&gt;python3 ./build/fbcode_builder/getdeps.py show-build-dir&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Running tests with ctests also works if you cd to the build dir, e.g. &lt;code&gt;(cd $(python3 ./build/fbcode_builder/getdeps.py show-build-dir) &amp;amp;&amp;amp; ctest)&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Finding dependencies in non-default locations&lt;/h3&gt; 
&lt;p&gt;If you have boost, gtest, or other dependencies installed in a non-default location, you can use the &lt;code&gt;CMAKE_INCLUDE_PATH&lt;/code&gt; and &lt;code&gt;CMAKE_LIBRARY_PATH&lt;/code&gt; variables to make CMAKE look also look for header files and libraries in non-standard locations. For example, to also search the directories &lt;code&gt;/alt/include/path1&lt;/code&gt; and &lt;code&gt;/alt/include/path2&lt;/code&gt; for header files and the directories &lt;code&gt;/alt/lib/path1&lt;/code&gt; and &lt;code&gt;/alt/lib/path2&lt;/code&gt; for libraries, you can invoke &lt;code&gt;cmake&lt;/code&gt; as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cmake \
  -DCMAKE_INCLUDE_PATH=/alt/include/path1:/alt/include/path2 \
  -DCMAKE_LIBRARY_PATH=/alt/lib/path1:/alt/lib/path2 ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ubuntu LTS, CentOS Stream, Fedora&lt;/h2&gt; 
&lt;p&gt;Use the &lt;code&gt;getdeps.py&lt;/code&gt; approach above. We test in CI on Ubuntu LTS, and occasionally on other distros.&lt;/p&gt; 
&lt;p&gt;If you find the set of system packages is not quite right for your chosen distro, you can specify distro version specific overrides in the dependency manifests (e.g. &lt;a href="https://github.com/facebook/folly/raw/main/build/fbcode_builder/manifests/boost"&gt;https://github.com/facebook/folly/blob/main/build/fbcode_builder/manifests/boost&lt;/a&gt; ). You could probably make it work on most recent Ubuntu/Debian or Fedora/Redhat derived distributions.&lt;/p&gt; 
&lt;p&gt;At time of writing (Dec 2021) there is a build break on GCC 11.x based systems in lang_badge_test. If you don't need badge functionality you can work around by commenting it out from CMakeLists.txt (unfortunately fbthrift does need it)&lt;/p&gt; 
&lt;h2&gt;Windows (Vcpkg)&lt;/h2&gt; 
&lt;p&gt;Note that many tests are disabled for folly Windows builds, you can see them in the log from the cmake configure step, or by looking for WINDOWS_DISABLED in &lt;code&gt;CMakeLists.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;That said, &lt;code&gt;getdeps.py&lt;/code&gt; builds work on Windows and are tested in CI.&lt;/p&gt; 
&lt;p&gt;If you prefer, you can try Vcpkg. folly is available in &lt;a href="https://github.com/Microsoft/vcpkg#vcpkg"&gt;Vcpkg&lt;/a&gt; and releases may be built via &lt;code&gt;vcpkg install folly:x64-windows&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You may also use &lt;code&gt;vcpkg install folly:x64-windows --head&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;macOS&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;getdeps.py&lt;/code&gt; builds work on macOS and are tested in CI, however if you prefer, you can try one of the macOS package managers&lt;/p&gt; 
&lt;h3&gt;Homebrew&lt;/h3&gt; 
&lt;p&gt;folly is available as a Formula and releases may be built via &lt;code&gt;brew install folly&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You may also use &lt;code&gt;folly/build/bootstrap-osx-homebrew.sh&lt;/code&gt; to build against &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  ./folly/build/bootstrap-osx-homebrew.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create a build directory &lt;code&gt;_build&lt;/code&gt; in the top-level.&lt;/p&gt; 
&lt;h3&gt;MacPorts&lt;/h3&gt; 
&lt;p&gt;Install the required packages from MacPorts:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  sudo port install \
    boost \
    cmake \
    gflags \
    git \
    google-glog \
    libevent \
    libtool \
    lz4 \
    lzma \
    openssl \
    snappy \
    xz \
    zlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download and install double-conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  git clone https://github.com/google/double-conversion.git
  cd double-conversion
  cmake -DBUILD_SHARED_LIBS=ON .
  make
  sudo make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download and install folly with the parameters listed below:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  git clone https://github.com/facebook/folly.git
  cd folly
  mkdir _build
  cd _build
  cmake ..
  make
  sudo make install
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ReVanced/revanced-patches</title>
      <link>https://github.com/ReVanced/revanced-patches</link>
      <description>&lt;p&gt;üß© Patches for ReVanced&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source width="256px" media="(prefers-color-scheme: dark)" srcset="assets/revanced-headline/revanced-headline-vertical-dark.svg" /&gt; 
  &lt;img width="256px" src="https://raw.githubusercontent.com/ReVanced/revanced-patches/main/assets/revanced-headline/revanced-headline-vertical-light.svg?sanitize=true" /&gt; 
 &lt;/picture&gt; &lt;br /&gt; &lt;a href="https://revanced.app/"&gt; 
  &lt;picture&gt; 
   &lt;source height="24px" media="(prefers-color-scheme: dark)" srcset="assets/revanced-logo/revanced-logo.svg" /&gt; 
   &lt;img height="24px" src="https://raw.githubusercontent.com/ReVanced/revanced-patches/main/assets/revanced-logo/revanced-logo.svg?sanitize=true" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/ReVanced"&gt; 
  &lt;picture&gt; 
   &lt;source height="24px" media="(prefers-color-scheme: dark)" srcset="https://i.ibb.co/dMMmCrW/Git-Hub-Mark.png" /&gt; 
   &lt;img height="24px" src="https://i.ibb.co/9wV3HGF/Git-Hub-Mark-Light.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="http://revanced.app/discord"&gt; 
  &lt;picture&gt; 
   &lt;source height="24px" media="(prefers-color-scheme: dark)" srcset="https://user-images.githubusercontent.com/13122796/178032563-d4e084b7-244e-4358-af50-26bde6dd4996.png" /&gt; 
   &lt;img height="24px" src="https://user-images.githubusercontent.com/13122796/178032563-d4e084b7-244e-4358-af50-26bde6dd4996.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://reddit.com/r/revancedapp"&gt; 
  &lt;picture&gt; 
   &lt;source height="24px" media="(prefers-color-scheme: dark)" srcset="https://user-images.githubusercontent.com/13122796/178032351-9d9d5619-8ef7-470a-9eec-2744ece54553.png" /&gt; 
   &lt;img height="24px" src="https://user-images.githubusercontent.com/13122796/178032351-9d9d5619-8ef7-470a-9eec-2744ece54553.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://t.me/app_revanced"&gt; 
  &lt;picture&gt; 
   &lt;source height="24px" media="(prefers-color-scheme: dark)" srcset="https://user-images.githubusercontent.com/13122796/178032213-faf25ab8-0bc3-4a94-a730-b524c96df124.png" /&gt; 
   &lt;img height="24px" src="https://user-images.githubusercontent.com/13122796/178032213-faf25ab8-0bc3-4a94-a730-b524c96df124.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://x.com/revancedapp"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://user-images.githubusercontent.com/93124920/270180600-7c1b38bf-889b-4d68-bd5e-b9d86f91421a.png" /&gt; 
   &lt;img height="24px" src="https://user-images.githubusercontent.com/93124920/270108715-d80743fa-b330-4809-b1e6-79fbdc60d09c.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://www.youtube.com/@ReVanced"&gt; 
  &lt;picture&gt; 
   &lt;source height="24px" media="(prefers-color-scheme: dark)" srcset="https://user-images.githubusercontent.com/13122796/178032714-c51c7492-0666-44ac-99c2-f003a695ab50.png" /&gt; 
   &lt;img height="24px" src="https://user-images.githubusercontent.com/13122796/178032714-c51c7492-0666-44ac-99c2-f003a695ab50.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;br /&gt; Continuing the legacy of Vanced &lt;/p&gt; 
&lt;h1&gt;üß© ReVanced Patches&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/ReVanced/revanced-patches/release.yml" alt="GitHub Workflow Status (with event)" /&gt; &lt;img src="https://img.shields.io/badge/License-GPL%20v3-yellow.svg?sanitize=true" alt="GPLv3 License" /&gt;&lt;/p&gt; 
&lt;p&gt;This repository contains a collection of ReVanced Patches.&lt;/p&gt; 
&lt;h2&gt;‚ùì About&lt;/h2&gt; 
&lt;p&gt;Patches are small modifications to Android apps that allow you to change the behavior of or add new features, block ads, customize the appearance, and much more.&lt;/p&gt; 
&lt;h2&gt;üí™ Features&lt;/h2&gt; 
&lt;p&gt;Some of the features the patches provide are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üö´ &lt;strong&gt;Block ads&lt;/strong&gt;: Say goodbye to ads&lt;/li&gt; 
 &lt;li&gt;‚≠ê &lt;strong&gt;Customize your app&lt;/strong&gt;: Personalize the appearance of apps with various layouts and themes&lt;/li&gt; 
 &lt;li&gt;ü™Ñ &lt;strong&gt;Add new features&lt;/strong&gt;: Extend the functionality of apps with lots of new features&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Miscellaneous and general purpose&lt;/strong&gt;: Rename packages, enable debugging, disable screen capture restrictions, export activities, etc.&lt;/li&gt; 
 &lt;li&gt;‚ú® &lt;strong&gt;And much more!&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For a complete list of all available patches, visit &lt;a href="https://revanced.app/patches"&gt;revanced.app/patches&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üöÄ How to get started&lt;/h2&gt; 
&lt;p&gt;You can use &lt;a href="https://github.com/ReVanced/revanced-cli"&gt;ReVanced CLI&lt;/a&gt; or &lt;a href="https://github.com/ReVanced/revanced-manager"&gt;ReVanced Manager&lt;/a&gt; to use ReVanced Patches.&lt;/p&gt; 
&lt;h2&gt;üìö Everything else&lt;/h2&gt; 
&lt;h3&gt;üìô Contributing&lt;/h3&gt; 
&lt;p&gt;Thank you for considering contributing to ReVanced Patches. You can find the contribution guidelines &lt;a href="https://raw.githubusercontent.com/ReVanced/revanced-patches/main/CONTRIBUTING.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Building&lt;/h3&gt; 
&lt;p&gt;To build ReVanced Patches, you can follow the &lt;a href="https://github.com/ReVanced/revanced-documentation"&gt;ReVanced documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìú Licence&lt;/h2&gt; 
&lt;p&gt;ReVanced Patches is licensed under the GPLv3 license. Please see the &lt;a href="https://raw.githubusercontent.com/ReVanced/revanced-patches/main/LICENSE"&gt;license file&lt;/a&gt; for more information. &lt;a href="https://www.tldrlegal.com/license/gnu-general-public-license-v3-gpl-3"&gt;tl;dr&lt;/a&gt; you may copy, distribute and modify ReVanced Patches as long as you track changes/dates in source files. Any modifications to ReVanced Patches must also be made available under the GPL, along with build &amp;amp; install instructions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trueadm/ripple</title>
      <link>https://github.com/trueadm/ripple</link>
      <description>&lt;p&gt;the elegant TypeScript UI framework&lt;/p&gt;&lt;hr&gt;&lt;a href="https://ripplejs.com"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="assets/ripple-dark.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/trueadm/ripple/main/assets/ripple-light.png" alt="Ripple - the elegant TypeScript UI framework" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;&lt;a href="https://github.com/trueadm/ripple/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/trueadm/ripple/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/JBF2ySrh2W"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Server-7289da?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://stackblitz.com/github/trueadm/ripple/tree/main/templates/basic"&gt;&lt;img src="https://developer.stackblitz.com/img/open_in_stackblitz_small.svg?sanitize=true" alt="Open in StackBlitz" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;What is Ripple?&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Currently, this project is still in early development, and should not be used in production.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Ripple is a TypeScript UI framework that takes the best parts of React, Solid and Svelte and combines them into one package.&lt;/p&gt; 
&lt;p&gt;I wrote Ripple as a love letter for frontend web ‚Äì and this is largely a project that I built in less than a week, so it's very raw.&lt;/p&gt; 
&lt;p&gt;Personally, I (&lt;a href="https://github.com/trueadm"&gt;@trueadm&lt;/a&gt;) have been involved in some truly amazing frontend frameworks along their journeys ‚Äì from &lt;a href="https://github.com/infernojs/inferno"&gt;Inferno&lt;/a&gt;, where it all began, to &lt;a href="https://github.com/facebook/react"&gt;React&lt;/a&gt; and the journey of React Hooks, to creating &lt;a href="https://github.com/facebook/lexical"&gt;Lexical&lt;/a&gt;, to &lt;a href="https://github.com/sveltejs/svelte"&gt;Svelte 5&lt;/a&gt; and its new compiler and signal-based reactivity runtime. Along that journey, I collected ideas, and intriguing thoughts that may or may not pay off. Given my time between roles, I decided it was the best opportunity to try them out, and for open source to see what I was cooking.&lt;/p&gt; 
&lt;p&gt;Ripple was designed to be a JS/TS-first framework, rather than HTML-first. Ripple modules have their own &lt;code&gt;.ripple&lt;/code&gt; extension, and these modules fully support TypeScript. By introducing a new extension, it allows Ripple to invent its own superset language, which plays really nicely with TypeScript and JSX, but with a few interesting touches. In my experience, this has led to better DX not only for humans, but also for LLMs.&lt;/p&gt; 
&lt;p&gt;Right now, there will be plenty of bugs, things just won't work either and you'll find TODOs everywhere. At this stage, Ripple is more of an early alpha version of something that &lt;em&gt;might&lt;/em&gt; be, rather than something you should try and adopt. If anything, maybe some of the ideas can be shared and incubated back into other frameworks. There's also a lot of similarities with Svelte 5, and that's not by accident; that's because of my recent time working on Svelte 5.&lt;/p&gt; 
&lt;p&gt;If you'd like to know more, join the &lt;a href="https://discord.gg/JBF2ySrh2W"&gt;Ripple Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reactive State Management&lt;/strong&gt;: Built-in reactivity with &lt;code&gt;$&lt;/code&gt; prefixed variables and object properties&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Component-Based Architecture&lt;/strong&gt;: Clean, reusable components with props and children&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JSX-like Syntax&lt;/strong&gt;: Familiar templating with Ripple-specific enhancements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Fine-grain rendering, with industry-leading performance and memory usage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TypeScript Support&lt;/strong&gt;: Full TypeScript integration with type checking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VSCode Integration&lt;/strong&gt;: Rich editor support with diagnostics, syntax highlighting, and IntelliSense&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prettier Support&lt;/strong&gt;: Full Prettier formatting support for &lt;code&gt;.ripple&lt;/code&gt; modules&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Missing Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SSR&lt;/strong&gt;: Ripple is currently an SPA only, this is because I haven't gotten around to it&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Types&lt;/strong&gt;: The codebase is very raw with limited types; we're getting around to it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Try Ripple&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We're working hard on getting an online playground available. Watch this space!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can try Ripple now by using our basic Vite template either via &lt;a href="https://stackblitz.com/github/trueadm/ripple/tree/main/templates/basic"&gt;StackBlitz&lt;/a&gt;, or by running these commands in your terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx degit trueadm/ripple/templates/basic my-app
cd my-app
npm i # or yarn or pnpm
npm run dev # or yarn or pnpm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;VSCode Extension&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://marketplace.visualstudio.com/items?itemName=ripplejs.ripple-vscode-plugin"&gt;Ripple VSCode extension&lt;/a&gt; provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Syntax Highlighting&lt;/strong&gt; for &lt;code&gt;.ripple&lt;/code&gt; files&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Diagnostics&lt;/strong&gt; for compilation errors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TypeScript Integration&lt;/strong&gt; for type checking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IntelliSense&lt;/strong&gt; for autocompletion&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can find the extension on the VS Code Marketplace as &lt;a href="https://marketplace.visualstudio.com/items?itemName=ripplejs.ripple-vscode-plugin"&gt;&lt;code&gt;Ripple for VS Code&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also &lt;a href="https://github.com/trueadm/ripple/raw/refs/heads/main/packages/ripple-vscode-plugin/published/ripple-vscode-plugin.vsix"&gt;manually install the extension&lt;/a&gt; &lt;code&gt;.vsix&lt;/code&gt; that have been manually packaged.&lt;/p&gt; 
&lt;h3&gt;Mounting your app&lt;/h3&gt; 
&lt;p&gt;You can use the &lt;code&gt;mount&lt;/code&gt; API from the &lt;code&gt;ripple&lt;/code&gt; package to render your Ripple component, using the &lt;code&gt;target&lt;/code&gt; option to specify what DOM element you want to render the component.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;// index.ts
import { mount } from 'ripple';
import { App } from '/App.ripple';

mount(App, {
  props: {
    title: 'Hello world!',
  },
  target: document.getElementById('root'),
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Key Concepts&lt;/h2&gt; 
&lt;h3&gt;Components&lt;/h3&gt; 
&lt;p&gt;Define reusable components with the &lt;code&gt;component&lt;/code&gt; keyword. These are similar to functions in that they have &lt;code&gt;props&lt;/code&gt;, but crucially, they allow for a JSX-like syntax to be defined alongside standard TypeScript. That means you do not &lt;em&gt;return JSX&lt;/em&gt; like in other frameworks, but you instead use it like a JavaScript statement, as shown:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Button(props: { text: string, onClick: () =&amp;gt; void }) {
  &amp;lt;button onClick={props.onClick}&amp;gt;
    {props.text}
  &amp;lt;/button&amp;gt;
}

// Usage
export component App() {
  &amp;lt;Button text="Click me" onClick={() =&amp;gt; console.log("Clicked!")} /&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ripple's templating language also supports shorthands and object spreads too:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-svelte"&gt;// you can do a normal prop
&amp;lt;div onClick={onClick}&amp;gt;{text}&amp;lt;/div&amp;gt;

// or using the shorthand prop
&amp;lt;div {onClick}&amp;gt;{text}&amp;lt;/div&amp;gt;

// and you can spread props
&amp;lt;div {...properties}&amp;gt;{text}&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reactive Variables&lt;/h3&gt; 
&lt;p&gt;Variables prefixed with &lt;code&gt;$&lt;/code&gt; are automatically reactive:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;let $name = 'World';
let $count = 0;

// Updates automatically trigger re-renders
$count++;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Object properties prefixed with &lt;code&gt;$&lt;/code&gt; are also automatically reactive:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;let counter = { $current: 0 };

// Updates automatically trigger re-renders
counter.$current++;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Derived values are simply &lt;code&gt;$&lt;/code&gt; variables that combined different parts of state:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;let $count = 0;
let $double = $count * 2;
let $quadruple = $double * 2;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That means &lt;code&gt;$count&lt;/code&gt; itself might be derived if it were to reference another reactive property. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Counter({ $startingCount }) {
  let $count = $startingCount;
  let $double = $count * 2;
  let $quadruple = $double * 2;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now given &lt;code&gt;$startingCount&lt;/code&gt; is reactive, it would mean that &lt;code&gt;$count&lt;/code&gt; might reset each time an incoming change to &lt;code&gt;$startingCount&lt;/code&gt; occurs. That might not be desirable, so Ripple provides a way to &lt;code&gt;untrack&lt;/code&gt; reactivity in those cases:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { untrack } from 'ripple';

component Counter({ $startingCount }) {
  let $count = untrack(() =&amp;gt; $startingCount);
  let $double = $count * 2;
  let $quadruple = $double * 2;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now &lt;code&gt;$count&lt;/code&gt; will only reactively create its value on initialization.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: you cannot define reactive variables in module/global scope, they have to be created on access from an active component&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Transporting Reactivity&lt;/h4&gt; 
&lt;p&gt;Ripple doesn't constrain reactivity to components only. Reactivity can be used inside other functions (and classes in the future) and be composed in a way to improve expressivity and co-location.&lt;/p&gt; 
&lt;p&gt;Ripple provides a very nice way to transport reactivity between boundaries so that it's persisted ‚Äì using objects and arrays. Here's an example using arrays to transport reactivity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { effect } from 'ripple';

function createDouble([ $count ]) {
  const $double = $count * 2;

  effect(() =&amp;gt; {
    console.log('Count:', $count)
  });

  return [ $double ];
}

export component App() {
  let $count = 0;

  const [ $double ] = createDouble([ $count ]);

  &amp;lt;div&amp;gt;{'Double: ' + $double}&amp;lt;/div&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; { $count++; }}&amp;gt;{'Increment'}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can do the same with objects too:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { effect } from 'ripple';

function createDouble({ $count }) {
  const $double = $count * 2;

  effect(() =&amp;gt; {
    console.log('Count:', $count)
  });

  return { $double };
}

export component App() {
  let $count = 0;
  const { $double } = createDouble({ $count });

  &amp;lt;div&amp;gt;{'Double: ' + $double}&amp;lt;/div&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; { $count++; }}&amp;gt;{'Increment'}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Just remember, reactive state must be connected to a component and it can't be global or created within the top-level of a module ‚Äì because then Ripple won't be able to link it to your component tree.&lt;/p&gt; 
&lt;h4&gt;Reactive Arrays&lt;/h4&gt; 
&lt;p&gt;Just like, objects, you can use the &lt;code&gt;$&lt;/code&gt; prefix in an array literal to specify that the field is reactive.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;let $first = 0;
let $second = 0;
const arr = [$first, $second];

const $total = arr.reduce((a, b) =&amp;gt; a + b, 0);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Like shown in the above example, you can compose normal arrays with reactivity and pass them through props or boundaries.&lt;/p&gt; 
&lt;p&gt;However, if you need the entire array to be fully reactive, including when new elements get added, you should use the reactive array that Ripple provides.&lt;/p&gt; 
&lt;p&gt;You'll need to import the &lt;code&gt;RippleArray&lt;/code&gt; class from Ripple. It extends the standard JS &lt;code&gt;Array&lt;/code&gt; class, and supports all of its methods and properties.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { RippleArray } from 'ripple';

// using the new constructor
const arr = new RippleArray(1, 2, 3);

// using static from method
const arr = RippleArray.from([1, 2, 3]);

// using static of method
const arr = RippleArray.of(1, 2, 3);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;RippleArray&lt;/code&gt; is a reactive array, and that means you can access properties normally using numeric index. However, accessing the &lt;code&gt;length&lt;/code&gt; property of a &lt;code&gt;RippleArray&lt;/code&gt; will be not be reactive, instead you should use &lt;code&gt;$length&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Reactive Set&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;RippleSet&lt;/code&gt; extends the standard JS &lt;code&gt;Set&lt;/code&gt; class, and supports all of its methods and properties. However, accessing the &lt;code&gt;size&lt;/code&gt; property of a &lt;code&gt;RippleSet&lt;/code&gt; will be not be reactive, instead you should use &lt;code&gt;$size&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { RippleSet } from 'ripple';

const set = new RippleSet([1, 2, 3]);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RippleSet's reactive methods or properties can be used directly or assigned to reactive variables.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { RippleSet } from 'ripple';

export component App() {
  const set = new RippleSet([1, 2, 3]);

  // direct usage
  &amp;lt;p&amp;gt;{"Direct usage: set contains 2: "}{set.has(2)}&amp;lt;/p&amp;gt;

  // reactive assignment with prefixed `$`
  let $has = set.has(2);
  &amp;lt;p&amp;gt;{"Assigned usage: set contains 2: "}{$has}&amp;lt;/p&amp;gt;

  &amp;lt;button onClick={() =&amp;gt; set.delete(2)}&amp;gt;{"Delete 2"}&amp;lt;/button&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; set.add(2)}&amp;gt;{"Add 2"}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Reactive Map&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;RippleMap&lt;/code&gt; extends the standard JS &lt;code&gt;Map&lt;/code&gt; class, and supports all of its methods and properties. However, accessing the &lt;code&gt;size&lt;/code&gt; property of a &lt;code&gt;RippleMap&lt;/code&gt; will be not be reactive, instead you should use &lt;code&gt;$size&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { RippleMap } from 'ripple';

const map = new RippleMap([[1,1], [2,2], [3,3], [4,4]]);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RippleMap's reactive methods or properties can be used directly or assigned to reactive variables.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { RippleMap } from 'ripple';

export component App() {
  const map = new RippleMap([[1,1], [2,2], [3,3], [4,4]]);

  // direct usage
  &amp;lt;p&amp;gt;{"Direct usage: map has an item with key 2: "}{map.has(2)}&amp;lt;/p&amp;gt;

  // reactive assignment with prefixed `$`
  let $has = map.has(2);
  &amp;lt;p&amp;gt;{"Assigned usage: map has an item with key 2: "}{$has}&amp;lt;/p&amp;gt;

  &amp;lt;button onClick={() =&amp;gt; map.delete(2)}&amp;gt;{"Delete item with key 2"}&amp;lt;/button&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; map.set(2, 2)}&amp;gt;{"Add key 2 with value 2"}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Effects&lt;/h3&gt; 
&lt;p&gt;When dealing with reactive state, you might want to be able to create side-effects based upon changes that happen upon updates. To do this, you can use &lt;code&gt;effect&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { effect } from 'ripple';

export component App() {
  let $count = 0;

  effect(() =&amp;gt; {
    console.log($count);
  });

  &amp;lt;button onClick={() =&amp;gt; $count++}&amp;gt;{'Increment'}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Control flow&lt;/h3&gt; 
&lt;p&gt;The JSX-like syntax might take some time to get used to if you're coming from another framework. For one, templating in Ripple can only occur &lt;em&gt;inside&lt;/em&gt; a &lt;code&gt;component&lt;/code&gt; body ‚Äì you can't create JSX inside functions, or assign it to variables as an expression.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;&amp;lt;div&amp;gt;
  // you can create variables inside the template!
  const str = "hello world";

  console.log(str); // and function calls too!

  debugger; // you can put breakpoints anywhere to help debugging!

  {str}
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that strings inside the template need to be inside &lt;code&gt;{"string"}&lt;/code&gt;, you can't do &lt;code&gt;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&lt;/code&gt; as Ripple has no idea if &lt;code&gt;hello&lt;/code&gt; is a string or maybe some JavaScript code that needs evaluating, so just ensure you wrap them in curly braces. This shouldn't be an issue in the real-world anyway, as you'll likely use an i18n library that means using JavaScript expressions regardless.&lt;/p&gt; 
&lt;h3&gt;If statements&lt;/h3&gt; 
&lt;p&gt;If blocks work seamlessly with Ripple's templating language, you can put them inside the JSX-like statements, making control-flow far easier to read and reason with.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Truthy({ x }) {
  &amp;lt;div&amp;gt;
    if (x) {
      &amp;lt;span&amp;gt;{'x is truthy'}&amp;lt;/span&amp;gt;
    } else {
      &amp;lt;span&amp;gt;{'x is falsy'}&amp;lt;/span&amp;gt;
    }
  &amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For statements&lt;/h3&gt; 
&lt;p&gt;You can render collections using a &lt;code&gt;for...of&lt;/code&gt; block, and you don't need to specify a &lt;code&gt;key&lt;/code&gt; prop like other frameworks.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component ListView({ title, items }) {
  &amp;lt;h2&amp;gt;{title}&amp;lt;/h2&amp;gt;
  &amp;lt;ul&amp;gt;
    for (const item of items) {
      &amp;lt;li&amp;gt;{item.text}&amp;lt;/li&amp;gt;
    }
  &amp;lt;/ul&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use Ripple's reactive arrays to easily compose contents of an array.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { RippleArray } from 'ripple';

component Numbers() {
  const items = new RippleArray(1, 2, 3);

  for (const item of items) {
    &amp;lt;div&amp;gt;{item}&amp;lt;/div&amp;gt;
  }

  &amp;lt;button onClick={() =&amp;gt; items.push(`Item ${items.$length + 1}`)}&amp;gt;{"Add Item"}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Clicking the &lt;code&gt;&amp;lt;button&amp;gt;&lt;/code&gt; will create a new item, note that &lt;code&gt;items&lt;/code&gt; is not &lt;code&gt;$&lt;/code&gt; prefixed, because it's not reactive, but rather its properties are instead.&lt;/p&gt; 
&lt;h3&gt;Try statements&lt;/h3&gt; 
&lt;p&gt;Try blocks work to build the foundation for &lt;strong&gt;error boundaries&lt;/strong&gt;, when the runtime encounters an error in the &lt;code&gt;try&lt;/code&gt; block, you can easily render a fallback in the &lt;code&gt;catch&lt;/code&gt; block.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { reportError } from 'some-library';

component ErrorBoundary() {
  &amp;lt;div&amp;gt;
    try {
      &amp;lt;ComponentThatFails /&amp;gt;
    } catch (e) {
      reportError(e);

      &amp;lt;div&amp;gt;{'An error occurred! ' + e.message}&amp;lt;/div&amp;gt;
    }
  &amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Props&lt;/h3&gt; 
&lt;p&gt;If you want a prop to be reactive, you should also give it a &lt;code&gt;$&lt;/code&gt; prefix.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Button(props: { $text: string, onClick: () =&amp;gt; void }) {
  &amp;lt;button onClick={props.onClick}&amp;gt;
    {props.$text}
  &amp;lt;/button&amp;gt;
}

// Usage
&amp;lt;Button $text={some_text} onClick={() =&amp;gt; console.log("Clicked!")} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This also applies to DOM elements, if you want an attribute or property to be reactive, it needs to have a &lt;code&gt;$&lt;/code&gt; prefix.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tsx"&gt;&amp;lt;div $class={props.$someClass} $id={$someId}&amp;gt;
  {$someText}
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Otherwise changes to the attribute or property will not be reactively updated.&lt;/p&gt; 
&lt;h3&gt;Children&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;$children&lt;/code&gt; prop and then use it in the form of &lt;code&gt;&amp;lt;$children /&amp;gt;&lt;/code&gt; for component composition.&lt;/p&gt; 
&lt;p&gt;When you pass in children to a component, it gets implicitly passed as the &lt;code&gt;$children&lt;/code&gt; prop, in the form of a component.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import type { Component } from 'ripple';

component Card(props: { $children: Component }) {
  &amp;lt;div class="card"&amp;gt;
    &amp;lt;props.$children /&amp;gt;
  &amp;lt;/div&amp;gt;
}

// Usage
&amp;lt;Card&amp;gt;
  &amp;lt;p&amp;gt;{"Card content here"}&amp;lt;/p&amp;gt;
&amp;lt;/Card&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You could also explicitly write the same code as shown:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import type { Component } from 'ripple';

component Card(props: { $children: Component }) {
  &amp;lt;div class="card"&amp;gt;
    &amp;lt;props.$children /&amp;gt;
  &amp;lt;/div&amp;gt;
}

// Usage with explicit component
&amp;lt;Card&amp;gt;
  component $children() {
    &amp;lt;p&amp;gt;{"Card content here"}&amp;lt;/p&amp;gt;
  }
&amp;lt;/Card&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessor Props&lt;/h3&gt; 
&lt;p&gt;When working with props on composite components (&lt;code&gt;&amp;lt;Foo&amp;gt;&lt;/code&gt; rather than &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt;), it can sometimes be difficult to debug why a certain value is a certain way. JavaScript gives us a way to do this on objects using the &lt;code&gt;get&lt;/code&gt; syntax:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;let name = 'Bob';

const object = {
  get name() {
    // I can easily debug when this property gets
    // access and track it easily
    console.log(name);
    return name;
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;So Ripple provides similar capabilities when working with composite components in a template, specifically using &lt;code&gt;$prop:={}&lt;/code&gt; rather than the typical &lt;code&gt;$prop={}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In fact, when you use an accessor, you must pass a function, and the prop must be &lt;code&gt;$&lt;/code&gt; prefixed, as Ripple considers accessor props as reactive:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

const getName = () =&amp;gt; {
  // I can easily debug when this property gets
  // access and track it easily
  console.log(name);
  return $name;
};

&amp;lt;Person $name:={getName} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also inline the function too:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

&amp;lt;Person $name:={() =&amp;gt; {
  // I can easily debug when this property gets
  // access and track it easily
  console.log(name);
  return $name;
}} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Furthermore, just like property accessors in JavaScript, Ripple provides a way of capturing the &lt;code&gt;set&lt;/code&gt; too, enabling two-way data-flow on composite component props. You just need to provide a second function after the first, separated using a comma:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

const getName = () =&amp;gt; {
  return $name;
}

const setName = (newName) =&amp;gt; {
  $name = newName;
}

&amp;lt;Person $name:={getName, setName} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or an inlined version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

&amp;lt;Person $name:={() =&amp;gt; $name, (newName) =&amp;gt; $name = $newName} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now changes in the &lt;code&gt;Person&lt;/code&gt; to its &lt;code&gt;props&lt;/code&gt; will propagate to its parent component:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Person(props) {
  const updateName = (newName) =&amp;gt; {
    props.$name = newName;
  }

  &amp;lt;NameInput onChange={updateName}&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Decorators&lt;/h3&gt; 
&lt;p&gt;Ripple provides a consistent way to capture the underlying DOM element ‚Äì decorators. Specifically, using the syntax &lt;code&gt;{@use fn}&lt;/code&gt; where &lt;code&gt;fn&lt;/code&gt; is a function that captures the DOM element. If you're familiar with other frameworks, then this is identical to &lt;code&gt;{@attach fn}&lt;/code&gt; in Svelte 5 and somewhat similar to &lt;code&gt;ref&lt;/code&gt; in React. The hook function will receive the reference to the underlying DOM element.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;export component App() {
  let $node;

  const ref = (node) =&amp;gt; {
    $node = node;
    console.log("mounted", node);

    return () =&amp;gt; {
      $node = undefined;
      console.log("unmounted", node);
    };
  };

  &amp;lt;div {@use ref}&amp;gt;{"Hello world"}&amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also create &lt;code&gt;{@use}&lt;/code&gt; functions inline.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;export component App() {
  let $node;

  &amp;lt;div {@use (node) =&amp;gt; {
    $node = node;
    return () =&amp;gt; $node = null;
  }}&amp;gt;{"Hello world"}&amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use function factories to define properties, these are functions that return functions that do the same thing. However, you can use this pattern to pass reactive properties.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { fadeIn } from 'some-library';

export component App({ $ms }) {
  &amp;lt;div {@use fadeIn({ $ms })}&amp;gt;{"Hello world"}&amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Lastly, you can use decorators on composite components.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;&amp;lt;Image {@use (node) =&amp;gt; console.log(node)} {...props} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When passing decorators to composite components (rather than HTML elements) as shown above, they will be passed a &lt;code&gt;Symbol&lt;/code&gt; property, as they are not named. This still means that it can be spread to HTML template elements later on and still work.&lt;/p&gt; 
&lt;h3&gt;Event Props&lt;/h3&gt; 
&lt;p&gt;Like React, events are props that start with &lt;code&gt;on&lt;/code&gt; and then continue with an uppercase character, such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;onClick&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerMove&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerDown&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onKeyDown&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For &lt;code&gt;capture&lt;/code&gt; phase events, just add &lt;code&gt;Capture&lt;/code&gt; to the end of the prop name:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;onClickCapture&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerMoveCapture&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerDownCapture&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onKeyDownCapture&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Some events are automatically delegated where possible by Ripple to improve runtime performance.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Styling&lt;/h3&gt; 
&lt;p&gt;Ripple supports native CSS styling that is localized to the given component using the &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; element.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component MyComponent() {
  &amp;lt;div class="container"&amp;gt;&amp;lt;h1&amp;gt;{'Hello World'}&amp;lt;/h1&amp;gt;&amp;lt;/div&amp;gt;

  &amp;lt;style&amp;gt;
    .container {
      background: blue;
      padding: 1rem;
    }

    h1 {
      color: white;
      font-size: 2rem;
    }
  &amp;lt;/style&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: the &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; element must be top-level within a &lt;code&gt;component&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Context&lt;/h3&gt; 
&lt;p&gt;Ripple has the concept of &lt;code&gt;context&lt;/code&gt; where a value or reactive object can be shared through the component tree ‚Äì like in other frameworks. This all happens from the &lt;code&gt;createContext&lt;/code&gt; function that is imported from &lt;code&gt;ripple&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;When you create a context, you can &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;set&lt;/code&gt; the values, but this must happen within the component. Using them outside will result in an error being thrown.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { createContext } from 'ripple';

const MyContext = createContext(null);

component Child() {
  // Context is read in the Child component
  const value = MyContext.get(MyContext);

  // value is "Hello from context!"
  console.log(value);
}

component Parent() {
  const value = MyContext.get(MyContext);

  // Context is read in the Parent component, but hasn't yet
  // been set, so we fallback to the initial context value.
  // So the value is `null`
  console.log(value);

  // Context is set in the Parent component
  MyContext.set("Hello from context!");

  &amp;lt;Child /&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are happy for your interest in contributing. Please see our &lt;a href="https://raw.githubusercontent.com/trueadm/ripple/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/trueadm/ripple/main/LICENSE"&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>grpc/grpc-go</title>
      <link>https://github.com/grpc/grpc-go</link>
      <description>&lt;p&gt;The Go language implementation of gRPC. HTTP/2 based RPC&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gRPC-Go&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pkg.go.dev/google.golang.org/grpc"&gt;&lt;img src="https://pkg.go.dev/badge/google.golang.org/grpc" alt="GoDoc" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/grpc/grpc-go"&gt;&lt;img src="https://goreportcard.com/badge/grpc/grpc-go" alt="GoReportCard" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/grpc/grpc-go"&gt;&lt;img src="https://codecov.io/gh/grpc/grpc-go/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://golang.org"&gt;Go&lt;/a&gt; implementation of &lt;a href="https://grpc.io"&gt;gRPC&lt;/a&gt;: A high performance, open source, general RPC framework that puts mobile and HTTP/2 first. For more information see the &lt;a href="https://grpc.io/docs/languages/go"&gt;Go gRPC docs&lt;/a&gt;, or jump directly into the &lt;a href="https://grpc.io/docs/languages/go/quickstart"&gt;quick start&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://golang.org"&gt;Go&lt;/a&gt;&lt;/strong&gt;: any one of the &lt;strong&gt;two latest major&lt;/strong&gt; &lt;a href="https://golang.org/doc/devel/release.html"&gt;releases&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Simply add the following import to your code, and then &lt;code&gt;go [build|run|test]&lt;/code&gt; will automatically fetch the necessary dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import "google.golang.org/grpc"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you are trying to access &lt;code&gt;grpc-go&lt;/code&gt; from &lt;strong&gt;China&lt;/strong&gt;, see the &lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/#FAQ"&gt;FAQ&lt;/a&gt; below.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Learn more&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://grpc.io/docs/languages/go"&gt;Go gRPC docs&lt;/a&gt;, which include a &lt;a href="https://grpc.io/docs/languages/go/quickstart"&gt;quick start&lt;/a&gt; and &lt;a href="https://pkg.go.dev/google.golang.org/grpc"&gt;API reference&lt;/a&gt; among other resources&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/Documentation"&gt;Low-level technical docs&lt;/a&gt; from this repository&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://performance-dot-grpc-testing.appspot.com/explore?dashboard=5180705743044608"&gt;Performance benchmark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/CONTRIBUTING.md"&gt;Contribution guidelines&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;I/O Timeout Errors&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;golang.org&lt;/code&gt; domain may be blocked from some countries. &lt;code&gt;go get&lt;/code&gt; usually produces an error like the following when this happens:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ go get -u google.golang.org/grpc
package google.golang.org/grpc: unrecognized import path "google.golang.org/grpc" (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To build Go code, there are several options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Set up a VPN and access google.golang.org through that.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;With Go module support: it is possible to use the &lt;code&gt;replace&lt;/code&gt; feature of &lt;code&gt;go mod&lt;/code&gt; to create aliases for golang.org packages. In your project's directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;go mod edit -replace=google.golang.org/grpc=github.com/grpc/grpc-go@latest
go mod tidy
go mod vendor
go build -mod=vendor
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Again, this will need to be done for all transitive dependencies hosted on golang.org as well. For details, refer to &lt;a href="https://github.com/golang/go/issues/28652"&gt;golang/go issue #28652&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Compiling error, undefined: grpc.SupportPackageIsVersion&lt;/h3&gt; 
&lt;p&gt;Please update to the latest version of gRPC-Go using &lt;code&gt;go get google.golang.org/grpc&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;How to turn on logging&lt;/h3&gt; 
&lt;p&gt;The default logger is controlled by environment variables. Turn everything on like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ export GRPC_GO_LOG_VERBOSITY_LEVEL=99
$ export GRPC_GO_LOG_SEVERITY_LEVEL=info
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;The RPC failed with error &lt;code&gt;"code = Unavailable desc = transport is closing"&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;This error means the connection the RPC is using was closed, and there are many possible reasons, including:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;mis-configured transport credentials, connection failed on handshaking&lt;/li&gt; 
 &lt;li&gt;bytes disrupted, possibly by a proxy in between&lt;/li&gt; 
 &lt;li&gt;server shutdown&lt;/li&gt; 
 &lt;li&gt;Keepalive parameters caused connection shutdown, for example if you have configured your server to terminate connections regularly to &lt;a href="https://github.com/grpc/grpc-go/issues/3170#issuecomment-552517779"&gt;trigger DNS lookups&lt;/a&gt;. If this is the case, you may want to increase your &lt;a href="https://pkg.go.dev/google.golang.org/grpc/keepalive?tab=doc#ServerParameters"&gt;MaxConnectionAgeGrace&lt;/a&gt;, to allow longer RPC calls to finish.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;It can be tricky to debug this because the error happens on the client side but the root cause of the connection being closed is on the server side. Turn on logging on &lt;strong&gt;both client and server&lt;/strong&gt;, and see if there are any transport errors.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>fla-org/flash-linear-attention</title>
      <link>https://github.com/fla-org/flash-linear-attention</link>
      <description>&lt;p&gt;üöÄ Efficient implementations of state-of-the-art linear attention models&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;üí• Flash Linear Attention&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/fla-hub"&gt;&lt;img src="https://img.shields.io/badge/-Models-gray.svg?logo=huggingface&amp;amp;style=flat-square" alt="hf_model" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/vDaJTmKNcS"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;This repo aims at providing a collection of efficient Triton-based implementations for state-of-the-art linear attention models. &lt;strong&gt;All implementations are written purely in PyTorch and Triton, making them platform-agnostic.&lt;/strong&gt; Currently verified platforms include NVIDIA, AMD, and Intel. &lt;strong&gt;Any pull requests are welcome!&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img width="400" alt="image" src="https://github.com/fla-org/flash-linear-attention/assets/18402347/02ff2e26-1495-4088-b701-e72cd65ac6cf" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#news"&gt;News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#models"&gt;Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#usage"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#token-mixing"&gt;Token Mixing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#fused-modules"&gt;Fused Modules&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#generation"&gt;Generation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#hybrid-models"&gt;Hybrid Models&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#training"&gt;Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#evaluation"&gt;Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#benchmarks"&gt;Benchmarks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-09]}$:&lt;/strong&gt; üêª Thrilled to announce that &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/ops/gated_delta_rule"&gt;GDN&lt;/a&gt; has been integrated into Qwen3-Next. Check out &lt;a href="https://github.com/huggingface/transformers/pull/40771"&gt;the PR&lt;/a&gt; and &lt;a href="https://qwenlm.github.io/blog/qwen3_next/"&gt;their blog post&lt;/a&gt; for more infos!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-08]}$:&lt;/strong&gt; üå≤ Add Log-Linear Attention implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2506.04761"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-08]}$:&lt;/strong&gt; üéì Add MoM implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2502.13685"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-07]}$:&lt;/strong&gt; üê≥ Add MLA implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2405.04434"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-07]}$:&lt;/strong&gt; üõ£Ô∏è Added PaTH Attention to fla (&lt;a href="https://arxiv.org/abs/2505.16381"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-06]}$:&lt;/strong&gt; üéâ Added MesaNet to fla (&lt;a href="https://arxiv.org/abs/2506.05233"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-06]}$:&lt;/strong&gt; üêç Add Comba implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2506.02475"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-05]}$:&lt;/strong&gt; üéâ Add Rodimus* implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2410.06577"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-04]}$:&lt;/strong&gt; üéâ Add DeltaProduct implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2502.10297"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-04]}$:&lt;/strong&gt; üéâ Add FoX implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2503.02130"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-03]}$:&lt;/strong&gt; &lt;del&gt;We have changed the default &lt;code&gt;initializer_range&lt;/code&gt; to the magic üê≥ 0.006&lt;/del&gt; The &lt;code&gt;initializer_range&lt;/code&gt; was rolled back to the default value of 0.02. For actual training, we recommend trying both.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-02]}$:&lt;/strong&gt; üê≥ Add NSA implementations to &lt;code&gt;fla&lt;/code&gt;. See kernels &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/ops/nsa"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-01]}$:&lt;/strong&gt; üî• We are migrating to &lt;code&gt;torchtitan&lt;/code&gt;-based training framework. Check out the &lt;a href="https://github.com/fla-org/flame"&gt;flame&lt;/a&gt; repo for more details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-01]}$:&lt;/strong&gt; ü¶Ö Add RWKV7 implementations (both kernels and models) to &lt;code&gt;fla&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-12]}$:&lt;/strong&gt; Integrated &lt;code&gt;flash-bidirectional-attention&lt;/code&gt; to &lt;code&gt;fla-org&lt;/code&gt; (&lt;a href="https://github.com/fla-org/flash-bidirectional-linear-attention"&gt;repo&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-12]}$:&lt;/strong&gt; üéâ Add Gated DeltaNet implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2412.06464"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-12]}$:&lt;/strong&gt; üöÄ &lt;code&gt;fla&lt;/code&gt; now officially supports kernels with variable-length inputs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-11]}$:&lt;/strong&gt; The inputs are now switched from head-first to seq-first format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-11]}$:&lt;/strong&gt; üí• &lt;code&gt;fla&lt;/code&gt; now provides a flexible way for training hybrid models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-10]}$:&lt;/strong&gt; üî• Announcing &lt;code&gt;flame&lt;/code&gt;, a minimal and scalable framework for training &lt;code&gt;fla&lt;/code&gt; models. Check out the details &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/training/README.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-09]}$:&lt;/strong&gt; &lt;code&gt;fla&lt;/code&gt; now includes a fused linear and cross-entropy layer, significantly reducing memory usage during training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-09]}$:&lt;/strong&gt; üéâ Add GSA implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2409.07146"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-05]}$:&lt;/strong&gt; üéâ Add DeltaNet implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2102.11174"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-05]}$:&lt;/strong&gt; üí• &lt;code&gt;fla&lt;/code&gt; v0.1: a variety of subquadratic kernels/layers/models integrated (RetNet/GLA/Mamba/HGRN/HGRN2/RWKV6, etc., see &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#models"&gt;Models&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2023-12]}$:&lt;/strong&gt; üí• Launched &lt;code&gt;fla&lt;/code&gt;, offering a collection of implementations for state-of-the-art linear attention models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;p&gt;Roughly sorted according to the timeline supported in &lt;code&gt;fla&lt;/code&gt;. The recommended training mode is &lt;code&gt;chunk&lt;/code&gt; when available.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Year&lt;/th&gt; 
   &lt;th align="left"&gt;Venue&lt;/th&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="left"&gt;Paper&lt;/th&gt; 
   &lt;th align="left"&gt;Code&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2023&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;RetNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2307.08621"&gt;Retentive network: a successor to transformer for large language models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/torchscale/tree/main"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/multiscale_retention.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;ICML&lt;/td&gt; 
   &lt;td align="left"&gt;GLA&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2312.06635"&gt;Gated Linear Attention Transformers with Hardware-Efficient Training&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/berlino/gated_linear_attention"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/gla.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;ICML&lt;/td&gt; 
   &lt;td align="left"&gt;Based&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2402.18668"&gt;Simple linear attention language models balance the recall-throughput tradeoff&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/HazyResearch/based"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/based.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;ACL&lt;/td&gt; 
   &lt;td align="left"&gt;Rebased&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2402.10644"&gt;Linear Transformers with Learnable Kernel Functions are Better In-Context Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/corl-team/rebased/"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/rebased.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;NeurIPS&lt;/td&gt; 
   &lt;td align="left"&gt;DeltaNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2406.06484"&gt;Parallelizing Linear Transformers with Delta Rule over Sequence Length&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/delta_net.py"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/delta_net.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2022&lt;/td&gt; 
   &lt;td align="left"&gt;ACL&lt;/td&gt; 
   &lt;td align="left"&gt;ABC&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2110.02488"&gt;ABC: Attention with Bounded-memory Control&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/abc.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2023&lt;/td&gt; 
   &lt;td align="left"&gt;NeurIPS&lt;/td&gt; 
   &lt;td align="left"&gt;HGRN&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://openreview.net/forum?id=P1TCHxJwLB"&gt;Hierarchically Gated Recurrent Neural Network for Sequence Modeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/OpenNLPLab/HGRN"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/hgrn.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;COLM&lt;/td&gt; 
   &lt;td align="left"&gt;HGRN2&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2404.07904"&gt;HGRN2: Gated Linear RNNs with State Expansion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/OpenNLPLab/HGRN2"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/hgrn2.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;COLM&lt;/td&gt; 
   &lt;td align="left"&gt;RWKV6&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2404.05892"&gt;Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/RWKV/RWKV-LM"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/rwkv6.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;LightNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2405.21022"&gt;You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/OpenNLPLab/LightNet"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/lightnet.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;ICLR&lt;/td&gt; 
   &lt;td align="left"&gt;Samba&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2406.07522"&gt;Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/Samba"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/models/samba"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;ICML&lt;/td&gt; 
   &lt;td align="left"&gt;Mamba2&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2405.21060"&gt;Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/state-spaces/mamba"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/models/mamba2"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;NeurIPS&lt;/td&gt; 
   &lt;td align="left"&gt;GSA&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2409.07146"&gt;Gated Slot Attention for Efficient Linear-Time Sequence Modeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/models/gsa"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/models/gsa"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;ICLR&lt;/td&gt; 
   &lt;td align="left"&gt;Gated DeltaNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2412.06464"&gt;Gated Delta Networks: Improving Mamba2 with Delta Rule&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/NVlabs/GatedDeltaNet"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/gated_delta_rule"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;RWKV7&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2503.14456"&gt;RWKV-7 "Goose" with Expressive Dynamic State Evolution&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/rwkv7"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;NSA&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.11089"&gt;Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/nsa"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;ICLR&lt;/td&gt; 
   &lt;td align="left"&gt;FoX&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2503.02130"&gt;Forgetting Transformer: Softmax Attention with a Forget Gate&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/zhixuan-lin/forgetting-transformer"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/forgetting_attn"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;DeltaProduct&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.10297"&gt;DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/layers/gated_deltaproduct.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;ICLR&lt;/td&gt; 
   &lt;td align="left"&gt;Rodimus*&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2410.06577"&gt;Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/codefuse-ai/rodimus"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/rodimus.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;MesaNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2506.05233"&gt;MesaNet: Sequence Modeling by Locally Optimal Test-Time Training&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/mesa_net.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Comba&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2506.02475"&gt;Comba: Improving Bilinear RNNs with Closed-loop Control&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/AwesomeSeq/Comba-triton"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/comba.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;PaTH&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2505.16381"&gt;PaTH Attention: Position Encoding via Accumulating Householder Transformations&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/path_attn.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;MoM&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.13685"&gt;MoM: Linear Sequence Modeling with Mixture-of-Memories&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/OpenSparseLLMs/MoM"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/mom.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Log-Linear Attention&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2506.04761"&gt;Log-Linear Attention&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/HanGuo97/log-linear-attention"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/log_linear_attn"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-4090.yml"&gt;&lt;img src="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-4090.yml/badge.svg?branch=main&amp;amp;event=push" alt="nvidia-4090-ci" /&gt;&lt;/a&gt; &lt;a href="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-a100.yml"&gt;&lt;img src="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-a100.yml/badge.svg?branch=main" alt="nvidia-a100-ci" /&gt;&lt;/a&gt; &lt;a href="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-h100.yml"&gt;&lt;img src="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-h100.yml/badge.svg?branch=main&amp;amp;event=push" alt="nvidia-h100-ci" /&gt;&lt;/a&gt; &lt;a href="https://github.com/fla-org/flash-linear-attention/actions/workflows/intel-b580.yml"&gt;&lt;img src="https://github.com/fla-org/flash-linear-attention/actions/workflows/intel-b580.yml/badge.svg?event=push" alt="intel-b580-ci" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The following requirements should be satisfied&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt; &amp;gt;= 2.5&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/triton"&gt;Triton&lt;/a&gt; &amp;gt;=3.0 (or nightly version, see &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/FAQs.md"&gt;FAQs&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://einops.rocks/"&gt;einops&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt; &amp;gt;=4.45.0&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/datasets"&gt;datasets&lt;/a&gt; &amp;gt;=3.3.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Starting from v0.3.2, the packages published on PyPI are &lt;code&gt;fla-core&lt;/code&gt; and &lt;code&gt;flash-linear-attention&lt;/code&gt;. The former contains all our customized kernels and only depends on PyTorch, Triton, and einops. The latter is an extension package of the former, containing &lt;code&gt;fla/layers&lt;/code&gt; and &lt;code&gt;fla/models&lt;/code&gt;, and depends on transformers. We also provide Triton implementations for conv1d operations, so causal-conv1d is not required.&lt;/p&gt; 
&lt;p&gt;You can install &lt;code&gt;fla&lt;/code&gt; with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install flash-linear-attention
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;As &lt;code&gt;fla&lt;/code&gt; is actively developed now, for the latest features and updates, an alternative way is to install the package from source. Note that installing from git uses the default mode, so you need to uninstall both &lt;code&gt;fla-core&lt;/code&gt; and &lt;code&gt;flash-linear-attention&lt;/code&gt; first:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# uninstall both packages first to ensure a successful upgrade
pip uninstall fla-core flash-linear-attention -y &amp;amp;&amp;amp; pip install -U git+https://github.com/fla-org/flash-linear-attention
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or manage &lt;code&gt;fla&lt;/code&gt; with submodules&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git submodule add https://github.com/fla-org/flash-linear-attention.git 3rdparty/flash-linear-attention
ln -s 3rdparty/flash-linear-attention/fla fla
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have installed &lt;code&gt;triton-nightly&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt; pre version, please use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install einops ninja datasets transformers numpy
# uninstall both packages first to ensure a successful upgrade
pip uninstall fla-core flash-linear-attention -y &amp;amp;&amp;amp; pip install -U --no-use-pep517 git+https://github.com/fla-org/flash-linear-attention --no-deps
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Token Mixing&lt;/h3&gt; 
&lt;p&gt;We provide ``token mixing'' linear attention layers in &lt;code&gt;fla.layers&lt;/code&gt; for you to use. You can replace the standard multihead attention layer in your model with other linear attention layers. Example usage is as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; from fla.layers import MultiScaleRetention
&amp;gt;&amp;gt;&amp;gt; batch_size, num_heads, seq_len, hidden_size = 32, 4, 2048, 1024
&amp;gt;&amp;gt;&amp;gt; device, dtype = 'cuda:0', torch.bfloat16
&amp;gt;&amp;gt;&amp;gt; retnet = MultiScaleRetention(hidden_size=hidden_size, num_heads=num_heads).to(device=device, dtype=dtype)
&amp;gt;&amp;gt;&amp;gt; retnet
MultiScaleRetention(
  (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
  (v_proj): Linear(in_features=1024, out_features=2048, bias=False)
  (g_proj): Linear(in_features=1024, out_features=2048, bias=False)
  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
  (g_norm_swish_gate): FusedRMSNormGated(512, eps=1e-05, activation=swish)
  (rotary): RotaryEmbedding(dim=256, base=10000.0, interleaved=False, pos_idx_in_fp32=True)
)
&amp;gt;&amp;gt;&amp;gt; x = torch.randn(batch_size, seq_len, hidden_size).to(device=device, dtype=dtype)
&amp;gt;&amp;gt;&amp;gt; y, *_ = retnet(x)
&amp;gt;&amp;gt;&amp;gt; y.shape
torch.Size([32, 2048, 1024])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We provide the implementations of models that are compatible with ü§ó Transformers library. Here's an example of how to initialize a GLA model from the default configs in &lt;code&gt;fla&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; from fla.models import GLAConfig
&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM
&amp;gt;&amp;gt;&amp;gt; config = GLAConfig()
&amp;gt;&amp;gt;&amp;gt; config
GLAConfig {
  "attn": null,
  "attn_mode": "chunk",
  "bos_token_id": 1,
  "clamp_min": null,
  "conv_size": 4,
  "elementwise_affine": true,
  "eos_token_id": 2,
  "expand_k": 0.5,
  "expand_v": 1,
  "feature_map": null,
  "fuse_cross_entropy": true,
  "fuse_norm": true,
  "fuse_swiglu": true,
  "hidden_act": "swish",
  "hidden_ratio": 4,
  "hidden_size": 2048,
  "initializer_range": 0.006,
  "intermediate_size": null,
  "max_position_embeddings": 2048,
  "model_type": "gla",
  "norm_eps": 1e-06,
  "num_heads": 4,
  "num_hidden_layers": 24,
  "num_kv_heads": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.50.1",
  "use_cache": true,
  "use_gk": true,
  "use_gv": false,
  "use_output_gate": true,
  "use_short_conv": false,
  "vocab_size": 32000
}

&amp;gt;&amp;gt;&amp;gt; AutoModelForCausalLM.from_config(config)
GLAForCausalLM(
  (model): GLAModel(
    (embeddings): Embedding(32000, 2048)
    (layers): ModuleList(
      (0-23): 24 x GLABlock(
        (attn_norm): RMSNorm(2048, eps=1e-06)
        (attn): GatedLinearAttention(
          (q_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (g_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (gk_proj): Sequential(
            (0): Linear(in_features=2048, out_features=16, bias=False)
            (1): Linear(in_features=16, out_features=1024, bias=True)
          )
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (g_norm_swish_gate): FusedRMSNormGated(512, eps=1e-06, activation=swish)
        )
        (mlp_norm): RMSNorm(2048, eps=1e-06)
        (mlp): GatedMLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (swiglu_linear): SwiGLULinear()
        )
      )
    )
    (norm): RMSNorm(2048, eps=1e-06)
  )
  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fused Modules&lt;/h3&gt; 
&lt;p&gt;We offer a collection of fused modules in &lt;code&gt;fla.modules&lt;/code&gt; to facilitate faster training:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/rotary.py"&gt;&lt;code&gt;Rotary Embedding&lt;/code&gt;&lt;/a&gt;: rotary positional embeddings as adopted by the Llama architecture, a.k.a., Transformer++.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/layernorm.py"&gt;&lt;code&gt;Norm Layers&lt;/code&gt;&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;RMSNorm&lt;/code&gt;, &lt;code&gt;LayerNorm&lt;/code&gt; and &lt;code&gt;GroupNorm&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;RMSNormLinear&lt;/code&gt;, &lt;code&gt;LayerNormLinear&lt;/code&gt; and &lt;code&gt;GroupNormLinear&lt;/code&gt; to reduce memory usage of intermediate tensors for improved memory efficiency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_norm_gate.py"&gt;&lt;code&gt;Norm Layers with Gating&lt;/code&gt;&lt;/a&gt;: combine norm layers with element-wise sigmoid or swish gating, as used by RetNet/GLA.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_cross_entropy.py"&gt;&lt;code&gt;Cross Entropy&lt;/code&gt;&lt;/a&gt;: faster Triton implementation of cross entropy loss.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_linear_cross_entropy.py"&gt;&lt;code&gt;Linear Cross Entropy&lt;/code&gt;&lt;/a&gt;: fused linear layer and cross entropy loss to avoid the materialization of large logits tensors. Also refer to implementations by &lt;a href="https://github.com/mgmalek/efficient_cross_entropy"&gt;mgmalek&lt;/a&gt; and &lt;a href="https://github.com/linkedin/Liger-Kernel/raw/main/src/liger_kernel/ops/fused_linear_cross_entropy.py"&gt;Liger-Kernel&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_kl_div.py"&gt;&lt;code&gt;Linear KL Divergence&lt;/code&gt;&lt;/a&gt;: fused linear layer and KL divergence loss in a similar vein as CE loss.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] You can control using &lt;code&gt;fuse_linear_cross_entropy&lt;/code&gt; in the model configuration to enable/disable the fused linear cross entropy loss.&lt;/p&gt; 
 &lt;p&gt;This fused implementation is more memory-efficient but may reduce numerical precision. Due to this trade-off, it is disabled by default. If you enable this feature and encounter training instability (e.g., loss divergence), we recommend disabling it to see if the issue is resolved.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Generation&lt;/h3&gt; 
&lt;p&gt;Upon successfully pretraining a model, it becomes accessible for generating text using the ü§ó text generation APIs. In the following, we give a generation example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; import fla
&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM, AutoTokenizer
&amp;gt;&amp;gt;&amp;gt; name = 'fla-hub/gla-1.3B-100B'
&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(name)
&amp;gt;&amp;gt;&amp;gt; model = AutoModelForCausalLM.from_pretrained(name).cuda()
&amp;gt;&amp;gt;&amp;gt; input_prompt = "Power goes with permanence. Impermanence is impotence. And rotation is castration."
&amp;gt;&amp;gt;&amp;gt; input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids.cuda()
&amp;gt;&amp;gt;&amp;gt; outputs = model.generate(input_ids, max_length=64)
&amp;gt;&amp;gt;&amp;gt; tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We also provide a simple script &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/benchmarks/benchmark_generation.py"&gt;here&lt;/a&gt; for benchmarking the generation speed. Simply run it by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ python -m benchmarks.benchmark_generation \
  --path 'fla-hub/gla-1.3B-100B' \
  --repetition_penalty 2. \
  --prompt="Hello everyone, I'm Songlin Yang"

Prompt:
Hello everyone, I'm Songlin Yang
Generated:
Hello everyone, I'm Songlin Yang.
I am a 20 year old girl from China who is currently studying in the United States of America for my Master degree and also working as an English teacher at school here on campus since last summer (1st semester). My main goal to be able do well with this course so that we can have

Prompt length: 10, generation length: 64
Total prompt processing + decoding time: 4593ms
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All of the pretrained models currently available can be found in &lt;a href="https://huggingface.co/fla-hub"&gt;&lt;code&gt;fla-hub&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; from huggingface_hub import list_models
&amp;gt;&amp;gt;&amp;gt; for model in list_models(author='fla-hub'): print(model.id)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hybrid Models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;fla&lt;/code&gt; provides a flexible method to incorporate standard attention layers into existing linear attention models. This is easily achieved by specifying the &lt;code&gt;attn&lt;/code&gt; argument in the model configuration.&lt;/p&gt; 
&lt;p&gt;For example, to create a 2-layer Samba model with interleaved Mamba and local attention layers, using a sliding window size of 2048:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; from fla.models import SambaConfig
&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM
&amp;gt;&amp;gt;&amp;gt; config = SambaConfig(num_hidden_layers=2)
&amp;gt;&amp;gt;&amp;gt; config.attn = {
  'layers': [1],
  'num_heads': 18,
  'num_kv_heads': 18,
  'qkv_bias': False,
  'rope_theta': 10000.,
  'window_size': 2048
}
&amp;gt;&amp;gt;&amp;gt; config
SambaConfig {
  "attn": {
    "layers": [
      1
    ],
    "num_heads": 18,
    "num_kv_heads": 18,
    "qkv_bias": false,
    "rope_theta": 10000.0,
    "window_size": 2048
  },
  "bos_token_id": 1,
  "conv_kernel": 4,
  "eos_token_id": 2,
  "expand": 2,
  "fuse_cross_entropy": true,
  "fuse_norm": true,
  "fuse_swiglu": true,
  "hidden_act": "swish",
  "hidden_ratio": 4,
  "hidden_size": 2304,
  "initializer_range": 0.02,
  "intermediate_size": 4608,
  "max_position_embeddings": 2048,
  "model_type": "samba",
  "norm_eps": 1e-05,
  "num_hidden_layers": 2,
  "pad_token_id": 0,
  "rescale_prenorm_residual": false,
  "residual_in_fp32": false,
  "state_size": 16,
  "tie_word_embeddings": false,
  "time_step_floor": 0.0001,
  "time_step_init_scheme": "random",
  "time_step_max": 0.1,
  "time_step_min": 0.001,
  "time_step_rank": 144,
  "time_step_scale": 1.0,
  "transformers_version": "4.50.1",
  "use_bias": false,
  "use_cache": true,
  "use_conv_bias": true,
  "vocab_size": 32000
}

&amp;gt;&amp;gt;&amp;gt; AutoModelForCausalLM.from_config(config)
SambaForCausalLM(
  (backbone): SambaModel(
    (embeddings): Embedding(32000, 2304)
    (layers): ModuleList(
      (0): SambaBlock(
        (mixer_norm): RMSNorm(2304, eps=1e-05)
        (mixer): Mamba(
          (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)
          (in_proj): Linear(in_features=2304, out_features=9216, bias=False)
          (x_proj): Linear(in_features=4608, out_features=176, bias=False)
          (dt_proj): Linear(in_features=144, out_features=4608, bias=True)
          (out_proj): Linear(in_features=4608, out_features=2304, bias=False)
        )
        (mlp_norm): RMSNorm(2304, eps=1e-05)
        (mlp): GatedMLP(
          (gate_proj): Linear(in_features=2304, out_features=6144, bias=False)
          (up_proj): Linear(in_features=2304, out_features=6144, bias=False)
          (down_proj): Linear(in_features=6144, out_features=2304, bias=False)
          (swiglu_linear): SwiGLULinear()
        )
      )
      (1): SambaBlock(
        (mixer_norm): RMSNorm(2304, eps=1e-05)
        (mixer): Attention(
          (q_proj): Linear(in_features=2304, out_features=2304, bias=False)
          (k_proj): Linear(in_features=2304, out_features=2304, bias=False)
          (v_proj): Linear(in_features=2304, out_features=2304, bias=False)
          (o_proj): Linear(in_features=2304, out_features=2304, bias=False)
          (rotary): RotaryEmbedding(dim=128, base=10000.0, interleaved=False, pos_idx_in_fp32=True)
        )
        (mlp_norm): RMSNorm(2304, eps=1e-05)
        (mlp): GatedMLP(
          (gate_proj): Linear(in_features=2304, out_features=6144, bias=False)
          (up_proj): Linear(in_features=2304, out_features=6144, bias=False)
          (down_proj): Linear(in_features=6144, out_features=2304, bias=False)
          (swiglu_linear): SwiGLULinear()
        )
      )
    )
    (norm_f): RMSNorm(2304, eps=1e-05)
  )
  (lm_head): Linear(in_features=2304, out_features=32000, bias=False)
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;During inference, you &lt;strong&gt;DO NOT&lt;/strong&gt; need to revise anything for generation! The model will produce output as-is, without any need for additional configurations or modifications.&lt;/p&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;p&gt;We provide a minimal framework called &lt;a href="https://github.com/fla-org/flame"&gt;üî• &lt;code&gt;flame&lt;/code&gt;&lt;/a&gt; built on top of &lt;code&gt;torchtitan&lt;/code&gt;, for efficient training of &lt;code&gt;fla&lt;/code&gt; models.&lt;/p&gt; 
&lt;p&gt;Checkout &lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/examples/training.md"&gt;the GLA example&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://github.com/EleutherAI/lm-evaluation-harness"&gt;lm-evaluation-harness&lt;/a&gt; library allows you to easily perform (zero-shot) model evaluations. Follow the steps below to use this library:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;lm_eval&lt;/code&gt; following &lt;a href="https://github.com/EleutherAI/lm-evaluation-harness/raw/main/README.md"&gt;their instructions&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run evaluation with:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ MODEL='fla-hub/gla-1.3B-100B'
$ python -m evals.harness --model hf \
    --model_args pretrained=$MODEL,dtype=bfloat16 \
    --tasks wikitext,lambada_openai,piqa,hellaswag,winogrande,arc_easy,arc_challenge,boolq,sciq,copa,openbookqa \
    --batch_size 64 \
    --num_fewshot 0 \
    --device cuda \
    --show_config
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We've made &lt;code&gt;fla&lt;/code&gt; compatible with hf-style evaluations, you can call &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/evals/harness.py"&gt;evals.harness&lt;/a&gt; to finish the evaluations. Running the command above will provide the task results reported in the GLA paper.&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Multi-GPU Evaluation with Hugging Face accelerate üöÄ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To perform data-parallel evaluation (where each GPU loads a separate full copy of the model), we leverage the accelerate launcher as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ MODEL='fla-hub/gla-1.3B-100B'
$ accelerate launch -m evals.harness --model hf  \
    --model_args pretrained=$MODEL,dtype=bfloat16,trust_remote_code=True  \
    --tasks wikitext,lambada_openai,piqa,hellaswag,winogrande,arc_easy,arc_challenge,boolq,sciq,copa,openbookqa \
    --batch_size 64  \
    --num_fewshot 0  \
    --device cuda  \
    --show_config  \
    --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;üìè RULER Benchmark suite&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The RULER benchmarks are commonly used for evaluating model performance on long-context tasks. You can evaluate &lt;code&gt;fla&lt;/code&gt; models on RULER directly using &lt;code&gt;lm-evaluation-harness&lt;/code&gt;. RULER is only available in a relatively recent version of &lt;code&gt;lm-evaluation-harness&lt;/code&gt;, so make sure you have the latest version installed.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, install the necessary dependencies for RULER:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install lm_eval["ruler"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and run evaluation by (e.g., 32k contexts):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ accelerate launch -m evals.harness \
    --output_path $OUTPUT \
    --tasks niah_single_1,niah_single_2,niah_single_3,niah_multikey_1,niah_multikey_2,niah_multikey_3,niah_multiquery,niah_multivalue,ruler_vt,ruler_cwe,ruler_fwe,ruler_qa_hotpot,ruler_qa_squad \
    --model_args pretrained=$MODEL,dtype=bfloat16,max_length=32768,trust_remote_code=True \
    --metadata='{"max_seq_lengths":[4096,8192,16384,32768]}' \
    --batch_size 2 \
    --show_config  \
    --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If a GPU can't load a full copy of the model, please refer to &lt;a href="https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#multi-gpu-evaluation-with-hugging-face-accelerate"&gt;this link&lt;/a&gt; for FSDP settings.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Tip] If you are using &lt;code&gt;lm-evaluation-harness&lt;/code&gt; as an external library and can't find (almost) any tasks available, before calling &lt;code&gt;lm_eval.evaluate()&lt;/code&gt; or &lt;code&gt;lm_eval.simple_evaluate()&lt;/code&gt;, simply run the following to load the library's stock tasks!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; from lm_eval.tasks import TaskManager; TaskManager().initialize_tasks()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;We compared our Triton-based RetNet implementation with CUDA-based FlashAttention2, using a batch size of 8, 32 heads, and a head dimension of 128, across different sequence lengths. These tests were conducted on a single H100 80GB GPU, as illustrated in the following graph&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;# you might have to first install `fla` to enable its import via `pip install -e .`
$ python benchmark_retention.py
Performance:
         T  chunk_fwd  parallel_fwd  flash_fwd  chunk_fwdbwd  parallel_fwdbwd  flash_fwdbwd
0    128.0   0.264032      0.243536   0.083488      1.301856         1.166784      0.320704
1    256.0   0.273472      0.252848   0.094304      1.345872         1.300608      0.807936
2    512.0   0.303600      0.278896   0.098112      1.503168         1.433184      0.857216
3   1024.0   0.357248      0.367360   0.156528      1.773552         2.303424      1.160864
4   2048.0   0.454624      0.605616   0.340928      2.283728         4.483360      1.955936
5   4096.0   0.638960      1.378016   1.004992      3.374720        12.271215      4.813776
6   8192.0   1.012352      4.201344   3.625008      5.581808        40.833618     15.023697
7  16384.0   1.748512     14.489664  13.710080     10.191552       153.093765     54.336864
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img width="500" alt="image" src="https://github.com/user-attachments/assets/c2607015-63af-43d1-90d1-ad5fe1670a03" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this repository helpful, please cite our work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@software{yang2024fla,
  title  = {FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism},
  author = {Yang, Songlin and Zhang, Yu},
  url    = {https://github.com/fla-org/flash-linear-attention},
  month  = jan,
  year   = {2024}
}

@inproceedings{yang2024gdn,
  title     = {Gated Delta Networks: Improving Mamba2 with Delta Rule},
  author    = {Songlin Yang and Jan Kautz and Ali Hatamizadeh},
  booktitle = {Proceedings of ICLR},
  year      = {2025}
}

@inproceedings{yang2024deltanet,
  title     = {Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
  author    = {Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  booktitle = {Proceedings of NeurIPS},
  year      = {2024}
}

@inproceedings{zhang2024gsa,
  title     = {Gated Slot Attention for Efficient Linear-Time Sequence Modeling},
  author    = {Zhang, Yu and Yang, Songlin and Zhu, Ruijie and Zhang, Yue and Cui, Leyang and Wang, Yiqiao and Wang, Bolun and Shi, Freda and Wang, Bailin and Bi, Wei and Zhou, Peng and Fu, Guohong},
  booktitle = {Proceedings of NeurIPS},
  year      = {2024}
}

@inproceedings{qin2024hgrn2,
  title     = {HGRN2: Gated Linear RNNs with State Expansion},
  author    = {Qin, Zhen and Yang, Songlin and Sun, Weixuan and Shen, Xuyang and Li, Dong and Sun, Weigao and Zhong, Yiran},
  booktitle = {Proceedings of COLM},
  year      = {2024}
}

@inproceedings{yang2024gla,
  title     = {Gated Linear Attention Transformers with Hardware-Efficient Training},
  author    = {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  booktitle = {Proceedings of ICML},
  year      = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/stargazers"&gt;&lt;img src="https://bytecrank.com/nastyox/reporoster/php/stargazersSVG.php?user=fla-org&amp;amp;repo=flash-linear-attention" alt="Stargazers repo roster for @fla-org/flash-linear-attention" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#fla-org/flash-linear-attention&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=fla-org/flash-linear-attention&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We extend our gratitude to &lt;a href="https://www.bitdeer.com/"&gt;Bitdeer&lt;/a&gt; for providing CI server resources that power our infrastructure.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Azure/azure-sdk-for-python</title>
      <link>https://github.com/Azure/azure-sdk-for-python</link>
      <description>&lt;p&gt;This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://learn.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Azure SDK for Python&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://azure.github.io/azure-sdk/releases/latest/python.html"&gt;&lt;img src="https://img.shields.io/badge/packages-latest-blue.svg?sanitize=true" alt="Packages" /&gt;&lt;/a&gt; &lt;a href="https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencies.html"&gt;&lt;img src="https://img.shields.io/badge/dependency-report-blue.svg?sanitize=true" alt="Dependencies" /&gt;&lt;/a&gt; &lt;a href="https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencyGraph/index.html"&gt;&lt;img src="https://img.shields.io/badge/dependency-graph-blue.svg?sanitize=true" alt="DepGraph" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/azure/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/azure-core.svg?maxAge=2592000" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/azure-sdk/public/_build/latest?definitionId=458&amp;amp;branchName=main"&gt;&lt;img src="https://dev.azure.com/azure-sdk/public/_apis/build/status/python/python%20-%20core%20-%20ci?branchName=main" alt="Build Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repository is for the active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our &lt;a href="https://docs.microsoft.com/python/azure/"&gt;public developer docs&lt;/a&gt; or our versioned &lt;a href="https://azure.github.io/azure-sdk-for-python"&gt;developer docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;For your convenience, each service has a separate set of libraries that you can choose to use instead of one, large Azure package. To get started with a specific library, see the &lt;code&gt;README.md&lt;/code&gt; (or &lt;code&gt;README.rst&lt;/code&gt;) file located in the library's project folder.&lt;/p&gt; 
&lt;p&gt;You can find service libraries in the &lt;code&gt;/sdk&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;The client libraries are supported on Python 3.9 or later. For more details, please read our page on &lt;a href="https://github.com/Azure/azure-sdk-for-python/wiki/Azure-SDKs-Python-version-support-policy"&gt;Azure SDK for Python version support policy&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Packages available&lt;/h2&gt; 
&lt;p&gt;Each service might have a number of libraries available from each of the following categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#client-new-releases"&gt;Client - New Releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#client-previous-versions"&gt;Client - Previous Versions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#management-new-releases"&gt;Management - New Releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#management-previous-versions"&gt;Management - Previous Versions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Client: New Releases&lt;/h3&gt; 
&lt;p&gt;New wave of packages that we are announcing as &lt;strong&gt;GA&lt;/strong&gt; and several that are currently releasing in &lt;strong&gt;preview&lt;/strong&gt;. These libraries allow you to use and consume existing resources and interact with them, for example: upload a blob. These libraries share several core functionalities such as: retries, logging, transport protocols, authentication protocols, etc. that can be found in the &lt;a href="https://github.com/Azure/azure-sdk-for-python/raw/main/sdk/core/azure-core"&gt;azure-core&lt;/a&gt; library. You can learn more about these libraries by reading guidelines that they follow &lt;a href="https://azure.github.io/azure-sdk/python/guidelines/index.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find the &lt;a href="https://azure.github.io/azure-sdk/releases/latest/index.html#python"&gt;most up to date list of all of the new packages on our page&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Client: Previous Versions&lt;/h3&gt; 
&lt;p&gt;Last stable versions of packages that have been provided for usage with Azure and are production-ready. These libraries provide you with similar functionalities to the Preview ones as they allow you to use and consume existing resources and interact with them, for example: upload a blob. They might not implement the &lt;a href="https://azure.github.io/azure-sdk/python/guidelines/index.html"&gt;guidelines&lt;/a&gt; or have the same feature set as the November releases. They do however offer wider coverage of services.&lt;/p&gt; 
&lt;h3&gt;Management: New Releases&lt;/h3&gt; 
&lt;p&gt;A new set of management libraries that follow the &lt;a href="https://azure.github.io/azure-sdk/python/guidelines/"&gt;Azure SDK Design Guidelines for Python&lt;/a&gt; are now available. These new libraries provide a number of core capabilities that are shared amongst all Azure SDKs, including the intuitive Azure Identity library, an HTTP Pipeline with custom policies, error-handling, distributed tracing, and much more. Documentation and code samples for these new libraries can be found &lt;a href="https://aka.ms/azsdk/python/mgmt"&gt;here&lt;/a&gt;. In addition, a migration guide that shows how to transition from older versions of libraries is located &lt;a href="https://github.com/Azure/azure-sdk-for-python/raw/main/doc/sphinx/mgmt_quickstart.rst#migration-guide"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find the &lt;a href="https://azure.github.io/azure-sdk/releases/latest/mgmt/python.html"&gt;most up to date list of all of the new packages on our page&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries. Also, if you are experiencing authentication issues with the management libraries after upgrading certain packages, it's possible that you upgraded to the new versions of SDK without changing the authentication code, please refer to the migration guide mentioned above for proper instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Management: Previous Versions&lt;/h3&gt; 
&lt;p&gt;For a complete list of management libraries that enable you to provision and manage Azure resources, please &lt;a href="https://azure.github.io/azure-sdk/releases/latest/all/python.html"&gt;check here&lt;/a&gt;. They might not have the same feature set as the new releases but they do offer wider coverage of services. Management libraries can be identified by namespaces that start with &lt;code&gt;azure-mgmt-&lt;/code&gt;, e.g. &lt;code&gt;azure-mgmt-compute&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Need help?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For detailed documentation visit our &lt;a href="https://aka.ms/python-docs"&gt;Azure SDK for Python documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;File an issue via &lt;a href="https://github.com/Azure/azure-sdk-for-python/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check &lt;a href="https://stackoverflow.com/questions/tagged/azure+python"&gt;previous questions&lt;/a&gt; or ask new ones on StackOverflow using &lt;code&gt;azure&lt;/code&gt; and &lt;code&gt;python&lt;/code&gt; tags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Data Collection&lt;/h2&gt; 
&lt;p&gt;The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described below. You can learn more about data collection and use in the help documentation and Microsoft‚Äôs &lt;a href="https://go.microsoft.com/fwlink/?LinkID=824704"&gt;privacy statement&lt;/a&gt;. For more information on the data collected by the Azure SDK, please visit the &lt;a href="https://azure.github.io/azure-sdk/general_azurecore.html#telemetry-policy"&gt;Telemetry Guidelines&lt;/a&gt; page.&lt;/p&gt; 
&lt;h3&gt;Telemetry Configuration&lt;/h3&gt; 
&lt;p&gt;Telemetry collection is on by default.&lt;/p&gt; 
&lt;p&gt;To opt out, you can disable telemetry at client construction. Define a &lt;code&gt;NoUserAgentPolicy&lt;/code&gt; class that is a subclass of &lt;code&gt;UserAgentPolicy&lt;/code&gt; with an &lt;code&gt;on_request&lt;/code&gt; method that does nothing. Then pass instance of this class as kwargs &lt;code&gt;user_agent_policy=NoUserAgentPolicy()&lt;/code&gt; during client creation. This will disable telemetry for all methods in the client. Do this for every new client.&lt;/p&gt; 
&lt;p&gt;The example below uses the &lt;code&gt;azure-storage-blob&lt;/code&gt; package. In your code, you can replace &lt;code&gt;azure-storage-blob&lt;/code&gt; with the package you are using.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient
from azure.core.pipeline.policies import UserAgentPolicy


# Create your credential you want to use
mi_credential = ManagedIdentityCredential()

account_url = "https://&amp;lt;storageaccountname&amp;gt;.blob.core.windows.net"

# Set up user-agent override
class NoUserAgentPolicy(UserAgentPolicy):
    def on_request(self, request):
        pass

# Create the BlobServiceClient object
blob_service_client = BlobServiceClient(account_url, credential=mi_credential, user_agent_policy=NoUserAgentPolicy())

container_client = blob_service_client.get_container_client(container=&amp;lt;container_name&amp;gt;) 
# TODO: do something with the container client like download blob to a file
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reporting security issues and security bugs&lt;/h3&gt; 
&lt;p&gt;Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) &lt;a href="mailto:secure@microsoft.com"&gt;secure@microsoft.com&lt;/a&gt;. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the &lt;a href="https://www.microsoft.com/msrc/faqs-report-an-issue"&gt;Security TechCenter&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;For details on contributing to this repository, see the &lt;a href="https://github.com/Azure/azure-sdk-for-python/raw/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.microsoft.com"&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>