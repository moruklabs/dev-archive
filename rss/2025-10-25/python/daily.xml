<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Fri, 24 Oct 2025 01:39:44 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>meta-pytorch/torchforge</title>
      <link>https://github.com/meta-pytorch/torchforge</link>
      <description>&lt;p&gt;PyTorch-native post-training at scale&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img width="35" height="35" alt="image" src="https://github.com/user-attachments/assets/2700a971-e5d6-4036-b03f-2f89c9791609" /&gt; torchforge&lt;/h1&gt; 
&lt;h4&gt;A PyTorch-native agentic RL library that lets you focus on algorithmsâ€”not infra.&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/meta-pytorch/forge/actions/workflows/gpu_test.yaml?query=branch%3Amain"&gt;&lt;img src="https://github.com/meta-pytorch/forge/actions/workflows/gpu_test.yaml/badge.svg?branch=main" alt="GPU Tests" /&gt;&lt;/a&gt; &lt;a href="https://meta-pytorch.org/torchforge/"&gt;&lt;img src="https://img.shields.io/badge/Docs-meta--pytorch.org-blue?style=flat&amp;amp;logo=readthedocs&amp;amp;logoColor=white" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/YsTYBh6PD9"&gt;&lt;img src="https://img.shields.io/badge/Discord-OpenEnv-7289da?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;The primary purpose of the torchforge ecosystem is to delineate infra concerns from model concerns thereby making RL experimentation easier. torchforge delivers this by providing clear RL abstractions and one scalable implementation of these abstractions. When you need fine-grained control over placement, fault handling/redirecting training loads during a run, or communication patterns, the primitives are there. When you donâ€™t, you can focus purely on your RL algorithm.&lt;/p&gt; 
&lt;p&gt;Key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Usability for rapid research (isolating the RL loop from infrastructure)&lt;/li&gt; 
 &lt;li&gt;Hackability for power users (all parts of the RL loop can be easily modified without interacting with infrastructure)&lt;/li&gt; 
 &lt;li&gt;Scalability (ability to shift between async and synchronous training and across thousands of GPUs)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ &lt;strong&gt;Early Development Warning&lt;/strong&gt; torchforge is currently in an experimental stage. You should expect bugs, incomplete features, and APIs that may change in future versions. The project welcomes bugfixes, but to make sure things are well coordinated you should discuss any significant change before starting the work. It's recommended that you signal your intention to contribute in the issue tracker, either by filing a new issue or by claiming an existing one.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ“– Documentation (Coming Soon)&lt;/h2&gt; 
&lt;p&gt;View torchforge's hosted documentation: &lt;a href="https://meta-pytorch.org/torchforge"&gt;https://meta-pytorch.org/torchforge&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tutorials&lt;/h2&gt; 
&lt;p&gt;You can also find our notebook tutorials (coming soon)&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Basic&lt;/h3&gt; 
&lt;p&gt;torchforge requires PyTorch 2.9.0 with &lt;a href="https://github.com/meta-pytorch/monarch"&gt;Monarch&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;, and &lt;a href="https://github.com/pytorch/torchtitan"&gt;torchtitan&lt;/a&gt;. (Note that the basic install script uses &lt;a href="https://docs.fedoraproject.org/en-US/quick-docs/dnf/"&gt;DNF&lt;/a&gt;, but could be easily extended to other Linux OS.)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n forge python=3.12
conda activate forge
./scripts/install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optional: By default, the packages installation uses conda. If user wants to install system packages on the target machine instead of conda, they can pass the &lt;code&gt;--use-sudo&lt;/code&gt; to the installation script: &lt;code&gt;./script/install.sh --use-sudo&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;After install, you can run the following command and should see output confirming GRPO training is running (you need a minimum 3 GPU devices):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m apps.grpo.main --config apps/grpo/qwen3_1_7b.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;To run SFT on a Llama3 8B model, run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.sft.main --config apps/sft/llama3_8b.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Source code is made available under a &lt;a href="https://raw.githubusercontent.com/meta-pytorch/torchforge/main/LICENSE"&gt;BSD 3 license&lt;/a&gt;, however you may have other legal obligations that govern your use of other content linked in this repository, such as the license or terms of service for third-party data and models.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>harvard-edge/cs249r_book</title>
      <link>https://github.com/harvard-edge/cs249r_book</link>
      <description>&lt;p&gt;Introduction to Machine Learning Systems&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Systems&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Principles and Practices of Engineering Artificially Intelligent Systems&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; 
  &lt;!-- Row 1: Project Health --&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/actions/workflows/validate-dev.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/validate-dev.yml?branch=dev&amp;amp;label=Build&amp;amp;logo=githubactions&amp;amp;cacheSeconds=300" alt="Build" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Last%20Commit&amp;amp;logo=git&amp;amp;cacheSeconds=300" alt="Last Commit" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; 
  &lt;!-- Row 2: Access &amp; Ecosystem --&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://mlsysbook.ai"&gt;&lt;img src="https://img.shields.io/website?url=https%3A%2F%2Fmlsysbook.ai&amp;amp;label=Website&amp;amp;logo=readthedocs" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://mlsysbook.org"&gt;&lt;img src="https://img.shields.io/website?url=https%3A%2F%2Fmlsysbook.org&amp;amp;label=Ecosystem&amp;amp;logo=internet-explorer" alt="Ecosystem" /&gt;&lt;/a&gt; &lt;a href="https://mlsysbook.org"&gt;&lt;img src="https://img.shields.io/badge/Cite-IEEE%20CODES%2B%20ISSS%202024-blue?logo=academia" alt="Citation" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; 
  &lt;!-- Row 3: Support --&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://opencollective.com/mlsysbook"&gt;&lt;img src="https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective" alt="Funding" /&gt;&lt;/a&gt; &lt;a href="https://github.com/harvard-edge/cs249r_book/raw/dev/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-CC--BY--NC--SA%204.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://www.netlify.com"&gt;&lt;img src="https://img.shields.io/badge/Powered%20by-Netlify-00C7B7?logo=netlify&amp;amp;logoColor=white" alt="Powered by Netlify" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; 
  &lt;!-- Reader Navigation --&gt; &lt;/p&gt;
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://mlsysbook.ai"&gt;ğŸ“– Read Online&lt;/a&gt;&lt;/strong&gt; â€¢ &lt;strong&gt;&lt;a href="https://mlsysbook.ai/pdf"&gt;ğŸ’¾ Download PDF&lt;/a&gt;&lt;/strong&gt; â€¢ &lt;strong&gt;&lt;a href="https://mlsysbook.ai/epub"&gt;ğŸ’¾ Download ePub&lt;/a&gt;&lt;/strong&gt; â€¢ &lt;strong&gt;&lt;a href="https://mlsysbook.org"&gt;ğŸŒ Explore Ecosystem&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;ğŸ“š &lt;strong&gt;Hardcopy edition coming 2026 via MIT Press!&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About This Book&lt;/h2&gt; 
&lt;p&gt;The &lt;strong&gt;open-source textbook&lt;/strong&gt; that teaches you to build real-world AI systems â€” from edge devices to cloud deployment. Originally developed as Harvard University's CS249r course by &lt;a href="https://github.com/profvjreddi/homepage"&gt;Prof. Vijay Janapa Reddi&lt;/a&gt;, now used by universities and students worldwide.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Our mission:&lt;/strong&gt; Expand access to AI systems education worldwide â€” empowering learners, one chapter and one lab at a time.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Why This Book Exists&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;"This grew out of a concern that while students could train AI models, few understood how to build the systems that actually make them work. As AI becomes more capable and autonomous, the critical bottleneck won't be the algorithms - it will be the engineers who can build efficient, scalable, and sustainable systems that safely harness that intelligence."&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;â€” Vijay Janapa Reddi&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“š What You'll Learn&lt;/h2&gt; 
&lt;p&gt;Go beyond training models â€” master the &lt;strong&gt;full stack&lt;/strong&gt; of real-world ML systems.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;What You'll Build&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;System Design&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Scalable, maintainable ML architectures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Data Engineering&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Robust pipelines for collection, labeling, and processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Model Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Production-ready systems from prototypes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MLOps &amp;amp; Monitoring&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reliable, continuously operating systems&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Edge AI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Resource-efficient deployment on mobile, embedded, and IoT&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â­ Support This Work&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;Show Your Support&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;Star this repository&lt;/strong&gt; to help us demonstrate the value of open AI education to funders and institutions.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&amp;amp;logo=github&amp;amp;color=gold" alt="Stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; 10,000 stars = $100,000 in additional education funding&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book"&gt;&lt;strong&gt;â­ Star Now&lt;/strong&gt;&lt;/a&gt; â€” &lt;em&gt;takes 2 seconds!&lt;/em&gt;&lt;/p&gt; 
 &lt;h3&gt;Fund the Mission (New!)&lt;/h3&gt; 
 &lt;p&gt;We've graduated this project from Harvard to enable global access and expand AI systems education worldwide. Please help us support educators globally, especially in the Global South, by providing TinyML kits for students, funding workshops, and sustaining our open-source infrastructure.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://opencollective.com/mlsysbook"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%92%9D%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge" alt="Open Collective" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;From $15/month to sponsor a learner to $250 for workshops â€” every contribution democratizes AI education.&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸŒ Community &amp;amp; Resources&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Resource&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.org"&gt;ğŸ“š &lt;strong&gt;Main Site&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Complete learning platform&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.org/tinytorch"&gt;ğŸ”¥ &lt;strong&gt;TinyTorch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Educational ML framework&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/discussions"&gt;ğŸ’¬ &lt;strong&gt;Discussions&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ask questions, share insights&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.org/community"&gt;ğŸ‘¥ &lt;strong&gt;Community&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our global learning community&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¯ For Different Audiences&lt;/h2&gt; 
&lt;h3&gt;ğŸ“ Students&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.ai"&gt;ğŸ“– Read online&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.ai/Machine-Learning-Systems.pdf"&gt;ğŸ“„ Download PDF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.org"&gt;ğŸ§ª Try hands-on labs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ‘©â€ğŸ« Educators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.org"&gt;ğŸ“‹ Course materials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.org"&gt;ğŸ¯ Instructor resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlsysbook.org"&gt;ğŸ’¡ Teaching guides&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ› ï¸ Contributors&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/contribute.md"&gt;ğŸ¤ Contribution guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/#development"&gt;âš¡ Development setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/discussions"&gt;ğŸ’¬ Join discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;h3&gt;For Readers&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Read online (continuously updated)
open https://mlsysbook.ai

# Or download PDF for offline access
curl -O https://mlsysbook.ai/Machine-Learning-Systems.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For Contributors&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/harvard-edge/cs249r_book.git
cd cs249r_book

# Quick setup (recommended)
./binder setup      # Setup environment and dependencies
./binder doctor     # Check system health

# Fast development workflow
./binder preview intro    # Fast chapter development
./binder build intro      # Build specific chapter
./binder build            # Build complete book (HTML)
./binder help            # See all commands
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the global community! Here's how you can help:&lt;/p&gt; 
&lt;h3&gt;Ways to Contribute&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ Content&lt;/strong&gt; â€” Suggest edits, improvements, or new examples&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ› ï¸ Tools&lt;/strong&gt; â€” Enhance development scripts and automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¨ Design&lt;/strong&gt; â€” Improve figures, diagrams, and visual elements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸŒ Localization&lt;/strong&gt; â€” Translate content for global accessibility&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ Infrastructure&lt;/strong&gt; â€” Help with build systems and deployment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quality Standards&lt;/h3&gt; 
&lt;p&gt;All contributions benefit from automated quality assurance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Pre-commit validation&lt;/strong&gt; â€” Automatic cleanup and checks&lt;/li&gt; 
 &lt;li&gt;ğŸ“‹ &lt;strong&gt;Content review&lt;/strong&gt; â€” Formatting and style validation&lt;/li&gt; 
 &lt;li&gt;ğŸ§ª &lt;strong&gt;Testing&lt;/strong&gt; â€” Build and link verification&lt;/li&gt; 
 &lt;li&gt;ğŸ‘¥ &lt;strong&gt;Peer review&lt;/strong&gt; â€” Community feedback&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/contribute.md"&gt;&lt;strong&gt;Start Contributing â†’&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ› ï¸ Development&lt;/h2&gt; 
&lt;h3&gt;Book Binder CLI (Recommended)&lt;/h3&gt; 
&lt;p&gt;The &lt;strong&gt;Book Binder&lt;/strong&gt; is our lightning-fast development CLI for streamlined building and iteration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Chapter development (fast iteration)
./binder preview intro                # Build and preview single chapter
./binder preview intro,ml_systems     # Build and preview multiple chapters

# Complete book building
./binder build                        # Build complete website (HTML)
./binder pdf                          # Build complete PDF
./binder epub                         # Build complete EPUB

# Management
./binder clean                        # Clean artifacts
./binder status                       # Show current status
./binder doctor                       # Run health check
./binder help                         # Show all commands
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Commands&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Book Binder CLI (Recommended)
./binder setup            # First-time setup
./binder build            # Build complete HTML book
./binder pdf              # Build complete PDF book  
./binder epub             # Build complete EPUB book
./binder preview intro    # Preview chapter development

# Traditional setup (if needed)
python3 -m venv .venv
source .venv/bin/activate
pip install -r tools/dependencies/requirements.txt
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Project Structure&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;MLSysBook/
â”œâ”€â”€ binder                   # âš¡ Fast development CLI (recommended)
â”œâ”€â”€ quarto/                  # Main book content (Quarto)
â”‚   â”œâ”€â”€ contents/            # Chapter content
â”‚   â”‚   â”œâ”€â”€ core/            # Core chapters
â”‚   â”‚   â”œâ”€â”€ labs/            # Hands-on labs
â”‚   â”‚   â”œâ”€â”€ frontmatter/     # Preface, acknowledgments
â”‚   â”‚   â”œâ”€â”€ backmatter/      # References and resources
â”‚   â”‚   â””â”€â”€ parts/           # Book parts and sections
â”‚   â”œâ”€â”€ _extensions/         # Quarto extensions
â”‚   â”œâ”€â”€ config/              # Build configurations
â”‚   â”‚   â”œâ”€â”€ _quarto-html.yml # Website build configuration
â”‚   â”‚   â””â”€â”€ _quarto-pdf.yml  # PDF build configuration
â”‚   â”œâ”€â”€ data/                # Cross-reference and metadata files
â”‚   â”œâ”€â”€ assets/              # Images, styles, media
â”‚   â”œâ”€â”€ filters/             # Lua filters
â”‚   â”œâ”€â”€ scripts/             # Build scripts
â”‚   â””â”€â”€ _quarto.yml          # Active config (symlink)
â”œâ”€â”€ tools/                   # Development automation
â”‚   â”œâ”€â”€ scripts/             # Organized development scripts
â”‚   â”‚   â”œâ”€â”€ content/         # Content management tools
â”‚   â”‚   â”œâ”€â”€ cross_refs/      # Cross-reference management
â”‚   â”‚   â”œâ”€â”€ genai/           # AI-assisted content tools
â”‚   â”‚   â”œâ”€â”€ maintenance/     # System maintenance scripts
â”‚   â”‚   â”œâ”€â”€ testing/         # Test and validation scripts
â”‚   â”‚   â””â”€â”€ utilities/       # General utility scripts
â”‚   â”œâ”€â”€ dependencies/        # Package requirements  
â”‚   â””â”€â”€ setup/               # Setup and configuration
â”œâ”€â”€ config/                  # Project configuration
â”‚   â”œâ”€â”€ dev/                 # Development configurations
â”‚   â”œâ”€â”€ linting/             # Code quality configurations
â”‚   â””â”€â”€ quarto/              # Quarto publishing settings
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ BINDER.md            # Binder CLI guide
â”‚   â”œâ”€â”€ BUILD.md             # Build instructions
â”‚   â”œâ”€â”€ DEVELOPMENT.md       # Development guide
â”‚   â””â”€â”€ contribute.md        # Contribution guidelines
â”œâ”€â”€ CHANGELOG.md             # Project changelog
â”œâ”€â”€ CITATION.bib             # Citation information
â”œâ”€â”€ pyproject.toml           # Python project configuration
â””â”€â”€ README.md                # This file
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/BINDER.md"&gt;âš¡ Binder CLI Guide&lt;/a&gt; â€” Fast development with the Book Binder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/DEVELOPMENT.md"&gt;ğŸ“‹ Development Guide&lt;/a&gt; â€” Comprehensive setup and workflow&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/MAINTENANCE_GUIDE.md"&gt;ğŸ› ï¸ Maintenance Guide&lt;/a&gt; â€” Daily tasks and troubleshooting&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/BUILD.md"&gt;ğŸ”¨ Build Instructions&lt;/a&gt; â€” Detailed build process&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/contribute.md"&gt;ğŸ¤ Contribution Guidelines&lt;/a&gt; â€” How to contribute effectively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Publishing&lt;/h3&gt; 
&lt;p&gt;Publishing is handled through GitHub Actions workflows for consistent, automated deployment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build locally to test before publishing
./binder build        # Build HTML
./binder pdf          # Build PDF  
./binder epub         # Build EPUB

# Publishing happens via GitHub Actions
# See docs/PUBLISH_LIVE_WORKFLOW.md for details
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Publishing Workflow:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Deployment&lt;/strong&gt; â€” GitHub Actions workflows handle all publishing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Checks&lt;/strong&gt; â€” Automated validation before deployment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Formats&lt;/strong&gt; â€” HTML, PDF, and EPUB published simultaneously&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Preview Deployments&lt;/strong&gt; â€” Pull requests get automatic preview deployments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/docs/PUBLISH_LIVE_WORKFLOW.md"&gt;Publishing Documentation&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# First time setup
./binder setup

# Check system health
./binder doctor

# Quick preview
./binder preview intro
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“‹ Citation &amp;amp; License&lt;/h2&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{reddi2024mlsysbook,
  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},
  author       = {Reddi, Vijay Janapa},
  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},
  pages        = {41--42},
  year         = {2024},
  organization = {IEEE},
  url          = {https://mlsysbook.org}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;This work is licensed under &lt;strong&gt;Creative Commons Attributionâ€“NonCommercialâ€“ShareAlike 4.0 International&lt;/strong&gt; (CC BY-NC-SA 4.0). You may share and adapt the material for non-commercial purposes with appropriate credit.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ™ Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks goes to these wonderful people who have contributed to making this resource better for everyone:&lt;/p&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/profvjreddi"&gt;&lt;img src="https://avatars.githubusercontent.com/profvjreddi?s=100" width="100px;" alt="Vijay Janapa Reddi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vijay Janapa Reddi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/hzeljko"&gt;&lt;img src="https://avatars.githubusercontent.com/hzeljko?s=100" width="100px;" alt="Zeljko Hrcek" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zeljko Hrcek&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Mjrovai"&gt;&lt;img src="https://avatars.githubusercontent.com/Mjrovai?s=100" width="100px;" alt="Marcelo Rovai" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marcelo Rovai&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jasonjabbour"&gt;&lt;img src="https://avatars.githubusercontent.com/jasonjabbour?s=100" width="100px;" alt="Jason Jabbour" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jason Jabbour&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/uchendui"&gt;&lt;img src="https://avatars.githubusercontent.com/uchendui?s=100" width="100px;" alt="Ikechukwu Uchendu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ikechukwu Uchendu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/kai4avaya"&gt;&lt;img src="https://avatars.githubusercontent.com/kai4avaya?s=100" width="100px;" alt="Kai Kleinbard" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kai Kleinbard&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Naeemkh"&gt;&lt;img src="https://avatars.githubusercontent.com/Naeemkh?s=100" width="100px;" alt="Naeem Khoshnevis" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Naeem Khoshnevis&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Sara-Khosravi"&gt;&lt;img src="https://avatars.githubusercontent.com/Sara-Khosravi?s=100" width="100px;" alt="Sara Khosravi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sara Khosravi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/V0XNIHILI"&gt;&lt;img src="https://avatars.githubusercontent.com/V0XNIHILI?s=100" width="100px;" alt="Douwe den Blanken" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Douwe den Blanken&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/18jeffreyma"&gt;&lt;img src="https://avatars.githubusercontent.com/18jeffreyma?s=100" width="100px;" alt="Jeffrey Ma" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jeffrey Ma&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/shanzehbatool"&gt;&lt;img src="https://avatars.githubusercontent.com/shanzehbatool?s=100" width="100px;" alt="shanzehbatool" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shanzehbatool&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/eliasab16"&gt;&lt;img src="https://avatars.githubusercontent.com/eliasab16?s=100" width="100px;" alt="Elias" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Elias&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/JaredP94"&gt;&lt;img src="https://avatars.githubusercontent.com/JaredP94?s=100" width="100px;" alt="Jared Ping" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jared Ping&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ishapira1"&gt;&lt;img src="https://avatars.githubusercontent.com/ishapira1?s=100" width="100px;" alt="Itai Shapira" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Itai Shapira&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/8863743b4f26c1a20e730fcf7ebc3bc0?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Maximilian Lam" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Maximilian Lam&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jaysonzlin"&gt;&lt;img src="https://avatars.githubusercontent.com/jaysonzlin?s=100" width="100px;" alt="Jayson Lin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jayson Lin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/sophiacho1"&gt;&lt;img src="https://avatars.githubusercontent.com/sophiacho1?s=100" width="100px;" alt="Sophia Cho" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sophia Cho&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/andreamurillomtz"&gt;&lt;img src="https://avatars.githubusercontent.com/andreamurillomtz?s=100" width="100px;" alt="Andrea" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrea&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/alxrod"&gt;&lt;img src="https://avatars.githubusercontent.com/alxrod?s=100" width="100px;" alt="Alex Rodriguez" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alex Rodriguez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/korneelf1"&gt;&lt;img src="https://avatars.githubusercontent.com/korneelf1?s=100" width="100px;" alt="Korneel Van den Berghe" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Korneel Van den Berghe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/colbybanbury"&gt;&lt;img src="https://avatars.githubusercontent.com/colbybanbury?s=100" width="100px;" alt="Colby Banbury" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Colby Banbury&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/zishenwan"&gt;&lt;img src="https://avatars.githubusercontent.com/zishenwan?s=100" width="100px;" alt="Zishen Wan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zishen Wan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/mmaz"&gt;&lt;img src="https://avatars.githubusercontent.com/mmaz?s=100" width="100px;" alt="Mark Mazumder" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mark Mazumder&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/DivyaAmirtharaj"&gt;&lt;img src="https://avatars.githubusercontent.com/DivyaAmirtharaj?s=100" width="100px;" alt="Divya Amirtharaj" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Divya Amirtharaj&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ma3mool"&gt;&lt;img src="https://avatars.githubusercontent.com/ma3mool?s=100" width="100px;" alt="Abdulrahman Mahmoud" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abdulrahman Mahmoud&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/srivatsankrishnan"&gt;&lt;img src="https://avatars.githubusercontent.com/srivatsankrishnan?s=100" width="100px;" alt="Srivatsan Krishnan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Srivatsan Krishnan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/James-QiuHaoran"&gt;&lt;img src="https://avatars.githubusercontent.com/James-QiuHaoran?s=100" width="100px;" alt="Haoran Qiu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Haoran Qiu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/aptl26"&gt;&lt;img src="https://avatars.githubusercontent.com/aptl26?s=100" width="100px;" alt="Aghyad Deeb" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aghyad Deeb&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/arnaumarin"&gt;&lt;img src="https://avatars.githubusercontent.com/arnaumarin?s=100" width="100px;" alt="marin-llobet" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;marin-llobet&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jared-ni"&gt;&lt;img src="https://avatars.githubusercontent.com/jared-ni?s=100" width="100px;" alt="Jared Ni" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jared Ni&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/oishib"&gt;&lt;img src="https://avatars.githubusercontent.com/oishib?s=100" width="100px;" alt="oishib" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;oishib&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/MichaelSchnebly"&gt;&lt;img src="https://avatars.githubusercontent.com/MichaelSchnebly?s=100" width="100px;" alt="Michael Schnebly" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Michael Schnebly&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ELSuitorHarvard"&gt;&lt;img src="https://avatars.githubusercontent.com/ELSuitorHarvard?s=100" width="100px;" alt="ELSuitorHarvard" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ELSuitorHarvard&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Ekhao"&gt;&lt;img src="https://avatars.githubusercontent.com/Ekhao?s=100" width="100px;" alt="Emil Njor" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emil Njor&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/VThuong99"&gt;&lt;img src="https://avatars.githubusercontent.com/VThuong99?s=100" width="100px;" alt="Thuong Duong" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Thuong Duong&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/AditiR-42"&gt;&lt;img src="https://avatars.githubusercontent.com/AditiR-42?s=100" width="100px;" alt="Aditi Raju" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aditi Raju&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jaywonchung"&gt;&lt;img src="https://avatars.githubusercontent.com/jaywonchung?s=100" width="100px;" alt="Jae-Won Chung" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jae-Won Chung&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/leo47007"&gt;&lt;img src="https://avatars.githubusercontent.com/leo47007?s=100" width="100px;" alt="Yu-Shun Hsiao" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yu-Shun Hsiao&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/BaeHenryS"&gt;&lt;img src="https://avatars.githubusercontent.com/BaeHenryS?s=100" width="100px;" alt="Henry Bae" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Henry Bae&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ShvetankPrakash"&gt;&lt;img src="https://avatars.githubusercontent.com/ShvetankPrakash?s=100" width="100px;" alt="Shvetank Prakash" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shvetank Prakash&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/af39c27c6090c50a1921a9b6366e81cc?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Emeka Ezike" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emeka Ezike&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/arbass22"&gt;&lt;img src="https://avatars.githubusercontent.com/arbass22?s=100" width="100px;" alt="Andrew Bass" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrew Bass&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jzhou1318"&gt;&lt;img src="https://avatars.githubusercontent.com/jzhou1318?s=100" width="100px;" alt="Jennifer Zhou" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jennifer Zhou&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/aryatschand"&gt;&lt;img src="https://avatars.githubusercontent.com/aryatschand?s=100" width="100px;" alt="Arya Tschand" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Arya Tschand&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/pongtr"&gt;&lt;img src="https://avatars.githubusercontent.com/pongtr?s=100" width="100px;" alt="Pong Trairatvorakul" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Pong Trairatvorakul&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/0c931fcfd03cd548d44c90602dd773ba?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Matthew Stewart" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Matthew Stewart&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/marcozennaro"&gt;&lt;img src="https://avatars.githubusercontent.com/marcozennaro?s=100" width="100px;" alt="Marco Zennaro" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marco Zennaro&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/euranofshin"&gt;&lt;img src="https://avatars.githubusercontent.com/euranofshin?s=100" width="100px;" alt="Eura Nofshin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Eura Nofshin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/BrunoScaglione"&gt;&lt;img src="https://avatars.githubusercontent.com/BrunoScaglione?s=100" width="100px;" alt="Bruno Scaglione" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bruno Scaglione&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/taunoe"&gt;&lt;img src="https://avatars.githubusercontent.com/taunoe?s=100" width="100px;" alt="Tauno Erik" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tauno Erik&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/alex-oesterling"&gt;&lt;img src="https://avatars.githubusercontent.com/alex-oesterling?s=100" width="100px;" alt="Alex Oesterling" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alex Oesterling&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/gnodipac886"&gt;&lt;img src="https://avatars.githubusercontent.com/gnodipac886?s=100" width="100px;" alt="gnodipac886" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;gnodipac886&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Gjain234"&gt;&lt;img src="https://avatars.githubusercontent.com/Gjain234?s=100" width="100px;" alt="Gauri Jain" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Gauri Jain&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Allen-Kuang"&gt;&lt;img src="https://avatars.githubusercontent.com/Allen-Kuang?s=100" width="100px;" alt="Allen-Kuang" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Allen-Kuang&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/TheHiddenLayer"&gt;&lt;img src="https://avatars.githubusercontent.com/TheHiddenLayer?s=100" width="100px;" alt="TheHiddenLayer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;TheHiddenLayer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/FinAminToastCrunch"&gt;&lt;img src="https://avatars.githubusercontent.com/FinAminToastCrunch?s=100" width="100px;" alt="Fin Amin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fin Amin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/468ef35acc69f3266efd700992daa369?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Fatima Shah" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fatima Shah&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/serco425"&gt;&lt;img src="https://avatars.githubusercontent.com/serco425?s=100" width="100px;" alt="Sercan AygÃ¼n" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sercan AygÃ¼n&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/vitasam"&gt;&lt;img src="https://avatars.githubusercontent.com/vitasam?s=100" width="100px;" alt="The Random DIY" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;The Random DIY&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/BravoBaldo"&gt;&lt;img src="https://avatars.githubusercontent.com/BravoBaldo?s=100" width="100px;" alt="Baldassarre Cesarano" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Baldassarre Cesarano&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/YangZhou1997"&gt;&lt;img src="https://avatars.githubusercontent.com/YangZhou1997?s=100" width="100px;" alt="Yang Zhou" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yang Zhou&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/YLab-UChicago"&gt;&lt;img src="https://avatars.githubusercontent.com/YLab-UChicago?s=100" width="100px;" alt="yanjingl" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;yanjingl&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/AbenezerKb"&gt;&lt;img src="https://avatars.githubusercontent.com/AbenezerKb?s=100" width="100px;" alt="Abenezer Angamo" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abenezer Angamo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jasonlyik"&gt;&lt;img src="https://avatars.githubusercontent.com/jasonlyik?s=100" width="100px;" alt="Jason Yik" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jason Yik&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/aethernavshulkraven-allain"&gt;&lt;img src="https://avatars.githubusercontent.com/aethernavshulkraven-allain?s=100" width="100px;" alt="à¤…à¤°à¤¨à¤µ à¤¶à¥à¤•à¥à¤²à¤¾ | Arnav Shukla" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;à¤…à¤°à¤¨à¤µ à¤¶à¥à¤•à¥à¤²à¤¾ | Arnav Shukla&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/arighosh05"&gt;&lt;img src="https://avatars.githubusercontent.com/arighosh05?s=100" width="100px;" alt="Aritra Ghosh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aritra Ghosh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/happyappledog"&gt;&lt;img src="https://avatars.githubusercontent.com/happyappledog?s=100" width="100px;" alt="happyappledog" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;happyappledog&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/abigailswallow"&gt;&lt;img src="https://avatars.githubusercontent.com/abigailswallow?s=100" width="100px;" alt="abigailswallow" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;abigailswallow&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/bilgeacun"&gt;&lt;img src="https://avatars.githubusercontent.com/bilgeacun?s=100" width="100px;" alt="Bilge Acun" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bilge Acun&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/atcheng2"&gt;&lt;img src="https://avatars.githubusercontent.com/atcheng2?s=100" width="100px;" alt="Andy Cheng" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andy Cheng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/cursoragent"&gt;&lt;img src="https://avatars.githubusercontent.com/cursoragent?s=100" width="100px;" alt="Cursor Agent" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Cursor Agent&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/emmanuel2406"&gt;&lt;img src="https://avatars.githubusercontent.com/emmanuel2406?s=100" width="100px;" alt="Emmanuel Rassou" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emmanuel Rassou&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jessicaquaye"&gt;&lt;img src="https://avatars.githubusercontent.com/jessicaquaye?s=100" width="100px;" alt="Jessica Quaye" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jessica Quaye&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/vijay-edu"&gt;&lt;img src="https://avatars.githubusercontent.com/vijay-edu?s=100" width="100px;" alt="Vijay Edupuganti" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vijay Edupuganti&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/sjohri20"&gt;&lt;img src="https://avatars.githubusercontent.com/sjohri20?s=100" width="100px;" alt="Shreya Johri" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shreya Johri&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/skmur"&gt;&lt;img src="https://avatars.githubusercontent.com/skmur?s=100" width="100px;" alt="Sonia Murthy" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sonia Murthy&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/fc4f3460cdfb9365ab59bdeafb06413e?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Costin-Andrei Oncescu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Costin-Andrei Oncescu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/0d6b8616427d8b19d425c9808692e347?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="formlsysbookissue" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;formlsysbookissue&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/7cd8d5dfd83071f23979019d97655dc5?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Annie Laurie Cook" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Annie Laurie Cook&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/f88052cca4f401d9b0f43aed0a53434a?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Jothi Ramaswamy" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jothi Ramaswamy&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/35a8d9ffd03f05e79a2c6ce6206a56f2?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Batur Arslan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Batur Arslan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/bd53d146aa888548c8db4da02bf81e7a?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Curren Iyer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Curren Iyer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/468ef35acc69f3266efd700992daa369?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Fatima Shah" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fatima Shah&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/8d8410338458e08bd5e4b96f58e1c217?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Edward Jin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Edward Jin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/a5a47df988ab1720dd706062e523ca32?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="a-saraf" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;a-saraf&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/c2dc311aa8122d5f5f061e1db14682b1?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="songhan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;songhan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/4814aad67982ab07a69006a1ce9d2a72?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="jvijay" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;jvijay&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/43b1feff77c8a95fd581774fb8ec891f?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Zishen" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zishen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Made with â¤ï¸ for AI learners worldwide&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Our goal is to educate 1 million AI systems engineers for the future at the edge of AI.&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>crewAIInc/crewAI</title>
      <link>https://github.com/crewAIInc/crewAI</link>
      <description>&lt;p&gt;Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/crewai_logo.png" width="600px" alt="Open source Multi-AI Agent orchestration framework" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"&gt; &lt;a href="https://trendshift.io/repositories/11239" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/11239" alt="crewAIInc%2FcrewAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://crewai.com"&gt;Homepage&lt;/a&gt; Â· &lt;a href="https://docs.crewai.com"&gt;Docs&lt;/a&gt; Â· &lt;a href="https://app.crewai.com"&gt;Start Cloud Trial&lt;/a&gt; Â· &lt;a href="https://blog.crewai.com"&gt;Blog&lt;/a&gt; Â· &lt;a href="https://community.crewai.com"&gt;Forum&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://img.shields.io/github/stars/crewAIInc/crewAI" alt="GitHub Repo stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/network/members"&gt; &lt;img src="https://img.shields.io/github/forks/crewAIInc/crewAI" alt="GitHub forks" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/issues"&gt; &lt;img src="https://img.shields.io/github/issues/crewAIInc/crewAI" alt="GitHub issues" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/pulls"&gt; &lt;img src="https://img.shields.io/github/issues-pr/crewAIInc/crewAI" alt="GitHub pull requests" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="License: MIT" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/v/crewai" alt="PyPI version" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/dm/crewai" alt="PyPI downloads" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/crewAIInc"&gt; &lt;img src="https://img.shields.io/twitter/follow/crewAIInc?style=social" alt="Twitter Follow" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Fast and Flexible Multi-Agent Automation Framework&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;CrewAI is a lean, lightning-fast Python framework built entirely from scratchâ€”completely &lt;strong&gt;independent of LangChain or other agent frameworks&lt;/strong&gt;. It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Crews&lt;/strong&gt;: Optimize for autonomy and collaborative intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Flows&lt;/strong&gt;: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With over 100,000 developers certified through our community courses at &lt;a href="https://learn.crewai.com"&gt;learn.crewai.com&lt;/a&gt;, CrewAI is rapidly becoming the standard for enterprise-ready AI automation.&lt;/p&gt; 
&lt;h1&gt;CrewAI AMP Suite&lt;/h1&gt; 
&lt;p&gt;CrewAI AMP Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.&lt;/p&gt; 
&lt;p&gt;You can try one part of the suite the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane for free&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Crew Control Plane Key Features:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tracing &amp;amp; Observability&lt;/strong&gt;: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified Control Plane&lt;/strong&gt;: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integrations&lt;/strong&gt;: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Security&lt;/strong&gt;: Built-in robust security and compliance measures ensuring safe deployment and management.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Actionable Insights&lt;/strong&gt;: Real-time analytics and reporting to optimize performance and decision-making.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;24/7 Support&lt;/strong&gt;: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-premise and Cloud Deployment Options&lt;/strong&gt;: Deploy CrewAI AMP on-premise or in the cloud, depending on your security and compliance requirements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI AMP is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient, intelligent automations.&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#why-crewai"&gt;Why CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#understanding-flows-and-crews"&gt;Understanding Flows and Crews&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;CrewAI vs LangGraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#examples"&gt;Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#quick-tutorial"&gt;Quick Tutorial&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#write-job-descriptions"&gt;Write Job Descriptions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#trip-planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#stock-analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#using-crews-and-flows-together"&gt;Using Crews and Flows Together&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#connecting-your-crew-to-a-model"&gt;Connecting Your Crew to a Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;How CrewAI Compares&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#frequently-asked-questions-faq"&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#telemetry"&gt;Telemetry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why CrewAI?&lt;/h2&gt; 
&lt;div align="center" style="margin-bottom: 30px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/asset.png" alt="CrewAI Logo" width="100%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone Framework&lt;/strong&gt;: Built from scratch, independent of LangChain or any other agent framework.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Optimized for speed and minimal resource usage, enabling faster execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Low Level Customization&lt;/strong&gt;: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ideal for Every Use Case&lt;/strong&gt;: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Robust Community&lt;/strong&gt;: Backed by a rapidly growing community of over &lt;strong&gt;100,000 certified&lt;/strong&gt; developers offering comprehensive support and resources.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Setup and run your first CrewAI agents by following this tutorial.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-kSOTtYzgEw" title="CrewAI Getting Started Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg" alt="CrewAI Getting Started Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;p&gt;Learning Resources&lt;/p&gt; 
&lt;p&gt;Learn CrewAI through our comprehensive courses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/"&gt;Multi AI Agent Systems with CrewAI&lt;/a&gt; - Master the fundamentals of multi-agent systems&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/"&gt;Practical Multi AI Agents and Advanced Use Cases&lt;/a&gt; - Deep dive into advanced implementations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Understanding Flows and Crews&lt;/h3&gt; 
&lt;p&gt;CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Crews&lt;/strong&gt;: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Natural, autonomous decision-making between agents&lt;/li&gt; 
   &lt;li&gt;Dynamic task delegation and collaboration&lt;/li&gt; 
   &lt;li&gt;Specialized roles with defined goals and expertise&lt;/li&gt; 
   &lt;li&gt;Flexible problem-solving approaches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flows&lt;/strong&gt;: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fine-grained control over execution paths for real-world scenarios&lt;/li&gt; 
   &lt;li&gt;Secure, consistent state management between tasks&lt;/li&gt; 
   &lt;li&gt;Clean integration of AI agents with production Python code&lt;/li&gt; 
   &lt;li&gt;Conditional branching for complex business logic&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build complex, production-grade applications&lt;/li&gt; 
 &lt;li&gt;Balance autonomy with precise control&lt;/li&gt; 
 &lt;li&gt;Handle sophisticated real-world scenarios&lt;/li&gt; 
 &lt;li&gt;Maintain clean, maintainable code structure&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting Started with Installation&lt;/h3&gt; 
&lt;p&gt;To get started with CrewAI, follow these simple steps:&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;Ensure you have Python &amp;gt;=3.10 &amp;lt;3.14 installed on your system. CrewAI uses &lt;a href="https://docs.astral.sh/uv/"&gt;UV&lt;/a&gt; for dependency management and package handling, offering a seamless setup and execution experience.&lt;/p&gt; 
&lt;p&gt;First, install CrewAI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to install the 'crewai' package along with its optional features that include additional tools for agents, you can do so by using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command above installs the basic package and also adds extra components which require more dependencies to function.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting Dependencies&lt;/h3&gt; 
&lt;p&gt;If you encounter issues during installation or usage, here are some common solutions:&lt;/p&gt; 
&lt;h4&gt;Common Issues&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ModuleNotFoundError: No module named 'tiktoken'&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Install tiktoken explicitly: &lt;code&gt;pip install 'crewai[embeddings]'&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If using embedchain or other tools: &lt;code&gt;pip install 'crewai[tools]'&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Failed building wheel for tiktoken&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure Rust compiler is installed (see installation steps above)&lt;/li&gt; 
   &lt;li&gt;For Windows: Verify Visual C++ Build Tools are installed&lt;/li&gt; 
   &lt;li&gt;Try upgrading pip: &lt;code&gt;pip install --upgrade pip&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If issues persist, use a pre-built wheel: &lt;code&gt;pip install tiktoken --prefer-binary&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. Setting Up Your Crew with the YAML Configuration&lt;/h3&gt; 
&lt;p&gt;To create a new CrewAI project, run the following CLI (Command Line Interface) command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew &amp;lt;project_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command creates a new project folder with the following structure:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my_project/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â””â”€â”€ src/
    â””â”€â”€ my_project/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ main.py
        â”œâ”€â”€ crew.py
        â”œâ”€â”€ tools/
        â”‚   â”œâ”€â”€ custom_tool.py
        â”‚   â””â”€â”€ __init__.py
        â””â”€â”€ config/
            â”œâ”€â”€ agents.yaml
            â””â”€â”€ tasks.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now start developing your crew by editing the files in the &lt;code&gt;src/my_project&lt;/code&gt; folder. The &lt;code&gt;main.py&lt;/code&gt; file is the entry point of the project, the &lt;code&gt;crew.py&lt;/code&gt; file is where you define your crew, the &lt;code&gt;agents.yaml&lt;/code&gt; file is where you define your agents, and the &lt;code&gt;tasks.yaml&lt;/code&gt; file is where you define your tasks.&lt;/p&gt; 
&lt;h4&gt;To customize your project, you can:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/agents.yaml&lt;/code&gt; to define your agents.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/tasks.yaml&lt;/code&gt; to define your tasks.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/crew.py&lt;/code&gt; to add your own logic, tools, and specific arguments.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/main.py&lt;/code&gt; to add custom inputs for your agents and tasks.&lt;/li&gt; 
 &lt;li&gt;Add your environment variables into the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Example of a simple crew with a sequential process:&lt;/h4&gt; 
&lt;p&gt;Instantiate your crew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew latest-ai-development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modify the files as needed to fit your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;agents.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/agents.yaml
researcher:
  role: &amp;gt;
    {topic} Senior Data Researcher
  goal: &amp;gt;
    Uncover cutting-edge developments in {topic}
  backstory: &amp;gt;
    You're a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &amp;gt;
    {topic} Reporting Analyst
  goal: &amp;gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &amp;gt;
    You're a meticulous analyst with a keen eye for detail. You're known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;tasks.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/tasks.yaml
research_task:
  description: &amp;gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &amp;gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &amp;gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &amp;gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without '```'
  agent: reporting_analyst
  output_file: report.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;crew.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	"""LatestAiDevelopment crew"""
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['researcher'],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['reporting_analyst'],
			verbose=True
		)

	@task
	def research_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['research_task'],
		)

	@task
	def reporting_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['reporting_task'],
			output_file='report.md'
		)

	@crew
	def crew(self) -&amp;gt; Crew:
		"""Creates the LatestAiDevelopment crew"""
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;main.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    """
    Run the crew.
    """
    inputs = {
        'topic': 'AI Agents'
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Running Your Crew&lt;/h3&gt; 
&lt;p&gt;Before running your crew, make sure you have the following keys set as environment variables in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An &lt;a href="https://platform.openai.com/account/api-keys"&gt;OpenAI API key&lt;/a&gt; (or other LLM API key): &lt;code&gt;OPENAI_API_KEY=sk-...&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; API key: &lt;code&gt;SERPER_API_KEY=YOUR_KEY_HERE&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd my_project
crewai install (Optional)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run your crew, execute the following command in the root of your project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python src/my_project/main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If an error happens due to the usage of poetry, please run the following command to update your crewai package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai update
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the output in the console and the &lt;code&gt;report.md&lt;/code&gt; file should be created in the root of your project with the full final report.&lt;/p&gt; 
&lt;p&gt;In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. &lt;a href="https://docs.crewai.com/core-concepts/Processes/"&gt;See more about the processes here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise controlâ€”free from the complexity and limitations found in other agent frameworks.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone &amp;amp; Lean&lt;/strong&gt;: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible &amp;amp; Precise&lt;/strong&gt;: Easily orchestrate autonomous agents through intuitive &lt;a href="https://docs.crewai.com/concepts/crews"&gt;Crews&lt;/a&gt; or precise &lt;a href="https://docs.crewai.com/concepts/flows"&gt;Flows&lt;/a&gt;, achieving perfect balance for your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt;: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Customization&lt;/strong&gt;: Tailor every aspectâ€”from high-level workflows down to low-level internal prompts and agent behaviors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Performance&lt;/strong&gt;: Consistent results across simple tasks and complex, enterprise-level automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Thriving Community&lt;/strong&gt;: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;You can test different real life examples of AI crews in the &lt;a href="https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file"&gt;CrewAI-examples repo&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/landing_page_generator"&gt;Landing Page Generator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.crewai.com/how-to/Human-Input-on-Execution"&gt;Having Human input on the execution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Tutorial&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tnejrr-0a94" title="CrewAI Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg" alt="CrewAI Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Write Job Descriptions&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/job-posting"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=u98wEMz-9to" title="Jobs postings"&gt;&lt;img src="https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg" alt="Jobs postings" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Trip Planner&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=xis7rWp-hjs" title="Trip Planner"&gt;&lt;img src="https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg" alt="Trip Planner" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Stock Analysis&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=e0Uj4yWdaAg" title="Stock Analysis"&gt;&lt;img src="https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg" alt="Stock Analysis" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Using Crews and Flows Together&lt;/h3&gt; 
&lt;p&gt;CrewAI's power truly shines when combining Crews with Flows to create sophisticated automation pipelines. CrewAI flows support logical operators like &lt;code&gt;or_&lt;/code&gt; and &lt;code&gt;and_&lt;/code&gt; to combine multiple conditions. This can be used with &lt;code&gt;@start&lt;/code&gt;, &lt;code&gt;@listen&lt;/code&gt;, or &lt;code&gt;@router&lt;/code&gt; decorators to create complex triggering conditions.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;or_&lt;/code&gt;: Triggers when any of the specified conditions are met.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;and_&lt;/code&gt;Triggers when all of the specified conditions are met.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here's how you can orchestrate multiple Crews within a Flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = "neutral"
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = "analyzing"
        return {"sector": "tech", "timeframe": "1W"}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role="Senior Market Analyst",
            goal="Conduct deep market analysis with expert insight",
            backstory="You're a veteran analyst known for identifying subtle market patterns"
        )
        researcher = Agent(
            role="Data Researcher",
            goal="Gather and validate supporting market data",
            backstory="You excel at finding and correlating multiple data sources"
        )

        analysis_task = Task(
            description="Analyze {sector} sector data for the past {timeframe}",
            expected_output="Detailed market analysis with confidence score",
            agent=analyst
        )
        research_task = Task(
            description="Find supporting data to validate the analysis",
            expected_output="Corroborating evidence and potential contradictions",
            agent=researcher
        )

        # Demonstrate crew autonomy
        analysis_crew = Crew(
            agents=[analyst, researcher],
            tasks=[analysis_task, research_task],
            process=Process.sequential,
            verbose=True
        )
        return analysis_crew.kickoff(inputs=market_data)  # Pass market_data as named inputs

    @router(analyze_with_crew)
    def determine_next_steps(self):
        # Show flow control with conditional routing
        if self.state.confidence &amp;gt; 0.8:
            return "high_confidence"
        elif self.state.confidence &amp;gt; 0.5:
            return "medium_confidence"
        return "low_confidence"

    @listen("high_confidence")
    def execute_strategy(self):
        # Demonstrate complex decision making
        strategy_crew = Crew(
            agents=[
                Agent(role="Strategy Expert",
                      goal="Develop optimal market strategy")
            ],
            tasks=[
                Task(description="Create detailed strategy based on analysis",
                     expected_output="Step-by-step action plan")
            ]
        )
        return strategy_crew.kickoff()

    @listen(or_("medium_confidence", "low_confidence"))
    def request_additional_analysis(self):
        self.state.recommendations.append("Gather more data")
        return "Additional analysis required"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This example demonstrates how to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use Python code for basic data operations&lt;/li&gt; 
 &lt;li&gt;Create and execute Crews as steps in your workflow&lt;/li&gt; 
 &lt;li&gt;Use Flow decorators to manage the sequence of operations&lt;/li&gt; 
 &lt;li&gt;Implement conditional branching based on Crew results&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Connecting Your Crew to a Model&lt;/h2&gt; 
&lt;p&gt;CrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several other ways to allow your agents to connect to models. For example, you can configure your agents to use a local model via the Ollama tool.&lt;/p&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;Connect CrewAI to LLMs&lt;/a&gt; page for details on configuring your agents' connections to models.&lt;/p&gt; 
&lt;h2&gt;How CrewAI Compares&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CrewAI's Advantage&lt;/strong&gt;: CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture. The framework excels at both high-level orchestration and low-level customization, enabling complex, production-grade systems with granular control.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: While LangGraph provides a foundation for building agent workflows, its approach requires significant boilerplate code and complex state management patterns. The framework's tight coupling with LangChain can limit flexibility when implementing custom agent behaviors or integrating with external systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;P.S. CrewAI demonstrates significant performance advantages over LangGraph, executing 5.76x faster in certain cases like this QA task example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/QA%20Agent"&gt;see comparison&lt;/a&gt;) while achieving higher evaluation scores with faster completion times in certain coding tasks, like in this example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/raw/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/Coding%20Assistant/coding_assistant_eval.ipynb"&gt;detailed analysis&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Autogen&lt;/strong&gt;: While Autogen excels at creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents' interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ChatDev&lt;/strong&gt;: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;CrewAI is open-source and we welcome contributions. If you're looking to contribute, please:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fork the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your feature.&lt;/li&gt; 
 &lt;li&gt;Add your feature or improvement.&lt;/li&gt; 
 &lt;li&gt;Send a pull request.&lt;/li&gt; 
 &lt;li&gt;We appreciate your input!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing Dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv lock
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Virtual Env&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit hooks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Tests&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running static type checks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx mypy src
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Packaging&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing Locally&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install dist/*.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;CrewAI uses anonymous telemetry to collect usage data with the main purpose of helping us improve the library by focusing our efforts on the most used features, integrations and tools.&lt;/p&gt; 
&lt;p&gt;It's pivotal to understand that &lt;strong&gt;NO data is collected&lt;/strong&gt; concerning prompts, task descriptions, agents' backstories or goals, usage of tools, API calls, responses, any data processed by the agents, or secrets and environment variables, with the exception of the conditions mentioned. When the &lt;code&gt;share_crew&lt;/code&gt; feature is enabled, detailed data including task descriptions, agents' backstories or goals, and other specific attributes are collected to provide deeper insights while respecting user privacy. Users can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.&lt;/p&gt; 
&lt;p&gt;Data collected includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Version of CrewAI 
  &lt;ul&gt; 
   &lt;li&gt;So we can understand how many users are using the latest version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Version of Python 
  &lt;ul&gt; 
   &lt;li&gt;So we can decide on what versions to better support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;General OS (e.g. number of CPUs, macOS/Windows/Linux) 
  &lt;ul&gt; 
   &lt;li&gt;So we know what OS we should focus on and if we could build specific OS related features&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Number of agents and tasks in a crew 
  &lt;ul&gt; 
   &lt;li&gt;So we make sure we are testing internally with similar use cases and educate people on the best practices&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Crew Process being used 
  &lt;ul&gt; 
   &lt;li&gt;Understand where we should focus our efforts&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Agents are using memory or allowing delegation 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we improved the features or maybe even drop them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Tasks are being executed in parallel or sequentially 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we should focus more on parallel execution&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Language model being used 
  &lt;ul&gt; 
   &lt;li&gt;Improved support on most used languages&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Roles of agents in a crew 
  &lt;ul&gt; 
   &lt;li&gt;Understand high level use cases so we can build better tools, integrations and examples about it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Tools names available 
  &lt;ul&gt; 
   &lt;li&gt;Understand out of the publicly available tools, which ones are being used the most so we can improve them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Users can opt-in to Further Telemetry, sharing the complete telemetry data by setting the &lt;code&gt;share_crew&lt;/code&gt; attribute to &lt;code&gt;True&lt;/code&gt; on their Crews. Enabling &lt;code&gt;share_crew&lt;/code&gt; results in the collection of detailed crew and task execution data, including &lt;code&gt;goal&lt;/code&gt;, &lt;code&gt;backstory&lt;/code&gt;, &lt;code&gt;context&lt;/code&gt;, and &lt;code&gt;output&lt;/code&gt; of tasks. This enables a deeper insight into usage patterns while respecting the user's choice to share.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;CrewAI is released under the &lt;a href="https://github.com/crewAIInc/crewAI/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Frequently Asked Questions (FAQ)&lt;/h2&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-exactly-is-crewai"&gt;What exactly is CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-do-i-install-crewai"&gt;How do I install CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-depend-on-langchain"&gt;Does CrewAI depend on LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-open-source"&gt;Is CrewAI open-source?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-collect-data-from-users"&gt;Does CrewAI collect data from users?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Features and Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-crewai-handle-complex-use-cases"&gt;Can CrewAI handle complex use cases?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-use-crewai-with-local-ai-models"&gt;Can I use CrewAI with local AI models?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-makes-crews-different-from-flows"&gt;What makes Crews different from Flows?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-is-crewai-better-than-langchain"&gt;How is CrewAI better than LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-support-fine-tuning-or-training-custom-models"&gt;Does CrewAI support fine-tuning or training custom models?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resources and Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-where-can-i-find-real-world-crewai-examples"&gt;Where can I find real-world CrewAI examples?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-can-i-contribute-to-crewai"&gt;How can I contribute to CrewAI?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-additional-features-does-crewai-amp-offer"&gt;What additional features does CrewAI AMP offer?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-amp-available-for-cloud-and-on-premise-deployments"&gt;Is CrewAI AMP available for cloud and on-premise deployments?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-try-crewai-amp-for-free"&gt;Can I try CrewAI AMP for free?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: What exactly is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is a standalone, lean, and fast Python framework built specifically for orchestrating autonomous AI agents. Unlike frameworks like LangChain, CrewAI does not rely on external dependencies, making it leaner, faster, and simpler.&lt;/p&gt; 
&lt;h3&gt;Q: How do I install CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Install CrewAI using pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For additional tools, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Q: Does CrewAI depend on LangChain?&lt;/h3&gt; 
&lt;p&gt;A: No. CrewAI is built entirely from the ground up, with no dependencies on LangChain or other agent frameworks. This ensures a lean, fast, and flexible experience.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI handle complex use cases?&lt;/h3&gt; 
&lt;p&gt;A: Yes. CrewAI excels at both simple and highly complex real-world scenarios, offering deep customization options at both high and low levels, from internal prompts to sophisticated workflow orchestration.&lt;/p&gt; 
&lt;h3&gt;Q: Can I use CrewAI with local AI models?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI supports various language models, including local ones. Tools like Ollama and LM Studio allow seamless integration. Check the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;LLM Connections documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Q: What makes Crews different from Flows?&lt;/h3&gt; 
&lt;p&gt;A: Crews provide autonomous agent collaboration, ideal for tasks requiring flexible decision-making and dynamic interaction. Flows offer precise, event-driven control, ideal for managing detailed execution paths and secure state management. You can seamlessly combine both for maximum effectiveness.&lt;/p&gt; 
&lt;h3&gt;Q: How is CrewAI better than LangChain?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI provides simpler, more intuitive APIs, faster execution speeds, more reliable and consistent results, robust documentation, and an active communityâ€”addressing common criticisms and limitations associated with LangChain.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI open-source?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is open-source and actively encourages community contributions and collaboration.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI collect data from users?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI collects anonymous telemetry data strictly for improvement purposes. Sensitive data such as prompts, tasks, or API responses are never collected unless explicitly enabled by the user.&lt;/p&gt; 
&lt;h3&gt;Q: Where can I find real-world CrewAI examples?&lt;/h3&gt; 
&lt;p&gt;A: Check out practical examples in the &lt;a href="https://github.com/crewAIInc/crewAI-examples"&gt;CrewAI-examples repository&lt;/a&gt;, covering use cases like trip planners, stock analysis, and job postings.&lt;/p&gt; 
&lt;h3&gt;Q: How can I contribute to CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Contributions are warmly welcomed! Fork the repository, create your branch, implement your changes, and submit a pull request. See the Contribution section of the README for detailed guidelines.&lt;/p&gt; 
&lt;h3&gt;Q: What additional features does CrewAI AMP offer?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI AMP provides advanced features such as a unified control plane, real-time observability, secure integrations, advanced security, actionable insights, and dedicated 24/7 enterprise support.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI AMP available for cloud and on-premise deployments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI AMP supports both cloud-based and on-premise deployment options, allowing enterprises to meet their specific security and compliance requirements.&lt;/p&gt; 
&lt;h3&gt;Q: Can I try CrewAI AMP for free?&lt;/h3&gt; 
&lt;p&gt;A: Yes, you can explore part of the CrewAI AMP Suite by accessing the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane&lt;/a&gt; for free.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI support fine-tuning or training custom models?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI can integrate with custom-trained or fine-tuned models, allowing you to enhance your agents with domain-specific knowledge and accuracy.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI agents interact with external tools and APIs?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI agents can easily integrate with external tools, APIs, and databases, empowering them to leverage real-world data and resources.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI suitable for production environments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is explicitly designed with production-grade standards, ensuring reliability, stability, and scalability for enterprise deployments.&lt;/p&gt; 
&lt;h3&gt;Q: How scalable is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is highly scalable, supporting simple automations and large-scale enterprise workflows involving numerous agents and complex tasks simultaneously.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer debugging and monitoring tools?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI AMP includes advanced debugging, tracing, and real-time observability features, simplifying the management and troubleshooting of your automations.&lt;/p&gt; 
&lt;h3&gt;Q: What programming languages does CrewAI support?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is primarily Python-based but easily integrates with services and APIs written in any programming language through its flexible API integration capabilities.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer educational resources for beginners?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI provides extensive beginner-friendly tutorials, courses, and documentation through learn.crewai.com, supporting developers at all skill levels.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI automate human-in-the-loop workflows?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI fully supports human-in-the-loop workflows, allowing seamless collaboration between human experts and AI agents for enhanced decision-making.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>paperless-ngx/paperless-ngx</title>
      <link>https://github.com/paperless-ngx/paperless-ngx</link>
      <description>&lt;p&gt;A community-supported supercharged document management system: scan, index and archive all your documents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/paperless-ngx/paperless-ngx/actions"&gt;&lt;img src="https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg?sanitize=true" alt="ci" /&gt;&lt;/a&gt; &lt;a href="https://crowdin.com/project/paperless-ngx"&gt;&lt;img src="https://badges.crowdin.net/paperless-ngx/localized.svg?sanitize=true" alt="Crowdin" /&gt;&lt;/a&gt; &lt;a href="https://docs.paperless-ngx.com"&gt;&lt;img src="https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/paperless-ngx/paperless-ngx"&gt;&lt;img src="https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/%23paperlessngx%3Amatrix.org"&gt;&lt;img src="https://matrix.to/img/matrix-badge.svg?sanitize=true" alt="Chat on Matrix" /&gt;&lt;/a&gt; &lt;a href="https://demo.paperless-ngx.com"&gt;&lt;img src="https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg?sanitize=true" alt="demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png" width="50%" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%" /&gt; 
  &lt;img src="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h1&gt;Paperless-ngx&lt;/h1&gt; 
&lt;p&gt;Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, &lt;em&gt;less paper&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Paperless-ngx is the official successor to the original &lt;a href="https://github.com/the-paperless-project/paperless"&gt;Paperless&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jonaswinkler/paperless-ng"&gt;Paperless-ng&lt;/a&gt; projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. &lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support"&gt;Consider joining us!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thanks to the generous folks at &lt;a href="https://m.do.co/c/8d70b916d462"&gt;DigitalOcean&lt;/a&gt;, a demo is available at &lt;a href="https://demo.paperless-ngx.com"&gt;demo.paperless-ngx.com&lt;/a&gt; using login &lt;code&gt;demo&lt;/code&gt; / &lt;code&gt;demo&lt;/code&gt;. &lt;em&gt;Note: demo content is reset frequently and confidential information should not be uploaded.&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#contributing"&gt;Contributing&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support"&gt;Community Support&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#translation"&gt;Translation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#bugs"&gt;Bugs&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#related-projects"&gt;Related Projects&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#important-note"&gt;Important Note&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;This project is supported by:&lt;br /&gt; &lt;a href="https://m.do.co/c/8d70b916d462" style="padding-top: 4px; display: block;"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg" width="140px" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg" width="140px" /&gt; 
   &lt;img src="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg?sanitize=true" width="140px" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;A full list of &lt;a href="https://docs.paperless-ngx.com/#features"&gt;features&lt;/a&gt; and &lt;a href="https://docs.paperless-ngx.com/#screenshots"&gt;screenshots&lt;/a&gt; are available in the &lt;a href="https://docs.paperless-ngx.com/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Getting started&lt;/h1&gt; 
&lt;p&gt;The easiest way to deploy paperless is &lt;code&gt;docker compose&lt;/code&gt;. The files in the &lt;a href="https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose"&gt;&lt;code&gt;/docker/compose&lt;/code&gt; directory&lt;/a&gt; are configured to pull the image from the GitHub container registry.&lt;/p&gt; 
&lt;p&gt;If you'd like to jump right in, you can configure a &lt;code&gt;docker compose&lt;/code&gt; environment with our install script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash -c "$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More details and step-by-step guides for alternative installation methods can be found in &lt;a href="https://docs.paperless-ngx.com/setup/#installation"&gt;the documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Migrating from Paperless-ng is easy, just drop in the new docker image! See the &lt;a href="https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx"&gt;documentation on migrating&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;The documentation for Paperless-ngx is available at &lt;a href="https://docs.paperless-ngx.com/"&gt;https://docs.paperless-ngx.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The &lt;a href="https://docs.paperless-ngx.com/development/"&gt;documentation&lt;/a&gt; has some basic information on how to get started.&lt;/p&gt; 
&lt;h2&gt;Community Support&lt;/h2&gt; 
&lt;p&gt;People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the &lt;a href="https://matrix.to/#/#paperless:matrix.org"&gt;Matrix Room&lt;/a&gt;. If you would like to contribute to the project on an ongoing basis there are multiple &lt;a href="https://github.com/orgs/paperless-ngx/people"&gt;teams&lt;/a&gt; (frontend, ci/cd, etc) that could use your help so please reach out!&lt;/p&gt; 
&lt;h2&gt;Translation&lt;/h2&gt; 
&lt;p&gt;Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to &lt;a href="https://crowdin.com/project/paperless-ngx"&gt;https://crowdin.com/project/paperless-ngx&lt;/a&gt;, and thank you! More details can be found in &lt;a href="https://github.com/paperless-ngx/paperless-ngx/raw/main/CONTRIBUTING.md#translating-paperless-ngx"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;Feature requests can be submitted via &lt;a href="https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests"&gt;GitHub Discussions&lt;/a&gt;, you can search for existing ideas, add your own and vote for the ones you care about.&lt;/p&gt; 
&lt;h2&gt;Bugs&lt;/h2&gt; 
&lt;p&gt;For bugs please &lt;a href="https://github.com/paperless-ngx/paperless-ngx/issues"&gt;open an issue&lt;/a&gt; or &lt;a href="https://github.com/paperless-ngx/paperless-ngx/discussions"&gt;start a discussion&lt;/a&gt; if you have questions.&lt;/p&gt; 
&lt;h1&gt;Related Projects&lt;/h1&gt; 
&lt;p&gt;Please see &lt;a href="https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects"&gt;the wiki&lt;/a&gt; for a user-maintained list of related projects and software that is compatible with Paperless-ngx.&lt;/p&gt; 
&lt;h1&gt;Important Note&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. &lt;strong&gt;Paperless-ngx should never be run on an untrusted host&lt;/strong&gt; because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk. &lt;strong&gt;The safest way to run Paperless-ngx is on a local server in your own home with backups in place&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>guofei9987/blind_watermark</title>
      <link>https://github.com/guofei9987/blind_watermark</link>
      <description>&lt;p&gt;Blind&amp;Invisible Watermark ï¼Œå›¾ç‰‡ç›²æ°´å°ï¼Œæå–æ°´å°æ— é¡»åŸå›¾ï¼&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;blind-watermark&lt;/h1&gt; 
&lt;p&gt;Blind watermark based on DWT-DCT-SVD.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/blind_watermark/"&gt;&lt;img src="https://img.shields.io/pypi/v/blind_watermark" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://travis-ci.com/guofei9987/blind_watermark"&gt;&lt;img src="https://travis-ci.com/guofei9987/blind_watermark.svg?branch=master" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/guofei9987/blind_watermark"&gt;&lt;img src="https://codecov.io/gh/guofei9987/blind_watermark/branch/master/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/blind_watermark.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-%3E=3.5-green.svg?sanitize=true" alt="Python" /&gt; &lt;img src="https://img.shields.io/badge/platform-windows%20%7C%20linux%20%7C%20macos-green.svg?sanitize=true" alt="Platform" /&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/"&gt;&lt;img src="https://img.shields.io/github/stars/guofei9987/blind_watermark.svg?style=social" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/fork"&gt;&lt;img src="https://img.shields.io/github/forks/guofei9987/blind_watermark?style=social" alt="fork" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/blind-watermark"&gt;&lt;img src="https://pepy.tech/badge/blind-watermark" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/discussions"&gt;&lt;img src="https://img.shields.io/badge/discussions-green.svg?sanitize=true" alt="Discussions" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/repository/guofei9987/blind_watermark" target="_blank"&gt;&lt;img src="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=3834302ff46a40f188a651ef8bd26ff5&amp;amp;claim_uid=se0WHo8cbiLv2w1&amp;amp;theme=small" alt="Featuredï½œHelloGitHub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://BlindWatermark.github.io/blind_watermark/#/en/"&gt;https://BlindWatermark.github.io/blind_watermark/#/en/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ–‡æ¡£ï¼š&lt;/strong&gt; &lt;a href="https://BlindWatermark.github.io/blind_watermark/#/zh/"&gt;https://BlindWatermark.github.io/blind_watermark/#/zh/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¸­æ–‡ readme&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/README_cn.md"&gt;README_cn.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source code:&lt;/strong&gt; &lt;a href="https://github.com/guofei9987/blind_watermark"&gt;https://github.com/guofei9987/blind_watermark&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;install&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install blind-watermark
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the current developer version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bach"&gt;git clone git@github.com:guofei9987/blind_watermark.git
cd blind_watermark
pip install .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;How to use&lt;/h1&gt; 
&lt;h2&gt;Use in bash&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# embed watermark into image:
blind_watermark --embed --pwd 1234 examples/pic/ori_img.jpeg "watermark text" examples/output/embedded.png
# extract watermark from image:
blind_watermark --extract --pwd 1234 --wm_shape 111 examples/output/embedded.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use in Python&lt;/h2&gt; 
&lt;p&gt;Original Image + Watermark = Watermarked Image&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%8E%9F%E5%9B%BE.jpeg" alt="origin_image" /&gt; + '@guofei9987 å¼€æºä¸‡å²ï¼' = &lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%89%93%E4%B8%8A%E6%B0%B4%E5%8D%B0%E7%9A%84%E5%9B%BE.jpg" alt="æ‰“ä¸Šæ°´å°çš„å›¾" /&gt;&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/examples/example_str.py"&gt;codes&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Embed watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_img('pic/ori_img.jpg')
wm = '@guofei9987 å¼€æºä¸‡å²ï¼'
bwm1.read_wm(wm, mode='str')
bwm1.embed('output/embedded.png')
len_wm = len(bwm1.wm_bit)
print('Put down the length of wm_bit {len_wm}'.format(len_wm=len_wm))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extract watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;bwm1 = WaterMark(password_img=1, password_wm=1)
wm_extract = bwm1.extract('output/embedded.png', wm_shape=len_wm, mode='str')
print(wm_extract)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Output:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;@guofei9987 å¼€æºä¸‡å²ï¼&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;attacks on Watermarked Image&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;attack method&lt;/th&gt; 
   &lt;th&gt;image after attack&lt;/th&gt; 
   &lt;th&gt;extracted watermark&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rotate 45 Degrees&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%97%8B%E8%BD%AC%E6%94%BB%E5%87%BB.jpg" alt="æ—‹è½¬æ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 å¼€æºä¸‡å²ï¼'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Random crop&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%88%AA%E5%B1%8F%E6%94%BB%E5%87%BB2_%E8%BF%98%E5%8E%9F.jpg" alt="æˆªå±æ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 å¼€æºä¸‡å²ï¼'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Masks&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB.jpg" alt="å¤šé®æŒ¡æ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 å¼€æºä¸‡å²ï¼'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertical cut&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%A8%AA%E5%90%91%E8%A3%81%E5%89%AA%E6%94%BB%E5%87%BB_%E5%A1%AB%E8%A1%A5.jpg" alt="æ¨ªå‘è£å‰ªæ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 å¼€æºä¸‡å²ï¼'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Horizontal cut&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E7%BA%B5%E5%90%91%E8%A3%81%E5%89%AA%E6%94%BB%E5%87%BB_%E5%A1%AB%E8%A1%A5.jpg" alt="çºµå‘è£å‰ªæ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 å¼€æºä¸‡å²ï¼'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Resize&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E7%BC%A9%E6%94%BE%E6%94%BB%E5%87%BB.jpg" alt="ç¼©æ”¾æ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 å¼€æºä¸‡å²ï¼'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pepper Noise&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%A4%92%E7%9B%90%E6%94%BB%E5%87%BB.jpg" alt="æ¤’ç›æ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 å¼€æºä¸‡å²ï¼'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Brightness 10% Down&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E4%BA%AE%E5%BA%A6%E6%94%BB%E5%87%BB.jpg" alt="äº®åº¦æ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 å¼€æºä¸‡å²ï¼'&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;embed images&lt;/h3&gt; 
&lt;p&gt;embed watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from blind_watermark import WaterMark

bwm1 = WaterMark(password_wm=1, password_img=1)
# read original image
bwm1.read_img('pic/ori_img.jpg')
# read watermark
bwm1.read_wm('pic/watermark.png')
# embed
bwm1.embed('output/embedded.png')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extract watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;bwm1 = WaterMark(password_wm=1, password_img=1)
# notice that wm_shape is necessary
bwm1.extract(filename='output/embedded.png', wm_shape=(128, 128), out_wm_name='output/extracted.png', )
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;attack method&lt;/th&gt; 
   &lt;th&gt;image after attack&lt;/th&gt; 
   &lt;th&gt;extracted watermark&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rotate 45 Degrees&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%97%8B%E8%BD%AC%E6%94%BB%E5%87%BB.jpg" alt="æ—‹è½¬æ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%97%8B%E8%BD%AC%E6%94%BB%E5%87%BB_%E6%8F%90%E5%8F%96%E6%B0%B4%E5%8D%B0.png" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Random crop&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%88%AA%E5%B1%8F%E6%94%BB%E5%87%BB2_%E8%BF%98%E5%8E%9F.jpg" alt="æˆªå±æ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB_%E6%8F%90%E5%8F%96%E6%B0%B4%E5%8D%B0.png" alt="å¤šé®æŒ¡_æå–æ°´å°" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mask&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB.jpg" alt="å¤šé®æŒ¡æ”»å‡»" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB_%E6%8F%90%E5%8F%96%E6%B0%B4%E5%8D%B0.png" alt="å¤šé®æŒ¡_æå–æ°´å°" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;embed array of bits&lt;/h3&gt; 
&lt;p&gt;See it &lt;a href="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/examples/example_bit.py"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;As demo, we embed 6 bytes data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;wm = [True, False, True, True, True, False]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Embed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_ori_img('pic/ori_img.jpg')
bwm1.read_wm([True, False, True, True, True, False], mode='bit')
bwm1.embed('output/embedded.png')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extract:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;bwm1 = WaterMark(password_img=1, password_wm=1, wm_shape=6)
wm_extract = bwm1.extract('output/æ‰“ä¸Šæ°´å°çš„å›¾.png', mode='bit')
print(wm_extract)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notice that &lt;code&gt;wm_shape&lt;/code&gt; (shape of watermark) is necessary&lt;/p&gt; 
&lt;p&gt;The output &lt;code&gt;wm_extract&lt;/code&gt; is an array of float. set a threshold such as 0.5.&lt;/p&gt; 
&lt;h1&gt;Concurrency&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;WaterMark(..., processes=None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;processes&lt;/code&gt; number of processes, can be integer. Default &lt;code&gt;None&lt;/code&gt;, which means using all processes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related Project&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;text_blind_watermark (Embed message into text): &lt;a href="https://github.com/guofei9987/text_blind_watermark"&gt;https://github.com/guofei9987/text_blind_watermark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;HideInfoï¼ˆhide as image, hide as sounds, hide as textï¼‰ï¼š&lt;a href="https://github.com/guofei9987/HideInfo"&gt;https://github.com/guofei9987/HideInfo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>deepseek-ai/DeepSeek-V3</title>
      <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://www.deepseek.com/"&gt;&lt;img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true" /&gt;&lt;/a&gt; 
 &lt;a href="https://chat.deepseek.com/"&gt;&lt;img alt="Chat" src="https://img.shields.io/badge/ğŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://discord.gg/Tc7c45Zzu5"&gt;&lt;img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/qr.jpeg?raw=true"&gt;&lt;img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://twitter.com/deepseek_ai"&gt;&lt;img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-CODE"&gt;&lt;img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-MODEL"&gt;&lt;img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://arxiv.org/pdf/2412.19437"&gt;&lt;b&gt;Paper Link&lt;/b&gt;ğŸ‘ï¸&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#1-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#2-model-summary"&gt;Model Summary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#3-model-downloads"&gt;Model Downloads&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#4-evaluation-results"&gt;Evaluation Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#5-chat-website--api-platform"&gt;Chat Website &amp;amp; API Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How to Run Locally&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#7-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#8-citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#9-contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. Introduction&lt;/h2&gt; 
&lt;p&gt;We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;2. Model Summary&lt;/h2&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Architecture: Innovative Load Balancing Strategy and Training Objective&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.&lt;/li&gt; 
 &lt;li&gt;We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Pre-Training: Towards Ultimate Training Efficiency&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.&lt;/li&gt; 
 &lt;li&gt;Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.&lt;br /&gt; This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.&lt;/li&gt; 
 &lt;li&gt;At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Post-Training: Knowledge Distillation from DeepSeek-R1&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;3. Model Downloads&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Total Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Activated Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3-Base&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base"&gt;ğŸ¤— Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3"&gt;ğŸ¤— Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How_to Run_Locally&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For developers looking to dive deeper, we recommend exploring &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/README_WEIGHTS.md"&gt;README_WEIGHTS.md&lt;/a&gt; for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.&lt;/p&gt; 
&lt;h2&gt;4. Evaluation Results&lt;/h2&gt; 
&lt;h3&gt;Base Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;Benchmark (Metric)&lt;/th&gt; 
    &lt;th&gt;# Shots&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V2&lt;/th&gt; 
    &lt;th&gt;Qwen2.5 72B&lt;/th&gt; 
    &lt;th&gt;LLaMA3.1 405B&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V3&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;Pile-test (BPB)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;0.606&lt;/td&gt; 
    &lt;td&gt;0.638&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;0.542&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;0.548&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;BBH (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.8&lt;/td&gt; 
    &lt;td&gt;79.8&lt;/td&gt; 
    &lt;td&gt;82.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;78.4&lt;/td&gt; 
    &lt;td&gt;85.0&lt;/td&gt; 
    &lt;td&gt;84.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;75.6&lt;/td&gt; 
    &lt;td&gt;83.2&lt;/td&gt; 
    &lt;td&gt;81.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;51.4&lt;/td&gt; 
    &lt;td&gt;58.3&lt;/td&gt; 
    &lt;td&gt;52.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (F1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Easy (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;97.6&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;98.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Challenge (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;92.2&lt;/td&gt; 
    &lt;td&gt;94.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;HellaSwag (Acc.)&lt;/td&gt; 
    &lt;td&gt;10-shot&lt;/td&gt; 
    &lt;td&gt;87.1&lt;/td&gt; 
    &lt;td&gt;84.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;PIQA (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;83.9&lt;/td&gt; 
    &lt;td&gt;82.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;WinoGrande (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.3&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;84.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-Middle (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;73.1&lt;/td&gt; 
    &lt;td&gt;68.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;74.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;67.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-High (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;52.6&lt;/td&gt; 
    &lt;td&gt;50.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;56.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;TriviaQA (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;71.9&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;NaturalQuestions (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;38.6&lt;/td&gt; 
    &lt;td&gt;33.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;41.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;40.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;AGIEval (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;60.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval (Pass@1)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;43.3&lt;/td&gt; 
    &lt;td&gt;53.0&lt;/td&gt; 
    &lt;td&gt;54.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MBPP (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;65.0&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;68.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;75.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench-Base (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;11.6&lt;/td&gt; 
    &lt;td&gt;12.9&lt;/td&gt; 
    &lt;td&gt;15.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;19.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-I (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-O (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;49.8&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;69.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;GSM8K (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;81.6&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH (EM)&lt;/td&gt; 
    &lt;td&gt;4-shot&lt;/td&gt; 
    &lt;td&gt;43.4&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MGSM (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;63.6&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;69.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMath (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.7&lt;/td&gt; 
    &lt;td&gt;84.5&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;82.0&lt;/td&gt; 
    &lt;td&gt;82.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;83.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;81.4&lt;/td&gt; 
    &lt;td&gt;89.2&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;84.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.7&lt;/td&gt; 
    &lt;td&gt;88.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMRC (EM)&lt;/td&gt; 
    &lt;td&gt;1-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;77.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;76.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C3 (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.7&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CCPM (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;93.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.5&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;92.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Multilingual&lt;/td&gt; 
    &lt;td&gt;MMMLU-non-English (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;64.0&lt;/td&gt; 
    &lt;td&gt;74.8&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks. For more evaluation details, please check our paper.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Context Window&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/niah.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Evaluation results on the &lt;code&gt;Needle In A Haystack&lt;/code&gt; (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to &lt;strong&gt;128K&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;Chat Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks (Models larger than 67B)&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Benchmark (Metric)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2-0506&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2.5-0905&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Qwen2.5 72B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Llama3.1 405B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Claude-3.5-Sonnet-1022&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;GPT-4o 0513&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V3&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;MMLU (EM)&lt;/td&gt; 
    &lt;td&gt;78.2&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;85.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;87.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (EM)&lt;/td&gt; 
    &lt;td&gt;77.9&lt;/td&gt; 
    &lt;td&gt;80.3&lt;/td&gt; 
    &lt;td&gt;85.6&lt;/td&gt; 
    &lt;td&gt;86.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (EM)&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;66.2&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;78.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;75.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (3-shot F1)&lt;/td&gt; 
    &lt;td&gt;83.0&lt;/td&gt; 
    &lt;td&gt;87.8&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;88.7&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;IF-Eval (Prompt Strict)&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;84.1&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.3&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;GPQA-Diamond (Pass@1)&lt;/td&gt; 
    &lt;td&gt;35.3&lt;/td&gt; 
    &lt;td&gt;41.3&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;49.9&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;9.0&lt;/td&gt; 
    &lt;td&gt;10.2&lt;/td&gt; 
    &lt;td&gt;9.1&lt;/td&gt; 
    &lt;td&gt;17.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;38.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;24.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;FRAMES (Acc.)&lt;/td&gt; 
    &lt;td&gt;66.9&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;70.0&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;80.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LongBench v2 (Acc.)&lt;/td&gt; 
    &lt;td&gt;31.6&lt;/td&gt; 
    &lt;td&gt;35.4&lt;/td&gt; 
    &lt;td&gt;39.4&lt;/td&gt; 
    &lt;td&gt;36.1&lt;/td&gt; 
    &lt;td&gt;41.0&lt;/td&gt; 
    &lt;td&gt;48.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;48.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval-Mul (Pass@1)&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;77.2&lt;/td&gt; 
    &lt;td&gt;81.7&lt;/td&gt; 
    &lt;td&gt;80.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1-COT)&lt;/td&gt; 
    &lt;td&gt;18.8&lt;/td&gt; 
    &lt;td&gt;29.2&lt;/td&gt; 
    &lt;td&gt;31.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;36.3&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;40.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1)&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;28.7&lt;/td&gt; 
    &lt;td&gt;30.1&lt;/td&gt; 
    &lt;td&gt;32.8&lt;/td&gt; 
    &lt;td&gt;34.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;37.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Codeforces (Percentile)&lt;/td&gt; 
    &lt;td&gt;17.5&lt;/td&gt; 
    &lt;td&gt;35.6&lt;/td&gt; 
    &lt;td&gt;24.8&lt;/td&gt; 
    &lt;td&gt;25.3&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;23.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SWE Verified (Resolved)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;22.6&lt;/td&gt; 
    &lt;td&gt;23.8&lt;/td&gt; 
    &lt;td&gt;24.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;50.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;38.8&lt;/td&gt; 
    &lt;td&gt;42.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Edit (Acc.)&lt;/td&gt; 
    &lt;td&gt;60.3&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;63.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;84.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.9&lt;/td&gt; 
    &lt;td&gt;79.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Polyglot (Acc.)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;18.2&lt;/td&gt; 
    &lt;td&gt;7.6&lt;/td&gt; 
    &lt;td&gt;5.8&lt;/td&gt; 
    &lt;td&gt;45.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;49.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;AIME 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;4.6&lt;/td&gt; 
    &lt;td&gt;16.7&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;9.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;39.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH-500 (EM)&lt;/td&gt; 
    &lt;td&gt;56.3&lt;/td&gt; 
    &lt;td&gt;74.7&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;78.3&lt;/td&gt; 
    &lt;td&gt;74.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CNMO 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;2.8&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;15.9&lt;/td&gt; 
    &lt;td&gt;6.8&lt;/td&gt; 
    &lt;td&gt;13.1&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;43.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;89.9&lt;/td&gt; 
    &lt;td&gt;90.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
    &lt;td&gt;85.4&lt;/td&gt; 
    &lt;td&gt;87.9&lt;/td&gt; 
    &lt;td&gt;90.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (EM)&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;48.5&lt;/td&gt; 
    &lt;td&gt;54.1&lt;/td&gt; 
    &lt;td&gt;48.4&lt;/td&gt; 
    &lt;td&gt;50.4&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
    &lt;td&gt;59.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Open Ended Generation Evaluation&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Arena-Hard&lt;/th&gt; 
    &lt;th&gt;AlpacaEval 2.0&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V2.5-0905&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;50.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Qwen2.5-72B-Instruct&lt;/td&gt; 
    &lt;td&gt;81.2&lt;/td&gt; 
    &lt;td&gt;49.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LLaMA-3.1 405B&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;40.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GPT-4o-0513&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Claude-Sonnet-3.5-1022&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;52.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;5. Chat Website &amp;amp; API Platform&lt;/h2&gt; 
&lt;p&gt;You can chat with DeepSeek-V3 on DeepSeek's official website: &lt;a href="https://chat.deepseek.com/sign_in"&gt;chat.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We also provide OpenAI-Compatible API at DeepSeek Platform: &lt;a href="https://platform.deepseek.com/"&gt;platform.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. How to Run Locally&lt;/h2&gt; 
&lt;p&gt;DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeek-Infer Demo&lt;/strong&gt;: We provide a simple and lightweight demo for FP8 and BF16 inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SGLang&lt;/strong&gt;: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;coming soon&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LMDeploy&lt;/strong&gt;: Enables efficient FP8 and BF16 inference for local and cloud deployment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt;: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LightLLM&lt;/strong&gt;: Supports efficient single-node or multi-node deployment for FP8 and BF16.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AMD GPU&lt;/strong&gt;: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Huawei Ascend NPU&lt;/strong&gt;: Supports running DeepSeek-V3 on Huawei Ascend devices in both INT8 and BF16.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.&lt;/p&gt; 
&lt;p&gt;Here is an example of converting FP8 weights to BF16:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Hugging Face's Transformers has not been directly supported yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;6.1 Inference with DeepSeek-Infer Demo (example only)&lt;/h3&gt; 
&lt;h4&gt;System Requirements&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Linux with Python 3.10 only. Mac and Windows are not supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-pip-requirements"&gt;torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Weights &amp;amp; Demo Code Preparation&lt;/h4&gt; 
&lt;p&gt;First, clone our DeepSeek-V3 GitHub repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/deepseek-ai/DeepSeek-V3.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Navigate to the &lt;code&gt;inference&lt;/code&gt; folder and install dependencies listed in &lt;code&gt;requirements.txt&lt;/code&gt;. Easiest way is to use a package manager like &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;uv&lt;/code&gt; to create a new virtual environment and install the dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd DeepSeek-V3/inference
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download the model weights from Hugging Face, and put them into &lt;code&gt;/path/to/DeepSeek-V3&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h4&gt;Model Weights Conversion&lt;/h4&gt; 
&lt;p&gt;Convert Hugging Face model weights to a specific format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run&lt;/h4&gt; 
&lt;p&gt;Then you can chat with DeepSeek-V3:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or batch inference on a given file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.2 Inference with SGLang (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; currently supports &lt;a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations"&gt;MLA optimizations&lt;/a&gt;, &lt;a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models"&gt;DP Attention&lt;/a&gt;, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.&lt;/p&gt; 
&lt;p&gt;Notably, &lt;a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1"&gt;SGLang v0.4.1&lt;/a&gt; fully supports running DeepSeek-V3 on both &lt;strong&gt;NVIDIA and AMD GPUs&lt;/strong&gt;, making it a highly versatile and robust solution.&lt;/p&gt; 
&lt;p&gt;SGLang also supports &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208"&gt;multi-node tensor parallelism&lt;/a&gt;, enabling you to run this model on multiple network-connected machines.&lt;/p&gt; 
&lt;p&gt;Multi-Token Prediction (MTP) is in development, and progress can be tracked in the &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;optimization plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Here are the launch instructions from the SGLang team: &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3"&gt;https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.3 Inference with LMDeploy (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/InternLM/lmdeploy"&gt;LMDeploy&lt;/a&gt;, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.&lt;/p&gt; 
&lt;p&gt;For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: &lt;a href="https://github.com/InternLM/lmdeploy/issues/2960"&gt;https://github.com/InternLM/lmdeploy/issues/2960&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.4 Inference with TRT-LLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt; now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3"&gt;https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.5 Inference with vLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers &lt;em&gt;pipeline parallelism&lt;/em&gt; allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the &lt;a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html"&gt;vLLM instructions&lt;/a&gt;. Please feel free to follow &lt;a href="https://github.com/vllm-project/vllm/issues/11539"&gt;the enhancement plan&lt;/a&gt; as well.&lt;/p&gt; 
&lt;h3&gt;6.6 Inference with LightLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ModelTC/lightllm/tree/main"&gt;LightLLM&lt;/a&gt; v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to &lt;a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html"&gt;LightLLM instructions&lt;/a&gt;. Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.&lt;/p&gt; 
&lt;h3&gt;6.7 Recommended Inference Functionality with AMD GPUs&lt;/h3&gt; 
&lt;p&gt;In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#63-inference-with-lmdeploy-recommended"&gt;SGLang instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.8 Recommended Inference Functionality with Huawei Ascend NPUs&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://www.hiascend.com/en/software/mindie"&gt;MindIE&lt;/a&gt; framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the &lt;a href="https://modelers.cn/models/MindIE/deepseekv3"&gt;instructions here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;7. License&lt;/h2&gt; 
&lt;p&gt;This code repository is licensed under &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-CODE"&gt;the MIT License&lt;/a&gt;. The use of DeepSeek-V3 Base/Chat models is subject to &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-MODEL"&gt;the Model License&lt;/a&gt;. DeepSeek-V3 series (including Base and Chat) supports commercial use.&lt;/p&gt; 
&lt;h2&gt;8. Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;9. Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/service@deepseek.com"&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rossant/awesome-math</title>
      <link>https://github.com/rossant/awesome-math</link>
      <description>&lt;p&gt;A curated list of awesome mathematics resources&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Math &lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;A curated list of awesome mathematics resources.&lt;/p&gt; 
&lt;p&gt;All resources are freely available except those with a ğŸ’² icon.&lt;/p&gt; 
&lt;h1&gt;Contents&lt;/h1&gt; 
&lt;!-- START_TOC --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#contents"&gt;Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#general-resources"&gt;General Resources&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#learning-platforms"&gt;Learning Platforms&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#learn-to-learn"&gt;Learn to Learn&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#youtube-series"&gt;Youtube Series&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#questions-and-answers"&gt;Questions and Answers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#encyclopedia"&gt;Encyclopedia&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#books"&gt;Books&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#magazines"&gt;Magazines&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#blogs"&gt;Blogs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#meetings-and-conferences"&gt;Meetings and Conferences&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#misc"&gt;Misc&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#branches-of-mathematics"&gt;Branches of Mathematics&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#foundations-of-mathematics"&gt;Foundations of Mathematics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#transition-to-pure-rigour-math"&gt;Transition To Pure Rigour Math&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#set-theory"&gt;Set Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#logic"&gt;Logic&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#category-theory"&gt;Category Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#type-theory"&gt;Type Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#homotopy-type-theory"&gt;Homotopy Type Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#surreal-numbers"&gt;Surreal Numbers&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#number-theory"&gt;Number Theory&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-number-theory"&gt;Algebraic Number Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#analytic-number-theory"&gt;Analytic Number Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebra"&gt;Algebra&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#abstract-algebra"&gt;Abstract Algebra&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#group-theory"&gt;Group Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#linear-algebra"&gt;Linear Algebra&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#ring-theory"&gt;Ring Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#galois-theory"&gt;Galois Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#lie-algebras"&gt;Lie Algebras&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#combinatorics"&gt;Combinatorics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#graph-theory"&gt;Graph Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#geometry-and-topology"&gt;Geometry and Topology&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#differential-geometry"&gt;Differential Geometry&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-geometry"&gt;Algebraic Geometry&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-statistics"&gt;Algebraic Statistics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#topology"&gt;Topology&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-topology"&gt;Algebraic Topology&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#analysis"&gt;Analysis&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#real-analysis"&gt;Real Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#harmonic-analysis"&gt;Harmonic Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#complex-analysis"&gt;Complex Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#functional-analysis"&gt;Functional Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#measure-theory"&gt;Measure Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#ordinary-differential-equations"&gt;Ordinary Differential Equations&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#partial-differential-equations"&gt;Partial Differential Equations&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#chaos-theory"&gt;Chaos Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#probability-and-statistics"&gt;Probability and Statistics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#probability-theory"&gt;Probability Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#statistics"&gt;Statistics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#statistical-learning"&gt;Statistical Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#stochastic-processes"&gt;Stochastic processes&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#numerical-analysis"&gt;Numerical Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#signal-processing"&gt;Signal processing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematics-for-computer-science"&gt;Mathematics for Computer Science&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematical-biology"&gt;Mathematical Biology&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematical-physics"&gt;Mathematical Physics&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#students-lecture-notes"&gt;Students Lecture Notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#related-awesome-lists"&gt;Related Awesome Lists&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END_TOC --&gt; 
&lt;h1&gt;General Resources&lt;/h1&gt; 
&lt;h2&gt;Learning Platforms&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.khanacademy.org/math"&gt;Khan Academy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.coursera.org/courses?query=mathematics&amp;amp;languages=en"&gt;Coursera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://ocw.mit.edu/courses/mathematics/"&gt;MIT OpenCourseWare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.edx.org/course/subject/math"&gt;edX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://brilliant.org/courses/#math-foundational"&gt;Brilliant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://misterwootube.com/"&gt;WooTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathigon.org/"&gt;Mathigon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://calculus.org/"&gt;Calculus.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ximera.osu.edu/"&gt;Ximera&lt;/a&gt; : free interactive mathematics textbooks (Ohio State University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.almostfun.org/lessons/"&gt;Almost Fun&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/c/OxfordMathematics"&gt;Oxford Mathematics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathacademy.com/"&gt;Math Academy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn to Learn&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nelson-brochado/understanding-math"&gt;Understanding Mathematics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Youtube Series&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@BrandonFoltz"&gt;Brandon Foltz&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw"&gt;StatQuest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@3blue1brown"&gt;3Blue1Brown&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@iit"&gt;NPTEL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@patrickjmt"&gt;PatrickJMT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@ProfessorLeonard"&gt;Professor Leonard&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESsmwELdrzhcGiRhk5DjwLP"&gt;Precalculus - College Algebra/Trigonometry&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLF797E961509B4EB5"&gt;Calculus 1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6EQ2J4vgsN1HyBeRADEh4Cw-"&gt;Calculus 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESk16YRmzuJ8f6-rnuy0Ry7"&gt;Calculus 3&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESPaHy2QUKVaXNZuQNxkYQ_"&gt;Differential Equations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ETc1ZwHWijCBcZ2gOvS2tTN"&gt;To The Point Math&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@crashcourse"&gt;Crash Course&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@harvard"&gt;Harvard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@mitocw"&gt;MIT OpenCourseWare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@Mathologer"&gt;Mathologer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@TheMathDistrict"&gt;The Math District&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@mathematicalmonk"&gt;Mathematical Monk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@TheMathSorcerer"&gt;The Math Sorcerer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.symbolab.com/"&gt;Symbolab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.desmos.com/calculator"&gt;Desmos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.mathwords.com/"&gt;Math Words&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.wolframalpha.com/"&gt;Wolfram Alpha&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://maxima.sourceforge.io/"&gt;Maxima&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sympy.org/"&gt;Sympy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.sagemath.org/"&gt;Sagemath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Nonanti/MathFlow"&gt;MathFlow&lt;/a&gt; - C# math expression library with symbolic computation (differentiation, simplification, equation solving)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://unitconverters.net"&gt;Unit Converter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.geogebra.org/?lang=en"&gt;GeoGebra&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www2.macaulay2.com/Macaulay2/"&gt;Macaulay2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.singular.uni-kl.de/"&gt;Singular&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/octave/"&gt;GNU Octave&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://magma.maths.usyd.edu.au/magma/"&gt;Magma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.maplesoft.com/products/Maple/"&gt;Maple&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.mathworks.com/products/matlab.html"&gt;Matlab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.wolfram.com/mathematica/"&gt;Wolfram Mathematica&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://freemathapp.org"&gt;Free Math&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://chrome.google.com/webstore/detail/xhub/anidddebgkllnnnnjfkmjcaallemhjee"&gt;xhub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.copypastemathjax.com/"&gt;CopyPasteMathjax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.financecharts.com/pages/5724-retirement-calculators-and-stock-market-tips"&gt;Finance calculators&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathcheap.xyz"&gt;Mathcheap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://midpointcalculator.co"&gt;Midpoint Calculator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://quartilecalculator.net"&gt;Quartiles Calculator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://corca.io/"&gt;Corca Editor&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Questions and Answers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://math.stackexchange.com/"&gt;Mathematics Stack Exchange&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mathoverflow.net/"&gt;MathOverflow&lt;/a&gt; - for professional mathematicians&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Encyclopedia&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.encyclopediaofmath.org"&gt;Encyclopedia of Mathematics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://planetmath.org/"&gt;Planetmath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://proofwiki.org/wiki/Main_Page"&gt;ProofWiki&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mathworld.wolfram.com/"&gt;Wolfram Mathworld&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://oeis.org"&gt;The On-Line Encyclopedia of Integer Sequences&lt;/a&gt; - Great compendium of many different integer sequences. Founded 1964 by N. J. A. Sloane.&lt;/li&gt; 
 &lt;li&gt;ğŸ’² &lt;a href="https://press.princeton.edu/books/hardcover/9780691118802/the-princeton-companion-to-mathematics"&gt;The Princeton Companion to Mathematics&lt;/a&gt; - Timothy Gowers (Professor, Fields medallist), June Barrow-Green (Professor), and Imre Leader (Professor).&lt;/li&gt; 
 &lt;li&gt;ğŸ’² &lt;a href="https://link.springer.com/book/10.1007/978-3-662-52844-0"&gt;Encyclopedia of Distances (4th Edition)&lt;/a&gt; - Michel Marie Deza, Elena Deza.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Books&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://archive.org/details/TarasovCalculus"&gt;Calculus: Basic Concepts for High Schools&lt;/a&gt; - L.V. Tarasov&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.cis.upenn.edu/~jean/math-basics.pdf"&gt;Basics of Algebra, Topology, and Differential Calculus&lt;/a&gt; - Jean Gallier (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://people.math.gatech.edu/%7Ecain/notes/calculus.html"&gt;Multivariable Calculus&lt;/a&gt; - G. Cain, J. Herod (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://en.wikibooks.org/wiki/Wikibooks:Mathematics_bookshelf"&gt;Wikibooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://people.math.gatech.edu/~cain/textbooks/onlinebooks.html"&gt;Online Mathematics Textbooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.wallace.ccfaculty.org/book/Beginning_and_Intermediate_Algebra.pdf"&gt;Beginning and Intermediate Algebra&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/raw/master/books/free-programming-books-subjects.md#mathematics"&gt;Free Mathematics Books&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.mecmath.net/trig/trigbook.pdf"&gt;Trigonometry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.manning.com/books/math-for-frontend-web-dev"&gt;Math for Frontend Web Dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.manning.com/books/grokking-statistics"&gt;Grokking Statistics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Magazines&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.quantamagazine.org/mathematics/"&gt;Quanta Magazine&lt;/a&gt; - Features latest research breakthroughs in an accessible style for non-experts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.ams.org/journals/bull/all_issues.html"&gt;Bulletin of the American Mathematical Society&lt;/a&gt; - Expository articles on contemporary mathematical research, written in a way that gives insight to mathematicians who may not be experts in the particular topic.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.ams.org/cgi-bin/notices/amsnotices.pl?article_id=fullissue&amp;amp;article_type=gallery&amp;amp;gallery_type=fullissue"&gt;Notices of the American Mathematical Society&lt;/a&gt; - Publicizes activities of the Society and features surveys, reports, news, announcements, and opinions on industry trends, academia, and research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://euromathsoc.org/magazine"&gt;European Mathematical Society Magazine&lt;/a&gt; - The Magazine features announcements about meetings and conferences, articles outlining current trends in scientific development, reports on member societies, and many other informational items.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ima.org.uk/publications/mathematics-today/"&gt;Mathematics Today by Institute of Mathematics and its Applications&lt;/a&gt; - News, opinions, and articles related to mathematics, so the reader stays updated.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cms.math.ca/publications/crux/"&gt;Crux Mathematicorum by Canadian Mathematical Society&lt;/a&gt; - source of unique and challenging mathematical problems designed for the secondary and undergraduate levels. It includes an Olympiad Corner which is helpful for math competitions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://betterexplained.com/"&gt;BetterExplained&lt;/a&gt; - Maintained by Kalid Azad&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://ilovemaths.com/"&gt;ILoveMaths&lt;/a&gt; - For grades 6 thru 12 in K-12 system&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.3blue1brown.com/"&gt;3blue1brown&lt;/a&gt; - Animated Maths&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.mathsisfun.com"&gt;Mathsisfun&lt;/a&gt; simple text lightweight site for students up to highschool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://calculus123.com/wiki/Peter_Saveliev"&gt;MathematicsIsAScience&lt;/a&gt; - Peter Saveliev (Professor of mathematics at Marshall University, Huntington WV, USA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Meetings and Conferences&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mathsjam.com/"&gt;MathsJam&lt;/a&gt; - monthly local recreational maths/puzzle meetups and an annual gathering in Staffordshire, England&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://talkingmathsinpublic.uk/"&gt;Talking Maths in Public&lt;/a&gt; - a conference for maths communicators, running every two years, usually in the UK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.bridgesmathart.org/"&gt;Bridges&lt;/a&gt; - an annual conference on mathematical connections in art, music, architecture, and culture. The 2025 meeting is in Eindhoven, Netherlands.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Areas_of_mathematics"&gt;Areas of mathematics on Wikipedia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://tutorial.math.lamar.edu/"&gt;Paul's Online Math Notes&lt;/a&gt; - Paul Dawkins (Lamar University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://faculty.atu.edu/mfinan/nnotes.html"&gt;List of electronic textbooks&lt;/a&gt; - Marcel B. Finan (Arkansas Tech University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://at.yorku.ca/topology/"&gt;Topology Atlas&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://djm.cc/library/Recreations_in_Mathematics_Licks_edited.pdf"&gt;Recreations in Math&lt;/a&gt; - H. E. Licks (1917)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://djm.cc/library/Magic_Squares_Cubes_Andrews_edited.pdf"&gt;Magic Squares and Cubes&lt;/a&gt; - W. S. Andrews (1917)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://web.stanford.edu/~boyd/cvxbook/"&gt;Convex Optimization&lt;/a&gt; - Stephen Boyd and Lieven Vandenberghe&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fabricebaudoin.wordpress.com/"&gt;Fabrice Baudoin's Notes&lt;/a&gt; - Both research and lecture notes on many topics, Including Diffusions on foliated manifold, Stochastic Calculus, Global analysis in Dirichlet spaces, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Branches of Mathematics&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Content Format&lt;/strong&gt; &lt;br /&gt; ğŸ“– Books &lt;br /&gt; ğŸ¥ Videos &lt;br /&gt; ğŸ“ Lecture notes, slides, articles, papers&lt;/p&gt; 
&lt;h2&gt;Foundations of Mathematics&lt;/h2&gt; 
&lt;h3&gt;Transition To Pure Rigour Math&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.trillia.com/zakon1.html"&gt;Basic Concepts of Mathematics&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://richardhammack.github.io/BookOfProof/"&gt;Book of Proof&lt;/a&gt; - Richard Hammak (Virginia Commonwealth University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;a href="https://ia800501.us.archive.org/7/items/how-to-prove-it-a-structured-approach-daniel-j.-velleman/How%20to%20Prove%20It%20A%20Structured%20Approach%20%28Daniel%20J.%20Velleman%29.pdf"&gt;How to Prove It: A Structured Approach (3rd Edition)&lt;/a&gt; - Daniel J. Velleman (Professor).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Set Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cosc.brocku.ca/~duentsch/papers/methprimer1.html"&gt;Sets, Relations, Functions&lt;/a&gt; - Ivo DÃ¼ntsch, GÃ¼nther Gediga&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.toronto.edu/weiss/set_theory.pdf"&gt;An Introduction to Set Theory&lt;/a&gt; - William A. R. Weiss&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.settheory.net/"&gt;Set Theory and Foundations of Mathematics&lt;/a&gt; - Sylvain Poirier&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://plato.stanford.edu/entries/set-theory/"&gt;Set Theory on the Stanford Encyclopedia of Philosophy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Logic&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://pdfs.semanticscholar.org/6967/f52773d9c2ccfc94658657a5761e0f00e95a.pdf"&gt;Introduction to Logic&lt;/a&gt; - Michael Genesereth, Eric Kao (Stanford University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.fecundity.com/codex/forallx.pdf"&gt;An Introduction to Formal Logic&lt;/a&gt; - P.D. Magnus (University at Albany)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://euclid.trentu.ca/math/sb/pcml/pcml-16.pdf"&gt;A Problem Course in Mathematical Logic&lt;/a&gt; - Stefan Bilaniuk (Trent University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://poincare.matf.bg.ac.rs/~zarkom/Book_Math__Cutland_Computability.pdf"&gt;Computability - An introduction to recursive function theory&lt;/a&gt; - Nigel Cutland (University of Hull)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://homepages.uc.edu/~martinj/Symbolic_Logic/341%20Syllabus,%20Textbook,%20Handouts,%20Notes/LPL%20textbook.pdf"&gt;Language, Proof, and Logic&lt;/a&gt; - Jon Barwise, John Etchemendy&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.uni-muenchen.de/~schwicht/lectures/logic/ws03/ml.pdf"&gt;Mathematical Logic&lt;/a&gt; - Helmut Schwichtenberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.personal.psu.edu/t20/notes/logic.pdf"&gt;Mathematical Logic&lt;/a&gt; - Stephen G. Simpson (Pennsylvania State University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://maude.sip.ucm.es/~miguelpt/papers/flogic.pdf"&gt;Formal Logic&lt;/a&gt; - Miguel Palomino&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.math.princeton.edu/~nelson/books/pa.pdf"&gt;Predictive Arithmetic&lt;/a&gt; - Edward Nelson&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.uleth.ca/~dave.morris/books/proofs+concepts.html"&gt;Proofs and Concepts: the fundamentals of abstract mathematics&lt;/a&gt; - Joy Morris, Dave Morris&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.tedsundstrom.com/mathreasoning"&gt;Mathematical Reasoning: Writing and Proof&lt;/a&gt; - Ted Sundstrom&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://leanprover.github.io/logic_and_proof/"&gt;Logic and Proof&lt;/a&gt; - Jeremy Avigad, Robert Y. Lewis, and Floris van Doorn&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://teorth.github.io/QED"&gt;QED - an interactive textbook&lt;/a&gt; - Terence Tao&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://builds.openlogicproject.org/"&gt;Open Logic Textbook&lt;/a&gt; - collaborative effort, main contributors listed &lt;a href="https://openlogicproject.org/people/"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Category Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.tu-darmstadt.de/~streicher/CTCL.pdf"&gt;Introduction to Category Theory and Categorical Logic&lt;/a&gt; - Thomas Streicher&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cs.man.ac.uk/~hsimmons/zCATS.pdf"&gt;An Introduction to Category Theory&lt;/a&gt; - Harold Simmons&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.4754&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Category Theory&lt;/a&gt; - Steve Awodey (Carnegie Mellon University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.uni-muenchen.de/~pareigis/Vorlesungen/04SS/Cats1.pdf"&gt;Category Theory&lt;/a&gt; - B. Pareigis&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20181221233252/http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf"&gt;Category Theory for Computing Science&lt;/a&gt; - Michael Barr, Charles Wells&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/12/tr12.pdf"&gt;Toposes, Triples and Theories&lt;/a&gt; - Michael Barr, Charles Wells&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/3/tr3abs.html"&gt;Abelian Categories&lt;/a&gt; - Peter Freyd&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/7/tr7abs.html"&gt;Categories and Groupoids&lt;/a&gt; - P. J. Higgins&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html"&gt;Basic Concepts of Enriched Category Theory&lt;/a&gt; - G. M. Kelley&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/17/tr17abs.html"&gt;Abstract and Concrete Categories: The Joy of Cats&lt;/a&gt; - Jiri Adamek, Horst Herrlich, George Strecker&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf"&gt;Seven Sketches in Compositionality: An Invitation to Applied Category Theory&lt;/a&gt; - Brendan Fong and David I. Spivak (MIT)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.jhu.edu/~eriehl/context/"&gt;Category Theory in Context&lt;/a&gt; - Emily Riehl (John Hopkins University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Type Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.paultaylor.eu/stable/prot.pdf"&gt;Proofs and Types&lt;/a&gt; - Jean-Yves Girard&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf"&gt;Intuitionistic Type Theory&lt;/a&gt; - Per Martin-Lof&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/"&gt;Type Theory and Functional Programming&lt;/a&gt; - Simon Thompson&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cse.chalmers.se/research/group/logic/book/book.pdf"&gt;Programming in Martin-Lofâ€™s Type Theory&lt;/a&gt; - Bengt Nordstrom, Kent Petersson, Jan M. Smith&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Homotopy Type Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://hottheory.files.wordpress.com/2013/03/hott-online-611-ga1a258c.pdf"&gt;Homotopy Type Theory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Surreal Numbers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~knill/teaching/mathe320_2015_fall/blog15/surreal1.pdf"&gt;Surreal Numbers - How two ex-students turned on to pure mathematics and found total happiness&lt;/a&gt; - D. E. Knuth&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://web.mit.edu/sp.268/www/2010/surreal.pdf"&gt;Surreal Numbers and Games&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ohio.edu/people/ehrlich/ConwayNames.pdf"&gt;Conway names, the simplicity hierarchy and the surreal number tree&lt;/a&gt; - Philip Ehrlich&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Number Theory&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://wstein.org/ent/ent.pdf"&gt;Elementary Number Theory: Primes, Congruences, and Secrets&lt;/a&gt; - William Stein&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.utoledo.edu/~codenth/Spring_13/3200/ENT-books/Elementary_Number_Theory-Clark.pdf"&gt;Elementary Number Theory&lt;/a&gt; - W. Edwin Clark (University of South Florida)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/nt.pdf"&gt;A Course on Number Theory&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://shoup.net/ntb/ntb-v2.pdf"&gt;A Computational Introduction to Number Theory and Algebra&lt;/a&gt; - Victor Shoup&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://alpha.math.uga.edu/~pete/4400FULL.pdf"&gt;Number Theory: A Contemporary Introduction&lt;/a&gt; - Pete L. Clark&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.trillia.com/moser-number.html"&gt;An Introduction to the Theory of Numbers&lt;/a&gt; - Leo Moser&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.poritz.net/jonathan/share/yaintt/"&gt;Yet Another Introductory Number Theory Textbook&lt;/a&gt; - Jonathan A. Poritz&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Number Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://feog.github.io/ANT10.pdf"&gt;Introduction to Algebraic Number Theory&lt;/a&gt; - F. Oggier&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jmilne.org/math/CourseNotes/ANT.pdf"&gt;Algebraic Number Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.math.gatech.edu/~mbaker/pdf/ANTBook.pdf"&gt;Algebraic Number Theory Course Notes&lt;/a&gt; - Matthew Baker (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~r-ash/ANT.html"&gt;A Course In Algebraic Number Theory&lt;/a&gt; - Robert Ash&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Analytic Number Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~hildebr/ant/main.pdf"&gt;Introduction to Analytic Number Theory&lt;/a&gt; - A.J. Hildebrand (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.nsc.ru/~vdovin/lectures/numth_eng.pdf"&gt;Elements of Analytic Number Theory&lt;/a&gt; - P. S. Kolesnikov, E. P. Vdovin (Novosibirsk)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.uni-muenchen.de/~forster/v/ann/annth_all.pdf"&gt;Analytic Number Theory&lt;/a&gt; - Otto Forster (LMU Munich)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www2.math.uu.se/~astrombe/analtalt08/www_notes.pdf"&gt;Analytic Number Theory - Lecture Notes based on Davenportâ€™s book&lt;/a&gt; - Andreas StrÃ¶mbergsson&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Algebra&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uwaterloo.ca/~snburris/htdocs/ualg.html"&gt;A Course in Universal Algebra&lt;/a&gt; - S. Burris, H.P. Sankappanavar&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://faculty.math.illinois.edu/~r-ash/ComAlg.html"&gt;A Course in Commutative Algebra&lt;/a&gt; - Robert Ash&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/First_Algebra_Hawkes_Luby_Touton_edited.pdf"&gt;First Course in Algebra&lt;/a&gt; - Herbert E. Hawkes, William A. Luby, Frank C. Touton (1910)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Second_Algebra_Hawkes_Luby_Touton_edited.pdf"&gt;Second Course in Algebra&lt;/a&gt; - Herbert E. Hawkes, William A. Luby, Frank C. Touton (1911)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Algebra_Elementary_Text-Book_Part_I_Chrystal_edited.pdf"&gt;Algebra: An Elementary Text-Book, Part I&lt;/a&gt; - G. Chrystal (1904)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Algebra_Elementary_Text-Book_Part_II_Chrystal_edited02.pdf"&gt;Algebra: An Elementary Text-Book, Part II&lt;/a&gt; - G. Chrystal (1900)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://jamesbrennan.org/algebra"&gt;Understanding Algebra&lt;/a&gt; - James W. Brennan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Abstract Algebra&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://zodml.org/sites/default/files/Introduction_to_Abstract_Algebra_0.pdf"&gt;Introduction to Abstract Algebra&lt;/a&gt; - D. S. Malik, John N. Mordeson, M.K. Sen (Creighton University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://aleph0.clarku.edu/~djoyce/ma225/algebra.pdf"&gt;Introduction to Modern Algebra&lt;/a&gt; - David Joyce (Clark University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://feog.github.io/AA11.pdf"&gt;Algebraic Methods&lt;/a&gt; - F. Oggier&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://abstract.ups.edu/download/aata-20150812.pdf"&gt;Abstract Algebra : Theory and Applications&lt;/a&gt; - Thomas W. Judson, Robert A. Beezer (Austin State University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.usyd.edu.au/u/bobh/UoS/rfwhole.pdf"&gt;An Undergraduate Course in Abstract Algebra&lt;/a&gt; - Robert Howlett&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.miami.edu/~ec/book"&gt;Elements of Abstract and Linear Algebra&lt;/a&gt; - E.H. Connell (University of Miami)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~r-ash/Algebra.html"&gt;Abstract Algebra: The Basic Graduate Year&lt;/a&gt; - Robert Ash&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20150528171650/extension.harvard.edu/open-learning-initiative/abstract-algebra"&gt;Abstract Algebra: Harvard Extension (Archived)&lt;/a&gt; - Benedict Gross&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.youtube.com/playlist?list=PLA58AC5CABC1321A3"&gt;Abstract Algebra: Harvard Extension Videos&lt;/a&gt; - Benedict Gross&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Group Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www2.bc.edu/mark-reeder/Groups.pdf"&gt;Notes on Group Theory&lt;/a&gt; - Mark Reeder&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jmilne.org/math/CourseNotes/GT.pdf"&gt;Group Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/gt.pdf"&gt;Notes on Finite Group Theory&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cns.gatech.edu/GroupTheory/index.html"&gt;Group Theory&lt;/a&gt; - Pedrag Civitanovic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Linear Algebra&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ubc.ca/~carrell/NB.pdf"&gt;Fundamentals of Linear Algebra&lt;/a&gt; - James B. Carrell&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20140824074655/http://mathstat.helsinki.fi/~fluch/linear_algebra_1-sp07/la1.pdf"&gt;Linear Algebra and Matrices&lt;/a&gt; - Martin Fluch&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.usyd.edu.au/u/bobh/UoS/MATH2902/vswhole.pdf"&gt;Vector Space Theory&lt;/a&gt; - Robert Howlett&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://joshua.smcvt.edu/linearalgebra"&gt;Linear Algebra&lt;/a&gt; - Jim Hefferon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://github.com/juanklopper/MIT_OCW_Linear_Algebra_18_06"&gt;MIT OpenCourseWare Lectures on Linear Algebra (18.06) as Jupyter Notebooks&lt;/a&gt; - Juan Klopper&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.numbertheory.org/book/"&gt;Elementary Linear Algebra&lt;/a&gt; - Keith Matthews&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://linear.ups.edu/"&gt;A First Courses in Linear Algebra&lt;/a&gt; - Rob Breezer&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.ucdavis.edu/~linear/"&gt;Linear Algebra&lt;/a&gt; - David Cherney, Tom Denton, Andrew Waldron&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/2502"&gt;Introduction to vectors and tensors, Vol 1: linear and multilinear algebra&lt;/a&gt; - Ray M Bowen, C. C. Wang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/3609"&gt;Introduction to vectors and tensors, Vol 2: vector and tensor analysis&lt;/a&gt; - Ray M Bowen, C. C. Wang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.stanford.edu/~boyd/vmls/vmls.pdf"&gt;Introduction to Applied Linear Algebra&lt;/a&gt; - Stephen Boyd (Stanford University), Lieven Vandenberghe (UCLA)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.brown.edu/~treil/papers/LADW/LADW_2017-09-04.pdf"&gt;Linear Algebra Done Wrong&lt;/a&gt; - Sergei Treil&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://immersivemath.com/ila/index.html"&gt;Immersive Linear Algebra&lt;/a&gt; - J. StrÃ¶m, K. Ã…strÃ¶m, and T. Akenine-MÃ¶ller&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://textbooks.math.gatech.edu/ila/"&gt;Interactive Linear Algebra&lt;/a&gt; - Dan Margalit and Joseph Rabinoff&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://people.math.gatech.edu/~herod/Hspace/Hspace.html"&gt;Linear Algebra, Infinite Dimensions, and Maple&lt;/a&gt; - James Herod&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;a href="https://linear.axler.net/"&gt;Linear Algebra Done Right&lt;/a&gt; - Sheldon Axler&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ring Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uni-duesseldorf.de/~wisbauer/book.pdf"&gt;Foundations of Module and Ring Theory&lt;/a&gt; - Robert Wisbauer (University of DÃ¼sseldorf)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Galois Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.gla.ac.uk/~ajb/dvi-ps/Galois.pdf"&gt;An Introduction to Galois Theory&lt;/a&gt; - Andrew Baker (University of Glasgow)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jmilne.org/math/CourseNotes/FT.pdf"&gt;Fields and Galois Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://homepages.warwick.ac.uk/~masda/MA3D5/Galois.pdf"&gt;Galois theory&lt;/a&gt; - Miles Reid&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://eclass.uoa.gr/modules/document/file.php/MATH594/Stewart%20Galois%204th%20edition.pdf"&gt;Galois Theory&lt;/a&gt; - Ian Stewart&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://arxiv.org/pdf/2408.07499"&gt;Galois Theory&lt;/a&gt; â€” Tom Leinster (University of Edinburgh)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Lie Algebras&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~shlomo/docs/lie_algebras.pdf"&gt;Lie Algebras&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Combinatorics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://web.math.utk.edu/~wagner/papers/comb.pdf"&gt;Basic Combinatorics&lt;/a&gt; - Carl G. Wagner (University of Tennessee)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.math.gatech.edu/~trotter/book.pdf"&gt;Applied Combinatorics&lt;/a&gt; - Mitchel T. Keller, William T. Trotter&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/comb.pdf"&gt;Notes on Combinatorics&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://algo.inria.fr/flajolet/Publications/book.pdf"&gt;Analytic Combinatorics&lt;/a&gt; - Philippe Flajolet, Robert Sedgewick&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.upenn.edu/~wilf/DownldGF.html"&gt;generatingfunctionology&lt;/a&gt; - Herbert Wilf&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Graph Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.personal.psu.edu/cxg286/Math485.pdf"&gt;Graph Theory: Lecture Notes&lt;/a&gt; - Christopher Griffin&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cs.unibo.it/babaoglu/courses/cas00-01/tutorials/GraphTheory.pdf"&gt;Graph Theory&lt;/a&gt; - Reinhard Diestel&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://learngraphtheory.org/"&gt;Graph Theory : Interactive Algorithm Visualizer | Graph Theory Learning Platform&lt;/a&gt; - Hadjoudj Mohammed Islam&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Geometry and Topology&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://polly.phys.msu.ru/~belyaev/geometry.pdf"&gt;Fundamentals of Geometry&lt;/a&gt; - Oleg A. Belyaev&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.upenn.edu/~wilf/AeqB.html"&gt;A=B&lt;/a&gt; - M. Petkovsek, H. Wilf, D. Zeilberger&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://aleph0.clarku.edu/~djoyce/java/elements/toc.html"&gt;Elements&lt;/a&gt; - Euclid&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://starrhorse.com/euclid/"&gt;Euclid's Elements Redux&lt;/a&gt; - Daniel Callahan&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ubc.ca/~cass/graphics/manual/"&gt;Mathematical Illustrations&lt;/a&gt; - Bill Casselman&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.c82.net/euclid/"&gt;Byrne's Euclid&lt;/a&gt; - Oliver Byrne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Plane_Geometry_Wentworth_Smith_edited.pdf"&gt;Plane Geometry&lt;/a&gt; - George Wentworth and David Eugene Smith (1913)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Plane_Spherical_Trigonometry_Wentworth_Smith_edited_2.pdf"&gt;Planes and Spherical Trigonometry&lt;/a&gt; - George Wentworth and David Eugene Smith (1915)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Coordinate_Geometry_Fine_Thompson_edited03.pdf"&gt;Coordinate Geometry&lt;/a&gt; - Henry Buchard Fine and Henry Dallas Thompson (1911)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Analytic_Geometry_Siceloff_Wentworth_Smith_edited.pdf"&gt;Analytic Geometry&lt;/a&gt; - Lewis Parker Siceloff, George Wentworth, David Eugene Smith (1922)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Differential Geometry&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://people.math.ethz.ch/~salamon/PREPRINTS/diffgeo.pdf"&gt;Introduction to Differential Geometry&lt;/a&gt; - Joel W. Robbin, Dietmar A. Salamon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.cis.upenn.edu/~jean/gbooks/manif.html"&gt;Notes on Differential Geometry and Lie Groups&lt;/a&gt; - Jean Gallier (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mat.univie.ac.at/~michor/dgbook.pdf"&gt;Topics in Differential Geometry&lt;/a&gt; - Peter W. Michor&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://mysite.science.uottawa.ca/rossmann/Differential%20Geometry%20book_files/Diffgeo.pdf"&gt;Lectures on Differential Geometry&lt;/a&gt; - Wulf Rossmann&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.matematik.lu.se/matematiklu/personal/sigma/Riemann.pdf"&gt;An Introduction to Riemannian Geometry&lt;/a&gt; - Sigmundur Gudmundsson (Lund University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://msri.org/publications/books/gt3m/"&gt;The Geometry and Topology of Three-Manifolds&lt;/a&gt; - W. Thurston&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~shlomo/docs/semi_riemannian_geometry.pdf"&gt;Semi-Riemann Geometry and General Relativity&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf"&gt;Discrete Differential Geometry&lt;/a&gt; - Keenan Crane&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Geometry&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ksda.ccny.cuny.edu/PostedPapers/rickksda1107.pdf"&gt;A Brief Introduction to Algebraic Geometry&lt;/a&gt; - R.C. Churchill&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.lsa.umich.edu/~idolga/631.pdf"&gt;Introduction to Algebraic Geometry&lt;/a&gt; - Igor V. Dolgachev&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.stanford.edu/~vakil/216blog/FOAGjun1113public.pdf"&gt;Foundations of Algebraic Geometry&lt;/a&gt; - Ravi Vakil&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cis.upenn.edu/~jean/algeoms.pdf"&gt;Algebraic Geometry&lt;/a&gt; - Jean Gallier, Stephen S. Shatz (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jmilne.org/math/CourseNotes/AG.pdf"&gt;Algebraic Geometry&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.uni-kl.de/~gathmann/class/alggeom-2002/main.pdf"&gt;Algebraic Geometry&lt;/a&gt; - Andreas Gathmann (University of Kaiserslautern)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://stacks.math.columbia.edu/"&gt;The Stacks Project&lt;/a&gt; - Maintained by Aise Johan de Jong (Columbia)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Statistics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://math.berkeley.edu/~bernd/owl.pdf"&gt;Lectures on Algebraic Statistics&lt;/a&gt; - Mathias Drton, Bernd Sturmfels, Seth Sullivant&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www3.diism.unisi.it/~chiantini/did/00Book.pdf"&gt;An Introduction to Algebraic Statistics&lt;/a&gt; - Cristiano Bocci, Luca Chiantini and Anthony V. Geramita&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://tore.tuhh.de/dspace-cris-server/api/core/bitstreams/a0c378d5-ce8e-442a-8891-9e7f763b4279/content"&gt;Algebraic Statistics&lt;/a&gt; - Karl-Heinz Zimmermann&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://yaroslavvb.com/papers/pachter-algebraic.pdf"&gt;Algebraic Statistics for Computational Biology&lt;/a&gt; - Pachter, and Sturmfels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Topology&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.upenn.edu/~ghrist/notes.html"&gt;Elementary Applied Topology&lt;/a&gt; - Robert Ghrist (UPenn)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.colostate.edu/~renzo/teaching/Topology10/Notes.pdf"&gt;Introduction to Topology&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.bme.hu/~kalex/Teaching/Spring10/Topology/TopNotes_Spring10.pdf"&gt;Introduction to Topology&lt;/a&gt; - Alex KÃ¼ronya&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.clemson.edu/~jimlb/Teaching/2009-10/Math986/Topology.pdf"&gt;Introductory Topology&lt;/a&gt; - Jim L. Brown&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://webusers.imj-prg.fr/~pierre.schapira/lectnotes/Topo.pdf"&gt;General Topology&lt;/a&gt; - Pierre Schapira (Paris VI University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.pdmi.ras.ru/~olegviro/topoman/eng-book-nopfs.pdf"&gt;Elementary Topology Problem Textbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ku.dk/~moller/e03/3gt/notes/gtnotes.pdf"&gt;General Topology&lt;/a&gt; - Jesper M. MÃ¸ller&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://mathonline.wikidot.com/topology"&gt;Topology Topics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Topology&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.cornell.edu/~hatcher/AT/AT.pdf"&gt;Algebraic Topology&lt;/a&gt; - Allen Hatcher&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uchicago.edu/~may/CONCISE/ConciseRevised.pdf"&gt;A Concise Course in Algebraic Topology&lt;/a&gt; - J. P. May&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.muni.cz/~cadek/at/at.pdf"&gt;Introduction to Algebraic Topology&lt;/a&gt; - Martin Cadek&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://webusers.imj-prg.fr/~pierre.schapira/lectnotes/AlTo.pdf"&gt;Algebra and Topology&lt;/a&gt; - Pierre Schapira (Paris VI University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.indiana.edu/~jfdavis/teaching/m623/book.pdf"&gt;Lecture Notes in Algebraic Topology&lt;/a&gt; - James F. Davis, Paul Kirk (Indiana University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.ma.utexas.edu/ibl1/courses/resources/12_15_07_grad_alg_top_mooremethod.pdf"&gt;Algebraic Topology&lt;/a&gt; - Michael Starbird&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.nus.edu.sg/~matwujie/ma5209.pdf"&gt;Lecture Notes on Algebraic Topology&lt;/a&gt; - Jie Wu&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Analysis&lt;/h2&gt; 
&lt;h3&gt;Real Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook/"&gt;MIT OpenCourseWare Lectures on Calculus&lt;/a&gt; - G. Strang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.wisc.edu/~keisler/calc.html"&gt;Elementary Calculus: An Approach Using Infinitesimals&lt;/a&gt; - Professor H. Jerome Keisler&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.ucdavis.edu/~hunter/intro_analysis_pdf/intro_analysis.pdf"&gt;An Introduction to Real Analysis&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://ramanujan.math.trinity.edu/wtrench/texts/TRENCH_REAL_ANALYSIS.PDF"&gt;Introduction to Real Analysis&lt;/a&gt; - William F. Trench (Trinity University, Texas)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jirka.org/ra/realanal.pdf"&gt;Basic Analysis: Introduction to Real Analysis&lt;/a&gt; - JiÅ™Ã­ Lebl&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://prac.im.pwr.wroc.pl/~kwasnicki/pl/stuff/tbb-hyper.pdf"&gt;Elementary Real Analysis&lt;/a&gt; - Thomson, Bruckner&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://ms.mcmaster.ca/~sawyer/Publications/Real_Analysis.pdf"&gt;Lecture Notes in Real Analysis&lt;/a&gt; - Eric T. Sawyer (McMaster University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.harvard.edu/~ctm/papers/home/text/class/harvard/212a/course/course.pdf"&gt;Real Analysis&lt;/a&gt; - C. McMullen&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://bass.math.uconn.edu/3rd.pdf"&gt;Real Analysis for Graduate Students&lt;/a&gt; - Richard F. Bass&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.purdue.edu/~torres/pubs/Modern-real-analysis.pdf"&gt;Modern Real Analysis&lt;/a&gt; - William P. Ziemer (Indiana University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.trillia.com/zakon-analysisI.html"&gt;Mathematical Analysis Vol I&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.trillia.com/zakon-analysisII.html"&gt;Mathematical Analysis Vol II&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~shlomo/docs/Advanced_Calculus.pdf"&gt;Advanced Calculus&lt;/a&gt; - Lynn Loomis, Schlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://spot.colorado.edu/~baggett/analysis.html"&gt; Analysis of Functions of a Single Variable&lt;/a&gt; - Lawerence Baggett&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.synechism.org/wp/the-calculus-of-functions-of-several-variables/"&gt;The Calculus of Functions of Several Variables&lt;/a&gt; - Dan Sloughter&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://web.pdx.edu/~erdman/PTAC/problemtext_pdf.pdf"&gt;A ProblemText in Advanced Calculus&lt;/a&gt; - John M. Erdman&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://hdl.handle.net/2027/spo.5597602.0001.001"&gt;Calculus and Linear Algebra. Vol. 1&lt;/a&gt; - Wilfred Kaplan, Donald J. Lewis&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://quod.lib.umich.edu/s/spobooks/5597602.0002.001"&gt;Calculus and Linear Algebra. Vol. 2&lt;/a&gt; - Wilfred Kaplan, Donald J. Lewis&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.odu.edu/~jhh/counter10.html"&gt;Introduction to Calculus I and II&lt;/a&gt; - J.H. Heinbockel&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://faculty.gvsu.edu/boelkinm/Home/Active_Calculus.html"&gt;Active Calculus&lt;/a&gt; - Matt Boelkins&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://math.berkeley.edu/~gbergman/ug.hndts/#Rudin"&gt;Supplements to the Exercises in Chapters 1-7 of Walter Rudin's "Principles of Mathematical Analysis"&lt;/a&gt; - George M. Bergman&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://calculusmadeeasy.org/"&gt;Calculus Made Easy&lt;/a&gt; - Silvanus P. Thompson (1910)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Elements_Differential_Integral_Calculus_Granville_edited_2.pdf"&gt;Elements of Differential and Integral Calculus&lt;/a&gt; - William Anthony Granville (1911)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://stitz-zeager.com/szprecalculus07042013.pdf"&gt;Precalculus&lt;/a&gt; - Carl Stitz, Jeff Zeager&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Harmonic Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~laugesen/545/545Lectures.pdf"&gt;Harmonic Analysis Lecture Notes&lt;/a&gt; - Richard S. Laugesen (University of Illinois at Urbanaâ€“Champaign)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uchicago.edu/~schlag/harmonicnotes.pdf"&gt;Harmonic Analysis&lt;/a&gt; - W. Schlag&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf"&gt;Lecture Notes: Fourier Transform and its Applications&lt;/a&gt; - Brad Osgood&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.reed.edu/physics/courses/Physics331.f08/pdf/Fourier.pdf"&gt;Fourier Analysis&lt;/a&gt; - Lucas Illing&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ccrma.stanford.edu/~jos/mdft"&gt;Mathematics of the Discrete Fourier Transform (DFT) with Audio Applications&lt;/a&gt; - Julius O. Smith III (Stanford University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complex Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://mtaylor.web.unc.edu/wp-content/uploads/sites/16915/2018/04/complex.pdf"&gt;Introduction to Complex Analysis&lt;/a&gt; - Michael Taylor&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~jpda/jpd-complex-geometry-book-5-refs-bip.pdf"&gt;An Introduction to Complex Analysis and Geometry&lt;/a&gt; - John P. D'Angelo (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.sfsu.edu/beck/papers/complex.pdf"&gt;A First Course in Complex Analysis&lt;/a&gt; - Matthias Beck, Gerald Marchesi, Dennis Pixton, Lucas Sabalka&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.wustl.edu/~sk/books/guide.pdf"&gt;A Guide to Complex Variables&lt;/a&gt; - Steven G. Krantz&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.manchester.ac.uk/~cwalkden/complex-analysis/complex_analysis.pdf"&gt;Complex Analysis&lt;/a&gt; - Charles Walkden&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ku.dk/noter/filer/koman-12.pdf"&gt;Complex Analysis&lt;/a&gt; - Christian Berg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.math.sc.edu/girardi/m7034/book/AshComplexVariablesWithHyperlinks.pdf"&gt;Complex Variables&lt;/a&gt; - R. B. Ash, W.P. Novinger&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.lth.se/matematiklu/personal/olofsson/CompHT06.pdf"&gt;Complex Analysis&lt;/a&gt; - Christer Bennewitz&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20150620124453/https://www.math.washington.edu/~marshall/math_536/Notes.pdf"&gt;Complex Analysis&lt;/a&gt; - Donald E. Marshall&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://gauss.math.yale.edu/~ws442/complex.pdf"&gt;A Concise Course in Complex Analysis and Riemann Surfaces&lt;/a&gt; - Wilhelm Schlag&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.math.gatech.edu/%7Ecain/winter99/complex.html"&gt;Complex Analysis&lt;/a&gt; - G. Cain (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://complex-analysis.com/"&gt;Complex Analysis&lt;/a&gt; - Juan Carlos Ponce Campuzano&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Functional Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.uwaterloo.ca/~lwmarcou/notes/pmath453.pdf"&gt;An Introduction to Functional Analysis&lt;/a&gt; - Laurent W. Marcoux (University of Waterloo)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://users.math.msu.edu/users/jeffrey/920/920notes.pdf"&gt;Functional Analysis: Lecture Notes&lt;/a&gt; - Jeff Schenker (Michigan State University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://archive.org/details/TB_Ward___Functional_analysis_lecture_notes"&gt;Functional Analysis Lecture Notes&lt;/a&gt; - T.B. Ward (University of East Anglia)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.lancs.ac.uk/~belton/www/notes/fa_notes.pdf"&gt;Functional Analysis&lt;/a&gt; - Alexander C. R. Belton&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.mat.univie.ac.at/~gerald/ftp/book-fa/fa.pdf"&gt;Topics in Real and Functional Analysis&lt;/a&gt; - Gerald Teschl&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www2.math.ou.edu/~cremling/teaching/lecturenotes/fa-new/LN-I.pdf"&gt;Functional Analysis&lt;/a&gt; - Christian Remling&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~shlomo/docs/Real_Variables.pdf"&gt;Theory of Functions of a Real Variable&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://spot.colorado.edu/~baggett/functional.html"&gt;Functional Analysis&lt;/a&gt; - Lawerence Baggett&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Measure Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://terrytao.files.wordpress.com/2012/12/gsm-126-tao5-measure-book.pdf"&gt;An Introduction to Measure Theory&lt;/a&gt; - Terence Tao (UCLA)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mat.uniroma2.it/~cannarsa/cam_0607.pdf"&gt;Lecture Notes on Measure Theory and Functional Analysis&lt;/a&gt; - P. Cannarsa, T. Dâ€™Aprile&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.chalmers.se/~borell/MeasureTheory.pdf"&gt;Lecture Notes in Measure Theory&lt;/a&gt; - Christer Borell&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.gold-saucer.org/math/lebesgue/lebesgue.pdf"&gt;A Crash Course on the Lebesgue Integral and Measure Theory&lt;/a&gt; - Steve Cheng&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.ucdavis.edu/~hunter/measure_theory/measure_notes.pdf"&gt;Measure Theory&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://people.math.ethz.ch/~salamon/PREPRINTS/measure.pdf"&gt;Measure and Integration&lt;/a&gt; - Dietmar A. Salamon (ETH ZÃ¼rich)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ucsd.edu/~bdriver/240-00-01/Lecture_Notes/measurep.pdf"&gt;Lecture notes: Measure Theory&lt;/a&gt; - Bruce K. Driver&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ordinary Differential Equations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.synechism.org/wp/difference-equations-to-differential-equations/"&gt;Difference Equations To Differential Equations&lt;/a&gt; - Dan Sloughter&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.uni-bielefeld.de/~grigor/odelec2008.pdf"&gt;Ordinary Differential Equation&lt;/a&gt; - Alexander Grigorian (University of Bielefeld)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cs.bgu.ac.il/~leonid/ode_bio_files/Ionascu_LectNotes.pdf"&gt;Ordinary Differential Equations: Lecture Notes&lt;/a&gt; - Eugen J. Ionascu&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.lmu.de/~philip/publications/lectureNotes/ODE.pdf"&gt;Ordinary Differential Equations&lt;/a&gt; - Peter Philip&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://users.math.msu.edu/users/gnagy/teaching/ode.pdf"&gt;Ordinary Differential Equations&lt;/a&gt; - Gabriel Nagy&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mat.univie.ac.at/~gerald/ftp/book-ode/ode.pdf"&gt;Ordinary Differential Equations and Dynamical Systems&lt;/a&gt; - Gerald Teschl&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://leipper.org/manuals/zip-fill/dn-difeq-notes.pdf"&gt;Notes on Differential Equations&lt;/a&gt; - Bob Terrell&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://digitalcommons.trinity.edu/mono/8/"&gt;Elementary Differential Equations&lt;/a&gt; - William F. Trench&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://digitalcommons.trinity.edu/mono/9/"&gt;Elementary Differential Equations With Boundary Value Problems&lt;/a&gt; - William F. Trench&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jirka.org/diffyqs/"&gt;Notes on Diffy Qs: Differential Equations for Engineers&lt;/a&gt; - JiÅ™Ã­ Lebl&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Differential_Equations_Phillips_edited.pdf"&gt;Differential Equations&lt;/a&gt; - H. B. Phillips (1922)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Partial Differential Equations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.ucdavis.edu/~hunter/pdes/pde_notes.pdf"&gt;Notes on Partial Differential Equations&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uni-leipzig.de/~miersemann/pdebook.pdf"&gt;Partial Differential Equations: Lecture Notes&lt;/a&gt; - Erich Miersemann (Leipzig University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathphysics.com/pde/"&gt;Linear Methods of Applied Mathematics&lt;/a&gt; - E. Harrell, J. Herod (Georgia Tech)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Chaos Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://archive.org/details/chaosmakingnewsc0000unse"&gt;Chaos: Making a New Science&lt;/a&gt; - James Gleick&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://archive.org/details/complexityguided0000mitc?utm_source=chatgpt.com"&gt;Complexity: A Guided Tour&lt;/a&gt; - Melanie Mitchell (Oxford University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Probability and Statistics&lt;/h2&gt; 
&lt;h3&gt;Probability Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf"&gt;Introduction to Probability&lt;/a&gt; - Charles M. Grinstead, J. Laurie Snell&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://vfu.bg/en/e-Learning/Math--Bertsekas_Tsitsiklis_Introduction_to_probability.pdf"&gt;Introduction to Probability&lt;/a&gt; - Dimitri P. Bertsekas, John N. Tsitsiklis (MIT)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.uq.edu.au/~kroese/asitp.pdf"&gt;A Short Introduction to Probability&lt;/a&gt; - Dirk P. Kroese (University of Queensland)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.duke.edu/~rtd/PTE/PTE4_1.pdf"&gt;Probability: Theory and Examples&lt;/a&gt; - Rick Durrett&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://github.com/mavam/stat-cookbook/releases/download/0.2.3/stat-cookbook.pdf"&gt;Probability and Statistics Cookbook&lt;/a&gt; - Matthias Vallentin (UC Berkeley)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.wzchen.com/probability-cheatsheet/"&gt;The Only Probability Cheatsheet You'll Ever Need&lt;/a&gt; - William Chen&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ellerman.org/Davids-Stuff/Maths/Rota-Baclawski-Prob-Theory-79.pdf"&gt;An Introduction to Probability and Random Processes&lt;/a&gt; - Gian-Carlo Rota, Kenneth Baclawski&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://arxiv.org/pdf/1906.01803.pdf"&gt;Foundations of Constructive Probability Theory&lt;/a&gt; - Yuen-Kwok Chan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Statistics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://homepages.math.uic.edu/~rgmartin/Teaching/Stat411/Notes/411notes.pdf"&gt;Lecture Notes on Statistical Theory&lt;/a&gt; - Ryan Martin (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www-library.desy.de/preparch/books/vstatmp_engl.pdf"&gt;Introduction to Statistics and Data Analysis for Physicists&lt;/a&gt; - Gerhard Bohm, GÃ¼nter Zech&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.iiserpune.ac.in/~ayan/MTH201/Sahoo_textbook.pdf"&gt;Probability and Mathematical Statistics&lt;/a&gt; - Prasanna Sahoo (University of Louisville)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.arizona.edu/~faris/stat.pdf"&gt;Lectures on Statistics&lt;/a&gt; - William G. Faris&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://pages.pomona.edu/~ajr04747/Fall2009/Math152/Notes/Math152NotesFall09.pdf"&gt;Statistical Theory&lt;/a&gt; - Adolfo J. Rumbos&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://mason.gmu.edu/~jgentle/books/MathStat.pdf"&gt;Theory of Statistics&lt;/a&gt; - James E. Gentle (George Mason University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.arizona.edu/~jwatkins/notests.pdf"&gt;Theory of Statistics&lt;/a&gt; - Joseph C. Watkins (University of Arizona)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20130523134625/http://www.aiaccess.net/e_gm.htm"&gt;Glossary of Data Modeling&lt;/a&gt; - AI Access&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ats.ucla.edu/stat/papers/"&gt;Statistics Papers&lt;/a&gt; - List of statistics papers curated by the Institute for Digital Research and Education (IDRE) at UCLA on methods such as bootstrap and factor invariance.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://itl.nist.gov/div898/handbook/index.htm"&gt;NIST Handbook of Statistical Methods&lt;/a&gt; - Resource on practical statistics directed towards scientists and engineers.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://vassarstats.net/textbook/"&gt;Concepts and Applications of Inferential Statistics&lt;/a&gt; - Richard Lowry&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cosc.brocku.ca/~duentsch/papers/methprimer2.html"&gt;Rough set data analysis: A road to non-invasive knowledge discovery&lt;/a&gt; - Ivo DÃ¼ntsch, GÃ¼nther Gediga&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://statsthinking21.org/"&gt;Statistical Thinking for the 21st Century&lt;/a&gt; - Russell A. Poldrack&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://jonathanweisberg.org/vip/"&gt;Odds and Ends: Introducing Probability &amp;amp; Decision with a Visual Emphasis&lt;/a&gt; - Jonathan Weisberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://seeing-theory.brown.edu/"&gt;Seeing Theory&lt;/a&gt; - Daniel Kunin, Jingru Guo, Tyler Dae Devlin, and Daniel Xiang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.statisticsdonewrong.com/"&gt;Statistics Done Wrong&lt;/a&gt; - Alex Reinhart&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://link.springer.com/book/10.1007/978-0-387-21736-9"&gt;All of Statistics: A Concise Course in Statistical Inference&lt;/a&gt; - Larry Wasserman&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Statistical Learning&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"&gt;An Introduction to Statistical Learning with Applications in R&lt;/a&gt; - Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf"&gt;The Elements of Statistical Learning&lt;/a&gt; - Trevor Hastie, Robert Tibshirani, Jerome Friedman&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.stanford.edu/class/cs229t/notes.pdf"&gt;Statistical Learning Theory&lt;/a&gt; - Percy Liang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - Richard S. Sutton, Andrew G. Barto&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Stochastic processes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.tifr.res.in/~publ/ln/tifr24.pdf"&gt;Lectures on Stochastic Processes&lt;/a&gt; - K. Ito (Tata Institute of Fundamental Research, Bombay)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~knill/teaching/math144_1994/probability.pdf"&gt;Probability and Stochastic Processes with Applications&lt;/a&gt; - Oliver Knill (Harvard University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://statweb.stanford.edu/~adembo/math-136/nnotes.pdf"&gt;Stochastic Processes&lt;/a&gt; - Amir Dembo (Stanford University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mi.fu-berlin.de/wiki/pub/CompMolBio/MarkovKetten15/stochastic_processes_2011.pdf"&gt;Lecture Notes on Stochastic Processes&lt;/a&gt; - Frank NoÃ©, Bettina Keller and Jan-Hendrik Prinz (Freie UniversitÃ¤t Berlin)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.ma.utexas.edu/users/gordanz/notes/introduction_to_stochastic_processes.pdf"&gt;Introduction to Stochastic Processes - Lecture Notes&lt;/a&gt; - Gordan Å½itkoviÄ‡ (University of Texas)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.uwaterloo.ca/~mscott/Little_Notes.pdf"&gt;Applied Stochastic Processes in science and engineering&lt;/a&gt; - Matt Scott (University of Waterloo)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.leidenuniv.nl/~spieksma/colleges/sp-master/sp-hvz1.pdf"&gt;An Introduction to Stochastic Processes in Continuous Time&lt;/a&gt; - Flora Spieksma (Leiden University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf"&gt;Markov Chains and Mixing Times&lt;/a&gt; - David A. Levin, Yuval Peres, Elizabeth L. Wilmer&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.stat.yale.edu/~pollard/Books/1984book/pollard1984.pdf"&gt;Convergence of Stochastic Processes&lt;/a&gt; - David Pollard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Numerical Analysis&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.umd.edu/~dlevy/resources/notes.pdf"&gt;Introduction to Numerical Analysis&lt;/a&gt; - Doron Levy (University of Maryland)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ima.umn.edu/~arnold/597.00-01/nabook.pdf"&gt;A Concise Introduction to Numerical Analysis&lt;/a&gt; - Douglas N. Arnold (University of Minnesota)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.cs.uchicago.edu/~ridg/newna/nalrs.pdf"&gt;Numerical Analysis&lt;/a&gt; - L. Ridgway Scott&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1000&amp;amp;context=math_textbooks"&gt;Lectures In Basic Computational Numerical Analysis&lt;/a&gt; - J. M. McDonough (University of Kentucky)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://user.math.uni-bremen.de/schmi/SS04/YSU_Notes.pdf"&gt;Advanced Numerical Methods and Their Applications to Industrial Problems: Adaptive Finite Element Methods&lt;/a&gt; - Alfred Schmidt, Arsen Narimanyan&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ece.uwaterloo.ca/~dwharder/nm/"&gt;Numerical Analysis for Engineers&lt;/a&gt; - Douglas Wilhelm Harder&lt;/li&gt; 
 &lt;li&gt;ğŸ“ğŸ¥ &lt;a href="https://www.cs.utexas.edu/users/flame/laff/alaff/frontmatter.html"&gt;Advanced Linear Algebra: Foundations to Frontiers&lt;/a&gt; - Robert van de Geijn, Margaret Myers (University of Texas at Austin)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Signal processing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ece.rutgers.edu/~orfanidi/intro2sp/orfanidis-i2sp.pdf"&gt;Introduction to Signal Processing&lt;/a&gt; - Sophocles J. Orfanidis (Rutgers University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.fourierandwavelets.org/FSP_v1.1_2014.pdf"&gt;Foundations of Signal Processing&lt;/a&gt; - Martin Vetterli, Jelena Kovacevic, Vivek K Goyal&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ee.stanford.edu/~gray/sp.pdf"&gt;An Introduction to Statistical Signal Processing&lt;/a&gt; - Robert M. Gray, Lee D. Davisson&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://greenteapress.com/wp/think-dsp/"&gt;Think DSP&lt;/a&gt; - Allen B. Downey&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.uio.no/studier/emner/matnat/math/MAT-INF2360/v15/kompendium/applinalgpython.pdf"&gt;Linear algebra, signal processing, and wavelets. A unified approach.&lt;/a&gt; - Ã˜yvind Ryan (University of Oslo)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematics for Computer Science&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://people.csail.mit.edu/meyer/mcs.pdf"&gt;Mathematics for Computer Science&lt;/a&gt; - Eric Lehman, F. Thomson Leighton, Albert R. Meyer&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.upenn.edu/%7Ewilf/AlgComp3.html"&gt;Algorithms and Complexity&lt;/a&gt; - H. Wilf&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.eecs.berkeley.edu/~varaiya/papers_ps.dir/NOO.pdf"&gt;Lecture Notes on Optimization&lt;/a&gt; - Pravin Varaiya&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.inference.org.uk/mackay/itila/book.html"&gt;Information Theory, Inference, and Learning Algorithms&lt;/a&gt; - David J. C. MacKay&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://hypertextbook.com/chaos/"&gt;The Chaos Textbook: Mathematics in the age of the computer&lt;/a&gt; - Glenn Elert&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematical Biology&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ust.hk/~machas/mathematical-biology.pdf"&gt;Mathematical Biology&lt;/a&gt; - Jeffrey Chasnov&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematical Physics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/2501"&gt;Introduction to Continuum Mechanics&lt;/a&gt; - Ray. M. Bowen&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.physics.miami.edu/nearing/mathmethods/"&gt;Mathematical Tools for Physics&lt;/a&gt; - James Nearing&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.malaspina.com/etext/heavens.htm"&gt;Mechanism of the Heavens (1831)&lt;/a&gt; - Mary Somerville&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Students Lecture Notes&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://web.evanchen.cc/coursework.html"&gt;Evan Chen&lt;/a&gt; - MIT. 2012 ~ 2018. Covers Combinatorics, Number Theory, Honors Algebra, Set Theory, Real Analysis, Graph Theory, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dec41.user.srcf.net/notes/"&gt;Dexter Chua&lt;/a&gt; - Harvard. 2013 ~ 2018. Covers Analysis, Probability, Linear Algebra, Complex Analysis, Numerical Analysis, Statistics, Optimization, Algebraic Topology, Quantum Field Theory, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Related Awesome Lists&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mostafatouny/awesome-theoretical-computer-science"&gt;Theoretical Computer Science&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;a href="http://creativecommons.org/publicdomain/zero/1.0/"&gt;&lt;img src="http://i.creativecommons.org/p/zero/1.0/88x31.png" alt="CC0" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To the extent possible under law, &lt;a href="http://cyrille.rossant.net"&gt;Cyrille Rossant&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AsyncFuncAI/deepwiki-open</title>
      <link>https://github.com/AsyncFuncAI/deepwiki-open</link>
      <description>&lt;p&gt;Open Source DeepWiki: AI-Powered Wiki Generator for GitHub/Gitlab/Bitbucket Repositories. Join the discord: https://discord.gg/gMwThUMeme&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepWiki-Open&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/Deepwiki.png" alt="DeepWiki Banner" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;DeepWiki&lt;/strong&gt; is my own implementation attempt of DeepWiki, automatically creates beautiful, interactive wikis for any GitHub, GitLab, or BitBucket repository! Just enter a repo name, and DeepWiki will:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Analyze the code structure&lt;/li&gt; 
 &lt;li&gt;Generate comprehensive documentation&lt;/li&gt; 
 &lt;li&gt;Create visual diagrams to explain how everything works&lt;/li&gt; 
 &lt;li&gt;Organize it all into an easy-to-navigate wiki&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://buymeacoffee.com/sheing"&gt;&lt;img src="https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png" alt="&amp;quot;Buy Me A Coffee&amp;quot;" /&gt;&lt;/a&gt; &lt;a href="https://tip.md/sng-asyncfunc"&gt;&lt;img src="https://tip.md/badge.svg?sanitize=true" alt="Tip in Crypto" /&gt;&lt;/a&gt; &lt;a href="https://x.com/sashimikun_void"&gt;&lt;img src="https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&amp;amp;logo=twitter&amp;amp;logoColor=white" alt="Twitter/X" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/VQMBGR8u5v"&gt;&lt;img src="https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.zh.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.zh-tw.md"&gt;ç¹é«”ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.es.md"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.kr.md"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.vi.md"&gt;Tiáº¿ng Viá»‡t&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.pt-br.md"&gt;PortuguÃªs Brasileiro&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.fr.md"&gt;FranÃ§ais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.ru.md"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Instant Documentation&lt;/strong&gt;: Turn any GitHub, GitLab or BitBucket repo into a wiki in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Private Repository Support&lt;/strong&gt;: Securely access private repositories with personal access tokens&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Analysis&lt;/strong&gt;: AI-powered understanding of code structure and relationships&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Beautiful Diagrams&lt;/strong&gt;: Automatic Mermaid diagrams to visualize architecture and data flow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Navigation&lt;/strong&gt;: Simple, intuitive interface to explore the wiki&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ask Feature&lt;/strong&gt;: Chat with your repository using RAG-powered AI to get accurate answers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepResearch&lt;/strong&gt;: Multi-turn research process that thoroughly investigates complex topics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Model Providers&lt;/strong&gt;: Support for Google Gemini, OpenAI, OpenRouter, and local Ollama models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Embeddings&lt;/strong&gt;: Choose between OpenAI, Google AI, or local Ollama embeddings for optimal performance&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Quick Start (Super Easy!)&lt;/h2&gt; 
&lt;h3&gt;Option 1: Using Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Create a .env file with your API keys
echo "GOOGLE_API_KEY=your_google_api_key" &amp;gt; .env
echo "OPENAI_API_KEY=your_openai_api_key" &amp;gt;&amp;gt; .env
# Optional: Use Google AI embeddings instead of OpenAI (recommended if using Google models)
echo "DEEPWIKI_EMBEDDER_TYPE=google" &amp;gt;&amp;gt; .env
# Optional: Add OpenRouter API key if you want to use OpenRouter models
echo "OPENROUTER_API_KEY=your_openrouter_api_key" &amp;gt;&amp;gt; .env
# Optional: Add Ollama host if not local. defaults to http://localhost:11434
echo "OLLAMA_HOST=your_ollama_host" &amp;gt;&amp;gt; .env
# Optional: Add Azure API key, endpoint and version if you want to use azure openai models
echo "AZURE_OPENAI_API_KEY=your_azure_openai_api_key" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_VERSION=your_azure_openai_version" &amp;gt;&amp;gt; .env
# Run with Docker Compose
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed instructions on using DeepWiki with Ollama and Docker, see &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/Ollama-instruction.md"&gt;Ollama Instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ’¡ &lt;strong&gt;Where to get these keys:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Get a Google API key from &lt;a href="https://makersuite.google.com/app/apikey"&gt;Google AI Studio&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Get an OpenAI API key from &lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI Platform&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Get Azure OpenAI credentials from &lt;a href="https://portal.azure.com/"&gt;Azure Portal&lt;/a&gt; - create an Azure OpenAI resource and get the API key, endpoint, and API version&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Option 2: Manual Setup (Recommended)&lt;/h3&gt; 
&lt;h4&gt;Step 1: Set Up Your API Keys&lt;/h4&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the project root with these keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# Optional: Use Google AI embeddings (recommended if using Google models)
DEEPWIKI_EMBEDDER_TYPE=google
# Optional: Add this if you want to use OpenRouter models
OPENROUTER_API_KEY=your_openrouter_api_key
# Optional: Add this if you want to use Azure OpenAI models
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
AZURE_OPENAI_VERSION=your_azure_openai_version
# Optional: Add Ollama host if not local. default: http://localhost:11434
OLLAMA_HOST=your_ollama_host
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 2: Start the Backend&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python dependencies
python -m pip install poetry==1.8.2 &amp;amp;&amp;amp; poetry install -C api

# Start the API server
python -m api.main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 3: Start the Frontend&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install JavaScript dependencies
npm install
# or
yarn install

# Start the web app
npm run dev
# or
yarn dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 4: Use DeepWiki!&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt; in your browser&lt;/li&gt; 
 &lt;li&gt;Enter a GitHub, GitLab, or Bitbucket repository (like &lt;code&gt;https://github.com/openai/codex&lt;/code&gt;, &lt;code&gt;https://github.com/microsoft/autogen&lt;/code&gt;, &lt;code&gt;https://gitlab.com/gitlab-org/gitlab&lt;/code&gt;, or &lt;code&gt;https://bitbucket.org/redradish/atlassian_app_versions&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;For private repositories, click "+ Add access tokens" and enter your GitHub or GitLab personal access token&lt;/li&gt; 
 &lt;li&gt;Click "Generate Wiki" and watch the magic happen!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ” How It Works&lt;/h2&gt; 
&lt;p&gt;DeepWiki uses AI to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone and analyze the GitHub, GitLab, or Bitbucket repository (including private repos with token authentication)&lt;/li&gt; 
 &lt;li&gt;Create embeddings of the code for smart retrieval&lt;/li&gt; 
 &lt;li&gt;Generate documentation with context-aware AI (using Google Gemini, OpenAI, OpenRouter, Azure OpenAI, or local Ollama models)&lt;/li&gt; 
 &lt;li&gt;Create visual diagrams to explain code relationships&lt;/li&gt; 
 &lt;li&gt;Organize everything into a structured wiki&lt;/li&gt; 
 &lt;li&gt;Enable intelligent Q&amp;amp;A with the repository through the Ask feature&lt;/li&gt; 
 &lt;li&gt;Provide in-depth research capabilities with DeepResearch&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;graph TD
    A[User inputs GitHub/GitLab/Bitbucket repo] --&amp;gt; AA{Private repo?}
    AA --&amp;gt;|Yes| AB[Add access token]
    AA --&amp;gt;|No| B[Clone Repository]
    AB --&amp;gt; B
    B --&amp;gt; C[Analyze Code Structure]
    C --&amp;gt; D[Create Code Embeddings]

    D --&amp;gt; M{Select Model Provider}
    M --&amp;gt;|Google Gemini| E1[Generate with Gemini]
    M --&amp;gt;|OpenAI| E2[Generate with OpenAI]
    M --&amp;gt;|OpenRouter| E3[Generate with OpenRouter]
    M --&amp;gt;|Local Ollama| E4[Generate with Ollama]
    M --&amp;gt;|Azure| E5[Generate with Azure]

    E1 --&amp;gt; E[Generate Documentation]
    E2 --&amp;gt; E
    E3 --&amp;gt; E
    E4 --&amp;gt; E
    E5 --&amp;gt; E

    D --&amp;gt; F[Create Visual Diagrams]
    E --&amp;gt; G[Organize as Wiki]
    F --&amp;gt; G
    G --&amp;gt; H[Interactive DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4,E5 process;
    class H result;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ› ï¸ Project Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;deepwiki/
â”œâ”€â”€ api/                  # Backend API server
â”‚   â”œâ”€â”€ main.py           # API entry point
â”‚   â”œâ”€â”€ api.py            # FastAPI implementation
â”‚   â”œâ”€â”€ rag.py            # Retrieval Augmented Generation
â”‚   â”œâ”€â”€ data_pipeline.py  # Data processing utilities
â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”‚
â”œâ”€â”€ src/                  # Frontend Next.js app
â”‚   â”œâ”€â”€ app/              # Next.js app directory
â”‚   â”‚   â””â”€â”€ page.tsx      # Main application page
â”‚   â””â”€â”€ components/       # React components
â”‚       â””â”€â”€ Mermaid.tsx   # Mermaid diagram renderer
â”‚
â”œâ”€â”€ public/               # Static assets
â”œâ”€â”€ package.json          # JavaScript dependencies
â””â”€â”€ .env                  # Environment variables (create this)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¤– Provider-Based Model Selection System&lt;/h2&gt; 
&lt;p&gt;DeepWiki now implements a flexible provider-based model selection system supporting multiple LLM providers:&lt;/p&gt; 
&lt;h3&gt;Supported Providers and Models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt;: Default &lt;code&gt;gemini-2.5-flash&lt;/code&gt;, also supports &lt;code&gt;gemini-2.5-flash-lite&lt;/code&gt;, &lt;code&gt;gemini-2.5-pro&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: Default &lt;code&gt;gpt-5-nano&lt;/code&gt;, also supports &lt;code&gt;gpt-5&lt;/code&gt;, &lt;code&gt;4o&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;: Access to multiple models via a unified API, including Claude, Llama, Mistral, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Azure OpenAI&lt;/strong&gt;: Default &lt;code&gt;gpt-4o&lt;/code&gt;, also supports &lt;code&gt;o4-mini&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: Support for locally running open-source models like &lt;code&gt;llama3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;Each provider requires its corresponding API key environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# API Keys
GOOGLE_API_KEY=your_google_api_key        # Required for Google Gemini models
OPENAI_API_KEY=your_openai_api_key        # Required for OpenAI models
OPENROUTER_API_KEY=your_openrouter_api_key # Required for OpenRouter models
AZURE_OPENAI_API_KEY=your_azure_openai_api_key  #Required for Azure OpenAI models
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint  #Required for Azure OpenAI models
AZURE_OPENAI_VERSION=your_azure_openai_version  #Required for Azure OpenAI models

# OpenAI API Base URL Configuration
OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # Optional, for custom OpenAI API endpoints

# Ollama host
OLLAMA_HOST=your_ollama_host # Optional, if Ollama is not local. default: http://localhost:11434

# Configuration Directory
DEEPWIKI_CONFIG_DIR=/path/to/custom/config/dir  # Optional, for custom config file location
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuration Files&lt;/h3&gt; 
&lt;p&gt;DeepWiki uses JSON configuration files to manage various aspects of the system:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;generator.json&lt;/code&gt;&lt;/strong&gt;: Configuration for text generation models&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Defines available model providers (Google, OpenAI, OpenRouter, Azure, Ollama)&lt;/li&gt; 
   &lt;li&gt;Specifies default and available models for each provider&lt;/li&gt; 
   &lt;li&gt;Contains model-specific parameters like temperature and top_p&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;embedder.json&lt;/code&gt;&lt;/strong&gt;: Configuration for embedding models and text processing&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Defines embedding models for vector storage&lt;/li&gt; 
   &lt;li&gt;Contains retriever configuration for RAG&lt;/li&gt; 
   &lt;li&gt;Specifies text splitter settings for document chunking&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;repo.json&lt;/code&gt;&lt;/strong&gt;: Configuration for repository handling&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Contains file filters to exclude certain files and directories&lt;/li&gt; 
   &lt;li&gt;Defines repository size limits and processing rules&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;By default, these files are located in the &lt;code&gt;api/config/&lt;/code&gt; directory. You can customize their location using the &lt;code&gt;DEEPWIKI_CONFIG_DIR&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h3&gt;Custom Model Selection for Service Providers&lt;/h3&gt; 
&lt;p&gt;The custom model selection feature is specifically designed for service providers who need to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can offer multiple AI model choices to users within your organization&lt;/li&gt; 
 &lt;li&gt;You can quickly adapt to the rapidly evolving LLM landscape without code changes&lt;/li&gt; 
 &lt;li&gt;You can support specialized or fine-tuned models that aren't in the predefined list&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Service providers can implement their model offerings by selecting from the predefined options or entering custom model identifiers in the frontend interface.&lt;/p&gt; 
&lt;h3&gt;Base URL Configuration for Enterprise Private Channels&lt;/h3&gt; 
&lt;p&gt;The OpenAI Client's base_url configuration is designed primarily for enterprise users with private API channels. This feature:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Enables connection to private or enterprise-specific API endpoints&lt;/li&gt; 
 &lt;li&gt;Allows organizations to use their own self-hosted or custom-deployed LLM services&lt;/li&gt; 
 &lt;li&gt;Supports integration with third-party OpenAI API-compatible services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Coming Soon&lt;/strong&gt;: In future updates, DeepWiki will support a mode where users need to provide their own API keys in requests. This will allow enterprise customers with private channels to use their existing API arrangements without sharing credentials with the DeepWiki deployment.&lt;/p&gt; 
&lt;h2&gt;ğŸ§© Using OpenAI-Compatible Embedding Models (e.g., Alibaba Qwen)&lt;/h2&gt; 
&lt;p&gt;If you want to use embedding models compatible with the OpenAI API (such as Alibaba Qwen), follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Replace the contents of &lt;code&gt;api/config/embedder.json&lt;/code&gt; with those from &lt;code&gt;api/config/embedder_openai_compatible.json&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;In your project root &lt;code&gt;.env&lt;/code&gt; file, set the relevant environment variables, for example: &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=your_api_key
OPENAI_BASE_URL=your_openai_compatible_endpoint
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;The program will automatically substitute placeholders in embedder.json with the values from your environment variables.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This allows you to seamlessly switch to any OpenAI-compatible embedding service without code changes.&lt;/p&gt; 
&lt;h2&gt;ğŸ§  Using Google AI Embeddings&lt;/h2&gt; 
&lt;p&gt;DeepWiki now supports Google AI's latest embedding models as an alternative to OpenAI embeddings. This provides better integration when you're already using Google Gemini models for text generation.&lt;/p&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Latest Model&lt;/strong&gt;: Uses Google's &lt;code&gt;text-embedding-004&lt;/code&gt; model&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Same API Key&lt;/strong&gt;: Uses your existing &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; (no additional setup required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Integration&lt;/strong&gt;: Optimized for use with Google Gemini text generation models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task-Specific&lt;/strong&gt;: Supports semantic similarity, retrieval, and classification tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Batch Processing&lt;/strong&gt;: Efficient processing of multiple texts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to Enable Google AI Embeddings&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable (Recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Set the embedder type in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Your existing Google API key
GOOGLE_API_KEY=your_google_api_key

# Enable Google AI embeddings
DEEPWIKI_EMBEDDER_TYPE=google
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: Docker Environment&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e DEEPWIKI_EMBEDDER_TYPE=google \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Docker Compose&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Add to your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GOOGLE_API_KEY=your_google_api_key
DEEPWIKI_EMBEDDER_TYPE=google
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Available Embedder Types&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;API Key Required&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI embeddings (default)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Uses &lt;code&gt;text-embedding-3-small&lt;/code&gt; model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;google&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Google AI embeddings&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Uses &lt;code&gt;text-embedding-004&lt;/code&gt; model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Local Ollama embeddings&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
   &lt;td&gt;Requires local Ollama installation&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Why Use Google AI Embeddings?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: If you're using Google Gemini for text generation, using Google embeddings provides better semantic consistency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Google's latest embedding model offers excellent performance for retrieval tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: Competitive pricing compared to OpenAI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Additional Setup&lt;/strong&gt;: Uses the same API key as your text generation models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Switching Between Embedders&lt;/h3&gt; 
&lt;p&gt;You can easily switch between different embedding providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use OpenAI embeddings (default)
export DEEPWIKI_EMBEDDER_TYPE=openai

# Use Google AI embeddings
export DEEPWIKI_EMBEDDER_TYPE=google

# Use local Ollama embeddings
export DEEPWIKI_EMBEDDER_TYPE=ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: When switching embedders, you may need to regenerate your repository embeddings as different models produce different vector spaces.&lt;/p&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;DeepWiki uses Python's built-in &lt;code&gt;logging&lt;/code&gt; module for diagnostic output. You can configure the verbosity and log file destination via environment variables:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LOG_LEVEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).&lt;/td&gt; 
   &lt;td&gt;INFO&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LOG_FILE_PATH&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the log file. If set, logs will be written to this file.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;api/logs/application.log&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To enable debug logging and direct logs to a custom file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export LOG_LEVEL=DEBUG
export LOG_FILE_PATH=./debug.log
python -m api.main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or with Docker Compose:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;LOG_LEVEL=DEBUG LOG_FILE_PATH=./debug.log docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When running with Docker Compose, the container's &lt;code&gt;api/logs&lt;/code&gt; directory is bind-mounted to &lt;code&gt;./api/logs&lt;/code&gt; on your host (see the &lt;code&gt;volumes&lt;/code&gt; section in &lt;code&gt;docker-compose.yml&lt;/code&gt;), ensuring log files persist across restarts.&lt;/p&gt; 
&lt;p&gt;Alternatively, you can store these settings in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;LOG_LEVEL=DEBUG
LOG_FILE_PATH=./debug.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Logging Path Security Considerations:&lt;/strong&gt; In production environments, ensure the &lt;code&gt;api/logs&lt;/code&gt; directory and any custom log file path are secured with appropriate filesystem permissions and access controls. The application enforces that &lt;code&gt;LOG_FILE_PATH&lt;/code&gt; resides within the project's &lt;code&gt;api/logs&lt;/code&gt; directory to prevent path traversal or unauthorized writes.&lt;/p&gt; 
&lt;h2&gt;ğŸ› ï¸ Advanced Setup&lt;/h2&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Google Gemini API key for AI generation and embeddings&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required for Google Gemini models and Google AI embeddings&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API key for embeddings and models&lt;/td&gt; 
   &lt;td&gt;Conditional&lt;/td&gt; 
   &lt;td&gt;Required if using OpenAI embeddings or models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API key for alternative models&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use OpenRouter models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI API key&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use Azure OpenAI models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI endpoint&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use Azure OpenAI models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI version&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use Azure OpenAI models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_HOST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama Host (default: &lt;a href="http://localhost:11434"&gt;http://localhost:11434&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use external Ollama server&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;DEEPWIKI_EMBEDDER_TYPE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Embedder type: &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;google&lt;/code&gt;, or &lt;code&gt;ollama&lt;/code&gt; (default: &lt;code&gt;openai&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Controls which embedding provider to use&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PORT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Port for the API server (default: 8001)&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;If you host API and frontend on the same machine, make sure change port of &lt;code&gt;SERVER_BASE_URL&lt;/code&gt; accordingly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SERVER_BASE_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Base URL for the API server (default: &lt;a href="http://localhost:8001"&gt;http://localhost:8001&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Set to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt; to enable authorization mode.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Defaults to &lt;code&gt;false&lt;/code&gt;. If enabled, &lt;code&gt;DEEPWIKI_AUTH_CODE&lt;/code&gt; is required.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;DEEPWIKI_AUTH_CODE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The secret code required for wiki generation when &lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt; is enabled.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Only used if &lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;API Key Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If using &lt;code&gt;DEEPWIKI_EMBEDDER_TYPE=openai&lt;/code&gt; (default): &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is required&lt;/li&gt; 
 &lt;li&gt;If using &lt;code&gt;DEEPWIKI_EMBEDDER_TYPE=google&lt;/code&gt;: &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; is required&lt;/li&gt; 
 &lt;li&gt;If using &lt;code&gt;DEEPWIKI_EMBEDDER_TYPE=ollama&lt;/code&gt;: No API key required (local processing)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Other API keys are only required when configuring and using models from the corresponding providers.&lt;/p&gt; 
&lt;h2&gt;Authorization Mode&lt;/h2&gt; 
&lt;p&gt;DeepWiki can be configured to run in an authorization mode, where wiki generation requires a valid authorization code. This is useful if you want to control who can use the generation feature. Restricts frontend initiation and protects cache deletion, but doesn't fully prevent backend generation if API endpoints are hit directly.&lt;/p&gt; 
&lt;p&gt;To enable authorization mode, set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt;: Set this to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;. When enabled, the frontend will display an input field for the authorization code.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DEEPWIKI_AUTH_CODE&lt;/code&gt;: Set this to the desired secret code. Restricts frontend initiation and protects cache deletion, but doesn't fully prevent backend generation if API endpoints are hit directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If &lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt; is not set or is set to &lt;code&gt;false&lt;/code&gt; (or any other value than &lt;code&gt;true&lt;/code&gt;/&lt;code&gt;1&lt;/code&gt;), the authorization feature will be disabled, and no code will be required.&lt;/p&gt; 
&lt;h3&gt;Docker Setup&lt;/h3&gt; 
&lt;p&gt;You can use Docker to run DeepWiki:&lt;/p&gt; 
&lt;h4&gt;Running the Container&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Pull the image from GitHub Container Registry
docker pull ghcr.io/asyncfuncai/deepwiki-open:latest

# Run the container with environment variables
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -e OLLAMA_HOST=your_ollama_host \
  -e AZURE_OPENAI_API_KEY=your_azure_openai_api_key \
  -e AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint \
  -e AZURE_OPENAI_VERSION=your_azure_openai_version \

  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command also mounts &lt;code&gt;~/.adalflow&lt;/code&gt; on your host to &lt;code&gt;/root/.adalflow&lt;/code&gt; in the container. This path is used to store:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cloned repositories (&lt;code&gt;~/.adalflow/repos/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Their embeddings and indexes (&lt;code&gt;~/.adalflow/databases/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Cached generated wiki content (&lt;code&gt;~/.adalflow/wikicache/&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This ensures that your data persists even if the container is stopped or removed.&lt;/p&gt; 
&lt;p&gt;Or use the provided &lt;code&gt;docker-compose.yml&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Edit the .env file with your API keys first
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(The &lt;code&gt;docker-compose.yml&lt;/code&gt; file is pre-configured to mount &lt;code&gt;~/.adalflow&lt;/code&gt; for data persistence, similar to the &lt;code&gt;docker run&lt;/code&gt; command above.)&lt;/p&gt; 
&lt;h4&gt;Using a .env file with Docker&lt;/h4&gt; 
&lt;p&gt;You can also mount a .env file to the container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a .env file with your API keys
echo "GOOGLE_API_KEY=your_google_api_key" &amp;gt; .env
echo "OPENAI_API_KEY=your_openai_api_key" &amp;gt;&amp;gt; .env
echo "OPENROUTER_API_KEY=your_openrouter_api_key" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_API_KEY=your_azure_openai_api_key" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_VERSION=your_azure_openai_version"  &amp;gt;&amp;gt;.env
echo "OLLAMA_HOST=your_ollama_host" &amp;gt;&amp;gt; .env

# Run the container with the .env file mounted
docker run -p 8001:8001 -p 3000:3000 \
  -v $(pwd)/.env:/app/.env \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command also mounts &lt;code&gt;~/.adalflow&lt;/code&gt; on your host to &lt;code&gt;/root/.adalflow&lt;/code&gt; in the container. This path is used to store:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cloned repositories (&lt;code&gt;~/.adalflow/repos/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Their embeddings and indexes (&lt;code&gt;~/.adalflow/databases/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Cached generated wiki content (&lt;code&gt;~/.adalflow/wikicache/&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This ensures that your data persists even if the container is stopped or removed.&lt;/p&gt; 
&lt;h4&gt;Building the Docker image locally&lt;/h4&gt; 
&lt;p&gt;If you want to build the Docker image locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Build the Docker image
docker build -t deepwiki-open .

# Run the container
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -e AZURE_OPENAI_API_KEY=your_azure_openai_api_key \
  -e AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint \
  -e AZURE_OPENAI_VERSION=your_azure_openai_version \
  -e OLLAMA_HOST=your_ollama_host \
  deepwiki-open
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Self-Signed Certificates in Docker&lt;/h4&gt; 
&lt;p&gt;If you're in an environment that uses self-signed certificates, you can include them in the Docker build:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a directory for your certificates (default is &lt;code&gt;certs&lt;/code&gt; in your project root)&lt;/li&gt; 
 &lt;li&gt;Copy your &lt;code&gt;.crt&lt;/code&gt; or &lt;code&gt;.pem&lt;/code&gt; certificate files into this directory&lt;/li&gt; 
 &lt;li&gt;Build the Docker image:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build with default certificates directory (certs)
docker build .

# Or build with a custom certificates directory
docker build --build-arg CUSTOM_CERT_DIR=my-custom-certs .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;API Server Details&lt;/h3&gt; 
&lt;p&gt;The API server provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Repository cloning and indexing&lt;/li&gt; 
 &lt;li&gt;RAG (Retrieval Augmented Generation)&lt;/li&gt; 
 &lt;li&gt;Streaming chat completions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more details, see the &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/api/README.md"&gt;API README&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ”Œ OpenRouter Integration&lt;/h2&gt; 
&lt;p&gt;DeepWiki now supports &lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt; as a model provider, giving you access to hundreds of AI models through a single API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Model Options&lt;/strong&gt;: Access models from OpenAI, Anthropic, Google, Meta, Mistral, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simple Configuration&lt;/strong&gt;: Just add your OpenRouter API key and select the model you want to use&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: Choose models that fit your budget and performance needs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Switching&lt;/strong&gt;: Toggle between different models without changing your code&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to Use OpenRouter with DeepWiki&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Get an API Key&lt;/strong&gt;: Sign up at &lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt; and get your API key&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add to Environment&lt;/strong&gt;: Add &lt;code&gt;OPENROUTER_API_KEY=your_key&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enable in UI&lt;/strong&gt;: Check the "Use OpenRouter API" option on the homepage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Select Model&lt;/strong&gt;: Choose from popular models like GPT-4o, Claude 3.5 Sonnet, Gemini 2.0, and more&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;OpenRouter is particularly useful if you want to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try different models without signing up for multiple services&lt;/li&gt; 
 &lt;li&gt;Access models that might be restricted in your region&lt;/li&gt; 
 &lt;li&gt;Compare performance across different model providers&lt;/li&gt; 
 &lt;li&gt;Optimize for cost vs. performance based on your needs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤– Ask &amp;amp; DeepResearch Features&lt;/h2&gt; 
&lt;h3&gt;Ask Feature&lt;/h3&gt; 
&lt;p&gt;The Ask feature allows you to chat with your repository using Retrieval Augmented Generation (RAG):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Context-Aware Responses&lt;/strong&gt;: Get accurate answers based on the actual code in your repository&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG-Powered&lt;/strong&gt;: The system retrieves relevant code snippets to provide grounded responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Streaming&lt;/strong&gt;: See responses as they're generated for a more interactive experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversation History&lt;/strong&gt;: The system maintains context between questions for more coherent interactions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;DeepResearch Feature&lt;/h3&gt; 
&lt;p&gt;DeepResearch takes repository analysis to the next level with a multi-turn research process:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;In-Depth Investigation&lt;/strong&gt;: Thoroughly explores complex topics through multiple research iterations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structured Process&lt;/strong&gt;: Follows a clear research plan with updates and a comprehensive conclusion&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Continuation&lt;/strong&gt;: The AI automatically continues research until reaching a conclusion (up to 5 iterations)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Research Stages&lt;/strong&gt;: 
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Research Plan&lt;/strong&gt;: Outlines the approach and initial findings&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Research Updates&lt;/strong&gt;: Builds on previous iterations with new insights&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Final Conclusion&lt;/strong&gt;: Provides a comprehensive answer based on all iterations&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use DeepResearch, simply toggle the "Deep Research" switch in the Ask interface before submitting your question.&lt;/p&gt; 
&lt;h2&gt;ğŸ“± Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/Interface.png" alt="DeepWiki Main Interface" /&gt; &lt;em&gt;The main interface of DeepWiki&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/privaterepo.png" alt="Private Repository Support" /&gt; &lt;em&gt;Access private repositories with personal access tokens&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/DeepResearch.png" alt="DeepResearch Feature" /&gt; &lt;em&gt;DeepResearch conducts multi-turn investigations for complex topics&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Demo Video&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/zGANs8US8B4"&gt;&lt;img src="https://img.youtube.com/vi/zGANs8US8B4/0.jpg" alt="DeepWiki Demo Video" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Watch DeepWiki in action!&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;â“ Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;API Key Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"Missing environment variables"&lt;/strong&gt;: Make sure your &lt;code&gt;.env&lt;/code&gt; file is in the project root and contains the required API keys&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"API key not valid"&lt;/strong&gt;: Check that you've copied the full key correctly with no extra spaces&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"OpenRouter API error"&lt;/strong&gt;: Verify your OpenRouter API key is valid and has sufficient credits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"Azure OpenAI API error"&lt;/strong&gt;: Verify your Azure OpenAI credentials (API key, endpoint, and version) are correct and the service is properly deployed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Connection Problems&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"Cannot connect to API server"&lt;/strong&gt;: Make sure the API server is running on port 8001&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"CORS error"&lt;/strong&gt;: The API is configured to allow all origins, but if you're having issues, try running both frontend and backend on the same machine&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Generation Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"Error generating wiki"&lt;/strong&gt;: For very large repositories, try a smaller one first&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"Invalid repository format"&lt;/strong&gt;: Make sure you're using a valid GitHub, GitLab or Bitbucket URL format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"Could not fetch repository structure"&lt;/strong&gt;: For private repositories, ensure you've entered a valid personal access token with appropriate permissions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"Diagram rendering error"&lt;/strong&gt;: The app will automatically try to fix broken diagrams&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Common Solutions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Restart both servers&lt;/strong&gt;: Sometimes a simple restart fixes most issues&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check console logs&lt;/strong&gt;: Open browser developer tools to see any JavaScript errors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check API logs&lt;/strong&gt;: Look at the terminal where the API is running for Python errors&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open issues for bugs or feature requests&lt;/li&gt; 
 &lt;li&gt;Submit pull requests to improve the code&lt;/li&gt; 
 &lt;li&gt;Share your feedback and ideas&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;â­ Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#AsyncFuncAI/deepwiki-open&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ“š ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;Docker&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;GPU options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-pre-built-docker-container"&gt;Docker Run&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Docker Build&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide"&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations"&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“š Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;ğŸ™ï¸ High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; (and more).&lt;/li&gt; 
 &lt;li&gt;ğŸ—£ï¸ Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;ğŸŒ Supports +1110 languages (English by default). &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ–¥ï¸ Designed to run on 4GB RAM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1100 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4gb RAM minimum, 8GB recommended&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU) *available very soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd  # Run launch script or double click on it
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual Python Install&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# (for experts only!)
REQUIRED_PROGRAMS=("calibre" "ffmpeg" "nodejs" "mecab" "espeak-ng" "rust" "sox")
REQUIRED_PYTHON_VERSION="3.12"
pip install -r requirements.txt  # Install Python Requirements
python app.py  # Run Ebook2Audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;python app.py --share&lt;/code&gt; (all OS) &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; \
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt;
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file'
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file'
    
Tip: to add of silence (1.4 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] equals to 1.4 seconds&lt;/p&gt; 
&lt;h4&gt;Docker GPU Options&lt;/h4&gt; 
&lt;p&gt;Available pre-build tags: &lt;code&gt;latest&lt;/code&gt; (CUDA 11.8)&lt;/p&gt; 
&lt;h4&gt;Edit: IF GPU isn't detected then you'll have to build the image -&amp;gt; &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Building the Docker Container&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Running the pre-built Docker Container&lt;/h4&gt; 
&lt;p&gt;-Run with CPU only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;-Run with GPU Speedup (NVIDIA compatible only)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more options add the parameter &lt;code&gt;--help&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Building the Docker Container&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can build the docker image with the command:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker build -t athomasson2/ebook2audiobook .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Avalible Docker Build Arguments&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;--build-arg TORCH_VERSION=cuda118&lt;/code&gt; Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu]&lt;/p&gt; 
&lt;p&gt;All CUDA version numbers should work, Ex: CUDA 11.6-&amp;gt; cuda116&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--build-arg SKIP_XTTS_TEST=true&lt;/code&gt; (Saves space by not baking XTTSv2 model into docker image)&lt;/p&gt; 
&lt;h2&gt;Docker container file locations&lt;/h2&gt; 
&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/app/audiobooks&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Docker headless guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Before you do run this you need to create a dir named "input-folder" in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm athomasson2/ebook2audiobook --help

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That will output this &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Steps to Run&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven't already): &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start the service:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Docker
docker-compose up -d # To update add --build

# Podman
podman compose -f podman-compose.yml up -d # To update add --build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href="http://localhost:7860"&gt;http://localhost:7860&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common Docker Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;python: can't open file '/home/user/app/app.py': [Errno 2] No such file or directory&lt;/code&gt; (Just remove all post arguments as I replaced the &lt;code&gt;CMD&lt;/code&gt; with &lt;code&gt;ENTRYPOINT&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Example: &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker&lt;/code&gt; - &amp;gt; corrected - &amp;gt; &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arguments can be easily added like this now &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook --share&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/191"&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creates a &lt;code&gt;['m4b', 'm4a', 'mp4', 'webm', 'mov', 'mp3', 'flac', 'wav', 'ogg', 'aac']&lt;/code&gt; (set in ./lib/conf.py) file with metadata and chapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Updating to Latest Version&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git pull # Locally/Compose

docker pull athomasson2/ebook2audiobook:latest # For Pre-build docker images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7

athomasson2/ebook2audiobook:VERSION_NUM # For Pre-build docker images -&amp;gt; Example: athomasson2/ebook2audiobook:v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while NVIDIA GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.ğŸ˜Š&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! ğŸ™Œ&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Do you need to rent a GPU to boost service from us?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A poll is open here &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/889"&gt;https://github.com/DrewThomasson/ebook2audiobook/discussions/889&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>guofei9987/scikit-opt</title>
      <link>https://github.com/guofei9987/scikit-opt</link>
      <description>&lt;p&gt;Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing, Ant Colony Optimization Algorithm,Immune Algorithm, Artificial Fish Swarm Algorithm, Differential Evolution and TSP(Traveling salesman)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href="https://github.com/guofei9987/scikit-opt"&gt;scikit-opt&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/scikit-opt/"&gt;&lt;img src="https://img.shields.io/pypi/v/scikit-opt" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://travis-ci.com/guofei9987/scikit-opt"&gt;&lt;img src="https://travis-ci.com/guofei9987/scikit-opt.svg?branch=master" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/guofei9987/scikit-opt"&gt;&lt;img src="https://codecov.io/gh/guofei9987/scikit-opt/branch/master/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/scikit-opt.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-%3E=3.5-green.svg?sanitize=true" alt="Python" /&gt; &lt;img src="https://img.shields.io/badge/platform-windows%20%7C%20linux%20%7C%20macos-green.svg?sanitize=true" alt="Platform" /&gt; &lt;a href="https://github.com/guofei9987/scikit-opt/fork"&gt;&lt;img src="https://img.shields.io/github/forks/guofei9987/scikit-opt?style=social" alt="fork" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/scikit-opt"&gt;&lt;img src="https://pepy.tech/badge/scikit-opt" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/scikit-opt/discussions"&gt;&lt;img src="https://img.shields.io/badge/discussions-green.svg?sanitize=true" alt="Discussions" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/repository/guofei9987/scikit-opt" target="_blank"&gt;&lt;img src="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=6763d615842e4449a02f024f3e2e345c&amp;amp;claim_uid=se0WHo8cbiLv2w1&amp;amp;theme=small" alt="Featuredï½œHelloGitHub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Swarm Intelligence in Python&lt;br /&gt; (Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing, Ant Colony Algorithm, Immune Algorithm, Artificial Fish Swarm Algorithm in Python)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://scikit-opt.github.io/scikit-opt/#/en/"&gt;https://scikit-opt.github.io/scikit-opt/#/en/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ–‡æ¡£ï¼š&lt;/strong&gt; &lt;a href="https://scikit-opt.github.io/scikit-opt/#/zh/"&gt;https://scikit-opt.github.io/scikit-opt/#/zh/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source code:&lt;/strong&gt; &lt;a href="https://github.com/guofei9987/scikit-opt"&gt;https://github.com/guofei9987/scikit-opt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Help us improve scikit-opt&lt;/strong&gt; &lt;a href="https://www.wjx.cn/jq/50964691.aspx"&gt;https://www.wjx.cn/jq/50964691.aspx&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;install&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install scikit-opt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the current developer version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bach"&gt;git clone git@github.com:guofei9987/scikit-opt.git
cd scikit-opt
pip install .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;h2&gt;Feature1: UDF&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;UDF&lt;/strong&gt; (user defined function) is available now!&lt;/p&gt; 
&lt;p&gt;For example, you just worked out a new type of &lt;code&gt;selection&lt;/code&gt; function.&lt;br /&gt; Now, your &lt;code&gt;selection&lt;/code&gt; function is like this:&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_udf.py#L1"&gt;examples/demo_ga_udf.py#s1&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# step1: define your own operator:
def selection_tournament(algorithm, tourn_size):
    FitV = algorithm.FitV
    sel_index = []
    for i in range(algorithm.size_pop):
        aspirants_index = np.random.choice(range(algorithm.size_pop), size=tourn_size)
        sel_index.append(max(aspirants_index, key=lambda i: FitV[i]))
    algorithm.Chrom = algorithm.Chrom[sel_index, :]  # next generation
    return algorithm.Chrom


&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Import and build ga&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_udf.py#L12"&gt;examples/demo_ga_udf.py#s2&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
from sko.GA import GA, GA_TSP

demo_func = lambda x: x[0] ** 2 + (x[1] - 0.05) ** 2 + (x[2] - 0.5) ** 2
ga = GA(func=demo_func, n_dim=3, size_pop=100, max_iter=500, prob_mut=0.001,
        lb=[-1, -10, -5], ub=[2, 10, 2], precision=[1e-7, 1e-7, 1])

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Regist your udf to GA&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_udf.py#L20"&gt;examples/demo_ga_udf.py#s3&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ga.register(operator_name='selection', operator=selection_tournament, tourn_size=3)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;scikit-opt also provide some operators&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_udf.py#L22"&gt;examples/demo_ga_udf.py#s4&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sko.operators import ranking, selection, crossover, mutation

ga.register(operator_name='ranking', operator=ranking.ranking). \
    register(operator_name='crossover', operator=crossover.crossover_2point). \
    register(operator_name='mutation', operator=mutation.mutation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now do GA as usual&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_udf.py#L28"&gt;examples/demo_ga_udf.py#s5&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;best_x, best_y = ga.run()
print('best_x:', best_x, '\n', 'best_y:', best_y)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Until Now, the &lt;strong&gt;udf&lt;/strong&gt; surport &lt;code&gt;crossover&lt;/code&gt;, &lt;code&gt;mutation&lt;/code&gt;, &lt;code&gt;selection&lt;/code&gt;, &lt;code&gt;ranking&lt;/code&gt; of GA scikit-opt provide a dozen of operators, see &lt;a href="https://github.com/guofei9987/scikit-opt/tree/master/sko/operators"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For advanced users:&lt;/p&gt; 
&lt;p&gt;-&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_udf.py#L31"&gt;examples/demo_ga_udf.py#s6&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class MyGA(GA):
    def selection(self, tourn_size=3):
        FitV = self.FitV
        sel_index = []
        for i in range(self.size_pop):
            aspirants_index = np.random.choice(range(self.size_pop), size=tourn_size)
            sel_index.append(max(aspirants_index, key=lambda i: FitV[i]))
        self.Chrom = self.Chrom[sel_index, :]  # next generation
        return self.Chrom

    ranking = ranking.ranking


demo_func = lambda x: x[0] ** 2 + (x[1] - 0.05) ** 2 + (x[2] - 0.5) ** 2
my_ga = MyGA(func=demo_func, n_dim=3, size_pop=100, max_iter=500, lb=[-1, -10, -5], ub=[2, 10, 2],
             precision=[1e-7, 1e-7, 1])
best_x, best_y = my_ga.run()
print('best_x:', best_x, '\n', 'best_y:', best_y)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;feature2: continue to run&lt;/h2&gt; 
&lt;p&gt;(New in version 0.3.6)&lt;br /&gt; Run an algorithm for 10 iterations, and then run another 20 iterations base on the 10 iterations before:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sko.GA import GA

func = lambda x: x[0] ** 2
ga = GA(func=func, n_dim=1)
ga.run(10)
ga.run(20)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;feature3: 4-ways to accelerate&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;vectorization&lt;/li&gt; 
 &lt;li&gt;multithreading&lt;/li&gt; 
 &lt;li&gt;multiprocessing&lt;/li&gt; 
 &lt;li&gt;cached&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;see &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/example_function_modes.py"&gt;https://github.com/guofei9987/scikit-opt/blob/master/examples/example_function_modes.py&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;feature4: GPU computation&lt;/h2&gt; 
&lt;p&gt;We are developing GPU computation, which will be stable on version 1.0.0&lt;br /&gt; An example is already available: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_gpu.py"&gt;https://github.com/guofei9987/scikit-opt/blob/master/examples/demo_ga_gpu.py&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Quick start&lt;/h1&gt; 
&lt;h2&gt;1. Differential Evolution&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Step1&lt;/strong&gt;ï¼šdefine your problem&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_de.py#L1"&gt;examples/demo_de.py#s1&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;'''
min f(x1, x2, x3) = x1^2 + x2^2 + x3^2
s.t.
    x1*x2 &amp;gt;= 1
    x1*x2 &amp;lt;= 5
    x2 + x3 = 1
    0 &amp;lt;= x1, x2, x3 &amp;lt;= 5
'''


def obj_func(p):
    x1, x2, x3 = p
    return x1 ** 2 + x2 ** 2 + x3 ** 2


constraint_eq = [
    lambda x: 1 - x[1] - x[2]
]

constraint_ueq = [
    lambda x: 1 - x[0] * x[1],
    lambda x: x[0] * x[1] - 5
]

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step2&lt;/strong&gt;: do Differential Evolution&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_de.py#L25"&gt;examples/demo_de.py#s2&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sko.DE import DE

de = DE(func=obj_func, n_dim=3, size_pop=50, max_iter=800, lb=[0, 0, 0], ub=[5, 5, 5],
        constraint_eq=constraint_eq, constraint_ueq=constraint_ueq)

best_x, best_y = de.run()
print('best_x:', best_x, '\n', 'best_y:', best_y)

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2. Genetic Algorithm&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Step1&lt;/strong&gt;ï¼šdefine your problem&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga.py#L1"&gt;examples/demo_ga.py#s1&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np


def schaffer(p):
    '''
    This function has plenty of local minimum, with strong shocks
    global minimum at (0,0) with value 0
    https://en.wikipedia.org/wiki/Test_functions_for_optimization
    '''
    x1, x2 = p
    part1 = np.square(x1) - np.square(x2)
    part2 = np.square(x1) + np.square(x2)
    return 0.5 + (np.square(np.sin(part1)) - 0.5) / np.square(1 + 0.001 * part2)


&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step2&lt;/strong&gt;: do Genetic Algorithm&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga.py#L16"&gt;examples/demo_ga.py#s2&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sko.GA import GA

ga = GA(func=schaffer, n_dim=2, size_pop=50, max_iter=800, prob_mut=0.001, lb=[-1, -1], ub=[1, 1], precision=1e-7)
best_x, best_y = ga.run()
print('best_x:', best_x, '\n', 'best_y:', best_y)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;-&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga.py#L22"&gt;examples/demo_ga.py#s3&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
import matplotlib.pyplot as plt

Y_history = pd.DataFrame(ga.all_history_Y)
fig, ax = plt.subplots(2, 1)
ax[0].plot(Y_history.index, Y_history.values, '.', color='red')
Y_history.min(axis=1).cummin().plot(kind='line')
plt.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://img1.github.io/heuristic_algorithm/ga_1.png" alt="Figure_1-1" /&gt;&lt;/p&gt; 
&lt;h3&gt;2.2 Genetic Algorithm for TSP(Travelling Salesman Problem)&lt;/h3&gt; 
&lt;p&gt;Just import the &lt;code&gt;GA_TSP&lt;/code&gt;, it overloads the &lt;code&gt;crossover&lt;/code&gt;, &lt;code&gt;mutation&lt;/code&gt; to solve the TSP&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step1&lt;/strong&gt;: define your problem. Prepare your points coordinate and the distance matrix.&lt;br /&gt; Here I generate the data randomly as a demo:&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_tsp.py#L1"&gt;examples/demo_ga_tsp.py#s1&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
from scipy import spatial
import matplotlib.pyplot as plt

num_points = 50

points_coordinate = np.random.rand(num_points, 2)  # generate coordinate of points
distance_matrix = spatial.distance.cdist(points_coordinate, points_coordinate, metric='euclidean')


def cal_total_distance(routine):
    '''The objective function. input routine, return total distance.
    cal_total_distance(np.arange(num_points))
    '''
    num_points, = routine.shape
    return sum([distance_matrix[routine[i % num_points], routine[(i + 1) % num_points]] for i in range(num_points)])


&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step2&lt;/strong&gt;: do GA&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_tsp.py#L19"&gt;examples/demo_ga_tsp.py#s2&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;
from sko.GA import GA_TSP

ga_tsp = GA_TSP(func=cal_total_distance, n_dim=num_points, size_pop=50, max_iter=500, prob_mut=1)
best_points, best_distance = ga_tsp.run()

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step3&lt;/strong&gt;: Plot the result:&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ga_tsp.py#L26"&gt;examples/demo_ga_tsp.py#s3&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;fig, ax = plt.subplots(1, 2)
best_points_ = np.concatenate([best_points, [best_points[0]]])
best_points_coordinate = points_coordinate[best_points_, :]
ax[0].plot(best_points_coordinate[:, 0], best_points_coordinate[:, 1], 'o-r')
ax[1].plot(ga_tsp.generation_best_Y)
plt.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://img1.github.io/heuristic_algorithm/ga_tsp.png" alt="GA_TPS" /&gt;&lt;/p&gt; 
&lt;h2&gt;3. PSO(Particle swarm optimization)&lt;/h2&gt; 
&lt;h3&gt;3.1 PSO&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Step1&lt;/strong&gt;: define your problem:&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_pso.py#L1"&gt;examples/demo_pso.py#s1&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def demo_func(x):
    x1, x2, x3 = x
    return x1 ** 2 + (x2 - 0.05) ** 2 + x3 ** 2


&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step2&lt;/strong&gt;: do PSO&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_pso.py#L6"&gt;examples/demo_pso.py#s2&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sko.PSO import PSO

pso = PSO(func=demo_func, n_dim=3, pop=40, max_iter=150, lb=[0, -1, 0.5], ub=[1, 1, 1], w=0.8, c1=0.5, c2=0.5)
pso.run()
print('best_x is ', pso.gbest_x, 'best_y is', pso.gbest_y)

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step3&lt;/strong&gt;: Plot the result&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_pso.py#L13"&gt;examples/demo_pso.py#s3&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import matplotlib.pyplot as plt

plt.plot(pso.gbest_y_hist)
plt.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://img1.github.io/heuristic_algorithm/pso.png" alt="PSO_TPS" /&gt;&lt;/p&gt; 
&lt;h3&gt;3.2 PSO with nonlinear constraint&lt;/h3&gt; 
&lt;p&gt;If you need nolinear constraint like &lt;code&gt;(x[0] - 1) ** 2 + (x[1] - 0) ** 2 - 0.5 ** 2&amp;lt;=0&lt;/code&gt;&lt;br /&gt; Codes are like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;constraint_ueq = (
    lambda x: (x[0] - 1) ** 2 + (x[1] - 0) ** 2 - 0.5 ** 2
    ,
)
pso = PSO(func=demo_func, n_dim=2, pop=40, max_iter=max_iter, lb=[-2, -2], ub=[2, 2]
          , constraint_ueq=constraint_ueq)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that, you can add more then one nonlinear constraint. Just add it to &lt;code&gt;constraint_ueq&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;More over, we have an animation:&lt;br /&gt; &lt;img src="https://img1.github.io/heuristic_algorithm/pso.gif" alt="pso_ani" /&gt;&lt;br /&gt; â†‘&lt;strong&gt;see &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_pso_ani.py"&gt;examples/demo_pso_ani.py&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;4. SA(Simulated Annealing)&lt;/h2&gt; 
&lt;h3&gt;4.1 SA for multiple function&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Step1&lt;/strong&gt;: define your problem&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_sa.py#L1"&gt;examples/demo_sa.py#s1&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;demo_func = lambda x: x[0] ** 2 + (x[1] - 0.05) ** 2 + x[2] ** 2

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step2&lt;/strong&gt;: do SA&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_sa.py#L3"&gt;examples/demo_sa.py#s2&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sko.SA import SA

sa = SA(func=demo_func, x0=[1, 1, 1], T_max=1, T_min=1e-9, L=300, max_stay_counter=150)
best_x, best_y = sa.run()
print('best_x:', best_x, 'best_y', best_y)

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step3&lt;/strong&gt;: Plot the result&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_sa.py#L10"&gt;examples/demo_sa.py#s3&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import matplotlib.pyplot as plt
import pandas as pd

plt.plot(pd.DataFrame(sa.best_y_history).cummin(axis=0))
plt.show()

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://img1.github.io/heuristic_algorithm/sa.png" alt="sa" /&gt;&lt;/p&gt; 
&lt;p&gt;Moreover, scikit-opt provide 3 types of Simulated Annealing: Fast, Boltzmann, Cauchy. See &lt;a href="https://scikit-opt.github.io/scikit-opt/#/en/more_sa"&gt;more sa&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;4.2 SA for TSP&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Step1&lt;/strong&gt;: oh, yes, define your problems. To boring to copy this step.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step2&lt;/strong&gt;: DO SA for TSP&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_sa_tsp.py#L21"&gt;examples/demo_sa_tsp.py#s2&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sko.SA import SA_TSP

sa_tsp = SA_TSP(func=cal_total_distance, x0=range(num_points), T_max=100, T_min=1, L=10 * num_points)

best_points, best_distance = sa_tsp.run()
print(best_points, best_distance, cal_total_distance(best_points))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step3&lt;/strong&gt;: plot the result&lt;br /&gt; -&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_sa_tsp.py#L28"&gt;examples/demo_sa_tsp.py#s3&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from matplotlib.ticker import FormatStrFormatter

fig, ax = plt.subplots(1, 2)

best_points_ = np.concatenate([best_points, [best_points[0]]])
best_points_coordinate = points_coordinate[best_points_, :]
ax[0].plot(sa_tsp.best_y_history)
ax[0].set_xlabel("Iteration")
ax[0].set_ylabel("Distance")
ax[1].plot(best_points_coordinate[:, 0], best_points_coordinate[:, 1],
           marker='o', markerfacecolor='b', color='c', linestyle='-')
ax[1].xaxis.set_major_formatter(FormatStrFormatter('%.3f'))
ax[1].yaxis.set_major_formatter(FormatStrFormatter('%.3f'))
ax[1].set_xlabel("Longitude")
ax[1].set_ylabel("Latitude")
plt.show()

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://img1.github.io/heuristic_algorithm/sa_tsp.png" alt="sa" /&gt;&lt;/p&gt; 
&lt;p&gt;More: Plot the animation:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img1.github.io/heuristic_algorithm/sa_tsp1.gif" alt="sa" /&gt;&lt;br /&gt; â†‘&lt;strong&gt;see &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_sa_tsp.py"&gt;examples/demo_sa_tsp.py&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;5. ACA (Ant Colony Algorithm) for tsp&lt;/h2&gt; 
&lt;p&gt;-&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_aca_tsp.py#L17"&gt;examples/demo_aca_tsp.py#s2&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from sko.ACA import ACA_TSP

aca = ACA_TSP(func=cal_total_distance, n_dim=num_points,
              size_pop=50, max_iter=200,
              distance_matrix=distance_matrix)

best_x, best_y = aca.run()

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://img1.github.io/heuristic_algorithm/aca_tsp.png" alt="ACA" /&gt;&lt;/p&gt; 
&lt;h2&gt;6. immune algorithm (IA)&lt;/h2&gt; 
&lt;p&gt;-&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_ia.py#L6"&gt;examples/demo_ia.py#s2&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;
from sko.IA import IA_TSP

ia_tsp = IA_TSP(func=cal_total_distance, n_dim=num_points, size_pop=500, max_iter=800, prob_mut=0.2,
                T=0.7, alpha=0.95)
best_points, best_distance = ia_tsp.run()
print('best routine:', best_points, 'best_distance:', best_distance)

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://img1.github.io/heuristic_algorithm/ia2.png" alt="IA" /&gt;&lt;/p&gt; 
&lt;h2&gt;7. Artificial Fish Swarm Algorithm (AFSA)&lt;/h2&gt; 
&lt;p&gt;-&amp;gt; Demo code: &lt;a href="https://github.com/guofei9987/scikit-opt/raw/master/examples/demo_afsa.py#L1"&gt;examples/demo_afsa.py#s1&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def func(x):
    x1, x2 = x
    return 1 / x1 ** 2 + x1 ** 2 + 1 / x2 ** 2 + x2 ** 2


from sko.AFSA import AFSA

afsa = AFSA(func, n_dim=2, size_pop=50, max_iter=300,
            max_try_num=100, step=0.5, visual=0.3,
            q=0.98, delta=0.5)
best_x, best_y = afsa.run()
print(best_x, best_y)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Projects using scikit-opt&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ieeexplore.ieee.org/abstract/document/9607026/"&gt;Yu, J., He, Y., Yan, Q., &amp;amp; Kang, X. (2021). SpecView: Malware Spectrum Visualization Framework With Singular Spectrum Transformation. IEEE Transactions on Information Forensics and Security, 16, 5093-5107.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sciencedirect.com/science/article/pii/S2352484721012737"&gt;Zhen, H., Zhai, H., Ma, W., Zhao, L., Weng, Y., Xu, Y., ... &amp;amp; He, X. (2021). Design and tests of reinforcement-learning-based optimal power flow solution generator. Energy Reports.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sciencedirect.com/science/article/pii/S016792362100004X"&gt;Heinrich, K., Zschech, P., Janiesch, C., &amp;amp; Bonin, M. (2021). Process data properties matter: Introducing gated convolutional neural networks (GCNN) and key-value-predict attention networks (KVP) for next event prediction with deep learning. Decision Support Systems, 143, 113494.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2104.08564"&gt;Tang, H. K., &amp;amp; Goh, S. K. (2021). A Novel Non-population-based Meta-heuristic Optimizer Inspired by the Philosophy of Yi Jing. arXiv preprint arXiv:2104.08564.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://link.springer.com/article/10.1007/s11280-021-00934-y"&gt;Wu, G., Li, L., Li, X., Chen, Y., Chen, Z., Qiao, B., ... &amp;amp; Xia, L. (2021). Graph embedding based real-time social event matching for EBSNs recommendation. World Wide Web, 1-22.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0925400521005517"&gt;Pan, X., Zhang, Z., Zhang, H., Wen, Z., Ye, W., Yang, Y., ... &amp;amp; Zhao, X. (2021). A fast and robust mixture gases identification and concentration detection algorithm based on attention mechanism equipped recurrent neural network with double loss function. Sensors and Actuators B: Chemical, 342, 129982.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://upcommons.upc.edu/handle/2117/350262"&gt;Castella Balcell, M. (2021). Optimization of the station keeping system for the WindCrete floating offshore wind turbine.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2107.14406"&gt;Zhai, B., Wang, Y., Wang, W., &amp;amp; Wu, B. (2021). Optimal Variable Speed Limit Control Strategy on Freeway Segments under Fog Conditions. arXiv preprint arXiv:2107.14406.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://etd.ohiolink.edu/apexprod/rws_olink/r/1501/10?clear=10&amp;amp;p10_accession_num=wright162901936395651"&gt;Yap, X. H. (2021). Multi-label classification on locally-linear data: Application to chemical toxicity prediction.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ad-publications.cs.uni-freiburg.de/theses/Master_Lukas_Gebhard_2021.pdf"&gt;Gebhard, L. (2021). Expansion Planning of Low-Voltage Grids Using Ant Colony Optimization Ausbauplanung von Niederspannungsnetzen mithilfe eines Ameisenalgorithmus.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ieeexplore.ieee.org/abstract/document/9585654"&gt;Ma, X., Zhou, H., &amp;amp; Li, Z. (2021). Optimal Design for Interdependencies between Hydrogen and Power Systems. IEEE Transactions on Industry Applications.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://d1wqtxts1xzle7.cloudfront.net/67649721/TCC_Thibor_Final-with-cover-page-v2.pdf?Expires=1639140872&amp;amp;Signature=LDZoVsAGO0mLMlVsQjnzpLlRhLyt5wdIDmBjm1yWog5bsx6apyRE9aHuwfnFnc96uvam573wiHMeV08QlK2vhRcQS1d0buenBT5fwoRuq6PTDoMsXmpBb-lGtu9ETiMb4sBYvcQb-X3C7Hh0Ec1FoJZ040gXJPWdAli3e1TdOcGrnOaBZMgNiYX6aKFIZaaXmiQeV3418~870bH4IOQXOapIE6-23lcOL-32T~FSjsOrENoLUkcosv6UHPourKgsRufAY-C2HBUWP36iJ7CoH0jSTo1e45dVgvqNDvsHz7tmeI~0UPGH-A8MWzQ9h2ElCbCN~UNQ8ycxOa4TUKfpCw__&amp;amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"&gt;de Curso, T. D. C. (2021). Estudo do modelo Johansen-Ledoit-Sornette de bolhas financeiras.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ieeexplore.ieee.org/abstract/document/9543607"&gt;Wu, T., Liu, J., Liu, J., Huang, Z., Wu, H., Zhang, C., ... &amp;amp; Zhang, G. (2021). A Novel AI-based Framework for AoI-optimal Trajectory Planning in UAV-assisted Wireless Sensor Networks. IEEE Transactions on Wireless Communications.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dl.acm.org/doi/abs/10.1145/3472456.3472474"&gt;Liu, H., Wen, Z., &amp;amp; Cai, W. (2021, August). FastPSO: Towards Efficient Swarm Intelligence Algorithm on GPUs. In 50th International Conference on Parallel Processing (pp. 1-10).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raiyanmahbub.com/images/Research_Paper.pdf"&gt;Mahbub, R. (2020). Algorithms and Optimization Techniques for Solving TSP.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.201900029"&gt;Li, J., Chen, T., Lim, K., Chen, L., Khan, S. A., Xie, J., &amp;amp; Wang, X. (2019). Deep learning accelerated gold nanocluster synthesis. Advanced Intelligent Systems, 1(3), 1900029.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;"DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;" /&gt; &lt;/td&gt; 
    &lt;td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;"&gt; &lt;pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;"&gt;    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•      â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/14665" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;!-- &lt;img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/&gt; --&gt; 
 &lt;h1&gt;&lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;" /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align="center"&gt;
  &lt;img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white" alt="Version"&gt;

  &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white" alt="License"&gt;
  &lt;img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white" alt="AI"&gt;
  &lt;img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white" alt="HKU"&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/HKUDS/DeepCode/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/ğŸPython-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/deepcode-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/DeepCode/issues/11"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;ğŸ–¥ï¸ &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;ğŸ–¥ï¸ &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;ğŸš€ Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;âš¡ Fast command-line workflow&lt;br /&gt;ğŸ”§ Developer-friendly interface&lt;br /&gt;ğŸ“Š Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;ğŸŒ &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;ğŸ¨ Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;ğŸ–±ï¸ Intuitive drag-and-drop&lt;br /&gt;ğŸ“± Responsive design&lt;br /&gt;ğŸ¯ Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h3&gt;ğŸ¬ &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style="margin: 20px 0;"&gt; 
   &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;" /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;ğŸ¯ &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/â–¶ï¸_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white" alt="Watch Video" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;"Where AI Agents Transform Ideas into Production-Ready Code"&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“‘ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features"&gt;ğŸš€ Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture"&gt;ğŸ—ï¸ Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;ğŸš€ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples"&gt;ğŸ’¡ Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations"&gt;ğŸ¬ Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history"&gt;â­ Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license"&gt;ğŸ“„ License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;ğŸš€ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;ğŸ¨ &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;âš™ï¸ &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;ğŸ¯ &lt;strong&gt;Autonomous Multi-Agent Workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ“„ &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”¬ &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;â±ï¸ &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”„ &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    A["ğŸ“„ Research Papers&amp;lt;br/&amp;gt;ğŸ’¬ Text Prompts&amp;lt;br/&amp;gt;ğŸŒ URLs &amp;amp; Document&amp;lt;br/&amp;gt;ğŸ“ Files: PDF, DOC, PPTX, TXT, HTML"] --&amp;gt; B["ğŸ§  DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine"]
    B --&amp;gt; C["ğŸš€ Algorithm Implementation &amp;lt;br/&amp;gt;ğŸ¨ Frontend Development &amp;lt;br/&amp;gt;âš™ï¸ Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ—ï¸ Architecture&lt;/h2&gt; 
&lt;h3&gt;ğŸ“Š &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;ğŸ¯ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;ğŸ§¬ &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;ğŸª„ &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;âš¡ &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;ğŸ’ &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;ğŸ”® &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ”§ &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ§  &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ’¾ &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ” &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ¤– &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ¯ Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“ Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“„ Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ—ï¸ Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“š Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ§¬ Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;ğŸ› ï¸ &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ”§ Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;ğŸ“¡ &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ğŸ› ï¸ &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;ğŸ”§ &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;ğŸ’¡ &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ” brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸŒ bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“‚ filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸŒ fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“¥ github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“‹ file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âš¡ command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ§¬ code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“š code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“„ document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;ğŸ”§ &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ğŸ› ï¸ &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;ğŸ¯ &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“„ read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âœï¸ write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“ get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âš™ï¸ set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“Š get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;ğŸ›ï¸ &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸš€ Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ğŸŒŸ &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; ğŸ’¡ &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; ğŸ“„ Research Papers â€¢ ğŸ’¬ Natural Language â€¢ ğŸŒ URLs â€¢ ğŸ“‹ Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ğŸ¯ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making â€¢ Workflow Coordination â€¢ Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ“ &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ“„ &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ğŸ“‹ &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis â€¢ Code Requirements Parsing â€¢ Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ” &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ“š &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ğŸ§¬ &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation â€¢ Testing â€¢ Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; âš¡ &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; ğŸ“¦ Complete Codebase â€¢ ğŸ§ª Test Suite â€¢ ğŸ“š Documentation â€¢ ğŸš€ Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;ğŸ”„ &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" style="border: none;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;"&gt; 
      &lt;h4&gt;ğŸ¯ Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;"&gt; 
      &lt;h4&gt;ğŸ§  Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;"&gt; 
      &lt;h4&gt;ğŸ” Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;"&gt; 
      &lt;h4&gt;âš¡ Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;h3&gt;ğŸ“¦ &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;âš¡ &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸš€ Install DeepCode package directly
pip install deepcode-hku

# ğŸ”‘ Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# ğŸ”‘ Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# ğŸ”‘ Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# ğŸ“„ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ğŸ”§ &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‚ Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;ğŸ”¥ &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸ”½ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# ğŸ“¦ Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# ğŸ”§ Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# ğŸ”‘ Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# ğŸ”‘ Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# ğŸ“„ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;ğŸ &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸ”½ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# ğŸ“¦ Install dependencies
pip install -r requirements.txt

# ğŸ”‘ Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# ğŸ”‘ Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# ğŸ“„ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;ğŸªŸ &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you're using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;ğŸ” &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Brave Search&lt;/strong&gt; (&lt;code&gt;"brave"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸŒ Bocha-MCP&lt;/strong&gt; (&lt;code&gt;"bocha-mcp"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;âš¡ &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;ğŸš€ &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸŒ Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ğŸ› ï¸ &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;ğŸŒ &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Web Access" /&gt; 
&lt;/div&gt; 
&lt;h5&gt;ğŸ–¥ï¸ &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white" alt="CLI Mode" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;ğŸ¯ &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“„ Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;âš¡ Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ’¡ Examples&lt;/h2&gt; 
&lt;h3&gt;ğŸ¬ &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;ğŸ“„ &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt; &lt;img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt;â–¶ï¸ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;ğŸ–¼ï¸ &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt; &lt;img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt;â–¶ï¸ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;ğŸŒ &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt; &lt;img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt;â–¶ï¸ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;ğŸ†• &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;ğŸ“„ &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We're continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;ğŸ”§ &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ğŸ“Š &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;âš¡ &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â­ Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href="https://star-history.com/#HKUDS/DeepCode&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸš€ &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;&lt;img src="https://img.shields.io/badge/ğŸš€_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Get Started" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS"&gt;&lt;img src="https://img.shields.io/badge/ğŸ›ï¸_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="View on GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/deepcode-agent"&gt;&lt;img src="https://img.shields.io/badge/â­_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white" alt="Star Project" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;ğŸ“„ &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; 
 &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff" alt="Visitors" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>yt-dlp/yt-dlp</title>
      <link>https://github.com/yt-dlp/yt-dlp</link>
      <description>&lt;p&gt;A feature-rich command-line audio/video downloader&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#readme"&gt;&lt;img src="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg?sanitize=true" alt="YT-DLP" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#installation" title="Installation"&gt;&lt;img src="https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;amp;label=Download&amp;amp;style=for-the-badge" alt="Release version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/yt-dlp" title="PyPI"&gt;&lt;img src="https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;amp;labelColor=555555&amp;amp;style=for-the-badge" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/Maintainers.md#maintainers" title="Donate"&gt;&lt;img src="https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;amp;labelColor=555555&amp;amp;style=for-the-badge" alt="Donate" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/H5MNcFW63r" title="Discord"&gt;&lt;img src="https://img.shields.io/discord/807245652072857610?color=blue&amp;amp;labelColor=555555&amp;amp;label=&amp;amp;logo=discord&amp;amp;style=for-the-badge" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/supportedsites.md" title="Supported Sites"&gt;&lt;img src="https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge" alt="Supported Sites" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/LICENSE" title="License"&gt;&lt;img src="https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge" alt="License: Unlicense" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/actions" title="CI Status"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;amp;label=Tests&amp;amp;style=for-the-badge" alt="CI Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/commits" title="Commit History"&gt;&lt;img src="https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;amp;style=for-the-badge" alt="Commits" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/pulse/monthly" title="Last activity"&gt;&lt;img src="https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;amp;style=for-the-badge&amp;amp;display_timestamp=committer" alt="Last Commit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;yt-dlp is a feature-rich command-line audio/video downloader with support for &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/supportedsites.md"&gt;thousands of sites&lt;/a&gt;. The project is a fork of &lt;a href="https://github.com/ytdl-org/youtube-dl"&gt;youtube-dl&lt;/a&gt; based on the now inactive &lt;a href="https://github.com/blackjack4494/yt-dlc"&gt;youtube-dlc&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- MANPAGE: MOVE "USAGE AND OPTIONS" SECTION HERE --&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#installation"&gt;INSTALLATION&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation"&gt;Detailed instructions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#release-files"&gt;Release Files&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#update"&gt;Update&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#dependencies"&gt;Dependencies&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#compile"&gt;Compile&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#usage-and-options"&gt;USAGE AND OPTIONS&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#general-options"&gt;General Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#network-options"&gt;Network Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#geo-restriction"&gt;Geo-restriction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#video-selection"&gt;Video Selection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#download-options"&gt;Download Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#filesystem-options"&gt;Filesystem Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#thumbnail-options"&gt;Thumbnail Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#internet-shortcut-options"&gt;Internet Shortcut Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#verbosity-and-simulation-options"&gt;Verbosity and Simulation Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#workarounds"&gt;Workarounds&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#video-format-options"&gt;Video Format Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#subtitle-options"&gt;Subtitle Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#authentication-options"&gt;Authentication Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#post-processing-options"&gt;Post-processing Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sponsorblock-options"&gt;SponsorBlock Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#extractor-options"&gt;Extractor Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#preset-aliases"&gt;Preset Aliases&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;CONFIGURATION&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration-file-encoding"&gt;Configuration file encoding&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#authentication-with-netrc"&gt;Authentication with netrc&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#notes-about-environment-variables"&gt;Notes about environment variables&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;OUTPUT TEMPLATE&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template-examples"&gt;Output template examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection"&gt;FORMAT SELECTION&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#filtering-formats"&gt;Filtering Formats&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats"&gt;Sorting Formats&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection-examples"&gt;Format Selection examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#modifying-metadata"&gt;MODIFYING METADATA&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#modifying-metadata-examples"&gt;Modifying metadata examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#extractor-arguments"&gt;EXTRACTOR ARGUMENTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#plugins"&gt;PLUGINS&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#installing-plugins"&gt;Installing Plugins&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#developing-plugins"&gt;Developing Plugins&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#embedding-yt-dlp"&gt;EMBEDDING YT-DLP&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#embedding-examples"&gt;Embedding examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#changes-from-youtube-dl"&gt;CHANGES FROM YOUTUBE-DL&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#new-features"&gt;New features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#differences-in-default-behavior"&gt;Differences in default behavior&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#deprecated-options"&gt;Deprecated options&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#contributing-to-yt-dlp"&gt;CONTRIBUTING&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#opening-an-issue"&gt;Opening an Issue&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#developer-instructions"&gt;Developer Instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/wiki"&gt;WIKI&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/FAQ"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;h1&gt;INSTALLATION&lt;/h1&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;p&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe"&gt;&lt;img src="https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;amp;logo=windows" alt="Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp"&gt;&lt;img src="https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;amp;logo=linux" alt="Unix" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos"&gt;&lt;img src="https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;amp;logo=apple" alt="MacOS" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/yt-dlp"&gt;&lt;img src="https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;amp;labelColor=555555&amp;amp;style=for-the-badge" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz"&gt;&lt;img src="https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge" alt="Source Tarball" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#release-files"&gt;&lt;img src="https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge" alt="Other variants" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/releases"&gt;&lt;img src="https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge" alt="All versions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;You can install yt-dlp using &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#release-files"&gt;the binaries&lt;/a&gt;, &lt;a href="https://pypi.org/project/yt-dlp"&gt;pip&lt;/a&gt; or one using a third-party package manager. See &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation"&gt;the wiki&lt;/a&gt; for detailed instructions&lt;/p&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;h2&gt;RELEASE FILES&lt;/h2&gt; 
&lt;h4&gt;Recommended&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;File&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp"&gt;yt-dlp&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Platform-independent &lt;a href="https://docs.python.org/3/library/zipimport.html"&gt;zipimport&lt;/a&gt; binary. Needs Python (recommended for &lt;strong&gt;Linux/BSD&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe"&gt;yt-dlp.exe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Windows (Win8+) standalone x64 binary (recommended for &lt;strong&gt;Windows&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos"&gt;yt-dlp_macos&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Universal MacOS (10.15+) standalone executable (recommended for &lt;strong&gt;MacOS&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Alternatives&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;File&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux"&gt;yt-dlp_linux&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Linux (glibc 2.17+) standalone x86_64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip"&gt;yt-dlp_linux.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (glibc 2.17+) x86_64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64"&gt;yt-dlp_linux_aarch64&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Linux (glibc 2.17+) standalone aarch64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64.zip"&gt;yt-dlp_linux_aarch64.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (glibc 2.17+) aarch64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l.zip"&gt;yt-dlp_linux_armv7l.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (glibc 2.31+) armv7l executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux"&gt;yt-dlp_musllinux&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Linux (musl 1.2+) standalone x86_64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux.zip"&gt;yt-dlp_musllinux.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (musl 1.2+) x86_64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64"&gt;yt-dlp_musllinux_aarch64&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Linux (musl 1.2+) standalone aarch64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64.zip"&gt;yt-dlp_musllinux_aarch64.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (musl 1.2+) aarch64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe"&gt;yt-dlp_x86.exe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Windows (Win8+) standalone x86 (32-bit) binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_x86.zip"&gt;yt-dlp_win_x86.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Windows (Win8+) x86 (32-bit) executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_arm64.exe"&gt;yt-dlp_arm64.exe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Windows (Win10+) standalone ARM64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_arm64.zip"&gt;yt-dlp_win_arm64.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Windows (Win10+) ARM64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip"&gt;yt-dlp_win.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Windows (Win8+) x64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip"&gt;yt-dlp_macos.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged MacOS (10.15+) executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Misc&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;File&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz"&gt;yt-dlp.tar.gz&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Source tarball&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS"&gt;SHA2-512SUMS&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;GNU-style SHA512 sums&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig"&gt;SHA2-512SUMS.sig&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;GPG signature file for SHA512 sums&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS"&gt;SHA2-256SUMS&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;GNU-style SHA256 sums&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig"&gt;SHA2-256SUMS.sig&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;GPG signature file for SHA256 sums&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The public key that can be used to verify the GPG signatures is &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/public.key"&gt;available here&lt;/a&gt; Example usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Licensing&lt;/h4&gt; 
&lt;p&gt;While yt-dlp is licensed under the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/LICENSE"&gt;Unlicense&lt;/a&gt;, many of the release files contain code from other projects with different licenses.&lt;/p&gt; 
&lt;p&gt;Most notably, the PyInstaller-bundled executables include GPLv3+ licensed code, and as such the combined work is licensed under &lt;a href="https://www.gnu.org/licenses/gpl-3.0.html"&gt;GPLv3+&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/THIRD_PARTY_LICENSES.txt"&gt;THIRD_PARTY_LICENSES.txt&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;The zipimport binary (&lt;code&gt;yt-dlp&lt;/code&gt;), the source tarball (&lt;code&gt;yt-dlp.tar.gz&lt;/code&gt;), and the PyPI source distribution &amp;amp; wheel only contain code licensed under the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/LICENSE"&gt;Unlicense&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The manpages, shell completion (autocomplete) files etc. are available inside the &lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz"&gt;source tarball&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;UPDATE&lt;/h2&gt; 
&lt;p&gt;You can use &lt;code&gt;yt-dlp -U&lt;/code&gt; to update if you are using the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#release-files"&gt;release binaries&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip"&gt;installed with pip&lt;/a&gt;, simply re-run the same command that was used to install the program&lt;/p&gt; 
&lt;p&gt;For other third-party package managers, see &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers"&gt;the wiki&lt;/a&gt; or refer to their documentation&lt;/p&gt; 
&lt;p&gt;&lt;a id="update-channels"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;There are currently three release channels for binaries: &lt;code&gt;stable&lt;/code&gt;, &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;master&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt; is the default channel, and many of its changes have been tested by users of the &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;master&lt;/code&gt; channels.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;nightly&lt;/code&gt; channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project's new patches and changes. This is the &lt;strong&gt;recommended channel for regular users&lt;/strong&gt; of yt-dlp. The &lt;code&gt;nightly&lt;/code&gt; releases are available from &lt;a href="https://github.com/yt-dlp/yt-dlp-nightly-builds/releases"&gt;yt-dlp/yt-dlp-nightly-builds&lt;/a&gt; or as development releases of the &lt;code&gt;yt-dlp&lt;/code&gt; PyPI package (which can be installed with pip's &lt;code&gt;--pre&lt;/code&gt; flag).&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;master&lt;/code&gt; channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from &lt;a href="https://github.com/yt-dlp/yt-dlp-master-builds/releases"&gt;yt-dlp/yt-dlp-master-builds&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When using &lt;code&gt;--update&lt;/code&gt;/&lt;code&gt;-U&lt;/code&gt;, a release binary will only update to its current channel. &lt;code&gt;--update-to CHANNEL&lt;/code&gt; can be used to switch to a different channel when a newer version is available. &lt;code&gt;--update-to [CHANNEL@]TAG&lt;/code&gt; can also be used to upgrade or downgrade to specific tags from a channel.&lt;/p&gt; 
&lt;p&gt;You may also use &lt;code&gt;--update-to &amp;lt;repository&amp;gt;&lt;/code&gt; (&lt;code&gt;&amp;lt;owner&amp;gt;/&amp;lt;repository&amp;gt;&lt;/code&gt;) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.&lt;/p&gt; 
&lt;p&gt;Example usage:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;yt-dlp --update-to master&lt;/code&gt; switch to the &lt;code&gt;master&lt;/code&gt; channel and update to its latest release&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;yt-dlp --update-to stable@2023.07.06&lt;/code&gt; upgrade/downgrade to release to &lt;code&gt;stable&lt;/code&gt; channel tag &lt;code&gt;2023.07.06&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;yt-dlp --update-to 2023.10.07&lt;/code&gt; upgrade/downgrade to tag &lt;code&gt;2023.10.07&lt;/code&gt; if it exists on the current channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;yt-dlp --update-to example/yt-dlp@2023.09.24&lt;/code&gt; upgrade/downgrade to the release from the &lt;code&gt;example/yt-dlp&lt;/code&gt; repository, tag &lt;code&gt;2023.09.24&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Any user experiencing an issue with the &lt;code&gt;stable&lt;/code&gt; release should install or update to the &lt;code&gt;nightly&lt;/code&gt; release before submitting a bug report:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python3 -m pip install -U --pre "yt-dlp[default]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version. You can suppress this warning by adding &lt;code&gt;--no-update&lt;/code&gt; to your command or configuration file.&lt;/p&gt; 
&lt;h2&gt;DEPENDENCIES&lt;/h2&gt; 
&lt;p&gt;Python versions 3.10+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.&lt;/p&gt; 
&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt; 
&lt;p&gt;While all the other dependencies are optional, &lt;code&gt;ffmpeg&lt;/code&gt; and &lt;code&gt;ffprobe&lt;/code&gt; are highly recommended&lt;/p&gt; 
&lt;h3&gt;Strongly recommended&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.ffmpeg.org"&gt;&lt;strong&gt;ffmpeg&lt;/strong&gt; and &lt;strong&gt;ffprobe&lt;/strong&gt;&lt;/a&gt; - Required for &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection"&gt;merging separate video and audio files&lt;/a&gt;, as well as for various &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#post-processing-options"&gt;post-processing&lt;/a&gt; tasks. License &lt;a href="https://www.ffmpeg.org/legal.html"&gt;depends on the build&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide &lt;a href="https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds"&gt;custom builds&lt;/a&gt; with patches for some of these issues at &lt;a href="https://github.com/yt-dlp/FFmpeg-Builds"&gt;yt-dlp/FFmpeg-Builds&lt;/a&gt;. See &lt;a href="https://github.com/yt-dlp/FFmpeg-Builds#patches-applied"&gt;the readme&lt;/a&gt; for details on the specific issues solved by these builds&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: What you need is ffmpeg &lt;em&gt;binary&lt;/em&gt;, &lt;strong&gt;NOT&lt;/strong&gt; &lt;a href="https://pypi.org/project/ffmpeg"&gt;the Python package of the same name&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Networking&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/certifi/python-certifi"&gt;&lt;strong&gt;certifi&lt;/strong&gt;&lt;/a&gt;* - Provides Mozilla's root certificate bundle. Licensed under &lt;a href="https://github.com/certifi/python-certifi/raw/master/LICENSE"&gt;MPLv2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/brotli"&gt;&lt;strong&gt;brotli&lt;/strong&gt;&lt;/a&gt;* or &lt;a href="https://github.com/python-hyper/brotlicffi"&gt;&lt;strong&gt;brotlicffi&lt;/strong&gt;&lt;/a&gt; - &lt;a href="https://en.wikipedia.org/wiki/Brotli"&gt;Brotli&lt;/a&gt; content encoding support. Both licensed under MIT &lt;sup&gt;&lt;a href="https://github.com/google/brotli/raw/master/LICENSE"&gt;1&lt;/a&gt; &lt;a href="https://github.com/python-hyper/brotlicffi/raw/master/LICENSE"&gt;2&lt;/a&gt; &lt;/sup&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aaugustin/websockets"&gt;&lt;strong&gt;websockets&lt;/strong&gt;&lt;/a&gt;* - For downloading over websocket. Licensed under &lt;a href="https://github.com/aaugustin/websockets/raw/main/LICENSE"&gt;BSD-3-Clause&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/psf/requests"&gt;&lt;strong&gt;requests&lt;/strong&gt;&lt;/a&gt;* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under &lt;a href="https://github.com/psf/requests/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Impersonation&lt;/h4&gt; 
&lt;p&gt;The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lexiforest/curl_cffi"&gt;&lt;strong&gt;curl_cffi&lt;/strong&gt;&lt;/a&gt; (recommended) - Python binding for &lt;a href="https://github.com/lexiforest/curl-impersonate"&gt;curl-impersonate&lt;/a&gt;. Provides impersonation targets for Chrome, Edge and Safari. Licensed under &lt;a href="https://github.com/lexiforest/curl_cffi/raw/main/LICENSE"&gt;MIT&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Can be installed with the &lt;code&gt;curl-cffi&lt;/code&gt; group, e.g. &lt;code&gt;pip install "yt-dlp[default,curl-cffi]"&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Currently included in most builds &lt;em&gt;except&lt;/em&gt; &lt;code&gt;yt-dlp&lt;/code&gt; (Unix zipimport binary), &lt;code&gt;yt-dlp_x86&lt;/code&gt; (Windows 32-bit) and &lt;code&gt;yt-dlp_musllinux_aarch64&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Metadata&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/quodlibet/mutagen"&gt;&lt;strong&gt;mutagen&lt;/strong&gt;&lt;/a&gt;* - For &lt;code&gt;--embed-thumbnail&lt;/code&gt; in certain formats. Licensed under &lt;a href="https://github.com/quodlibet/mutagen/raw/master/COPYING"&gt;GPLv2+&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/wez/atomicparsley"&gt;&lt;strong&gt;AtomicParsley&lt;/strong&gt;&lt;/a&gt; - For &lt;code&gt;--embed-thumbnail&lt;/code&gt; in &lt;code&gt;mp4&lt;/code&gt;/&lt;code&gt;m4a&lt;/code&gt; files when &lt;code&gt;mutagen&lt;/code&gt;/&lt;code&gt;ffmpeg&lt;/code&gt; cannot. Licensed under &lt;a href="https://github.com/wez/atomicparsley/raw/master/COPYING"&gt;GPLv2+&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xattr/xattr"&gt;&lt;strong&gt;xattr&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/iustin/pyxattr"&gt;&lt;strong&gt;pyxattr&lt;/strong&gt;&lt;/a&gt; or &lt;a href="http://savannah.nongnu.org/projects/attr"&gt;&lt;strong&gt;setfattr&lt;/strong&gt;&lt;/a&gt; - For writing xattr metadata (&lt;code&gt;--xattrs&lt;/code&gt;) on &lt;strong&gt;Mac&lt;/strong&gt; and &lt;strong&gt;BSD&lt;/strong&gt;. Licensed under &lt;a href="https://github.com/xattr/xattr/raw/master/LICENSE.txt"&gt;MIT&lt;/a&gt;, &lt;a href="https://github.com/iustin/pyxattr/raw/master/COPYING"&gt;LGPL2.1&lt;/a&gt; and &lt;a href="http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING"&gt;GPLv2+&lt;/a&gt; respectively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Misc&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Legrandin/pycryptodome"&gt;&lt;strong&gt;pycryptodomex&lt;/strong&gt;&lt;/a&gt;* - For decrypting AES-128 HLS streams and various other data. Licensed under &lt;a href="https://github.com/Legrandin/pycryptodome/raw/master/LICENSE.rst"&gt;BSD-2-Clause&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ariya/phantomjs"&gt;&lt;strong&gt;phantomjs&lt;/strong&gt;&lt;/a&gt; - Used in extractors where javascript needs to be run. Licensed under &lt;a href="https://github.com/ariya/phantomjs/raw/master/LICENSE.BSD"&gt;BSD-3-Clause&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mitya57/secretstorage"&gt;&lt;strong&gt;secretstorage&lt;/strong&gt;&lt;/a&gt;* - For &lt;code&gt;--cookies-from-browser&lt;/code&gt; to access the &lt;strong&gt;Gnome&lt;/strong&gt; keyring while decrypting cookies of &lt;strong&gt;Chromium&lt;/strong&gt;-based browsers on &lt;strong&gt;Linux&lt;/strong&gt;. Licensed under &lt;a href="https://github.com/mitya57/secretstorage/raw/master/LICENSE"&gt;BSD-3-Clause&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Any external downloader that you want to use with &lt;code&gt;--downloader&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deprecated&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://rtmpdump.mplayerhq.hu"&gt;&lt;strong&gt;rtmpdump&lt;/strong&gt;&lt;/a&gt; - For downloading &lt;code&gt;rtmp&lt;/code&gt; streams. ffmpeg can be used instead with &lt;code&gt;--downloader ffmpeg&lt;/code&gt;. Licensed under &lt;a href="http://rtmpdump.mplayerhq.hu"&gt;GPLv2+&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mplayerhq.hu/design7/info.html"&gt;&lt;strong&gt;mplayer&lt;/strong&gt;&lt;/a&gt; or &lt;a href="https://mpv.io"&gt;&lt;strong&gt;mpv&lt;/strong&gt;&lt;/a&gt; - For downloading &lt;code&gt;rstp&lt;/code&gt;/&lt;code&gt;mms&lt;/code&gt; streams. ffmpeg can be used instead with &lt;code&gt;--downloader ffmpeg&lt;/code&gt;. Licensed under &lt;a href="https://github.com/mpv-player/mpv/raw/master/Copyright"&gt;GPLv2+&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use or redistribute the dependencies, you must agree to their respective licensing terms.&lt;/p&gt; 
&lt;p&gt;The standalone release binaries are built with the Python interpreter and the packages marked with &lt;strong&gt;*&lt;/strong&gt; included.&lt;/p&gt; 
&lt;p&gt;If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the &lt;code&gt;--verbose&lt;/code&gt; output&lt;/p&gt; 
&lt;h2&gt;COMPILE&lt;/h2&gt; 
&lt;h3&gt;Standalone PyInstaller Builds&lt;/h3&gt; 
&lt;p&gt;To build the standalone executable, you must have Python and &lt;code&gt;pyinstaller&lt;/code&gt; (plus any of yt-dlp's &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#dependencies"&gt;optional dependencies&lt;/a&gt; if needed). The executable will be built for the same CPU architecture as the Python used.&lt;/p&gt; 
&lt;p&gt;You can run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 devscripts/install_deps.py --include pyinstaller
python3 devscripts/make_lazy_extractors.py
python3 -m bundle.pyinstaller
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On some systems, you may need to use &lt;code&gt;py&lt;/code&gt; or &lt;code&gt;python&lt;/code&gt; instead of &lt;code&gt;python3&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python -m bundle.pyinstaller&lt;/code&gt; accepts any arguments that can be passed to &lt;code&gt;pyinstaller&lt;/code&gt;, such as &lt;code&gt;--onefile/-F&lt;/code&gt; or &lt;code&gt;--onedir/-D&lt;/code&gt;, which is further &lt;a href="https://pyinstaller.org/en/stable/usage.html#what-to-generate"&gt;documented here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Pyinstaller versions below 4.4 &lt;a href="https://github.com/pyinstaller/pyinstaller#requirements-and-tested-platforms"&gt;do not support&lt;/a&gt; Python installed from the Windows store without using a virtual environment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Running &lt;code&gt;pyinstaller&lt;/code&gt; directly &lt;strong&gt;instead of&lt;/strong&gt; using &lt;code&gt;python -m bundle.pyinstaller&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; officially supported. This may or may not work correctly.&lt;/p&gt; 
&lt;h3&gt;Platform-independent Binary (UNIX)&lt;/h3&gt; 
&lt;p&gt;You will need the build tools &lt;code&gt;python&lt;/code&gt; (3.10+), &lt;code&gt;zip&lt;/code&gt;, &lt;code&gt;make&lt;/code&gt; (GNU), &lt;code&gt;pandoc&lt;/code&gt;* and &lt;code&gt;pytest&lt;/code&gt;*.&lt;/p&gt; 
&lt;p&gt;After installing these, simply run &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can also run &lt;code&gt;make yt-dlp&lt;/code&gt; instead to compile only the binary without updating any of the additional files. (The build tools marked with &lt;strong&gt;*&lt;/strong&gt; are not needed for this)&lt;/p&gt; 
&lt;h3&gt;Related scripts&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/install_deps.py&lt;/code&gt;&lt;/strong&gt; - Install dependencies for yt-dlp.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/update-version.py&lt;/code&gt;&lt;/strong&gt; - Update the version number based on the current date.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/set-variant.py&lt;/code&gt;&lt;/strong&gt; - Set the build variant of the executable.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/make_changelog.py&lt;/code&gt;&lt;/strong&gt; - Create a markdown changelog using short commit messages and update &lt;code&gt;CONTRIBUTORS&lt;/code&gt; file.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/make_lazy_extractors.py&lt;/code&gt;&lt;/strong&gt; - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable &lt;code&gt;YTDLP_NO_LAZY_EXTRACTORS&lt;/code&gt; to something nonempty to forcefully disable lazy extractor loading.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note: See their &lt;code&gt;--help&lt;/code&gt; for more info.&lt;/p&gt; 
&lt;h3&gt;Forking the project&lt;/h3&gt; 
&lt;p&gt;If you fork the project on GitHub, you can run your fork's &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/workflows/build.yml"&gt;build workflow&lt;/a&gt; to automatically build the selected version(s) as artifacts. Alternatively, you can run the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/workflows/release.yml"&gt;release workflow&lt;/a&gt; or enable the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/workflows/release-nightly.yml"&gt;nightly workflow&lt;/a&gt; to create full (pre-)releases.&lt;/p&gt; 
&lt;h1&gt;USAGE AND OPTIONS&lt;/h1&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;pre&gt;&lt;code&gt;yt-dlp [OPTIONS] [--] URL [URL...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Tip: Use &lt;code&gt;CTRL&lt;/code&gt;+&lt;code&gt;F&lt;/code&gt; (or &lt;code&gt;Command&lt;/code&gt;+&lt;code&gt;F&lt;/code&gt;) to search by keywords&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;!-- Auto generated --&gt; 
&lt;h2&gt;General Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-h, --help                      Print this help text and exit
--version                       Print program version and exit
-U, --update                    Update this program to the latest version
--no-update                     Do not check for updates (default)
--update-to [CHANNEL]@[TAG]     Upgrade/downgrade to a specific version.
                                CHANNEL can be a repository as well. CHANNEL
                                and TAG default to "stable" and "latest"
                                respectively if omitted; See "UPDATE" for
                                details. Supported channels: stable,
                                nightly, master
-i, --ignore-errors             Ignore download and postprocessing errors.
                                The download will be considered successful
                                even if the postprocessing fails
--no-abort-on-error             Continue with next video on download errors;
                                e.g. to skip unavailable videos in a
                                playlist (default)
--abort-on-error                Abort downloading of further videos if an
                                error occurs (Alias: --no-ignore-errors)
--list-extractors               List all supported extractors and exit
--extractor-descriptions        Output descriptions of all supported
                                extractors and exit
--use-extractors NAMES          Extractor names to use separated by commas.
                                You can also use regexes, "all", "default"
                                and "end" (end URL matching); e.g. --ies
                                "holodex.*,end,youtube". Prefix the name
                                with a "-" to exclude it, e.g. --ies
                                default,-generic. Use --list-extractors for
                                a list of extractor names. (Alias: --ies)
--default-search PREFIX         Use this prefix for unqualified URLs. E.g.
                                "gvsearch2:python" downloads two videos from
                                google videos for the search term "python".
                                Use the value "auto" to let yt-dlp guess
                                ("auto_warning" to emit a warning when
                                guessing). "error" just throws an error. The
                                default value "fixup_error" repairs broken
                                URLs, but emits an error if this is not
                                possible instead of searching
--ignore-config                 Don't load any more configuration files
                                except those given to --config-locations.
                                For backward compatibility, if this option
                                is found inside the system configuration
                                file, the user configuration is not loaded.
                                (Alias: --no-config)
--no-config-locations           Do not load any custom configuration files
                                (default). When given inside a configuration
                                file, ignore all previous --config-locations
                                defined in the current file
--config-locations PATH         Location of the main configuration file;
                                either the path to the config or its
                                containing directory ("-" for stdin). Can be
                                used multiple times and inside other
                                configuration files
--plugin-dirs PATH              Path to an additional directory to search
                                for plugins. This option can be used
                                multiple times to add multiple directories.
                                Use "default" to search the default plugin
                                directories (default)
--no-plugin-dirs                Clear plugin directories to search,
                                including defaults and those provided by
                                previous --plugin-dirs
--flat-playlist                 Do not extract a playlist's URL result
                                entries; some entry metadata may be missing
                                and downloading may be bypassed
--no-flat-playlist              Fully extract the videos of a playlist
                                (default)
--live-from-start               Download livestreams from the start.
                                Currently experimental and only supported
                                for YouTube and Twitch
--no-live-from-start            Download livestreams from the current time
                                (default)
--wait-for-video MIN[-MAX]      Wait for scheduled streams to become
                                available. Pass the minimum number of
                                seconds (or range) to wait between retries
--no-wait-for-video             Do not wait for scheduled streams (default)
--mark-watched                  Mark videos watched (even with --simulate)
--no-mark-watched               Do not mark videos watched (default)
--color [STREAM:]POLICY         Whether to emit color codes in output,
                                optionally prefixed by the STREAM (stdout or
                                stderr) to apply the setting to. Can be one
                                of "always", "auto" (default), "never", or
                                "no_color" (use non color terminal
                                sequences). Use "auto-tty" or "no_color-tty"
                                to decide based on terminal support only.
                                Can be used multiple times
--compat-options OPTS           Options that can help keep compatibility
                                with youtube-dl or youtube-dlc
                                configurations by reverting some of the
                                changes made in yt-dlp. See "Differences in
                                default behavior" for details
--alias ALIASES OPTIONS         Create aliases for an option string. Unless
                                an alias starts with a dash "-", it is
                                prefixed with "--". Arguments are parsed
                                according to the Python string formatting
                                mini-language. E.g. --alias get-audio,-X "-S
                                aext:{0},abr -x --audio-format {0}" creates
                                options "--get-audio" and "-X" that takes an
                                argument (ARG0) and expands to "-S
                                aext:ARG0,abr -x --audio-format ARG0". All
                                defined aliases are listed in the --help
                                output. Alias options can trigger more
                                aliases; so be careful to avoid defining
                                recursive options. As a safety measure, each
                                alias may be triggered a maximum of 100
                                times. This option can be used multiple times
-t, --preset-alias PRESET       Applies a predefined set of options. e.g.
                                --preset-alias mp3. The following presets
                                are available: mp3, aac, mp4, mkv, sleep.
                                See the "Preset Aliases" section at the end
                                for more info. This option can be used
                                multiple times
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Network Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--proxy URL                     Use the specified HTTP/HTTPS/SOCKS proxy. To
                                enable SOCKS proxy, specify a proper scheme,
                                e.g. socks5://user:pass@127.0.0.1:1080/.
                                Pass in an empty string (--proxy "") for
                                direct connection
--socket-timeout SECONDS        Time to wait before giving up, in seconds
--source-address IP             Client-side IP address to bind to
--impersonate CLIENT[:OS]       Client to impersonate for requests. E.g.
                                chrome, chrome-110, chrome:windows-10. Pass
                                --impersonate="" to impersonate any client.
                                Note that forcing impersonation for all
                                requests may have a detrimental impact on
                                download speed and stability
--list-impersonate-targets      List available clients to impersonate.
-4, --force-ipv4                Make all connections via IPv4
-6, --force-ipv6                Make all connections via IPv6
--enable-file-urls              Enable file:// URLs. This is disabled by
                                default for security reasons.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Geo-restriction:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--geo-verification-proxy URL    Use this proxy to verify the IP address for
                                some geo-restricted sites. The default proxy
                                specified by --proxy (or none, if the option
                                is not present) is used for the actual
                                downloading
--xff VALUE                     How to fake X-Forwarded-For HTTP header to
                                try bypassing geographic restriction. One of
                                "default" (only when known to be useful),
                                "never", an IP block in CIDR notation, or a
                                two-letter ISO 3166-2 country code
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Video Selection:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-I, --playlist-items ITEM_SPEC  Comma separated playlist_index of the items
                                to download. You can specify a range using
                                "[START]:[STOP][:STEP]". For backward
                                compatibility, START-STOP is also supported.
                                Use negative indices to count from the right
                                and negative STEP to download in reverse
                                order. E.g. "-I 1:3,7,-5::2" used on a
                                playlist of size 15 will download the items
                                at index 1,2,3,7,11,13,15
--min-filesize SIZE             Abort download if filesize is smaller than
                                SIZE, e.g. 50k or 44.6M
--max-filesize SIZE             Abort download if filesize is larger than
                                SIZE, e.g. 50k or 44.6M
--date DATE                     Download only videos uploaded on this date.
                                The date can be "YYYYMMDD" or in the format 
                                [now|today|yesterday][-N[day|week|month|year]].
                                E.g. "--date today-2weeks" downloads only
                                videos uploaded on the same day two weeks ago
--datebefore DATE               Download only videos uploaded on or before
                                this date. The date formats accepted are the
                                same as --date
--dateafter DATE                Download only videos uploaded on or after
                                this date. The date formats accepted are the
                                same as --date
--match-filters FILTER          Generic video filter. Any "OUTPUT TEMPLATE"
                                field can be compared with a number or a
                                string using the operators defined in
                                "Filtering Formats". You can also simply
                                specify a field to match if the field is
                                present, use "!field" to check if the field
                                is not present, and "&amp;amp;" to check multiple
                                conditions. Use a "\" to escape "&amp;amp;" or
                                quotes if needed. If used multiple times,
                                the filter matches if at least one of the
                                conditions is met. E.g. --match-filters
                                !is_live --match-filters "like_count&amp;gt;?100 &amp;amp;
                                description~='(?i)\bcats \&amp;amp; dogs\b'" matches
                                only videos that are not live OR those that
                                have a like count more than 100 (or the like
                                field is not available) and also has a
                                description that contains the phrase "cats &amp;amp;
                                dogs" (caseless). Use "--match-filters -" to
                                interactively ask whether to download each
                                video
--no-match-filters              Do not use any --match-filters (default)
--break-match-filters FILTER    Same as "--match-filters" but stops the
                                download process when a video is rejected
--no-break-match-filters        Do not use any --break-match-filters (default)
--no-playlist                   Download only the video, if the URL refers
                                to a video and a playlist
--yes-playlist                  Download the playlist, if the URL refers to
                                a video and a playlist
--age-limit YEARS               Download only videos suitable for the given
                                age
--download-archive FILE         Download only videos not listed in the
                                archive file. Record the IDs of all
                                downloaded videos in it
--no-download-archive           Do not use archive file (default)
--max-downloads NUMBER          Abort after downloading NUMBER files
--break-on-existing             Stop the download process when encountering
                                a file that is in the archive supplied with
                                the --download-archive option
--no-break-on-existing          Do not stop the download process when
                                encountering a file that is in the archive
                                (default)
--break-per-input               Alters --max-downloads, --break-on-existing,
                                --break-match-filters, and autonumber to
                                reset per input URL
--no-break-per-input            --break-on-existing and similar options
                                terminates the entire download queue
--skip-playlist-after-errors N  Number of allowed failures until the rest of
                                the playlist is skipped
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Download Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-N, --concurrent-fragments N    Number of fragments of a dash/hlsnative
                                video that should be downloaded concurrently
                                (default is 1)
-r, --limit-rate RATE           Maximum download rate in bytes per second,
                                e.g. 50K or 4.2M
--throttled-rate RATE           Minimum download rate in bytes per second
                                below which throttling is assumed and the
                                video data is re-extracted, e.g. 100K
-R, --retries RETRIES           Number of retries (default is 10), or
                                "infinite"
--file-access-retries RETRIES   Number of times to retry on file access
                                error (default is 3), or "infinite"
--fragment-retries RETRIES      Number of retries for a fragment (default is
                                10), or "infinite" (DASH, hlsnative and ISM)
--retry-sleep [TYPE:]EXPR       Time to sleep between retries in seconds
                                (optionally) prefixed by the type of retry
                                (http (default), fragment, file_access,
                                extractor) to apply the sleep to. EXPR can
                                be a number, linear=START[:END[:STEP=1]] or
                                exp=START[:END[:BASE=2]]. This option can be
                                used multiple times to set the sleep for the
                                different retry types, e.g. --retry-sleep
                                linear=1::2 --retry-sleep fragment:exp=1:20
--skip-unavailable-fragments    Skip unavailable fragments for DASH,
                                hlsnative and ISM downloads (default)
                                (Alias: --no-abort-on-unavailable-fragments)
--abort-on-unavailable-fragments
                                Abort download if a fragment is unavailable
                                (Alias: --no-skip-unavailable-fragments)
--keep-fragments                Keep downloaded fragments on disk after
                                downloading is finished
--no-keep-fragments             Delete downloaded fragments after
                                downloading is finished (default)
--buffer-size SIZE              Size of download buffer, e.g. 1024 or 16K
                                (default is 1024)
--resize-buffer                 The buffer size is automatically resized
                                from an initial value of --buffer-size
                                (default)
--no-resize-buffer              Do not automatically adjust the buffer size
--http-chunk-size SIZE          Size of a chunk for chunk-based HTTP
                                downloading, e.g. 10485760 or 10M (default
                                is disabled). May be useful for bypassing
                                bandwidth throttling imposed by a webserver
                                (experimental)
--playlist-random               Download playlist videos in random order
--lazy-playlist                 Process entries in the playlist as they are
                                received. This disables n_entries,
                                --playlist-random and --playlist-reverse
--no-lazy-playlist              Process videos in the playlist only after
                                the entire playlist is parsed (default)
--hls-use-mpegts                Use the mpegts container for HLS videos;
                                allowing some players to play the video
                                while downloading, and reducing the chance
                                of file corruption if download is
                                interrupted. This is enabled by default for
                                live streams
--no-hls-use-mpegts             Do not use the mpegts container for HLS
                                videos. This is default when not downloading
                                live streams
--download-sections REGEX       Download only chapters that match the
                                regular expression. A "*" prefix denotes
                                time-range instead of chapter. Negative
                                timestamps are calculated from the end.
                                "*from-url" can be used to download between
                                the "start_time" and "end_time" extracted
                                from the URL. Needs ffmpeg. This option can
                                be used multiple times to download multiple
                                sections, e.g. --download-sections
                                "*10:15-inf" --download-sections "intro"
--downloader [PROTO:]NAME       Name or path of the external downloader to
                                use (optionally) prefixed by the protocols
                                (http, ftp, m3u8, dash, rstp, rtmp, mms) to
                                use it for. Currently supports native,
                                aria2c, axel, curl, ffmpeg, httpie, wget.
                                You can use this option multiple times to
                                set different downloaders for different
                                protocols. E.g. --downloader aria2c
                                --downloader "dash,m3u8:native" will use
                                aria2c for http/ftp downloads, and the
                                native downloader for dash/m3u8 downloads
                                (Alias: --external-downloader)
--downloader-args NAME:ARGS     Give these arguments to the external
                                downloader. Specify the downloader name and
                                the arguments separated by a colon ":". For
                                ffmpeg, arguments can be passed to different
                                positions using the same syntax as
                                --postprocessor-args. You can use this
                                option multiple times to give different
                                arguments to different downloaders (Alias:
                                --external-downloader-args)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Filesystem Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-a, --batch-file FILE           File containing URLs to download ("-" for
                                stdin), one URL per line. Lines starting
                                with "#", ";" or "]" are considered as
                                comments and ignored
--no-batch-file                 Do not read URLs from batch file (default)
-P, --paths [TYPES:]PATH        The paths where the files should be
                                downloaded. Specify the type of file and the
                                path separated by a colon ":". All the same
                                TYPES as --output are supported.
                                Additionally, you can also provide "home"
                                (default) and "temp" paths. All intermediary
                                files are first downloaded to the temp path
                                and then the final files are moved over to
                                the home path after download is finished.
                                This option is ignored if --output is an
                                absolute path
-o, --output [TYPES:]TEMPLATE   Output filename template; see "OUTPUT
                                TEMPLATE" for details
--output-na-placeholder TEXT    Placeholder for unavailable fields in
                                --output (default: "NA")
--restrict-filenames            Restrict filenames to only ASCII characters,
                                and avoid "&amp;amp;" and spaces in filenames
--no-restrict-filenames         Allow Unicode characters, "&amp;amp;" and spaces in
                                filenames (default)
--windows-filenames             Force filenames to be Windows-compatible
--no-windows-filenames          Sanitize filenames only minimally
--trim-filenames LENGTH         Limit the filename length (excluding
                                extension) to the specified number of
                                characters
-w, --no-overwrites             Do not overwrite any files
--force-overwrites              Overwrite all video and metadata files. This
                                option includes --no-continue
--no-force-overwrites           Do not overwrite the video, but overwrite
                                related files (default)
-c, --continue                  Resume partially downloaded files/fragments
                                (default)
--no-continue                   Do not resume partially downloaded
                                fragments. If the file is not fragmented,
                                restart download of the entire file
--part                          Use .part files instead of writing directly
                                into output file (default)
--no-part                       Do not use .part files - write directly into
                                output file
--mtime                         Use the Last-modified header to set the file
                                modification time
--no-mtime                      Do not use the Last-modified header to set
                                the file modification time (default)
--write-description             Write video description to a .description file
--no-write-description          Do not write video description (default)
--write-info-json               Write video metadata to a .info.json file
                                (this may contain personal information)
--no-write-info-json            Do not write video metadata (default)
--write-playlist-metafiles      Write playlist metadata in addition to the
                                video metadata when using --write-info-json,
                                --write-description etc. (default)
--no-write-playlist-metafiles   Do not write playlist metadata when using
                                --write-info-json, --write-description etc.
--clean-info-json               Remove some internal metadata such as
                                filenames from the infojson (default)
--no-clean-info-json            Write all fields to the infojson
--write-comments                Retrieve video comments to be placed in the
                                infojson. The comments are fetched even
                                without this option if the extraction is
                                known to be quick (Alias: --get-comments)
--no-write-comments             Do not retrieve video comments unless the
                                extraction is known to be quick (Alias:
                                --no-get-comments)
--load-info-json FILE           JSON file containing the video information
                                (created with the "--write-info-json" option)
--cookies FILE                  Netscape formatted file to read cookies from
                                and dump cookie jar in
--no-cookies                    Do not read/dump cookies from/to file
                                (default)
--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]
                                The name of the browser to load cookies
                                from. Currently supported browsers are:
                                brave, chrome, chromium, edge, firefox,
                                opera, safari, vivaldi, whale. Optionally,
                                the KEYRING used for decrypting Chromium
                                cookies on Linux, the name/path of the
                                PROFILE to load cookies from, and the
                                CONTAINER name (if Firefox) ("none" for no
                                container) can be given with their
                                respective separators. By default, all
                                containers of the most recently accessed
                                profile are used. Currently supported
                                keyrings are: basictext, gnomekeyring,
                                kwallet, kwallet5, kwallet6
--no-cookies-from-browser       Do not load cookies from browser (default)
--cache-dir DIR                 Location in the filesystem where yt-dlp can
                                store some downloaded information (such as
                                client ids and signatures) permanently. By
                                default ${XDG_CACHE_HOME}/yt-dlp
--no-cache-dir                  Disable filesystem caching
--rm-cache-dir                  Delete all filesystem cache files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Thumbnail Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-thumbnail               Write thumbnail image to disk
--no-write-thumbnail            Do not write thumbnail image to disk (default)
--write-all-thumbnails          Write all thumbnail image formats to disk
--list-thumbnails               List available thumbnails of each video.
                                Simulate unless --no-simulate is used
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Internet Shortcut Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-link                    Write an internet shortcut file, depending
                                on the current platform (.url, .webloc or
                                .desktop). The URL may be cached by the OS
--write-url-link                Write a .url Windows internet shortcut. The
                                OS caches the URL based on the file path
--write-webloc-link             Write a .webloc macOS internet shortcut
--write-desktop-link            Write a .desktop Linux internet shortcut
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Verbosity and Simulation Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-q, --quiet                     Activate quiet mode. If used with --verbose,
                                print the log to stderr
--no-quiet                      Deactivate quiet mode. (Default)
--no-warnings                   Ignore warnings
-s, --simulate                  Do not download the video and do not write
                                anything to disk
--no-simulate                   Download the video even if printing/listing
                                options are used
--ignore-no-formats-error       Ignore "No video formats" error. Useful for
                                extracting metadata even if the videos are
                                not actually available for download
                                (experimental)
--no-ignore-no-formats-error    Throw error when no downloadable video
                                formats are found (default)
--skip-download                 Do not download the video but write all
                                related files (Alias: --no-download)
-O, --print [WHEN:]TEMPLATE     Field name or output template to print to
                                screen, optionally prefixed with when to
                                print it, separated by a ":". Supported
                                values of "WHEN" are the same as that of
                                --use-postprocessor (default: video).
                                Implies --quiet. Implies --simulate unless
                                --no-simulate or later stages of WHEN are
                                used. This option can be used multiple times
--print-to-file [WHEN:]TEMPLATE FILE
                                Append given template to the file. The
                                values of WHEN and TEMPLATE are the same as
                                that of --print. FILE uses the same syntax
                                as the output template. This option can be
                                used multiple times
-j, --dump-json                 Quiet, but print JSON information for each
                                video. Simulate unless --no-simulate is
                                used. See "OUTPUT TEMPLATE" for a
                                description of available keys
-J, --dump-single-json          Quiet, but print JSON information for each
                                URL or infojson passed. Simulate unless
                                --no-simulate is used. If the URL refers to
                                a playlist, the whole playlist information
                                is dumped in a single line
--force-write-archive           Force download archive entries to be written
                                as far as no errors occur, even if -s or
                                another simulation option is used (Alias:
                                --force-download-archive)
--newline                       Output progress bar as new lines
--no-progress                   Do not print progress bar
--progress                      Show progress bar, even if in quiet mode
--console-title                 Display progress in console titlebar
--progress-template [TYPES:]TEMPLATE
                                Template for progress outputs, optionally
                                prefixed with one of "download:" (default),
                                "download-title:" (the console title),
                                "postprocess:",  or "postprocess-title:".
                                The video's fields are accessible under the
                                "info" key and the progress attributes are
                                accessible under "progress" key. E.g.
                                --console-title --progress-template
                                "download-title:%(info.id)s-%(progress.eta)s"
--progress-delta SECONDS        Time between progress output (default: 0)
-v, --verbose                   Print various debugging information
--dump-pages                    Print downloaded pages encoded using base64
                                to debug problems (very verbose)
--write-pages                   Write downloaded intermediary pages to files
                                in the current directory to debug problems
--print-traffic                 Display sent and read HTTP traffic
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Workarounds:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--encoding ENCODING             Force the specified encoding (experimental)
--legacy-server-connect         Explicitly allow HTTPS connection to servers
                                that do not support RFC 5746 secure
                                renegotiation
--no-check-certificates         Suppress HTTPS certificate validation
--prefer-insecure               Use an unencrypted connection to retrieve
                                information about the video (Currently
                                supported only for YouTube)
--add-headers FIELD:VALUE       Specify a custom HTTP header and its value,
                                separated by a colon ":". You can use this
                                option multiple times
--bidi-workaround               Work around terminals that lack
                                bidirectional text support. Requires bidiv
                                or fribidi executable in PATH
--sleep-requests SECONDS        Number of seconds to sleep between requests
                                during data extraction
--sleep-interval SECONDS        Number of seconds to sleep before each
                                download. This is the minimum time to sleep
                                when used along with --max-sleep-interval
                                (Alias: --min-sleep-interval)
--max-sleep-interval SECONDS    Maximum number of seconds to sleep. Can only
                                be used along with --min-sleep-interval
--sleep-subtitles SECONDS       Number of seconds to sleep before each
                                subtitle download
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Video Format Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-f, --format FORMAT             Video format code, see "FORMAT SELECTION"
                                for more details
-S, --format-sort SORTORDER     Sort the formats by the fields given, see
                                "Sorting Formats" for more details
--format-sort-force             Force user specified sort order to have
                                precedence over all fields, see "Sorting
                                Formats" for more details (Alias: --S-force)
--no-format-sort-force          Some fields have precedence over the user
                                specified sort order (default)
--video-multistreams            Allow multiple video streams to be merged
                                into a single file
--no-video-multistreams         Only one video stream is downloaded for each
                                output file (default)
--audio-multistreams            Allow multiple audio streams to be merged
                                into a single file
--no-audio-multistreams         Only one audio stream is downloaded for each
                                output file (default)
--prefer-free-formats           Prefer video formats with free containers
                                over non-free ones of the same quality. Use
                                with "-S ext" to strictly prefer free
                                containers irrespective of quality
--no-prefer-free-formats        Don't give any special preference to free
                                containers (default)
--check-formats                 Make sure formats are selected only from
                                those that are actually downloadable
--check-all-formats             Check all formats for whether they are
                                actually downloadable
--no-check-formats              Do not check that the formats are actually
                                downloadable
-F, --list-formats              List available formats of each video.
                                Simulate unless --no-simulate is used
--merge-output-format FORMAT    Containers that may be used when merging
                                formats, separated by "/", e.g. "mp4/mkv".
                                Ignored if no merge is required. (currently
                                supported: avi, flv, mkv, mov, mp4, webm)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Subtitle Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-subs                    Write subtitle file
--no-write-subs                 Do not write subtitle file (default)
--write-auto-subs               Write automatically generated subtitle file
                                (Alias: --write-automatic-subs)
--no-write-auto-subs            Do not write auto-generated subtitles
                                (default) (Alias: --no-write-automatic-subs)
--list-subs                     List available subtitles of each video.
                                Simulate unless --no-simulate is used
--sub-format FORMAT             Subtitle format; accepts formats preference
                                separated by "/", e.g. "srt" or "ass/srt/best"
--sub-langs LANGS               Languages of the subtitles to download (can
                                be regex) or "all" separated by commas, e.g.
                                --sub-langs "en.*,ja" (where "en.*" is a
                                regex pattern that matches "en" followed by
                                0 or more of any character). You can prefix
                                the language code with a "-" to exclude it
                                from the requested languages, e.g. --sub-
                                langs all,-live_chat. Use --list-subs for a
                                list of available language tags
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Authentication Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-u, --username USERNAME         Login with this account ID
-p, --password PASSWORD         Account password. If this option is left
                                out, yt-dlp will ask interactively
-2, --twofactor TWOFACTOR       Two-factor authentication code
-n, --netrc                     Use .netrc authentication data
--netrc-location PATH           Location of .netrc authentication data;
                                either the path or its containing directory.
                                Defaults to ~/.netrc
--netrc-cmd NETRC_CMD           Command to execute to get the credentials
                                for an extractor.
--video-password PASSWORD       Video-specific password
--ap-mso MSO                    Adobe Pass multiple-system operator (TV
                                provider) identifier, use --ap-list-mso for
                                a list of available MSOs
--ap-username USERNAME          Multiple-system operator account login
--ap-password PASSWORD          Multiple-system operator account password.
                                If this option is left out, yt-dlp will ask
                                interactively
--ap-list-mso                   List all supported multiple-system operators
--client-certificate CERTFILE   Path to client certificate file in PEM
                                format. May include the private key
--client-certificate-key KEYFILE
                                Path to private key file for client
                                certificate
--client-certificate-password PASSWORD
                                Password for client certificate private key,
                                if encrypted. If not provided, and the key
                                is encrypted, yt-dlp will ask interactively
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Post-Processing Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-x, --extract-audio             Convert video files to audio-only files
                                (requires ffmpeg and ffprobe)
--audio-format FORMAT           Format to convert the audio to when -x is
                                used. (currently supported: best (default),
                                aac, alac, flac, m4a, mp3, opus, vorbis,
                                wav). You can specify multiple rules using
                                similar syntax as --remux-video
--audio-quality QUALITY         Specify ffmpeg audio quality to use when
                                converting the audio with -x. Insert a value
                                between 0 (best) and 10 (worst) for VBR or a
                                specific bitrate like 128K (default 5)
--remux-video FORMAT            Remux the video into another container if
                                necessary (currently supported: avi, flv,
                                gif, mkv, mov, mp4, webm, aac, aiff, alac,
                                flac, m4a, mka, mp3, ogg, opus, vorbis,
                                wav). If the target container does not
                                support the video/audio codec, remuxing will
                                fail. You can specify multiple rules; e.g.
                                "aac&amp;gt;m4a/mov&amp;gt;mp4/mkv" will remux aac to m4a,
                                mov to mp4 and anything else to mkv
--recode-video FORMAT           Re-encode the video into another format if
                                necessary. The syntax and supported formats
                                are the same as --remux-video
--postprocessor-args NAME:ARGS  Give these arguments to the postprocessors.
                                Specify the postprocessor/executable name
                                and the arguments separated by a colon ":"
                                to give the argument to the specified
                                postprocessor/executable. Supported PP are:
                                Merger, ModifyChapters, SplitChapters,
                                ExtractAudio, VideoRemuxer, VideoConvertor,
                                Metadata, EmbedSubtitle, EmbedThumbnail,
                                SubtitlesConvertor, ThumbnailsConvertor,
                                FixupStretched, FixupM4a, FixupM3u8,
                                FixupTimestamp and FixupDuration. The
                                supported executables are: AtomicParsley,
                                FFmpeg and FFprobe. You can also specify
                                "PP+EXE:ARGS" to give the arguments to the
                                specified executable only when being used by
                                the specified postprocessor. Additionally,
                                for ffmpeg/ffprobe, "_i"/"_o" can be
                                appended to the prefix optionally followed
                                by a number to pass the argument before the
                                specified input/output file, e.g. --ppa
                                "Merger+ffmpeg_i1:-v quiet". You can use
                                this option multiple times to give different
                                arguments to different postprocessors.
                                (Alias: --ppa)
-k, --keep-video                Keep the intermediate video file on disk
                                after post-processing
--no-keep-video                 Delete the intermediate video file after
                                post-processing (default)
--post-overwrites               Overwrite post-processed files (default)
--no-post-overwrites            Do not overwrite post-processed files
--embed-subs                    Embed subtitles in the video (only for mp4,
                                webm and mkv videos)
--no-embed-subs                 Do not embed subtitles (default)
--embed-thumbnail               Embed thumbnail in the video as cover art
--no-embed-thumbnail            Do not embed thumbnail (default)
--embed-metadata                Embed metadata to the video file. Also
                                embeds chapters/infojson if present unless
                                --no-embed-chapters/--no-embed-info-json are
                                used (Alias: --add-metadata)
--no-embed-metadata             Do not add metadata to file (default)
                                (Alias: --no-add-metadata)
--embed-chapters                Add chapter markers to the video file
                                (Alias: --add-chapters)
--no-embed-chapters             Do not add chapter markers (default) (Alias:
                                --no-add-chapters)
--embed-info-json               Embed the infojson as an attachment to
                                mkv/mka video files
--no-embed-info-json            Do not embed the infojson as an attachment
                                to the video file
--parse-metadata [WHEN:]FROM:TO
                                Parse additional metadata like title/artist
                                from other fields; see "MODIFYING METADATA"
                                for details. Supported values of "WHEN" are
                                the same as that of --use-postprocessor
                                (default: pre_process)
--replace-in-metadata [WHEN:]FIELDS REGEX REPLACE
                                Replace text in a metadata field using the
                                given regex. This option can be used
                                multiple times. Supported values of "WHEN"
                                are the same as that of --use-postprocessor
                                (default: pre_process)
--xattrs                        Write metadata to the video file's xattrs
                                (using Dublin Core and XDG standards)
--concat-playlist POLICY        Concatenate videos in a playlist. One of
                                "never", "always", or "multi_video"
                                (default; only when the videos form a single
                                show). All the video files must have the
                                same codecs and number of streams to be
                                concatenable. The "pl_video:" prefix can be
                                used with "--paths" and "--output" to set
                                the output filename for the concatenated
                                files. See "OUTPUT TEMPLATE" for details
--fixup POLICY                  Automatically correct known faults of the
                                file. One of never (do nothing), warn (only
                                emit a warning), detect_or_warn (the
                                default; fix the file if we can, warn
                                otherwise), force (try fixing even if the
                                file already exists)
--ffmpeg-location PATH          Location of the ffmpeg binary; either the
                                path to the binary or its containing directory
--exec [WHEN:]CMD               Execute a command, optionally prefixed with
                                when to execute it, separated by a ":".
                                Supported values of "WHEN" are the same as
                                that of --use-postprocessor (default:
                                after_move). The same syntax as the output
                                template can be used to pass any field as
                                arguments to the command. If no fields are
                                passed, %(filepath,_filename|)q is appended
                                to the end of the command. This option can
                                be used multiple times
--no-exec                       Remove any previously defined --exec
--convert-subs FORMAT           Convert the subtitles to another format
                                (currently supported: ass, lrc, srt, vtt).
                                Use "--convert-subs none" to disable
                                conversion (default) (Alias: --convert-
                                subtitles)
--convert-thumbnails FORMAT     Convert the thumbnails to another format
                                (currently supported: jpg, png, webp). You
                                can specify multiple rules using similar
                                syntax as "--remux-video". Use "--convert-
                                thumbnails none" to disable conversion
                                (default)
--split-chapters                Split video into multiple files based on
                                internal chapters. The "chapter:" prefix can
                                be used with "--paths" and "--output" to set
                                the output filename for the split files. See
                                "OUTPUT TEMPLATE" for details
--no-split-chapters             Do not split video based on chapters (default)
--remove-chapters REGEX         Remove chapters whose title matches the
                                given regular expression. The syntax is the
                                same as --download-sections. This option can
                                be used multiple times
--no-remove-chapters            Do not remove any chapters from the file
                                (default)
--force-keyframes-at-cuts       Force keyframes at cuts when
                                downloading/splitting/removing sections.
                                This is slow due to needing a re-encode, but
                                the resulting video may have fewer artifacts
                                around the cuts
--no-force-keyframes-at-cuts    Do not force keyframes around the chapters
                                when cutting/splitting (default)
--use-postprocessor NAME[:ARGS]
                                The (case-sensitive) name of plugin
                                postprocessors to be enabled, and
                                (optionally) arguments to be passed to it,
                                separated by a colon ":". ARGS are a
                                semicolon ";" delimited list of NAME=VALUE.
                                The "when" argument determines when the
                                postprocessor is invoked. It can be one of
                                "pre_process" (after video extraction),
                                "after_filter" (after video passes filter),
                                "video" (after --format; before
                                --print/--output), "before_dl" (before each
                                video download), "post_process" (after each
                                video download; default), "after_move"
                                (after moving the video file to its final
                                location), "after_video" (after downloading
                                and processing all formats of a video), or
                                "playlist" (at end of playlist). This option
                                can be used multiple times to add different
                                postprocessors
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;SponsorBlock Options:&lt;/h2&gt; 
&lt;p&gt;Make chapter entries for, or remove various segments (sponsor, introductions, etc.) from downloaded YouTube videos using the &lt;a href="https://sponsor.ajay.app"&gt;SponsorBlock API&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--sponsorblock-mark CATS        SponsorBlock categories to create chapters
                                for, separated by commas. Available
                                categories are sponsor, intro, outro,
                                selfpromo, preview, filler, interaction,
                                music_offtopic, poi_highlight, chapter, all
                                and default (=all). You can prefix the
                                category with a "-" to exclude it. See [1]
                                for descriptions of the categories. E.g.
                                --sponsorblock-mark all,-preview
                                [1] https://wiki.sponsor.ajay.app/w/Segment_Categories
--sponsorblock-remove CATS      SponsorBlock categories to be removed from
                                the video file, separated by commas. If a
                                category is present in both mark and remove,
                                remove takes precedence. The syntax and
                                available categories are the same as for
                                --sponsorblock-mark except that "default"
                                refers to "all,-filler" and poi_highlight,
                                chapter are not available
--sponsorblock-chapter-title TEMPLATE
                                An output template for the title of the
                                SponsorBlock chapters created by
                                --sponsorblock-mark. The only available
                                fields are start_time, end_time, category,
                                categories, name, category_names. Defaults
                                to "[SponsorBlock]: %(category_names)l"
--no-sponsorblock               Disable both --sponsorblock-mark and
                                --sponsorblock-remove
--sponsorblock-api URL          SponsorBlock API location, defaults to
                                https://sponsor.ajay.app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Extractor Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--extractor-retries RETRIES     Number of retries for known extractor errors
                                (default is 3), or "infinite"
--allow-dynamic-mpd             Process dynamic DASH manifests (default)
                                (Alias: --no-ignore-dynamic-mpd)
--ignore-dynamic-mpd            Do not process dynamic DASH manifests
                                (Alias: --no-allow-dynamic-mpd)
--hls-split-discontinuity       Split HLS playlists to different formats at
                                discontinuities such as ad breaks
--no-hls-split-discontinuity    Do not split HLS playlists into different
                                formats at discontinuities such as ad breaks
                                (default)
--extractor-args IE_KEY:ARGS    Pass ARGS arguments to the IE_KEY extractor.
                                See "EXTRACTOR ARGUMENTS" for details. You
                                can use this option multiple times to give
                                arguments for different extractors
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Preset Aliases:&lt;/h2&gt; 
&lt;p&gt;Predefined aliases for convenience and ease of use. Note that future versions of yt-dlp may add or adjust presets, but the existing preset names will not be changed or removed&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-t mp3                          -f 'ba[acodec^=mp3]/ba/b' -x --audio-format
                                mp3

-t aac                          -f
                                'ba[acodec^=aac]/ba[acodec^=mp4a.40.]/ba/b'
                                -x --audio-format aac

-t mp4                          --merge-output-format mp4 --remux-video mp4
                                -S vcodec:h264,lang,quality,res,fps,hdr:12,a
                                codec:aac

-t mkv                          --merge-output-format mkv --remux-video mkv

-t sleep                        --sleep-subtitles 5 --sleep-requests 0.75
                                --sleep-interval 10 --max-sleep-interval 20
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;CONFIGURATION&lt;/h1&gt; 
&lt;p&gt;You can configure yt-dlp by placing any supported command line option in a configuration file. The configuration is loaded from the following locations:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Main Configuration&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The file given to &lt;code&gt;--config-location&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Portable Configuration&lt;/strong&gt;: (Recommended for portable installations)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;If using a binary, &lt;code&gt;yt-dlp.conf&lt;/code&gt; in the same directory as the binary&lt;/li&gt; 
   &lt;li&gt;If running from source-code, &lt;code&gt;yt-dlp.conf&lt;/code&gt; in the parent directory of &lt;code&gt;yt_dlp&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Home Configuration&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;yt-dlp.conf&lt;/code&gt; in the home path given to &lt;code&gt;-P&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If &lt;code&gt;-P&lt;/code&gt; is not given, the current directory is searched&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;User Configuration&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp.conf&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp/config&lt;/code&gt; (recommended on Linux/macOS)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp/config.txt&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp.conf&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp/config&lt;/code&gt; (recommended on Windows)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp/config.txt&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;~/yt-dlp.conf&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;~/yt-dlp.conf.txt&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;~/.yt-dlp/config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;~/.yt-dlp/config.txt&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;See also: &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#notes-about-environment-variables"&gt;Notes about environment variables&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;System Configuration&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;/etc/yt-dlp.conf&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;/etc/yt-dlp/config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;/etc/yt-dlp/config.txt&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;E.g. with the following configuration file, yt-dlp will always extract the audio, copy the mtime, use a proxy and save all videos under &lt;code&gt;YouTube&lt;/code&gt; directory in your home directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Lines starting with # are comments

# Always extract audio
-x

# Copy the mtime
--mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under YouTube directory in your home directory
-o ~/YouTube/%(title)s.%(ext)s
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Options in a configuration file are just the same options aka switches used in regular command line calls; thus there &lt;strong&gt;must be no whitespace&lt;/strong&gt; after &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;--&lt;/code&gt;, e.g. &lt;code&gt;-o&lt;/code&gt; or &lt;code&gt;--proxy&lt;/code&gt; but not &lt;code&gt;- o&lt;/code&gt; or &lt;code&gt;-- proxy&lt;/code&gt;. They must also be quoted when necessary, as if it were a UNIX shell.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;--ignore-config&lt;/code&gt; if you want to disable all configuration files for a particular yt-dlp run. If &lt;code&gt;--ignore-config&lt;/code&gt; is found inside any configuration file, no further configuration will be loaded. For example, having the option in the portable configuration file prevents loading of home, user, and system configurations. Additionally, (for backward compatibility) if &lt;code&gt;--ignore-config&lt;/code&gt; is found inside the system configuration file, the user configuration is not loaded.&lt;/p&gt; 
&lt;h3&gt;Configuration file encoding&lt;/h3&gt; 
&lt;p&gt;The configuration files are decoded according to the UTF BOM if present, and in the encoding from system locale otherwise.&lt;/p&gt; 
&lt;p&gt;If you want your file to be decoded differently, add &lt;code&gt;# coding: ENCODING&lt;/code&gt; to the beginning of the file (e.g. &lt;code&gt;# coding: shift-jis&lt;/code&gt;). There must be no characters before that, even spaces or BOM.&lt;/p&gt; 
&lt;h3&gt;Authentication with netrc&lt;/h3&gt; 
&lt;p&gt;You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with &lt;code&gt;--username&lt;/code&gt; and &lt;code&gt;--password&lt;/code&gt;) in order not to pass credentials as command line arguments on every yt-dlp execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a &lt;a href="https://stackoverflow.com/tags/.netrc/info"&gt;&lt;code&gt;.netrc&lt;/code&gt; file&lt;/a&gt; on a per-extractor basis. For that, you will need to create a &lt;code&gt;.netrc&lt;/code&gt; file in &lt;code&gt;--netrc-location&lt;/code&gt; and restrict permissions to read/write by only you:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;touch ${HOME}/.netrc
chmod a-rwx,u+rw ${HOME}/.netrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After that, you can add credentials for an extractor in the following format, where &lt;em&gt;extractor&lt;/em&gt; is the name of the extractor in lowercase:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;machine &amp;lt;extractor&amp;gt; login &amp;lt;username&amp;gt; password &amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;E.g.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To activate authentication with the &lt;code&gt;.netrc&lt;/code&gt; file you should pass &lt;code&gt;--netrc&lt;/code&gt; to yt-dlp or place it in the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The default location of the .netrc file is &lt;code&gt;~&lt;/code&gt; (see below).&lt;/p&gt; 
&lt;p&gt;As an alternative to using the &lt;code&gt;.netrc&lt;/code&gt; file, which has the disadvantage of keeping your passwords in a plain text file, you can configure a custom shell command to provide the credentials for an extractor. This is done by providing the &lt;code&gt;--netrc-cmd&lt;/code&gt; parameter, it shall output the credentials in the netrc format and return &lt;code&gt;0&lt;/code&gt; on success, other values will be treated as an error. &lt;code&gt;{}&lt;/code&gt; in the command will be replaced by the name of the extractor to make it possible to select the credentials for the right extractor.&lt;/p&gt; 
&lt;p&gt;E.g. To use an encrypted &lt;code&gt;.netrc&lt;/code&gt; file stored as &lt;code&gt;.authinfo.gpg&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;yt-dlp --netrc-cmd 'gpg --decrypt ~/.authinfo.gpg' 'https://www.youtube.com/watch?v=BaW_jenozKc'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Notes about environment variables&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Environment variables are normally specified as &lt;code&gt;${VARIABLE}&lt;/code&gt;/&lt;code&gt;$VARIABLE&lt;/code&gt; on UNIX and &lt;code&gt;%VARIABLE%&lt;/code&gt; on Windows; but is always shown as &lt;code&gt;${VARIABLE}&lt;/code&gt; in this documentation&lt;/li&gt; 
 &lt;li&gt;yt-dlp also allows using UNIX-style variables on Windows for path-like options; e.g. &lt;code&gt;--output&lt;/code&gt;, &lt;code&gt;--config-location&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;If unset, &lt;code&gt;${XDG_CONFIG_HOME}&lt;/code&gt; defaults to &lt;code&gt;~/.config&lt;/code&gt; and &lt;code&gt;${XDG_CACHE_HOME}&lt;/code&gt; to &lt;code&gt;~/.cache&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;On Windows, &lt;code&gt;~&lt;/code&gt; points to &lt;code&gt;${HOME}&lt;/code&gt; if present; or, &lt;code&gt;${USERPROFILE}&lt;/code&gt; or &lt;code&gt;${HOMEDRIVE}${HOMEPATH}&lt;/code&gt; otherwise&lt;/li&gt; 
 &lt;li&gt;On Windows, &lt;code&gt;${USERPROFILE}&lt;/code&gt; generally points to &lt;code&gt;C:\Users\&amp;lt;user name&amp;gt;&lt;/code&gt; and &lt;code&gt;${APPDATA}&lt;/code&gt; to &lt;code&gt;${USERPROFILE}\AppData\Roaming&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;OUTPUT TEMPLATE&lt;/h1&gt; 
&lt;p&gt;The &lt;code&gt;-o&lt;/code&gt; option is used to indicate a template for the output file names while &lt;code&gt;-P&lt;/code&gt; option is used to specify the path each type of file should be saved to.&lt;/p&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;The simplest usage of &lt;code&gt;-o&lt;/code&gt; is not to set any template arguments when downloading a single file, like in &lt;code&gt;yt-dlp -o funny_video.flv "https://some/video"&lt;/code&gt; (hard-coding file extension like this is &lt;em&gt;not&lt;/em&gt; recommended and could break some post-processing).&lt;/p&gt; 
&lt;p&gt;It may however also contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to &lt;a href="https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting"&gt;Python string formatting operations&lt;/a&gt;, e.g. &lt;code&gt;%(NAME)s&lt;/code&gt; or &lt;code&gt;%(NAME)05d&lt;/code&gt;. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations.&lt;/p&gt; 
&lt;p&gt;The field names themselves (the part inside the parenthesis) can also have some special formatting:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Object traversal&lt;/strong&gt;: The dictionaries and lists available in metadata can be traversed by using a dot &lt;code&gt;.&lt;/code&gt; separator; e.g. &lt;code&gt;%(tags.0)s&lt;/code&gt;, &lt;code&gt;%(subtitles.en.-1.ext)s&lt;/code&gt;. You can do Python slicing with colon &lt;code&gt;:&lt;/code&gt;; E.g. &lt;code&gt;%(id.3:7)s&lt;/code&gt;, &lt;code&gt;%(id.6:2:-1)s&lt;/code&gt;, &lt;code&gt;%(formats.:.format_id)s&lt;/code&gt;. Curly braces &lt;code&gt;{}&lt;/code&gt; can be used to build dictionaries with only specific keys; e.g. &lt;code&gt;%(formats.:.{format_id,height})#j&lt;/code&gt;. An empty field name &lt;code&gt;%()s&lt;/code&gt; refers to the entire infodict; e.g. &lt;code&gt;%(.{id,title})s&lt;/code&gt;. Note that all the fields that become available using this method are not listed below. Use &lt;code&gt;-j&lt;/code&gt; to see such fields&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Arithmetic&lt;/strong&gt;: Simple arithmetic can be done on numeric fields using &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;*&lt;/code&gt;. E.g. &lt;code&gt;%(playlist_index+10)03d&lt;/code&gt;, &lt;code&gt;%(n_entries+1-playlist_index)d&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Date/time Formatting&lt;/strong&gt;: Date/time fields can be formatted according to &lt;a href="https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes"&gt;strftime formatting&lt;/a&gt; by specifying it separated from the field name using a &lt;code&gt;&amp;gt;&lt;/code&gt;. E.g. &lt;code&gt;%(duration&amp;gt;%H-%M-%S)s&lt;/code&gt;, &lt;code&gt;%(upload_date&amp;gt;%Y-%m-%d)s&lt;/code&gt;, &lt;code&gt;%(epoch-3600&amp;gt;%H-%M-%S)s&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternatives&lt;/strong&gt;: Alternate fields can be specified separated with a &lt;code&gt;,&lt;/code&gt;. E.g. &lt;code&gt;%(release_date&amp;gt;%Y,upload_date&amp;gt;%Y|Unknown)s&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Replacement&lt;/strong&gt;: A replacement value can be specified using a &lt;code&gt;&amp;amp;&lt;/code&gt; separator according to the &lt;a href="https://docs.python.org/3/library/string.html#format-specification-mini-language"&gt;&lt;code&gt;str.format&lt;/code&gt; mini-language&lt;/a&gt;. If the field is &lt;em&gt;not&lt;/em&gt; empty, this replacement value will be used instead of the actual field content. This is done after alternate fields are considered; thus the replacement is used if &lt;em&gt;any&lt;/em&gt; of the alternative fields is &lt;em&gt;not&lt;/em&gt; empty. E.g. &lt;code&gt;%(chapters&amp;amp;has chapters|no chapters)s&lt;/code&gt;, &lt;code&gt;%(title&amp;amp;TITLE={:&amp;gt;20}|NO TITLE)s&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Default&lt;/strong&gt;: A literal default value can be specified for when the field is empty using a &lt;code&gt;|&lt;/code&gt; separator. This overrides &lt;code&gt;--output-na-placeholder&lt;/code&gt;. E.g. &lt;code&gt;%(uploader|Unknown)s&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;More Conversions&lt;/strong&gt;: In addition to the normal format types &lt;code&gt;diouxXeEfFgGcrs&lt;/code&gt;, yt-dlp additionally supports converting to &lt;code&gt;B&lt;/code&gt; = &lt;strong&gt;B&lt;/strong&gt;ytes, &lt;code&gt;j&lt;/code&gt; = &lt;strong&gt;j&lt;/strong&gt;son (flag &lt;code&gt;#&lt;/code&gt; for pretty-printing, &lt;code&gt;+&lt;/code&gt; for Unicode), &lt;code&gt;h&lt;/code&gt; = HTML escaping, &lt;code&gt;l&lt;/code&gt; = a comma separated &lt;strong&gt;l&lt;/strong&gt;ist (flag &lt;code&gt;#&lt;/code&gt; for &lt;code&gt;\n&lt;/code&gt; newline-separated), &lt;code&gt;q&lt;/code&gt; = a string &lt;strong&gt;q&lt;/strong&gt;uoted for the terminal (flag &lt;code&gt;#&lt;/code&gt; to split a list into different arguments), &lt;code&gt;D&lt;/code&gt; = add &lt;strong&gt;D&lt;/strong&gt;ecimal suffixes (e.g. 10M) (flag &lt;code&gt;#&lt;/code&gt; to use 1024 as factor), and &lt;code&gt;S&lt;/code&gt; = &lt;strong&gt;S&lt;/strong&gt;anitize as filename (flag &lt;code&gt;#&lt;/code&gt; for restricted)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unicode normalization&lt;/strong&gt;: The format type &lt;code&gt;U&lt;/code&gt; can be used for NFC &lt;a href="https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize"&gt;Unicode normalization&lt;/a&gt;. The alternate form flag (&lt;code&gt;#&lt;/code&gt;) changes the normalization to NFD and the conversion flag &lt;code&gt;+&lt;/code&gt; can be used for NFKC/NFKD compatibility equivalence normalization. E.g. &lt;code&gt;%(title)+.100U&lt;/code&gt; is NFKC&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To summarize, the general syntax for a field is:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;%(name[.keys][addition][&amp;gt;strf][,alternate][&amp;amp;replacement][|default])[flags][width][.precision][length]type
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additionally, you can set different output templates for the various metadata files separately from the general output template by specifying the type of file followed by the template separated by a colon &lt;code&gt;:&lt;/code&gt;. The different file types supported are &lt;code&gt;subtitle&lt;/code&gt;, &lt;code&gt;thumbnail&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, &lt;code&gt;annotation&lt;/code&gt; (deprecated), &lt;code&gt;infojson&lt;/code&gt;, &lt;code&gt;link&lt;/code&gt;, &lt;code&gt;pl_thumbnail&lt;/code&gt;, &lt;code&gt;pl_description&lt;/code&gt;, &lt;code&gt;pl_infojson&lt;/code&gt;, &lt;code&gt;chapter&lt;/code&gt;, &lt;code&gt;pl_video&lt;/code&gt;. E.g. &lt;code&gt;-o "%(title)s.%(ext)s" -o "thumbnail:%(title)s\%(title)s.%(ext)s"&lt;/code&gt; will put the thumbnails in a folder with the same name as the video. If any of the templates is empty, that type of file will not be written. E.g. &lt;code&gt;--write-thumbnail -o "thumbnail:"&lt;/code&gt; will write thumbnails only for playlists and not for video.&lt;/p&gt; 
&lt;p&gt;&lt;a id="outtmpl-postprocess-note"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Due to post-processing (i.e. merging etc.), the actual output filename might differ. Use &lt;code&gt;--print after_move:filepath&lt;/code&gt; to get the name after all post-processing is complete.&lt;/p&gt; 
&lt;p&gt;The available fields are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;id&lt;/code&gt; (string): Video identifier&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;title&lt;/code&gt; (string): Video title&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fulltitle&lt;/code&gt; (string): Video title ignoring live timestamp and generic title&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt; (string): Video filename extension&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;alt_title&lt;/code&gt; (string): A secondary title of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;description&lt;/code&gt; (string): The description of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;display_id&lt;/code&gt; (string): An alternative identifier for the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader&lt;/code&gt; (string): Full name of the video uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader_id&lt;/code&gt; (string): Nickname or id of the video uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader_url&lt;/code&gt; (string): URL to the video uploader's profile&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;license&lt;/code&gt; (string): License name the video is licensed under&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;creators&lt;/code&gt; (list): The creators of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;creator&lt;/code&gt; (string): The creators of the video; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video became available&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;upload_date&lt;/code&gt; (string): Video upload date in UTC (YYYYMMDD)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video was released&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_date&lt;/code&gt; (string): The date (YYYYMMDD) when the video was released in UTC&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_year&lt;/code&gt; (numeric): Year (YYYY) when the video or album was released&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;modified_timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video was last modified&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;modified_date&lt;/code&gt; (string): The date (YYYYMMDD) when the video was last modified in UTC&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel&lt;/code&gt; (string): Full name of the channel the video is uploaded on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_id&lt;/code&gt; (string): Id of the channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_url&lt;/code&gt; (string): URL of the channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_follower_count&lt;/code&gt; (numeric): Number of followers of the channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_is_verified&lt;/code&gt; (boolean): Whether the channel is verified on the platform&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;location&lt;/code&gt; (string): Physical location where the video was filmed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;duration&lt;/code&gt; (numeric): Length of the video in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;duration_string&lt;/code&gt; (string): Length of the video (HH:mm:ss)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;view_count&lt;/code&gt; (numeric): How many users have watched the video on the platform&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;concurrent_view_count&lt;/code&gt; (numeric): How many users are currently watching the video on the platform.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;like_count&lt;/code&gt; (numeric): Number of positive ratings of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dislike_count&lt;/code&gt; (numeric): Number of negative ratings of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;repost_count&lt;/code&gt; (numeric): Number of reposts of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;average_rating&lt;/code&gt; (numeric): Average rating given by users, the scale used depends on the webpage&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;comment_count&lt;/code&gt; (numeric): Number of comments on the video (For some extractors, comments are only downloaded at the end, and so this field cannot be used)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;age_limit&lt;/code&gt; (numeric): Age restriction for the video (years)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;live_status&lt;/code&gt; (string): One of "not_live", "is_live", "is_upcoming", "was_live", "post_live" (was live, but VOD is not yet processed)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;is_live&lt;/code&gt; (boolean): Whether this video is a live stream or a fixed-length video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;was_live&lt;/code&gt; (boolean): Whether this video was originally a live stream&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playable_in_embed&lt;/code&gt; (string): Whether this video is allowed to play in embedded players on other sites&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;availability&lt;/code&gt; (string): Whether the video is "private", "premium_only", "subscriber_only", "needs_auth", "unlisted" or "public"&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;media_type&lt;/code&gt; (string): The type of media as classified by the site, e.g. "episode", "clip", "trailer"&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;start_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should start, as specified in the URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;end_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should end, as specified in the URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;extractor&lt;/code&gt; (string): Name of the extractor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;extractor_key&lt;/code&gt; (string): Key name of the extractor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;epoch&lt;/code&gt; (numeric): Unix epoch of when the information extraction was completed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;autonumber&lt;/code&gt; (numeric): Number that will be increased with each download, starting at &lt;code&gt;--autonumber-start&lt;/code&gt;, padded with leading zeros to 5 digits&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;video_autonumber&lt;/code&gt; (numeric): Number that will be increased with each video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;n_entries&lt;/code&gt; (numeric): Total number of extracted items in the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_id&lt;/code&gt; (string): Identifier of the playlist that contains the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_title&lt;/code&gt; (string): Name of the playlist that contains the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist&lt;/code&gt; (string): &lt;code&gt;playlist_title&lt;/code&gt; if available or else &lt;code&gt;playlist_id&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_count&lt;/code&gt; (numeric): Total number of items in the playlist. May not be known if entire playlist is not extracted&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_index&lt;/code&gt; (numeric): Index of the video in the playlist padded with leading zeros according the final index&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_autonumber&lt;/code&gt; (numeric): Position of the video in the playlist download queue padded with leading zeros according to the total length of the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_uploader&lt;/code&gt; (string): Full name of the playlist uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_uploader_id&lt;/code&gt; (string): Nickname or id of the playlist uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_channel&lt;/code&gt; (string): Display name of the channel that uploaded the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_channel_id&lt;/code&gt; (string): Identifier of the channel that uploaded the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_webpage_url&lt;/code&gt; (string): URL of the playlist webpage&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webpage_url&lt;/code&gt; (string): A URL to the video webpage which, if given to yt-dlp, should yield the same result again&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webpage_url_basename&lt;/code&gt; (string): The basename of the webpage URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webpage_url_domain&lt;/code&gt; (string): The domain of the webpage URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;original_url&lt;/code&gt; (string): The URL given by the user (or the same as &lt;code&gt;webpage_url&lt;/code&gt; for playlist entries)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (list): List of categories the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tags&lt;/code&gt; (list): List of tags assigned to the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cast&lt;/code&gt; (list): List of cast members&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All the fields in &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#filtering-formats"&gt;Filtering Formats&lt;/a&gt; can also be used&lt;/p&gt; 
&lt;p&gt;Available for the video that belongs to some logical chapter or section:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;chapter&lt;/code&gt; (string): Name or title of the chapter the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;chapter_number&lt;/code&gt; (numeric): Number of the chapter the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;chapter_id&lt;/code&gt; (string): Id of the chapter the video belongs to&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the video that is an episode of some series or program:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;series&lt;/code&gt; (string): Title of the series or program the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;series_id&lt;/code&gt; (string): Id of the series or program the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season&lt;/code&gt; (string): Title of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season_number&lt;/code&gt; (numeric): Number of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season_id&lt;/code&gt; (string): Id of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode&lt;/code&gt; (string): Title of the video episode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode_number&lt;/code&gt; (numeric): Number of the video episode within a season&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode_id&lt;/code&gt; (string): Id of the video episode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the media that is a track or a part of a music album:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;track&lt;/code&gt; (string): Title of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;track_number&lt;/code&gt; (numeric): Number of the track within an album or a disc&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;track_id&lt;/code&gt; (string): Id of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;artists&lt;/code&gt; (list): Artist(s) of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;artist&lt;/code&gt; (string): Artist(s) of the track; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;genres&lt;/code&gt; (list): Genre(s) of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;genre&lt;/code&gt; (string): Genre(s) of the track; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;composers&lt;/code&gt; (list): Composer(s) of the piece&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;composer&lt;/code&gt; (string): Composer(s) of the piece; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album&lt;/code&gt; (string): Title of the album the track belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_type&lt;/code&gt; (string): Type of the album&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_artists&lt;/code&gt; (list): All artists appeared on the album&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_artist&lt;/code&gt; (string): All artists appeared on the album; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;disc_number&lt;/code&gt; (numeric): Number of the disc or other physical medium the track belongs to&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available only when using &lt;code&gt;--download-sections&lt;/code&gt; and for &lt;code&gt;chapter:&lt;/code&gt; prefix when using &lt;code&gt;--split-chapters&lt;/code&gt; for videos with internal chapters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;section_title&lt;/code&gt; (string): Title of the chapter&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;section_number&lt;/code&gt; (numeric): Number of the chapter within the file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;section_start&lt;/code&gt; (numeric): Start time of the chapter in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;section_end&lt;/code&gt; (numeric): End time of the chapter in seconds&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available only when used in &lt;code&gt;--print&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;urls&lt;/code&gt; (string): The URLs of all requested formats, one in each line&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filename&lt;/code&gt; (string): Name of the video file. Note that the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#outtmpl-postprocess-note"&gt;actual filename may differ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;formats_table&lt;/code&gt; (table): The video format table as printed by &lt;code&gt;--list-formats&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;thumbnails_table&lt;/code&gt; (table): The thumbnail format table as printed by &lt;code&gt;--list-thumbnails&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;subtitles_table&lt;/code&gt; (table): The subtitle format table as printed by &lt;code&gt;--list-subs&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;automatic_captions_table&lt;/code&gt; (table): The automatic subtitle format table as printed by &lt;code&gt;--list-subs&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available only after the video is downloaded (&lt;code&gt;post_process&lt;/code&gt;/&lt;code&gt;after_move&lt;/code&gt;):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;filepath&lt;/code&gt;: Actual path of downloaded video file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available only in &lt;code&gt;--sponsorblock-chapter-title&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;start_time&lt;/code&gt; (numeric): Start time of the chapter in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;end_time&lt;/code&gt; (numeric): End time of the chapter in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (list): The &lt;a href="https://wiki.sponsor.ajay.app/w/Types#Category"&gt;SponsorBlock categories&lt;/a&gt; the chapter belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;category&lt;/code&gt; (string): The smallest SponsorBlock category the chapter belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;category_names&lt;/code&gt; (list): Friendly names of the categories&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt; (string): Friendly name of the smallest category&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;type&lt;/code&gt; (string): The &lt;a href="https://wiki.sponsor.ajay.app/w/Types#Action_Type"&gt;SponsorBlock action type&lt;/a&gt; of the chapter&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. E.g. for &lt;code&gt;-o %(title)s-%(id)s.%(ext)s&lt;/code&gt; and an mp4 video with title &lt;code&gt;yt-dlp test video&lt;/code&gt; and id &lt;code&gt;BaW_jenozKc&lt;/code&gt;, this will result in a &lt;code&gt;yt-dlp test video-BaW_jenozKc.mp4&lt;/code&gt; file created in the current directory.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Some of the sequences are not guaranteed to be present, since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with placeholder value provided with &lt;code&gt;--output-na-placeholder&lt;/code&gt; (&lt;code&gt;NA&lt;/code&gt; by default).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: Look at the &lt;code&gt;-j&lt;/code&gt; output to identify which fields are available for the particular URL&lt;/p&gt; 
&lt;p&gt;For numeric sequences, you can use &lt;a href="https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting"&gt;numeric related formatting&lt;/a&gt;; e.g. &lt;code&gt;%(view_count)05d&lt;/code&gt; will result in a string with view count padded with zeros up to 5 characters, like in &lt;code&gt;00042&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Output templates can also contain arbitrary hierarchical path, e.g. &lt;code&gt;-o "%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s"&lt;/code&gt; which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.&lt;/p&gt; 
&lt;p&gt;To use percent literals in an output template use &lt;code&gt;%%&lt;/code&gt;. To output to stdout use &lt;code&gt;-o -&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The current default template is &lt;code&gt;%(title)s [%(id)s].%(ext)s&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In some cases, you don't want special characters such as ä¸­, spaces, or &amp;amp;, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the &lt;code&gt;--restrict-filenames&lt;/code&gt; flag to get a shorter title.&lt;/p&gt; 
&lt;h4&gt;Output template examples&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ yt-dlp --print filename -o "test video.%(ext)s" BaW_jenozKc
test video.webm    # Literal name with correct extension

$ yt-dlp --print filename -o "%(title)s.%(ext)s" BaW_jenozKc
youtube-dl test video ''_Ã¤â†­ğ•.webm    # All kinds of weird characters

$ yt-dlp --print filename -o "%(title)s.%(ext)s" BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.webm    # Restricted file name

# Download YouTube playlist videos in separate directory indexed by video order in a playlist
$ yt-dlp -o "%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s" "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"

# Download YouTube playlist videos in separate directories according to their uploaded year
$ yt-dlp -o "%(upload_date&amp;gt;%Y)s/%(title)s.%(ext)s" "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"

# Prefix playlist index with " - " separator, but only if it is available
$ yt-dlp -o "%(playlist_index&amp;amp;{} - |)s%(title)s.%(ext)s" BaW_jenozKc "https://www.youtube.com/user/TheLinuxFoundation/playlists"

# Download all playlists of YouTube channel/user keeping each playlist in separate directory:
$ yt-dlp -o "%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s" "https://www.youtube.com/user/TheLinuxFoundation/playlists"

# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home
$ yt-dlp -u user -p password -P "~/MyVideos" -o "%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s" "https://www.udemy.com/java-tutorial"

# Download entire series season keeping each series and each season in separate directory under C:/MyVideos
$ yt-dlp -P "C:/MyVideos" -o "%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s" "https://videomore.ru/kino_v_detalayah/5_sezon/367617"

# Download video as "C:\MyVideos\uploader\title.ext", subtitles as "C:\MyVideos\subs\uploader\title.ext"
# and put all temporary files in "C:\MyVideos\tmp"
$ yt-dlp -P "C:/MyVideos" -P "temp:tmp" -P "subtitle:subs" -o "%(uploader)s/%(title)s.%(ext)s" BaW_jenozKc --write-subs

# Download video as "C:\MyVideos\uploader\title.ext" and subtitles as "C:\MyVideos\uploader\subs\title.ext"
$ yt-dlp -P "C:/MyVideos" -o "%(uploader)s/%(title)s.%(ext)s" -o "subtitle:%(uploader)s/subs/%(title)s.%(ext)s" BaW_jenozKc --write-subs

# Stream the video being downloaded to stdout
$ yt-dlp -o - BaW_jenozKc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;FORMAT SELECTION&lt;/h1&gt; 
&lt;p&gt;By default, yt-dlp tries to download the best available quality if you &lt;strong&gt;don't&lt;/strong&gt; pass any options. This is generally equivalent to using &lt;code&gt;-f bestvideo*+bestaudio/best&lt;/code&gt;. However, if multiple audiostreams is enabled (&lt;code&gt;--audio-multistreams&lt;/code&gt;), the default format changes to &lt;code&gt;-f bestvideo+bestaudio/best&lt;/code&gt;. Similarly, if ffmpeg is unavailable, or if you use yt-dlp to stream to &lt;code&gt;stdout&lt;/code&gt; (&lt;code&gt;-o -&lt;/code&gt;), the default becomes &lt;code&gt;-f best/bestvideo+bestaudio&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deprecation warning&lt;/strong&gt;: Latest versions of yt-dlp can stream multiple formats to the stdout simultaneously using ffmpeg. So, in future versions, the default for this will be set to &lt;code&gt;-f bv*+ba/b&lt;/code&gt; similar to normal downloads. If you want to preserve the &lt;code&gt;-f b/bv+ba&lt;/code&gt; setting, it is recommended to explicitly specify it in the configuration options.&lt;/p&gt; 
&lt;p&gt;The general syntax for format selection is &lt;code&gt;-f FORMAT&lt;/code&gt; (or &lt;code&gt;--format FORMAT&lt;/code&gt;) where &lt;code&gt;FORMAT&lt;/code&gt; is a &lt;em&gt;selector expression&lt;/em&gt;, i.e. an expression that describes format or formats you would like to download.&lt;/p&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;The simplest case is requesting a specific format; e.g. with &lt;code&gt;-f 22&lt;/code&gt; you can download the format with format code equal to 22. You can get the list of available format codes for particular video using &lt;code&gt;--list-formats&lt;/code&gt; or &lt;code&gt;-F&lt;/code&gt;. Note that these format codes are extractor specific.&lt;/p&gt; 
&lt;p&gt;You can also use a file extension (currently &lt;code&gt;3gp&lt;/code&gt;, &lt;code&gt;aac&lt;/code&gt;, &lt;code&gt;flv&lt;/code&gt;, &lt;code&gt;m4a&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, &lt;code&gt;mp4&lt;/code&gt;, &lt;code&gt;ogg&lt;/code&gt;, &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;webm&lt;/code&gt; are supported) to download the best quality format of a particular file extension served as a single file, e.g. &lt;code&gt;-f webm&lt;/code&gt; will download the best quality format with the &lt;code&gt;webm&lt;/code&gt; extension served as a single file.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;-f -&lt;/code&gt; to interactively provide the format selector &lt;em&gt;for each video&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;You can also use special names to select particular edge case formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;all&lt;/code&gt;: Select &lt;strong&gt;all formats&lt;/strong&gt; separately&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mergeall&lt;/code&gt;: Select and &lt;strong&gt;merge all formats&lt;/strong&gt; (Must be used with &lt;code&gt;--audio-multistreams&lt;/code&gt;, &lt;code&gt;--video-multistreams&lt;/code&gt; or both)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;b*&lt;/code&gt;, &lt;code&gt;best*&lt;/code&gt;: Select the best quality format that &lt;strong&gt;contains either&lt;/strong&gt; a video or an audio or both (i.e.; &lt;code&gt;vcodec!=none or acodec!=none&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;b&lt;/code&gt;, &lt;code&gt;best&lt;/code&gt;: Select the best quality format that &lt;strong&gt;contains both&lt;/strong&gt; video and audio. Equivalent to &lt;code&gt;best*[vcodec!=none][acodec!=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;bv&lt;/code&gt;, &lt;code&gt;bestvideo&lt;/code&gt;: Select the best quality &lt;strong&gt;video-only&lt;/strong&gt; format. Equivalent to &lt;code&gt;best*[acodec=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;bv*&lt;/code&gt;, &lt;code&gt;bestvideo*&lt;/code&gt;: Select the best quality format that &lt;strong&gt;contains video&lt;/strong&gt;. It may also contain audio. Equivalent to &lt;code&gt;best*[vcodec!=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ba&lt;/code&gt;, &lt;code&gt;bestaudio&lt;/code&gt;: Select the best quality &lt;strong&gt;audio-only&lt;/strong&gt; format. Equivalent to &lt;code&gt;best*[vcodec=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ba*&lt;/code&gt;, &lt;code&gt;bestaudio*&lt;/code&gt;: Select the best quality format that &lt;strong&gt;contains audio&lt;/strong&gt;. It may also contain video. Equivalent to &lt;code&gt;best*[acodec!=none]&lt;/code&gt; (&lt;a href="https://github.com/yt-dlp/yt-dlp/issues/979#issuecomment-919629354"&gt;Do not use!&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;w*&lt;/code&gt;, &lt;code&gt;worst*&lt;/code&gt;: Select the worst quality format that contains either a video or an audio&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;w&lt;/code&gt;, &lt;code&gt;worst&lt;/code&gt;: Select the worst quality format that contains both video and audio. Equivalent to &lt;code&gt;worst*[vcodec!=none][acodec!=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wv&lt;/code&gt;, &lt;code&gt;worstvideo&lt;/code&gt;: Select the worst quality video-only format. Equivalent to &lt;code&gt;worst*[acodec=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wv*&lt;/code&gt;, &lt;code&gt;worstvideo*&lt;/code&gt;: Select the worst quality format that contains video. It may also contain audio. Equivalent to &lt;code&gt;worst*[vcodec!=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wa&lt;/code&gt;, &lt;code&gt;worstaudio&lt;/code&gt;: Select the worst quality audio-only format. Equivalent to &lt;code&gt;worst*[vcodec=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wa*&lt;/code&gt;, &lt;code&gt;worstaudio*&lt;/code&gt;: Select the worst quality format that contains audio. It may also contain video. Equivalent to &lt;code&gt;worst*[acodec!=none]&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, to download the worst quality video-only format you can use &lt;code&gt;-f worstvideo&lt;/code&gt;. It is, however, recommended not to use &lt;code&gt;worst&lt;/code&gt; and related options. When your format selector is &lt;code&gt;worst&lt;/code&gt;, the format which is worst in all respects is selected. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use &lt;code&gt;-S +size&lt;/code&gt; or more rigorously, &lt;code&gt;-S +size,+br,+res,+fps&lt;/code&gt; instead of &lt;code&gt;-f worst&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats"&gt;Sorting Formats&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;You can select the n'th best format of a type by using &lt;code&gt;best&amp;lt;type&amp;gt;.&amp;lt;n&amp;gt;&lt;/code&gt;. For example, &lt;code&gt;best.2&lt;/code&gt; will select the 2nd best combined format. Similarly, &lt;code&gt;bv*.3&lt;/code&gt; will select the 3rd best format that contains a video stream.&lt;/p&gt; 
&lt;p&gt;If you want to download multiple videos, and they don't have the same formats available, you can specify the order of preference using slashes. Note that formats on the left hand side are preferred; e.g. &lt;code&gt;-f 22/17/18&lt;/code&gt; will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.&lt;/p&gt; 
&lt;p&gt;If you want to download several formats of the same video use a comma as a separator, e.g. &lt;code&gt;-f 22,17,18&lt;/code&gt; will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: &lt;code&gt;-f 136/137/mp4/bestvideo,140/m4a/bestaudio&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can merge the video and audio of multiple formats into a single file using &lt;code&gt;-f &amp;lt;format1&amp;gt;+&amp;lt;format2&amp;gt;+...&lt;/code&gt; (requires ffmpeg installed); e.g. &lt;code&gt;-f bestvideo+bestaudio&lt;/code&gt; will download the best video-only format, the best audio-only format and mux them together with ffmpeg.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deprecation warning&lt;/strong&gt;: Since the &lt;em&gt;below&lt;/em&gt; described behavior is complex and counter-intuitive, this will be removed and multistreams will be enabled by default in the future. A new operator will be instead added to limit formats to single audio/video&lt;/p&gt; 
&lt;p&gt;Unless &lt;code&gt;--video-multistreams&lt;/code&gt; is used, all formats with a video stream except the first one are ignored. Similarly, unless &lt;code&gt;--audio-multistreams&lt;/code&gt; is used, all formats with an audio stream except the first one are ignored. E.g. &lt;code&gt;-f bestvideo+best+bestaudio --video-multistreams --audio-multistreams&lt;/code&gt; will download and merge all 3 given formats. The resulting file will have 2 video streams and 2 audio streams. But &lt;code&gt;-f bestvideo+best+bestaudio --no-video-multistreams&lt;/code&gt; will download and merge only &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt;. &lt;code&gt;best&lt;/code&gt; is ignored since another format containing a video stream (&lt;code&gt;bestvideo&lt;/code&gt;) has already been selected. The order of the formats is therefore important. &lt;code&gt;-f best+bestaudio --no-audio-multistreams&lt;/code&gt; will download only &lt;code&gt;best&lt;/code&gt; while &lt;code&gt;-f bestaudio+best --no-audio-multistreams&lt;/code&gt; will ignore &lt;code&gt;best&lt;/code&gt; and download only &lt;code&gt;bestaudio&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Filtering Formats&lt;/h2&gt; 
&lt;p&gt;You can also filter the video formats by putting a condition in brackets, as in &lt;code&gt;-f "best[height=720]"&lt;/code&gt; (or &lt;code&gt;-f "[filesize&amp;gt;10M]"&lt;/code&gt; since filters without a selector are interpreted as &lt;code&gt;best&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;The following numeric meta fields can be used with comparisons &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;!=&lt;/code&gt; (not equals):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;filesize&lt;/code&gt;: The number of bytes, if known in advance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filesize_approx&lt;/code&gt;: An estimate for the number of bytes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;width&lt;/code&gt;: Width of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;height&lt;/code&gt;: Height of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;aspect_ratio&lt;/code&gt;: Aspect ratio of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tbr&lt;/code&gt;: Average bitrate of audio and video in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;abr&lt;/code&gt;: Average audio bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vbr&lt;/code&gt;: Average video bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;asr&lt;/code&gt;: Audio sampling rate in Hertz&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fps&lt;/code&gt;: Frame rate&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;audio_channels&lt;/code&gt;: The number of audio channels&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stretched_ratio&lt;/code&gt;: &lt;code&gt;width:height&lt;/code&gt; of the video's pixels, if not square&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also filtering work for comparisons &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;^=&lt;/code&gt; (starts with), &lt;code&gt;$=&lt;/code&gt; (ends with), &lt;code&gt;*=&lt;/code&gt; (contains), &lt;code&gt;~=&lt;/code&gt; (matches regex) and following string meta fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;url&lt;/code&gt;: Video URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: File extension&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;acodec&lt;/code&gt;: Name of the audio codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: Name of the video codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;container&lt;/code&gt;: Name of the container format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;protocol&lt;/code&gt;: The protocol that will be used for the actual download, lower-case (&lt;code&gt;http&lt;/code&gt;, &lt;code&gt;https&lt;/code&gt;, &lt;code&gt;rtsp&lt;/code&gt;, &lt;code&gt;rtmp&lt;/code&gt;, &lt;code&gt;rtmpe&lt;/code&gt;, &lt;code&gt;mms&lt;/code&gt;, &lt;code&gt;f4m&lt;/code&gt;, &lt;code&gt;ism&lt;/code&gt;, &lt;code&gt;http_dash_segments&lt;/code&gt;, &lt;code&gt;m3u8&lt;/code&gt;, or &lt;code&gt;m3u8_native&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;language&lt;/code&gt;: Language code&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dynamic_range&lt;/code&gt;: The dynamic range of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_id&lt;/code&gt;: A short description of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt;: A human-readable description of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_note&lt;/code&gt;: Additional info about the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resolution&lt;/code&gt;: Textual description of width and height&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Any string comparison may be prefixed with negation &lt;code&gt;!&lt;/code&gt; in order to produce an opposite comparison, e.g. &lt;code&gt;!*=&lt;/code&gt; (does not contain). The comparand of a string comparison needs to be quoted with either double or single quotes if it contains spaces or special characters other than &lt;code&gt;._-&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: None of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by the particular extractor, i.e. the metadata offered by the website. Any other field made available by the extractor can also be used for filtering.&lt;/p&gt; 
&lt;p&gt;Formats for which the value is not known are excluded unless you put a question mark (&lt;code&gt;?&lt;/code&gt;) after the operator. You can combine format filters, so &lt;code&gt;-f "bv[height&amp;lt;=?720][tbr&amp;gt;500]"&lt;/code&gt; selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 kbps. You can also use the filters with &lt;code&gt;all&lt;/code&gt; to download all formats that satisfy the filter, e.g. &lt;code&gt;-f "all[vcodec=none]"&lt;/code&gt; selects all audio-only formats.&lt;/p&gt; 
&lt;p&gt;Format selectors can also be grouped using parentheses; e.g. &lt;code&gt;-f "(mp4,webm)[height&amp;lt;480]"&lt;/code&gt; will download the best pre-merged mp4 and webm formats with a height lower than 480.&lt;/p&gt; 
&lt;h2&gt;Sorting Formats&lt;/h2&gt; 
&lt;p&gt;You can change the criteria for being considered the &lt;code&gt;best&lt;/code&gt; by using &lt;code&gt;-S&lt;/code&gt; (&lt;code&gt;--format-sort&lt;/code&gt;). The general format for this is &lt;code&gt;--format-sort field1,field2...&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The available fields are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;hasvid&lt;/code&gt;: Gives priority to formats that have a video stream&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;hasaud&lt;/code&gt;: Gives priority to formats that have an audio stream&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ie_pref&lt;/code&gt;: The format preference&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;lang&lt;/code&gt;: The language preference as determined by the extractor (e.g. original language preferred over audio description)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;quality&lt;/code&gt;: The quality of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;source&lt;/code&gt;: The preference of the source&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;proto&lt;/code&gt;: Protocol used for download (&lt;code&gt;https&lt;/code&gt;/&lt;code&gt;ftps&lt;/code&gt; &amp;gt; &lt;code&gt;http&lt;/code&gt;/&lt;code&gt;ftp&lt;/code&gt; &amp;gt; &lt;code&gt;m3u8_native&lt;/code&gt;/&lt;code&gt;m3u8&lt;/code&gt; &amp;gt; &lt;code&gt;http_dash_segments&lt;/code&gt;&amp;gt; &lt;code&gt;websocket_frag&lt;/code&gt; &amp;gt; &lt;code&gt;mms&lt;/code&gt;/&lt;code&gt;rtsp&lt;/code&gt; &amp;gt; &lt;code&gt;f4f&lt;/code&gt;/&lt;code&gt;f4m&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: Video Codec (&lt;code&gt;av01&lt;/code&gt; &amp;gt; &lt;code&gt;vp9.2&lt;/code&gt; &amp;gt; &lt;code&gt;vp9&lt;/code&gt; &amp;gt; &lt;code&gt;h265&lt;/code&gt; &amp;gt; &lt;code&gt;h264&lt;/code&gt; &amp;gt; &lt;code&gt;vp8&lt;/code&gt; &amp;gt; &lt;code&gt;h263&lt;/code&gt; &amp;gt; &lt;code&gt;theora&lt;/code&gt; &amp;gt; other)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;acodec&lt;/code&gt;: Audio Codec (&lt;code&gt;flac&lt;/code&gt;/&lt;code&gt;alac&lt;/code&gt; &amp;gt; &lt;code&gt;wav&lt;/code&gt;/&lt;code&gt;aiff&lt;/code&gt; &amp;gt; &lt;code&gt;opus&lt;/code&gt; &amp;gt; &lt;code&gt;vorbis&lt;/code&gt; &amp;gt; &lt;code&gt;aac&lt;/code&gt; &amp;gt; &lt;code&gt;mp4a&lt;/code&gt; &amp;gt; &lt;code&gt;mp3&lt;/code&gt; &amp;gt; &lt;code&gt;ac4&lt;/code&gt; &amp;gt; &lt;code&gt;eac3&lt;/code&gt; &amp;gt; &lt;code&gt;ac3&lt;/code&gt; &amp;gt; &lt;code&gt;dts&lt;/code&gt; &amp;gt; other)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;codec&lt;/code&gt;: Equivalent to &lt;code&gt;vcodec,acodec&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vext&lt;/code&gt;: Video Extension (&lt;code&gt;mp4&lt;/code&gt; &amp;gt; &lt;code&gt;mov&lt;/code&gt; &amp;gt; &lt;code&gt;webm&lt;/code&gt; &amp;gt; &lt;code&gt;flv&lt;/code&gt; &amp;gt; other). If &lt;code&gt;--prefer-free-formats&lt;/code&gt; is used, &lt;code&gt;webm&lt;/code&gt; is preferred.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;aext&lt;/code&gt;: Audio Extension (&lt;code&gt;m4a&lt;/code&gt; &amp;gt; &lt;code&gt;aac&lt;/code&gt; &amp;gt; &lt;code&gt;mp3&lt;/code&gt; &amp;gt; &lt;code&gt;ogg&lt;/code&gt; &amp;gt; &lt;code&gt;opus&lt;/code&gt; &amp;gt; &lt;code&gt;webm&lt;/code&gt; &amp;gt; other). If &lt;code&gt;--prefer-free-formats&lt;/code&gt; is used, the order changes to &lt;code&gt;ogg&lt;/code&gt; &amp;gt; &lt;code&gt;opus&lt;/code&gt; &amp;gt; &lt;code&gt;webm&lt;/code&gt; &amp;gt; &lt;code&gt;mp3&lt;/code&gt; &amp;gt; &lt;code&gt;m4a&lt;/code&gt; &amp;gt; &lt;code&gt;aac&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: Equivalent to &lt;code&gt;vext,aext&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filesize&lt;/code&gt;: Exact filesize, if known in advance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fs_approx&lt;/code&gt;: Approximate filesize&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;size&lt;/code&gt;: Exact filesize if available, otherwise approximate filesize&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;height&lt;/code&gt;: Height of video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;width&lt;/code&gt;: Width of video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;res&lt;/code&gt;: Video resolution, calculated as the smallest dimension.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fps&lt;/code&gt;: Framerate of video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;hdr&lt;/code&gt;: The dynamic range of the video (&lt;code&gt;DV&lt;/code&gt; &amp;gt; &lt;code&gt;HDR12&lt;/code&gt; &amp;gt; &lt;code&gt;HDR10+&lt;/code&gt; &amp;gt; &lt;code&gt;HDR10&lt;/code&gt; &amp;gt; &lt;code&gt;HLG&lt;/code&gt; &amp;gt; &lt;code&gt;SDR&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channels&lt;/code&gt;: The number of audio channels&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tbr&lt;/code&gt;: Total average bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vbr&lt;/code&gt;: Average video bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;abr&lt;/code&gt;: Average audio bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;br&lt;/code&gt;: Average bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;, &lt;code&gt;tbr&lt;/code&gt;/&lt;code&gt;vbr&lt;/code&gt;/&lt;code&gt;abr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;asr&lt;/code&gt;: Audio sample rate in Hz&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Deprecation warning&lt;/strong&gt;: Many of these fields have (currently undocumented) aliases, that may be removed in a future version. It is recommended to use only the documented field names.&lt;/p&gt; 
&lt;p&gt;All fields, unless specified otherwise, are sorted in descending order. To reverse this, prefix the field with a &lt;code&gt;+&lt;/code&gt;. E.g. &lt;code&gt;+res&lt;/code&gt; prefers format with the smallest resolution. Additionally, you can suffix a preferred value for the fields, separated by a &lt;code&gt;:&lt;/code&gt;. E.g. &lt;code&gt;res:720&lt;/code&gt; prefers larger videos, but no larger than 720p and the smallest video if there are no videos less than 720p. For &lt;code&gt;codec&lt;/code&gt; and &lt;code&gt;ext&lt;/code&gt;, you can provide two preferred values, the first for video and the second for audio. E.g. &lt;code&gt;+codec:avc:m4a&lt;/code&gt; (equivalent to &lt;code&gt;+vcodec:avc,+acodec:m4a&lt;/code&gt;) sets the video codec preference to &lt;code&gt;h264&lt;/code&gt; &amp;gt; &lt;code&gt;h265&lt;/code&gt; &amp;gt; &lt;code&gt;vp9&lt;/code&gt; &amp;gt; &lt;code&gt;vp9.2&lt;/code&gt; &amp;gt; &lt;code&gt;av01&lt;/code&gt; &amp;gt; &lt;code&gt;vp8&lt;/code&gt; &amp;gt; &lt;code&gt;h263&lt;/code&gt; &amp;gt; &lt;code&gt;theora&lt;/code&gt; and audio codec preference to &lt;code&gt;mp4a&lt;/code&gt; &amp;gt; &lt;code&gt;aac&lt;/code&gt; &amp;gt; &lt;code&gt;vorbis&lt;/code&gt; &amp;gt; &lt;code&gt;opus&lt;/code&gt; &amp;gt; &lt;code&gt;mp3&lt;/code&gt; &amp;gt; &lt;code&gt;ac3&lt;/code&gt; &amp;gt; &lt;code&gt;dts&lt;/code&gt;. You can also make the sorting prefer the nearest values to the provided by using &lt;code&gt;~&lt;/code&gt; as the delimiter. E.g. &lt;code&gt;filesize~1G&lt;/code&gt; prefers the format with filesize closest to 1 GiB.&lt;/p&gt; 
&lt;p&gt;The fields &lt;code&gt;hasvid&lt;/code&gt; and &lt;code&gt;ie_pref&lt;/code&gt; are always given highest priority in sorting, irrespective of the user-defined order. This behavior can be changed by using &lt;code&gt;--format-sort-force&lt;/code&gt;. Apart from these, the default order used is: &lt;code&gt;lang,quality,res,fps,hdr:12,vcodec,channels,acodec,size,br,asr,proto,ext,hasaud,source,id&lt;/code&gt;. The extractors may override this default order, but they cannot override the user-provided order.&lt;/p&gt; 
&lt;p&gt;Note that the default for hdr is &lt;code&gt;hdr:12&lt;/code&gt;; i.e. Dolby Vision is not preferred. This choice was made since DV formats are not yet fully compatible with most devices. This may be changed in the future.&lt;/p&gt; 
&lt;p&gt;If your format selector is &lt;code&gt;worst&lt;/code&gt;, the last item is selected after sorting. This means it will select the format that is worst in all respects. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use &lt;code&gt;-f best -S +size,+br,+res,+fps&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can use the &lt;code&gt;-v -F&lt;/code&gt; to see how the formats have been sorted (worst to best).&lt;/p&gt; 
&lt;h2&gt;Format Selection examples&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download and merge the best video-only format and the best audio-only format,
# or download the best combined format if video-only format is not available
$ yt-dlp -f "bv+ba/b"

# Download best format that contains video,
# and if it doesn't already have an audio stream, merge it with best audio-only format
$ yt-dlp -f "bv*+ba/b"

# Same as above
$ yt-dlp

# Download the best video-only format and the best audio-only format without merging them
# For this case, an output template should be used since
# by default, bestvideo and bestaudio will have the same file name.
$ yt-dlp -f "bv,ba" -o "%(title)s.f%(format_id)s.%(ext)s"

# Download and merge the best format that has a video stream,
# and all audio-only formats into one file
$ yt-dlp -f "bv*+mergeall[vcodec=none]" --audio-multistreams

# Download and merge the best format that has a video stream,
# and the best 2 audio-only formats into one file
$ yt-dlp -f "bv*+ba+ba.2" --audio-multistreams


# The following examples show the old method (without -S) of format selection
# and how to use -S to achieve a similar but (generally) better result

# Download the worst video available (old method)
$ yt-dlp -f "wv*+wa/w"

# Download the best video available but with the smallest resolution
$ yt-dlp -S "+res"

# Download the smallest video available
$ yt-dlp -S "+size,+br"



# Download the best mp4 video available, or the best video if no mp4 available
$ yt-dlp -f "bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b"

# Download the best video with the best extension
# (For video, mp4 &amp;gt; mov &amp;gt; webm &amp;gt; flv. For audio, m4a &amp;gt; aac &amp;gt; mp3 ...)
$ yt-dlp -S "ext"



# Download the best video available but no better than 480p,
# or the worst video if there is no video under 480p
$ yt-dlp -f "bv*[height&amp;lt;=480]+ba/b[height&amp;lt;=480] / wv*+ba/w"

# Download the best video available with the largest height but no better than 480p,
# or the best video with the smallest resolution if there is no video under 480p
$ yt-dlp -S "height:480"

# Download the best video available with the largest resolution but no better than 480p,
# or the best video with the smallest resolution if there is no video under 480p
# Resolution is determined by using the smallest dimension.
# So this works correctly for vertical videos as well
$ yt-dlp -S "res:480"



# Download the best video (that also has audio) but no bigger than 50 MB,
# or the worst video (that also has audio) if there is no video under 50 MB
$ yt-dlp -f "b[filesize&amp;lt;50M] / w"

# Download the largest video (that also has audio) but no bigger than 50 MB,
# or the smallest video (that also has audio) if there is no video under 50 MB
$ yt-dlp -f "b" -S "filesize:50M"

# Download the best video (that also has audio) that is closest in size to 50 MB
$ yt-dlp -f "b" -S "filesize~50M"



# Download best video available via direct link over HTTP/HTTPS protocol,
# or the best video available via any protocol if there is no such video
$ yt-dlp -f "(bv*+ba/b)[protocol^=http][protocol!*=dash] / (bv*+ba/b)"

# Download best video available via the best protocol
# (https/ftps &amp;gt; http/ftp &amp;gt; m3u8_native &amp;gt; m3u8 &amp;gt; http_dash_segments ...)
$ yt-dlp -S "proto"



# Download the best video with either h264 or h265 codec,
# or the best video if there is no such video
$ yt-dlp -f "(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)"

# Download the best video with best codec no better than h264,
# or the best video with worst codec if there is no such video
$ yt-dlp -S "codec:h264"

# Download the best video with worst codec no worse than h264,
# or the best video with best codec if there is no such video
$ yt-dlp -S "+codec:h264"



# More complex examples

# Download the best video no better than 720p preferring framerate greater than 30,
# or the worst video (still preferring framerate greater than 30) if there is no such video
$ yt-dlp -f "((bv*[fps&amp;gt;30]/bv*)[height&amp;lt;=720]/(wv*[fps&amp;gt;30]/wv*)) + ba / (b[fps&amp;gt;30]/b)[height&amp;lt;=720]/(w[fps&amp;gt;30]/w)"

# Download the video with the largest resolution no better than 720p,
# or the video with the smallest resolution available if there is no such video,
# preferring larger framerate for formats with the same resolution
$ yt-dlp -S "res:720,fps"



# Download the video with smallest resolution no worse than 480p,
# or the video with the largest resolution available if there is no such video,
# preferring better codec and then larger total bitrate for the same resolution
$ yt-dlp -S "+res:480,codec,br"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;MODIFYING METADATA&lt;/h1&gt; 
&lt;p&gt;The metadata obtained by the extractors can be modified by using &lt;code&gt;--parse-metadata&lt;/code&gt; and &lt;code&gt;--replace-in-metadata&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--replace-in-metadata FIELDS REGEX REPLACE&lt;/code&gt; is used to replace text in any metadata field using &lt;a href="https://docs.python.org/3/library/re.html#regular-expression-syntax"&gt;Python regular expression&lt;/a&gt;. &lt;a href="https://docs.python.org/3/library/re.html?highlight=backreferences#re.sub"&gt;Backreferences&lt;/a&gt; can be used in the replace string for advanced use.&lt;/p&gt; 
&lt;p&gt;The general syntax of &lt;code&gt;--parse-metadata FROM:TO&lt;/code&gt; is to give the name of a field or an &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; to extract data from, and the format to interpret it as, separated by a colon &lt;code&gt;:&lt;/code&gt;. Either a &lt;a href="https://docs.python.org/3/library/re.html#regular-expression-syntax"&gt;Python regular expression&lt;/a&gt; with named capture groups, a single field name, or a similar syntax to the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; (only &lt;code&gt;%(field)s&lt;/code&gt; formatting is supported) can be used for &lt;code&gt;TO&lt;/code&gt;. The option can be used multiple times to parse and modify various fields.&lt;/p&gt; 
&lt;p&gt;Note that these options preserve their relative order, allowing replacements to be made in parsed fields and vice versa. Also, any field thus created can be used in the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; and will also affect the media file's metadata added when using &lt;code&gt;--embed-metadata&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;This option also has a few special uses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can download an additional URL based on the metadata of the currently downloaded video. To do this, set the field &lt;code&gt;additional_urls&lt;/code&gt; to the URL that you want to download. E.g. &lt;code&gt;--parse-metadata "description:(?P&amp;lt;additional_urls&amp;gt;https?://www\.vimeo\.com/\d+)"&lt;/code&gt; will download the first vimeo video found in the description&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can use this to change the metadata that is embedded in the media file. To do this, set the value of the corresponding field with a &lt;code&gt;meta_&lt;/code&gt; prefix. For example, any value you set to &lt;code&gt;meta_description&lt;/code&gt; field will be added to the &lt;code&gt;description&lt;/code&gt; field in the file - you can use this to set a different "description" and "synopsis". To modify the metadata of individual streams, use the &lt;code&gt;meta&amp;lt;n&amp;gt;_&lt;/code&gt; prefix (e.g. &lt;code&gt;meta1_language&lt;/code&gt;). Any value set to the &lt;code&gt;meta_&lt;/code&gt; field will overwrite all default values.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Metadata modification happens before format selection, post-extraction and other post-processing operations. Some fields may be added or changed during these steps, overriding your changes.&lt;/p&gt; 
&lt;p&gt;For reference, these are the fields yt-dlp adds by default to the file metadata:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Metadata fields&lt;/th&gt; 
   &lt;th align="left"&gt;From&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;title&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;track&lt;/code&gt; or &lt;code&gt;title&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;date&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;upload_date&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;description&lt;/code&gt;, &lt;code&gt;synopsis&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;purl&lt;/code&gt;, &lt;code&gt;comment&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;webpage_url&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;track&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;track_number&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;artist&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;artist&lt;/code&gt;, &lt;code&gt;artists&lt;/code&gt;, &lt;code&gt;creator&lt;/code&gt;, &lt;code&gt;creators&lt;/code&gt;, &lt;code&gt;uploader&lt;/code&gt; or &lt;code&gt;uploader_id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;composer&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;composer&lt;/code&gt; or &lt;code&gt;composers&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;genre&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;genre&lt;/code&gt; or &lt;code&gt;genres&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;album&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;album&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;album_artist&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;album_artist&lt;/code&gt; or &lt;code&gt;album_artists&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;disc&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;disc_number&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;show&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;series&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;season_number&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;season_number&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;episode_id&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;episode&lt;/code&gt; or &lt;code&gt;episode_id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;episode_sort&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;episode_number&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;language&lt;/code&gt; of each stream&lt;/td&gt; 
   &lt;td align="left"&gt;the format's &lt;code&gt;language&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The file format may not support some of these fields&lt;/p&gt; 
&lt;h2&gt;Modifying metadata examples&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Interpret the title as "Artist - Title"
$ yt-dlp --parse-metadata "title:%(artist)s - %(title)s"

# Regex example
$ yt-dlp --parse-metadata "description:Artist - (?P&amp;lt;artist&amp;gt;.+)"

# Set title as "Series name S01E05"
$ yt-dlp --parse-metadata "%(series)s S%(season_number)02dE%(episode_number)02d:%(title)s"

# Prioritize uploader as the "artist" field in video metadata
$ yt-dlp --parse-metadata "%(uploader|)s:%(meta_artist)s" --embed-metadata

# Set "comment" field in video metadata using description instead of webpage_url,
# handling multiple lines correctly
$ yt-dlp --parse-metadata "description:(?s)(?P&amp;lt;meta_comment&amp;gt;.+)" --embed-metadata

# Do not set any "synopsis" in the video metadata
$ yt-dlp --parse-metadata ":(?P&amp;lt;meta_synopsis&amp;gt;)"

# Remove "formats" field from the infojson by setting it to an empty string
$ yt-dlp --parse-metadata "video::(?P&amp;lt;formats&amp;gt;)" --write-info-json

# Replace all spaces and "_" in title and uploader with a `-`
$ yt-dlp --replace-in-metadata "title,uploader" "[ _]" "-"

&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;EXTRACTOR ARGUMENTS&lt;/h1&gt; 
&lt;p&gt;Some extractors accept additional arguments which can be passed using &lt;code&gt;--extractor-args KEY:ARGS&lt;/code&gt;. &lt;code&gt;ARGS&lt;/code&gt; is a &lt;code&gt;;&lt;/code&gt; (semicolon) separated string of &lt;code&gt;ARG=VAL1,VAL2&lt;/code&gt;. E.g. &lt;code&gt;--extractor-args "youtube:player-client=tv,mweb;formats=incomplete" --extractor-args "twitter:api=syndication"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Note: In CLI, &lt;code&gt;ARG&lt;/code&gt; can use &lt;code&gt;-&lt;/code&gt; instead of &lt;code&gt;_&lt;/code&gt;; e.g. &lt;code&gt;youtube:player-client"&lt;/code&gt; becomes &lt;code&gt;youtube:player_client"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;The following extractors use this feature:&lt;/p&gt; 
&lt;h4&gt;youtube&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;lang&lt;/code&gt;: Prefer translated metadata (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt; etc) of this language code (case-sensitive). By default, the video primary language metadata is preferred, with a fallback to &lt;code&gt;en&lt;/code&gt; translated. See &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/415b4c9f955b1a0391204bd24a7132590e7b3bdb/yt_dlp/extractor/youtube/_base.py#L402-L409"&gt;youtube/_base.py&lt;/a&gt; for the list of supported content language codes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;skip&lt;/code&gt;: One or more of &lt;code&gt;hls&lt;/code&gt;, &lt;code&gt;dash&lt;/code&gt; or &lt;code&gt;translated_subs&lt;/code&gt; to skip extraction of the m3u8 manifests, dash manifests and &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/4090#issuecomment-1158102032"&gt;auto-translated subtitles&lt;/a&gt; respectively&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_client&lt;/code&gt;: Clients to extract video data from. The currently available clients are &lt;code&gt;web&lt;/code&gt;, &lt;code&gt;web_safari&lt;/code&gt;, &lt;code&gt;web_embedded&lt;/code&gt;, &lt;code&gt;web_music&lt;/code&gt;, &lt;code&gt;web_creator&lt;/code&gt;, &lt;code&gt;mweb&lt;/code&gt;, &lt;code&gt;ios&lt;/code&gt;, &lt;code&gt;android&lt;/code&gt;, &lt;code&gt;android_sdkless&lt;/code&gt;, &lt;code&gt;android_vr&lt;/code&gt;, &lt;code&gt;tv&lt;/code&gt;, &lt;code&gt;tv_simply&lt;/code&gt; and &lt;code&gt;tv_embedded&lt;/code&gt;. By default, &lt;code&gt;android_sdkless,tv,web_safari,web&lt;/code&gt; is used. &lt;code&gt;android_sdkless&lt;/code&gt; is omitted if cookies are passed. If premium cookies are passed, &lt;code&gt;tv,web_creator,web_safari,web&lt;/code&gt; is used instead. The &lt;code&gt;web_music&lt;/code&gt; client is added for &lt;code&gt;music.youtube.com&lt;/code&gt; URLs when logged-in cookies are used. The &lt;code&gt;web_embedded&lt;/code&gt; client is added for age-restricted videos but only works if the video is embeddable. The &lt;code&gt;tv_embedded&lt;/code&gt; and &lt;code&gt;web_creator&lt;/code&gt; clients are added for age-restricted videos if account age-verification is required. Some clients, such as &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;web_music&lt;/code&gt;, require a &lt;code&gt;po_token&lt;/code&gt; for their formats to be downloadable. Some clients, such as &lt;code&gt;web_creator&lt;/code&gt;, will only work with authentication. Not all clients support authentication via cookies. You can use &lt;code&gt;default&lt;/code&gt; for the default clients, or you can use &lt;code&gt;all&lt;/code&gt; for all clients (not recommended). You can prefix a client with &lt;code&gt;-&lt;/code&gt; to exclude it, e.g. &lt;code&gt;youtube:player_client=default,-ios&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_skip&lt;/code&gt;: Skip some network requests that are generally needed for robust extraction. One or more of &lt;code&gt;configs&lt;/code&gt; (skip client configs), &lt;code&gt;webpage&lt;/code&gt; (skip initial webpage), &lt;code&gt;js&lt;/code&gt; (skip js player), &lt;code&gt;initial_data&lt;/code&gt; (skip initial data/next ep request). While these options can help reduce the number of requests needed or avoid some rate-limiting, they could cause issues such as missing formats or metadata. See &lt;a href="https://github.com/yt-dlp/yt-dlp/pull/860"&gt;#860&lt;/a&gt; and &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/12826"&gt;#12826&lt;/a&gt; for more details&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webpage_skip&lt;/code&gt;: Skip extraction of embedded webpage data. One or both of &lt;code&gt;player_response&lt;/code&gt;, &lt;code&gt;initial_data&lt;/code&gt;. These options are for testing purposes and don't skip any network requests&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_params&lt;/code&gt;: YouTube player parameters to use for player requests. Will overwrite any default ones set by yt-dlp.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_js_variant&lt;/code&gt;: The player javascript variant to use for n/sig deciphering. The known variants are: &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;tcc&lt;/code&gt;, &lt;code&gt;tce&lt;/code&gt;, &lt;code&gt;es5&lt;/code&gt;, &lt;code&gt;es6&lt;/code&gt;, &lt;code&gt;tv&lt;/code&gt;, &lt;code&gt;tv_es6&lt;/code&gt;, &lt;code&gt;phone&lt;/code&gt;, &lt;code&gt;tablet&lt;/code&gt;. The default is &lt;code&gt;main&lt;/code&gt;, and the others are for debugging purposes. You can use &lt;code&gt;actual&lt;/code&gt; to go with what is prescribed by the site&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_js_version&lt;/code&gt;: The player javascript version to use for n/sig deciphering, in the format of &lt;code&gt;signature_timestamp@hash&lt;/code&gt; (e.g. &lt;code&gt;20348@0004de42&lt;/code&gt;). The default is to use what is prescribed by the site, and can be selected with &lt;code&gt;actual&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;comment_sort&lt;/code&gt;: &lt;code&gt;top&lt;/code&gt; or &lt;code&gt;new&lt;/code&gt; (default) - choose comment sorting mode (on YouTube's side)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;max_comments&lt;/code&gt;: Limit the amount of comments to gather. Comma-separated list of integers representing &lt;code&gt;max-comments,max-parents,max-replies,max-replies-per-thread&lt;/code&gt;. Default is &lt;code&gt;all,all,all,all&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;E.g. &lt;code&gt;all,all,1000,10&lt;/code&gt; will get a maximum of 1000 replies total, with up to 10 replies per thread. &lt;code&gt;1000,all,100&lt;/code&gt; will get a maximum of 1000 comments, with a maximum of 100 replies total&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;formats&lt;/code&gt;: Change the types of formats to return. &lt;code&gt;dashy&lt;/code&gt; (convert HTTP to DASH), &lt;code&gt;duplicate&lt;/code&gt; (identical content but different URLs or protocol; includes &lt;code&gt;dashy&lt;/code&gt;), &lt;code&gt;incomplete&lt;/code&gt; (cannot be downloaded completely - live dash and post-live m3u8), &lt;code&gt;missing_pot&lt;/code&gt; (include formats that require a PO Token but are missing one)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;innertube_host&lt;/code&gt;: Innertube API host to use for all API requests; e.g. &lt;code&gt;studio.youtube.com&lt;/code&gt;, &lt;code&gt;youtubei.googleapis.com&lt;/code&gt;. Note that cookies exported from one subdomain will not work on others&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;innertube_key&lt;/code&gt;: Innertube API key to use for all API requests. By default, no API key is used&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;raise_incomplete_data&lt;/code&gt;: &lt;code&gt;Incomplete Data Received&lt;/code&gt; raises an error instead of reporting a warning&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;data_sync_id&lt;/code&gt;: Overrides the account Data Sync ID used in Innertube API requests. This may be needed if you are using an account with &lt;code&gt;youtube:player_skip=webpage,configs&lt;/code&gt; or &lt;code&gt;youtubetab:skip=webpage&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;visitor_data&lt;/code&gt;: Overrides the Visitor Data used in Innertube API requests. This should be used with &lt;code&gt;player_skip=webpage,configs&lt;/code&gt; and without cookies. Note: this may have adverse effects if used improperly. If a session from a browser is wanted, you should pass cookies instead (which contain the Visitor ID)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;po_token&lt;/code&gt;: Proof of Origin (PO) Token(s) to use. Comma seperated list of PO Tokens in the format &lt;code&gt;CLIENT.CONTEXT+PO_TOKEN&lt;/code&gt;, e.g. &lt;code&gt;youtube:po_token=web.gvs+XXX,web.player=XXX,web_safari.gvs+YYY&lt;/code&gt;. Context can be any of &lt;code&gt;gvs&lt;/code&gt; (Google Video Server URLs), &lt;code&gt;player&lt;/code&gt; (Innertube player request) or &lt;code&gt;subs&lt;/code&gt; (Subtitles)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pot_trace&lt;/code&gt;: Enable debug logging for PO Token fetching. Either &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; (default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fetch_pot&lt;/code&gt;: Policy to use for fetching a PO Token from providers. One of &lt;code&gt;always&lt;/code&gt; (always try fetch a PO Token regardless if the client requires one for the given context), &lt;code&gt;never&lt;/code&gt; (never fetch a PO Token), or &lt;code&gt;auto&lt;/code&gt; (default; only fetch a PO Token if the client requires one for the given context)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playback_wait&lt;/code&gt;: Duration (in seconds) to wait inbetween the extraction and download stages in order to ensure the formats are available. The default is &lt;code&gt;6&lt;/code&gt; seconds&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;youtubepot-webpo&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bind_to_visitor_id&lt;/code&gt;: Whether to use the Visitor ID instead of Visitor Data for caching WebPO tokens. Either &lt;code&gt;true&lt;/code&gt; (default) or &lt;code&gt;false&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;youtubetab (YouTube playlists, channels, feeds, etc.)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;skip&lt;/code&gt;: One or more of &lt;code&gt;webpage&lt;/code&gt; (skip initial webpage download), &lt;code&gt;authcheck&lt;/code&gt; (allow the download of playlists requiring authentication when no initial webpage is downloaded. This may cause unwanted behavior, see &lt;a href="https://github.com/yt-dlp/yt-dlp/pull/1122"&gt;#1122&lt;/a&gt; for more details)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;approximate_date&lt;/code&gt;: Extract approximate &lt;code&gt;upload_date&lt;/code&gt; and &lt;code&gt;timestamp&lt;/code&gt; in flat-playlist. This may cause date-based filters to be slightly off&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;generic&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;fragment_query&lt;/code&gt;: Passthrough any query in mpd/m3u8 manifest URLs to their fragments if no value is provided, or else apply the query string given as &lt;code&gt;fragment_query=VALUE&lt;/code&gt;. Note that if the stream has an HLS AES-128 key, then the query parameters will be passed to the key URI as well, unless the &lt;code&gt;key_query&lt;/code&gt; extractor-arg is passed, or unless an external key URI is provided via the &lt;code&gt;hls_key&lt;/code&gt; extractor-arg. Does not apply to ffmpeg&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;variant_query&lt;/code&gt;: Passthrough the master m3u8 URL query to its variant playlist URLs if no value is provided, or else apply the query string given as &lt;code&gt;variant_query=VALUE&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;key_query&lt;/code&gt;: Passthrough the master m3u8 URL query to its HLS AES-128 decryption key URI if no value is provided, or else apply the query string given as &lt;code&gt;key_query=VALUE&lt;/code&gt;. Note that this will have no effect if the key URI is provided via the &lt;code&gt;hls_key&lt;/code&gt; extractor-arg. Does not apply to ffmpeg&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;hls_key&lt;/code&gt;: An HLS AES-128 key URI &lt;em&gt;or&lt;/em&gt; key (as hex), and optionally the IV (as hex), in the form of &lt;code&gt;(URI|KEY)[,IV]&lt;/code&gt;; e.g. &lt;code&gt;generic:hls_key=ABCDEF1234567980,0xFEDCBA0987654321&lt;/code&gt;. Passing any of these values will force usage of the native HLS downloader and override the corresponding values found in the m3u8 playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;is_live&lt;/code&gt;: Bypass live HLS detection and manually set &lt;code&gt;live_status&lt;/code&gt; - a value of &lt;code&gt;false&lt;/code&gt; will set &lt;code&gt;not_live&lt;/code&gt;, any other value (or no value) will set &lt;code&gt;is_live&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;impersonate&lt;/code&gt;: Target(s) to try and impersonate with the initial webpage request; e.g. &lt;code&gt;generic:impersonate=safari,chrome-110&lt;/code&gt;. Use &lt;code&gt;generic:impersonate&lt;/code&gt; to impersonate any available target, and use &lt;code&gt;generic:impersonate=false&lt;/code&gt; to disable impersonation (default)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;vikichannel&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;video_types&lt;/code&gt;: Types of videos to download - one or more of &lt;code&gt;episodes&lt;/code&gt;, &lt;code&gt;movies&lt;/code&gt;, &lt;code&gt;clips&lt;/code&gt;, &lt;code&gt;trailers&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;youtubewebarchive&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;check_all&lt;/code&gt;: Try to check more at the cost of more requests. One or more of &lt;code&gt;thumbnails&lt;/code&gt;, &lt;code&gt;captures&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;gamejolt&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;comment_sort&lt;/code&gt;: &lt;code&gt;hot&lt;/code&gt; (default), &lt;code&gt;you&lt;/code&gt; (cookies needed), &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;new&lt;/code&gt; - choose comment sorting mode (on GameJolt's side)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;hotstar&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;res&lt;/code&gt;: resolution to ignore - one or more of &lt;code&gt;sd&lt;/code&gt;, &lt;code&gt;hd&lt;/code&gt;, &lt;code&gt;fhd&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: vcodec to ignore - one or more of &lt;code&gt;h264&lt;/code&gt;, &lt;code&gt;h265&lt;/code&gt;, &lt;code&gt;dvh265&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dr&lt;/code&gt;: dynamic range to ignore - one or more of &lt;code&gt;sdr&lt;/code&gt;, &lt;code&gt;hdr10&lt;/code&gt;, &lt;code&gt;dv&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;instagram&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;app_id&lt;/code&gt;: The value of the &lt;code&gt;X-IG-App-ID&lt;/code&gt; header used for API requests. Default is the web app ID, &lt;code&gt;936619743392459&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;niconicochannelplus&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;max_comments&lt;/code&gt;: Maximum number of comments to extract - default is &lt;code&gt;120&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;tiktok&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;api_hostname&lt;/code&gt;: Hostname to use for mobile API calls, e.g. &lt;code&gt;api22-normal-c-alisg.tiktokv.com&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;app_name&lt;/code&gt;: Default app name to use with mobile API calls, e.g. &lt;code&gt;trill&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;app_version&lt;/code&gt;: Default app version to use with mobile API calls - should be set along with &lt;code&gt;manifest_app_version&lt;/code&gt;, e.g. &lt;code&gt;34.1.2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;manifest_app_version&lt;/code&gt;: Default numeric app version to use with mobile API calls, e.g. &lt;code&gt;2023401020&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;aid&lt;/code&gt;: Default app ID to use with mobile API calls, e.g. &lt;code&gt;1180&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;app_info&lt;/code&gt;: Enable mobile API extraction with one or more app info strings in the format of &lt;code&gt;&amp;lt;iid&amp;gt;/[app_name]/[app_version]/[manifest_app_version]/[aid]&lt;/code&gt;, where &lt;code&gt;iid&lt;/code&gt; is the unique app install ID. &lt;code&gt;iid&lt;/code&gt; is the only required value; all other values and their &lt;code&gt;/&lt;/code&gt; separators can be omitted, e.g. &lt;code&gt;tiktok:app_info=1234567890123456789&lt;/code&gt; or &lt;code&gt;tiktok:app_info=123,456/trill///1180,789//34.0.1/340001&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;device_id&lt;/code&gt;: Enable mobile API extraction with a genuine device ID to be used with mobile API calls. Default is a random 19-digit string&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;rokfinchannel&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;tab&lt;/code&gt;: Which tab to download - one of &lt;code&gt;new&lt;/code&gt;, &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;videos&lt;/code&gt;, &lt;code&gt;podcasts&lt;/code&gt;, &lt;code&gt;streams&lt;/code&gt;, &lt;code&gt;stacks&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;twitter&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;api&lt;/code&gt;: Select one of &lt;code&gt;graphql&lt;/code&gt; (default), &lt;code&gt;legacy&lt;/code&gt; or &lt;code&gt;syndication&lt;/code&gt; as the API for tweet extraction. Has no effect if logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;stacommu, wrestleuniverse&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;device_id&lt;/code&gt;: UUID value assigned by the website and used to enforce device limits for paid livestream content. Can be found in browser local storage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;twitch&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;client_id&lt;/code&gt;: Client ID value to be sent with GraphQL requests, e.g. &lt;code&gt;twitch:client_id=kimne78kx3ncx6brgo4mv6wki5h1ko&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;nhkradirulive (NHK ã‚‰ã˜ã‚‹â˜…ã‚‰ã˜ã‚‹ LIVE)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;area&lt;/code&gt;: Which regional variation to extract. Valid areas are: &lt;code&gt;sapporo&lt;/code&gt;, &lt;code&gt;sendai&lt;/code&gt;, &lt;code&gt;tokyo&lt;/code&gt;, &lt;code&gt;nagoya&lt;/code&gt;, &lt;code&gt;osaka&lt;/code&gt;, &lt;code&gt;hiroshima&lt;/code&gt;, &lt;code&gt;matsuyama&lt;/code&gt;, &lt;code&gt;fukuoka&lt;/code&gt;. Defaults to &lt;code&gt;tokyo&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;nflplusreplay&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type&lt;/code&gt;: Type(s) of game replays to extract. Valid types are: &lt;code&gt;full_game&lt;/code&gt;, &lt;code&gt;full_game_spanish&lt;/code&gt;, &lt;code&gt;condensed_game&lt;/code&gt; and &lt;code&gt;all_22&lt;/code&gt;. You can use &lt;code&gt;all&lt;/code&gt; to extract all available replay types, which is the default&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;jiocinema&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;refresh_token&lt;/code&gt;: The &lt;code&gt;refreshToken&lt;/code&gt; UUID from browser local storage can be passed to extend the life of your login session when logging in with &lt;code&gt;token&lt;/code&gt; as username and the &lt;code&gt;accessToken&lt;/code&gt; from browser local storage as password&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;jiosaavn&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bitrate&lt;/code&gt;: Audio bitrates to request. One or more of &lt;code&gt;16&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt;, &lt;code&gt;64&lt;/code&gt;, &lt;code&gt;128&lt;/code&gt;, &lt;code&gt;320&lt;/code&gt;. Default is &lt;code&gt;128,320&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;afreecatvlive&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cdn&lt;/code&gt;: One or more CDN IDs to use with the API call for stream URLs, e.g. &lt;code&gt;gcp_cdn&lt;/code&gt;, &lt;code&gt;gs_cdn_pc_app&lt;/code&gt;, &lt;code&gt;gs_cdn_mobile_web&lt;/code&gt;, &lt;code&gt;gs_cdn_pc_web&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;soundcloud&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;formats&lt;/code&gt;: Formats to request from the API. Requested values should be in the format of &lt;code&gt;{protocol}_{codec}&lt;/code&gt;, e.g. &lt;code&gt;hls_opus,http_aac&lt;/code&gt;. The &lt;code&gt;*&lt;/code&gt; character functions as a wildcard, e.g. &lt;code&gt;*_mp3&lt;/code&gt;, and can be passed by itself to request all formats. Known protocols include &lt;code&gt;http&lt;/code&gt;, &lt;code&gt;hls&lt;/code&gt; and &lt;code&gt;hls-aes&lt;/code&gt;; known codecs include &lt;code&gt;aac&lt;/code&gt;, &lt;code&gt;opus&lt;/code&gt; and &lt;code&gt;mp3&lt;/code&gt;. Original &lt;code&gt;download&lt;/code&gt; formats are always extracted. Default is &lt;code&gt;http_aac,hls_aac,http_opus,hls_opus,http_mp3,hls_mp3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;orfon (orf:on)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;prefer_segments_playlist&lt;/code&gt;: Prefer a playlist of program segments instead of a single complete video when available. If individual segments are desired, use &lt;code&gt;--concat-playlist never --extractor-args "orfon:prefer_segments_playlist"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;bilibili&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;prefer_multi_flv&lt;/code&gt;: Prefer extracting flv formats over mp4 for older videos that still provide legacy formats&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;sonylivseries&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;sort_order&lt;/code&gt;: Episode sort order for series extraction - one of &lt;code&gt;asc&lt;/code&gt; (ascending, oldest first) or &lt;code&gt;desc&lt;/code&gt; (descending, newest first). Default is &lt;code&gt;asc&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;tver&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;backend&lt;/code&gt;: Backend API to use for extraction - one of &lt;code&gt;streaks&lt;/code&gt; (default) or &lt;code&gt;brightcove&lt;/code&gt; (deprecated)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;vimeo&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;client&lt;/code&gt;: Client to extract video data from. The currently available clients are &lt;code&gt;android&lt;/code&gt;, &lt;code&gt;ios&lt;/code&gt;, and &lt;code&gt;web&lt;/code&gt;. Only one client can be used. The &lt;code&gt;web&lt;/code&gt; client is used by default. The &lt;code&gt;web&lt;/code&gt; client only works with account cookies or login credentials. The &lt;code&gt;android&lt;/code&gt; and &lt;code&gt;ios&lt;/code&gt; clients only work with previously cached OAuth tokens&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;original_format_policy&lt;/code&gt;: Policy for when to try extracting original formats. One of &lt;code&gt;always&lt;/code&gt;, &lt;code&gt;never&lt;/code&gt;, or &lt;code&gt;auto&lt;/code&gt;. The default &lt;code&gt;auto&lt;/code&gt; policy tries to avoid exceeding the web client's API rate-limit by only making an extra request when Vimeo publicizes the video's downloadability&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: These options may be changed/removed in the future without concern for backward compatibility&lt;/p&gt; 
&lt;!-- MANPAGE: MOVE "INSTALLATION" SECTION HERE --&gt; 
&lt;h1&gt;PLUGINS&lt;/h1&gt; 
&lt;p&gt;Note that &lt;strong&gt;all&lt;/strong&gt; plugins are imported even if not invoked, and that &lt;strong&gt;there are no checks&lt;/strong&gt; performed on plugin code. &lt;strong&gt;Use plugins at your own risk and only if you trust the code!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Plugins can be of &lt;code&gt;&amp;lt;type&amp;gt;&lt;/code&gt;s &lt;code&gt;extractor&lt;/code&gt; or &lt;code&gt;postprocessor&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Extractor plugins do not need to be enabled from the CLI and are automatically invoked when the input URL is suitable for it.&lt;/li&gt; 
 &lt;li&gt;Extractor plugins take priority over built-in extractors.&lt;/li&gt; 
 &lt;li&gt;Postprocessor plugins can be invoked using &lt;code&gt;--use-postprocessor NAME&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Plugins are loaded from the namespace packages &lt;code&gt;yt_dlp_plugins.extractor&lt;/code&gt; and &lt;code&gt;yt_dlp_plugins.postprocessor&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In other words, the file structure on the disk looks something like:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    yt_dlp_plugins/
        extractor/
            myplugin.py
        postprocessor/
            myplugin.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;yt-dlp looks for these &lt;code&gt;yt_dlp_plugins&lt;/code&gt; namespace folders in many locations (see below) and loads in plugins from &lt;strong&gt;all&lt;/strong&gt; of them. Set the environment variable &lt;code&gt;YTDLP_NO_PLUGINS&lt;/code&gt; to something nonempty to disable loading plugins entirely.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Plugins"&gt;wiki for some known plugins&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installing Plugins&lt;/h2&gt; 
&lt;p&gt;Plugins can be installed using various methods and locations.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configuration directories&lt;/strong&gt;: Plugin packages (containing a &lt;code&gt;yt_dlp_plugins&lt;/code&gt; namespace folder) can be dropped into the following standard &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;configuration locations&lt;/a&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;User Plugins&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp/plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt; (recommended on Linux/macOS)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp/plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt; (recommended on Windows)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;~/.yt-dlp/plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;~/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;System Plugins&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;/etc/yt-dlp/plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;/etc/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Executable location&lt;/strong&gt;: Plugin packages can similarly be installed in a &lt;code&gt;yt-dlp-plugins&lt;/code&gt; directory under the executable location (recommended for portable installations):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Binary: where &lt;code&gt;&amp;lt;root-dir&amp;gt;/yt-dlp.exe&lt;/code&gt;, &lt;code&gt;&amp;lt;root-dir&amp;gt;/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Source: where &lt;code&gt;&amp;lt;root-dir&amp;gt;/yt_dlp/__main__.py&lt;/code&gt;, &lt;code&gt;&amp;lt;root-dir&amp;gt;/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pip and other locations in &lt;code&gt;PYTHONPATH&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Plugin packages can be installed and managed using &lt;code&gt;pip&lt;/code&gt;. See &lt;a href="https://github.com/yt-dlp/yt-dlp-sample-plugins"&gt;yt-dlp-sample-plugins&lt;/a&gt; for an example. 
    &lt;ul&gt; 
     &lt;li&gt;Note: plugin files between plugin packages installed with pip must have unique filenames.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Any path in &lt;code&gt;PYTHONPATH&lt;/code&gt; is searched in for the &lt;code&gt;yt_dlp_plugins&lt;/code&gt; namespace folder. 
    &lt;ul&gt; 
     &lt;li&gt;Note: This does not apply for Pyinstaller builds.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;.zip&lt;/code&gt;, &lt;code&gt;.egg&lt;/code&gt; and &lt;code&gt;.whl&lt;/code&gt; archives containing a &lt;code&gt;yt_dlp_plugins&lt;/code&gt; namespace folder in their root are also supported as plugin packages.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;e.g. &lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp/plugins/mypluginpkg.zip&lt;/code&gt; where &lt;code&gt;mypluginpkg.zip&lt;/code&gt; contains &lt;code&gt;yt_dlp_plugins/&amp;lt;type&amp;gt;/myplugin.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run yt-dlp with &lt;code&gt;--verbose&lt;/code&gt; to check if the plugin has been loaded.&lt;/p&gt; 
&lt;h2&gt;Developing Plugins&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/yt-dlp/yt-dlp-sample-plugins"&gt;yt-dlp-sample-plugins&lt;/a&gt; repo for a template plugin package and the &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Plugin-Development"&gt;Plugin Development&lt;/a&gt; section of the wiki for a plugin development guide.&lt;/p&gt; 
&lt;p&gt;All public classes with a name ending in &lt;code&gt;IE&lt;/code&gt;/&lt;code&gt;PP&lt;/code&gt; are imported from each file for extractors and postprocessors respectively. This respects underscore prefix (e.g. &lt;code&gt;_MyBasePluginIE&lt;/code&gt; is private) and &lt;code&gt;__all__&lt;/code&gt;. Modules can similarly be excluded by prefixing the module name with an underscore (e.g. &lt;code&gt;_myplugin.py&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;To replace an existing extractor with a subclass of one, set the &lt;code&gt;plugin_name&lt;/code&gt; class keyword argument (e.g. &lt;code&gt;class MyPluginIE(ABuiltInIE, plugin_name='myplugin')&lt;/code&gt; will replace &lt;code&gt;ABuiltInIE&lt;/code&gt; with &lt;code&gt;MyPluginIE&lt;/code&gt;). Since the extractor replaces the parent, you should exclude the subclass extractor from being imported separately by making it private using one of the methods described above.&lt;/p&gt; 
&lt;p&gt;If you are a plugin author, add &lt;a href="https://github.com/topics/yt-dlp-plugins"&gt;yt-dlp-plugins&lt;/a&gt; as a topic to your repository for discoverability.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/CONTRIBUTING.md#developer-instructions"&gt;Developer Instructions&lt;/a&gt; on how to write and test an extractor.&lt;/p&gt; 
&lt;h1&gt;EMBEDDING YT-DLP&lt;/h1&gt; 
&lt;p&gt;yt-dlp makes the best effort to be a good command-line program, and thus should be callable from any programming language.&lt;/p&gt; 
&lt;p&gt;Your program should avoid parsing the normal stdout since they may change in future versions. Instead, they should use options such as &lt;code&gt;-J&lt;/code&gt;, &lt;code&gt;--print&lt;/code&gt;, &lt;code&gt;--progress-template&lt;/code&gt;, &lt;code&gt;--exec&lt;/code&gt; etc to create console output that you can reliably reproduce and parse.&lt;/p&gt; 
&lt;p&gt;From a Python program, you can embed yt-dlp in a more powerful fashion, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from yt_dlp import YoutubeDL

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']
with YoutubeDL() as ydl:
    ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Most likely, you'll want to use various options. For a list of options available, have a look at &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/yt_dlp/YoutubeDL.py#L183"&gt;&lt;code&gt;yt_dlp/YoutubeDL.py&lt;/code&gt;&lt;/a&gt; or &lt;code&gt;help(yt_dlp.YoutubeDL)&lt;/code&gt; in a Python shell. If you are already familiar with the CLI, you can use &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/devscripts/cli_to_api.py"&gt;&lt;code&gt;devscripts/cli_to_api.py&lt;/code&gt;&lt;/a&gt; to translate any CLI switches to &lt;code&gt;YoutubeDL&lt;/code&gt; params.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: If you are porting your code from youtube-dl to yt-dlp, one important point to look out for is that we do not guarantee the return value of &lt;code&gt;YoutubeDL.extract_info&lt;/code&gt; to be json serializable, or even be a dictionary. It will be dictionary-like, but if you want to ensure it is a serializable dictionary, pass it through &lt;code&gt;YoutubeDL.sanitize_info&lt;/code&gt; as shown in the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#extracting-information"&gt;example below&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Embedding examples&lt;/h2&gt; 
&lt;h4&gt;Extracting information&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import json
import yt_dlp

URL = 'https://www.youtube.com/watch?v=BaW_jenozKc'

# â„¹ï¸ See help(yt_dlp.YoutubeDL) for a list of available options and public functions
ydl_opts = {}
with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    info = ydl.extract_info(URL, download=False)

    # â„¹ï¸ ydl.sanitize_info makes the info json-serializable
    print(json.dumps(ydl.sanitize_info(info)))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Download using an info-json&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

INFO_FILE = 'path/to/video.info.json'

with yt_dlp.YoutubeDL() as ydl:
    error_code = ydl.download_with_info_file(INFO_FILE)

print('Some videos failed to download' if error_code
      else 'All videos successfully downloaded')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extract audio&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

ydl_opts = {
    'format': 'm4a/bestaudio/best',
    # â„¹ï¸ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments
    'postprocessors': [{  # Extract audio using ffmpeg
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'm4a',
    }]
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Filter videos&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

def longer_than_a_minute(info, *, incomplete):
    """Download only videos longer than a minute (or with unknown duration)"""
    duration = info.get('duration')
    if duration and duration &amp;lt; 60:
        return 'The video is too short'

ydl_opts = {
    'match_filter': longer_than_a_minute,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Adding logger and progress hook&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

class MyLogger:
    def debug(self, msg):
        # For compatibility with youtube-dl, both debug and info are passed into debug
        # You can distinguish them by the prefix '[debug] '
        if msg.startswith('[debug] '):
            pass
        else:
            self.info(msg)

    def info(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        print(msg)


# â„¹ï¸ See "progress_hooks" in help(yt_dlp.YoutubeDL)
def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading, now post-processing ...')


ydl_opts = {
    'logger': MyLogger(),
    'progress_hooks': [my_hook],
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Add a custom PostProcessor&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

# â„¹ï¸ See help(yt_dlp.postprocessor.PostProcessor)
class MyCustomPP(yt_dlp.postprocessor.PostProcessor):
    def run(self, info):
        self.to_screen('Doing stuff')
        return [], info


with yt_dlp.YoutubeDL() as ydl:
    # â„¹ï¸ "when" can take any value in yt_dlp.utils.POSTPROCESS_WHEN
    ydl.add_post_processor(MyCustomPP(), when='pre_process')
    ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Use a custom format selector&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

def format_selector(ctx):
    """ Select the best video and the best audio that won't result in an mkv.
    NOTE: This is just an example and does not handle all cases """

    # formats are already sorted worst to best
    formats = ctx.get('formats')[::-1]

    # acodec='none' means there is no audio
    best_video = next(f for f in formats
                      if f['vcodec'] != 'none' and f['acodec'] == 'none')

    # find compatible audio extension
    audio_ext = {'mp4': 'm4a', 'webm': 'webm'}[best_video['ext']]
    # vcodec='none' means there is no video
    best_audio = next(f for f in formats if (
        f['acodec'] != 'none' and f['vcodec'] == 'none' and f['ext'] == audio_ext))

    # These are the minimum required fields for a merged format
    yield {
        'format_id': f'{best_video["format_id"]}+{best_audio["format_id"]}',
        'ext': best_video['ext'],
        'requested_formats': [best_video, best_audio],
        # Must be + separated list of protocols
        'protocol': f'{best_video["protocol"]}+{best_audio["protocol"]}'
    }


ydl_opts = {
    'format': format_selector,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;CHANGES FROM YOUTUBE-DL&lt;/h1&gt; 
&lt;h3&gt;New features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Forked from &lt;a href="https://github.com/blackjack4494/yt-dlc/commit/f9401f2a91987068139c5f757b12fc711d4c0cee"&gt;&lt;strong&gt;yt-dlc@f9401f2&lt;/strong&gt;&lt;/a&gt; and merged with &lt;a href="https://github.com/ytdl-org/youtube-dl/commit/a08f2b7e4567cdc50c0614ee0a4ffdff49b8b6e6"&gt;&lt;strong&gt;youtube-dl@a08f2b7&lt;/strong&gt;&lt;/a&gt; (&lt;a href="https://github.com/yt-dlp/yt-dlp/issues/21"&gt;exceptions&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sponsorblock-options"&gt;SponsorBlock Integration&lt;/a&gt;&lt;/strong&gt;: You can mark/remove sponsor sections in YouTube videos by utilizing the &lt;a href="https://sponsor.ajay.app"&gt;SponsorBlock&lt;/a&gt; API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats"&gt;Format Sorting&lt;/a&gt;&lt;/strong&gt;: The default format sorting options have been changed so that higher resolution and better codecs will be now preferred instead of simply using larger bitrate. Furthermore, you can now specify the sort order using &lt;code&gt;-S&lt;/code&gt;. This allows for much easier format selection than what is possible by simply using &lt;code&gt;--format&lt;/code&gt; (&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection-examples"&gt;examples&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Merged with animelover1984/youtube-dl&lt;/strong&gt;: You get most of the features and improvements from &lt;a href="https://github.com/animelover1984/youtube-dl"&gt;animelover1984/youtube-dl&lt;/a&gt; including &lt;code&gt;--write-comments&lt;/code&gt;, &lt;code&gt;BiliBiliSearch&lt;/code&gt;, &lt;code&gt;BilibiliChannel&lt;/code&gt;, Embedding thumbnail in mp4/ogg/opus, playlist infojson etc. See &lt;a href="https://github.com/yt-dlp/yt-dlp/pull/31"&gt;#31&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;YouTube improvements&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Supports Clips, Stories (&lt;code&gt;ytstories:&amp;lt;channel UCID&amp;gt;&lt;/code&gt;), Search (including filters)&lt;strong&gt;*&lt;/strong&gt;, YouTube Music Search, Channel-specific search, Search prefixes (&lt;code&gt;ytsearch:&lt;/code&gt;, &lt;code&gt;ytsearchdate:&lt;/code&gt;)&lt;strong&gt;*&lt;/strong&gt;, Mixes, and Feeds (&lt;code&gt;:ytfav&lt;/code&gt;, &lt;code&gt;:ytwatchlater&lt;/code&gt;, &lt;code&gt;:ytsubs&lt;/code&gt;, &lt;code&gt;:ythistory&lt;/code&gt;, &lt;code&gt;:ytrec&lt;/code&gt;, &lt;code&gt;:ytnotif&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;Fix for &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/29326"&gt;n-sig based throttling&lt;/a&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;Download livestreams from the start using &lt;code&gt;--live-from-start&lt;/code&gt; (&lt;em&gt;experimental&lt;/em&gt;)&lt;/li&gt; 
   &lt;li&gt;Channel URLs download all uploads of the channel, including shorts and live&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cookies from browser&lt;/strong&gt;: Cookies can be automatically extracted from all major web browsers using &lt;code&gt;--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Download time range&lt;/strong&gt;: Videos can be downloaded partially based on either timestamps or chapters using &lt;code&gt;--download-sections&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Split video by chapters&lt;/strong&gt;: Videos can be split into multiple files based on chapters using &lt;code&gt;--split-chapters&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-threaded fragment downloads&lt;/strong&gt;: Download multiple fragments of m3u8/mpd videos in parallel. Use &lt;code&gt;--concurrent-fragments&lt;/code&gt; (&lt;code&gt;-N&lt;/code&gt;) option to set the number of threads used&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Aria2c with HLS/DASH&lt;/strong&gt;: You can use &lt;code&gt;aria2c&lt;/code&gt; as the external downloader for DASH(mpd) and HLS(m3u8) formats&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;New and fixed extractors&lt;/strong&gt;: Many new extractors have been added and a lot of existing ones have been fixed. See the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/Changelog.md"&gt;changelog&lt;/a&gt; or the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/supportedsites.md"&gt;list of supported sites&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MSOs&lt;/strong&gt;: Philo, Spectrum, SlingTV, Cablevision, RCN etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Subtitle extraction from manifests&lt;/strong&gt;: Subtitles can be extracted from streaming media manifests. See &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/be6202f12b97858b9d716e608394b51065d0419f"&gt;commit/be6202f&lt;/a&gt; for details&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple paths and output templates&lt;/strong&gt;: You can give different &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output templates&lt;/a&gt; and download paths for different types of files. You can also set a temporary path where intermediary files are downloaded to using &lt;code&gt;--paths&lt;/code&gt; (&lt;code&gt;-P&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Portable Configuration&lt;/strong&gt;: Configuration files are automatically loaded from the home and root directories. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;CONFIGURATION&lt;/a&gt; for details&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Output template improvements&lt;/strong&gt;: Output templates can now have date-time formatting, numeric offsets, object traversal etc. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; for details. Even more advanced operations can also be done with the help of &lt;code&gt;--parse-metadata&lt;/code&gt; and &lt;code&gt;--replace-in-metadata&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other new options&lt;/strong&gt;: Many new options have been added such as &lt;code&gt;--alias&lt;/code&gt;, &lt;code&gt;--print&lt;/code&gt;, &lt;code&gt;--concat-playlist&lt;/code&gt;, &lt;code&gt;--wait-for-video&lt;/code&gt;, &lt;code&gt;--retry-sleep&lt;/code&gt;, &lt;code&gt;--sleep-requests&lt;/code&gt;, &lt;code&gt;--convert-thumbnails&lt;/code&gt;, &lt;code&gt;--force-download-archive&lt;/code&gt;, &lt;code&gt;--force-overwrites&lt;/code&gt;, &lt;code&gt;--break-match-filters&lt;/code&gt; etc&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Improvements&lt;/strong&gt;: Regex and other operators in &lt;code&gt;--format&lt;/code&gt;/&lt;code&gt;--match-filters&lt;/code&gt;, multiple &lt;code&gt;--postprocessor-args&lt;/code&gt; and &lt;code&gt;--downloader-args&lt;/code&gt;, faster archive checking, more &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection"&gt;format selection options&lt;/a&gt;, merge multi-video/audio, multiple &lt;code&gt;--config-locations&lt;/code&gt;, &lt;code&gt;--exec&lt;/code&gt; at different stages, etc&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Plugins&lt;/strong&gt;: Extractors and PostProcessors can be loaded from an external file. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#plugins"&gt;plugins&lt;/a&gt; for details&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Self updater&lt;/strong&gt;: The releases can be updated using &lt;code&gt;yt-dlp -U&lt;/code&gt;, and downgraded using &lt;code&gt;--update-to&lt;/code&gt; if required&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automated builds&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#update-channels"&gt;Nightly/master builds&lt;/a&gt; can be used with &lt;code&gt;--update-to nightly&lt;/code&gt; and &lt;code&gt;--update-to master&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/Changelog.md"&gt;changelog&lt;/a&gt; or &lt;a href="https://github.com/yt-dlp/yt-dlp/commits"&gt;commits&lt;/a&gt; for the full list of changes&lt;/p&gt; 
&lt;p&gt;Features marked with a &lt;strong&gt;*&lt;/strong&gt; have been back-ported to youtube-dl&lt;/p&gt; 
&lt;h3&gt;Differences in default behavior&lt;/h3&gt; 
&lt;p&gt;Some of yt-dlp's default options are different from that of youtube-dl and youtube-dlc:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;yt-dlp supports only &lt;a href="##" title="Windows 8"&gt;Python 3.10+&lt;/a&gt;, and will remove support for more versions as they &lt;a href="https://devguide.python.org/versions/#python-release-cycle"&gt;become EOL&lt;/a&gt;; while &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/30568#issue-1118238743"&gt;youtube-dl still supports Python 2.6+ and 3.2+&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The options &lt;code&gt;--auto-number&lt;/code&gt; (&lt;code&gt;-A&lt;/code&gt;), &lt;code&gt;--title&lt;/code&gt; (&lt;code&gt;-t&lt;/code&gt;) and &lt;code&gt;--literal&lt;/code&gt; (&lt;code&gt;-l&lt;/code&gt;), no longer work. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#Removed"&gt;removed options&lt;/a&gt; for details&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;avconv&lt;/code&gt; is not supported as an alternative to &lt;code&gt;ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;yt-dlp stores config files in slightly different locations to youtube-dl. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;CONFIGURATION&lt;/a&gt; for a list of correct locations&lt;/li&gt; 
 &lt;li&gt;The default &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; is &lt;code&gt;%(title)s [%(id)s].%(ext)s&lt;/code&gt;. There is no real reason for this change. This was changed before yt-dlp was ever made public and now there are no plans to change it back to &lt;code&gt;%(title)s-%(id)s.%(ext)s&lt;/code&gt;. Instead, you may use &lt;code&gt;--compat-options filename&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;The default &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats"&gt;format sorting&lt;/a&gt; is different from youtube-dl and prefers higher resolution and better codecs rather than higher bitrates. You can use the &lt;code&gt;--format-sort&lt;/code&gt; option to change this to any order you prefer, or use &lt;code&gt;--compat-options format-sort&lt;/code&gt; to use youtube-dl's sorting order. Older versions of yt-dlp preferred VP9 due to its broader compatibility; you can use &lt;code&gt;--compat-options prefer-vp9-sort&lt;/code&gt; to revert to that format sorting preference. These two compat options cannot be used together&lt;/li&gt; 
 &lt;li&gt;The default format selector is &lt;code&gt;bv*+ba/b&lt;/code&gt;. This means that if a combined video + audio format that is better than the best video-only format is found, the former will be preferred. Use &lt;code&gt;-f bv+ba/b&lt;/code&gt; or &lt;code&gt;--compat-options format-spec&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;Unlike youtube-dlc, yt-dlp does not allow merging multiple audio/video streams into one file by default (since this conflicts with the use of &lt;code&gt;-f bv*+ba&lt;/code&gt;). If needed, this feature must be enabled using &lt;code&gt;--audio-multistreams&lt;/code&gt; and &lt;code&gt;--video-multistreams&lt;/code&gt;. You can also use &lt;code&gt;--compat-options multistreams&lt;/code&gt; to enable both&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-abort-on-error&lt;/code&gt; is enabled by default. Use &lt;code&gt;--abort-on-error&lt;/code&gt; or &lt;code&gt;--compat-options abort-on-error&lt;/code&gt; to abort on errors instead&lt;/li&gt; 
 &lt;li&gt;When writing metadata files such as thumbnails, description or infojson, the same information (if available) is also written for playlists. Use &lt;code&gt;--no-write-playlist-metafiles&lt;/code&gt; or &lt;code&gt;--compat-options no-playlist-metafiles&lt;/code&gt; to not write these files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--add-metadata&lt;/code&gt; attaches the &lt;code&gt;infojson&lt;/code&gt; to &lt;code&gt;mkv&lt;/code&gt; files in addition to writing the metadata when used with &lt;code&gt;--write-info-json&lt;/code&gt;. Use &lt;code&gt;--no-embed-info-json&lt;/code&gt; or &lt;code&gt;--compat-options no-attach-info-json&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;Some metadata are embedded into different fields when using &lt;code&gt;--add-metadata&lt;/code&gt; as compared to youtube-dl. Most notably, &lt;code&gt;comment&lt;/code&gt; field contains the &lt;code&gt;webpage_url&lt;/code&gt; and &lt;code&gt;synopsis&lt;/code&gt; contains the &lt;code&gt;description&lt;/code&gt;. You can &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#modifying-metadata"&gt;use &lt;code&gt;--parse-metadata&lt;/code&gt;&lt;/a&gt; to modify this to your liking or use &lt;code&gt;--compat-options embed-metadata&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_index&lt;/code&gt; behaves differently when used with options like &lt;code&gt;--playlist-reverse&lt;/code&gt; and &lt;code&gt;--playlist-items&lt;/code&gt;. See &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/302"&gt;#302&lt;/a&gt; for details. You can use &lt;code&gt;--compat-options playlist-index&lt;/code&gt; if you want to keep the earlier behavior&lt;/li&gt; 
 &lt;li&gt;The output of &lt;code&gt;-F&lt;/code&gt; is listed in a new format. Use &lt;code&gt;--compat-options list-formats&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;Live chats (if available) are considered as subtitles. Use &lt;code&gt;--sub-langs all,-live_chat&lt;/code&gt; to download all subtitles except live chat. You can also use &lt;code&gt;--compat-options no-live-chat&lt;/code&gt; to prevent any live chat/danmaku from downloading&lt;/li&gt; 
 &lt;li&gt;YouTube channel URLs download all uploads of the channel. To download only the videos in a specific tab, pass the tab's URL. If the channel does not show the requested tab, an error will be raised. Also, &lt;code&gt;/live&lt;/code&gt; URLs raise an error if there are no live videos instead of silently downloading the entire channel. You may use &lt;code&gt;--compat-options no-youtube-channel-redirect&lt;/code&gt; to revert all these redirections&lt;/li&gt; 
 &lt;li&gt;Unavailable videos are also listed for YouTube playlists. Use &lt;code&gt;--compat-options no-youtube-unavailable-videos&lt;/code&gt; to remove this&lt;/li&gt; 
 &lt;li&gt;The upload dates extracted from YouTube are in UTC.&lt;/li&gt; 
 &lt;li&gt;If &lt;code&gt;ffmpeg&lt;/code&gt; is used as the downloader, the downloading and merging of formats happen in a single step when possible. Use &lt;code&gt;--compat-options no-direct-merge&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;Thumbnail embedding in &lt;code&gt;mp4&lt;/code&gt; is done with mutagen if possible. Use &lt;code&gt;--compat-options embed-thumbnail-atomicparsley&lt;/code&gt; to force the use of AtomicParsley instead&lt;/li&gt; 
 &lt;li&gt;Some internal metadata such as filenames are removed by default from the infojson. Use &lt;code&gt;--no-clean-infojson&lt;/code&gt; or &lt;code&gt;--compat-options no-clean-infojson&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;When &lt;code&gt;--embed-subs&lt;/code&gt; and &lt;code&gt;--write-subs&lt;/code&gt; are used together, the subtitles are written to disk and also embedded in the media file. You can use just &lt;code&gt;--embed-subs&lt;/code&gt; to embed the subs and automatically delete the separate file. See &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/630#issuecomment-893659460"&gt;#630 (comment)&lt;/a&gt; for more info. &lt;code&gt;--compat-options no-keep-subs&lt;/code&gt; can be used to revert this&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;certifi&lt;/code&gt; will be used for SSL root certificates, if installed. If you want to use system certificates (e.g. self-signed), use &lt;code&gt;--compat-options no-certifi&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;yt-dlp's sanitization of invalid characters in filenames is different/smarter than in youtube-dl. You can use &lt;code&gt;--compat-options filename-sanitization&lt;/code&gt; to revert to youtube-dl's behavior&lt;/li&gt; 
 &lt;li&gt;&lt;del&gt;yt-dlp tries to parse the external downloader outputs into the standard progress output if possible (Currently implemented: &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/5931"&gt;aria2c&lt;/a&gt;). You can use &lt;code&gt;--compat-options no-external-downloader-progress&lt;/code&gt; to get the downloader output as-is&lt;/del&gt;&lt;/li&gt; 
 &lt;li&gt;yt-dlp versions between 2021.09.01 and 2023.01.02 applies &lt;code&gt;--match-filters&lt;/code&gt; to nested playlists. This was an unintentional side-effect of &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/8f18aca8717bb0dd49054555af8d386e5eda3a88"&gt;8f18ac&lt;/a&gt; and is fixed in &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/d7b460d0e5fc710950582baed2e3fc616ed98a80"&gt;d7b460&lt;/a&gt;. Use &lt;code&gt;--compat-options playlist-match-filter&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;yt-dlp versions between 2021.11.10 and 2023.06.21 estimated &lt;code&gt;filesize_approx&lt;/code&gt; values for fragmented/manifest formats. This was added for convenience in &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/f2fe69c7b0d208bdb1f6292b4ae92bc1e1a7444a"&gt;f2fe69&lt;/a&gt;, but was reverted in &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/0dff8e4d1e6e9fb938f4256ea9af7d81f42fd54f"&gt;0dff8e&lt;/a&gt; due to the potentially extreme inaccuracy of the estimated values. Use &lt;code&gt;--compat-options manifest-filesize-approx&lt;/code&gt; to keep extracting the estimated values&lt;/li&gt; 
 &lt;li&gt;yt-dlp uses modern http client backends such as &lt;code&gt;requests&lt;/code&gt;. Use &lt;code&gt;--compat-options prefer-legacy-http-handler&lt;/code&gt; to prefer the legacy http handler (&lt;code&gt;urllib&lt;/code&gt;) to be used for standard http requests.&lt;/li&gt; 
 &lt;li&gt;The sub-modules &lt;code&gt;swfinterp&lt;/code&gt;, &lt;code&gt;casefold&lt;/code&gt; are removed.&lt;/li&gt; 
 &lt;li&gt;Passing &lt;code&gt;--simulate&lt;/code&gt; (or calling &lt;code&gt;extract_info&lt;/code&gt; with &lt;code&gt;download=False&lt;/code&gt;) no longer alters the default format selection. See &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/9843"&gt;#9843&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;yt-dlp no longer applies the server modified time to downloaded files by default. Use &lt;code&gt;--mtime&lt;/code&gt; or &lt;code&gt;--compat-options mtime-by-default&lt;/code&gt; to revert this.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For ease of use, a few more compat options are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options all&lt;/code&gt;: Use all compat options (&lt;strong&gt;Do NOT use this!&lt;/strong&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options youtube-dl&lt;/code&gt;: Same as &lt;code&gt;--compat-options all,-multistreams,-playlist-match-filter,-manifest-filesize-approx,-allow-unsafe-ext,-prefer-vp9-sort&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options youtube-dlc&lt;/code&gt;: Same as &lt;code&gt;--compat-options all,-no-live-chat,-no-youtube-channel-redirect,-playlist-match-filter,-manifest-filesize-approx,-allow-unsafe-ext,-prefer-vp9-sort&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options 2021&lt;/code&gt;: Same as &lt;code&gt;--compat-options 2022,no-certifi,filename-sanitization&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options 2022&lt;/code&gt;: Same as &lt;code&gt;--compat-options 2023,playlist-match-filter,no-external-downloader-progress,prefer-legacy-http-handler,manifest-filesize-approx&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options 2023&lt;/code&gt;: Same as &lt;code&gt;--compat-options 2024,prefer-vp9-sort&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options 2024&lt;/code&gt;: Same as &lt;code&gt;--compat-options mtime-by-default&lt;/code&gt;. Use this to enable all future compat options&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following compat options restore vulnerable behavior from before security patches:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--compat-options allow-unsafe-ext&lt;/code&gt;: Allow files with any extension (including unsafe ones) to be downloaded (&lt;a href="https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-79w7-vh3h-8g4j"&gt;GHSA-79w7-vh3h-8g4j&lt;/a&gt;)&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Only use if a valid file download is rejected because its extension is detected as uncommon&lt;/p&gt; 
   &lt;p&gt;&lt;strong&gt;This option can enable remote code execution! Consider &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/new/choose"&gt;opening an issue&lt;/a&gt; instead!&lt;/strong&gt;&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deprecated options&lt;/h3&gt; 
&lt;p&gt;These are all the deprecated options and the current alternative to achieve the same effect&lt;/p&gt; 
&lt;h4&gt;Almost redundant options&lt;/h4&gt; 
&lt;p&gt;While these options are almost the same as their new counterparts, there are some differences that prevents them being redundant&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-j, --dump-json                  --print "%()j"
-F, --list-formats               --print formats_table
--list-thumbnails                --print thumbnails_table --print playlist:thumbnails_table
--list-subs                      --print automatic_captions_table --print subtitles_table
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Redundant options&lt;/h4&gt; 
&lt;p&gt;While these options are redundant, they are still expected to be used due to their ease of use&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--get-description                --print description
--get-duration                   --print duration_string
--get-filename                   --print filename
--get-format                     --print format
--get-id                         --print id
--get-thumbnail                  --print thumbnail
-e, --get-title                  --print title
-g, --get-url                    --print urls
--match-title REGEX              --match-filters "title ~= (?i)REGEX"
--reject-title REGEX             --match-filters "title !~= (?i)REGEX"
--min-views COUNT                --match-filters "view_count &amp;gt;=? COUNT"
--max-views COUNT                --match-filters "view_count &amp;lt;=? COUNT"
--break-on-reject                Use --break-match-filters
--user-agent UA                  --add-headers "User-Agent:UA"
--referer URL                    --add-headers "Referer:URL"
--playlist-start NUMBER          -I NUMBER:
--playlist-end NUMBER            -I :NUMBER
--playlist-reverse               -I ::-1
--no-playlist-reverse            Default
--no-colors                      --color no_color
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Not recommended&lt;/h4&gt; 
&lt;p&gt;While these options still work, their use is not recommended since there are other alternatives to achieve the same&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--force-generic-extractor        --ies generic,default
--exec-before-download CMD       --exec "before_dl:CMD"
--no-exec-before-download        --no-exec
--all-formats                    -f all
--all-subs                       --sub-langs all --write-subs
--print-json                     -j --no-simulate
--autonumber-size NUMBER         Use string formatting, e.g. %(autonumber)03d
--autonumber-start NUMBER        Use internal field formatting like %(autonumber+NUMBER)s
--id                             -o "%(id)s.%(ext)s"
--metadata-from-title FORMAT     --parse-metadata "%(title)s:FORMAT"
--hls-prefer-native              --downloader "m3u8:native"
--hls-prefer-ffmpeg              --downloader "m3u8:ffmpeg"
--list-formats-old               --compat-options list-formats (Alias: --no-list-formats-as-table)
--list-formats-as-table          --compat-options -list-formats [Default]
--geo-bypass                     --xff "default"
--no-geo-bypass                  --xff "never"
--geo-bypass-country CODE        --xff CODE
--geo-bypass-ip-block IP_BLOCK   --xff IP_BLOCK
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Developer options&lt;/h4&gt; 
&lt;p&gt;These options are not intended to be used by the end-user&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--test                           Download only part of video for testing extractors
--load-pages                     Load pages dumped by --write-pages
--allow-unplayable-formats       List unplayable formats also
--no-allow-unplayable-formats    Default
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Old aliases&lt;/h4&gt; 
&lt;p&gt;These are aliases that are no longer documented for various reasons&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--clean-infojson                 --clean-info-json
--force-write-download-archive   --force-write-archive
--no-clean-infojson              --no-clean-info-json
--no-split-tracks                --no-split-chapters
--no-write-srt                   --no-write-subs
--prefer-unsecure                --prefer-insecure
--rate-limit RATE                --limit-rate RATE
--split-tracks                   --split-chapters
--srt-lang LANGS                 --sub-langs LANGS
--trim-file-names LENGTH         --trim-filenames LENGTH
--write-srt                      --write-subs
--yes-overwrites                 --force-overwrites
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Sponskrub Options&lt;/h4&gt; 
&lt;p&gt;Support for &lt;a href="https://github.com/faissaloo/SponSkrub"&gt;SponSkrub&lt;/a&gt; has been removed in favor of the &lt;code&gt;--sponsorblock&lt;/code&gt; options&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--sponskrub                      --sponsorblock-mark all
--no-sponskrub                   --no-sponsorblock
--sponskrub-cut                  --sponsorblock-remove all
--no-sponskrub-cut               --sponsorblock-remove -all
--sponskrub-force                Not applicable
--no-sponskrub-force             Not applicable
--sponskrub-location             Not applicable
--sponskrub-args                 Not applicable
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;No longer supported&lt;/h4&gt; 
&lt;p&gt;These options may no longer work as intended&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--prefer-avconv                  avconv is not officially supported by yt-dlp (Alias: --no-prefer-ffmpeg)
--prefer-ffmpeg                  Default (Alias: --no-prefer-avconv)
-C, --call-home                  Not implemented
--no-call-home                   Default
--include-ads                    No longer supported
--no-include-ads                 Default
--write-annotations              No supported site has annotations now
--no-write-annotations           Default
--avconv-location                Removed alias for --ffmpeg-location
--cn-verification-proxy URL      Removed alias for --geo-verification-proxy URL
--dump-headers                   Removed alias for --print-traffic
--dump-intermediate-pages        Removed alias for --dump-pages
--youtube-skip-dash-manifest     Removed alias for --extractor-args "youtube:skip=dash" (Alias: --no-youtube-include-dash-manifest)
--youtube-skip-hls-manifest      Removed alias for --extractor-args "youtube:skip=hls" (Alias: --no-youtube-include-hls-manifest)
--youtube-include-dash-manifest  Default (Alias: --no-youtube-skip-dash-manifest)
--youtube-include-hls-manifest   Default (Alias: --no-youtube-skip-hls-manifest)
--youtube-print-sig-code         Removed testing functionality
--dump-user-agent                No longer supported
--xattr-set-filesize             No longer supported
--compat-options seperate-video-versions  No longer needed
--compat-options no-youtube-prefer-utc-upload-date  No longer supported
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Removed&lt;/h4&gt; 
&lt;p&gt;These options were deprecated since 2014 and have now been entirely removed&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-A, --auto-number                -o "%(autonumber)s-%(id)s.%(ext)s"
-t, -l, --title, --literal       -o "%(title)s-%(id)s.%(ext)s"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;CONTRIBUTING&lt;/h1&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#contributing-to-yt-dlp"&gt;CONTRIBUTING.md&lt;/a&gt; for instructions on &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#opening-an-issue"&gt;Opening an Issue&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#developer-instructions"&gt;Contributing code to the project&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;WIKI&lt;/h1&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki"&gt;Wiki&lt;/a&gt; for more information&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>