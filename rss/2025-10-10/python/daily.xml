<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 09 Oct 2025 01:35:46 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>BeehiveInnovations/zen-mcp-server</title>
      <link>https://github.com/BeehiveInnovations/zen-mcp-server</link>
      <description>&lt;p&gt;The power of Claude Code / GeminiCLI / CodexCLI + [Gemini / OpenAI / OpenRouter / Azure / Grok / Ollama / Custom Model / All Of The Above] working as one.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zen MCP: Many Workflows. One Context.&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0d26061e-5f21-4ab1-b7d0-f883ddc2c3da"&gt;Zen in action&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/#-watch-tools-in-action"&gt;Watch more examples&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;h3&gt;Your CLI + Multiple Models = Your AI Dev Team&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;Use the ü§ñ CLI you love:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt; ¬∑ &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt; ¬∑ &lt;a href="https://github.com/openai/codex"&gt;Codex CLI&lt;/a&gt; ¬∑ &lt;a href="https://qwenlm.github.io/qwen-code-docs/"&gt;Qwen Code CLI&lt;/a&gt; ¬∑ &lt;a href="https://cursor.com"&gt;Cursor&lt;/a&gt; ¬∑ &lt;em&gt;and more&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;With multiple models within a single prompt:&lt;/strong&gt;&lt;br /&gt; Gemini ¬∑ OpenAI ¬∑ Anthropic ¬∑ Grok ¬∑ Azure ¬∑ Ollama ¬∑ OpenRouter ¬∑ DIAL ¬∑ On-Device Model&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üÜï Now with CLI-to-CLI Bridge&lt;/h2&gt; 
&lt;p&gt;The new &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/clink.md"&gt;&lt;code&gt;clink&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; (CLI + Link) tool connects external AI CLIs directly into your workflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connect external CLIs&lt;/strong&gt; like &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;, &lt;a href="https://github.com/openai/codex"&gt;Codex CLI&lt;/a&gt;, and &lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt; directly into your workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CLI Subagents&lt;/strong&gt; - Launch isolated CLI instances from &lt;em&gt;within&lt;/em&gt; your current CLI! Claude Code can spawn Codex subagents, Codex can spawn Gemini CLI subagents, etc. Offload heavy tasks (code reviews, bug hunting) to fresh contexts while your main session's context window remains unpolluted. Each subagent returns only final results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context Isolation&lt;/strong&gt; - Run separate investigations without polluting your primary workspace&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Role Specialization&lt;/strong&gt; - Spawn &lt;code&gt;planner&lt;/code&gt;, &lt;code&gt;codereviewer&lt;/code&gt;, or custom role agents with specialized system prompts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full CLI Capabilities&lt;/strong&gt; - Web search, file inspection, MCP tool access, latest documentation lookups&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Continuity&lt;/strong&gt; - Sub-CLIs participate as first-class members with full conversation context between tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Codex spawns Codex subagent for isolated code review in fresh context
clink with codex codereviewer to audit auth module for security issues
# Subagent reviews in isolation, returns final report without cluttering your context as codex reads each file and walks the directory structure

# Consensus from different AI models ‚Üí Implementation handoff with full context preservation between tools
Use consensus with gpt-5 and gemini-pro to decide: dark mode or offline support next
Continue with clink gemini - implement the recommended feature
# Gemini receives full debate context and starts coding immediately
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/clink.md"&gt;Learn more about clink&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Why Zen MCP?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Why rely on one AI model when you can orchestrate them all?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A Model Context Protocol server that supercharges tools like &lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt;, &lt;a href="https://developers.openai.com/codex/cli"&gt;Codex CLI&lt;/a&gt;, and IDE clients such as &lt;a href="https://cursor.com"&gt;Cursor&lt;/a&gt; or the &lt;a href="https://marketplace.visualstudio.com/items?itemName=Anthropic.claude-vscode"&gt;Claude Dev VS Code extension&lt;/a&gt;. &lt;strong&gt;Zen MCP connects your favorite AI tool to multiple AI models&lt;/strong&gt; for enhanced code analysis, problem-solving, and collaborative development.&lt;/p&gt; 
&lt;h3&gt;True AI Collaboration with Conversation Continuity&lt;/h3&gt; 
&lt;p&gt;Zen supports &lt;strong&gt;conversation threading&lt;/strong&gt; so your CLI can &lt;strong&gt;discuss ideas with multiple AI models, exchange reasoning, get second opinions, and even run collaborative debates between models&lt;/strong&gt; to help you reach deeper insights and better solutions.&lt;/p&gt; 
&lt;p&gt;Your CLI always stays in control but gets perspectives from the best AI for each subtask. Context carries forward seamlessly across tools and models, enabling complex workflows like: code reviews with multiple models ‚Üí automated planning ‚Üí implementation ‚Üí pre-commit validation.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;You're in control.&lt;/strong&gt; Your CLI of choice orchestrates the AI team, but you decide the workflow. Craft powerful prompts that bring in Gemini Pro, GPT 5, Flash, or local offline models exactly when needed.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Reasons to Use Zen MCP&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;A typical workflow with Claude Code as an example:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Model Orchestration&lt;/strong&gt; - Claude coordinates with Gemini Pro, O3, GPT-5, and 50+ other models to get the best analysis for each task&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Context Revival Magic&lt;/strong&gt; - Even after Claude's context resets, continue conversations seamlessly by having other models "remind" Claude of the discussion&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Guided Workflows&lt;/strong&gt; - Enforces systematic investigation phases that prevent rushed analysis and ensure thorough code examination&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extended Context Windows&lt;/strong&gt; - Break Claude's limits by delegating to Gemini (1M tokens) or O3 (200K tokens) for massive codebases&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;True Conversation Continuity&lt;/strong&gt; - Full context flows across tools and models - Gemini remembers what O3 said 10 steps ago&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-Specific Strengths&lt;/strong&gt; - Extended thinking with Gemini Pro, blazing speed with Flash, strong reasoning with O3, privacy with local Ollama&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Professional Code Reviews&lt;/strong&gt; - Multi-pass analysis with severity levels, actionable feedback, and consensus from multiple AI experts&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Smart Debugging Assistant&lt;/strong&gt; - Systematic root cause analysis with hypothesis tracking and confidence levels&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automatic Model Selection&lt;/strong&gt; - Claude intelligently picks the right model for each subtask (or you can specify)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vision Capabilities&lt;/strong&gt; - Analyze screenshots, diagrams, and visual content with vision-enabled models&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Local Model Support&lt;/strong&gt; - Run Llama, Mistral, or other models locally for complete privacy and zero API costs&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bypass MCP Token Limits&lt;/strong&gt; - Automatically works around MCP's 25K limit for large prompts and responses&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;The Killer Feature:&lt;/strong&gt; When Claude's context resets, just ask to "continue with O3" - the other model's response magically revives Claude's understanding without re-ingesting documents!&lt;/p&gt; 
 &lt;h4&gt;Example: Multi-Model Code Review Workflow&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;code&gt;Perform a codereview using gemini pro and o3 and use planner to generate a detailed plan, implement the fixes and do a final precommit check by continuing from the previous codereview&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;This triggers a &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/codereview.md"&gt;&lt;code&gt;codereview&lt;/code&gt;&lt;/a&gt; workflow where Claude walks the code, looking for all kinds of issues&lt;/li&gt; 
  &lt;li&gt;After multiple passes, collects relevant code and makes note of issues along the way&lt;/li&gt; 
  &lt;li&gt;Maintains a &lt;code&gt;confidence&lt;/code&gt; level between &lt;code&gt;exploring&lt;/code&gt;, &lt;code&gt;low&lt;/code&gt;, &lt;code&gt;medium&lt;/code&gt;, &lt;code&gt;high&lt;/code&gt; and &lt;code&gt;certain&lt;/code&gt; to track how confidently it's been able to find and identify issues&lt;/li&gt; 
  &lt;li&gt;Generates a detailed list of critical -&amp;gt; low issues&lt;/li&gt; 
  &lt;li&gt;Shares the relevant files, findings, etc with &lt;strong&gt;Gemini Pro&lt;/strong&gt; to perform a deep dive for a second &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/codereview.md"&gt;&lt;code&gt;codereview&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Comes back with a response and next does the same with o3, adding to the prompt if a new discovery comes to light&lt;/li&gt; 
  &lt;li&gt;When done, Claude takes in all the feedback and combines a single list of all critical -&amp;gt; low issues, including good patterns in your code. The final list includes new findings or revisions in case Claude misunderstood or missed something crucial and one of the other models pointed this out&lt;/li&gt; 
  &lt;li&gt;It then uses the &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/planner.md"&gt;&lt;code&gt;planner&lt;/code&gt;&lt;/a&gt; workflow to break the work down into simpler steps if a major refactor is required&lt;/li&gt; 
  &lt;li&gt;Claude then performs the actual work of fixing highlighted issues&lt;/li&gt; 
  &lt;li&gt;When done, Claude returns to Gemini Pro for a &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/precommit.md"&gt;&lt;code&gt;precommit&lt;/code&gt;&lt;/a&gt; review&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;All within a single conversation thread! Gemini Pro in step 11 &lt;em&gt;knows&lt;/em&gt; what was recommended by O3 in step 7! Taking that context and review into consideration to aid with its final pre-commit review.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Think of it as Claude Code &lt;em&gt;for&lt;/em&gt; Claude Code.&lt;/strong&gt; This MCP isn't magic. It's just &lt;strong&gt;super-glue&lt;/strong&gt;.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Remember:&lt;/strong&gt; Claude stays in full control ‚Äî but &lt;strong&gt;YOU&lt;/strong&gt; call the shots. Zen is designed to have Claude engage other models only when needed ‚Äî and to follow through with meaningful back-and-forth. &lt;strong&gt;You're&lt;/strong&gt; the one who crafts the powerful prompt that makes Claude bring in Gemini, Flash, O3 ‚Äî or fly solo. You're the guide. The prompter. The puppeteer.&lt;/p&gt; 
  &lt;h4&gt;You are the AI - &lt;strong&gt;Actually Intelligent&lt;/strong&gt;.&lt;/h4&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h4&gt;Recommended AI Stack&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;For Claude Code Users&lt;/summary&gt; 
 &lt;p&gt;For best results when using &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Sonnet 4.5&lt;/strong&gt; - All agentic work and orchestration&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt; OR &lt;strong&gt;GPT-5-Pro&lt;/strong&gt; - Deep thinking, additional code reviews, debugging and validations, pre-commit analysis&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;For Codex Users&lt;/summary&gt; 
 &lt;p&gt;For best results when using &lt;a href="https://developers.openai.com/codex/cli"&gt;Codex CLI&lt;/a&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GPT-5 Codex Medium&lt;/strong&gt; - All agentic work and orchestration&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Gemini 2.5 Pro&lt;/strong&gt; OR &lt;strong&gt;GPT-5-Pro&lt;/strong&gt; - Deep thinking, additional code reviews, debugging and validations, pre-commit analysis&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quick Start (5 minutes)&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt; Python 3.10+, Git, &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installed&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. Get API Keys&lt;/strong&gt; (choose one or more):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt;&lt;/strong&gt; - Access multiple models with one API&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://makersuite.google.com/app/apikey"&gt;Gemini&lt;/a&gt;&lt;/strong&gt; - Google's latest models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI&lt;/a&gt;&lt;/strong&gt; - O3, GPT-5 series&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://learn.microsoft.com/azure/ai-services/openai/"&gt;Azure OpenAI&lt;/a&gt;&lt;/strong&gt; - Enterprise deployments of GPT-4o, GPT-4.1, GPT-5 family&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://console.x.ai/"&gt;X.AI&lt;/a&gt;&lt;/strong&gt; - Grok models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://dialx.ai/"&gt;DIAL&lt;/a&gt;&lt;/strong&gt; - Vendor-agnostic model access&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://ollama.ai/"&gt;Ollama&lt;/a&gt;&lt;/strong&gt; - Local models (free)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Install&lt;/strong&gt; (choose one):&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option A: Clone and Automatic Setup&lt;/strong&gt; (recommended)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/BeehiveInnovations/zen-mcp-server.git
cd zen-mcp-server

# Handles everything: setup, config, API keys from system environment. 
# Auto-configures Claude Desktop, Claude Code, Gemini CLI, Codex CLI, Qwen CLI
# Enable / disable additional settings in .env
./run-server.sh  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option B: Instant Setup with &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uvx&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;// Add to ~/.claude/settings.json or .mcp.json
// Don't forget to add your API keys under env
{
  "mcpServers": {
    "zen": {
      "command": "bash",
      "args": ["-c", "for p in $(which uvx 2&amp;gt;/dev/null) $HOME/.local/bin/uvx /opt/homebrew/bin/uvx /usr/local/bin/uvx uvx; do [ -x \"$p\" ] &amp;amp;&amp;amp; exec \"$p\" --from git+https://github.com/BeehiveInnovations/zen-mcp-server.git zen-mcp-server; done; echo 'uvx not found' &amp;gt;&amp;amp;2; exit 1"],
      "env": {
        "PATH": "/usr/local/bin:/usr/bin:/bin:/opt/homebrew/bin:~/.local/bin",
        "GEMINI_API_KEY": "your-key-here",
        "DISABLED_TOOLS": "analyze,refactor,testgen,secaudit,docgen,tracer",
        "DEFAULT_MODEL": "auto"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Start Using!&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Use zen to analyze this code for security issues with gemini pro"
"Debug this error with o3 and then get flash to suggest optimizations"
"Plan the migration strategy with zen, get consensus from multiple models"
"clink with cli_name=\"gemini\" role=\"planner\" to draft a phased rollout plan"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/getting-started.md"&gt;Complete Setup Guide&lt;/a&gt;&lt;/strong&gt; with detailed installation, configuration for Gemini / Codex / Qwen, and troubleshooting üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/getting-started.md#ide-clients"&gt;Cursor &amp;amp; VS Code Setup&lt;/a&gt;&lt;/strong&gt; for IDE integration instructions üì∫ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/#-watch-tools-in-action"&gt;Watch tools in action&lt;/a&gt;&lt;/strong&gt; to see real-world examples&lt;/p&gt; 
&lt;h2&gt;Provider Configuration&lt;/h2&gt; 
&lt;p&gt;Zen activates any provider that has credentials in your &lt;code&gt;.env&lt;/code&gt;. See &lt;code&gt;.env.example&lt;/code&gt; for deeper customization.&lt;/p&gt; 
&lt;h2&gt;Core Tools&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Each tool comes with its own multi-step workflow, parameters, and descriptions that consume valuable context window space even when not in use. To optimize performance, some tools are disabled by default. See &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/#tool-configuration"&gt;Tool Configuration&lt;/a&gt; below to enable them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Collaboration &amp;amp; Planning&lt;/strong&gt; &lt;em&gt;(Enabled by default)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/clink.md"&gt;&lt;code&gt;clink&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Bridge requests to external AI CLIs (Gemini planner, codereviewer, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/chat.md"&gt;&lt;code&gt;chat&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Brainstorm ideas, get second opinions, validate approaches. With capable models (GPT-5 Pro, Gemini 2.5 Pro), generates complete code / implementation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/thinkdeep.md"&gt;&lt;code&gt;thinkdeep&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Extended reasoning, edge case analysis, alternative perspectives&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/planner.md"&gt;&lt;code&gt;planner&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Break down complex projects into structured, actionable plans&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/consensus.md"&gt;&lt;code&gt;consensus&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Get expert opinions from multiple AI models with stance steering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Code Analysis &amp;amp; Quality&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/debug.md"&gt;&lt;code&gt;debug&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Systematic investigation and root cause analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/precommit.md"&gt;&lt;code&gt;precommit&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Validate changes before committing, prevent regressions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/codereview.md"&gt;&lt;code&gt;codereview&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Professional reviews with severity levels and actionable feedback&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/analyze.md"&gt;&lt;code&gt;analyze&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; &lt;em&gt;(disabled by default - &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/#tool-configuration"&gt;enable&lt;/a&gt;)&lt;/em&gt; - Understand architecture, patterns, dependencies across entire codebases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Development Tools&lt;/strong&gt; &lt;em&gt;(Disabled by default - &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/#tool-configuration"&gt;enable&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/refactor.md"&gt;&lt;code&gt;refactor&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Intelligent code refactoring with decomposition focus&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/testgen.md"&gt;&lt;code&gt;testgen&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Comprehensive test generation with edge cases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/secaudit.md"&gt;&lt;code&gt;secaudit&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Security audits with OWASP Top 10 analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/docgen.md"&gt;&lt;code&gt;docgen&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Generate documentation with complexity analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Utilities&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/apilookup.md"&gt;&lt;code&gt;apilookup&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Forces current-year API/SDK documentation lookups in a sub-process (saves tokens within the current context window), prevents outdated training data responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/challenge.md"&gt;&lt;code&gt;challenge&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Prevent "You're absolutely right!" responses with critical analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/tracer.md"&gt;&lt;code&gt;tracer&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; &lt;em&gt;(disabled by default - &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/#tool-configuration"&gt;enable&lt;/a&gt;)&lt;/em&gt; - Static analysis prompts for call-flow mapping&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b id="tool-configuration"&gt;üëâ Tool Configuration&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;Default Configuration&lt;/h3&gt; 
 &lt;p&gt;To optimize context window usage, only essential tools are enabled by default:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Enabled by default:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;chat&lt;/code&gt;, &lt;code&gt;thinkdeep&lt;/code&gt;, &lt;code&gt;planner&lt;/code&gt;, &lt;code&gt;consensus&lt;/code&gt; - Core collaboration tools&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;codereview&lt;/code&gt;, &lt;code&gt;precommit&lt;/code&gt;, &lt;code&gt;debug&lt;/code&gt; - Essential code quality tools&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;apilookup&lt;/code&gt; - Rapid API/SDK information lookup&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;challenge&lt;/code&gt; - Critical thinking utility&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Disabled by default:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;analyze&lt;/code&gt;, &lt;code&gt;refactor&lt;/code&gt;, &lt;code&gt;testgen&lt;/code&gt;, &lt;code&gt;secaudit&lt;/code&gt;, &lt;code&gt;docgen&lt;/code&gt;, &lt;code&gt;tracer&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Enabling Additional Tools&lt;/h3&gt; 
 &lt;p&gt;To enable additional tools, remove them from the &lt;code&gt;DISABLED_TOOLS&lt;/code&gt; list:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Option 1: Edit your .env file&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Default configuration (from .env.example)
DISABLED_TOOLS=analyze,refactor,testgen,secaudit,docgen,tracer

# To enable specific tools, remove them from the list
# Example: Enable analyze tool
DISABLED_TOOLS=refactor,testgen,secaudit,docgen,tracer

# To enable ALL tools
DISABLED_TOOLS=
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Option 2: Configure in MCP settings&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;// In ~/.claude/settings.json or .mcp.json
{
  "mcpServers": {
    "zen": {
      "env": {
        // Tool configuration
        "DISABLED_TOOLS": "refactor,testgen,secaudit,docgen,tracer",
        "DEFAULT_MODEL": "pro",
        "DEFAULT_THINKING_MODE_THINKDEEP": "high",
        
        // API configuration
        "GEMINI_API_KEY": "your-gemini-key",
        "OPENAI_API_KEY": "your-openai-key",
        "OPENROUTER_API_KEY": "your-openrouter-key",
        
        // Logging and performance
        "LOG_LEVEL": "INFO",
        "CONVERSATION_TIMEOUT_HOURS": "6",
        "MAX_CONVERSATION_TURNS": "50"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Option 3: Enable all tools&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;// Remove or empty the DISABLED_TOOLS to enable everything
{
  "mcpServers": {
    "zen": {
      "env": {
        "DISABLED_TOOLS": ""
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Essential tools (&lt;code&gt;version&lt;/code&gt;, &lt;code&gt;listmodels&lt;/code&gt;) cannot be disabled&lt;/li&gt; 
  &lt;li&gt;After changing tool configuration, restart your Claude session for changes to take effect&lt;/li&gt; 
  &lt;li&gt;Each tool adds to context window usage, so only enable what you need&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üì∫ Watch Tools In Action&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Chat Tool&lt;/b&gt; - Collaborative decision making and multi-turn conversations&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Picking Redis vs Memcached:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/41076cfe-dd49-4dfc-82f5-d7461b34705d"&gt;Chat Redis or Memcached_web.webm&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-turn conversation with continuation:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/37bd57ca-e8a6-42f7-b5fb-11de271e95db"&gt;Chat With Gemini_web.webm&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Consensus Tool&lt;/b&gt; - Multi-model debate and decision making&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-model consensus debate:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/76a23dd5-887a-4382-9cf0-642f5cf6219e"&gt;Zen Consensus Debate&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;PreCommit Tool&lt;/b&gt; - Comprehensive change validation&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Pre-commit validation workflow:&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://github.com/user-attachments/assets/584adfa6-d252-49b4-b5b0-0cd6e97fb2c6" width="950" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;API Lookup Tool&lt;/b&gt; - Current vs outdated API documentation&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Without Zen - outdated APIs:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/01a79dc9-ad16-4264-9ce1-76a56c3580ee"&gt;API without Zen&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;With Zen - current APIs:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5c847326-4b66-41f7-8f30-f380453dce22"&gt;API with Zen&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Challenge Tool&lt;/b&gt; - Critical thinking vs reflexive agreement&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Without Zen:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/64f3c9fb-7ca9-4876-b687-25e847edfd87" alt="without_zen@2x" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;With Zen:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/9d72f444-ba53-4ab1-83e5-250062c6ee70" alt="with_zen@2x" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;AI Orchestration&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Auto model selection&lt;/strong&gt; - Claude picks the right AI for each task&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-model workflows&lt;/strong&gt; - Chain different models in single conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversation continuity&lt;/strong&gt; - Context preserved across tools and models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/context-revival.md"&gt;Context revival&lt;/a&gt;&lt;/strong&gt; - Continue conversations even after context resets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Support&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple providers&lt;/strong&gt; - Gemini, OpenAI, Azure, X.AI, OpenRouter, DIAL, Ollama&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Latest models&lt;/strong&gt; - GPT-5, Gemini 2.5 Pro, O3, Grok-4, local Llama&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/advanced-usage.md#thinking-modes"&gt;Thinking modes&lt;/a&gt;&lt;/strong&gt; - Control reasoning depth vs cost&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vision support&lt;/strong&gt; - Analyze images, diagrams, screenshots&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Developer Experience&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Guided workflows&lt;/strong&gt; - Systematic investigation prevents rushed analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart file handling&lt;/strong&gt; - Auto-expand directories, manage token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web search integration&lt;/strong&gt; - Access current documentation and best practices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/advanced-usage.md#working-with-large-prompts"&gt;Large prompt support&lt;/a&gt;&lt;/strong&gt; - Bypass MCP's 25K token limit&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Example Workflows&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Multi-model Code Review:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Perform a codereview using gemini pro and o3, then use planner to create a fix strategy"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Üí Claude reviews code systematically ‚Üí Consults Gemini Pro ‚Üí Gets O3's perspective ‚Üí Creates unified action plan&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Collaborative Debugging:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Debug this race condition with max thinking mode, then validate the fix with precommit"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Üí Deep investigation ‚Üí Expert analysis ‚Üí Solution implementation ‚Üí Pre-commit validation&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Architecture Planning:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Plan our microservices migration, get consensus from pro and o3 on the approach"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Üí Structured planning ‚Üí Multiple expert opinions ‚Üí Consensus building ‚Üí Implementation roadmap&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/advanced-usage.md"&gt;Advanced Usage Guide&lt;/a&gt;&lt;/strong&gt; for complex workflows, model configuration, and power-user features&lt;/p&gt; 
&lt;h2&gt;Quick Links&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;üìñ Documentation&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/index.md"&gt;Docs Overview&lt;/a&gt; - High-level map of major guides&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/getting-started.md"&gt;Getting Started&lt;/a&gt; - Complete setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/tools/"&gt;Tools Reference&lt;/a&gt; - All tools with examples&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/advanced-usage.md"&gt;Advanced Usage&lt;/a&gt; - Power user features&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/configuration.md"&gt;Configuration&lt;/a&gt; - Environment variables, restrictions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/adding_providers.md"&gt;Adding Providers&lt;/a&gt; - Provider-specific setup (OpenAI, Azure, custom gateways)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/model_ranking.md"&gt;Model Ranking Guide&lt;/a&gt; - How intelligence scores drive auto-mode suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;üîß Setup &amp;amp; Support&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/wsl-setup.md"&gt;WSL Setup&lt;/a&gt; - Windows users&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt; - Common issues&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/docs/contributions.md"&gt;Contributing&lt;/a&gt; - Code standards, PR process&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 License - see &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/zen-mcp-server/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Built with the power of &lt;strong&gt;Multi-Model AI&lt;/strong&gt; collaboration ü§ù&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;ctual &lt;strong&gt;I&lt;/strong&gt;ntelligence by real Humans&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://modelcontextprotocol.com"&gt;MCP (Model Context Protocol)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developers.openai.com/codex/cli"&gt;Codex CLI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ai.google.dev/"&gt;Gemini&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/azure/ai-services/openai/"&gt;Azure OpenAI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Star History&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#BeehiveInnovations/zen-mcp-server&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=BeehiveInnovations/zen-mcp-server&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trycua/cua</title>
      <link>https://github.com/trycua/cua</link>
      <description>&lt;p&gt;Open-source infrastructure for Computer-Use Agents. Sandboxes, SDKs, and benchmarks to train and evaluate AI agents that can control full desktops (macOS, Linux, Windows).&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" alt="Cua logo" height="150" srcset="img/logo_white.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" alt="Cua logo" height="150" srcset="img/logo_black.png" /&gt; 
  &lt;img alt="Cua logo" height="150" src="https://raw.githubusercontent.com/trycua/cua/main/img/logo_black.png" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#"&gt;&lt;img src="https://img.shields.io/badge/Python-333333?logo=python&amp;amp;logoColor=white&amp;amp;labelColor=333333" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#"&gt;&lt;img src="https://img.shields.io/badge/Swift-F05138?logo=swift&amp;amp;logoColor=white" alt="Swift" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#"&gt;&lt;img src="https://img.shields.io/badge/macOS-000000?logo=apple&amp;amp;logoColor=F0F0F0" alt="macOS" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/mVnXXpdE85"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://trendshift.io/repositories/13685" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13685" alt="trycua%2Fcua | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We‚Äôre hosting the &lt;strong&gt;Computer-Use Agents SOTA Challenge&lt;/strong&gt; at &lt;a href="https://hackthenorth.com"&gt;Hack the North&lt;/a&gt; and online!&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Track A (On-site @ UWaterloo)&lt;/strong&gt;: Reserved for participants accepted to Hack the North. üèÜ Prize: &lt;strong&gt;YC interview guaranteed&lt;/strong&gt;.&lt;br /&gt; &lt;strong&gt;Track B (Remote)&lt;/strong&gt;: Open to everyone worldwide. üèÜ Prize: &lt;strong&gt;Cash award&lt;/strong&gt;.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;üëâ Sign up here: &lt;a href="https://www.trycua.com/hackathon"&gt;trycua.com/hackathon&lt;/a&gt;&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;cua&lt;/strong&gt; ("koo-ah") is Docker for &lt;a href="https://www.oneusefulthing.org/p/when-you-give-a-claude-a-mouse"&gt;Computer-Use Agents&lt;/a&gt; - it enables AI agents to control full operating systems in virtual containers and deploy them locally or to the cloud.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;video src="https://github.com/user-attachments/assets/c619b4ea-bb8e-4382-860e-f3757e36af20" width="600" controls&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;p&gt;With the Computer SDK, you can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;automate Windows, Linux, and macOS VMs with a consistent, &lt;a href="https://docs.trycua.com/docs/libraries/computer#interface-actions"&gt;pyautogui-like API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;create &amp;amp; manage VMs &lt;a href="https://docs.trycua.com/docs/computer-sdk/computers#cua-local-containers"&gt;locally&lt;/a&gt; or using &lt;a href="https://www.trycua.com/"&gt;cua cloud&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With the Agent SDK, you can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;run computer-use models with a &lt;a href="https://docs.trycua.com/docs/agent-sdk/message-format"&gt;consistent schema&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;benchmark on OSWorld-Verified, SheetBench-V2, and more &lt;a href="https://docs.trycua.com/docs/agent-sdk/integrations/hud"&gt;with a single line of code using HUD&lt;/a&gt; (&lt;a href="https://github.com/trycua/cua/raw/main/notebooks/eval_osworld.ipynb"&gt;Notebook&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;combine UI grounding models with any LLM using &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents"&gt;composed agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;use new UI agent models and UI grounding models from the Model Zoo below with just a model string (e.g., &lt;code&gt;ComputerAgent(model="openai/computer-use-preview")&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;use API or local inference by changing a prefix (e.g., &lt;code&gt;openai/&lt;/code&gt;, &lt;code&gt;openrouter/&lt;/code&gt;, &lt;code&gt;ollama/&lt;/code&gt;, &lt;code&gt;huggingface-local/&lt;/code&gt;, &lt;code&gt;mlx/&lt;/code&gt;, &lt;a href="https://docs.litellm.ai/docs/providers"&gt;etc.&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;CUA Model Zoo üê®&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents"&gt;All-in-one CUAs&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents"&gt;UI Grounding Models&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents"&gt;UI Planning Models&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;anthropic/claude-sonnet-4-5-20250929&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/xlangai/OpenCUA-{7B,32B}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;any all-in-one CUA&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;openai/computer-use-preview&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/HelloKKMe/GTA1-{7B,32B,72B}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;any VLM (using liteLLM, requires &lt;code&gt;tools&lt;/code&gt; parameter)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;openrouter/z-ai/glm-4.5v&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/Hcompany/Holo1.5-{3B,7B,72B}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;any LLM (using liteLLM, requires &lt;code&gt;moondream3+&lt;/code&gt; prefix )&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/OpenGVLab/InternVL3_5-{1B,2B,4B,8B,...}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;any all-in-one CUA&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/ByteDance-Seed/UI-TARS-1.5-7B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;moondream3+{ui planning}&lt;/code&gt; (supports text-only models)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;omniparser+{ui planning}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;{ui grounding}+{ui planning}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;human/human&lt;/code&gt; ‚Üí &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/human-in-the-loop"&gt;Human-in-the-Loop&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Missing a model? &lt;a href="https://github.com/trycua/cua/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;title=%5BAgent%5D%3A+Add+model+support+for+"&gt;Raise a feature request&lt;/a&gt; or &lt;a href="https://github.com/trycua/cua/raw/main/CONTRIBUTING.md"&gt;contribute&lt;/a&gt;!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.trycua.com/docs/quickstart-ui"&gt;Get started with a Computer-Use Agent UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.trycua.com/docs/quickstart-cli"&gt;Get started with the Computer-Use Agent CLI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.trycua.com/docs/quickstart-devs"&gt;Get started with the Python SDKs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h1&gt;Usage (&lt;a href="https://docs.trycua.com/docs"&gt;Docs&lt;/a&gt;)&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cua-agent[all]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agent import ComputerAgent

agent = ComputerAgent(
    model="anthropic/claude-3-5-sonnet-20241022",
    tools=[computer],
    max_trajectory_budget=5.0
)

messages = [{"role": "user", "content": "Take a screenshot and tell me what you see"}]

async for result in agent.run(messages):
    for item in result["output"]:
        if item["type"] == "message":
            print(item["content"][0]["text"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Output format (OpenAI Agent Responses Format):&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{ 
  "output": [
    # user input
    {
        "role": "user",
        "content": "go to trycua on gh"
    },
    # first agent turn adds the model output to the history
    {
        "summary": [
            {
                "text": "Searching Firefox for Trycua GitHub",
                "type": "summary_text"
            }
        ],
        "type": "reasoning"
    },
    {
        "action": {
            "text": "Trycua GitHub",
            "type": "type"
        },
        "call_id": "call_QI6OsYkXxl6Ww1KvyJc4LKKq",
        "status": "completed",
        "type": "computer_call"
    },
    # second agent turn adds the computer output to the history
    {
        "type": "computer_call_output",
        "call_id": "call_QI6OsYkXxl6Ww1KvyJc4LKKq",
        "output": {
            "type": "input_image",
            "image_url": "data:image/png;base64,..."
        }
    },
    # final agent turn adds the agent output text to the history
    {
        "type": "message",
        "role": "assistant",
        "content": [
          {
            "text": "Success! The Trycua GitHub page has been opened.",
            "type": "output_text"
          }
        ]
    }
  ], 
  "usage": {
      "prompt_tokens": 150,
      "completion_tokens": 75,
      "total_tokens": 225,
      "response_cost": 0.01,
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Computer (&lt;a href="https://docs.trycua.com/docs/computer-sdk/computers"&gt;Docs&lt;/a&gt;)&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cua-computer[all]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from computer import Computer

async with Computer(
    os_type="linux",
    provider_type="cloud",
    name="your-container-name",
    api_key="your-api-key"
) as computer:
    # Take screenshot
    screenshot = await computer.interface.screenshot()

    # Click and type
    await computer.interface.left_click(100, 100)
    await computer.interface.type("Hello!")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/mcp-server/README.md"&gt;How to use the MCP Server with Claude Desktop or other MCP clients&lt;/a&gt; - One of the easiest ways to get started with Cua&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/agent/README.md"&gt;How to use OpenAI Computer-Use, Anthropic, OmniParser, or UI-TARS for your Computer-Use Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lume/README.md"&gt;How to use Lume CLI for managing desktops&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.trycua.com/blog/training-computer-use-models-trajectories-1"&gt;Training Computer-Use Models: Collecting Human Trajectories with Cua (Part 1)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Modules&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Installation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lume/README.md"&gt;&lt;strong&gt;Lume&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;VM management for macOS/Linux using Apple's Virtualization.Framework&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lumier/README.md"&gt;&lt;strong&gt;Lumier&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Docker interface for macOS and Linux VMs&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;docker pull trycua/lumier:latest&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/computer/README.md"&gt;&lt;strong&gt;Computer (Python)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python Interface for controlling virtual machines&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install "cua-computer[all]"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/typescript/computer/README.md"&gt;&lt;strong&gt;Computer (Typescript)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Typescript Interface for controlling virtual machines&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;npm install @trycua/computer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/agent/README.md"&gt;&lt;strong&gt;Agent&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AI agent framework for automating tasks&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install "cua-agent[all]"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/mcp-server/README.md"&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP server for using CUA with Claude Desktop&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-mcp-server&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/som/README.md"&gt;&lt;strong&gt;SOM&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Self-of-Mark library for Agent&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-som&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/computer-server/README.md"&gt;&lt;strong&gt;Computer Server&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Server component for Computer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-computer-server&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/core/README.md"&gt;&lt;strong&gt;Core (Python)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python Core utilities&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-core&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/typescript/core/README.md"&gt;&lt;strong&gt;Core (Typescript)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Typescript Core utilities&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;npm install @trycua/core&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.com/invite/mVnXXpdE85"&gt;Discord community&lt;/a&gt; to discuss ideas, get assistance, or share your demos!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Cua is open-sourced under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;Portions of this project, specifically components adapted from Kasm Technologies Inc., are also licensed under the MIT License. See &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/kasm/LICENSE"&gt;libs/kasm/LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Microsoft's OmniParser, which is used in this project, is licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0). See the &lt;a href="https://github.com/microsoft/OmniParser/raw/master/LICENSE"&gt;OmniParser LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h3&gt;Third-Party Licenses and Optional Components&lt;/h3&gt; 
&lt;p&gt;Some optional extras for this project depend on third-party packages that are licensed under terms different from the MIT License.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The optional "omni" extra (installed via &lt;code&gt;pip install "cua-agent[omni]"&lt;/code&gt;) installs the &lt;code&gt;cua-som&lt;/code&gt; module, which includes &lt;code&gt;ultralytics&lt;/code&gt; and is licensed under the AGPL-3.0.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When you choose to install and use such optional extras, your use, modification, and distribution of those third-party components are governed by their respective licenses (e.g., AGPL-3.0 for &lt;code&gt;ultralytics&lt;/code&gt;).&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to Cua! Please refer to our &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;Apple, macOS, and Apple Silicon are trademarks of Apple Inc.&lt;br /&gt; Ubuntu and Canonical are registered trademarks of Canonical Ltd.&lt;br /&gt; Microsoft is a registered trademark of Microsoft Corporation.&lt;/p&gt; 
&lt;p&gt;This project is not affiliated with, endorsed by, or sponsored by Apple Inc., Canonical Ltd., Microsoft Corporation, or Kasm Technologies.&lt;/p&gt; 
&lt;h2&gt;Stargazers&lt;/h2&gt; 
&lt;p&gt;Thank you to all our supporters!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/trycua/cua"&gt;&lt;img src="https://starchart.cc/trycua/cua.svg?variant=adaptive" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>haotian-liu/LLaVA</title>
      <link>https://github.com/haotian-liu/LLaVA</link>
      <description>&lt;p&gt;[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üåã LLaVA: Large Language and Vision Assistant&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Visual instruction tuning towards large language and vision models with GPT-4 level capabilities.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;[üì¢ &lt;a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/"&gt;LLaVA-NeXT Blog&lt;/a&gt;] [&lt;a href="https://llava-vl.github.io/"&gt;Project Page&lt;/a&gt;] [&lt;a href="https://llava.hliu.cc/"&gt;Demo&lt;/a&gt;] [&lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Data.md"&gt;Data&lt;/a&gt;] [&lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;ü§ùCommunity Contributions: [&lt;a href="https://github.com/ggerganov/llama.cpp/pull/3436"&gt;llama.cpp&lt;/a&gt;] [&lt;a href="https://github.com/camenduru/LLaVA-colab"&gt;Colab&lt;/a&gt;] [&lt;a href="https://huggingface.co/spaces/badayvedat/LLaVA"&gt;ü§óSpace&lt;/a&gt;] [&lt;a href="https://replicate.com/yorickvp/llava-13b"&gt;Replicate&lt;/a&gt;] [&lt;a href="https://github.com/microsoft/autogen/raw/main/notebook/agentchat_lmm_llava.ipynb"&gt;AutoGen&lt;/a&gt;] [&lt;a href="https://github.com/SkunkworksAI/BakLLaVA"&gt;BakLLaVA&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Improved Baselines with Visual Instruction Tuning&lt;/strong&gt; [&lt;a href="https://arxiv.org/abs/2310.03744"&gt;Paper&lt;/a&gt;] [&lt;a href="https://huggingface.co/papers/2310.03744"&gt;HF&lt;/a&gt;] &lt;br /&gt; &lt;a href="https://hliu.cc"&gt;Haotian Liu&lt;/a&gt;, &lt;a href="https://chunyuan.li/"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="https://yuheng-li.github.io/"&gt;Yuheng Li&lt;/a&gt;, &lt;a href="https://pages.cs.wisc.edu/~yongjaelee/"&gt;Yong Jae Lee&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Visual Instruction Tuning&lt;/strong&gt; (NeurIPS 2023, &lt;strong&gt;Oral&lt;/strong&gt;) [&lt;a href="https://arxiv.org/abs/2304.08485"&gt;Paper&lt;/a&gt;] [&lt;a href="https://huggingface.co/papers/2304.08485"&gt;HF&lt;/a&gt;] &lt;br /&gt; &lt;a href="https://hliu.cc"&gt;Haotian Liu*&lt;/a&gt;, &lt;a href="https://chunyuan.li/"&gt;Chunyuan Li*&lt;/a&gt;, &lt;a href="https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&amp;amp;hl=en/"&gt;Qingyang Wu&lt;/a&gt;, &lt;a href="https://pages.cs.wisc.edu/~yongjaelee/"&gt;Yong Jae Lee&lt;/a&gt; (*Equal Contribution)&lt;/p&gt; 
&lt;!--p align="center"&gt;
    &lt;a href="https://llava.hliu.cc/"&gt;&lt;img src="images/llava_logo.png" width="50%"&gt;&lt;/a&gt; &lt;br&gt;
    Generated by &lt;a href="https://gligen.github.io/"&gt;GLIGEN&lt;/a&gt; via "a cute lava llama with glasses" and box prompt
&lt;/p--&gt; 
&lt;h2&gt;Release&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2024/05/10] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Stronger) models are released, stronger LMM with support of LLama-3 (8B) and Qwen-1.5 (72B/110B). [&lt;a href="https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/"&gt;Blog&lt;/a&gt;] [&lt;a href="https://huggingface.co/collections/lmms-lab/llava-next-6623288e2d61edba3ddbf5ff"&gt;Checkpoints&lt;/a&gt;] [&lt;a href="https://llava-next.lmms-lab.com/"&gt;Demo&lt;/a&gt;] [&lt;a href="https://github.com/LLaVA-VL/LLaVA-NeXT/"&gt;Code&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;[2024/05/10] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (Video) is released. The image-only-trained LLaVA-NeXT model is surprisingly strong on video tasks with zero-shot modality transfer. DPO training with AI feedback on videos can yield significant improvement. [&lt;a href="https://llava-vl.github.io/blog/2024-04-30-llava-next-video/"&gt;Blog&lt;/a&gt;] [&lt;a href="https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944"&gt;Checkpoints&lt;/a&gt;] [&lt;a href="https://github.com/LLaVA-VL/LLaVA-NeXT/"&gt;Code&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;[03/10] Releasing &lt;strong&gt;LMMs-Eval&lt;/strong&gt;, a highly efficient evaluation pipeline we used when developing LLaVA-NeXT. It supports the evaluation of LMMs on dozens of public datasets and allows new dataset onboarding, making the dev of new LMMs much faster. [&lt;a href="https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/"&gt;Blog&lt;/a&gt;] [&lt;a href="https://github.com/EvolvingLMMs-Lab/lmms-eval"&gt;Codebase&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;[1/30] üî• &lt;strong&gt;LLaVA-NeXT&lt;/strong&gt; (LLaVA-1.6) is out! With additional scaling to LLaVA-1.5, LLaVA-NeXT-34B outperforms Gemini Pro on some benchmarks. It can now process 4x more pixels and perform more tasks/applications than before. Check out the &lt;a href="https://llava-vl.github.io/blog/2024-01-30-llava-next/"&gt;blog post&lt;/a&gt;, and explore the &lt;a href="https://llava.hliu.cc/"&gt;demo&lt;/a&gt;! Models are available in &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;. Training/eval data and scripts coming soon.&lt;/li&gt; 
 &lt;li&gt;[11/10] &lt;a href="https://llava-vl.github.io/llava-plus/"&gt;LLaVA-Plus&lt;/a&gt; is released: Learning to Use Tools for Creating Multimodal Agents, with LLaVA-Plus (LLaVA that Plug and Learn to Use Skills). [&lt;a href="https://llava-vl.github.io/llava-plus/"&gt;Project Page&lt;/a&gt;] [&lt;a href="https://llavaplus.ngrok.io/"&gt;Demo&lt;/a&gt;] [&lt;a href="https://github.com/LLaVA-VL/LLaVA-Plus-Codebase"&gt;Code&lt;/a&gt;] [&lt;a href="https://arxiv.org/abs/2311.05437"&gt;Paper&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;[11/2] &lt;a href="https://llava-vl.github.io/llava-interactive/"&gt;LLaVA-Interactive&lt;/a&gt; is released: Experience the future of human-AI multimodal interaction with an all-in-one demo for Image Chat, Segmentation, Generation and Editing. [&lt;a href="https://llava-vl.github.io/llava-interactive/"&gt;Project Page&lt;/a&gt;] [&lt;a href="https://llavainteractive.ngrok.io/"&gt;Demo&lt;/a&gt;] [&lt;a href="https://github.com/LLaVA-VL/LLaVA-Interactive-Demo"&gt;Code&lt;/a&gt;] [&lt;a href="https://arxiv.org/abs/2311.00571"&gt;Paper&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;[10/26] üî• LLaVA-1.5 with LoRA achieves comparable performance as full-model finetuning, with a reduced GPU RAM requirement (&lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md#llava-v15"&gt;ckpts&lt;/a&gt;, &lt;a href="https://github.com/haotian-liu/LLaVA#train"&gt;script&lt;/a&gt;). We also provide a &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md"&gt;doc&lt;/a&gt; on how to finetune LLaVA-1.5 on your own dataset with LoRA.&lt;/li&gt; 
 &lt;li&gt;[10/12] Check out the Korean LLaVA (Ko-LLaVA), created by ETRI, who has generously supported our research! [&lt;a href="https://huggingface.co/spaces/etri-vilab/Ko-LLaVA"&gt;ü§ó Demo&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;[10/5] üî• LLaVA-1.5 is out! Achieving SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods like Qwen-VL-Chat that use billion-scale data. Check out the &lt;a href="https://arxiv.org/abs/2310.03744"&gt;technical report&lt;/a&gt;, and explore the &lt;a href="https://llava.hliu.cc/"&gt;demo&lt;/a&gt;! Models are available in &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;. The training data and scripts of LLaVA-1.5 are released &lt;a href="https://github.com/haotian-liu/LLaVA#train"&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md"&gt;here&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;[9/26] LLaVA is improved with reinforcement learning from human feedback (RLHF) to improve fact grounding and reduce hallucination. Check out the new SFT and RLHF checkpoints at project &lt;a href="https://llava-rlhf.github.io/"&gt;[LLavA-RLHF]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[9/22] &lt;a href="https://arxiv.org/abs/2304.08485"&gt;LLaVA&lt;/a&gt; is accepted by NeurIPS 2023 as &lt;strong&gt;oral presentation&lt;/strong&gt;, and &lt;a href="https://arxiv.org/abs/2306.00890"&gt;LLaVA-Med&lt;/a&gt; is accepted by NeurIPS 2023 Datasets and Benchmarks Track as &lt;strong&gt;spotlight presentation&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[11/6] Support &lt;strong&gt;Intel&lt;/strong&gt; dGPU and CPU platforms. &lt;a href="https://github.com/haotian-liu/LLaVA/tree/intel/docs/intel"&gt;More details here.&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;[10/12] LLaVA is now supported in &lt;a href="https://github.com/ggerganov/llama.cpp/pull/3436"&gt;llama.cpp&lt;/a&gt; with 4-bit / 5-bit quantization support!&lt;/li&gt; 
  &lt;li&gt;[10/11] The training data and scripts of LLaVA-1.5 are released &lt;a href="https://github.com/haotian-liu/LLaVA#train"&gt;here&lt;/a&gt;, and evaluation scripts are released &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md"&gt;here&lt;/a&gt;!&lt;/li&gt; 
  &lt;li&gt;[10/10] &lt;a href="https://blog.roboflow.com/first-impressions-with-llava-1-5/"&gt;Roboflow Deep Dive&lt;/a&gt;: First Impressions with LLaVA-1.5.&lt;/li&gt; 
  &lt;li&gt;[9/20] We summarize our empirical study of training 33B and 65B LLaVA models in a &lt;a href="https://arxiv.org/abs/2309.09958"&gt;note&lt;/a&gt;. Further, if you are interested in the comprehensive review, evolution and trend of multimodal foundation models, please check out our recent survey paper &lt;a href="https://arxiv.org/abs/2309.10020"&gt;``Multimodal Foundation Models: From Specialists to General-Purpose Assistants''.&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p align="center"&gt; &lt;img src="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/raw/main/images/mfm_evolution.jpeg?raw=true" width="50%/" /&gt; &lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[7/19] üî• We release a major upgrade, including support for LLaMA-2, LoRA training, 4-/8-bit inference, higher resolution (336x336), and a lot more. We release &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_Bench.md"&gt;LLaVA Bench&lt;/a&gt; for benchmarking open-ended visual chat with results from Bard and Bing-Chat. We also support and verify training with RTX 3090 and RTX A6000. Check out &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/LLaVA_from_LLaMA2.md"&gt;LLaVA-from-LLaMA-2&lt;/a&gt;, and our &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md"&gt;model zoo&lt;/a&gt;!&lt;/li&gt; 
  &lt;li&gt;[6/26] &lt;a href="https://vlp-tutorial.github.io/"&gt;CVPR 2023 Tutorial&lt;/a&gt; on &lt;strong&gt;Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4&lt;/strong&gt;! Please check out [&lt;a href="https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Chunyuan_cvpr2023_tutorial_lmm.pdf"&gt;Slides&lt;/a&gt;] [&lt;a href="https://arxiv.org/abs/2306.14895"&gt;Notes&lt;/a&gt;] [&lt;a href="https://youtu.be/mkI7EPD1vp8"&gt;YouTube&lt;/a&gt;] [&lt;a href="https://www.bilibili.com/video/BV1Ng4y1T7v3/"&gt;Bilibli&lt;/a&gt;].&lt;/li&gt; 
  &lt;li&gt;[6/11] We released the preview for the most requested feature: DeepSpeed and LoRA support! Please see documentations &lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/docs/LoRA.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[6/1] We released &lt;strong&gt;LLaVA-Med: Large Language and Vision Assistant for Biomedicine&lt;/strong&gt;, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href="https://arxiv.org/abs/2306.00890"&gt;paper&lt;/a&gt; and &lt;a href="https://github.com/microsoft/LLaVA-Med"&gt;page&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[5/6] We are releasing &lt;a href="https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview"&gt;LLaVA-Lighting-MPT-7B-preview&lt;/a&gt;, based on MPT-7B-Chat! See &lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#LLaVA-MPT-7b"&gt;here&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;[5/2] üî• We are releasing LLaVA-Lighting! Train a lite, multimodal GPT-4 with just $40 in 3 hours! See &lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#train-llava-lightning"&gt;here&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;[4/27] Thanks to the community effort, LLaVA-13B with 4-bit quantization allows you to run on a GPU with as few as 12GB VRAM! Try it out &lt;a href="https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[4/17] üî• We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href="https://arxiv.org/abs/2304.08485"&gt;paper&lt;/a&gt; and &lt;a href="https://llava.hliu.cc/"&gt;demo&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;!-- &lt;a href="https://llava.hliu.cc/"&gt;&lt;img src="assets/demo.gif" width="70%"&gt;&lt;/a&gt; --&gt; 
&lt;p&gt;&lt;a href="https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true" alt="Code License" /&gt;&lt;/a&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the &lt;a href="https://openai.com/policies/terms-of-use"&gt;OpenAI Terms of Use&lt;/a&gt; for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. &lt;a href="https://ai.meta.com/llama/license/"&gt;Llama community license&lt;/a&gt; for LLaMA-2 and Vicuna-v1.5). This project does not impose any additional constraints beyond those stipulated in the original licenses. Furthermore, users are reminded to ensure that their use of the dataset and checkpoints is in compliance with all applicable laws and regulations.&lt;/p&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install"&gt;Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-weights"&gt;LLaVA Weights&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#Demo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Data.md"&gt;Dataset&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#train"&gt;Train&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#evaluation"&gt;Evaluation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;If you are not using Linux, do &lt;em&gt;NOT&lt;/em&gt; proceed, see instructions for &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/macOS.md"&gt;macOS&lt;/a&gt; and &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Windows.md"&gt;Windows&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository and navigate to LLaVA folder&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/haotian-liu/LLaVA.git
cd LLaVA
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install Package&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;conda create -n llava python=3.10 -y
conda activate llava
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Install additional packages for training cases&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e ".[train]"
pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Upgrade to latest code base&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;git pull
pip install -e .

# if you see some import errors when you upgrade,
# please try running the command below (without #)
# pip install flash-attn --no-build-isolation --no-cache-dir
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quick Start With HuggingFace&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Example Code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-Python"&gt;from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path
from llava.eval.run_llava import eval_model

model_path = "liuhaotian/llava-v1.5-7b"

tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path=model_path,
    model_base=None,
    model_name=get_model_name_from_path(model_path)
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Check out the details wth the &lt;code&gt;load_pretrained_model&lt;/code&gt; function in &lt;code&gt;llava/model/builder.py&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;You can also use the &lt;code&gt;eval_model&lt;/code&gt; function in &lt;code&gt;llava/eval/run_llava.py&lt;/code&gt; to get the output easily. By doing so, you can use this code on Colab directly after downloading this repository.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;model_path = "liuhaotian/llava-v1.5-7b"
prompt = "What are the things I should be cautious about when I visit here?"
image_file = "https://llava-vl.github.io/static/images/view.jpg"

args = type('Args', (), {
    "model_path": model_path,
    "model_base": None,
    "model_name": get_model_name_from_path(model_path),
    "query": prompt,
    "conv_mode": None,
    "image_file": image_file,
    "sep": ",",
    "temperature": 0,
    "top_p": None,
    "num_beams": 1,
    "max_new_tokens": 512
})()

eval_model(args)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;LLaVA Weights&lt;/h2&gt; 
&lt;p&gt;Please check out our &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt; for all public LLaVA checkpoints, and the instructions of how to use the weights.&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;h3&gt;Gradio Web UI&lt;/h3&gt; 
&lt;p&gt;To launch a Gradio demo locally, please run the following commands one by one. If you plan to launch multiple model workers to compare between different checkpoints, you only need to launch the controller and the web server &lt;em&gt;ONCE&lt;/em&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart BT
    %% Declare Nodes
    gws("Gradio (UI Server)")
    c("Controller (API Server):&amp;lt;br/&amp;gt;PORT: 10000")
    mw7b("Model Worker:&amp;lt;br/&amp;gt;llava-v1.5-7b&amp;lt;br/&amp;gt;PORT: 40000")
    mw13b("Model Worker:&amp;lt;br/&amp;gt;llava-v1.5-13b&amp;lt;br/&amp;gt;PORT: 40001")
    sglw13b("SGLang Backend:&amp;lt;br/&amp;gt;llava-v1.6-34b&amp;lt;br/&amp;gt;http://localhost:30000")
    lsglw13b("SGLang Worker:&amp;lt;br/&amp;gt;llava-v1.6-34b&amp;lt;br/&amp;gt;PORT: 40002")

    %% Declare Styles
    classDef data fill:#3af,stroke:#48a,stroke-width:2px,color:#444
    classDef success fill:#8f8,stroke:#0a0,stroke-width:2px,color:#444
    classDef failure fill:#f88,stroke:#f00,stroke-width:2px,color:#444

    %% Assign Styles
    class id,od data;
    class cimg,cs_s,scsim_s success;
    class ncimg,cs_f,scsim_f failure;

    subgraph Demo Connections
        direction BT
        c&amp;lt;--&amp;gt;gws
        
        mw7b&amp;lt;--&amp;gt;c
        mw13b&amp;lt;--&amp;gt;c
        lsglw13b&amp;lt;--&amp;gt;c
        sglw13b&amp;lt;--&amp;gt;lsglw13b
    end
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Launch a controller&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You just launched the Gradio web interface. Now, you can open the web interface with the URL printed on the screen. You may notice that there is no model in the model list. Do not worry, as we have not launched any model worker yet. It will be automatically updated when you launch a model worker.&lt;/p&gt; 
&lt;h4&gt;Launch a SGLang worker&lt;/h4&gt; 
&lt;p&gt;This is the recommended way to serve LLaVA model with high throughput, and you need to install SGLang first. Note that currently &lt;code&gt;4-bit&lt;/code&gt; quantization is not supported yet on SGLang-LLaVA, and if you have limited GPU VRAM, please check out model worker with &lt;a href="https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#launch-a-model-worker-4-bit-8-bit-inference-quantized"&gt;quantization&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;pip install "sglang[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll first launch a SGLang backend worker which will execute the models on GPUs. Remember the &lt;code&gt;--port&lt;/code&gt; you've set and you'll use that later.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;# Single GPU
CUDA_VISIBLE_DEVICES=0 python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000

# Multiple GPUs with tensor parallel
CUDA_VISIBLE_DEVICES=0,1 python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-13b --tokenizer-path llava-hf/llava-1.5-13b-hf --port 30000 --tp 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Tokenizers (temporary): &lt;code&gt;llava-hf/llava-1.5-7b-hf&lt;/code&gt;, &lt;code&gt;llava-hf/llava-1.5-13b-hf&lt;/code&gt;, &lt;code&gt;liuhaotian/llava-v1.6-34b-tokenizer&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You'll then launch a LLaVA-SGLang worker that will communicate between LLaVA controller and SGLang backend to route the requests. Set &lt;code&gt;--sgl-endpoint&lt;/code&gt; to &lt;code&gt;http://127.0.0.1:port&lt;/code&gt; where &lt;code&gt;port&lt;/code&gt; is the one you just set (default: 30000).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python -m llava.serve.sglang_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --sgl-endpoint http://127.0.0.1:30000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Launch a model worker&lt;/h4&gt; 
&lt;p&gt;This is the actual &lt;em&gt;worker&lt;/em&gt; that performs the inference on the GPU. Each worker is responsible for a single model specified in &lt;code&gt;--model-path&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Wait until the process finishes loading the model and you see "Uvicorn running on ...". Now, refresh your Gradio web UI, and you will see the model you just launched in the model list.&lt;/p&gt; 
&lt;p&gt;You can launch as many workers as you want, and compare between different model checkpoints in the same Gradio interface. Please keep the &lt;code&gt;--controller&lt;/code&gt; the same, and modify the &lt;code&gt;--port&lt;/code&gt; and &lt;code&gt;--worker&lt;/code&gt; to a different port number for each worker.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port &amp;lt;different from 40000, say 40001&amp;gt; --worker http://localhost:&amp;lt;change accordingly, i.e. 40001&amp;gt; --model-path &amp;lt;ckpt2&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using an Apple device with an M1 or M2 chip, you can specify the mps device by using the &lt;code&gt;--device&lt;/code&gt; flag: &lt;code&gt;--device mps&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Launch a model worker (Multiple GPUs, when GPU VRAM &amp;lt;= 24GB)&lt;/h4&gt; 
&lt;p&gt;If the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs. Our latest code base will automatically try to use multiple GPUs if you have more than one GPU. You can specify which GPUs to use with &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;. Below is an example of running with the first two GPUs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;CUDA_VISIBLE_DEVICES=0,1 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Launch a model worker (4-bit, 8-bit inference, quantized)&lt;/h4&gt; 
&lt;p&gt;You can launch the model worker with quantized bits (4-bit, 8-bit), which allows you to run the inference with reduced GPU memory footprint, potentially allowing you to run on a GPU with as few as 12GB VRAM. Note that inference with quantized bits may not be as accurate as the full-precision model. Simply append &lt;code&gt;--load-4bit&lt;/code&gt; or &lt;code&gt;--load-8bit&lt;/code&gt; to the &lt;strong&gt;model worker&lt;/strong&gt; command that you are executing. Below is an example of running with 4-bit quantization.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b --load-4bit
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Launch a model worker (LoRA weights, unmerged)&lt;/h4&gt; 
&lt;p&gt;You can launch the model worker with LoRA weights, without merging them with the base checkpoint, to save disk space. There will be additional loading time, while the inference speed is the same as the merged checkpoints. Unmerged LoRA checkpoints do not have &lt;code&gt;lora-merge&lt;/code&gt; in the model name, and are usually much smaller (less than 1GB) than the merged checkpoints (13G for 7B, and 25G for 13B).&lt;/p&gt; 
&lt;p&gt;To load unmerged LoRA weights, you simply need to pass an additional argument &lt;code&gt;--model-base&lt;/code&gt;, which is the base LLM that is used to train the LoRA weights. You can check the base LLM of each LoRA weights in the &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md"&gt;model zoo&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3 --model-base lmsys/vicuna-13b-v1.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;CLI Inference&lt;/h3&gt; 
&lt;p&gt;Chat about images using LLaVA without the need of Gradio interface. It also supports multiple GPUs, 4-bit and 8-bit quantized inference. With 4-bit quantization, for our LLaVA-1.5-7B, it uses less than 8GB VRAM on a single GPU.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python -m llava.serve.cli \
    --model-path liuhaotian/llava-v1.5-7b \
    --image-file "https://llava-vl.github.io/static/images/view.jpg" \
    --load-4bit
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/images/demo_cli.gif" width="70%" /&gt; 
&lt;h2&gt;Train&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Below is the latest training configuration for LLaVA v1.5. For legacy models, please refer to README of &lt;a href="https://github.com/haotian-liu/LLaVA/tree/v1.0.1"&gt;this&lt;/a&gt; version for now. We'll add them in a separate doc later.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;LLaVA training consists of two stages: (1) feature alignment stage: use our 558K subset of the LAION-CC-SBU dataset to connect a &lt;em&gt;frozen pretrained&lt;/em&gt; vision encoder to a &lt;em&gt;frozen LLM&lt;/em&gt;; (2) visual instruction tuning stage: use 150K GPT-generated multimodal instruction-following data, plus around 515K VQA data from academic-oriented tasks, to teach the model to follow multimodal instructions.&lt;/p&gt; 
&lt;p&gt;LLaVA is trained on 8 A100 GPUs with 80GB memory. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly. Always keep the global batch size the same: &lt;code&gt;per_device_train_batch_size&lt;/code&gt; x &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; x &lt;code&gt;num_gpus&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Hyperparameters&lt;/h3&gt; 
&lt;p&gt;We use a similar set of hyperparameters as Vicuna in finetuning. Both hyperparameters used in pretraining and finetuning are provided below.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pretraining&lt;/li&gt; 
&lt;/ol&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Hyperparameter&lt;/th&gt; 
   &lt;th align="right"&gt;Global Batch Size&lt;/th&gt; 
   &lt;th align="right"&gt;Learning rate&lt;/th&gt; 
   &lt;th align="right"&gt;Epochs&lt;/th&gt; 
   &lt;th align="right"&gt;Max length&lt;/th&gt; 
   &lt;th align="right"&gt;Weight decay&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLaVA-v1.5-13B&lt;/td&gt; 
   &lt;td align="right"&gt;256&lt;/td&gt; 
   &lt;td align="right"&gt;1e-3&lt;/td&gt; 
   &lt;td align="right"&gt;1&lt;/td&gt; 
   &lt;td align="right"&gt;2048&lt;/td&gt; 
   &lt;td align="right"&gt;0&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Finetuning&lt;/li&gt; 
&lt;/ol&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Hyperparameter&lt;/th&gt; 
   &lt;th align="right"&gt;Global Batch Size&lt;/th&gt; 
   &lt;th align="right"&gt;Learning rate&lt;/th&gt; 
   &lt;th align="right"&gt;Epochs&lt;/th&gt; 
   &lt;th align="right"&gt;Max length&lt;/th&gt; 
   &lt;th align="right"&gt;Weight decay&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLaVA-v1.5-13B&lt;/td&gt; 
   &lt;td align="right"&gt;128&lt;/td&gt; 
   &lt;td align="right"&gt;2e-5&lt;/td&gt; 
   &lt;td align="right"&gt;1&lt;/td&gt; 
   &lt;td align="right"&gt;2048&lt;/td&gt; 
   &lt;td align="right"&gt;0&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Download Vicuna checkpoints (automatically)&lt;/h3&gt; 
&lt;p&gt;Our base model Vicuna v1.5, which is an instruction-tuned chatbot, will be downloaded automatically when you run our provided training scripts. No action is needed.&lt;/p&gt; 
&lt;h3&gt;Pretrain (feature alignment)&lt;/h3&gt; 
&lt;p&gt;Please download the 558K subset of the LAION-CC-SBU dataset with BLIP captions we use in the paper &lt;a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Pretrain takes around 5.5 hours for LLaVA-v1.5-13B on 8x A100 (80G), due to the increased resolution to 336px. It takes around 3.5 hours for LLaVA-v1.5-7B.&lt;/p&gt; 
&lt;p&gt;Training script with DeepSpeed ZeRO-2: &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/scripts/v1_5/pretrain.sh"&gt;&lt;code&gt;pretrain.sh&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mm_projector_type mlp2x_gelu&lt;/code&gt;: the two-layer MLP vision-language connector.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--vision_tower openai/clip-vit-large-patch14-336&lt;/code&gt;: CLIP ViT-L/14 336px.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Pretrain takes around 20 hours for LLaVA-7B on 8x V100 (32G)&lt;/summary&gt; 
 &lt;p&gt;We provide training script with DeepSpeed &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/scripts/pretrain_xformers.sh"&gt;here&lt;/a&gt;. Tips:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;If you are using V100 which is not supported by FlashAttention, you can use the &lt;a href="https://arxiv.org/abs/2112.05682"&gt;memory-efficient attention&lt;/a&gt; implemented in &lt;a href="https://github.com/facebookresearch/xformers"&gt;xFormers&lt;/a&gt;. Install xformers and replace &lt;code&gt;llava/train/train_mem.py&lt;/code&gt; above with &lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/llava/train/train_xformers.py"&gt;llava/train/train_xformers.py&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Visual Instruction Tuning&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Prepare data&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please download the annotation of the final mixture our instruction tuning data &lt;a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json"&gt;llava_v1_5_mix665k.json&lt;/a&gt;, and download the images from constituting datasets:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;COCO: &lt;a href="http://images.cocodataset.org/zips/train2017.zip"&gt;train2017&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;GQA: &lt;a href="https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip"&gt;images&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;OCR-VQA: &lt;a href="https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing"&gt;download script&lt;/a&gt;, &lt;strong&gt;we save all files as &lt;code&gt;.jpg&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;TextVQA: &lt;a href="https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip"&gt;train_val_images&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VisualGenome: &lt;a href="https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip"&gt;part1&lt;/a&gt;, &lt;a href="https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip"&gt;part2&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After downloading all of them, organize the data as follows in &lt;code&gt;./playground/data&lt;/code&gt;,&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ coco
‚îÇ   ‚îî‚îÄ‚îÄ train2017
‚îú‚îÄ‚îÄ gqa
‚îÇ   ‚îî‚îÄ‚îÄ images
‚îú‚îÄ‚îÄ ocr_vqa
‚îÇ   ‚îî‚îÄ‚îÄ images
‚îú‚îÄ‚îÄ textvqa
‚îÇ   ‚îî‚îÄ‚îÄ train_images
‚îî‚îÄ‚îÄ vg
    ‚îú‚îÄ‚îÄ VG_100K
    ‚îî‚îÄ‚îÄ VG_100K_2
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Start training!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You may download our pretrained projectors in &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;. It is not recommended to use legacy projectors, as they may be trained with a different version of the codebase, and if any option is off, the model will not function/train as we expected.&lt;/p&gt; 
&lt;p&gt;Visual instruction tuning takes around 20 hours for LLaVA-v1.5-13B on 8x A100 (80G), due to the increased resolution to 336px. It takes around 10 hours for LLaVA-v1.5-7B on 8x A100 (40G).&lt;/p&gt; 
&lt;p&gt;Training script with DeepSpeed ZeRO-3: &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/scripts/v1_5/finetune.sh"&gt;&lt;code&gt;finetune.sh&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are do not have enough GPU memory:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use LoRA: &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/scripts/v1_5/finetune_lora.sh"&gt;&lt;code&gt;finetune_lora.sh&lt;/code&gt;&lt;/a&gt;. We are able to fit 13B training in 8-A100-40G/8-A6000, and 7B training in 8-RTX3090. Make sure &lt;code&gt;per_device_train_batch_size*gradient_accumulation_steps&lt;/code&gt; is the same as the provided script for best reproducibility.&lt;/li&gt; 
 &lt;li&gt;Replace &lt;code&gt;zero3.json&lt;/code&gt; with &lt;code&gt;zero3_offload.json&lt;/code&gt; which offloads some parameters to CPU RAM. This slows down the training speed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are interested in finetuning LLaVA model to your own task/data, please check out &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Finetune_Custom_Data.md"&gt;&lt;code&gt;Finetune_Custom_Data.md&lt;/code&gt;&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;p&gt;New options to note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mm_projector_type mlp2x_gelu&lt;/code&gt;: the two-layer MLP vision-language connector.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--vision_tower openai/clip-vit-large-patch14-336&lt;/code&gt;: CLIP ViT-L/14 336px.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--image_aspect_ratio pad&lt;/code&gt;: this pads the non-square images to square, instead of cropping them; it slightly reduces hallucination.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--group_by_modality_length True&lt;/code&gt;: this should only be used when your instruction tuning dataset contains both language (e.g. ShareGPT) and multimodal (e.g. LLaVA-Instruct). It makes the training sampler only sample a single modality (either image or language) during training, which we observe to speed up training by ~25%, and does not affect the final outcome.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;In LLaVA-1.5, we evaluate models on a diverse set of 12 benchmarks. To ensure the reproducibility, we evaluate the models with greedy decoding. We do not evaluate using beam search to make the inference process consistent with the chat demo of real-time outputs.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md"&gt;Evaluation.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;GPT-assisted Evaluation&lt;/h3&gt; 
&lt;p&gt;Our GPT-assisted evaluation pipeline for multimodal modeling is provided for a comprehensive understanding of the capabilities of vision-language models. Please see our paper for more details.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate LLaVA responses&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python model_vqa.py \
    --model-path ./checkpoints/LLaVA-13B-v0 \
    --question-file \
    playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \
    --image-folder \
    /path/to/coco2014_val \
    --answers-file \
    /path/to/answer-file-our.jsonl
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Evaluate the generated responses. In our case, &lt;a href="https://raw.githubusercontent.com/haotian-liu/LLaVA/main/playground/data/coco2014_val_qa_eval/qa90_gpt4_answer.jsonl"&gt;&lt;code&gt;answer-file-ref.jsonl&lt;/code&gt;&lt;/a&gt; is the response generated by text-only GPT-4 (0314), with the context captions/boxes provided.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;OPENAI_API_KEY="sk-***********************************" python llava/eval/eval_gpt_review_visual.py \
    --question playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \
    --context llava/eval/table/caps_boxes_coco2014_val_80.jsonl \
    --answer-list \
    /path/to/answer-file-ref.jsonl \
    /path/to/answer-file-our.jsonl \
    --rule llava/eval/table/rule.json \
    --output /path/to/review.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Summarize the evaluation results&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-Shell"&gt;python summarize_gpt_review.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find LLaVA useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@misc{liu2023improvedllava,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      publisher={arXiv:2310.03744},
      year={2023},
}

@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lm-sys/FastChat"&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM"&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/LLaVA-Med"&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Luodian/Otter"&gt;Otter: In-Context Multi-Modal Instruction Tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For future project ideas, please check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once"&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/IDEA-Research/Grounded-Segment-Anything"&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href="https://github.com/IDEA-Research/GroundingDINO"&gt;Grounding DINO&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/segment-anything"&gt;Segment-Anything&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>daveebbelaar/ai-cookbook</title>
      <link>https://github.com/daveebbelaar/ai-cookbook</link>
      <description>&lt;p&gt;Examples and tutorials to help developers build AI systems&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;This Cookbook contains examples and tutorials to help developers build AI systems, offering copy/paste code snippets that you can easily integrate into your own projects.&lt;/p&gt; 
&lt;h2&gt;About Me&lt;/h2&gt; 
&lt;p&gt;Hi! I'm Dave, AI Engineer and founder of Datalumina¬Æ. On my &lt;a href="https://www.youtube.com/@daveebbelaar?sub_confirmation=1"&gt;YouTube channel&lt;/a&gt;, I share practical tutorials that teach developers how to build AI systems that actually work in the real world. Beyond these tutorials, I also help people start successful freelancing careers. Check out the links below to learn more.&lt;/p&gt; 
&lt;h3&gt;Explore More Resources&lt;/h3&gt; 
&lt;p&gt;Continue learning ‚Äì wherever you are in your AI journey.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Getting Started with AI &amp;amp; Python&lt;/strong&gt;&lt;br /&gt; Dive into foundational content to build your technical base&lt;br /&gt; ‚ñ∂Ô∏é &lt;a href="https://go.datalumina.com/XRPBiLb"&gt;Learn Python for AI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üõ†Ô∏è &lt;strong&gt;Learn production-ready AI engineering&lt;/strong&gt;&lt;br /&gt; Learn the full GenAI stack, from basics to deployment&lt;br /&gt; ‚ñ∂Ô∏é &lt;a href="https://go.datalumina.com/CkIsMAK"&gt;Join the GenAI Accelerator&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí∏ &lt;strong&gt;Launch or grow your freelance AI business&lt;/strong&gt;&lt;br /&gt; Discover how to land clients, structure work, and scale&lt;br /&gt; ‚ñ∂Ô∏é &lt;a href="https://go.datalumina.com/MVWhVn9"&gt;Find Freelance Projects&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>MODSetter/SurfSense</title>
      <link>https://github.com/MODSetter/SurfSense</link>
      <description>&lt;p&gt;Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65" alt="new_header" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://discord.gg/ejRNvftDp9"&gt; &lt;img src="https://img.shields.io/discord/1359368468260192417" alt="Discord" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;SurfSense&lt;/h1&gt; 
&lt;p&gt;While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13606" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13606" alt="MODSetter%2FSurfSense | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da"&gt;https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Podcast Sample&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7"&gt;https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;üí° &lt;strong&gt;Idea&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.&lt;/p&gt; 
&lt;h3&gt;üìÅ &lt;strong&gt;Multiple File Format Uploading Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Save content from your own personal files &lt;em&gt;(Documents, images, videos and supports &lt;strong&gt;50+ file extensions&lt;/strong&gt;)&lt;/em&gt; to your own personal knowledge base .&lt;/p&gt; 
&lt;h3&gt;üîç &lt;strong&gt;Powerful Search&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Quickly research or find anything in your saved content .&lt;/p&gt; 
&lt;h3&gt;üí¨ &lt;strong&gt;Chat with your Saved Content&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Interact in Natural Language and get cited answers.&lt;/p&gt; 
&lt;h3&gt;üìÑ &lt;strong&gt;Cited Answers&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Get Cited answers just like Perplexity.&lt;/p&gt; 
&lt;h3&gt;üîî &lt;strong&gt;Privacy &amp;amp; Local LLM Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Works Flawlessly with Ollama local LLMs.&lt;/p&gt; 
&lt;h3&gt;üè† &lt;strong&gt;Self Hostable&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Open source and easy to deploy locally.&lt;/p&gt; 
&lt;h3&gt;üéôÔ∏è Podcasts&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; 
 &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; 
 &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; 
 &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìä &lt;strong&gt;Advanced RAG Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports 100+ LLM's&lt;/li&gt; 
 &lt;li&gt;Supports 6000+ Embedding Models.&lt;/li&gt; 
 &lt;li&gt;Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)&lt;/li&gt; 
 &lt;li&gt;Uses Hierarchical Indices (2 tiered RAG setup).&lt;/li&gt; 
 &lt;li&gt;Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).&lt;/li&gt; 
 &lt;li&gt;RAG as a Service API Backend.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚ÑπÔ∏è &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; 
 &lt;li&gt;Slack&lt;/li&gt; 
 &lt;li&gt;Linear&lt;/li&gt; 
 &lt;li&gt;Jira&lt;/li&gt; 
 &lt;li&gt;ClickUp&lt;/li&gt; 
 &lt;li&gt;Confluence&lt;/li&gt; 
 &lt;li&gt;Notion&lt;/li&gt; 
 &lt;li&gt;Gmail&lt;/li&gt; 
 &lt;li&gt;Youtube Videos&lt;/li&gt; 
 &lt;li&gt;GitHub&lt;/li&gt; 
 &lt;li&gt;Discord&lt;/li&gt; 
 &lt;li&gt;Airtable&lt;/li&gt; 
 &lt;li&gt;Google Calendar&lt;/li&gt; 
 &lt;li&gt;Luma&lt;/li&gt; 
 &lt;li&gt;and more to come.....&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ &lt;strong&gt;Supported File Extensions&lt;/strong&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Documents &amp;amp; Text&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.docm&lt;/code&gt;, &lt;code&gt;.dot&lt;/code&gt;, &lt;code&gt;.dotm&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.wpd&lt;/code&gt;, &lt;code&gt;.pages&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.602&lt;/code&gt;, &lt;code&gt;.abw&lt;/code&gt;, &lt;code&gt;.cgm&lt;/code&gt;, &lt;code&gt;.cwk&lt;/code&gt;, &lt;code&gt;.hwp&lt;/code&gt;, &lt;code&gt;.lwp&lt;/code&gt;, &lt;code&gt;.mw&lt;/code&gt;, &lt;code&gt;.mcw&lt;/code&gt;, &lt;code&gt;.pbd&lt;/code&gt;, &lt;code&gt;.sda&lt;/code&gt;, &lt;code&gt;.sdd&lt;/code&gt;, &lt;code&gt;.sdp&lt;/code&gt;, &lt;code&gt;.sdw&lt;/code&gt;, &lt;code&gt;.sgl&lt;/code&gt;, &lt;code&gt;.sti&lt;/code&gt;, &lt;code&gt;.sxi&lt;/code&gt;, &lt;code&gt;.sxw&lt;/code&gt;, &lt;code&gt;.stw&lt;/code&gt;, &lt;code&gt;.sxg&lt;/code&gt;, &lt;code&gt;.uof&lt;/code&gt;, &lt;code&gt;.uop&lt;/code&gt;, &lt;code&gt;.uot&lt;/code&gt;, &lt;code&gt;.vor&lt;/code&gt;, &lt;code&gt;.wps&lt;/code&gt;, &lt;code&gt;.zabw&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;, &lt;code&gt;.markdown&lt;/code&gt;, &lt;code&gt;.rst&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.org&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.xhtml&lt;/code&gt;, &lt;code&gt;.adoc&lt;/code&gt;, &lt;code&gt;.asciidoc&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Presentations&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;, &lt;code&gt;.pptm&lt;/code&gt;, &lt;code&gt;.pot&lt;/code&gt;, &lt;code&gt;.potm&lt;/code&gt;, &lt;code&gt;.potx&lt;/code&gt;, &lt;code&gt;.odp&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Spreadsheets &amp;amp; Data&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsm&lt;/code&gt;, &lt;code&gt;.xlsb&lt;/code&gt;, &lt;code&gt;.xlw&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;, &lt;code&gt;.ods&lt;/code&gt;, &lt;code&gt;.fods&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.dbf&lt;/code&gt;, &lt;code&gt;.123&lt;/code&gt;, &lt;code&gt;.dif&lt;/code&gt;, &lt;code&gt;.sylk&lt;/code&gt;, &lt;code&gt;.slk&lt;/code&gt;, &lt;code&gt;.prn&lt;/code&gt;, &lt;code&gt;.et&lt;/code&gt;, &lt;code&gt;.uos1&lt;/code&gt;, &lt;code&gt;.uos2&lt;/code&gt;, &lt;code&gt;.wk1&lt;/code&gt;, &lt;code&gt;.wk2&lt;/code&gt;, &lt;code&gt;.wk3&lt;/code&gt;, &lt;code&gt;.wk4&lt;/code&gt;, &lt;code&gt;.wks&lt;/code&gt;, &lt;code&gt;.wq1&lt;/code&gt;, &lt;code&gt;.wq2&lt;/code&gt;, &lt;code&gt;.wb1&lt;/code&gt;, &lt;code&gt;.wb2&lt;/code&gt;, &lt;code&gt;.wb3&lt;/code&gt;, &lt;code&gt;.qpw&lt;/code&gt;, &lt;code&gt;.xlr&lt;/code&gt;, &lt;code&gt;.eth&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Images&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.gif&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.svg&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.web&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.heic&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Audio &amp;amp; Video &lt;em&gt;(Always Supported)&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;.mp3&lt;/code&gt;, &lt;code&gt;.mpga&lt;/code&gt;, &lt;code&gt;.m4a&lt;/code&gt;, &lt;code&gt;.wav&lt;/code&gt;, &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.mpeg&lt;/code&gt;, &lt;code&gt;.webm&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Email &amp;amp; Communication&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.eml&lt;/code&gt;, &lt;code&gt;.msg&lt;/code&gt;, &lt;code&gt;.p7s&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;üîñ Cross Browser Extension&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The SurfSense extension can be used to save any webpage you like.&lt;/li&gt; 
 &lt;li&gt;Its main usecase is to save any webpages protected beyond authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FEATURE REQUESTS AND FUTURE&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;SurfSense is actively being developed.&lt;/strong&gt; While it's not yet production-ready, you can help us speed up the process.&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.gg/ejRNvftDp9"&gt;SurfSense Discord&lt;/a&gt; and help shape the future of SurfSense!&lt;/p&gt; 
&lt;h2&gt;üöÄ Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with our development progress and upcoming features!&lt;br /&gt; Check out our public roadmap and contribute your ideas or feedback:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;View the Roadmap:&lt;/strong&gt; &lt;a href="https://github.com/users/MODSetter/projects/2"&gt;SurfSense Roadmap on GitHub Projects&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to get started?&lt;/h2&gt; 
&lt;h3&gt;Installation Options&lt;/h3&gt; 
&lt;p&gt;SurfSense provides two installation methods:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/docker-installation"&gt;Docker Installation&lt;/a&gt;&lt;/strong&gt; - The easiest way to get SurfSense up and running with all dependencies containerized.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Includes pgAdmin for database management through a web UI&lt;/li&gt; 
   &lt;li&gt;Supports environment variable customization via &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Flexible deployment options (full stack or core services only)&lt;/li&gt; 
   &lt;li&gt;No need to manually edit configuration files between environments&lt;/li&gt; 
   &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DOCKER_SETUP.md"&gt;Docker Setup Guide&lt;/a&gt; for detailed instructions&lt;/li&gt; 
   &lt;li&gt;For deployment scenarios and options, see &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DEPLOYMENT_GUIDE.md"&gt;Deployment Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/manual-installation"&gt;Manual Installation (Recommended)&lt;/a&gt;&lt;/strong&gt; - For users who prefer more control over their setup or need to customize their deployment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Both installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.&lt;/p&gt; 
&lt;p&gt;Before installation, make sure to complete the &lt;a href="https://www.surfsense.net/docs/"&gt;prerequisite setup steps&lt;/a&gt; including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PGVector setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Processing ETL Service&lt;/strong&gt; (choose one): 
  &lt;ul&gt; 
   &lt;li&gt;Unstructured.io API key (supports 34+ formats)&lt;/li&gt; 
   &lt;li&gt;LlamaIndex API key (enhanced parsing, supports 50+ formats)&lt;/li&gt; 
   &lt;li&gt;Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other required API keys&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4" alt="updated_researcher" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Search Spaces&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099" alt="search_spaces" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Manage Documents&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d" alt="documents" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Podcast Agent&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c" alt="podcasts" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agent Chat&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491" alt="git_chat" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40" alt="ext1" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7" alt="ext2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;BackEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;: Modern, fast web framework for building APIs with Python&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PostgreSQL with pgvector&lt;/strong&gt;: Database with vector search capabilities for similarity searches&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SQLAlchemy&lt;/strong&gt;: SQL toolkit and ORM (Object-Relational Mapping) for database interactions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alembic&lt;/strong&gt;: A database migrations tool for SQLAlchemy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI Users&lt;/strong&gt;: Authentication and user management with JWT and OAuth support&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: Framework for developing AI-agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: Framework for developing AI-powered applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Integration&lt;/strong&gt;: Integration with LLM models through LiteLLM&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rerankers&lt;/strong&gt;: Advanced result ranking for improved search relevance&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt;: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector Embeddings&lt;/strong&gt;: Document and text embeddings for semantic search&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgvector&lt;/strong&gt;: PostgreSQL extension for efficient vector similarity operations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chonkie&lt;/strong&gt;: Advanced document chunking and embedding library&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Uses &lt;code&gt;AutoEmbeddings&lt;/code&gt; for flexible embedding model selection&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;LateChunker&lt;/code&gt; for optimized document chunking based on embedding model's max sequence length&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;FrontEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next.js 15.2.3&lt;/strong&gt;: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React 19.0.0&lt;/strong&gt;: JavaScript library for building user interfaces.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: Static type-checking for JavaScript, enhancing code quality and developer experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vercel AI SDK Kit UI Stream Protocol&lt;/strong&gt;: To create scalable chat UI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tailwind CSS 4.x&lt;/strong&gt;: Utility-first CSS framework for building custom UI designs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Shadcn&lt;/strong&gt;: Headless components library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lucide React&lt;/strong&gt;: Icon set implemented as React components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Framer Motion&lt;/strong&gt;: Animation library for React.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sonner&lt;/strong&gt;: Toast notification library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Geist&lt;/strong&gt;: Font family from Vercel.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React Hook Form&lt;/strong&gt;: Form state management and validation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zod&lt;/strong&gt;: TypeScript-first schema validation with static type inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@hookform/resolvers&lt;/strong&gt;: Resolvers for using validation libraries with React Hook Form.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@tanstack/react-table&lt;/strong&gt;: Headless UI for building powerful tables &amp;amp; datagrids.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;DevOps&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Container platform for consistent deployment across environments&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;: Tool for defining and running multi-container Docker applications&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgAdmin&lt;/strong&gt;: Web-based PostgreSQL administration tool included in Docker setup&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Extension&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Manifest v3 on Plasmo&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add More Connectors.&lt;/li&gt; 
 &lt;li&gt;Patch minor bugs.&lt;/li&gt; 
 &lt;li&gt;Document Podcasts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome! A contribution can be as small as a ‚≠ê or even finding and creating issues. Fine-tuning the Backend is always desired.&lt;/p&gt; 
&lt;p&gt;For detailed contribution guidelines, please see our &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#MODSetter/SurfSense&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4" alt="Catalyst Project" width="200" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>giantpinkrobots/varia</title>
      <link>https://github.com/giantpinkrobots/varia</link>
      <description>&lt;p&gt;Download manager that supports regular downloads, torrents and videos&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;&lt;em&gt;Important for translators: There's now a Weblate instance for Varia, please refer to &lt;a href="https://github.com/giantpinkrobots/varia/raw/next/CONTRIBUTING.md"&gt;the contributing page&lt;/a&gt;.&lt;/em&gt;&lt;/h3&gt; 
&lt;br /&gt; 
&lt;h1&gt;&lt;/h1&gt; 
&lt;br /&gt; 
&lt;p float="left" align="middle"&gt; &lt;img src="https://raw.githubusercontent.com/giantpinkrobots/varia/main/windows/icon.ico" width="200" /&gt; &lt;/p&gt; 
&lt;h1&gt;Varia&lt;/h1&gt; 
&lt;h3&gt;Download manager for files, torrents and videos&lt;/h3&gt; 
&lt;h3&gt;&lt;a href="https://giantpinkrobots.github.io/varia"&gt;üåê Homepage&lt;/a&gt;&lt;/h3&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Download for Linux&lt;/th&gt; 
   &lt;th&gt;Download for Windows&lt;/th&gt; 
   &lt;th&gt;Browser Extension&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://flathub.org/apps/io.github.giantpinkrobots.varia"&gt;‚¨á Flathub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/giantpinkrobots/varia/releases/download/v2025.7.19/varia-windows-setup-amd64.exe"&gt;‚¨á Installer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://addons.mozilla.org/firefox/addon/varia-integrator/"&gt;‚ùñ Firefox&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://snapcraft.io/varia"&gt;‚¨á Snap Store&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/giantpinkrobots/varia/releases/download/v2025.7.19/varia-windows-portable-amd64.zip"&gt;‚¨á Portable&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://chrome.google.com/webstore/detail/dacakhfljjhgdfdlgjpabkkjhbpcmiff"&gt;‚ùñ Chrome&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aur.archlinux.org/packages/varia"&gt;‚¨á AUR (unofficial)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;p&gt;Varia is a download manager for Linux and Windows that supports regular files as well as torrents and video/audio streams. It is a frontend for aria2 and yt-dlp.&lt;/p&gt; 
&lt;p float="left" align="middle"&gt; &lt;img src="https://raw.githubusercontent.com/giantpinkrobots/varia/main/screenshots/Screenshot-Varia-1.png" width="350" /&gt; &lt;img src="https://raw.githubusercontent.com/giantpinkrobots/varia/main/screenshots/Screenshot-Varia-2.png" width="350" /&gt; &lt;/p&gt; 
&lt;p float="left" align="middle"&gt; &lt;/p&gt;
&lt;p&gt;&lt;img src="https://img.shields.io/github/stars/giantpinkrobots/varia.svg?sanitize=true" alt="" /&gt; &lt;img src="https://img.shields.io/github/watchers/giantpinkrobots/varia.svg?sanitize=true" alt="" /&gt; &lt;img src="https://img.shields.io/github/followers/giantpinkrobots.svg?style=social&amp;amp;label=Follow&amp;amp;maxAge=2592000" alt="" /&gt; &lt;img src="https://img.shields.io/github/license/giantpinkrobots/varia.svg?sanitize=true" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p float="left" align="middle"&gt; &lt;a href="https://hosted.weblate.org/engage/varia/"&gt; &lt;img src="https://hosted.weblate.org/widget/varia/multi-auto.svg?sanitize=true" alt="Translation status" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Get Varia&lt;/h2&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;h4&gt;Flatpak&lt;/h4&gt; 
&lt;p&gt;The main way to get Varia that is supported by me is via &lt;a href="https://flathub.org/apps/io.github.giantpinkrobots.varia"&gt;Flathub&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;flatpak install flathub io.github.giantpinkrobots.varia
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This requires you to have Flatpak and the Flathub Flatpak repository installed on your system.&lt;/p&gt; 
&lt;h4&gt;Snap&lt;/h4&gt; 
&lt;p&gt;You can get Varia through the &lt;a href="https://snapcraft.io/varia"&gt;Snap Store&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo snap install varia
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However, you will need to give it additional permissions through the terminal if you want to use the "Shutdown after Completion" feature:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo snap connect varia:shutdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;AUR (Arch Linux) (Unofficial)&lt;/h4&gt; 
&lt;p&gt;You can get Varia via the &lt;a href="https://aur.archlinux.org/packages/varia"&gt;AUR&lt;/a&gt; as well, but it is &lt;strong&gt;unofficial&lt;/strong&gt; and not handled by me.&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;You can find amd64 builds of Varia in the Releases section in both installer and portable forms. The installer version is recommended and it includes an auto updater function.&lt;/p&gt; 
&lt;h2&gt;Browser Extension&lt;/h2&gt; 
&lt;p&gt;Download it for &lt;a href="https://addons.mozilla.org/firefox/addon/varia-integrator/"&gt;Firefox&lt;/a&gt; or &lt;a href="https://chrome.google.com/webstore/detail/dacakhfljjhgdfdlgjpabkkjhbpcmiff"&gt;Chrome&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;There are two branches here: 'main' and 'next'. 'next' is where the feature developments for the next version happen.&lt;/p&gt; 
&lt;p&gt;The 'main' branch can be built with the instructions below. The 'next' branch may also be built with these instructions, but it's not guaranteed. If you want to build the 'next' branch, it can be built with GNOME Builder on Linux.&lt;/p&gt; 
&lt;h3&gt;for Linux&lt;/h3&gt; 
&lt;p&gt;The easiest way of building Varia is to use GNOME Builder. Just clone this repository, and open the folder using Builder. Then, press run. This is the way I make Varia, and the 'next' branch can only be reliably built this way.&lt;/p&gt; 
&lt;p&gt;Varia is developed to be Flatpak-first, so it can be built with GNOME Builder with a single click on the Run button. You can also run it directly using flatpak-builder:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;flatpak-builder --force-clean --install --user ./_build ./io.github.giantpinkrobots.varia.json &amp;amp;&amp;amp; flatpak run io.github.giantpinkrobots.varia
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To build Varia without Flatpak or GNOME Builder though, you'll need:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;meson&lt;/li&gt; 
 &lt;li&gt;python-setuptools&lt;/li&gt; 
 &lt;li&gt;Gtk4 and its development libraries&lt;/li&gt; 
 &lt;li&gt;Libadwaita&lt;/li&gt; 
 &lt;li&gt;gettext&lt;/li&gt; 
 &lt;li&gt;aria2 and the aria2p python package.&lt;/li&gt; 
 &lt;li&gt;yt-dlp python package&lt;/li&gt; 
 &lt;li&gt;FFmpeg (without GPL is okay)&lt;/li&gt; 
 &lt;li&gt;python-dbus-next&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To install the ones besides aria2p on some Linux systems:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Ubuntu, Debian, Mint etc:
sudo apt install meson ninja-build aria2 python3-setuptools libgtk-4-dev libadwaita-1-0 gettext ffmpeg python3-dbus-next

Fedora, RHEL etc:
sudo dnf install meson ninja-build aria2 python3-setuptools gtk4-devel libadwaita gettext ffmpeg python3-dbus-next

Arch, EndeavourOS, Manjaro etc:
sudo pacman -S meson aria2 python-setuptools gtk4 libadwaita gettext ffmpeg python-dbus-next
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install aria2p and yt-dlp using pip (your distro probably doesn't have them in its repos - they're on the AUR for Arch):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install aria2p
pip install yt-dlp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, you can use meson commands to build Varia:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/giantpinkrobots/varia
cd varia
meson setup builddir
cd builddir
meson compile
sudo meson install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;for Windows&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.msys2.org/"&gt;Get MSYS2.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Open the MSYS2 standard shell and update everything before continuing:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;pacman -Syyu
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Either clone Varia inside the shell or copy the folder to your MSYS2 home folder and navigate into it:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;cd varia
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run the build script:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;./build-for-windows.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or with the updater function enabled: (it just creates an empty file in the dist directory named 'updater-function-enabled')&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./build-for-windows.sh -u
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Varia will be built into src/dist/variamain. Main executable is variamain.exe.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/giantpinkrobots/varia/raw/main/CONTRIBUTING.md"&gt;Please refer to the contributing guide page.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/giantpinkrobots/varia/raw/main/LICENSE"&gt;Varia is licensed under the Mozilla Public License 2.0.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;But it also relies on many other libraries each with their own licenses, all of whom can be found in the dependencies_information directory.&lt;/p&gt; 
&lt;h2&gt;The name&lt;/h2&gt; 
&lt;p&gt;The name "Varia" comes from the aria2 software it is based on, and I added a "V" to make it "Varia". In the Metroid series of games, there is a special suit you eventually get named a "&lt;a href="https://metroid.fandom.com/wiki/Varia_Suit"&gt;Varia Suit&lt;/a&gt;" with its main feature being allowing Samus to withstand extreme temperatures. I spent some time thinking about how to connect the Varia Suit to my app, but couldn't, soooo... I think it just sounds cool.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/magentic-ui</title>
      <link>https://github.com/microsoft/magentic-ui</link>
      <description>&lt;p&gt;A research prototype of a human-centered web agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-readme-logo.svg?sanitize=true" alt="Magentic-UI Logo" /&gt; 
 &lt;p&gt;&lt;em&gt;Automate your web tasks while you stay in control&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/v/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/l/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue" alt="Python Versions" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Magentic-UI is a &lt;strong&gt;research prototype&lt;/strong&gt; of a human-centered interface powered by a multi-agent system that can browse and perform actions on the web, generate and execute code, and generate and analyze files.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7"&gt;https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Here's how you can get started with Magentic-UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Setup environment
python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui --upgrade

# 2. Set your API key
export OPENAI_API_KEY="your-api-key-here"

# 3. Launch Magentic-UI
magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt; in your browser to interact with Magentic-UI!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Requires Docker and Python 3.10+. Windows users should use WSL2. See &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#%EF%B8%8F-installation"&gt;detailed installation&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;‚ú® What's New&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;File Upload Support&lt;/strong&gt;: Upload any file through the UI for analysis or modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Agents&lt;/strong&gt;: Extend capabilities with your favorite MCP servers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier Installation&lt;/strong&gt;: We have uploaded our docker containers to GHCR so you no longer need to build any containers! Installation time now is much quicker.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Alternative Usage Options&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Without Docker&lt;/strong&gt; (limited functionality: no code execution):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --run-without-docker --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Command Line Interface&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-cli --work-dir PATH/TO/STORE/DATA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Custom LLM Clients&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Azure
pip install magentic-ui[azure]

# Ollama (local models)
pip install magentic-ui[ollama]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then pass a config file to the &lt;code&gt;magentic-ui&lt;/code&gt; command (&lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt; client config&lt;/a&gt;) or change the model client inside the UI settings.&lt;/p&gt; 
&lt;p&gt;For further details on installation please read the &lt;a href="#Ô∏è-installation"&gt;üõ†Ô∏è Installation&lt;/a&gt; section. For common installation issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;troubleshooting document&lt;/a&gt;. See advanced usage instructions with the command &lt;code&gt;magentic-ui --help&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Navigation:&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#-how-it-works"&gt;üü™ How it Works&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="#Ô∏è-installation"&gt;üõ†Ô∏è Installation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#troubleshooting"&gt;‚ö†Ô∏è Troubleshooting&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#contributing"&gt;ü§ù Contributing&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#license"&gt;üìÑ License&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üü™ How it Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magenticui_running.png" alt="Magentic-UI" height="400" /&gt; &lt;/p&gt; 
&lt;p&gt;Magentic-UI is especially useful for web tasks that require actions on the web (e.g., filling a form, customizing a food order), deep navigation through websites not indexed by search engines (e.g., filtering flights, finding a link from a personal site) or tasks that need web navigation and code execution (e.g., generate a chart from online data).&lt;/p&gt; 
&lt;p&gt;The interface of Magentic-UI is displayed in the screenshot above and consists of two panels. The left side panel is the sessions navigator where users can create new sessions to solve new tasks, switch between sessions and check on session progress with the session status indicators (üî¥ needs input, ‚úÖ task done, ‚Ü∫ task in progress).&lt;/p&gt; 
&lt;p&gt;The right-side panel displays the session selected. This is where you can type your query to Magentic-UI alongside any file attachments and observe detailed task progress as well as interact with the agents. The session display itself is split in two panels: the left side is where Magentic-UI presents the plan, task progress and asks for action approvals, the right side is a browser view where you can see web agent actions in real time and interact with the browser. Finally, at the top of the session display is a progress bar that updates as Magentic-UI makes progress.&lt;/p&gt; 
&lt;p&gt;The example below shows a step by step user interaction with Magentic-UI:&lt;/p&gt; 
&lt;!-- Screenshots --&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-landing.png" alt="Magentic-UI Landing" width="45%" style="margin:10px;" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-coplanning.png" alt="Co-Planning UI" width="45%" style="margin:10px;" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-cotasking.png" alt="Co-Tasking UI" width="45%" style="margin:10px;" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-actionguard.png" alt="Action Guard UI" width="45%" style="margin:10px;" /&gt; &lt;/p&gt; 
&lt;p&gt;What differentiates Magentic-UI from other browser use offerings is its transparent and controllable interface that allows for efficient human-in-the-loop involvement. Magentic-UI is built using &lt;a href="https://github.com/microsoft/autogen"&gt;AutoGen&lt;/a&gt; and provides a platform to study human-agent interaction and experiment with web agents. Key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üßë‚Äçü§ù‚Äçüßë &lt;strong&gt;Co-Planning&lt;/strong&gt;: Collaboratively create and approve step-by-step plans using chat and the plan editor.&lt;/li&gt; 
 &lt;li&gt;ü§ù &lt;strong&gt;Co-Tasking&lt;/strong&gt;: Interrupt and guide the task execution using the web browser directly or through chat. Magentic-UI can also ask for clarifications and help when needed.&lt;/li&gt; 
 &lt;li&gt;üõ°Ô∏è &lt;strong&gt;Action Guards&lt;/strong&gt;: Sensitive actions are only executed with explicit user approvals.&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;Plan Learning and Retrieval&lt;/strong&gt;: Learn from previous runs to improve future task automation and save them in a plan gallery. Automatically or manually retrieve saved plans in future tasks.&lt;/li&gt; 
 &lt;li&gt;üîÄ &lt;strong&gt;Parallel Task Execution&lt;/strong&gt;: You can run multiple tasks in parallel and session status indicators will let you know when Magentic-UI needs your input or has completed the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=wOs-5SR8xOc" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/wOs-5SR8xOc/maxresdefault.jpg" alt="Watch the demo video" width="600" /&gt; &lt;/a&gt; 
 &lt;br /&gt; ‚ñ∂Ô∏è 
 &lt;em&gt; Click to watch a video and learn more about Magentic-UI &lt;/em&gt; 
&lt;/div&gt; 
&lt;h3&gt;Autonomous Evaluation&lt;/h3&gt; 
&lt;p&gt;To evaluate its autonomous capabilities, Magentic-UI has been tested against several benchmarks when running with o4-mini: &lt;a href="https://huggingface.co/datasets/gaia-benchmark/GAIA"&gt;GAIA&lt;/a&gt; test set (42.52%), which assesses general AI assistants across reasoning, tool use, and web interaction tasks ; &lt;a href="https://huggingface.co/AssistantBench"&gt;AssistantBench&lt;/a&gt; test set (27.60%), focusing on realistic, time-consuming web tasks; &lt;a href="https://github.com/MinorJerry/WebVoyager"&gt;WebVoyager&lt;/a&gt; (82.2%), measuring end-to-end web navigation in real-world scenarios; and &lt;a href="https://webgames.convergence.ai/"&gt;WebGames&lt;/a&gt; (45.5%), evaluating general-purpose web-browsing agents through interactive challenges. To reproduce these experimental results, please see the following &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/experiments/eval/README.md"&gt;instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're interested in reading more checkout our &lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/magentic-ui-report.pdf"&gt;technical report&lt;/a&gt; and &lt;a href="https://www.microsoft.com/en-us/research/blog/magentic-ui-an-experimental-human-centered-web-agent/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; 
&lt;h3&gt;Pre-Requisites&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you're using Windows, we highly recommend using &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux).&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If running on &lt;strong&gt;Windows&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt; you should use &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; or if inside WSL2 you can install Docker directly inside WSL &lt;a href="https://gist.github.com/dehsilvadeveloper/c3bdf0f4cdcc5c177e2fe9be671820c7"&gt;docker in WSL2 guide&lt;/a&gt;. If running on &lt;strong&gt;Linux&lt;/strong&gt;, you should use &lt;a href="https://docs.docker.com/engine/install/"&gt;Docker Engine&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If using Docker Desktop, make sure it is set up to use WSL2: - Go to Settings &amp;gt; Resources &amp;gt; WSL Integration - Enable integration with your development distro You can find more detailed instructions about this step &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;During the Installation step, you will need to set up your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. To use other models, review the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt;Model Client Configuration&lt;/a&gt; section below.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You need at least &lt;a href="https://www.python.org/downloads/"&gt;Python 3.10&lt;/a&gt; installed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If you are on Windows, we recommend to run Magentic-UI inside &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux) for correct Docker and file path compatibility.&lt;/p&gt; 
&lt;h3&gt;PyPI Installation&lt;/h3&gt; 
&lt;p&gt;Magentic-UI is available on PyPI. We recommend using a virtual environment to avoid conflicts with other packages.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you use &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; for dependency management, you can install Magentic-UI with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
. .venv/bin/activate
uv pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Magentic-UI&lt;/h3&gt; 
&lt;p&gt;To run Magentic-UI, make sure that Docker is running, then run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Running this command for the first time will pull two docker images required for the Magentic-UI agents. If you encounter problems, you can build them directly with the following command:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker
sh build-all.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you face issues with Docker, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Once the server is running, you can access the UI at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;h4&gt;Model Client Configuration&lt;/h4&gt; 
&lt;p&gt;If you want to use a different OpenAI key, or if you want to configure use with Azure OpenAI or Ollama, you can do so inside the UI by navigating to settings (top right icon) and changing model configuration. Another option is to pass a yaml config file when you start Magentic-UI which will override any settings in the UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081 --config config.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where the &lt;code&gt;config.yaml&lt;/code&gt; should look as follows with an AutoGen model client configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;gpt4o_client: &amp;amp;gpt4o_client
    provider: OpenAIChatCompletionClient
    config:
      model: gpt-4o-2024-08-06
      api_key: null
      base_url: null
      max_retries: 5

orchestrator_client: *gpt4o_client
coder_client: *gpt4o_client
web_surfer_client: *gpt4o_client
file_surfer_client: *gpt4o_client
action_guard_client: *gpt4o_client
plan_learning_client: *gpt4o_client
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can change the client for each of the agents using the config file and use AzureOpenAI (&lt;code&gt;AzureOpenAIChatCompletionClient&lt;/code&gt;), Ollama and other clients.&lt;/p&gt; 
&lt;h4&gt;MCP Server Configuration&lt;/h4&gt; 
&lt;p&gt;You can also extend Magentic-UI's capabilities by adding custom "McpAgents" to the multi-agent team. Each McpAgent can have access to one or more MCP Servers. You can specify these agents via the &lt;code&gt;mcp_agent_configs&lt;/code&gt; parameter in your &lt;code&gt;config.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, here's an agent called "airbnb_surfer" that has access to the OpenBnb MCP Server running locally via Stdio.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp_agent_configs:
  - name: airbnb_surfer
    description: "The airbnb_surfer has direct access to AirBnB."
    model_client: 
      provider: OpenAIChatCompletionClient
      config:
        model: gpt-4.1-2025-04-14
      max_retries: 10
    system_message: |-
      You are AirBnb Surfer, a helpful digital assistant that can help users acces AirBnB.

      You have access to a suite of tools provided by the AirBnB API. Use those tools to satisfy the users requests.
    reflect_on_tool_use: false
    mcp_servers:
      - server_name: AirBnB
        server_params:
          type: StdioServerParams
          command: npx
          args:
            - -y
            - "@openbnb/mcp-server-airbnb"
            - --ignore-robots-txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Under the hood, each &lt;code&gt;McpAgent&lt;/code&gt; is just a &lt;code&gt;autogen_agentchat.agents.AssistantAgent&lt;/code&gt; with the set of MCP Servers exposed as an &lt;code&gt;AggregateMcpWorkbench&lt;/code&gt; which is simply a named collection of &lt;code&gt;autogen_ext.tools.mcp.McpWorkbench&lt;/code&gt; objects (one per MCP Server).&lt;/p&gt; 
&lt;p&gt;Currently the supported MCP Server types are &lt;code&gt;autogen_ext.tools.mcp.StdioServerParams&lt;/code&gt; and &lt;code&gt;autogen_ext.tools.mcp.SseServerParams&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Building Magentic-UI from source&lt;/h3&gt; 
&lt;p&gt;This step is primarily for users seeking to make modifications to the code, are having trouble with the pypi installation or want the latest code before a pypi version release.&lt;/p&gt; 
&lt;h4&gt;1. Make sure the above prerequisites are installed, and that Docker is running.&lt;/h4&gt; 
&lt;h4&gt;2. Clone the repository to your local machine:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/microsoft/magentic-ui.git
cd magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Install Magentic-UI's dependencies with uv or your favorite package manager:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv through https://docs.astral.sh/uv/getting-started/installation/
uv venv --python=3.12 .venv
uv sync --all-extras
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Build the frontend:&lt;/h4&gt; 
&lt;p&gt;First make sure to install node:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install nvm to install node
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
nvm install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install the frontend:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
npm install -g gatsby-cli
npm install --global yarn
yarn install
yarn build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Run Magentic-UI, as usual.&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the UI from source&lt;/h4&gt; 
&lt;p&gt;If you are making changes to the source code of the UI, you can run the frontend in development mode so that it will automatically update when you make changes for faster development.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open a separate terminal and change directory to the frontend&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a &lt;code&gt;.env.development&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.default .env.development
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Launch frontend server&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run start
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Then run the UI:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontend from source will be available at &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt;, and the compiled frontend will be available at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;If you were unable to get Magentic-UI running, do not worry! The first step is to make sure you have followed the steps outlined above, particularly with the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#pre-requisites"&gt;pre-requisites&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For common issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; file in this repository. If you do not see your problem there, please open a &lt;code&gt;GitHub Issue&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. For information about contributing to Magentic-UI, please see our &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide, which includes current issues to be resolved and other forms of contributing.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information, see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please cite our paper if you use our work in your research:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{mozannar2025magentic,
  title={Magentic-UI: Towards Human-in-the-loop Agentic Systems},
  author={Mozannar, Hussein and Bansal, Gagan and Tan, Cheng and Fourney, Adam and Dibia, Victor and Chen, Jingya and Gerrits, Jack and Payne, Tyler and Maldaner, Matheus Kunzler and Grunde-McLaughlin, Madeleine and others},
  journal={arXiv preprint arXiv:2507.22358},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Microsoft, and any contributors, grant you a license to any code in the repository under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt;. See the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft's general trademark guidelines can be found at &lt;a href="http://go.microsoft.com/fwlink/?LinkID=254653"&gt;http://go.microsoft.com/fwlink/?LinkID=254653&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;p&gt;Privacy information can be found at &lt;a href="https://go.microsoft.com/fwlink/?LinkId=521839"&gt;https://go.microsoft.com/fwlink/?LinkId=521839&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenPipe/ART</title>
      <link>https://github.com/OpenPipe/ART</link>
      <description>&lt;p&gt;Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://art.openpipe.ai"&gt;
   &lt;picture&gt; 
    &lt;img alt="ART logo" src="https://github.com/openpipe/art/raw/main/assets/ART_logo.png" width="160px" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Train multi-step agents for real-world tasks using GRPO. &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/openpipe/art/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true" alt="PRs-Welcome" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/openpipe-art/"&gt;&lt;img src="https://img.shields.io/pypi/v/openpipe-art?color=364fc7" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Train Agent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/zbBHRUpwf4"&gt;&lt;img src="https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://art.openpipe.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-orange?style=plastic&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üöÄ W&amp;amp;B Training: Serverless RL&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;W&amp;amp;B Training (Serverless RL)&lt;/strong&gt; is the first publicly available service for flexibly training models with reinforcement learning. It manages your training and inference infrastructure automatically, letting you focus on defining your data, environment and reward function‚Äîleading to faster feedback cycles, lower costs, and far less DevOps.&lt;/p&gt; 
&lt;p&gt;‚ú® &lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;40% lower cost&lt;/strong&gt; - Multiplexing on shared production-grade inference cluster&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;28% faster training&lt;/strong&gt; - Scale to 2000+ concurrent requests across many GPUs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero infra headaches&lt;/strong&gt; - Fully managed infrastructure that stays healthy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant deployment&lt;/strong&gt; - Every checkpoint instantly available via W&amp;amp;B Inference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Before: Hours of GPU setup and infra management
# RuntimeError: CUDA error: out of memory üò¢

# After: Serverless RL with instant feedback
from art.serverless.backend import ServerlessBackend

model = art.TrainableModel(
  project="voice-agent",
  name="agent-001",
  base_model="Qwen/Qwen2.5-14B-Instruct"
)

backend = ServerlessBackend(
    api_key="your_wandb_api_key"
)
model.register(backend)
# Edit and iterate in minutes, not hours!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.wandb.ai/guides/training"&gt;üìñ Learn more about W&amp;amp;B Training ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ART Overview&lt;/h2&gt; 
&lt;p&gt;ART is an open-source RL framework that improves agent reliability by allowing LLMs to &lt;strong&gt;learn from experience&lt;/strong&gt;. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the &lt;a href="https://art.openpipe.ai"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìí Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Task&lt;/th&gt; 
   &lt;th&gt;Example Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Comparative Performance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ART‚Ä¢E [Serverless]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 14B learns to search emails using RULER&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/dev/art-e/art_e/evaluate/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2048 [Serverless]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 14B learns to play 2048&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/2048/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ART‚Ä¢E LangGraph&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using LangGraph&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP‚Ä¢RL&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B masters the NWS MCP server&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Clue&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to solve Temporal Clue&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tic Tac Toe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Tic Tac Toe&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/tic_tac_toe/display-benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codenames&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Codenames&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png" height="72" /&gt; &lt;a href="https://github.com/OpenPipe/art-notebooks/raw/main/examples/codenames/Codenames_RL.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AutoRL [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Train Qwen 2.5 7B to master any task&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üì∞ ART News&lt;/h2&gt; 
&lt;p&gt;Explore our latest research and updates on building SOTA agents.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://art.openpipe.ai/integrations/langgraph-integration"&gt;ART now integrates seamlessly with LangGraph&lt;/a&gt;&lt;/strong&gt; - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://x.com/corbtt/status/1953171838382817625"&gt;MCP‚Ä¢RL: Teach Your Model to Master Any MCP Server&lt;/a&gt;&lt;/strong&gt; - Automatically train models to effectively use MCP server tools through reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://x.com/mattshumer_/status/1950572449025650733"&gt;AutoRL: Zero-Data Training for Any Task&lt;/a&gt;&lt;/strong&gt; - Train custom AI models without labeled data using automatic input generation and RULER evaluation.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards"&gt;RULER: Easy Mode for RL Rewards&lt;/a&gt;&lt;/strong&gt; is now available for automatic reward generation in reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ART¬∑E: How We Built an Email Research Agent That Beats o3&lt;/a&gt;&lt;/strong&gt; demonstrates a Qwen 2.5 14B email agent outperforming OpenAI's o3.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-trainer"&gt;ART Trainer: A New RL Trainer for Agents&lt;/a&gt;&lt;/strong&gt; enables easy training of LLM-based agents using GRPO.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://openpipe.ai/blog"&gt;üìñ See all blog posts ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why ART?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ART provides convenient wrappers for introducing RL training into &lt;strong&gt;existing applications&lt;/strong&gt;. We abstract the training server into a modular service that your code doesn't need to interface with.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train from anywhere.&lt;/strong&gt; Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.&lt;/li&gt; 
 &lt;li&gt;Integrations with hosted platforms like W&amp;amp;B, Langfuse, and OpenPipe provide flexible observability and &lt;strong&gt;simplify debugging&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ART is customizable with &lt;strong&gt;intelligent defaults&lt;/strong&gt;. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install openpipe-art
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ñ ART‚Ä¢E Agent&lt;/h2&gt; 
&lt;p&gt;Curious about how to use ART for a real-world task? Check out the &lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ART‚Ä¢E Agent&lt;/a&gt; blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!&lt;/p&gt; 
&lt;img src="https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png" width="700" /&gt; 
&lt;h2&gt;üîÅ Training Loop Overview&lt;/h2&gt; 
&lt;p&gt;ART's functionality is divided into a &lt;strong&gt;client&lt;/strong&gt; and a &lt;strong&gt;server&lt;/strong&gt;. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).&lt;/li&gt; 
   &lt;li&gt;Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.&lt;/li&gt; 
   &lt;li&gt;As the agent executes, each &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, and &lt;code&gt;assistant&lt;/code&gt; message is stored in a Trajectory.&lt;/li&gt; 
   &lt;li&gt;When a rollout finishes, your code assigns a &lt;code&gt;reward&lt;/code&gt; to its Trajectory, indicating the performance of the LLM.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.&lt;/li&gt; 
   &lt;li&gt;The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).&lt;/li&gt; 
   &lt;li&gt;The server saves the newly trained LoRA to a local directory and loads it into vLLM.&lt;/li&gt; 
   &lt;li&gt;Inference is unblocked and the loop resumes at step 1.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This training loop runs until a specified number of inference and training iterations have completed.&lt;/p&gt; 
&lt;h2&gt;üß© Supported Models&lt;/h2&gt; 
&lt;p&gt;ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth&lt;/a&gt;. Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on &lt;a href="https://discord.gg/zbBHRUpwf4"&gt;Discord&lt;/a&gt; or open an issue on &lt;a href="https://github.com/openpipe/art/issues"&gt;GitHub&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;ART is in active development, and contributions are most welcome! Please see the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚öñÔ∏è License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üôè Credits&lt;/h2&gt; 
&lt;p&gt;ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/trl"&gt;trl&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchtune"&gt;torchtune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-agents-python</title>
      <link>https://github.com/openai/openai-agents-python</link>
      <description>&lt;p&gt;A lightweight, powerful framework for multi-agent workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Agents SDK&lt;/h1&gt; 
&lt;p&gt;The OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows. It is provider-agnostic, supporting the OpenAI Responses and Chat Completions APIs, as well as 100+ other LLMs.&lt;/p&gt; 
&lt;img src="https://cdn.openai.com/API/docs/images/orchestration.png" alt="Image of the Agents Tracing UI" style="max-height: 803px;" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JavaScript/TypeScript version? Check out &lt;a href="https://github.com/openai/openai-agents-js"&gt;Agents SDK JS/TS&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Core concepts:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/agents"&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/a&gt;: LLMs configured with instructions, tools, guardrails, and handoffs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/handoffs/"&gt;&lt;strong&gt;Handoffs&lt;/strong&gt;&lt;/a&gt;: A specialized tool call used by the Agents SDK for transferring control between agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/guardrails/"&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;&lt;/a&gt;: Configurable safety checks for input and output validation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/#sessions"&gt;&lt;strong&gt;Sessions&lt;/strong&gt;&lt;/a&gt;: Automatic conversation history management across agent runs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/tracing/"&gt;&lt;strong&gt;Tracing&lt;/strong&gt;&lt;/a&gt;: Built-in tracking of agent runs, allowing you to view, debug and optimize your workflows&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Explore the &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples"&gt;examples&lt;/a&gt; directory to see the SDK in action, and read our &lt;a href="https://openai.github.io/openai-agents-python/"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;p&gt;To get started, set up your Python environment (Python 3.9 or newer required), and then install OpenAI Agents SDK package.&lt;/p&gt; 
&lt;h3&gt;venv&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install openai-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For voice support, install with the optional &lt;code&gt;voice&lt;/code&gt; group: &lt;code&gt;pip install 'openai-agents[voice]'&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For Redis session support, install with the optional &lt;code&gt;redis&lt;/code&gt; group: &lt;code&gt;pip install 'openai-agents[redis]'&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;uv&lt;/h3&gt; 
&lt;p&gt;If you're familiar with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, using the tool would be even similar:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
uv add openai-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For voice support, install with the optional &lt;code&gt;voice&lt;/code&gt; group: &lt;code&gt;uv add 'openai-agents[voice]'&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For Redis session support, install with the optional &lt;code&gt;redis&lt;/code&gt; group: &lt;code&gt;uv add 'openai-agents[redis]'&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Hello world example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(&lt;em&gt;If running this, ensure you set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable&lt;/em&gt;)&lt;/p&gt; 
&lt;p&gt;(&lt;em&gt;For Jupyter notebook users, see &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples/basic/hello_world_jupyter.ipynb"&gt;hello_world_jupyter.ipynb&lt;/a&gt;&lt;/em&gt;)&lt;/p&gt; 
&lt;h2&gt;Handoffs example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
)


async def main():
    result = await Runner.run(triage_agent, input="Hola, ¬øc√≥mo est√°s?")
    print(result.final_output)
    # ¬°Hola! Estoy bien, gracias por preguntar. ¬øY t√∫, c√≥mo est√°s?


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Functions example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio

from agents import Agent, Runner, function_tool


@function_tool
def get_weather(city: str) -&amp;gt; str:
    return f"The weather in {city} is sunny."


agent = Agent(
    name="Hello world",
    instructions="You are a helpful agent.",
    tools=[get_weather],
)


async def main():
    result = await Runner.run(agent, input="What's the weather in Tokyo?")
    print(result.final_output)
    # The weather in Tokyo is sunny.


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;The agent loop&lt;/h2&gt; 
&lt;p&gt;When you call &lt;code&gt;Runner.run()&lt;/code&gt;, we run a loop until we get a final output.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;We call the LLM, using the model and settings on the agent, and the message history.&lt;/li&gt; 
 &lt;li&gt;The LLM returns a response, which may include tool calls.&lt;/li&gt; 
 &lt;li&gt;If the response has a final output (see below for more on this), we return it and end the loop.&lt;/li&gt; 
 &lt;li&gt;If the response has a handoff, we set the agent to the new agent and go back to step 1.&lt;/li&gt; 
 &lt;li&gt;We process the tool calls (if any) and append the tool responses messages. Then we go to step 1.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There is a &lt;code&gt;max_turns&lt;/code&gt; parameter that you can use to limit the number of times the loop executes.&lt;/p&gt; 
&lt;h3&gt;Final output&lt;/h3&gt; 
&lt;p&gt;Final output is the last thing the agent produces in the loop.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If you set an &lt;code&gt;output_type&lt;/code&gt; on the agent, the final output is when the LLM returns something of that type. We use &lt;a href="https://platform.openai.com/docs/guides/structured-outputs"&gt;structured outputs&lt;/a&gt; for this.&lt;/li&gt; 
 &lt;li&gt;If there's no &lt;code&gt;output_type&lt;/code&gt; (i.e. plain text responses), then the first LLM response without any tool calls or handoffs is considered as the final output.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;As a result, the mental model for the agent loop is:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If the current agent has an &lt;code&gt;output_type&lt;/code&gt;, the loop runs until the agent produces structured output matching that type.&lt;/li&gt; 
 &lt;li&gt;If the current agent does not have an &lt;code&gt;output_type&lt;/code&gt;, the loop runs until the current agent produces a message without any tool calls/handoffs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common agent patterns&lt;/h2&gt; 
&lt;p&gt;The Agents SDK is designed to be highly flexible, allowing you to model a wide range of LLM workflows including deterministic flows, iterative loops, and more. See examples in &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples/agent_patterns"&gt;&lt;code&gt;examples/agent_patterns&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tracing&lt;/h2&gt; 
&lt;p&gt;The Agents SDK automatically traces your agent runs, making it easy to track and debug the behavior of your agents. Tracing is extensible by design, supporting custom spans and a wide variety of external destinations, including &lt;a href="https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents"&gt;Logfire&lt;/a&gt;, &lt;a href="https://docs.agentops.ai/v1/integrations/agentssdk"&gt;AgentOps&lt;/a&gt;, &lt;a href="https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk"&gt;Braintrust&lt;/a&gt;, &lt;a href="https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration"&gt;Scorecard&lt;/a&gt;, and &lt;a href="https://docs.keywordsai.co/integration/development-frameworks/openai-agent"&gt;Keywords AI&lt;/a&gt;. For more details about how to customize or disable tracing, see &lt;a href="http://openai.github.io/openai-agents-python/tracing"&gt;Tracing&lt;/a&gt;, which also includes a larger list of &lt;a href="http://openai.github.io/openai-agents-python/tracing/#external-tracing-processors-list"&gt;external tracing processors&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Long running agents &amp;amp; human-in-the-loop&lt;/h2&gt; 
&lt;p&gt;You can use the Agents SDK &lt;a href="https://temporal.io/"&gt;Temporal&lt;/a&gt; integration to run durable, long-running workflows, including human-in-the-loop tasks. View a demo of Temporal and the Agents SDK working in action to complete long-running tasks &lt;a href="https://www.youtube.com/watch?v=fFBZqzT4DD8"&gt;in this video&lt;/a&gt;, and &lt;a href="https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents"&gt;view docs here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Sessions&lt;/h2&gt; 
&lt;p&gt;The Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle &lt;code&gt;.to_input_list()&lt;/code&gt; between turns.&lt;/p&gt; 
&lt;h3&gt;Quick start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner, SQLiteSession

# Create agent
agent = Agent(
    name="Assistant",
    instructions="Reply very concisely.",
)

# Create a session instance
session = SQLiteSession("conversation_123")

# First turn
result = await Runner.run(
    agent,
    "What city is the Golden Gate Bridge in?",
    session=session
)
print(result.final_output)  # "San Francisco"

# Second turn - agent automatically remembers previous context
result = await Runner.run(
    agent,
    "What state is it in?",
    session=session
)
print(result.final_output)  # "California"

# Also works with synchronous runner
result = Runner.run_sync(
    agent,
    "What's the population?",
    session=session
)
print(result.final_output)  # "Approximately 39 million"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Session options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No memory&lt;/strong&gt; (default): No session memory when session parameter is omitted&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;session: Session = DatabaseSession(...)&lt;/code&gt;&lt;/strong&gt;: Use a Session instance to manage conversation history&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner, SQLiteSession

# SQLite - file-based or in-memory database
session = SQLiteSession("user_123", "conversations.db")

# Redis - for scalable, distributed deployments
# from agents.extensions.memory import RedisSession
# session = RedisSession.from_url("user_123", url="redis://localhost:6379/0")

agent = Agent(name="Assistant")

# Different session IDs maintain separate conversation histories
result1 = await Runner.run(
    agent,
    "Hello",
    session=session
)
result2 = await Runner.run(
    agent,
    "Hello",
    session=SQLiteSession("user_456", "conversations.db")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Custom session implementations&lt;/h3&gt; 
&lt;p&gt;You can implement your own session memory by creating a class that follows the &lt;code&gt;Session&lt;/code&gt; protocol:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents.memory import Session
from typing import List

class MyCustomSession:
    """Custom session implementation following the Session protocol."""

    def __init__(self, session_id: str):
        self.session_id = session_id
        # Your initialization here

    async def get_items(self, limit: int | None = None) -&amp;gt; List[dict]:
        # Retrieve conversation history for the session
        pass

    async def add_items(self, items: List[dict]) -&amp;gt; None:
        # Store new items for the session
        pass

    async def pop_item(self) -&amp;gt; dict | None:
        # Remove and return the most recent item from the session
        pass

    async def clear_session(self) -&amp;gt; None:
        # Clear all items for the session
        pass

# Use your custom session
agent = Agent(name="Assistant")
result = await Runner.run(
    agent,
    "Hello",
    session=MyCustomSession("my_session")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development (only needed if you need to edit the SDK/examples)&lt;/h2&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt;Ensure you have &lt;a href="https://docs.astral.sh/uv/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;(After making changes) lint/test&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;make check # run tests linter and typechecker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to run them individually:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;make tests  # run tests
make mypy   # run typechecker
make lint   # run linter
make format-check # run style checker
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We'd like to acknowledge the excellent work of the open-source community, especially:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.pydantic.dev/latest/"&gt;Pydantic&lt;/a&gt; (data validation) and &lt;a href="https://ai.pydantic.dev/"&gt;PydanticAI&lt;/a&gt; (advanced agent framework)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt; (unified interface for 100+ LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/squidfunk/mkdocs-material"&gt;MkDocs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mkdocstrings/griffe"&gt;Griffe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; and &lt;a href="https://github.com/astral-sh/ruff"&gt;ruff&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We're committed to continuing to build the Agents SDK as an open source framework so others in the community can expand on our approach.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelcontextprotocol/python-sdk</title>
      <link>https://github.com/modelcontextprotocol/python-sdk</link>
      <description>&lt;p&gt;The official Python SDK for Model Context Protocol servers and clients&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Python SDK&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Python implementation of the Model Context Protocol (MCP)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/mcp.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/mcp.svg?sanitize=true" alt="MIT licensed" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/mcp.svg?sanitize=true" alt="Python Version" /&gt;&lt;/a&gt; &lt;a href="https://modelcontextprotocol.github.io/python-sdk/"&gt;&lt;img src="https://img.shields.io/badge/docs-python--sdk-blue.svg?sanitize=true" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://modelcontextprotocol.io"&gt;&lt;img src="https://img.shields.io/badge/protocol-modelcontextprotocol.io-blue.svg?sanitize=true" alt="Protocol" /&gt;&lt;/a&gt; &lt;a href="https://spec.modelcontextprotocol.io"&gt;&lt;img src="https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg?sanitize=true" alt="Specification" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#mcp-python-sdk"&gt;MCP Python SDK&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#overview"&gt;Overview&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#installation"&gt;Installation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#adding-mcp-to-your-python-project"&gt;Adding MCP to your python project&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#running-the-standalone-mcp-development-tools"&gt;Running the standalone MCP development tools&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#quickstart"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#what-is-mcp"&gt;What is MCP?&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#core-concepts"&gt;Core Concepts&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#server"&gt;Server&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#tools"&gt;Tools&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#structured-output"&gt;Structured Output&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#prompts"&gt;Prompts&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#images"&gt;Images&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#context"&gt;Context&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#getting-context-in-functions"&gt;Getting Context in Functions&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#context-properties-and-methods"&gt;Context Properties and Methods&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#completions"&gt;Completions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#elicitation"&gt;Elicitation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#sampling"&gt;Sampling&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#logging-and-notifications"&gt;Logging and Notifications&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#authentication"&gt;Authentication&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#fastmcp-properties"&gt;FastMCP Properties&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#session-properties-and-methods"&gt;Session Properties and Methods&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#request-context-properties"&gt;Request Context Properties&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#running-your-server"&gt;Running Your Server&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#development-mode"&gt;Development Mode&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#claude-desktop-integration"&gt;Claude Desktop Integration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#direct-execution"&gt;Direct Execution&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#streamable-http-transport"&gt;Streamable HTTP Transport&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#cors-configuration-for-browser-based-clients"&gt;CORS Configuration for Browser-Based Clients&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#mounting-to-an-existing-asgi-server"&gt;Mounting to an Existing ASGI Server&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#streamablehttp-servers"&gt;StreamableHTTP servers&lt;/a&gt; 
        &lt;ul&gt; 
         &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#basic-mounting"&gt;Basic mounting&lt;/a&gt;&lt;/li&gt; 
         &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#host-based-routing"&gt;Host-based routing&lt;/a&gt;&lt;/li&gt; 
         &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#multiple-servers-with-path-configuration"&gt;Multiple servers with path configuration&lt;/a&gt;&lt;/li&gt; 
         &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#path-configuration-at-initialization"&gt;Path configuration at initialization&lt;/a&gt;&lt;/li&gt; 
        &lt;/ul&gt; &lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#sse-servers"&gt;SSE servers&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#advanced-usage"&gt;Advanced Usage&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#low-level-server"&gt;Low-Level Server&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#structured-output-support"&gt;Structured Output Support&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#pagination-advanced"&gt;Pagination (Advanced)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#writing-mcp-clients"&gt;Writing MCP Clients&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#client-display-utilities"&gt;Client Display Utilities&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#oauth-authentication-for-clients"&gt;OAuth Authentication for Clients&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#parsing-tool-results"&gt;Parsing Tool Results&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#mcp-primitives"&gt;MCP Primitives&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#server-capabilities"&gt;Server Capabilities&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;The Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build MCP clients that can connect to any MCP server&lt;/li&gt; 
 &lt;li&gt;Create MCP servers that expose resources, prompts and tools&lt;/li&gt; 
 &lt;li&gt;Use standard transports like stdio, SSE, and Streamable HTTP&lt;/li&gt; 
 &lt;li&gt;Handle all MCP protocol messages and lifecycle events&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Adding MCP to your python project&lt;/h3&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage your Python projects.&lt;/p&gt; 
&lt;p&gt;If you haven't created a uv-managed project yet, create one:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init mcp-server-demo
cd mcp-server-demo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then add MCP to your project dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add "mcp[cli]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, for projects using pip for dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "mcp[cli]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running the standalone MCP development tools&lt;/h3&gt; 
&lt;p&gt;To run the mcp command with uv:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Let's create a simple MCP server that exposes a calculator tool and some data:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/fastmcp_quickstart.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
FastMCP quickstart example.

cd to the `examples/snippets/clients` directory and run:
    uv run server fastmcp_quickstart stdio
"""

from mcp.server.fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP("Demo")


# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&amp;gt; int:
    """Add two numbers"""
    return a + b


# Add a dynamic greeting resource
@mcp.resource("greeting://{name}")
def get_greeting(name: str) -&amp;gt; str:
    """Get a personalized greeting"""
    return f"Hello, {name}!"


# Add a prompt
@mcp.prompt()
def greet_user(name: str, style: str = "friendly") -&amp;gt; str:
    """Generate a greeting prompt"""
    styles = {
        "friendly": "Please write a warm, friendly greeting",
        "formal": "Please write a formal, professional greeting",
        "casual": "Please write a casual, relaxed greeting",
    }

    return f"{styles.get(style, styles['friendly'])} for someone named {name}."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/fastmcp_quickstart.py"&gt;examples/snippets/servers/fastmcp_quickstart.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;You can install this server in &lt;a href="https://claude.ai/download"&gt;Claude Desktop&lt;/a&gt; and interact with it right away by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp install server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can test it with the MCP Inspector:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp dev server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What is MCP?&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://modelcontextprotocol.io"&gt;Model Context Protocol (MCP)&lt;/a&gt; lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Expose data through &lt;strong&gt;Resources&lt;/strong&gt; (think of these sort of like GET endpoints; they are used to load information into the LLM's context)&lt;/li&gt; 
 &lt;li&gt;Provide functionality through &lt;strong&gt;Tools&lt;/strong&gt; (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)&lt;/li&gt; 
 &lt;li&gt;Define interaction patterns through &lt;strong&gt;Prompts&lt;/strong&gt; (reusable templates for LLM interactions)&lt;/li&gt; 
 &lt;li&gt;And more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Core Concepts&lt;/h2&gt; 
&lt;h3&gt;Server&lt;/h3&gt; 
&lt;p&gt;The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/lifespan_example.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""Example showing lifespan support for startup/shutdown with strong typing."""

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from dataclasses import dataclass

from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession


# Mock database class for example
class Database:
    """Mock database class for example."""

    @classmethod
    async def connect(cls) -&amp;gt; "Database":
        """Connect to database."""
        return cls()

    async def disconnect(self) -&amp;gt; None:
        """Disconnect from database."""
        pass

    def query(self) -&amp;gt; str:
        """Execute a query."""
        return "Query result"


@dataclass
class AppContext:
    """Application context with typed dependencies."""

    db: Database


@asynccontextmanager
async def app_lifespan(server: FastMCP) -&amp;gt; AsyncIterator[AppContext]:
    """Manage application lifecycle with type-safe context."""
    # Initialize on startup
    db = await Database.connect()
    try:
        yield AppContext(db=db)
    finally:
        # Cleanup on shutdown
        await db.disconnect()


# Pass lifespan to server
mcp = FastMCP("My App", lifespan=app_lifespan)


# Access type-safe lifespan context in tools
@mcp.tool()
def query_db(ctx: Context[ServerSession, AppContext]) -&amp;gt; str:
    """Tool that uses initialized resources."""
    db = ctx.request_context.lifespan_context.db
    return db.query()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lifespan_example.py"&gt;examples/snippets/servers/lifespan_example.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Resources&lt;/h3&gt; 
&lt;p&gt;Resources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/basic_resource.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name="Resource Example")


@mcp.resource("file://documents/{name}")
def read_document(name: str) -&amp;gt; str:
    """Read a document by name."""
    # This would normally read from disk
    return f"Content of {name}"


@mcp.resource("config://settings")
def get_settings() -&amp;gt; str:
    """Get application settings."""
    return """{
  "theme": "dark",
  "language": "en",
  "debug": false
}"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/basic_resource.py"&gt;examples/snippets/servers/basic_resource.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;p&gt;Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/basic_tool.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name="Tool Example")


@mcp.tool()
def sum(a: int, b: int) -&amp;gt; int:
    """Add two numbers together."""
    return a + b


@mcp.tool()
def get_weather(city: str, unit: str = "celsius") -&amp;gt; str:
    """Get weather for a city."""
    # This would normally call a weather API
    return f"Weather in {city}: 22degrees{unit[0].upper()}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/basic_tool.py"&gt;examples/snippets/servers/basic_tool.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Tools can optionally receive a Context object by including a parameter with the &lt;code&gt;Context&lt;/code&gt; type annotation. This context is automatically injected by the FastMCP framework and provides access to MCP capabilities:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/tool_progress.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name="Progress Example")


@mcp.tool()
async def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -&amp;gt; str:
    """Execute a task with progress updates."""
    await ctx.info(f"Starting: {task_name}")

    for i in range(steps):
        progress = (i + 1) / steps
        await ctx.report_progress(
            progress=progress,
            total=1.0,
            message=f"Step {i + 1}/{steps}",
        )
        await ctx.debug(f"Completed step {i + 1}")

    return f"Task '{task_name}' completed"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/tool_progress.py"&gt;examples/snippets/servers/tool_progress.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h4&gt;Structured Output&lt;/h4&gt; 
&lt;p&gt;Tools will return structured results by default, if their return type annotation is compatible. Otherwise, they will return unstructured results.&lt;/p&gt; 
&lt;p&gt;Structured output supports these return types:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pydantic models (BaseModel subclasses)&lt;/li&gt; 
 &lt;li&gt;TypedDicts&lt;/li&gt; 
 &lt;li&gt;Dataclasses and other classes with type hints&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dict[str, T]&lt;/code&gt; (where T is any JSON-serializable type)&lt;/li&gt; 
 &lt;li&gt;Primitive types (str, int, float, bool, bytes, None) - wrapped in &lt;code&gt;{"result": value}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Generic types (list, tuple, Union, Optional, etc.) - wrapped in &lt;code&gt;{"result": value}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Classes without type hints cannot be serialized for structured output. Only classes with properly annotated attributes will be converted to Pydantic models for schema generation and validation.&lt;/p&gt; 
&lt;p&gt;Structured results are automatically validated against the output schema generated from the annotation. This ensures the tool returns well-typed, validated data that clients can easily process.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For backward compatibility, unstructured results are also returned. Unstructured results are provided for backward compatibility with previous versions of the MCP specification, and are quirks-compatible with previous versions of FastMCP in the current version of the SDK.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In cases where a tool function's return type annotation causes the tool to be classified as structured &lt;em&gt;and this is undesirable&lt;/em&gt;, the classification can be suppressed by passing &lt;code&gt;structured_output=False&lt;/code&gt; to the &lt;code&gt;@tool&lt;/code&gt; decorator.&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/structured_output.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""Example showing structured output with tools."""

from typing import TypedDict

from pydantic import BaseModel, Field

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Structured Output Example")


# Using Pydantic models for rich structured data
class WeatherData(BaseModel):
    """Weather information structure."""

    temperature: float = Field(description="Temperature in Celsius")
    humidity: float = Field(description="Humidity percentage")
    condition: str
    wind_speed: float


@mcp.tool()
def get_weather(city: str) -&amp;gt; WeatherData:
    """Get weather for a city - returns structured data."""
    # Simulated weather data
    return WeatherData(
        temperature=22.5,
        humidity=45.0,
        condition="sunny",
        wind_speed=5.2,
    )


# Using TypedDict for simpler structures
class LocationInfo(TypedDict):
    latitude: float
    longitude: float
    name: str


@mcp.tool()
def get_location(address: str) -&amp;gt; LocationInfo:
    """Get location coordinates"""
    return LocationInfo(latitude=51.5074, longitude=-0.1278, name="London, UK")


# Using dict[str, Any] for flexible schemas
@mcp.tool()
def get_statistics(data_type: str) -&amp;gt; dict[str, float]:
    """Get various statistics"""
    return {"mean": 42.5, "median": 40.0, "std_dev": 5.2}


# Ordinary classes with type hints work for structured output
class UserProfile:
    name: str
    age: int
    email: str | None = None

    def __init__(self, name: str, age: int, email: str | None = None):
        self.name = name
        self.age = age
        self.email = email


@mcp.tool()
def get_user(user_id: str) -&amp;gt; UserProfile:
    """Get user profile - returns structured data"""
    return UserProfile(name="Alice", age=30, email="alice@example.com")


# Classes WITHOUT type hints cannot be used for structured output
class UntypedConfig:
    def __init__(self, setting1, setting2):  # type: ignore[reportMissingParameterType]
        self.setting1 = setting1
        self.setting2 = setting2


@mcp.tool()
def get_config() -&amp;gt; UntypedConfig:
    """This returns unstructured output - no schema generated"""
    return UntypedConfig("value1", "value2")


# Lists and other types are wrapped automatically
@mcp.tool()
def list_cities() -&amp;gt; list[str]:
    """Get a list of cities"""
    return ["London", "Paris", "Tokyo"]
    # Returns: {"result": ["London", "Paris", "Tokyo"]}


@mcp.tool()
def get_temperature(city: str) -&amp;gt; float:
    """Get temperature as a simple float"""
    return 22.5
    # Returns: {"result": 22.5}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/structured_output.py"&gt;examples/snippets/servers/structured_output.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Prompts&lt;/h3&gt; 
&lt;p&gt;Prompts are reusable templates that help LLMs interact with your server effectively:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/basic_prompt.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp.prompts import base

mcp = FastMCP(name="Prompt Example")


@mcp.prompt(title="Code Review")
def review_code(code: str) -&amp;gt; str:
    return f"Please review this code:\n\n{code}"


@mcp.prompt(title="Debug Assistant")
def debug_error(error: str) -&amp;gt; list[base.Message]:
    return [
        base.UserMessage("I'm seeing this error:"),
        base.UserMessage(error),
        base.AssistantMessage("I'll help debug that. What have you tried so far?"),
    ]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/basic_prompt.py"&gt;examples/snippets/servers/basic_prompt.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Icons&lt;/h3&gt; 
&lt;p&gt;MCP servers can provide icons for UI display. Icons can be added to the server implementation, tools, resources, and prompts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import FastMCP, Icon

# Create an icon from a file path or URL
icon = Icon(
    src="icon.png",
    mimeType="image/png",
    sizes="64x64"
)

# Add icons to server
mcp = FastMCP(
    "My Server",
    website_url="https://example.com",
    icons=[icon]
)

# Add icons to tools, resources, and prompts
@mcp.tool(icons=[icon])
def my_tool():
    """Tool with an icon."""
    return "result"

@mcp.resource("demo://resource", icons=[icon])
def my_resource():
    """Resource with an icon."""
    return "content"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/fastmcp/icons_demo.py"&gt;examples/fastmcp/icons_demo.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Images&lt;/h3&gt; 
&lt;p&gt;FastMCP provides an &lt;code&gt;Image&lt;/code&gt; class that automatically handles image data:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/images.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""Example showing image handling with FastMCP."""

from PIL import Image as PILImage

from mcp.server.fastmcp import FastMCP, Image

mcp = FastMCP("Image Example")


@mcp.tool()
def create_thumbnail(image_path: str) -&amp;gt; Image:
    """Create a thumbnail from an image"""
    img = PILImage.open(image_path)
    img.thumbnail((100, 100))
    return Image(data=img.tobytes(), format="png")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/images.py"&gt;examples/snippets/servers/images.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Context&lt;/h3&gt; 
&lt;p&gt;The Context object is automatically injected into tool and resource functions that request it via type hints. It provides access to MCP capabilities like logging, progress reporting, resource reading, user interaction, and request metadata.&lt;/p&gt; 
&lt;h4&gt;Getting Context in Functions&lt;/h4&gt; 
&lt;p&gt;To use context in a tool or resource function, add a parameter with the &lt;code&gt;Context&lt;/code&gt; type annotation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP

mcp = FastMCP(name="Context Example")


@mcp.tool()
async def my_tool(x: int, ctx: Context) -&amp;gt; str:
    """Tool that uses context capabilities."""
    # The context parameter can have any name as long as it's type-annotated
    return await process_with_context(x, ctx)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Context Properties and Methods&lt;/h4&gt; 
&lt;p&gt;The Context object provides the following capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_id&lt;/code&gt; - Unique ID for the current request&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.client_id&lt;/code&gt; - Client ID if available&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp&lt;/code&gt; - Access to the FastMCP server instance (see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#fastmcp-properties"&gt;FastMCP Properties&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.session&lt;/code&gt; - Access to the underlying session for advanced communication (see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#session-properties-and-methods"&gt;Session Properties and Methods&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context&lt;/code&gt; - Access to request-specific data and lifespan resources (see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/#request-context-properties"&gt;Request Context Properties&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.debug(message)&lt;/code&gt; - Send debug log message&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.info(message)&lt;/code&gt; - Send info log message&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.warning(message)&lt;/code&gt; - Send warning log message&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.error(message)&lt;/code&gt; - Send error log message&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.log(level, message, logger_name=None)&lt;/code&gt; - Send log with custom level&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.report_progress(progress, total=None, message=None)&lt;/code&gt; - Report operation progress&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.read_resource(uri)&lt;/code&gt; - Read a resource by URI&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.elicit(message, schema)&lt;/code&gt; - Request additional information from user with validation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- snippet-source examples/snippets/servers/tool_progress.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name="Progress Example")


@mcp.tool()
async def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -&amp;gt; str:
    """Execute a task with progress updates."""
    await ctx.info(f"Starting: {task_name}")

    for i in range(steps):
        progress = (i + 1) / steps
        await ctx.report_progress(
            progress=progress,
            total=1.0,
            message=f"Step {i + 1}/{steps}",
        )
        await ctx.debug(f"Completed step {i + 1}")

    return f"Task '{task_name}' completed"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/tool_progress.py"&gt;examples/snippets/servers/tool_progress.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Completions&lt;/h3&gt; 
&lt;p&gt;MCP supports providing completion suggestions for prompt arguments and resource template parameters. With the context parameter, servers can provide completions based on previously resolved values:&lt;/p&gt; 
&lt;p&gt;Client usage:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/completion_client.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
cd to the `examples/snippets` directory and run:
    uv run completion-client
"""

import asyncio
import os

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.types import PromptReference, ResourceTemplateReference

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="uv",  # Using uv to run the server
    args=["run", "server", "completion", "stdio"],  # Server with completion support
    env={"UV_INDEX": os.environ.get("UV_INDEX", "")},
)


async def run():
    """Run the completion client example."""
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Initialize the connection
            await session.initialize()

            # List available resource templates
            templates = await session.list_resource_templates()
            print("Available resource templates:")
            for template in templates.resourceTemplates:
                print(f"  - {template.uriTemplate}")

            # List available prompts
            prompts = await session.list_prompts()
            print("\nAvailable prompts:")
            for prompt in prompts.prompts:
                print(f"  - {prompt.name}")

            # Complete resource template arguments
            if templates.resourceTemplates:
                template = templates.resourceTemplates[0]
                print(f"\nCompleting arguments for resource template: {template.uriTemplate}")

                # Complete without context
                result = await session.complete(
                    ref=ResourceTemplateReference(type="ref/resource", uri=template.uriTemplate),
                    argument={"name": "owner", "value": "model"},
                )
                print(f"Completions for 'owner' starting with 'model': {result.completion.values}")

                # Complete with context - repo suggestions based on owner
                result = await session.complete(
                    ref=ResourceTemplateReference(type="ref/resource", uri=template.uriTemplate),
                    argument={"name": "repo", "value": ""},
                    context_arguments={"owner": "modelcontextprotocol"},
                )
                print(f"Completions for 'repo' with owner='modelcontextprotocol': {result.completion.values}")

            # Complete prompt arguments
            if prompts.prompts:
                prompt_name = prompts.prompts[0].name
                print(f"\nCompleting arguments for prompt: {prompt_name}")

                result = await session.complete(
                    ref=PromptReference(type="ref/prompt", name=prompt_name),
                    argument={"name": "style", "value": ""},
                )
                print(f"Completions for 'style' argument: {result.completion.values}")


def main():
    """Entry point for the completion client."""
    asyncio.run(run())


if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/completion_client.py"&gt;examples/snippets/clients/completion_client.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Elicitation&lt;/h3&gt; 
&lt;p&gt;Request additional information from users. This example shows an Elicitation during a Tool Call:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/elicitation.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pydantic import BaseModel, Field

from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name="Elicitation Example")


class BookingPreferences(BaseModel):
    """Schema for collecting user preferences."""

    checkAlternative: bool = Field(description="Would you like to check another date?")
    alternativeDate: str = Field(
        default="2024-12-26",
        description="Alternative date (YYYY-MM-DD)",
    )


@mcp.tool()
async def book_table(date: str, time: str, party_size: int, ctx: Context[ServerSession, None]) -&amp;gt; str:
    """Book a table with date availability check."""
    # Check if date is available
    if date == "2024-12-25":
        # Date unavailable - ask user for alternative
        result = await ctx.elicit(
            message=(f"No tables available for {party_size} on {date}. Would you like to try another date?"),
            schema=BookingPreferences,
        )

        if result.action == "accept" and result.data:
            if result.data.checkAlternative:
                return f"[SUCCESS] Booked for {result.data.alternativeDate}"
            return "[CANCELLED] No booking made"
        return "[CANCELLED] Booking cancelled"

    # Date available
    return f"[SUCCESS] Booked for {date} at {time}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/elicitation.py"&gt;examples/snippets/servers/elicitation.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Elicitation schemas support default values for all field types. Default values are automatically included in the JSON schema sent to clients, allowing them to pre-populate forms.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;elicit()&lt;/code&gt; method returns an &lt;code&gt;ElicitationResult&lt;/code&gt; with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;action&lt;/code&gt;: "accept", "decline", or "cancel"&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;data&lt;/code&gt;: The validated response (only when accepted)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;validation_error&lt;/code&gt;: Any validation error message&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Sampling&lt;/h3&gt; 
&lt;p&gt;Tools can interact with LLMs through sampling (generating text):&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/sampling.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession
from mcp.types import SamplingMessage, TextContent

mcp = FastMCP(name="Sampling Example")


@mcp.tool()
async def generate_poem(topic: str, ctx: Context[ServerSession, None]) -&amp;gt; str:
    """Generate a poem using LLM sampling."""
    prompt = f"Write a short poem about {topic}"

    result = await ctx.session.create_message(
        messages=[
            SamplingMessage(
                role="user",
                content=TextContent(type="text", text=prompt),
            )
        ],
        max_tokens=100,
    )

    if result.content.type == "text":
        return result.content.text
    return str(result.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/sampling.py"&gt;examples/snippets/servers/sampling.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Logging and Notifications&lt;/h3&gt; 
&lt;p&gt;Tools can send logs and notifications through the context:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/notifications.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name="Notifications Example")


@mcp.tool()
async def process_data(data: str, ctx: Context[ServerSession, None]) -&amp;gt; str:
    """Process data with logging."""
    # Different log levels
    await ctx.debug(f"Debug: Processing '{data}'")
    await ctx.info("Info: Starting processing")
    await ctx.warning("Warning: This is experimental")
    await ctx.error("Error: (This is just a demo)")

    # Notify about resource changes
    await ctx.session.send_resource_list_changed()

    return f"Processed: {data}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/notifications.py"&gt;examples/snippets/servers/notifications.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Authentication&lt;/h3&gt; 
&lt;p&gt;Authentication can be used by servers that want to expose tools accessing protected resources.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;mcp.server.auth&lt;/code&gt; implements OAuth 2.1 resource server functionality, where MCP servers act as Resource Servers (RS) that validate tokens issued by separate Authorization Servers (AS). This follows the &lt;a href="https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization"&gt;MCP authorization specification&lt;/a&gt; and implements RFC 9728 (Protected Resource Metadata) for AS discovery.&lt;/p&gt; 
&lt;p&gt;MCP servers can use authentication by providing an implementation of the &lt;code&gt;TokenVerifier&lt;/code&gt; protocol:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/oauth_server.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/servers/oauth_server.py
"""

from pydantic import AnyHttpUrl

from mcp.server.auth.provider import AccessToken, TokenVerifier
from mcp.server.auth.settings import AuthSettings
from mcp.server.fastmcp import FastMCP


class SimpleTokenVerifier(TokenVerifier):
    """Simple token verifier for demonstration."""

    async def verify_token(self, token: str) -&amp;gt; AccessToken | None:
        pass  # This is where you would implement actual token validation


# Create FastMCP instance as a Resource Server
mcp = FastMCP(
    "Weather Service",
    # Token verifier for authentication
    token_verifier=SimpleTokenVerifier(),
    # Auth settings for RFC 9728 Protected Resource Metadata
    auth=AuthSettings(
        issuer_url=AnyHttpUrl("https://auth.example.com"),  # Authorization Server URL
        resource_server_url=AnyHttpUrl("http://localhost:3001"),  # This server's URL
        required_scopes=["user"],
    ),
)


@mcp.tool()
async def get_weather(city: str = "London") -&amp;gt; dict[str, str]:
    """Get weather data for a city"""
    return {
        "city": city,
        "temperature": "22",
        "condition": "Partly cloudy",
        "humidity": "65%",
    }


if __name__ == "__main__":
    mcp.run(transport="streamable-http")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/oauth_server.py"&gt;examples/snippets/servers/oauth_server.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;For a complete example with separate Authorization Server and Resource Server implementations, see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/examples/servers/simple-auth/"&gt;&lt;code&gt;examples/servers/simple-auth/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Architecture:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Authorization Server (AS)&lt;/strong&gt;: Handles OAuth flows, user authentication, and token issuance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Resource Server (RS)&lt;/strong&gt;: Your MCP server that validates tokens and serves protected resources&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Client&lt;/strong&gt;: Discovers AS through RFC 9728, obtains tokens, and uses them with the MCP server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/src/mcp/server/auth/provider.py"&gt;TokenVerifier&lt;/a&gt; for more details on implementing token validation.&lt;/p&gt; 
&lt;h3&gt;FastMCP Properties&lt;/h3&gt; 
&lt;p&gt;The FastMCP server instance accessible via &lt;code&gt;ctx.fastmcp&lt;/code&gt; provides access to server configuration and metadata:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp.name&lt;/code&gt; - The server's name as defined during initialization&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp.instructions&lt;/code&gt; - Server instructions/description provided to clients&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp.website_url&lt;/code&gt; - Optional website URL for the server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp.icons&lt;/code&gt; - Optional list of icons for UI display&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.fastmcp.settings&lt;/code&gt; - Complete server configuration object containing: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;debug&lt;/code&gt; - Debug mode flag&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;log_level&lt;/code&gt; - Current logging level&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;host&lt;/code&gt; and &lt;code&gt;port&lt;/code&gt; - Server network configuration&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;mount_path&lt;/code&gt;, &lt;code&gt;sse_path&lt;/code&gt;, &lt;code&gt;streamable_http_path&lt;/code&gt; - Transport paths&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;stateless_http&lt;/code&gt; - Whether the server operates in stateless mode&lt;/li&gt; 
   &lt;li&gt;And other configuration options&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@mcp.tool()
def server_info(ctx: Context) -&amp;gt; dict:
    """Get information about the current server."""
    return {
        "name": ctx.fastmcp.name,
        "instructions": ctx.fastmcp.instructions,
        "debug_mode": ctx.fastmcp.settings.debug,
        "log_level": ctx.fastmcp.settings.log_level,
        "host": ctx.fastmcp.settings.host,
        "port": ctx.fastmcp.settings.port,
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Session Properties and Methods&lt;/h3&gt; 
&lt;p&gt;The session object accessible via &lt;code&gt;ctx.session&lt;/code&gt; provides advanced control over client communication:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ctx.session.client_params&lt;/code&gt; - Client initialization parameters and declared capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_log_message(level, data, logger)&lt;/code&gt; - Send log messages with full control&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.create_message(messages, max_tokens)&lt;/code&gt; - Request LLM sampling/completion&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_progress_notification(token, progress, total, message)&lt;/code&gt; - Direct progress updates&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_resource_updated(uri)&lt;/code&gt; - Notify clients that a specific resource changed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_resource_list_changed()&lt;/code&gt; - Notify clients that the resource list changed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_tool_list_changed()&lt;/code&gt; - Notify clients that the tool list changed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;await ctx.session.send_prompt_list_changed()&lt;/code&gt; - Notify clients that the prompt list changed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@mcp.tool()
async def notify_data_update(resource_uri: str, ctx: Context) -&amp;gt; str:
    """Update data and notify clients of the change."""
    # Perform data update logic here
    
    # Notify clients that this specific resource changed
    await ctx.session.send_resource_updated(AnyUrl(resource_uri))
    
    # If this affects the overall resource list, notify about that too
    await ctx.session.send_resource_list_changed()
    
    return f"Updated {resource_uri} and notified clients"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Request Context Properties&lt;/h3&gt; 
&lt;p&gt;The request context accessible via &lt;code&gt;ctx.request_context&lt;/code&gt; contains request-specific information and resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context.lifespan_context&lt;/code&gt; - Access to resources initialized during server startup 
  &lt;ul&gt; 
   &lt;li&gt;Database connections, configuration objects, shared services&lt;/li&gt; 
   &lt;li&gt;Type-safe access to resources defined in your server's lifespan function&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context.meta&lt;/code&gt; - Request metadata from the client including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;progressToken&lt;/code&gt; - Token for progress notifications&lt;/li&gt; 
   &lt;li&gt;Other client-provided metadata&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context.request&lt;/code&gt; - The original MCP request object for advanced processing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ctx.request_context.request_id&lt;/code&gt; - Unique identifier for this request&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Example with typed lifespan context
@dataclass
class AppContext:
    db: Database
    config: AppConfig

@mcp.tool()
def query_with_config(query: str, ctx: Context) -&amp;gt; str:
    """Execute a query using shared database and configuration."""
    # Access typed lifespan context
    app_ctx: AppContext = ctx.request_context.lifespan_context
    
    # Use shared resources
    connection = app_ctx.db
    settings = app_ctx.config
    
    # Execute query with configuration
    result = connection.execute(query, timeout=settings.query_timeout)
    return str(result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full lifespan example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lifespan_example.py"&gt;examples/snippets/servers/lifespan_example.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Running Your Server&lt;/h2&gt; 
&lt;h3&gt;Development Mode&lt;/h3&gt; 
&lt;p&gt;The fastest way to test and debug your server is with the MCP Inspector:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp dev server.py

# Add dependencies
uv run mcp dev server.py --with pandas --with numpy

# Mount local code
uv run mcp dev server.py --with-editable .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Claude Desktop Integration&lt;/h3&gt; 
&lt;p&gt;Once your server is ready, install it in Claude Desktop:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run mcp install server.py

# Custom name
uv run mcp install server.py --name "My Analytics Server"

# Environment variables
uv run mcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...
uv run mcp install server.py -f .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Direct Execution&lt;/h3&gt; 
&lt;p&gt;For advanced scenarios like custom deployments:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/direct_execution.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""Example showing direct execution of an MCP server.

This is the simplest way to run an MCP server directly.
cd to the `examples/snippets` directory and run:
    uv run direct-execution-server
    or
    python servers/direct_execution.py
"""

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("My App")


@mcp.tool()
def hello(name: str = "World") -&amp;gt; str:
    """Say hello to someone."""
    return f"Hello, {name}!"


def main():
    """Entry point for the direct execution server."""
    mcp.run()


if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/direct_execution.py"&gt;examples/snippets/servers/direct_execution.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Run it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python servers/direct_execution.py
# or
uv run mcp run servers/direct_execution.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that &lt;code&gt;uv run mcp run&lt;/code&gt; or &lt;code&gt;uv run mcp dev&lt;/code&gt; only supports server using FastMCP and not the low-level server variant.&lt;/p&gt; 
&lt;h3&gt;Streamable HTTP Transport&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Streamable HTTP transport is superseding SSE transport for production deployments.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!-- snippet-source examples/snippets/servers/streamable_config.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/servers/streamable_config.py
"""

from mcp.server.fastmcp import FastMCP

# Stateful server (maintains session state)
mcp = FastMCP("StatefulServer")

# Other configuration options:
# Stateless server (no session persistence)
# mcp = FastMCP("StatelessServer", stateless_http=True)

# Stateless server (no session persistence, no sse stream with supported client)
# mcp = FastMCP("StatelessServer", stateless_http=True, json_response=True)


# Add a simple tool to demonstrate the server
@mcp.tool()
def greet(name: str = "World") -&amp;gt; str:
    """Greet someone by name."""
    return f"Hello, {name}!"


# Run server with streamable_http transport
if __name__ == "__main__":
    mcp.run(transport="streamable-http")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/streamable_config.py"&gt;examples/snippets/servers/streamable_config.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;You can mount multiple FastMCP servers in a Starlette application:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/streamable_starlette_mount.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uvicorn examples.snippets.servers.streamable_starlette_mount:app --reload
"""

import contextlib

from starlette.applications import Starlette
from starlette.routing import Mount

from mcp.server.fastmcp import FastMCP

# Create the Echo server
echo_mcp = FastMCP(name="EchoServer", stateless_http=True)


@echo_mcp.tool()
def echo(message: str) -&amp;gt; str:
    """A simple echo tool"""
    return f"Echo: {message}"


# Create the Math server
math_mcp = FastMCP(name="MathServer", stateless_http=True)


@math_mcp.tool()
def add_two(n: int) -&amp;gt; int:
    """Tool to add two to the input"""
    return n + 2


# Create a combined lifespan to manage both session managers
@contextlib.asynccontextmanager
async def lifespan(app: Starlette):
    async with contextlib.AsyncExitStack() as stack:
        await stack.enter_async_context(echo_mcp.session_manager.run())
        await stack.enter_async_context(math_mcp.session_manager.run())
        yield


# Create the Starlette app and mount the MCP servers
app = Starlette(
    routes=[
        Mount("/echo", echo_mcp.streamable_http_app()),
        Mount("/math", math_mcp.streamable_http_app()),
    ],
    lifespan=lifespan,
)

# Note: Clients connect to http://localhost:8000/echo/mcp and http://localhost:8000/math/mcp
# To mount at the root of each path (e.g., /echo instead of /echo/mcp):
# echo_mcp.settings.streamable_http_path = "/"
# math_mcp.settings.streamable_http_path = "/"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/streamable_starlette_mount.py"&gt;examples/snippets/servers/streamable_starlette_mount.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;For low level server with Streamable HTTP implementations, see:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Stateful server: &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/examples/servers/simple-streamablehttp/"&gt;&lt;code&gt;examples/servers/simple-streamablehttp/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Stateless server: &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/examples/servers/simple-streamablehttp-stateless/"&gt;&lt;code&gt;examples/servers/simple-streamablehttp-stateless/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The streamable HTTP transport supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Stateful and stateless operation modes&lt;/li&gt; 
 &lt;li&gt;Resumability with event stores&lt;/li&gt; 
 &lt;li&gt;JSON or SSE response formats&lt;/li&gt; 
 &lt;li&gt;Better scalability for multi-node deployments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;CORS Configuration for Browser-Based Clients&lt;/h4&gt; 
&lt;p&gt;If you'd like your server to be accessible by browser-based MCP clients, you'll need to configure CORS headers. The &lt;code&gt;Mcp-Session-Id&lt;/code&gt; header must be exposed for browser clients to access it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from starlette.applications import Starlette
from starlette.middleware.cors import CORSMiddleware

# Create your Starlette app first
starlette_app = Starlette(routes=[...])

# Then wrap it with CORS middleware
starlette_app = CORSMiddleware(
    starlette_app,
    allow_origins=["*"],  # Configure appropriately for production
    allow_methods=["GET", "POST", "DELETE"],  # MCP streamable HTTP methods
    expose_headers=["Mcp-Session-Id"],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This configuration is necessary because:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The MCP streamable HTTP transport uses the &lt;code&gt;Mcp-Session-Id&lt;/code&gt; header for session management&lt;/li&gt; 
 &lt;li&gt;Browsers restrict access to response headers unless explicitly exposed via CORS&lt;/li&gt; 
 &lt;li&gt;Without this configuration, browser-based clients won't be able to read the session ID from initialization responses&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Mounting to an Existing ASGI Server&lt;/h3&gt; 
&lt;p&gt;By default, SSE servers are mounted at &lt;code&gt;/sse&lt;/code&gt; and Streamable HTTP servers are mounted at &lt;code&gt;/mcp&lt;/code&gt;. You can customize these paths using the methods described below.&lt;/p&gt; 
&lt;p&gt;For more information on mounting applications in Starlette, see the &lt;a href="https://www.starlette.io/routing/#submounting-routes"&gt;Starlette documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;StreamableHTTP servers&lt;/h4&gt; 
&lt;p&gt;You can mount the StreamableHTTP server to an existing ASGI server using the &lt;code&gt;streamable_http_app&lt;/code&gt; method. This allows you to integrate the StreamableHTTP server with other ASGI applications.&lt;/p&gt; 
&lt;h5&gt;Basic mounting&lt;/h5&gt; 
&lt;!-- snippet-source examples/snippets/servers/streamable_http_basic_mounting.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Basic example showing how to mount StreamableHTTP server in Starlette.

Run from the repository root:
    uvicorn examples.snippets.servers.streamable_http_basic_mounting:app --reload
"""

from starlette.applications import Starlette
from starlette.routing import Mount

from mcp.server.fastmcp import FastMCP

# Create MCP server
mcp = FastMCP("My App")


@mcp.tool()
def hello() -&amp;gt; str:
    """A simple hello tool"""
    return "Hello from MCP!"


# Mount the StreamableHTTP server to the existing ASGI server
app = Starlette(
    routes=[
        Mount("/", app=mcp.streamable_http_app()),
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/streamable_http_basic_mounting.py"&gt;examples/snippets/servers/streamable_http_basic_mounting.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h5&gt;Host-based routing&lt;/h5&gt; 
&lt;!-- snippet-source examples/snippets/servers/streamable_http_host_mounting.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Example showing how to mount StreamableHTTP server using Host-based routing.

Run from the repository root:
    uvicorn examples.snippets.servers.streamable_http_host_mounting:app --reload
"""

from starlette.applications import Starlette
from starlette.routing import Host

from mcp.server.fastmcp import FastMCP

# Create MCP server
mcp = FastMCP("MCP Host App")


@mcp.tool()
def domain_info() -&amp;gt; str:
    """Get domain-specific information"""
    return "This is served from mcp.acme.corp"


# Mount using Host-based routing
app = Starlette(
    routes=[
        Host("mcp.acme.corp", app=mcp.streamable_http_app()),
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/streamable_http_host_mounting.py"&gt;examples/snippets/servers/streamable_http_host_mounting.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h5&gt;Multiple servers with path configuration&lt;/h5&gt; 
&lt;!-- snippet-source examples/snippets/servers/streamable_http_multiple_servers.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Example showing how to mount multiple StreamableHTTP servers with path configuration.

Run from the repository root:
    uvicorn examples.snippets.servers.streamable_http_multiple_servers:app --reload
"""

from starlette.applications import Starlette
from starlette.routing import Mount

from mcp.server.fastmcp import FastMCP

# Create multiple MCP servers
api_mcp = FastMCP("API Server")
chat_mcp = FastMCP("Chat Server")


@api_mcp.tool()
def api_status() -&amp;gt; str:
    """Get API status"""
    return "API is running"


@chat_mcp.tool()
def send_message(message: str) -&amp;gt; str:
    """Send a chat message"""
    return f"Message sent: {message}"


# Configure servers to mount at the root of each path
# This means endpoints will be at /api and /chat instead of /api/mcp and /chat/mcp
api_mcp.settings.streamable_http_path = "/"
chat_mcp.settings.streamable_http_path = "/"

# Mount the servers
app = Starlette(
    routes=[
        Mount("/api", app=api_mcp.streamable_http_app()),
        Mount("/chat", app=chat_mcp.streamable_http_app()),
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/streamable_http_multiple_servers.py"&gt;examples/snippets/servers/streamable_http_multiple_servers.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h5&gt;Path configuration at initialization&lt;/h5&gt; 
&lt;!-- snippet-source examples/snippets/servers/streamable_http_path_config.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Example showing path configuration during FastMCP initialization.

Run from the repository root:
    uvicorn examples.snippets.servers.streamable_http_path_config:app --reload
"""

from starlette.applications import Starlette
from starlette.routing import Mount

from mcp.server.fastmcp import FastMCP

# Configure streamable_http_path during initialization
# This server will mount at the root of wherever it's mounted
mcp_at_root = FastMCP("My Server", streamable_http_path="/")


@mcp_at_root.tool()
def process_data(data: str) -&amp;gt; str:
    """Process some data"""
    return f"Processed: {data}"


# Mount at /process - endpoints will be at /process instead of /process/mcp
app = Starlette(
    routes=[
        Mount("/process", app=mcp_at_root.streamable_http_app()),
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/streamable_http_path_config.py"&gt;examples/snippets/servers/streamable_http_path_config.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h4&gt;SSE servers&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: SSE transport is being superseded by &lt;a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http"&gt;Streamable HTTP transport&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can mount the SSE server to an existing ASGI server using the &lt;code&gt;sse_app&lt;/code&gt; method. This allows you to integrate the SSE server with other ASGI applications.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from starlette.applications import Starlette
from starlette.routing import Mount, Host
from mcp.server.fastmcp import FastMCP


mcp = FastMCP("My App")

# Mount the SSE server to the existing ASGI server
app = Starlette(
    routes=[
        Mount('/', app=mcp.sse_app()),
    ]
)

# or dynamically mount as host
app.router.routes.append(Host('mcp.acme.corp', app=mcp.sse_app()))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When mounting multiple MCP servers under different paths, you can configure the mount path in several ways:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from starlette.applications import Starlette
from starlette.routing import Mount
from mcp.server.fastmcp import FastMCP

# Create multiple MCP servers
github_mcp = FastMCP("GitHub API")
browser_mcp = FastMCP("Browser")
curl_mcp = FastMCP("Curl")
search_mcp = FastMCP("Search")

# Method 1: Configure mount paths via settings (recommended for persistent configuration)
github_mcp.settings.mount_path = "/github"
browser_mcp.settings.mount_path = "/browser"

# Method 2: Pass mount path directly to sse_app (preferred for ad-hoc mounting)
# This approach doesn't modify the server's settings permanently

# Create Starlette app with multiple mounted servers
app = Starlette(
    routes=[
        # Using settings-based configuration
        Mount("/github", app=github_mcp.sse_app()),
        Mount("/browser", app=browser_mcp.sse_app()),
        # Using direct mount path parameter
        Mount("/curl", app=curl_mcp.sse_app("/curl")),
        Mount("/search", app=search_mcp.sse_app("/search")),
    ]
)

# Method 3: For direct execution, you can also pass the mount path to run()
if __name__ == "__main__":
    search_mcp.run(transport="sse", mount_path="/search")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information on mounting applications in Starlette, see the &lt;a href="https://www.starlette.io/routing/#submounting-routes"&gt;Starlette documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Low-Level Server&lt;/h3&gt; 
&lt;p&gt;For more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/lowlevel/lifespan.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/servers/lowlevel/lifespan.py
"""

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from typing import Any

import mcp.server.stdio
import mcp.types as types
from mcp.server.lowlevel import NotificationOptions, Server
from mcp.server.models import InitializationOptions


# Mock database class for example
class Database:
    """Mock database class for example."""

    @classmethod
    async def connect(cls) -&amp;gt; "Database":
        """Connect to database."""
        print("Database connected")
        return cls()

    async def disconnect(self) -&amp;gt; None:
        """Disconnect from database."""
        print("Database disconnected")

    async def query(self, query_str: str) -&amp;gt; list[dict[str, str]]:
        """Execute a query."""
        # Simulate database query
        return [{"id": "1", "name": "Example", "query": query_str}]


@asynccontextmanager
async def server_lifespan(_server: Server) -&amp;gt; AsyncIterator[dict[str, Any]]:
    """Manage server startup and shutdown lifecycle."""
    # Initialize resources on startup
    db = await Database.connect()
    try:
        yield {"db": db}
    finally:
        # Clean up on shutdown
        await db.disconnect()


# Pass lifespan to server
server = Server("example-server", lifespan=server_lifespan)


@server.list_tools()
async def handle_list_tools() -&amp;gt; list[types.Tool]:
    """List available tools."""
    return [
        types.Tool(
            name="query_db",
            description="Query the database",
            inputSchema={
                "type": "object",
                "properties": {"query": {"type": "string", "description": "SQL query to execute"}},
                "required": ["query"],
            },
        )
    ]


@server.call_tool()
async def query_db(name: str, arguments: dict[str, Any]) -&amp;gt; list[types.TextContent]:
    """Handle database query tool call."""
    if name != "query_db":
        raise ValueError(f"Unknown tool: {name}")

    # Access lifespan context
    ctx = server.request_context
    db = ctx.lifespan_context["db"]

    # Execute query
    results = await db.query(arguments["query"])

    return [types.TextContent(type="text", text=f"Query results: {results}")]


async def run():
    """Run the server with lifespan management."""
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="example-server",
                server_version="0.1.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )


if __name__ == "__main__":
    import asyncio

    asyncio.run(run())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lowlevel/lifespan.py"&gt;examples/snippets/servers/lowlevel/lifespan.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;The lifespan API provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A way to initialize resources when the server starts and clean them up when it stops&lt;/li&gt; 
 &lt;li&gt;Access to initialized resources through the request context in handlers&lt;/li&gt; 
 &lt;li&gt;Type-safe context passing between lifespan and request handlers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- snippet-source examples/snippets/servers/lowlevel/basic.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
uv run examples/snippets/servers/lowlevel/basic.py
"""

import asyncio

import mcp.server.stdio
import mcp.types as types
from mcp.server.lowlevel import NotificationOptions, Server
from mcp.server.models import InitializationOptions

# Create a server instance
server = Server("example-server")


@server.list_prompts()
async def handle_list_prompts() -&amp;gt; list[types.Prompt]:
    """List available prompts."""
    return [
        types.Prompt(
            name="example-prompt",
            description="An example prompt template",
            arguments=[types.PromptArgument(name="arg1", description="Example argument", required=True)],
        )
    ]


@server.get_prompt()
async def handle_get_prompt(name: str, arguments: dict[str, str] | None) -&amp;gt; types.GetPromptResult:
    """Get a specific prompt by name."""
    if name != "example-prompt":
        raise ValueError(f"Unknown prompt: {name}")

    arg1_value = (arguments or {}).get("arg1", "default")

    return types.GetPromptResult(
        description="Example prompt",
        messages=[
            types.PromptMessage(
                role="user",
                content=types.TextContent(type="text", text=f"Example prompt text with argument: {arg1_value}"),
            )
        ],
    )


async def run():
    """Run the basic low-level server."""
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="example",
                server_version="0.1.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )


if __name__ == "__main__":
    asyncio.run(run())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lowlevel/basic.py"&gt;examples/snippets/servers/lowlevel/basic.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Caution: The &lt;code&gt;uv run mcp run&lt;/code&gt; and &lt;code&gt;uv run mcp dev&lt;/code&gt; tool doesn't support low-level server.&lt;/p&gt; 
&lt;h4&gt;Structured Output Support&lt;/h4&gt; 
&lt;p&gt;The low-level server supports structured output for tools, allowing you to return both human-readable content and machine-readable structured data. Tools can define an &lt;code&gt;outputSchema&lt;/code&gt; to validate their structured output:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/servers/lowlevel/structured_output.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/servers/lowlevel/structured_output.py
"""

import asyncio
from typing import Any

import mcp.server.stdio
import mcp.types as types
from mcp.server.lowlevel import NotificationOptions, Server
from mcp.server.models import InitializationOptions

server = Server("example-server")


@server.list_tools()
async def list_tools() -&amp;gt; list[types.Tool]:
    """List available tools with structured output schemas."""
    return [
        types.Tool(
            name="get_weather",
            description="Get current weather for a city",
            inputSchema={
                "type": "object",
                "properties": {"city": {"type": "string", "description": "City name"}},
                "required": ["city"],
            },
            outputSchema={
                "type": "object",
                "properties": {
                    "temperature": {"type": "number", "description": "Temperature in Celsius"},
                    "condition": {"type": "string", "description": "Weather condition"},
                    "humidity": {"type": "number", "description": "Humidity percentage"},
                    "city": {"type": "string", "description": "City name"},
                },
                "required": ["temperature", "condition", "humidity", "city"],
            },
        )
    ]


@server.call_tool()
async def call_tool(name: str, arguments: dict[str, Any]) -&amp;gt; dict[str, Any]:
    """Handle tool calls with structured output."""
    if name == "get_weather":
        city = arguments["city"]

        # Simulated weather data - in production, call a weather API
        weather_data = {
            "temperature": 22.5,
            "condition": "partly cloudy",
            "humidity": 65,
            "city": city,  # Include the requested city
        }

        # low-level server will validate structured output against the tool's
        # output schema, and additionally serialize it into a TextContent block
        # for backwards compatibility with pre-2025-06-18 clients.
        return weather_data
    else:
        raise ValueError(f"Unknown tool: {name}")


async def run():
    """Run the structured output server."""
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="structured-output-example",
                server_version="0.1.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )


if __name__ == "__main__":
    asyncio.run(run())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/lowlevel/structured_output.py"&gt;examples/snippets/servers/lowlevel/structured_output.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Tools can return data in three ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Content only&lt;/strong&gt;: Return a list of content blocks (default behavior before spec revision 2025-06-18)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structured data only&lt;/strong&gt;: Return a dictionary that will be serialized to JSON (Introduced in spec revision 2025-06-18)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Both&lt;/strong&gt;: Return a tuple of (content, structured_data) preferred option to use for backwards compatibility&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;When an &lt;code&gt;outputSchema&lt;/code&gt; is defined, the server automatically validates the structured output against the schema. This ensures type safety and helps catch errors early.&lt;/p&gt; 
&lt;h3&gt;Pagination (Advanced)&lt;/h3&gt; 
&lt;p&gt;For servers that need to handle large datasets, the low-level server provides paginated versions of list operations. This is an optional optimization - most servers won't need pagination unless they're dealing with hundreds or thousands of items.&lt;/p&gt; 
&lt;h4&gt;Server-side Implementation&lt;/h4&gt; 
&lt;!-- snippet-source examples/snippets/servers/pagination_example.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Example of implementing pagination with MCP server decorators.
"""

from pydantic import AnyUrl

import mcp.types as types
from mcp.server.lowlevel import Server

# Initialize the server
server = Server("paginated-server")

# Sample data to paginate
ITEMS = [f"Item {i}" for i in range(1, 101)]  # 100 items


@server.list_resources()
async def list_resources_paginated(request: types.ListResourcesRequest) -&amp;gt; types.ListResourcesResult:
    """List resources with pagination support."""
    page_size = 10

    # Extract cursor from request params
    cursor = request.params.cursor if request.params is not None else None

    # Parse cursor to get offset
    start = 0 if cursor is None else int(cursor)
    end = start + page_size

    # Get page of resources
    page_items = [
        types.Resource(uri=AnyUrl(f"resource://items/{item}"), name=item, description=f"Description for {item}")
        for item in ITEMS[start:end]
    ]

    # Determine next cursor
    next_cursor = str(end) if end &amp;lt; len(ITEMS) else None

    return types.ListResourcesResult(resources=page_items, nextCursor=next_cursor)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/servers/pagination_example.py"&gt;examples/snippets/servers/pagination_example.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h4&gt;Client-side Consumption&lt;/h4&gt; 
&lt;!-- snippet-source examples/snippets/clients/pagination_client.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Example of consuming paginated MCP endpoints from a client.
"""

import asyncio

from mcp.client.session import ClientSession
from mcp.client.stdio import StdioServerParameters, stdio_client
from mcp.types import Resource


async def list_all_resources() -&amp;gt; None:
    """Fetch all resources using pagination."""
    async with stdio_client(StdioServerParameters(command="uv", args=["run", "mcp-simple-pagination"])) as (
        read,
        write,
    ):
        async with ClientSession(read, write) as session:
            await session.initialize()

            all_resources: list[Resource] = []
            cursor = None

            while True:
                # Fetch a page of resources
                result = await session.list_resources(cursor=cursor)
                all_resources.extend(result.resources)

                print(f"Fetched {len(result.resources)} resources")

                # Check if there are more pages
                if result.nextCursor:
                    cursor = result.nextCursor
                else:
                    break

            print(f"Total resources: {len(all_resources)}")


if __name__ == "__main__":
    asyncio.run(list_all_resources())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/pagination_client.py"&gt;examples/snippets/clients/pagination_client.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h4&gt;Key Points&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cursors are opaque strings&lt;/strong&gt; - the server defines the format (numeric offsets, timestamps, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Return &lt;code&gt;nextCursor=None&lt;/code&gt;&lt;/strong&gt; when there are no more pages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward compatible&lt;/strong&gt; - clients that don't support pagination will still work (they'll just get the first page)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible page sizes&lt;/strong&gt; - Each endpoint can define its own page size based on data characteristics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/examples/servers/simple-pagination"&gt;simple-pagination example&lt;/a&gt; for a complete implementation.&lt;/p&gt; 
&lt;h3&gt;Writing MCP Clients&lt;/h3&gt; 
&lt;p&gt;The SDK provides a high-level client interface for connecting to MCP servers using various &lt;a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/transports"&gt;transports&lt;/a&gt;:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/stdio_client.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
cd to the `examples/snippets/clients` directory and run:
    uv run client
"""

import asyncio
import os

from pydantic import AnyUrl

from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client
from mcp.shared.context import RequestContext

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="uv",  # Using uv to run the server
    args=["run", "server", "fastmcp_quickstart", "stdio"],  # We're already in snippets dir
    env={"UV_INDEX": os.environ.get("UV_INDEX", "")},
)


# Optional: create a sampling callback
async def handle_sampling_message(
    context: RequestContext[ClientSession, None], params: types.CreateMessageRequestParams
) -&amp;gt; types.CreateMessageResult:
    print(f"Sampling request: {params.messages}")
    return types.CreateMessageResult(
        role="assistant",
        content=types.TextContent(
            type="text",
            text="Hello, world! from model",
        ),
        model="gpt-3.5-turbo",
        stopReason="endTurn",
    )


async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write, sampling_callback=handle_sampling_message) as session:
            # Initialize the connection
            await session.initialize()

            # List available prompts
            prompts = await session.list_prompts()
            print(f"Available prompts: {[p.name for p in prompts.prompts]}")

            # Get a prompt (greet_user prompt from fastmcp_quickstart)
            if prompts.prompts:
                prompt = await session.get_prompt("greet_user", arguments={"name": "Alice", "style": "friendly"})
                print(f"Prompt result: {prompt.messages[0].content}")

            # List available resources
            resources = await session.list_resources()
            print(f"Available resources: {[r.uri for r in resources.resources]}")

            # List available tools
            tools = await session.list_tools()
            print(f"Available tools: {[t.name for t in tools.tools]}")

            # Read a resource (greeting resource from fastmcp_quickstart)
            resource_content = await session.read_resource(AnyUrl("greeting://World"))
            content_block = resource_content.contents[0]
            if isinstance(content_block, types.TextContent):
                print(f"Resource content: {content_block.text}")

            # Call a tool (add tool from fastmcp_quickstart)
            result = await session.call_tool("add", arguments={"a": 5, "b": 3})
            result_unstructured = result.content[0]
            if isinstance(result_unstructured, types.TextContent):
                print(f"Tool result: {result_unstructured.text}")
            result_structured = result.structuredContent
            print(f"Structured tool result: {result_structured}")


def main():
    """Entry point for the client script."""
    asyncio.run(run())


if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/stdio_client.py"&gt;examples/snippets/clients/stdio_client.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;Clients can also connect using &lt;a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http"&gt;Streamable HTTP transport&lt;/a&gt;:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/streamable_basic.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Run from the repository root:
    uv run examples/snippets/clients/streamable_basic.py
"""

import asyncio

from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client


async def main():
    # Connect to a streamable HTTP server
    async with streamablehttp_client("http://localhost:8000/mcp") as (
        read_stream,
        write_stream,
        _,
    ):
        # Create a session using the client streams
        async with ClientSession(read_stream, write_stream) as session:
            # Initialize the connection
            await session.initialize()
            # List available tools
            tools = await session.list_tools()
            print(f"Available tools: {[tool.name for tool in tools.tools]}")


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/streamable_basic.py"&gt;examples/snippets/clients/streamable_basic.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;h3&gt;Client Display Utilities&lt;/h3&gt; 
&lt;p&gt;When building MCP clients, the SDK provides utilities to help display human-readable names for tools, resources, and prompts:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/display_utilities.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
cd to the `examples/snippets` directory and run:
    uv run display-utilities-client
"""

import asyncio
import os

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.shared.metadata_utils import get_display_name

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="uv",  # Using uv to run the server
    args=["run", "server", "fastmcp_quickstart", "stdio"],
    env={"UV_INDEX": os.environ.get("UV_INDEX", "")},
)


async def display_tools(session: ClientSession):
    """Display available tools with human-readable names"""
    tools_response = await session.list_tools()

    for tool in tools_response.tools:
        # get_display_name() returns the title if available, otherwise the name
        display_name = get_display_name(tool)
        print(f"Tool: {display_name}")
        if tool.description:
            print(f"   {tool.description}")


async def display_resources(session: ClientSession):
    """Display available resources with human-readable names"""
    resources_response = await session.list_resources()

    for resource in resources_response.resources:
        display_name = get_display_name(resource)
        print(f"Resource: {display_name} ({resource.uri})")

    templates_response = await session.list_resource_templates()
    for template in templates_response.resourceTemplates:
        display_name = get_display_name(template)
        print(f"Resource Template: {display_name}")


async def run():
    """Run the display utilities example."""
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Initialize the connection
            await session.initialize()

            print("=== Available Tools ===")
            await display_tools(session)

            print("\n=== Available Resources ===")
            await display_resources(session)


def main():
    """Entry point for the display utilities client."""
    asyncio.run(run())


if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/display_utilities.py"&gt;examples/snippets/clients/display_utilities.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;The &lt;code&gt;get_display_name()&lt;/code&gt; function implements the proper precedence rules for displaying names:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For tools: &lt;code&gt;title&lt;/code&gt; &amp;gt; &lt;code&gt;annotations.title&lt;/code&gt; &amp;gt; &lt;code&gt;name&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For other objects: &lt;code&gt;title&lt;/code&gt; &amp;gt; &lt;code&gt;name&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This ensures your client UI shows the most user-friendly names that servers provide.&lt;/p&gt; 
&lt;h3&gt;OAuth Authentication for Clients&lt;/h3&gt; 
&lt;p&gt;The SDK includes &lt;a href="https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization"&gt;authorization support&lt;/a&gt; for connecting to protected MCP servers:&lt;/p&gt; 
&lt;!-- snippet-source examples/snippets/clients/oauth_client.py --&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""
Before running, specify running MCP RS server URL.
To spin up RS server locally, see
    examples/servers/simple-auth/README.md

cd to the `examples/snippets` directory and run:
    uv run oauth-client
"""

import asyncio
from urllib.parse import parse_qs, urlparse

from pydantic import AnyUrl

from mcp import ClientSession
from mcp.client.auth import OAuthClientProvider, TokenStorage
from mcp.client.streamable_http import streamablehttp_client
from mcp.shared.auth import OAuthClientInformationFull, OAuthClientMetadata, OAuthToken


class InMemoryTokenStorage(TokenStorage):
    """Demo In-memory token storage implementation."""

    def __init__(self):
        self.tokens: OAuthToken | None = None
        self.client_info: OAuthClientInformationFull | None = None

    async def get_tokens(self) -&amp;gt; OAuthToken | None:
        """Get stored tokens."""
        return self.tokens

    async def set_tokens(self, tokens: OAuthToken) -&amp;gt; None:
        """Store tokens."""
        self.tokens = tokens

    async def get_client_info(self) -&amp;gt; OAuthClientInformationFull | None:
        """Get stored client information."""
        return self.client_info

    async def set_client_info(self, client_info: OAuthClientInformationFull) -&amp;gt; None:
        """Store client information."""
        self.client_info = client_info


async def handle_redirect(auth_url: str) -&amp;gt; None:
    print(f"Visit: {auth_url}")


async def handle_callback() -&amp;gt; tuple[str, str | None]:
    callback_url = input("Paste callback URL: ")
    params = parse_qs(urlparse(callback_url).query)
    return params["code"][0], params.get("state", [None])[0]


async def main():
    """Run the OAuth client example."""
    oauth_auth = OAuthClientProvider(
        server_url="http://localhost:8001",
        client_metadata=OAuthClientMetadata(
            client_name="Example MCP Client",
            redirect_uris=[AnyUrl("http://localhost:3000/callback")],
            grant_types=["authorization_code", "refresh_token"],
            response_types=["code"],
            scope="user",
        ),
        storage=InMemoryTokenStorage(),
        redirect_handler=handle_redirect,
        callback_handler=handle_callback,
    )

    async with streamablehttp_client("http://localhost:8001/mcp", auth=oauth_auth) as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()

            tools = await session.list_tools()
            print(f"Available tools: {[tool.name for tool in tools.tools]}")

            resources = await session.list_resources()
            print(f"Available resources: {[r.uri for r in resources.resources]}")


def run():
    asyncio.run(main())


if __name__ == "__main__":
    run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Full example: &lt;a href="https://github.com/modelcontextprotocol/python-sdk/raw/main/examples/snippets/clients/oauth_client.py"&gt;examples/snippets/clients/oauth_client.py&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- /snippet-source --&gt; 
&lt;p&gt;For a complete working example, see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/examples/clients/simple-auth-client/"&gt;&lt;code&gt;examples/clients/simple-auth-client/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Parsing Tool Results&lt;/h3&gt; 
&lt;p&gt;When calling tools through MCP, the &lt;code&gt;CallToolResult&lt;/code&gt; object contains the tool's response in a structured format. Understanding how to parse this result is essential for properly handling tool outputs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;"""examples/snippets/clients/parsing_tool_results.py"""

import asyncio

from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client


async def parse_tool_results():
    """Demonstrates how to parse different types of content in CallToolResult."""
    server_params = StdioServerParameters(
        command="python", args=["path/to/mcp_server.py"]
    )

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # Example 1: Parsing text content
            result = await session.call_tool("get_data", {"format": "text"})
            for content in result.content:
                if isinstance(content, types.TextContent):
                    print(f"Text: {content.text}")

            # Example 2: Parsing structured content from JSON tools
            result = await session.call_tool("get_user", {"id": "123"})
            if hasattr(result, "structuredContent") and result.structuredContent:
                # Access structured data directly
                user_data = result.structuredContent
                print(f"User: {user_data.get('name')}, Age: {user_data.get('age')}")

            # Example 3: Parsing embedded resources
            result = await session.call_tool("read_config", {})
            for content in result.content:
                if isinstance(content, types.EmbeddedResource):
                    resource = content.resource
                    if isinstance(resource, types.TextResourceContents):
                        print(f"Config from {resource.uri}: {resource.text}")
                    elif isinstance(resource, types.BlobResourceContents):
                        print(f"Binary data from {resource.uri}")

            # Example 4: Parsing image content
            result = await session.call_tool("generate_chart", {"data": [1, 2, 3]})
            for content in result.content:
                if isinstance(content, types.ImageContent):
                    print(f"Image ({content.mimeType}): {len(content.data)} bytes")

            # Example 5: Handling errors
            result = await session.call_tool("failing_tool", {})
            if result.isError:
                print("Tool execution failed!")
                for content in result.content:
                    if isinstance(content, types.TextContent):
                        print(f"Error: {content.text}")


async def main():
    await parse_tool_results()


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP Primitives&lt;/h3&gt; 
&lt;p&gt;The MCP protocol defines three core primitives that servers can implement:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Primitive&lt;/th&gt; 
   &lt;th&gt;Control&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Example Use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Prompts&lt;/td&gt; 
   &lt;td&gt;User-controlled&lt;/td&gt; 
   &lt;td&gt;Interactive templates invoked by user choice&lt;/td&gt; 
   &lt;td&gt;Slash commands, menu options&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Resources&lt;/td&gt; 
   &lt;td&gt;Application-controlled&lt;/td&gt; 
   &lt;td&gt;Contextual data managed by the client application&lt;/td&gt; 
   &lt;td&gt;File contents, API responses&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tools&lt;/td&gt; 
   &lt;td&gt;Model-controlled&lt;/td&gt; 
   &lt;td&gt;Functions exposed to the LLM to take actions&lt;/td&gt; 
   &lt;td&gt;API calls, data updates&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Server Capabilities&lt;/h3&gt; 
&lt;p&gt;MCP servers declare capabilities during initialization:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Capability&lt;/th&gt; 
   &lt;th&gt;Feature Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;prompts&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;listChanged&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Prompt template management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;resources&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;subscribe&lt;/code&gt;&lt;br /&gt;&lt;code&gt;listChanged&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Resource exposure and updates&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tools&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;listChanged&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Tool discovery and execution&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Server logging configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;completions&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Argument completion suggestions&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://modelcontextprotocol.github.io/python-sdk/api/"&gt;API Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://modelcontextprotocol.io"&gt;Model Context Protocol documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://spec.modelcontextprotocol.io"&gt;Model Context Protocol specification&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelcontextprotocol/servers"&gt;Officially supported servers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>psf/black</title>
      <link>https://github.com/psf/black</link>
      <description>&lt;p&gt;The uncompromising Python code formatter&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://black.readthedocs.io/en/stable/"&gt;&lt;img src="https://raw.githubusercontent.com/psf/black/main/docs/_static/logo2-readme.png" alt="Black Logo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2 align="center"&gt;The Uncompromising Code Formatter&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/psf/black/actions"&gt;&lt;img alt="Actions Status" src="https://github.com/psf/black/workflows/Test/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://black.readthedocs.io/en/stable/?badge=stable"&gt;&lt;img alt="Documentation Status" src="https://readthedocs.org/projects/black/badge/?version=stable" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/psf/black?branch=main"&gt;&lt;img alt="Coverage Status" src="https://coveralls.io/repos/github/psf/black/badge.svg?branch=main" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black/raw/main/LICENSE"&gt;&lt;img alt="License: MIT" src="https://black.readthedocs.io/en/stable/_static/license.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/black/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/black" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/black"&gt;&lt;img alt="Downloads" src="https://static.pepy.tech/badge/black" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/black/"&gt;&lt;img alt="conda-forge" src="https://img.shields.io/conda/dn/conda-forge/black.svg?label=conda-forge" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄúAny color you like.‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return, &lt;em&gt;Black&lt;/em&gt; gives you speed, determinism, and freedom from &lt;code&gt;pycodestyle&lt;/code&gt; nagging about formatting. You will save time and mental energy for more important matters.&lt;/p&gt; 
&lt;p&gt;Blackened code looks the same regardless of the project you're reading. Formatting becomes transparent after a while and you can focus on the content instead.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; makes code review faster by producing the smallest diffs possible.&lt;/p&gt; 
&lt;p&gt;Try it out now using the &lt;a href="https://black.vercel.app"&gt;Black Playground&lt;/a&gt;. Watch the &lt;a href="https://youtu.be/esZLCuWs_2Y"&gt;PyCon 2019 talk&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://black.readthedocs.io/en/stable"&gt;Read the documentation on ReadTheDocs!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation and usage&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; can be installed by running &lt;code&gt;pip install black&lt;/code&gt;. It requires Python 3.9+ to run. If you want to format Jupyter Notebooks, install with &lt;code&gt;pip install "black[jupyter]"&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you can't wait for the latest &lt;em&gt;hotness&lt;/em&gt; and want to install from GitHub, use:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install git+https://github.com/psf/black&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;To get started right away with sensible defaults:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;black {source_file_or_directory}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can run &lt;em&gt;Black&lt;/em&gt; as a package if running it as a script doesn't work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python -m black {source_file_or_directory}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Further information can be found in our docs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/usage_and_configuration/index.html"&gt;Usage and Configuration&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is already &lt;a href="https://github.com/psf/black#used-by"&gt;successfully used&lt;/a&gt; by many projects, small and big. &lt;em&gt;Black&lt;/em&gt; has a comprehensive test suite, with efficient parallel tests, and our own auto formatting and parallel Continuous Integration runner. Now that we have become stable, you should not expect large formatting changes in the future. Stylistic changes will mostly be responses to bug reports and support for new Python syntax. For more information please refer to &lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/index.html"&gt;The Black Code Style&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Also, as a safety measure which slows down processing, &lt;em&gt;Black&lt;/em&gt; will check that the reformatted code still produces a valid AST that is effectively equivalent to the original (see the &lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#ast-before-and-after-formatting"&gt;Pragmatism&lt;/a&gt; section for details). If you're feeling confident, use &lt;code&gt;--fast&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;The &lt;em&gt;Black&lt;/em&gt; code style&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is a PEP 8 compliant opinionated formatter. &lt;em&gt;Black&lt;/em&gt; reformats entire files in place. Style configuration options are deliberately limited and rarely added. It doesn't take previous formatting into account (see &lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism"&gt;Pragmatism&lt;/a&gt; for exceptions).&lt;/p&gt; 
&lt;p&gt;Our documentation covers the current &lt;em&gt;Black&lt;/em&gt; code style, but planned changes to it are also documented. They're both worth taking a look at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html"&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Current style&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/future_style.html"&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Future style&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Changes to the &lt;em&gt;Black&lt;/em&gt; code style are bound by the Stability Policy:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/index.html#stability-policy"&gt;The &lt;em&gt;Black&lt;/em&gt; Code Style: Stability Policy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please refer to this document before submitting an issue. What seems like a bug might be intended behaviour.&lt;/p&gt; 
&lt;h3&gt;Pragmatism&lt;/h3&gt; 
&lt;p&gt;Early versions of &lt;em&gt;Black&lt;/em&gt; used to be absolutist in some respects. They took after its initial author. This was fine at the time as it made the implementation simpler and there were not many users anyway. Not many edge cases were reported. As a mature tool, &lt;em&gt;Black&lt;/em&gt; does make some exceptions to rules it otherwise holds.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism"&gt;The &lt;em&gt;Black&lt;/em&gt; code style: Pragmatism&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please refer to this document before submitting an issue just like with the document above. What seems like a bug might be intended behaviour.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is able to read project-specific default values for its command line options from a &lt;code&gt;pyproject.toml&lt;/code&gt; file. This is especially useful for specifying custom &lt;code&gt;--include&lt;/code&gt; and &lt;code&gt;--exclude&lt;/code&gt;/&lt;code&gt;--force-exclude&lt;/code&gt;/&lt;code&gt;--extend-exclude&lt;/code&gt; patterns for your project.&lt;/p&gt; 
&lt;p&gt;You can find more details in our documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/usage_and_configuration/the_basics.html#configuration-via-a-file"&gt;The basics: Configuration via a file&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And if you're looking for more general configuration documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/stable/usage_and_configuration/index.html"&gt;Usage and Configuration&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Pro-tip&lt;/strong&gt;: If you're asking yourself "Do I need to configure anything?" the answer is "No". &lt;em&gt;Black&lt;/em&gt; is all about sensible defaults. Applying those defaults will have your code in compliance with many other &lt;em&gt;Black&lt;/em&gt; formatted projects.&lt;/p&gt; 
&lt;h2&gt;Used by&lt;/h2&gt; 
&lt;p&gt;The following notable open-source projects trust &lt;em&gt;Black&lt;/em&gt; with enforcing a consistent code style: pytest, tox, Pyramid, Django, Django Channels, Hypothesis, attrs, SQLAlchemy, Poetry, PyPA applications (Warehouse, Bandersnatch, Pipenv, virtualenv), pandas, Pillow, Twisted, LocalStack, every Datadog Agent Integration, Home Assistant, Zulip, Kedro, OpenOA, FLORIS, ORBIT, WOMBAT, and many more.&lt;/p&gt; 
&lt;p&gt;The following organizations use &lt;em&gt;Black&lt;/em&gt;: Dropbox, KeepTruckin, Lyft, Mozilla, Quora, Duolingo, QuantumBlack, Tesla, Archer Aviation.&lt;/p&gt; 
&lt;p&gt;Are we missing anyone? Let us know.&lt;/p&gt; 
&lt;h2&gt;Testimonials&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Mike Bayer&lt;/strong&gt;, &lt;a href="https://www.sqlalchemy.org/"&gt;author of &lt;code&gt;SQLAlchemy&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;I can't think of any single tool in my entire programming career that has given me a bigger productivity increase by its introduction. I can now do refactorings in about 1% of the keystrokes that it would have taken me previously when we had no way for code to format itself.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Dusty Phillips&lt;/strong&gt;, &lt;a href="https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;amp;field-keywords=dusty+phillips"&gt;writer&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Black&lt;/em&gt; is opinionated so you don't have to be.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Hynek Schlawack&lt;/strong&gt;, &lt;a href="https://www.attrs.org/"&gt;creator of &lt;code&gt;attrs&lt;/code&gt;&lt;/a&gt;, core developer of Twisted and CPython:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An auto-formatter that doesn't suck is all I want for Xmas!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Carl Meyer&lt;/strong&gt;, &lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; core developer:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;At least the name is good.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Kenneth Reitz&lt;/strong&gt;, creator of &lt;a href="https://requests.readthedocs.io/en/latest/"&gt;&lt;code&gt;requests&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://readthedocs.org/projects/pipenv/"&gt;&lt;code&gt;pipenv&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This vastly improves the formatting of our code. Thanks a ton!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Show your style&lt;/h2&gt; 
&lt;p&gt;Use the badge in your project's README.md:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-md"&gt;[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Using the badge in README.rst:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looks like this: &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Welcome! Happy to see you willing to make the project better. You can get started by reading this:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/latest/contributing/the_basics.html"&gt;Contributing: The basics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also take a look at the rest of the contributing docs or talk with the developers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://black.readthedocs.io/en/latest/contributing/index.html"&gt;Contributing documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/RtVdv86PrH"&gt;Chat on Discord&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Change log&lt;/h2&gt; 
&lt;p&gt;The log has become rather long. It moved to its own file.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://black.readthedocs.io/en/latest/change_log.html"&gt;CHANGES&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;The author list is quite long nowadays, so it lives in its own file.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/psf/black/main/AUTHORS.md"&gt;AUTHORS.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;Everyone participating in the &lt;em&gt;Black&lt;/em&gt; project, and in particular in the issue tracker, pull requests, and social media activity, is expected to treat other people with respect and more generally to follow the guidelines articulated in the &lt;a href="https://www.python.org/psf/codeofconduct/"&gt;Python Community Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;At the same time, humor is encouraged. In fact, basic familiarity with Monty Python's Flying Circus is expected. We are not savages.&lt;/p&gt; 
&lt;p&gt;And if you &lt;em&gt;really&lt;/em&gt; need to slap somebody, do it with a fish while dancing.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>simular-ai/Agent-S</title>
      <link>https://github.com/simular-ai/Agent-S</link>
      <description>&lt;p&gt;Agent S: an open agentic framework that uses computers like a human&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s.png" alt="Logo" style="vertical-align:middle" width="60" /&gt; Agent S: &lt;small&gt;Use Computer Like a Human&lt;/small&gt; &lt;/h1&gt; 
&lt;p align="center"&gt;&amp;nbsp; üåê &lt;a href="https://www.simular.ai/articles/agent-s3"&gt;[S3 blog]&lt;/a&gt;&amp;nbsp; üìÑ &lt;a href="https://arxiv.org/abs/2510.02250"&gt;[S3 Paper]&lt;/a&gt;&amp;nbsp; üé• &lt;a href="https://www.youtube.com/watch?v=VHr0a3UBsh4"&gt;[S3 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&amp;nbsp; üåê &lt;a href="https://www.simular.ai/articles/agent-s2-technical-review"&gt;[S2 blog]&lt;/a&gt;&amp;nbsp; üìÑ &lt;a href="https://arxiv.org/abs/2504.00906"&gt;[S2 Paper (COLM 2025)]&lt;/a&gt;&amp;nbsp; üé• &lt;a href="https://www.youtube.com/watch?v=wUGVQl7c0eg"&gt;[S2 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&amp;nbsp; üåê &lt;a href="https://www.simular.ai/agent-s"&gt;[S1 blog]&lt;/a&gt;&amp;nbsp; üìÑ &lt;a href="https://arxiv.org/abs/2410.08164"&gt;[S1 Paper (ICLR 2025)]&lt;/a&gt;&amp;nbsp; üé• &lt;a href="https://www.youtube.com/watch?v=OBDE3Knte0g"&gt;[S1 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&amp;nbsp; &lt;a href="https://trendshift.io/repositories/13151" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13151" alt="simular-ai%2FAgent-S | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/OS-Windows-blue?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt; &lt;img src="https://img.shields.io/badge/OS-macOS-black?logo=apple&amp;amp;logoColor=white" alt="macOS" /&gt; &lt;img src="https://img.shields.io/badge/OS-Linux-yellow?logo=linux&amp;amp;logoColor=black" alt="Linux" /&gt; &lt;a href="https://discord.gg/E2XfsK9fPV"&gt; &lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/E2XfsK9fPV?style=flat" alt="Discord" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://pepy.tech/projects/gui-agents"&gt; &lt;img src="https://static.pepy.tech/badge/gui-agents" alt="PyPI Downloads" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=de"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=es"&gt;Espa√±ol&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=fr"&gt;fran√ßais&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=pt"&gt;Portugu√™s&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  &amp;nbsp;&amp;nbsp; 
 &lt;p&gt;Skip the setup? Try Agent S in &lt;a href="https://cloud.simular.ai/"&gt;Simular Cloud&lt;/a&gt; &lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;ü•≥ Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/10/02&lt;/strong&gt;: Released Agent S3 and its &lt;a href="https://arxiv.org/abs/2510.02250"&gt;technical paper&lt;/a&gt;, setting a new SOTA of &lt;strong&gt;69.9%&lt;/strong&gt; on OSWorld (approaching 72% human performance), with strong generalizability on WindowsAgentArena and AndroidWorld! It is also simpler, faster, and more flexible.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/08/01&lt;/strong&gt;: Agent S2.5 is released (gui-agents v0.2.5): simpler, better, and faster! New SOTA on &lt;a href="https://os-world.github.io"&gt;OSWorld-Verified&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/07/07&lt;/strong&gt;: The &lt;a href="https://arxiv.org/abs/2504.00906"&gt;Agent S2 paper&lt;/a&gt; is accepted to COLM 2025! See you in Montreal!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/04/27&lt;/strong&gt;: The Agent S paper won the Best Paper Award üèÜ at ICLR 2025 Agentic AI for Science Workshop!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/04/01&lt;/strong&gt;: Released the &lt;a href="https://arxiv.org/abs/2504.00906"&gt;Agent S2 paper&lt;/a&gt; with new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/03/12&lt;/strong&gt;: Released Agent S2 along with v0.2.0 of &lt;a href="https://github.com/simular-ai/Agent-S"&gt;gui-agents&lt;/a&gt;, the new state-of-the-art for computer use agents (CUA), outperforming OpenAI's CUA/Operator and Anthropic's Claude 3.7 Sonnet Computer-Use!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/01/22&lt;/strong&gt;: The &lt;a href="https://arxiv.org/abs/2410.08164"&gt;Agent S paper&lt;/a&gt; is accepted to ICLR 2025!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/01/21&lt;/strong&gt;: Released v0.1.2 of &lt;a href="https://github.com/simular-ai/Agent-S"&gt;gui-agents&lt;/a&gt; library, with support for Linux and Windows!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2024/12/05&lt;/strong&gt;: Released v0.1.0 of &lt;a href="https://github.com/simular-ai/Agent-S"&gt;gui-agents&lt;/a&gt; library, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2024/10/10&lt;/strong&gt;: Released the &lt;a href="https://arxiv.org/abs/2410.08164"&gt;Agent S paper&lt;/a&gt; and codebase!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-introduction"&gt;üí° Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-current-results"&gt;üéØ Current Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#%EF%B8%8F-installation--setup"&gt;üõ†Ô∏è Installation &amp;amp; Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-usage"&gt;üöÄ Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-acknowledgements"&gt;ü§ù Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-citation"&gt;üí¨ Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üí° Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to &lt;strong&gt;Agent S&lt;/strong&gt;, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer.&lt;/p&gt; 
&lt;p&gt;Whether you're interested in AI, automation, or contributing to cutting-edge agent-based systems, we're excited to have you here!&lt;/p&gt; 
&lt;h2&gt;üéØ Current Results&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/s3_results.png" alt="Agent S3 Results" width="700" /&gt; &lt;/p&gt; 
&lt;p&gt;On OSWorld, Agent S3 alone reaches 62.6% in the 100-step setting, already exceeding the previous state of the art of 61.4% (Claude Sonnet 4.5). With the addition of Behavior Best-of-N, performance climbs even higher to 69.9%, bringing computer-use agents to within just a few points of human-level accuracy (72%).&lt;/p&gt; 
&lt;p&gt;Agent S3 also demonstrates strong zero-shot generalization. On WindowsAgentArena, accuracy rises from 50.2% using only Agent S3 to 56.6% by selecting from 3 rollouts. Similarly on AndroidWorld, performance improves from 68.1% to 71.6%&lt;/p&gt; 
&lt;h2&gt;üõ†Ô∏è Installation &amp;amp; Setup&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Single Monitor&lt;/strong&gt;: Our agent is designed for single monitor screens&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: The agent runs Python code to control your computer - use with care&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Platforms&lt;/strong&gt;: Linux, Mac, and Windows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;To install Agent S3 without cloning the repository, run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install gui-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you would like to test Agent S3 while making changes, clone the repository and install using&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Don't forget to also &lt;code&gt;brew install tesseract&lt;/code&gt;! Pytesseract requires this extra installation to work.&lt;/p&gt; 
&lt;h3&gt;API Configuration&lt;/h3&gt; 
&lt;h4&gt;Option 1: Environment Variables&lt;/h4&gt; 
&lt;p&gt;Add to your &lt;code&gt;.bashrc&lt;/code&gt; (Linux) or &lt;code&gt;.zshrc&lt;/code&gt; (MacOS):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=&amp;lt;YOUR_API_KEY&amp;gt;
export ANTHROPIC_API_KEY=&amp;lt;YOUR_ANTHROPIC_API_KEY&amp;gt;
export HF_TOKEN=&amp;lt;YOUR_HF_TOKEN&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Python Script&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["OPENAI_API_KEY"] = "&amp;lt;YOUR_API_KEY&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;p&gt;We support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. See &lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md"&gt;models.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h3&gt;Grounding Models (Required)&lt;/h3&gt; 
&lt;p&gt;For optimal performance, we recommend &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;UI-TARS-1.5-7B&lt;/a&gt; hosted on Hugging Face Inference Endpoints or another provider. See &lt;a href="https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints"&gt;Hugging Face Inference Endpoints&lt;/a&gt; for setup instructions.&lt;/p&gt; 
&lt;h2&gt;üöÄ Usage&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö°Ô∏è &lt;strong&gt;Recommended Setup:&lt;/strong&gt;&lt;br /&gt; For the best configuration, we recommend using &lt;strong&gt;OpenAI gpt-5-2025-08-07&lt;/strong&gt; as the main model, paired with &lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt; for grounding.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;CLI&lt;/h3&gt; 
&lt;p&gt;Note, this is running Agent S3, our improved agent, without bBoN.&lt;/p&gt; 
&lt;p&gt;Run Agent S3 with the required parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;agent_s \
    --provider openai \
    --model gpt-5-2025-08-07 \
    --ground_provider huggingface \
    --ground_url http://localhost:8080 \
    --ground_model ui-tars-1.5-7b \
    --grounding_width 1920 \
    --grounding_height 1080
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Local Coding Environment (Optional)&lt;/h4&gt; 
&lt;p&gt;For tasks that require code execution (e.g., data processing, file manipulation, system automation), you can enable the local coding environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;agent_s \
    --provider openai \
    --model gpt-5-2025-08-07 \
    --ground_provider huggingface \
    --ground_url http://localhost:8080 \
    --ground_model ui-tars-1.5-7b \
    --grounding_width 1920 \
    --grounding_height 1080 \
    --enable_local_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚ö†Ô∏è &lt;strong&gt;WARNING&lt;/strong&gt;: The local coding environment executes arbitrary Python and Bash code locally on your machine. Only use this feature in trusted environments and with trusted inputs.&lt;/p&gt; 
&lt;h4&gt;Required Parameters&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--provider&lt;/code&gt;&lt;/strong&gt;: Main generation model provider (e.g., openai, anthropic, etc.) - Default: "openai"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/strong&gt;: Main generation model name (e.g., gpt-5-2025-08-07) - Default: "gpt-5-2025-08-07"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_provider&lt;/code&gt;&lt;/strong&gt;: The provider for the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_url&lt;/code&gt;&lt;/strong&gt;: The URL of the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_model&lt;/code&gt;&lt;/strong&gt;: The model name for the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--grounding_width&lt;/code&gt;&lt;/strong&gt;: Width of the output coordinate resolution from the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--grounding_height&lt;/code&gt;&lt;/strong&gt;: Height of the output coordinate resolution from the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Optional Parameters&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model_temperature&lt;/code&gt;&lt;/strong&gt;: The temperature to fix all model calls to (necessary to set to 1.0 for models like o3 but can be left blank for other models)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Grounding Model Dimensions&lt;/h4&gt; 
&lt;p&gt;The grounding width and height should match the output coordinate resolution of your grounding model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt;: Use &lt;code&gt;--grounding_width 1920 --grounding_height 1080&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI-TARS-72B&lt;/strong&gt;: Use &lt;code&gt;--grounding_width 1000 --grounding_height 1000&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Optional Parameters&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model_url&lt;/code&gt;&lt;/strong&gt;: Custom API URL for main generation model - Default: ""&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model_api_key&lt;/code&gt;&lt;/strong&gt;: API key for main generation model - Default: ""&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_api_key&lt;/code&gt;&lt;/strong&gt;: API key for grounding model endpoint - Default: ""&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--max_trajectory_length&lt;/code&gt;&lt;/strong&gt;: Maximum number of image turns to keep in trajectory - Default: 8&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--enable_reflection&lt;/code&gt;&lt;/strong&gt;: Enable reflection agent to assist the worker agent - Default: True&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--enable_local_env&lt;/code&gt;&lt;/strong&gt;: Enable local coding environment for code execution (WARNING: Executes arbitrary code locally) - Default: False&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Local Coding Environment Details&lt;/h4&gt; 
&lt;p&gt;The local coding environment enables Agent S3 to execute Python and Bash code directly on your machine. This is particularly useful for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Data Processing&lt;/strong&gt;: Manipulating spreadsheets, CSV files, or databases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Operations&lt;/strong&gt;: Bulk file processing, content extraction, or file organization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System Automation&lt;/strong&gt;: Configuration changes, system setup, or automation scripts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Development&lt;/strong&gt;: Writing, editing, or executing code files&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Processing&lt;/strong&gt;: Document manipulation, content editing, or formatting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When enabled, the agent can use the &lt;code&gt;call_code_agent&lt;/code&gt; action to execute code blocks for tasks that can be completed through programming rather than GUI interaction.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: The same Python interpreter used to run Agent S3 (automatically detected)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bash&lt;/strong&gt;: Available at &lt;code&gt;/bin/bash&lt;/code&gt; (standard on macOS and Linux)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System Permissions&lt;/strong&gt;: The agent runs with the same permissions as the user executing it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Security Considerations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The local environment executes arbitrary code with the same permissions as the user running the agent&lt;/li&gt; 
 &lt;li&gt;Only enable this feature in trusted environments&lt;/li&gt; 
 &lt;li&gt;Be cautious when the agent generates code for system-level operations&lt;/li&gt; 
 &lt;li&gt;Consider running in a sandboxed environment for untrusted tasks&lt;/li&gt; 
 &lt;li&gt;Bash scripts are executed with a 30-second timeout to prevent hanging processes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;code&gt;gui_agents&lt;/code&gt; SDK&lt;/h3&gt; 
&lt;p&gt;First, we import the necessary modules. &lt;code&gt;AgentS3&lt;/code&gt; is the main agent class for Agent S3. &lt;code&gt;OSWorldACI&lt;/code&gt; is our grounding agent that translates agent actions into executable python code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pyautogui
import io
from gui_agents.s3.agents.agent_s import AgentS3
from gui_agents.s3.agents.grounding import OSWorldACI
from gui_agents.s3.utils.local_env import LocalEnv  # Optional: for local coding environment

# Load in your API keys.
from dotenv import load_dotenv
load_dotenv()

current_platform = "linux"  # "darwin", "windows"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, we define our engine parameters. &lt;code&gt;engine_params&lt;/code&gt; is used for the main agent, and &lt;code&gt;engine_params_for_grounding&lt;/code&gt; is for grounding. For &lt;code&gt;engine_params_for_grounding&lt;/code&gt;, we support custom endpoints like HuggingFace TGI, vLLM, and Open Router.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;engine_params = {
  "engine_type": provider,
  "model": model,
  "base_url": model_url,           # Optional
  "api_key": model_api_key,        # Optional
  "temperature": model_temperature # Optional
}

# Load the grounding engine from a custom endpoint
ground_provider = "&amp;lt;your_ground_provider&amp;gt;"
ground_url = "&amp;lt;your_ground_url&amp;gt;"
ground_model = "&amp;lt;your_ground_model&amp;gt;"
ground_api_key = "&amp;lt;your_ground_api_key&amp;gt;"

# Set grounding dimensions based on your model's output coordinate resolution
# UI-TARS-1.5-7B: grounding_width=1920, grounding_height=1080
# UI-TARS-72B: grounding_width=1000, grounding_height=1000
grounding_width = 1920  # Width of output coordinate resolution
grounding_height = 1080  # Height of output coordinate resolution

engine_params_for_grounding = {
  "engine_type": ground_provider,
  "model": ground_model,
  "base_url": ground_url,
  "api_key": ground_api_key,  # Optional
  "grounding_width": grounding_width,
  "grounding_height": grounding_height,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, we define our grounding agent and Agent S3.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Optional: Enable local coding environment
enable_local_env = False  # Set to True to enable local code execution
local_env = LocalEnv() if enable_local_env else None

grounding_agent = OSWorldACI(
    env=local_env,  # Pass local_env for code execution capability
    platform=current_platform,
    engine_params_for_generation=engine_params,
    engine_params_for_grounding=engine_params_for_grounding,
    width=1920,  # Optional: screen width
    height=1080  # Optional: screen height
)

agent = AgentS3(
    engine_params,
    grounding_agent,
    platform=current_platform,
    max_trajectory_length=8,  # Optional: maximum image turns to keep
    enable_reflection=True     # Optional: enable reflection agent
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, let's query the agent!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Get screenshot.
screenshot = pyautogui.screenshot()
buffered = io.BytesIO() 
screenshot.save(buffered, format="PNG")
screenshot_bytes = buffered.getvalue()

obs = {
  "screenshot": screenshot_bytes,
}

instruction = "Close VS Code"
info, action = agent.predict(instruction=instruction, observation=obs)

exec(action[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;code&gt;gui_agents/s3/cli_app.py&lt;/code&gt; for more details on how the inference loop works.&lt;/p&gt; 
&lt;h3&gt;OSWorld&lt;/h3&gt; 
&lt;p&gt;To deploy Agent S3 in OSWorld, follow the &lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/osworld_setup/s3/OSWorld.md"&gt;OSWorld Deployment instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üí¨ Citations&lt;/h2&gt; 
&lt;p&gt;If you find this codebase useful, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{Agent-S3,
      title={The Unreasonable Effectiveness of Scaling Agents for Computer Use}, 
      author={Gonzalo Gonzalez-Pumariega and Vincent Tu and Chih-Lun Lee and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2510.02250},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.02250}, 
}

@misc{Agent-S2,
      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, 
      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2504.00906},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.00906}, 
}

@inproceedings{Agent-S,
    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},
    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2025},
    url={https://arxiv.org/abs/2410.08164}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#simular-ai/Agent-S&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=simular-ai/Agent-S&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ScrapeGraphAI/Scrapegraph-ai</title>
      <link>https://github.com/ScrapeGraphAI/Scrapegraph-ai</link>
      <description>&lt;p&gt;Python scraper based on AI&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;üöÄ &lt;strong&gt;Looking for an even faster and simpler way to scrape at scale (only 5 lines of code)?&lt;/strong&gt; Check out our enhanced version at &lt;a href="https://scrapegraphai.com/?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=oss_cta&amp;amp;ut#m_content=top_banner"&gt;&lt;strong&gt;ScrapeGraphAI.com&lt;/strong&gt;&lt;/a&gt;! üöÄ&lt;/h2&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üï∑Ô∏è ScrapeGraphAI: You Only Scrape Once&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/raw/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/raw/main/docs/chinese.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/raw/main/docs/japanese.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/raw/main/docs/korean.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/raw/main/docs/russian.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/raw/main/docs/turkish.md"&gt;T√ºrk√ße&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/ScrapeGraphAI/Scrapegraph-ai?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/ScrapeGraphAI/Scrapegraph-ai?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/ScrapeGraphAI/Scrapegraph-ai?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/ScrapeGraphAI/Scrapegraph-ai?lang=pt"&gt;Portugu√™s&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pepy.tech/project/scrapegraphai"&gt;&lt;img src="https://img.shields.io/pepy/dt/scrapegraphai?style=for-the-badge" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pylint-dev/pylint"&gt;&lt;img src="https://img.shields.io/badge/linting-pylint-yellowgreen?style=for-the-badge" alt="linting: pylint" /&gt;&lt;/a&gt; &lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/actions/workflows/code-quality.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/VinciGit00/Scrapegraph-ai/code-quality.yml?label=Pylint&amp;amp;logo=github&amp;amp;style=for-the-badge" alt="Pylint" /&gt;&lt;/a&gt; &lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/actions/workflows/codeql.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/VinciGit00/Scrapegraph-ai/codeql.yml?label=CodeQL&amp;amp;logo=github&amp;amp;style=for-the-badge" alt="CodeQL" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/gkxQDAjfeX"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/gkxQDAjfeX" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://dashboard.scrapegraphai.com/login"&gt;&lt;img src="https://raw.githubusercontent.com/ScrapeGraphAI/Scrapegraph-ai/main/docs/assets/api_banner.png" alt="API Banner" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9761" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9761" alt="VinciGit00%2FScrapegraph-ai | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="https://scrapegraphai.com"&gt;ScrapeGraphAI&lt;/a&gt; is a &lt;em&gt;web scraping&lt;/em&gt; python library that uses LLM and direct graph logic to create scraping pipelines for websites and local documents (XML, HTML, JSON, Markdown, etc.).&lt;/p&gt; 
&lt;p&gt;Just say which information you want to extract and the library will do it for you!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/VinciGit00/Scrapegraph-ai/main/docs/assets/sgai-hero.png" alt="ScrapeGraphAI Hero" style="width: 100%;" /&gt; &lt;/p&gt; 
&lt;h2&gt;üöÄ Integrations&lt;/h2&gt; 
&lt;p&gt;ScrapeGraphAI offers seamless integration with popular frameworks and tools to enhance your scraping capabilities. Whether you're building with Python or Node.js, using LLM frameworks, or working with no-code platforms, we've got you covered with our comprehensive integration options..&lt;/p&gt; 
&lt;p&gt;You can find more informations at the following &lt;a href="https://scrapegraphai.com"&gt;link&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Integrations&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt;: &lt;a href="https://docs.scrapegraphai.com/introduction"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SDKs&lt;/strong&gt;: &lt;a href="https://docs.scrapegraphai.com/sdks/python"&gt;Python&lt;/a&gt;, &lt;a href="https://docs.scrapegraphai.com/sdks/javascript"&gt;Node&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Frameworks&lt;/strong&gt;: &lt;a href="https://docs.scrapegraphai.com/integrations/langchain"&gt;Langchain&lt;/a&gt;, &lt;a href="https://docs.scrapegraphai.com/integrations/llamaindex"&gt;Llama Index&lt;/a&gt;, &lt;a href="https://docs.scrapegraphai.com/integrations/crewai"&gt;Crew.ai&lt;/a&gt;, &lt;a href="https://docs.scrapegraphai.com/integrations/agno"&gt;Agno&lt;/a&gt;, &lt;a href="https://github.com/camel-ai/camel"&gt;CamelAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Low-code Frameworks&lt;/strong&gt;: &lt;a href="https://pipedream.com/apps/scrapegraphai"&gt;Pipedream&lt;/a&gt;, &lt;a href="https://bubble.io/plugin/scrapegraphai-1745408893195x213542371433906180"&gt;Bubble&lt;/a&gt;, &lt;a href="https://zapier.com/apps/scrapegraphai/integrations"&gt;Zapier&lt;/a&gt;, &lt;a href="http://localhost:5001/dashboard"&gt;n8n&lt;/a&gt;, &lt;a href="https://dify.ai"&gt;Dify&lt;/a&gt;, &lt;a href="https://app.toolhouse.ai/mcp-servers/scrapegraph_smartscraper"&gt;Toolhouse&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP server&lt;/strong&gt;: &lt;a href="https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp"&gt;Link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quick install&lt;/h2&gt; 
&lt;p&gt;The reference page for Scrapegraph-ai is available on the official page of PyPI: &lt;a href="https://pypi.org/project/scrapegraphai/"&gt;pypi&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install scrapegraphai

# IMPORTANT (for fetching websites content)
playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: it is recommended to install the library in a virtual environment to avoid conflicts with other libraries üê±&lt;/p&gt; 
&lt;h2&gt;üíª Usage&lt;/h2&gt; 
&lt;p&gt;There are multiple standard scraping pipelines that can be used to extract information from a website (or local file).&lt;/p&gt; 
&lt;p&gt;The most common one is the &lt;code&gt;SmartScraperGraph&lt;/code&gt;, which extracts information from a single page given a user prompt and a source URL.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from scrapegraphai.graphs import SmartScraperGraph

# Define the configuration for the scraping pipeline
graph_config = {
    "llm": {
        "model": "ollama/llama3.2",
        "model_tokens": 8192
    },
    "verbose": True,
    "headless": False,
}

# Create the SmartScraperGraph instance
smart_scraper_graph = SmartScraperGraph(
    prompt="Extract useful information from the webpage, including a description of what the company does, founders and social media links",
    source="https://scrapegraphai.com/",
    config=graph_config
)

# Run the pipeline
result = smart_scraper_graph.run()

import json
print(json.dumps(result, indent=4))
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] For OpenAI and other models you just need to change the llm config!&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;graph_config = {
   "llm": {
       "api_key": "YOUR_OPENAI_API_KEY",
       "model": "openai/gpt-4o-mini",
   },
   "verbose": True,
   "headless": False,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The output will be a dictionary like the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;{
    "description": "ScrapeGraphAI transforms websites into clean, organized data for AI agents and data analytics. It offers an AI-powered API for effortless and cost-effective data extraction.",
    "founders": [
        {
            "name": "",
            "role": "Founder &amp;amp; Technical Lead",
            "linkedin": "https://www.linkedin.com/in/perinim/"
        },
        {
            "name": "Marco Vinciguerra",
            "role": "Founder &amp;amp; Software Engineer",
            "linkedin": "https://www.linkedin.com/in/marco-vinciguerra-7ba365242/"
        },
        {
            "name": "Lorenzo Padoan",
            "role": "Founder &amp;amp; Product Engineer",
            "linkedin": "https://www.linkedin.com/in/lorenzo-padoan-4521a2154/"
        }
    ],
    "social_media_links": {
        "linkedin": "https://www.linkedin.com/company/101881123",
        "twitter": "https://x.com/scrapegraphai",
        "github": "https://github.com/ScrapeGraphAI/Scrapegraph-ai"
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are other pipelines that can be used to extract information from multiple pages, generate Python scripts, or even generate audio files.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Pipeline Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SmartScraperGraph&lt;/td&gt; 
   &lt;td&gt;Single-page scraper that only needs a user prompt and an input source.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SearchGraph&lt;/td&gt; 
   &lt;td&gt;Multi-page scraper that extracts information from the top n search results of a search engine.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SpeechGraph&lt;/td&gt; 
   &lt;td&gt;Single-page scraper that extracts information from a website and generates an audio file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ScriptCreatorGraph&lt;/td&gt; 
   &lt;td&gt;Single-page scraper that extracts information from a website and generates a Python script.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SmartScraperMultiGraph&lt;/td&gt; 
   &lt;td&gt;Multi-page scraper that extracts information from multiple pages given a single prompt and a list of sources.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ScriptCreatorMultiGraph&lt;/td&gt; 
   &lt;td&gt;Multi-page scraper that generates a Python script for extracting information from multiple pages and sources.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For each of these graphs there is the multi version. It allows to make calls of the LLM in parallel.&lt;/p&gt; 
&lt;p&gt;It is possible to use different LLM through APIs, such as &lt;strong&gt;OpenAI&lt;/strong&gt;, &lt;strong&gt;Groq&lt;/strong&gt;, &lt;strong&gt;Azure&lt;/strong&gt; and &lt;strong&gt;Gemini&lt;/strong&gt;, or local models using &lt;strong&gt;Ollama&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Remember to have &lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt; installed and download the models using the &lt;strong&gt;ollama pull&lt;/strong&gt; command, if you want to use local models.&lt;/p&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/1sEZBonBMGP44CtO6GQTwAlL0BGJXjtfd?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The documentation for ScrapeGraphAI can be found &lt;a href="https://scrapegraph-ai.readthedocs.io/en/latest/"&gt;here&lt;/a&gt;. Check out also the Docusaurus &lt;a href="https://docs-oss.scrapegraphai.com/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;Feel free to contribute and join our Discord server to discuss with us improvements and give us suggestions!&lt;/p&gt; 
&lt;p&gt;Please see the &lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/raw/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/uJN7TYcpNa"&gt;&lt;img src="https://skillicons.dev/icons?i=discord" alt="My Skills" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/scrapegraphai/"&gt;&lt;img src="https://skillicons.dev/icons?i=linkedin" alt="My Skills" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/scrapegraphai"&gt;&lt;img src="https://skillicons.dev/icons?i=twitter" alt="My Skills" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üîó ScrapeGraph API &amp;amp; SDKs&lt;/h2&gt; 
&lt;p&gt;If you are looking for a quick solution to integrate ScrapeGraph in your system, check out our powerful API &lt;a href="https://dashboard.scrapegraphai.com/login"&gt;here!&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/VinciGit00/Scrapegraph-ai/main/docs/assets/api-banner.png" alt="ScrapeGraph API Banner" style="width: 100%;" /&gt; &lt;/p&gt; 
&lt;p&gt;We offer SDKs in both Python and Node.js, making it easy to integrate into your projects. Check them out below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SDK&lt;/th&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;GitHub Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python SDK&lt;/td&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ScrapeGraphAI/scrapegraph-sdk/tree/main/scrapegraph-py"&gt;scrapegraph-py&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Node.js SDK&lt;/td&gt; 
   &lt;td&gt;Node.js&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ScrapeGraphAI/scrapegraph-sdk/tree/main/scrapegraph-js"&gt;scrapegraph-js&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The Official API Documentation can be found &lt;a href="https://docs.scrapegraphai.com/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìà Telemetry&lt;/h2&gt; 
&lt;p&gt;We collect anonymous usage metrics to enhance our package's quality and user experience. The data helps us prioritize improvements and ensure compatibility. If you wish to opt-out, set the environment variable SCRAPEGRAPHAI_TELEMETRY_ENABLED=false. For more information, please refer to the documentation &lt;a href="https://scrapegraph-ai.readthedocs.io/en/latest/scrapers/telemetry.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;‚ù§Ô∏è Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=VinciGit00/Scrapegraph-ai" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üéì Citations&lt;/h2&gt; 
&lt;p&gt;If you have used our library for research purposes please quote us with the following reference:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;  @misc{scrapegraph-ai,
    author = {Lorenzo Padoan, Marco Vinciguerra},
    title = {Scrapegraph-ai},
    year = {2024},
    url = {https://github.com/VinciGit00/Scrapegraph-ai},
    note = {A Python library for scraping leveraging large language models}
  }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Contact Info&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Marco Vinciguerra&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.linkedin.com/in/marco-vinciguerra-7ba365242/"&gt;&lt;img src="https://img.shields.io/badge/-Linkedin-blue?style=flat&amp;amp;logo=Linkedin&amp;amp;logoColor=white" alt="Linkedin Badge" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Lorenzo Padoan&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.linkedin.com/in/lorenzo-padoan-4521a2154/"&gt;&lt;img src="https://img.shields.io/badge/-Linkedin-blue?style=flat&amp;amp;logo=Linkedin&amp;amp;logoColor=white" alt="Linkedin Badge" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;ScrapeGraphAI is licensed under the MIT License. See the &lt;a href="https://github.com/VinciGit00/Scrapegraph-ai/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;We would like to thank all the contributors to the project and the open-source community for their support.&lt;/li&gt; 
 &lt;li&gt;ScrapeGraphAI is meant to be used for data exploration and research purposes only. We are not responsible for any misuse of the library.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Made with ‚ù§Ô∏è by &lt;a href="https://scrapegraphai.com"&gt;ScrapeGraph AI&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://static.scarf.sh/a.png?x-pxid=102d4b8c-cd6a-4b9e-9a16-d6d141b9212d"&gt;Scarf tracking&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dgtlmoon/changedetection.io</title>
      <link>https://github.com/dgtlmoon/changedetection.io</link>
      <description>&lt;p&gt;Best and simplest tool for website change detection, web page monitoring, and website change alerts. Perfect for tracking content changes, price drops, restock alerts, and website defacement monitoring‚Äîall for free or enjoy our SaaS plan!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Detect Website Changes Automatically ‚Äî Monitor Web Page Changes in Real Time&lt;/h1&gt; 
&lt;p&gt;Monitor websites for updates ‚Äî get notified via Discord, Email, Slack, Telegram, Webhook and many more.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Detect web page content changes and get instant alerts.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Ideal for monitoring price changes, content edits, conditional changes and more.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://changedetection.io?src=github"&gt;&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/screenshot.png" style="max-width:100%;" alt="Web site page change monitoring" title="Web site page change monitoring" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/dgtlmoon/changedetection.io/releases"&gt;&lt;img src="https://img.shields.io:/github/v/release/dgtlmoon/changedetection.io?style=for-the-badge" alt="Release Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dgtlmoon/changedetection.io"&gt;&lt;img src="https://img.shields.io/docker/pulls/dgtlmoon/changedetection.io?style=for-the-badge" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/LICENSE.md"&gt;&lt;img src="https://img.shields.io/github/license/dgtlmoon/changedetection.io.svg?style=for-the-badge" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/dgtlmoon/changedetection.io/actions/workflows/test-only.yml/badge.svg?branch=master" alt="changedetection.io" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://changedetection.io"&gt;&lt;strong&gt;Get started with website page change monitoring straight away. Don't have time? Try our $8.99/month subscription, use our proxies and support!&lt;/strong&gt;&lt;/a&gt; , &lt;em&gt;half the price of other website change monitoring services!&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Chrome browser included.&lt;/li&gt; 
 &lt;li&gt;Nothing to install, access via browser login after signup.&lt;/li&gt; 
 &lt;li&gt;Super fast, no registration needed setup.&lt;/li&gt; 
 &lt;li&gt;Get started watching and receiving website change notifications straight away.&lt;/li&gt; 
 &lt;li&gt;See our &lt;a href="https://changedetection.io/tutorials"&gt;tutorials and how-to page for more inspiration&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Target specific parts of the webpage using the Visual Selector tool.&lt;/h3&gt; 
&lt;p&gt;Available when connected to a &lt;a href="https://github.com/dgtlmoon/changedetection.io/wiki/Playwright-content-fetcher"&gt;playwright content fetcher&lt;/a&gt; (included as part of our subscription service)&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://changedetection.io?src=github"&gt;&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/visualselector-anim.gif" style="max-width:100%;" alt="Select parts and elements of a web page to monitor for changes" title="Select parts and elements of a web page to monitor for changes" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Easily see what changed, examine by word, line, or individual character.&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://changedetection.io?src=github"&gt;&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/screenshot-diff.png" style="max-width:100%;" alt="Self-hosted web page change monitoring context difference " title="Self-hosted web page change monitoring context difference " /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Perform interactive browser steps&lt;/h3&gt; 
&lt;p&gt;Fill in text boxes, click buttons and more, setup your changedetection scenario.&lt;/p&gt; 
&lt;p&gt;Using the &lt;strong&gt;Browser Steps&lt;/strong&gt; configuration, add basic steps before performing change detection, such as logging into websites, adding a product to a cart, accept cookie logins, entering dates and refining searches.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://changedetection.io?src=github"&gt;&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/browsersteps-anim.gif" style="max-width:100%;" alt="Website change detection with interactive browser steps, detect changes behind login and password, search queries and more" title="Website change detection with interactive browser steps, detect changes behind login and password, search queries and more" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;After &lt;strong&gt;Browser Steps&lt;/strong&gt; have been run, then visit the &lt;strong&gt;Visual Selector&lt;/strong&gt; tab to refine the content you're interested in. Requires Playwright to be enabled.&lt;/p&gt; 
&lt;h3&gt;Awesome restock and price change notifications&lt;/h3&gt; 
&lt;p&gt;Enable the &lt;em&gt;"Re-stock &amp;amp; Price detection for single product pages"&lt;/em&gt; option to activate the best way to monitor product pricing, this will extract any meta-data in the HTML page and give you many options to follow the pricing of the product.&lt;/p&gt; 
&lt;p&gt;Easily organise and monitor prices for products from the dashboard, get alerts and notifications when the price of a product changes or comes back in stock again!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://changedetection.io?src=github"&gt;&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/restock-overview.png" style="max-width:100%;" alt="Easily keep an eye on product price changes directly from the UI" title="Easily keep an eye on product price changes directly from the UI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Set price change notification parameters, upper and lower price, price change percentage and more. Always know when a product for sale drops in price.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://changedetection.io?src=github"&gt;&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/restock-settings.png" style="max-width:100%;" alt="Set upper lower and percentage price change notification values" title="Set upper lower and percentage price change notification values" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Example use cases&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Products and services have a change in pricing&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Out of stock notification&lt;/em&gt; and &lt;em&gt;Back In stock notification&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Monitor and track PDF file changes, know when a PDF file has text changes.&lt;/li&gt; 
 &lt;li&gt;Governmental department updates (changes are often only on their websites)&lt;/li&gt; 
 &lt;li&gt;New software releases, security advisories when you're not on their mailing list.&lt;/li&gt; 
 &lt;li&gt;Festivals with changes&lt;/li&gt; 
 &lt;li&gt;Discogs restock alerts and monitoring&lt;/li&gt; 
 &lt;li&gt;Realestate listing changes&lt;/li&gt; 
 &lt;li&gt;Know when your favourite whiskey is on sale, or other special deals are announced before anyone else&lt;/li&gt; 
 &lt;li&gt;COVID related news from government websites&lt;/li&gt; 
 &lt;li&gt;University/organisation news from their website&lt;/li&gt; 
 &lt;li&gt;Detect and monitor changes in JSON API responses&lt;/li&gt; 
 &lt;li&gt;JSON API monitoring and alerting&lt;/li&gt; 
 &lt;li&gt;Changes in legal and other documents&lt;/li&gt; 
 &lt;li&gt;Trigger API calls via notifications when text appears on a website&lt;/li&gt; 
 &lt;li&gt;Glue together APIs using the JSON filter and JSON notifications&lt;/li&gt; 
 &lt;li&gt;Create RSS feeds based on changes in web content&lt;/li&gt; 
 &lt;li&gt;Monitor HTML source code for unexpected changes, strengthen your PCI compliance&lt;/li&gt; 
 &lt;li&gt;You have a very sensitive list of URLs to watch and you do &lt;em&gt;not&lt;/em&gt; want to use the paid alternatives. (Remember, &lt;em&gt;you&lt;/em&gt; are the product)&lt;/li&gt; 
 &lt;li&gt;Get notified when certain keywords appear in Twitter search results&lt;/li&gt; 
 &lt;li&gt;Proactively search for jobs, get notified when companies update their careers page, search job portals for keywords.&lt;/li&gt; 
 &lt;li&gt;Get alerts when new job positions are open on Bamboo HR and other job platforms&lt;/li&gt; 
 &lt;li&gt;Website defacement monitoring&lt;/li&gt; 
 &lt;li&gt;Pok√©mon Card Restock Tracker / Pok√©mon TCG Tracker&lt;/li&gt; 
 &lt;li&gt;RegTech - stay ahead of regulatory changes, regulatory compliance&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Need an actual Chrome runner with Javascript support? We support fetching via WebDriver and Playwright!&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;Key Features&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Lots of trigger filters, such as "Trigger on text", "Remove text by selector", "Ignore text", "Extract text", also using regular-expressions!&lt;/li&gt; 
 &lt;li&gt;Target elements with xPath 1 and xPath 2, CSS Selectors, Easily monitor complex JSON with JSONPath or jq&lt;/li&gt; 
 &lt;li&gt;Switch between fast non-JS and Chrome JS based "fetchers"&lt;/li&gt; 
 &lt;li&gt;Track changes in PDF files (Monitor text changed in the PDF, Also monitor PDF filesize and checksums)&lt;/li&gt; 
 &lt;li&gt;Easily specify how often a site should be checked&lt;/li&gt; 
 &lt;li&gt;Execute JS before extracting text (Good for logging in, see examples in the UI!)&lt;/li&gt; 
 &lt;li&gt;Override Request Headers, Specify &lt;code&gt;POST&lt;/code&gt; or &lt;code&gt;GET&lt;/code&gt; and other methods&lt;/li&gt; 
 &lt;li&gt;Use the "Visual Selector" to help target specific elements&lt;/li&gt; 
 &lt;li&gt;Configurable &lt;a href="https://github.com/dgtlmoon/changedetection.io/wiki/Proxy-configuration"&gt;proxy per watch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Send a screenshot with the notification when a change is detected in the web page&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We &lt;a href="https://brightdata.grsm.io/n0r16zf7eivq"&gt;recommend and use Bright Data&lt;/a&gt; global proxy services, Bright Data will match any first deposit up to $150 using our signup link.&lt;/p&gt; 
&lt;p&gt;Please &lt;span&gt;‚≠ê&lt;/span&gt; star &lt;span&gt;‚≠ê&lt;/span&gt; this project and help it grow! &lt;a href="https://github.com/dgtlmoon/changedetection.io/"&gt;https://github.com/dgtlmoon/changedetection.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Conditional web page changes&lt;/h3&gt; 
&lt;p&gt;Easily &lt;a href="https://changedetection.io/tutorial/conditional-actions-web-page-changes"&gt;configure conditional actions&lt;/a&gt;, for example, only trigger when a price is above or below a preset amount, or &lt;a href="https://changedetection.io/tutorial/how-monitor-keywords-any-website"&gt;when a web page includes (or does not include) a keyword&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/web-page-change-conditions.png" style="max-width:80%;" alt="Conditional web page changes" title="Conditional web page changes" /&gt; 
&lt;h3&gt;Schedule web page watches in any timezone, limit by day of week and time.&lt;/h3&gt; 
&lt;p&gt;Easily set a re-check schedule, for example you could limit the web page change detection to only operate during business hours. Or perhaps based on a foreign timezone (for example, you want to check for the latest news-headlines in a foreign country at 0900 AM),&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/scheduler.png" style="max-width:80%;" alt="How to monitor web page changes according to a schedule" title="How to monitor web page changes according to a schedule" /&gt; 
&lt;p&gt;Includes quick short-cut buttons to setup a schedule for &lt;strong&gt;business hours only&lt;/strong&gt;, or &lt;strong&gt;weekends&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;We have a Chrome extension!&lt;/h3&gt; 
&lt;p&gt;Easily add the current web page to your changedetection.io tool, simply install the extension and click "Sync" to connect it to your existing changedetection.io install.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://chromewebstore.google.com/detail/changedetectionio-website/kefcfmgmlhmankjmnbijimhofdjekbop"&gt;&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/chrome-extension-screenshot.png" style="max-width:80%;" alt="Chrome Extension to easily add the current web-page to detect a change." title="Chrome Extension to easily add the current web-page to detect a change." /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://chromewebstore.google.com/detail/changedetectionio-website/kefcfmgmlhmankjmnbijimhofdjekbop"&gt;Goto the Chrome Webstore to download the extension.&lt;/a&gt; ( Or check out the &lt;a href="https://github.com/dgtlmoon/changedetection.io-browser-extension"&gt;GitHub repo&lt;/a&gt; )&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;With Docker composer, just clone this repository and..&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Docker standalone&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ docker run -d --restart always -p "127.0.0.1:5000:5000" -v datastore-volume:/datastore --name changedetection.io dgtlmoon/changedetection.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;:latest&lt;/code&gt; tag is our latest stable release, &lt;code&gt;:dev&lt;/code&gt; tag is our bleeding edge &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt; 
&lt;p&gt;Alternative docker repository over at ghcr - &lt;a href="https://ghcr.io/dgtlmoon/changedetection.io"&gt;ghcr.io/dgtlmoon/changedetection.io&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;See the install instructions at the wiki &lt;a href="https://github.com/dgtlmoon/changedetection.io/wiki/Microsoft-Windows"&gt;https://github.com/dgtlmoon/changedetection.io/wiki/Microsoft-Windows&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python Pip&lt;/h3&gt; 
&lt;p&gt;Check out our pypi page &lt;a href="https://pypi.org/project/changedetection.io/"&gt;https://pypi.org/project/changedetection.io/&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ pip3 install changedetection.io
$ changedetection.io -d /path/to/empty/data/dir -p 5000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then visit &lt;a href="http://127.0.0.1:5000"&gt;http://127.0.0.1:5000&lt;/a&gt; , You should now be able to access the UI.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Now with per-site configurable support for using a fast built in HTTP fetcher or use a Chrome based fetcher for monitoring of JavaScript websites!&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Updating changedetection.io&lt;/h2&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;docker pull dgtlmoon/changedetection.io
docker kill $(docker ps -a -f name=changedetection.io -q)
docker rm $(docker ps -a -f name=changedetection.io -q)
docker run -d --restart always -p "127.0.0.1:5000:5000" -v datastore-volume:/datastore --name changedetection.io dgtlmoon/changedetection.io
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;docker compose&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose pull &amp;amp;&amp;amp; docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the wiki for more information &lt;a href="https://github.com/dgtlmoon/changedetection.io/wiki"&gt;https://github.com/dgtlmoon/changedetection.io/wiki&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Filters&lt;/h2&gt; 
&lt;p&gt;XPath(1.0), JSONPath, jq, and CSS support comes baked in! You can be as specific as you need, use XPath exported from various XPath element query creation tools. (We support LXML &lt;code&gt;re:test&lt;/code&gt;, &lt;code&gt;re:match&lt;/code&gt; and &lt;code&gt;re:replace&lt;/code&gt;.)&lt;/p&gt; 
&lt;h2&gt;Notifications&lt;/h2&gt; 
&lt;p&gt;ChangeDetection.io supports a massive amount of notifications (including email, office365, custom APIs, etc) when a web-page has a change detected thanks to the &lt;a href="https://github.com/caronc/apprise"&gt;apprise&lt;/a&gt; library. Simply set one or more notification URL's in the &lt;em&gt;[edit]&lt;/em&gt; tab of that watch.&lt;/p&gt; 
&lt;p&gt;Just some examples&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;discord://webhook_id/webhook_token
flock://app_token/g:channel_id
gitter://token/room
gchat://workspace/key/token
msteams://TokenA/TokenB/TokenC/
o365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail
rocket://user:password@hostname/#Channel
mailto://user:pass@example.com?to=receivingAddress@example.com
json://someserver.com/custom-api
syslog://
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/caronc/apprise#popular-notification-services"&gt;And everything else in this list!&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/screenshot-notifications.png" style="max-width:100%;" alt="Self-hosted web page change monitoring notifications" title="Self-hosted web page change monitoring notifications" /&gt; 
&lt;p&gt;Now you can also customise your notification content and use &lt;a target="_new" href="https://jinja.palletsprojects.com/en/3.0.x/templates/"&gt;Jinja2 templating&lt;/a&gt; for their title and body!&lt;/p&gt; 
&lt;h2&gt;JSON API Monitoring&lt;/h2&gt; 
&lt;p&gt;Detect changes and monitor data in JSON API's by using either JSONPath or jq to filter, parse, and restructure JSON as needed.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/json-filter-field-example.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;This will re-parse the JSON and apply formatting to the text, making it super easy to monitor and detect changes in JSON API results&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/json-diff-example.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h3&gt;JSONPath or jq?&lt;/h3&gt; 
&lt;p&gt;For more complex parsing, filtering, and modifying of JSON data, jq is recommended due to the built-in operators and functions. Refer to the &lt;a href="https://stedolan.github.io/jq/manual/"&gt;documentation&lt;/a&gt; for more specific information on jq.&lt;/p&gt; 
&lt;p&gt;One big advantage of &lt;code&gt;jq&lt;/code&gt; is that you can use logic in your JSON filter, such as filters to only show items that have a value greater than/less than etc.&lt;/p&gt; 
&lt;p&gt;See the wiki &lt;a href="https://github.com/dgtlmoon/changedetection.io/wiki/JSON-Selector-Filter-help"&gt;https://github.com/dgtlmoon/changedetection.io/wiki/JSON-Selector-Filter-help&lt;/a&gt; for more information and examples&lt;/p&gt; 
&lt;h3&gt;Parse JSON embedded in HTML!&lt;/h3&gt; 
&lt;p&gt;When you enable a &lt;code&gt;json:&lt;/code&gt; or &lt;code&gt;jq:&lt;/code&gt; filter, you can even automatically extract and parse embedded JSON inside a HTML page! Amazingly handy for sites that build content based on JSON, such as many e-commerce websites.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;html&amp;gt;
...
&amp;lt;script type="application/ld+json"&amp;gt;

{
   "@context":"http://schema.org/",
   "@type":"Product",
   "offers":{
      "@type":"Offer",
      "availability":"http://schema.org/InStock",
      "price":"3949.99",
      "priceCurrency":"USD",
      "url":"https://www.newegg.com/p/3D5-000D-001T1"
   },
   "description":"Cobratype King Cobra Hero Desktop Gaming PC",
   "name":"Cobratype King Cobra Hero Desktop Gaming PC",
   "sku":"3D5-000D-001T1",
   "itemCondition":"NewCondition"
}
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;json:$..price&lt;/code&gt; or &lt;code&gt;jq:..price&lt;/code&gt; would give &lt;code&gt;3949.99&lt;/code&gt;, or you can extract the whole structure (use a JSONpath test website to validate with)&lt;/p&gt; 
&lt;p&gt;The application also supports notifying you that it can follow this information automatically&lt;/p&gt; 
&lt;h2&gt;Proxy Configuration&lt;/h2&gt; 
&lt;p&gt;See the wiki &lt;a href="https://github.com/dgtlmoon/changedetection.io/wiki/Proxy-configuration"&gt;https://github.com/dgtlmoon/changedetection.io/wiki/Proxy-configuration&lt;/a&gt; , we also support using &lt;a href="https://github.com/dgtlmoon/changedetection.io/wiki/Proxy-configuration#brightdata-proxy-support"&gt;Bright Data proxy services where possible&lt;/a&gt; and &lt;a href="https://oxylabs.go2cloud.org/SH2d"&gt;Oxylabs&lt;/a&gt; proxy services.&lt;/p&gt; 
&lt;h2&gt;Raspberry Pi support?&lt;/h2&gt; 
&lt;p&gt;Raspberry Pi and linux/arm/v6 linux/arm/v7 arm64 devices are supported! See the wiki for &lt;a href="https://github.com/dgtlmoon/changedetection.io/wiki/Fetching-pages-with-WebDriver"&gt;details&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Import support&lt;/h2&gt; 
&lt;p&gt;Easily &lt;a href="https://changedetection.io/tutorial/how-import-your-website-change-detection-lists-excel"&gt;import your list of websites to watch for changes in Excel .xslx file format&lt;/a&gt;, or paste in lists of website URLs as plaintext.&lt;/p&gt; 
&lt;p&gt;Excel import is recommended - that way you can better organise tags/groups of websites and other features.&lt;/p&gt; 
&lt;h2&gt;API Support&lt;/h2&gt; 
&lt;p&gt;Full REST API for programmatic management of watches, tags, notifications and more.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://changedetection.io/docs/api_v1/index.html"&gt;Interactive API Documentation&lt;/a&gt;&lt;/strong&gt; - Complete API reference with live testing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/api-spec.yaml"&gt;OpenAPI Specification&lt;/a&gt;&lt;/strong&gt; - Generate SDKs for any programming language&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support us&lt;/h2&gt; 
&lt;p&gt;Do you use changedetection.io to make money? does it save you time or money? Does it make your life easier? less stressful? Remember, we write this software when we should be doing actual paid work, we have to buy food and pay rent just like you.&lt;/p&gt; 
&lt;p&gt;Consider taking out an officially supported &lt;a href="https://changedetection.io?src=github"&gt;website change detection subscription&lt;/a&gt; , even if you don't use it, you still get the warm fuzzy feeling of helping out the project. (And who knows, you might just use it!)&lt;/p&gt; 
&lt;h2&gt;Commercial Support&lt;/h2&gt; 
&lt;p&gt;I offer commercial support, this software is depended on by network security, aerospace , data-science and data-journalist professionals just to name a few, please reach out at &lt;a href="mailto:dgtlmoon@gmail.com"&gt;dgtlmoon@gmail.com&lt;/a&gt; for any enquiries, I am more than glad to work with your organisation to further the possibilities of what can be done with changedetection.io&lt;/p&gt; 
&lt;h2&gt;Commercial Licencing&lt;/h2&gt; 
&lt;p&gt;If you are reselling this software either in part or full as part of any commercial arrangement, you must abide by our COMMERCIAL_LICENCE.md found in our code repository, please contact &lt;a href="mailto:dgtlmoon@gmail.com"&gt;dgtlmoon@gmail.com&lt;/a&gt; and &lt;a href="mailto:contact@changedetection.io"&gt;contact@changedetection.io&lt;/a&gt; .&lt;/p&gt; 
&lt;h2&gt;Third-party licenses&lt;/h2&gt; 
&lt;p&gt;changedetectionio.html_tools.elementpath_tostring: Copyright (c), 2018-2021, SISSA (Scuola Internazionale Superiore di Studi Avanzati), Licensed under &lt;a href="https://github.com/sissaschool/elementpath/raw/master/LICENSE"&gt;MIT license&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;Recognition of fantastic contributors to the project&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Constantin Hong &lt;a href="https://github.com/Constantin1489"&gt;https://github.com/Constantin1489&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>feder-cr/Jobs_Applier_AI_Agent_AIHawk</title>
      <link>https://github.com/feder-cr/Jobs_Applier_AI_Agent_AIHawk</link>
      <description>&lt;p&gt;AIHawk aims to easy job hunt process by automating the job application process. Utilizing artificial intelligence, it enables users to apply for multiple jobs in a tailored way.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;AIHawk: the first Jobs Applier AI Agent&lt;/h1&gt; 
 &lt;p&gt;AIHawk's core architecture remains &lt;strong&gt;open source&lt;/strong&gt;, allowing developers to inspect and extend the codebase. However, due to copyright considerations, we have removed all third‚Äëparty provider plugins from this repository.&lt;/p&gt; 
 &lt;p&gt;For a fully integrated experience, including managed provider connections: check out &lt;strong&gt;&lt;a href="https://laboro.co/"&gt;laboro.co&lt;/a&gt;&lt;/strong&gt; an AI‚Äëdriven job board where the agent &lt;strong&gt;automatically applies to jobs&lt;/strong&gt; for you.&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;AIHawk has been featured by major media outlets for revolutionizing how job seekers interact with the job market:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.businessinsider.com/aihawk-applies-jobs-for-you-linkedin-risks-inaccuracies-mistakes-2024-11"&gt;&lt;strong&gt;Business Insider&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://techcrunch.com/2024/10/10/a-reporter-used-ai-to-apply-to-2843-jobs/"&gt;&lt;strong&gt;TechCrunch&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.semafor.com/article/09/12/2024/linkedins-have-nots-and-have-bots"&gt;&lt;strong&gt;Semafor&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://devby.io/news/ya-razoslal-rezume-na-2843-vakansii-po-17-v-chas-kak-ii-boty-vytesnyaut-ludei-iz-protsessa-naima.amp"&gt;&lt;strong&gt;Dev.by&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.wired.it/article/aihawk-come-automatizzare-ricerca-lavoro/"&gt;&lt;strong&gt;Wired&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.theverge.com/2024/10/10/24266898/ai-is-enabling-job-seekers-to-think-like-spammers"&gt;&lt;strong&gt;The Verge&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.vanityfair.it/article/intelligenza-artificiale-candidature-di-lavoro"&gt;&lt;strong&gt;Vanity Fair&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.404media.co/i-applied-to-2-843-roles-the-rise-of-ai-powered-job-application-bots/"&gt;&lt;strong&gt;404 Media&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Zie619/n8n-workflows</title>
      <link>https://github.com/Zie619/n8n-workflows</link>
      <description>&lt;p&gt;all of the workflows of n8n i could find (also from the site itself)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;‚ö° N8N Workflow Collection &amp;amp; Documentation&lt;/h1&gt; 
&lt;p&gt;A professionally organized collection of &lt;strong&gt;2,057 n8n workflows&lt;/strong&gt; with a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è IMPORTANT NOTICE (Aug 14, 2025):&lt;/strong&gt; Repository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see &lt;a href="https://github.com/Zie619/n8n-workflows/issues/85"&gt;Issue 85&lt;/a&gt; for instructions on syncing your copy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support My Work&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/zie619"&gt;&lt;img src="https://img.shields.io/badge/-Buy%20Me%20a%20Coffee-ffdd00?logo=buy-me-a-coffee&amp;amp;logoColor=black&amp;amp;style=flat" alt="Buy Me a Coffee" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you'd like to say thanks, consider buying me a coffee‚Äîyour support helps me keep improving this project!&lt;/p&gt; 
&lt;h2&gt;üöÄ &lt;strong&gt;NEW: Public Search Interface &amp;amp; High-Performance Documentation&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;üåê &lt;a href="https://zie619.github.io/n8n-workflows"&gt;Browse workflows online&lt;/a&gt; - No installation required!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Or run locally for development with 100x performance improvement:&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Option 1: Online Search (Recommended for Users)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;üîó Visit: &lt;a href="https://zie619.github.io/n8n-workflows"&gt;zie619.github.io/n8n-workflows&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Instant access&lt;/strong&gt; - No setup required&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Search 2,057+ workflows&lt;/strong&gt; directly in browser&lt;/li&gt; 
 &lt;li&gt;üì± &lt;strong&gt;Mobile-friendly&lt;/strong&gt; interface&lt;/li&gt; 
 &lt;li&gt;üè∑Ô∏è &lt;strong&gt;Category filtering&lt;/strong&gt; across 15 categories&lt;/li&gt; 
 &lt;li&gt;üì• &lt;strong&gt;Direct download&lt;/strong&gt; of workflow JSON files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Option 2: Local Development System&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
pip install -r requirements.txt

# Start the fast API server
python run.py

# Open in browser
http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Sub-100ms response times&lt;/strong&gt; with SQLite FTS5 search&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Instant full-text search&lt;/strong&gt; with advanced filtering&lt;/li&gt; 
 &lt;li&gt;üì± &lt;strong&gt;Responsive design&lt;/strong&gt; - works perfectly on mobile&lt;/li&gt; 
 &lt;li&gt;üåô &lt;strong&gt;Dark/light themes&lt;/strong&gt; with system preference detection&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Live statistics&lt;/strong&gt; - 365 unique integrations, 29,445 total nodes&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Smart categorization&lt;/strong&gt; by trigger type and complexity&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Use case categorization&lt;/strong&gt; by service name mapped to categories&lt;/li&gt; 
 &lt;li&gt;üìÑ &lt;strong&gt;On-demand JSON viewing&lt;/strong&gt; and download&lt;/li&gt; 
 &lt;li&gt;üîó &lt;strong&gt;Mermaid diagram generation&lt;/strong&gt; for workflow visualization&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Real-time workflow naming&lt;/strong&gt; with intelligent formatting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Metric&lt;/th&gt; 
   &lt;th&gt;Old System&lt;/th&gt; 
   &lt;th&gt;New System&lt;/th&gt; 
   &lt;th&gt;Improvement&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;File Size&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;71MB HTML&lt;/td&gt; 
   &lt;td&gt;&amp;lt;100KB&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;700x smaller&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Load Time&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;10+ seconds&lt;/td&gt; 
   &lt;td&gt;&amp;lt;1 second&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;10x faster&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Search&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Client-side only&lt;/td&gt; 
   &lt;td&gt;Full-text with FTS5&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Instant&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;~2GB RAM&lt;/td&gt; 
   &lt;td&gt;&amp;lt;50MB RAM&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;40x less&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mobile Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Poor&lt;/td&gt; 
   &lt;td&gt;Excellent&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Fully responsive&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÇ Repository Organization&lt;/h2&gt; 
&lt;h3&gt;Workflow Collection&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,057 workflows&lt;/strong&gt; with meaningful, searchable names&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; across popular platforms&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;29,445 total nodes&lt;/strong&gt; with professional categorization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality assurance&lt;/strong&gt; - All workflows analyzed and categorized&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Naming System ‚ú®&lt;/h3&gt; 
&lt;p&gt;Our intelligent naming system converts technical filenames into readable titles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Before&lt;/strong&gt;: &lt;code&gt;2051_Telegram_Webhook_Automation_Webhook.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;After&lt;/strong&gt;: &lt;code&gt;Telegram Webhook Automation&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; with smart capitalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic integration detection&lt;/strong&gt; from node analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Use Case Category ‚ú®&lt;/h3&gt; 
&lt;p&gt;The search interface includes a dropdown filter that lets you browse 2,057+ workflows by category.&lt;/p&gt; 
&lt;p&gt;The system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.&lt;/p&gt; 
&lt;h3&gt;How Categorization Works&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the categorization script&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python create_categories.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Service Name Recognition&lt;/strong&gt; The script analyzes each workflow JSON filename to identify recognized service names (e.g., "Twilio", "Slack", "Gmail", etc.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Category Mapping&lt;/strong&gt; Each recognized service name is matched to its corresponding category using the definitions in &lt;code&gt;context/def_categories.json&lt;/code&gt;. For example:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Twilio ‚Üí Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Gmail ‚Üí Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Airtable ‚Üí Data Processing &amp;amp; Analysis&lt;/li&gt; 
   &lt;li&gt;Salesforce ‚Üí CRM &amp;amp; Sales&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Search Categories Generation&lt;/strong&gt; The script produces a &lt;code&gt;search_categories.json&lt;/code&gt; file that contains the categorized workflow data&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Filter Interface&lt;/strong&gt; Users can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Available Categories&lt;/h3&gt; 
&lt;p&gt;The categorization system includes the following main categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Agent Development&lt;/li&gt; 
 &lt;li&gt;Business Process Automation&lt;/li&gt; 
 &lt;li&gt;Cloud Storage &amp;amp; File Management&lt;/li&gt; 
 &lt;li&gt;Communication &amp;amp; Messaging&lt;/li&gt; 
 &lt;li&gt;Creative Content &amp;amp; Video Automation&lt;/li&gt; 
 &lt;li&gt;Creative Design Automation&lt;/li&gt; 
 &lt;li&gt;CRM &amp;amp; Sales&lt;/li&gt; 
 &lt;li&gt;Data Processing &amp;amp; Analysis&lt;/li&gt; 
 &lt;li&gt;E-commerce &amp;amp; Retail&lt;/li&gt; 
 &lt;li&gt;Financial &amp;amp; Accounting&lt;/li&gt; 
 &lt;li&gt;Marketing &amp;amp; Advertising Automation&lt;/li&gt; 
 &lt;li&gt;Project Management&lt;/li&gt; 
 &lt;li&gt;Social Media Management&lt;/li&gt; 
 &lt;li&gt;Technical Infrastructure &amp;amp; DevOps&lt;/li&gt; 
 &lt;li&gt;Web Scraping &amp;amp; Data Extraction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contribute Categories&lt;/h3&gt; 
&lt;p&gt;You can help expand the categorization by adding more service-to-category mappings (e.g., Twilio ‚Üí Communication &amp;amp; Messaging) in context/defs_categories.json.&lt;/p&gt; 
&lt;p&gt;Many workflow JSON files are conveniently named with the service name, often separated by underscores (_).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üõ† Usage Instructions&lt;/h2&gt; 
&lt;h3&gt;Option 1: Modern Fast System (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install Python dependencies
pip install -r requirements.txt

# Start the documentation server
python run.py

# Browse workflows at http://localhost:8000
# - Instant search across 2,057 workflows
# - Professional responsive interface
# - Real-time workflow statistics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Development Mode&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start with auto-reload for development
python run.py --dev

# Or specify custom host/port
python run.py --host 0.0.0.0 --port 3000

# Force database reindexing
python run.py --reindex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Import Workflows into n8n&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use the Python importer (recommended)
python import_workflows.py

# Or manually import individual workflows:
# 1. Open your n8n Editor UI
# 2. Click menu (‚ò∞) ‚Üí Import workflow
# 3. Choose any .json file from the workflows/ folder
# 4. Update credentials/webhook URLs before running
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìä Workflow Statistics&lt;/h2&gt; 
&lt;h3&gt;Current Collection Stats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Total Workflows&lt;/strong&gt;: 2,057 automation workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Active Workflows&lt;/strong&gt;: 215 (10.5% active rate)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Total Nodes&lt;/strong&gt;: 29,528 (avg 14.4 nodes per workflow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unique Integrations&lt;/strong&gt;: 367 different services and APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: SQLite with FTS5 full-text search&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Trigger Distribution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Complex&lt;/strong&gt;: 832 workflows (40.4%) - Multi-trigger systems&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Webhook&lt;/strong&gt;: 521 workflows (25.3%) - API-triggered automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual&lt;/strong&gt;: 478 workflows (23.2%) - User-initiated workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scheduled&lt;/strong&gt;: 226 workflows (11.0%) - Time-based executions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complexity Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Low (‚â§5 nodes)&lt;/strong&gt;: ~35% - Simple automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Medium (6-15 nodes)&lt;/strong&gt;: ~45% - Standard workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High (16+ nodes)&lt;/strong&gt;: ~20% - Complex enterprise systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Popular Integrations&lt;/h3&gt; 
&lt;p&gt;Top services by usage frequency:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Storage&lt;/strong&gt;: Google Drive, Google Sheets, Dropbox&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Databases&lt;/strong&gt;: PostgreSQL, MySQL, MongoDB, Airtable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI/ML&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: HTTP Request, Webhook, GraphQL&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîç Advanced Search Features&lt;/h2&gt; 
&lt;h3&gt;Smart Search Categories&lt;/h3&gt; 
&lt;p&gt;Our system automatically categorizes workflows into 15 main categories:&lt;/p&gt; 
&lt;h4&gt;Available Categories:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI Agent Development&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face, CalcsLive&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Process Automation&lt;/strong&gt;: Workflow utilities, scheduling, data processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Storage &amp;amp; File Management&lt;/strong&gt;: Google Drive, Dropbox, OneDrive, Box&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Communication &amp;amp; Messaging&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp, Email&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Creative Content &amp;amp; Video Automation&lt;/strong&gt;: YouTube, Vimeo, content creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Creative Design Automation&lt;/strong&gt;: Canva, Figma, image processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CRM &amp;amp; Sales&lt;/strong&gt;: Salesforce, HubSpot, Pipedrive, customer management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Processing &amp;amp; Analysis&lt;/strong&gt;: Database operations, analytics, data transformation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;E-commerce &amp;amp; Retail&lt;/strong&gt;: Shopify, Stripe, PayPal, online stores&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial &amp;amp; Accounting&lt;/strong&gt;: Financial tools, payment processing, accounting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Marketing &amp;amp; Advertising Automation&lt;/strong&gt;: Email marketing, campaigns, lead generation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Project Management&lt;/strong&gt;: Jira, Trello, Asana, task management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Social Media Management&lt;/strong&gt;: LinkedIn, Twitter/X, Facebook, Instagram&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technical Infrastructure &amp;amp; DevOps&lt;/strong&gt;: GitHub, deployment, monitoring&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web Scraping &amp;amp; Data Extraction&lt;/strong&gt;: HTTP requests, webhooks, data collection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;API Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Search workflows by text
curl "http://localhost:8000/api/workflows?q=telegram+automation"

# Filter by trigger type and complexity
curl "http://localhost:8000/api/workflows?trigger=Webhook&amp;amp;complexity=high"

# Find all messaging workflows
curl "http://localhost:8000/api/workflows/category/messaging"

# Get database statistics
curl "http://localhost:8000/api/stats"

# Browse available categories
curl "http://localhost:8000/api/categories"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèó Technical Architecture&lt;/h2&gt; 
&lt;h3&gt;Modern Stack&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SQLite Database&lt;/strong&gt; - FTS5 full-text search with 365 indexed integrations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FastAPI Backend&lt;/strong&gt; - RESTful API with automatic OpenAPI documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responsive Frontend&lt;/strong&gt; - Modern HTML5 with embedded CSS/JavaScript&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Analysis&lt;/strong&gt; - Automatic workflow categorization and naming&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Change Detection&lt;/strong&gt; - MD5 hashing for efficient re-indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt; - Non-blocking workflow analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compressed Responses&lt;/strong&gt; - Gzip middleware for optimal speed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Error Handling&lt;/strong&gt; - Graceful degradation and comprehensive logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile Optimization&lt;/strong&gt; - Touch-friendly interface design&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Database Performance&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;-- Optimized schema for lightning-fast queries
CREATE TABLE workflows (
    id INTEGER PRIMARY KEY,
    filename TEXT UNIQUE,
    name TEXT,
    active BOOLEAN,
    trigger_type TEXT,
    complexity TEXT,
    node_count INTEGER,
    integrations TEXT,  -- JSON array of 365 unique services
    description TEXT,
    file_hash TEXT,     -- MD5 for change detection
    analyzed_at TIMESTAMP
);

-- Full-text search with ranking
CREATE VIRTUAL TABLE workflows_fts USING fts5(
    filename, name, description, integrations, tags,
    content='workflows', content_rowid='id'
);
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîß Setup &amp;amp; Requirements&lt;/h2&gt; 
&lt;h3&gt;System Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.7+&lt;/strong&gt; - For running the documentation system&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modern Browser&lt;/strong&gt; - Chrome, Firefox, Safari, Edge&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;50MB Storage&lt;/strong&gt; - For SQLite database and indexes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Instance&lt;/strong&gt; - For importing and running workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install dependencies
pip install -r requirements.txt

# Start documentation server
python run.py

# Access at http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or .venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run with auto-reload for development
python api_server.py --reload

# Force database reindexing
python workflow_db.py --index --force
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìã Naming Convention&lt;/h2&gt; 
&lt;h3&gt;Intelligent Formatting System&lt;/h3&gt; 
&lt;p&gt;Our system automatically converts technical filenames to user-friendly names:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Automatic transformations:
2051_Telegram_Webhook_Automation_Webhook.json ‚Üí "Telegram Webhook Automation"
0250_HTTP_Discord_Import_Scheduled.json ‚Üí "HTTP Discord Import Scheduled"  
0966_OpenAI_Data_Processing_Manual.json ‚Üí "OpenAI Data Processing Manual"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Technical Format&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Smart Capitalization Rules&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP&lt;/strong&gt; ‚Üí HTTP (not Http)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt; ‚Üí API (not Api)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;webhook&lt;/strong&gt; ‚Üí Webhook&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;automation&lt;/strong&gt; ‚Üí Automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;scheduled&lt;/strong&gt; ‚Üí Scheduled&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ API Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Endpoints&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /&lt;/code&gt; - Main workflow browser interface&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/stats&lt;/code&gt; - Database statistics and metrics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows&lt;/code&gt; - Search with filters and pagination&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}&lt;/code&gt; - Detailed workflow information&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/download&lt;/code&gt; - Download workflow JSON&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/diagram&lt;/code&gt; - Generate Mermaid diagram&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Search&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/category/{category}&lt;/code&gt; - Search by service category&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/categories&lt;/code&gt; - List all available categories&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/integrations&lt;/code&gt; - Get integration statistics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;POST /api/reindex&lt;/code&gt; - Trigger background reindexing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Response Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;// GET /api/stats
{
  "total": 2053,
  "active": 215,
  "inactive": 1838,
  "triggers": {
    "Complex": 831,
    "Webhook": 519,
    "Manual": 477,
    "Scheduled": 226
  },
  "total_nodes": 29445,
  "unique_integrations": 365
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;üéâ This project solves &lt;a href="https://github.com/Zie619/n8n-workflows/issues/84"&gt;Issue #84&lt;/a&gt; - providing online access to workflows without requiring local setup!&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Adding New Workflows&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Export workflow&lt;/strong&gt; as JSON from n8n&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Name descriptively&lt;/strong&gt; following the established pattern: &lt;code&gt;[ID]_[Service]_[Purpose]_[Trigger].json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add to workflows/&lt;/strong&gt; directory (create service folder if needed)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Remove sensitive data&lt;/strong&gt; (credentials, personal URLs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add tags&lt;/strong&gt; for better searchability (calculation, automation, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Actions automatically&lt;/strong&gt; updates the public search interface&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Quality Standards&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ Workflow must be functional and tested&lt;/li&gt; 
 &lt;li&gt;‚úÖ Remove all credentials and sensitive data&lt;/li&gt; 
 &lt;li&gt;‚úÖ Follow naming convention for consistency&lt;/li&gt; 
 &lt;li&gt;‚úÖ Verify compatibility with recent n8n versions&lt;/li&gt; 
 &lt;li&gt;‚úÖ Include meaningful description or comments&lt;/li&gt; 
 &lt;li&gt;‚úÖ Add relevant tags for search optimization&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Custom Node Workflows&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ Include npm package links in descriptions&lt;/li&gt; 
 &lt;li&gt;‚úÖ Document custom node requirements&lt;/li&gt; 
 &lt;li&gt;‚úÖ Add installation instructions&lt;/li&gt; 
 &lt;li&gt;‚úÖ Use descriptive tags (like CalcsLive example)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Reindexing (for local development)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Force database reindexing after adding workflows
python run.py --reindex

# Or update search index only
python scripts/generate_search_index.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ö†Ô∏è Important Notes&lt;/h2&gt; 
&lt;h3&gt;Security &amp;amp; Privacy&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Review before use&lt;/strong&gt; - All workflows shared as-is for educational purposes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Update credentials&lt;/strong&gt; - Replace API keys, tokens, and webhooks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test safely&lt;/strong&gt; - Verify in development environment first&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check permissions&lt;/strong&gt; - Ensure proper access rights for integrations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Compatibility&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Version&lt;/strong&gt; - Compatible with n8n 1.0+ (most workflows)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Nodes&lt;/strong&gt; - Some workflows may require additional node installations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API Changes&lt;/strong&gt; - External services may have updated their APIs since creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt; - Verify required integrations before importing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìö Resources &amp;amp; References&lt;/h2&gt; 
&lt;h3&gt;Workflow Sources&lt;/h3&gt; 
&lt;p&gt;This comprehensive collection includes workflows from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Official n8n.io&lt;/strong&gt; - Documentation and community examples&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub repositories&lt;/strong&gt; - Open source community contributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Blog posts &amp;amp; tutorials&lt;/strong&gt; - Real-world automation patterns&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User submissions&lt;/strong&gt; - Tested and verified workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise use cases&lt;/strong&gt; - Business process automations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Learn More&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/"&gt;n8n Documentation&lt;/a&gt; - Official documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://community.n8n.io/"&gt;n8n Community&lt;/a&gt; - Community forum and support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://n8n.io/workflows/"&gt;Workflow Templates&lt;/a&gt; - Official template library&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/integrations/"&gt;Integration Docs&lt;/a&gt; - Service-specific guides&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèÜ Project Achievements&lt;/h2&gt; 
&lt;h3&gt;Repository Transformation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,053 workflows&lt;/strong&gt; professionally organized and named&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; automatically detected and categorized&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; (improved from basic filename patterns)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero data loss&lt;/strong&gt; during intelligent renaming process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced search&lt;/strong&gt; with 15 service categories&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Revolution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Sub-100ms search&lt;/strong&gt; with SQLite FTS5 full-text indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant filtering&lt;/strong&gt; across 29,445 workflow nodes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile-optimized&lt;/strong&gt; responsive design for all devices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time statistics&lt;/strong&gt; with live database queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Professional interface&lt;/strong&gt; with modern UX principles&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Reliability&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robust error handling&lt;/strong&gt; with graceful degradation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Change detection&lt;/strong&gt; for efficient database updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background processing&lt;/strong&gt; for non-blocking operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive logging&lt;/strong&gt; for debugging and monitoring&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Production-ready&lt;/strong&gt; with proper middleware and security&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;This repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üéØ Perfect for&lt;/strong&gt;: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Zie619/n8n-workflows/main/README_ZH.md"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>