<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Tue, 13 Jan 2026 01:51:05 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>EveryInc/compound-engineering-plugin</title>
      <link>https://github.com/EveryInc/compound-engineering-plugin</link>
      <description>&lt;p&gt;Official Claude Code compound engineering plugin&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Compound Engineering Plugin&lt;/h1&gt; 
&lt;p&gt;A Claude Code plugin that makes each unit of engineering work easier than the last.&lt;/p&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin marketplace add https://github.com/EveryInc/compound-engineering-plugin
/plugin install compound-engineering
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Workflow&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;Plan ‚Üí Work ‚Üí Review ‚Üí Compound ‚Üí Repeat
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/workflows:plan&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Turn feature ideas into detailed implementation plans&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/workflows:work&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Execute plans with worktrees and task tracking&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/workflows:review&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-agent code review before merging&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/workflows:compound&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document learnings to make future work easier&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Each cycle compounds: plans inform future plans, reviews catch more issues, patterns get documented.&lt;/p&gt; 
&lt;h2&gt;Philosophy&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Each unit of engineering work should make subsequent units easier‚Äînot harder.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Traditional development accumulates technical debt. Every feature adds complexity. The codebase becomes harder to work with over time.&lt;/p&gt; 
&lt;p&gt;Compound engineering inverts this. 80% is in planning and review, 20% is in execution:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Plan thoroughly before writing code&lt;/li&gt; 
 &lt;li&gt;Review to catch issues and capture learnings&lt;/li&gt; 
 &lt;li&gt;Codify knowledge so it's reusable&lt;/li&gt; 
 &lt;li&gt;Keep quality high so future changes are easy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn More&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EveryInc/compound-engineering-plugin/main/plugins/compound-engineering/README.md"&gt;Full component reference&lt;/a&gt; - all agents, commands, skills&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://every.to/chain-of-thought/compound-engineering-how-every-codes-with-agents"&gt;Compound engineering: how Every codes with agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://every.to/source-code/my-ai-had-already-fixed-the-code-before-i-saw-it"&gt;The story behind compounding engineering&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Financial data platform for analysts, quants and AI agents.&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-light.svg?raw=true#gh-light-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers" /&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20" /&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.&lt;/p&gt; 
&lt;p&gt;ODP operates as the "connect once, consume everywhere" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/python/reference"&gt;https://docs.openbb.co/python/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the Open Data Platform provides the open-source data integration foundation, &lt;strong&gt;OpenBB Workspace&lt;/strong&gt; offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's "connect once, consume everywhere" architecture enables seamless integration between the two.&lt;/p&gt; 
&lt;p&gt;You can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;. &lt;a href="https://pro.openbb.co"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating Open Data Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run an ODP backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate the ODP Backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: Open Data Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The ODP Python Package can be installed from &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/python/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ODP CLI installation&lt;/h3&gt; 
&lt;p&gt;The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ‚≠êÔ∏è)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/python/developer"&gt;Developer Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;among the existing issues&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the Open Data Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800" /&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>gyoridavid/ai_agents_az</title>
      <link>https://github.com/gyoridavid/ai_agents_az</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Agents A-Z&lt;/h1&gt; 
&lt;p&gt;In this repo, you can find the n8n templates we created for the episodes of &lt;a href="https://www.youtube.com/channel/UCloXqLhp_KGhHBe1kwaL2Tg"&gt;AI Agents A-Z&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Season 1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_1"&gt;Episode 1: Creating a prescription agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_2"&gt;Episode 2: Making a daily digest agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_3"&gt;Episode 3: Making LinkedIn posts using Human in the Loop approval process&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_4"&gt;Episode 4: Deep Research Agent using Google&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_5"&gt;Episode 5: Creating a blog writing system using deep research&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_6"&gt;Episode 6: Lead generation with X-Ray search and LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_7"&gt;Episode 7: Creating Youtube short videos using our custom MCP server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_8"&gt;Episode 8: Creating an AI influencer on Instagram using n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_9"&gt;Episode 9: Create revenge story videos for YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_10"&gt;Episode 10: n8n best practices&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_11"&gt;Episode 11: Create short (motivational) stories for YouTube and TikTok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_12"&gt;Episode 12: Scheduling social media posts with Postiz and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_13"&gt;Episode 13: Create AI videos with MiniMax Hailuo 2 and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_14"&gt;Episode 14: Create AI videos with Seedance and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_15"&gt;Episode 15: Generate AI startup ideas from Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_16"&gt;Episode 16: Create AI poem videos with n8n for TikTok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_17"&gt;Episode 17: Create Shopify product videos with Seedance, ElevenLabs, Latentsync, Flux Kontext and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_18"&gt;Episode 18: Scary story TikTok videos workflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_19"&gt;Episode 19: Run FLUX.1 Kontext [dev] with modal.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_20"&gt;Episode 20: Use Wan 2.2, ComfyUI and n8n to generate videos for free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_21"&gt;Episode 21: 10 EASY faceless niches that pay well - monetize in a MONTH (2025)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_22"&gt;Episode 22: Sleep long-form videos with GPT-5, ElevenMusic, Imagen4, Seendance and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_23"&gt;Episode 23: UGC videos with nanobanana and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_24"&gt;Episode 24: generate images with Qwen Image, Flux.1 [dev] and Flux.1 Schnell with modal.com and Cloudflare Workers AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_25"&gt;Episode 25: Fal.ai n8n subworkflows for Qwen Image Edit Plus and Wan 2.2 animate&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_31"&gt;Episode 31: Veo 3.1 is now in n8n - how to use it for FREE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_35"&gt;Episode 35: Instagram influencer machine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_36"&gt;Episode 36: Viral bodycam footage creator with Sora 2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_38"&gt;Episode 38: Create AI reaction videos with Veo 3.1 and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_39"&gt;Episode 39: Create infographics with Nano Banana Pro in n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_40"&gt;Episode 40: Flux.2[dev] with n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_41"&gt;Episode 41: FREE z-image-turbo with n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_42"&gt;Episode 42: 100% FREE explainer videos with n8n and Z-Image&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;servers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/gyoridavid/ai-agents-no-code-tools"&gt;AI Agents No-Code Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gyoridavid/short-video-maker"&gt;Short video maker MCP/REST server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/gyoridavid/narrated-story-creator"&gt;Narrated story creator REST/MCP server&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>davila7/claude-code-templates</title>
      <link>https://github.com/davila7/claude-code-templates</link>
      <description>&lt;p&gt;CLI tool for configuring and monitoring Claude Code&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/v/claude-code-templates.svg?sanitize=true" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/dt/claude-code-templates.svg?sanitize=true" alt="npm downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://z.ai/subscribe?ic=8JVLJQFSKB&amp;amp;utm_source=github&amp;amp;utm_medium=badge&amp;amp;utm_campaign=readme"&gt;&lt;img src="https://img.shields.io/badge/Sponsored%20by-Z.AI-2563eb?style=flat&amp;amp;logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMiAyMkgyMkwxMiAyWiIgZmlsbD0id2hpdGUiLz4KPC9zdmc+" alt="Sponsored by Z.AI" /&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.shields.io/badge/Buy%20Me%20A%20Coffee-support-yellow?style=flat&amp;amp;logo=buy-me-a-coffee" alt="Buy Me A Coffee" /&gt;&lt;/a&gt; &lt;a href="https://github.com/davila7/claude-code-templates"&gt;&lt;img src="https://img.shields.io/github/stars/davila7/claude-code-templates.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/15113" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15113" alt="davila7%2Fclaude-code-templates | Trendshift" style="width: 200px; height: 40px;" width="125" height="40" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://vercel.com/oss"&gt; &lt;img alt="Vercel OSS Program" src="https://vercel.com/oss/program-badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ü§ù Partnership&lt;/h3&gt; 
 &lt;p&gt; &lt;strong&gt;This project is sponsored by &lt;a href="https://z.ai" target="_blank"&gt;Z.AI&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; Supporting Claude Code Templates with the &lt;strong&gt;GLM CODING PLAN&lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://z.ai/subscribe?ic=8JVLJQFSKB&amp;amp;utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=partnership" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Get%2010%25%20OFF-GLM%20Coding%20Plan-2563eb?style=for-the-badge" alt="GLM Coding Plan" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;em&gt;Top-tier coding performance powered by GLM-4.6 ‚Ä¢ Starting at $3/month&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Seamlessly integrates with Claude Code, Cursor, Cline &amp;amp; 10+ AI coding tools&lt;/em&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;code&gt;npx claude-code-templates@latest --setting partnerships/glm-coding-plan --yes&lt;/code&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Claude Code Templates (&lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;)&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Ready-to-use configurations for Anthropic's Claude Code.&lt;/strong&gt; A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.&lt;/p&gt; 
&lt;h2&gt;Browse &amp;amp; Install Components and Templates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse All Templates&lt;/a&gt;&lt;/strong&gt; - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.&lt;/p&gt; 
&lt;img width="1049" height="855" alt="Screenshot 2025-08-19 at 08 09 24" src="https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737" /&gt; 
&lt;h2&gt;üöÄ Quick Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install a complete development stack
npx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration --yes

# Browse and install interactively
npx claude-code-templates@latest

# Install specific components
npx claude-code-templates@latest --agent development-tools/code-reviewer --yes
npx claude-code-templates@latest --command performance/optimize-bundle --yes
npx claude-code-templates@latest --setting performance/mcp-timeouts --yes
npx claude-code-templates@latest --hook git/pre-commit-validation --yes
npx claude-code-templates@latest --mcp database/postgresql-integration --yes
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What You Get&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü§ñ Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AI specialists for specific domains&lt;/td&gt; 
   &lt;td&gt;Security auditor, React performance optimizer, database architect&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° Commands&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom slash commands&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/generate-tests&lt;/code&gt;, &lt;code&gt;/optimize-bundle&lt;/code&gt;, &lt;code&gt;/check-security&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîå MCPs&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;External service integrations&lt;/td&gt; 
   &lt;td&gt;GitHub, PostgreSQL, Stripe, AWS, OpenAI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è Settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Claude Code configurations&lt;/td&gt; 
   &lt;td&gt;Timeouts, memory settings, output styles&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü™ù Hooks&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automation triggers&lt;/td&gt; 
   &lt;td&gt;Pre-commit validation, post-completion actions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üé® Skills&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reusable capabilities with progressive disclosure&lt;/td&gt; 
   &lt;td&gt;PDF processing, Excel automation, custom workflows&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üõ†Ô∏è Additional Tools&lt;/h2&gt; 
&lt;p&gt;Beyond the template catalog, Claude Code Templates includes powerful development tools:&lt;/p&gt; 
&lt;h3&gt;üìä Claude Code Analytics&lt;/h3&gt; 
&lt;p&gt;Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --analytics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üí¨ Conversation Monitor&lt;/h3&gt; 
&lt;p&gt;Mobile-optimized interface to view Claude responses in real-time with secure remote access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Local access
npx claude-code-templates@latest --chats

# Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîç Health Check&lt;/h3&gt; 
&lt;p&gt;Comprehensive diagnostics to ensure your Claude Code installation is optimized.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --health-check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîå Plugin Dashboard&lt;/h3&gt; 
&lt;p&gt;View marketplaces, installed plugins, and manage permissions from a unified interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.aitmpl.com/"&gt;üìö docs.aitmpl.com&lt;/a&gt;&lt;/strong&gt; - Complete guides, examples, and API reference for all components and tools.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! &lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse existing templates&lt;/a&gt;&lt;/strong&gt; to see what's available, then check our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to add your own agents, commands, MCPs, settings, or hooks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please read our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; before contributing.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Attribution&lt;/h2&gt; 
&lt;p&gt;This collection includes components from multiple sources:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Scientific Skills:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/K-Dense-AI/claude-scientific-skills"&gt;K-Dense-AI/claude-scientific-skills&lt;/a&gt;&lt;/strong&gt; by K-Dense Inc. - MIT License (139 scientific skills for biology, chemistry, medicine, and computational research)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Official Anthropic:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/anthropics/skills"&gt;anthropics/skills&lt;/a&gt;&lt;/strong&gt; - Official Anthropic skills (21 skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/anthropics/claude-code"&gt;anthropics/claude-code&lt;/a&gt;&lt;/strong&gt; - Development guides and examples (10 skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Community Skills &amp;amp; Agents:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/obra/superpowers"&gt;obra/superpowers&lt;/a&gt;&lt;/strong&gt; by Jesse Obra - MIT License (14 workflow skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/alirezarezvani/claude-skills"&gt;alirezarezvani/claude-skills&lt;/a&gt;&lt;/strong&gt; by Alireza Rezvani - MIT License (36 professional role skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/wshobson/agents"&gt;wshobson/agents&lt;/a&gt;&lt;/strong&gt; by wshobson - MIT License (48 agents)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NerdyChefsAI Skills&lt;/strong&gt; - Community contribution - MIT License (specialized enterprise skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands &amp;amp; Tools:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;awesome-claude-code&lt;/a&gt;&lt;/strong&gt; by hesreallyhim - CC0 1.0 Universal (21 commands)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/mehdi-lamrani/awesome-claude-skills"&gt;awesome-claude-skills&lt;/a&gt;&lt;/strong&gt; - Apache 2.0 (community skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;move-code-quality-skill&lt;/strong&gt; - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;cocoindex-claude&lt;/strong&gt; - Apache 2.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each of these resources retains its &lt;strong&gt;original license and attribution&lt;/strong&gt;, as defined by their respective authors. We respect and credit all original creators for their work and contributions to the Claude ecosystem.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üîó Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Browse Templates&lt;/strong&gt;: &lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Documentation&lt;/strong&gt;: &lt;a href="https://docs.aitmpl.com"&gt;docs.aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí¨ Community&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Issues&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Stargazers over time&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/davila7/claude-code-templates"&gt;&lt;img src="https://starchart.cc/davila7/claude-code-templates.svg?variant=adaptive" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;‚≠ê Found this useful? Give us a star to support the project!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.buymeacoffee.com/button-api/?text=Buy%20me%20a%20coffee&amp;amp;slug=daniavila&amp;amp;button_colour=FFDD00&amp;amp;font_colour=000000&amp;amp;font_family=Cookie&amp;amp;outline_colour=000000&amp;amp;coffee_colour=ffffff" alt="Buy Me A Coffee" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LuckyOne7777/ChatGPT-Micro-Cap-Experiment</title>
      <link>https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment</link>
      <description>&lt;p&gt;This repo powers my experiment where ChatGPT manages a real-money micro-cap stock portfolio.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT Micro-Cap Experiment&lt;/h1&gt; 
&lt;p&gt;Welcome to the repo behind my 6-month live trading experiment where ChatGPT manages a real-money micro-cap portfolio.&lt;/p&gt; 
&lt;h2&gt;Overview on getting started: &lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Start%20Your%20Own/README.md"&gt;Here&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Repository Structure&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;trading_script.py&lt;/code&gt;&lt;/strong&gt; - Main trading engine with portfolio management and stop-loss automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;Scripts and CSV Files/&lt;/code&gt;&lt;/strong&gt; - My personal portfolio (updates every trading day)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;Start Your Own/&lt;/code&gt;&lt;/strong&gt; - Template files and guide for starting your own experiment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;Weekly Deep Research (MD|PDF)/&lt;/code&gt;&lt;/strong&gt; - Research summaries and performance reports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;Experiment Details/&lt;/code&gt;&lt;/strong&gt; - Documentation, methodology, prompts, and Q&amp;amp;A&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;The Concept&lt;/h1&gt; 
&lt;p&gt;Every day, I kept seeing the same ad about having some A.I. pick undervalued stocks. It was obvious it was trying to get me to subscribe to some garbage, so I just rolled my eyes.&lt;br /&gt; Then I started wondering, "How well would that actually work?"&lt;/p&gt; 
&lt;p&gt;So, starting with just $100, I wanted to answer a simple but powerful question:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Can powerful large language models like ChatGPT actually generate alpha (or at least make smart trading decisions) using real-time data?&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Each trading day:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;I provide it trading data on the stocks in its portfolio.&lt;/li&gt; 
 &lt;li&gt;Strict stop-loss rules apply.&lt;/li&gt; 
 &lt;li&gt;Every week I allow it to use deep research to reevaluate its account.&lt;/li&gt; 
 &lt;li&gt;I track and publish performance data weekly on my blog: &lt;a href="https://nathanbsmith729.substack.com"&gt;Here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Research &amp;amp; Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Deep%20Research%20Index.md"&gt;Research Index&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Disclaimer.md"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Q%26A.md"&gt;Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Prompts.md"&gt;Prompts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Start%20Your%20Own/README.md"&gt;Starting Your Own&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/tree/main/Weekly%20Deep%20Research%20(MD)"&gt;Research Summaries (MD)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/tree/main/Weekly%20Deep%20Research%20(PDF)"&gt;Full Deep Research Reports (PDF)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Experiment%20Details/Chats.md"&gt;Chats&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Current Performance&lt;/h1&gt; 
&lt;!-- To update performance chart: 
     1. Replace the image file with updated results
     2. Update the dates and description below
     3. Update the "Last Updated" date --&gt; 
&lt;p&gt;&lt;strong&gt;Current Portfolio Results&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/main/Results.png" alt="Latest Performance Results" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Current Status:&lt;/strong&gt; Portfolio is underperforming the S&amp;amp;P 500 benchmark&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Performance data is updated after each trading day. See the CSV files in &lt;code&gt;Scripts and CSV Files/&lt;/code&gt; for detailed daily tracking.&lt;/em&gt;&lt;/p&gt; 
&lt;h1&gt;Features of This Repo&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Live trading scripts ‚Äî used to evaluate prices and update holdings daily&lt;/li&gt; 
 &lt;li&gt;LLM-powered decision engine ‚Äî ChatGPT picks the trades&lt;/li&gt; 
 &lt;li&gt;Performance tracking ‚Äî CSVs with daily PnL, total equity, and trade history&lt;/li&gt; 
 &lt;li&gt;Visualization tools ‚Äî Matplotlib graphs comparing ChatGPT vs. Index&lt;/li&gt; 
 &lt;li&gt;Logs &amp;amp; trade data ‚Äî auto-saved logs for transparency&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Want to Contribute?&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome! This project is community-oriented, and your help is invaluable.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Issues:&lt;/strong&gt; If you notice a bug or have an idea for improvement, please.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pull Requests:&lt;/strong&gt; Feel free to submit a PR ‚Äî I usually review within a few days.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Collaboration:&lt;/strong&gt; High-value contributors may be invited as maintainers/admins to help shape the project‚Äôs future.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Whether it‚Äôs fixing a typo, adding features, or discussing new ideas, all contributions are appreciated!&lt;/p&gt; 
&lt;p&gt;For more information, check out: &lt;a href="https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment/raw/main/Other/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Why This Matters&lt;/h1&gt; 
&lt;p&gt;AI is being hyped across every industry, but can it really manage money without guidance?&lt;/p&gt; 
&lt;p&gt;This project is an attempt to find out ‚Äî with transparency, data, and a real budget.&lt;/p&gt; 
&lt;h1&gt;Tech Stack &amp;amp; Features&lt;/h1&gt; 
&lt;h2&gt;Core Technologies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt; - Core scripting and automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;pandas + yFinance&lt;/strong&gt; - Market data fetching and analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Matplotlib&lt;/strong&gt; - Performance visualization and charting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ChatGPT-5&lt;/strong&gt; - AI-powered trading decision engine&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robust Data Sources&lt;/strong&gt; - Yahoo Finance primary, Stooq fallback for reliability&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Stop-Loss&lt;/strong&gt; - Automatic position management with configurable stop-losses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Trading&lt;/strong&gt; - Market-on-Open (MOO) and limit order support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting Support&lt;/strong&gt; - ASOF_DATE override for historical analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Analytics&lt;/strong&gt; - CAPM analysis, Sharpe/Sortino ratios, drawdown metrics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Trade Logging&lt;/strong&gt; - Complete transparency with detailed execution logs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;System Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.11+&lt;/li&gt; 
 &lt;li&gt;Internet connection for market data&lt;/li&gt; 
 &lt;li&gt;~10MB storage for CSV data files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Follow Along&lt;/h1&gt; 
&lt;p&gt;The experiment runs from June 2025 to December 2025.&lt;br /&gt; Every trading day I will update the portfolio CSV file.&lt;br /&gt; If you feel inspired to do something similar, feel free to use this as a blueprint.&lt;/p&gt; 
&lt;p&gt;Updates are posted weekly on my blog, more coming soon!&lt;/p&gt; 
&lt;p&gt;Blog: &lt;a href="https://nathanbsmith729.substack.com"&gt;A.I Controls Stock Account&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Have feature requests or any advice?&lt;/p&gt; 
&lt;p&gt;Please reach out here: &lt;strong&gt;&lt;a href="mailto:nathanbsmith.business@gmail.com"&gt;nathanbsmith.business@gmail.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;‚ùóÔ∏è&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)üìå&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;‚Ä¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Ä¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>camel-ai/owl</title>
      <link>https://github.com/camel-ai/owl</link>
      <description>&lt;p&gt;ü¶â OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; ü¶â OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://camel-ai.github.io/camel/index.html"&gt;&lt;img src="https://img.shields.io/badge/Documentation-EB3ECC" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://discord.camel-ai.org/"&gt;&lt;img src="https://img.shields.io/discord/1082486657678311454?logo=discord&amp;amp;labelColor=%20%235462eb&amp;amp;logoColor=%20%23f5f5f5&amp;amp;color=%20%235462eb" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/CamelAIOrg"&gt;&lt;img src="https://img.shields.io/twitter/follow/CamelAIOrg?style=social" alt="X" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/CamelAI/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/CamelAI?style=plastic&amp;amp;logo=reddit&amp;amp;label=r%2FCAMEL&amp;amp;labelColor=white" alt="Reddit" /&gt;&lt;/a&gt; &lt;a href="https://ghli.org/camel/wechat.png"&gt;&lt;img src="https://img.shields.io/badge/WeChat-CamelAIOrg-brightgreen?logo=wechat&amp;amp;logoColor=white" alt="Wechat" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/assets/qr_code.jpg"&gt;&lt;img src="https://img.shields.io/badge/WeChat-OWLProject-brightgreen?logo=wechat&amp;amp;logoColor=white" alt="Wechat" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/camel-ai"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-CAMEL--AI-ffc107?color=ffc107&amp;amp;logoColor=white" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/camel-ai/owl/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/camel-ai/owl?label=stars&amp;amp;logo=github&amp;amp;color=brightgreen" alt="Star" /&gt;&lt;/a&gt; &lt;a href="https://github.com/camel-ai/owl/raw/main/licenses/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="Package License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background-color: #e3f2fd; padding: 20px; border-radius: 15px; border: 3px solid #1976d2; margin: 25px 0;"&gt; 
 &lt;h2 style="color: #1976d2; margin: 0 0 15px 0; font-size: 1.8em;"&gt; üöÄ &lt;b&gt;Introducing Eigent: The World's First Multi-Agent Workforce Desktop Application&lt;/b&gt; üöÄ &lt;/h2&gt; 
 &lt;p style="font-size: 1.2em; margin: 10px 0; line-height: 1.6;"&gt; &lt;b&gt;Eigent&lt;/b&gt; empowers you to build, manage, and deploy a custom AI workforce that can turn your most complex workflows into automated tasks. &lt;/p&gt; 
 &lt;p style="font-size: 1.1em; margin: 15px 0;"&gt; ‚ú® &lt;b&gt;100% Open Source&lt;/b&gt; ‚Ä¢ üîß &lt;b&gt;Fully Customizable&lt;/b&gt; ‚Ä¢ üîí &lt;b&gt;Privacy-First&lt;/b&gt; ‚Ä¢ ‚ö° &lt;b&gt;Parallel Execution&lt;/b&gt; &lt;/p&gt; 
 &lt;p style="font-size: 1em; margin: 15px 0; font-style: italic;"&gt; Built on CAMEL-AI's acclaimed open-source project, Eigent introduces a Multi-Agent Workforce that boosts productivity through parallel execution, customization, and privacy protection. &lt;/p&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/eigent-ai/eigent" style="background-color: #d81b60; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;"&gt;üîó Visit Eigent Repo&lt;/a&gt; 
  &lt;a href="https://www.eigent.ai/" style="background-color: #1976d2; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;"&gt;Learn More&lt;/a&gt; 
  &lt;a href="https://www.eigent.ai/download" style="background-color: #43a047; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;"&gt;Get Started&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h4 align="center"&gt; &lt;p&gt;&lt;a href="https://github.com/camel-ai/owl/tree/main/README_zh.md"&gt;‰∏≠ÊñáÈòÖËØª&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl#community"&gt;Community&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;Installation&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl/tree/main/owl"&gt;Examples&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2505.23885"&gt;Paper&lt;/a&gt; |&lt;/p&gt; 
  &lt;!-- [Technical Report](https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f) | --&gt; &lt;p&gt;&lt;a href="https://github.com/camel-ai/owl#citation"&gt;Citation&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl/graphs/contributors"&gt;Contributing&lt;/a&gt; | &lt;a href="https://www.camel-ai.org/"&gt;CAMEL-AI&lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; 
 &lt;div align="center" style="background-color: #f0f7ff; padding: 10px; border-radius: 5px; margin: 15px 0;"&gt; 
  &lt;h3 style="color: #1e88e5; margin: 0;"&gt; üèÜ OWL achieves &lt;span style="color: #d81b60; font-weight: bold; font-size: 1.2em;"&gt;69.09&lt;/span&gt; average score on GAIA benchmark and ranks &lt;span style="color: #d81b60; font-weight: bold; font-size: 1.2em;"&gt;üèÖÔ∏è #1&lt;/span&gt; among open-source frameworks! üèÜ &lt;/h3&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;p&gt;ü¶â OWL is a cutting-edge framework for multi-agent collaboration that pushes the boundaries of task automation, built on top of the &lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL-AI Framework&lt;/a&gt;.&lt;/p&gt; 
  &lt;!-- OWL achieves **58.18** average score on [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark and ranks üèÖÔ∏è #1 among open-source frameworks. --&gt; 
  &lt;p&gt;Our vision is to revolutionize how AI agents collaborate to solve real-world tasks. By leveraging dynamic agent interactions, OWL enables more natural, efficient, and robust task automation across diverse domains.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/camel-ai/owl/main/assets/owl_architecture.png" alt="" /&gt;&lt;/p&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;!-- # Key Features --&gt; 
&lt;h1&gt;üìã Table of Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-table-of-contents"&gt;üìã Table of Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-eigent-multi-agent-workforce-desktop-application"&gt;üöÄ Eigent: Multi-Agent Workforce Desktop Application&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-news"&gt;üî• News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-demo-video"&gt;üé¨ Demo Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-core-features"&gt;‚ú®Ô∏è Core Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;üõ†Ô∏è Installation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#prerequisites"&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-python"&gt;Install Python&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#installation-options"&gt;&lt;strong&gt;Installation Options&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-1-using-uv-recommended"&gt;Option 1: Using uv (Recommended)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-2-using-venv-and-pip"&gt;Option 2: Using venv and pip&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-3-using-conda"&gt;Option 3: Using conda&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-4-using-docker"&gt;Option 4: Using Docker&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#using-pre-built-image-recommended"&gt;&lt;strong&gt;Using Pre-built Image (Recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#building-image-locally"&gt;&lt;strong&gt;Building Image Locally&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#using-convenience-scripts"&gt;&lt;strong&gt;Using Convenience Scripts&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#setup-environment-variables"&gt;&lt;strong&gt;Setup Environment Variables&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#setting-environment-variables-directly"&gt;Setting Environment Variables Directly&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#alternative-using-a-env-file"&gt;Alternative: Using a &lt;code&gt;.env&lt;/code&gt; File&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#mcp-desktop-commander-setup"&gt;&lt;strong&gt;MCP Desktop Commander Setup&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#basic-usage"&gt;Basic Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#running-with-different-models"&gt;Running with Different Models&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#model-requirements"&gt;Model Requirements&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#supported-models"&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#example-tasks"&gt;Example Tasks&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-toolkits-and-capabilities"&gt;üß∞ Toolkits and Capabilities&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-nodejs"&gt;&lt;strong&gt;Install Node.js&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#windows"&gt;Windows&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#linux"&gt;Linux&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#mac"&gt;Mac&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-playwright-mcp-service"&gt;&lt;strong&gt;Install Playwright MCP Service&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#available-toolkits"&gt;Available Toolkits&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#available-toolkits-1"&gt;Available Toolkits&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#multimodal-toolkits-require-multimodal-model-capabilities"&gt;Multimodal Toolkits (Require multimodal model capabilities)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#text-based-toolkits"&gt;Text-Based Toolkits&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#customizing-your-configuration"&gt;Customizing Your Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-web-interface"&gt;üåê Web Interface&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#starting-the-web-ui"&gt;Starting the Web UI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-experiments"&gt;üß™ Experiments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-future-plans"&gt;‚è±Ô∏è Future Plans&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-license"&gt;üìÑ License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-contributing"&gt;ü§ù Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-community"&gt;üî• Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-faq"&gt;‚ùì FAQ&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#general-questions"&gt;General Questions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#experiment-questions"&gt;Experiment Questions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-exploring-camel-dependency"&gt;üìö Exploring CAMEL Dependency&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#accessing-camel-source-code"&gt;Accessing CAMEL Source Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-cite"&gt;üñäÔ∏è Cite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-star-history"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üöÄ Eigent: Multi-Agent Workforce Desktop Application&lt;/h1&gt; 
&lt;div align="center" style="background-color: #f5f5f5; padding: 20px; border-radius: 10px; margin: 20px 0;"&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/eigent-ai/eigent"&gt;Eigent&lt;/a&gt;&lt;/strong&gt; is revolutionizing the way we work with AI agents. As the world's first Multi-Agent Workforce desktop application, Eigent transforms complex workflows into automated, intelligent processes.&lt;/p&gt; 
 &lt;h3&gt;Why Eigent?&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ü§ñ Multi-Agent Collaboration&lt;/strong&gt;: Deploy multiple specialized AI agents that work together seamlessly&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üöÄ Parallel Execution&lt;/strong&gt;: Boost productivity with agents that can work on multiple tasks simultaneously&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üé® Full Customization&lt;/strong&gt;: Build and configure your AI workforce to match your specific needs&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üîí Privacy-First Design&lt;/strong&gt;: Your data stays on your machine - no cloud dependencies required&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üíØ 100% Open Source&lt;/strong&gt;: Complete transparency and community-driven development&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Key Capabilities&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Build Custom Workflows&lt;/strong&gt;: Design complex multi-step processes that agents can execute autonomously&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Manage AI Teams&lt;/strong&gt;: Orchestrate multiple agents with different specializations working in concert&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Deploy Instantly&lt;/strong&gt;: From idea to execution in minutes, not hours&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Monitor Progress&lt;/strong&gt;: Real-time visibility into agent activities and task completion&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Use Cases&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üìä &lt;strong&gt;Data Analysis&lt;/strong&gt;: Automate complex data processing and analysis workflows&lt;/li&gt; 
  &lt;li&gt;üîç &lt;strong&gt;Research&lt;/strong&gt;: Deploy agents to gather, synthesize, and report on information&lt;/li&gt; 
  &lt;li&gt;üíª &lt;strong&gt;Development&lt;/strong&gt;: Accelerate coding tasks with AI-powered development teams&lt;/li&gt; 
  &lt;li&gt;üìù &lt;strong&gt;Content Creation&lt;/strong&gt;: Generate, edit, and optimize content at scale&lt;/li&gt; 
  &lt;li&gt;ü§ù &lt;strong&gt;Business Automation&lt;/strong&gt;: Transform repetitive business processes into automated workflows&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Get Started with Eigent&lt;/h3&gt; 
 &lt;p&gt;Eigent is built on top of the OWL framework, leveraging CAMEL-AI's powerful multi-agent capabilities.&lt;/p&gt; 
 &lt;p&gt;üîó &lt;strong&gt;&lt;a href="https://github.com/eigent-ai/eigent"&gt;Visit the Eigent Repository&lt;/a&gt;&lt;/strong&gt; to explore the codebase, contribute, or learn more about building your own AI workforce.&lt;/p&gt; 
 &lt;p&gt;Follow our &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;installation guide&lt;/a&gt; to start building your own AI workforce today!&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;üî• News&lt;/h1&gt; 
&lt;div align="center" style="background-color: #e8f5e9; padding: 15px; border-radius: 10px; border: 2px solid #4caf50; margin: 20px 0;"&gt; 
 &lt;h3 style="color: #2e7d32; margin: 0; font-size: 1.3em;"&gt; üß© &lt;b&gt;NEW: COMMUNITY AGENT CHALLENGES!&lt;/b&gt; üß© &lt;/h3&gt; 
 &lt;p style="font-size: 1.1em; margin: 10px 0;"&gt; Showcase your creativity by designing unique challenges for AI agents! &lt;br /&gt; Join our community and see your innovative ideas tackled by cutting-edge AI. &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://github.com/camel-ai/owl/raw/main/community_challenges.md" style="background-color: #2e7d32; color: white; padding: 8px 15px; text-decoration: none; border-radius: 5px; font-weight: bold;"&gt;View &amp;amp; Submit Challenges&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- &lt;div style="background-color: #e3f2fd; padding: 12px; border-radius: 8px; border-left: 4px solid #1e88e5; margin: 10px 0;"&gt;
  &lt;h4 style="color: #1e88e5; margin: 0 0 8px 0;"&gt;
    üéâ Latest Major Update - March 15, 2025
  &lt;/h4&gt;
  &lt;p style="margin: 0;"&gt;
    &lt;b&gt;Significant Improvements:&lt;/b&gt;
    &lt;ul style="margin: 5px 0 0 0; padding-left: 20px;"&gt;
      &lt;li&gt;Restructured web-based UI architecture for enhanced stability üèóÔ∏è&lt;/li&gt;
      &lt;li&gt;Optimized OWL Agent execution mechanisms for better performance üöÄ&lt;/li&gt;
    &lt;/ul&gt;
    &lt;i&gt;Try it now and experience the improved performance in your automation tasks!&lt;/i&gt;
  &lt;/p&gt;
&lt;/div&gt; --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.09.22]&lt;/strong&gt;: Exicited to announce that OWL has been accepted by NeurIPS 2025!üöÄ Check the latest paper &lt;a href="https://arxiv.org/abs/2505.23885"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.07.21]&lt;/strong&gt;: We open-sourced the training dataset and model checkpoints of OWL project. Training code coming soon. &lt;a href="https://huggingface.co/collections/camel-ai/optimized-workforce-learning-682ef4ab498befb9426e6e27"&gt;huggingface link&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.05.27]&lt;/strong&gt;: We released the technical report of OWL, including more details on the workforce (framework) and optimized workforce learning (training methodology). &lt;a href="https://arxiv.org/abs/2505.23885"&gt;paper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.05.18]&lt;/strong&gt;: We open-sourced an initial version for replicating workforce experiment on GAIA &lt;a href="https://github.com/camel-ai/owl/tree/gaia69"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.04.18]&lt;/strong&gt;: We uploaded OWL's new GAIA benchmark score of &lt;strong&gt;69.09%&lt;/strong&gt;, ranking #1 among open-source frameworks. Check the technical report &lt;a href="https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.27]&lt;/strong&gt;: Integrate SearxNGToolkit performing web searches using SearxNG search engine.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.26]&lt;/strong&gt;: Enhanced Browser Toolkit with multi-browser support for "chrome", "msedge", and "chromium" channels.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.25]&lt;/strong&gt;: Supported Gemini 2.5 Pro, added example run code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.21]&lt;/strong&gt;: Integrated OpenRouter model platform, fix bug with Gemini tool calling.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.20]&lt;/strong&gt;: Accept header in MCP Toolkit, support automatic playwright installation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.16]&lt;/strong&gt;: Support Bing search, Baidu search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.12]&lt;/strong&gt;: Added Bocha search in SearchToolkit, integrated Volcano Engine model platform, and enhanced Azure and OpenAI Compatible models with structured output and tool calling.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.11]&lt;/strong&gt;: We added MCPToolkit, FileWriteToolkit, and TerminalToolkit to enhance OWL agents with MCP tool calling, file writing capabilities, and terminal command execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.09]&lt;/strong&gt;: We added a web-based user interface that makes it easier to interact with the system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.07]&lt;/strong&gt;: We open-sourced the codebase of the ü¶â OWL project.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.03]&lt;/strong&gt;: OWL achieved the #1 position among open-source frameworks on the GAIA benchmark with a score of 58.18.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üé¨ Demo Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/2a2a825d-39ea-45c5-9ba1-f9d58efbc372"&gt;https://github.com/user-attachments/assets/2a2a825d-39ea-45c5-9ba1-f9d58efbc372&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://private-user-images.githubusercontent.com/55657767/420212194-e813fc05-136a-485f-8df3-f10d9b4e63ec.mp4"&gt;https://private-user-images.githubusercontent.com/55657767/420212194-e813fc05-136a-485f-8df3-f10d9b4e63ec.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This video demonstrates how to install OWL locally and showcases its capabilities as a cutting-edge framework for multi-agent collaboration: &lt;a href="https://www.youtube.com/watch?v=8XlqVyAZOr8"&gt;https://www.youtube.com/watch?v=8XlqVyAZOr8&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;‚ú®Ô∏è Core Features&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Online Search&lt;/strong&gt;: Support for multiple search engines (including Wikipedia, Google, DuckDuckGo, Baidu, Bocha, etc.) for real-time information retrieval and knowledge acquisition.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Processing&lt;/strong&gt;: Support for handling internet or local videos, images, and audio data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Browser Automation&lt;/strong&gt;: Utilize the Playwright framework for simulating browser interactions, including scrolling, clicking, input handling, downloading, navigation, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Parsing&lt;/strong&gt;: Extract content from Word, Excel, PDF, and PowerPoint files, converting them into text or Markdown format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Execution&lt;/strong&gt;: Write and execute Python code using interpreter.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in Toolkits&lt;/strong&gt;: Access to a comprehensive set of built-in toolkits including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: A universal protocol layer that standardizes AI model interactions with various tools and data sources&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Core Toolkits&lt;/strong&gt;: ArxivToolkit, AudioAnalysisToolkit, CodeExecutionToolkit, DalleToolkit, DataCommonsToolkit, ExcelToolkit, GitHubToolkit, GoogleMapsToolkit, GoogleScholarToolkit, ImageAnalysisToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, OpenAPIToolkit, RedditToolkit, SearchToolkit, SemanticScholarToolkit, SymPyToolkit, VideoAnalysisToolkit, WeatherToolkit, BrowserToolkit, and many more for specialized tasks&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üõ†Ô∏è Installation&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/h2&gt; 
&lt;h3&gt;Install Python&lt;/h3&gt; 
&lt;p&gt;Before installing OWL, ensure you have Python installed (version 3.10, 3.11, or 3.12 is supported):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note for GAIA Benchmark Users&lt;/strong&gt;: When running the GAIA benchmark evaluation, please use the &lt;code&gt;gaia58.18&lt;/code&gt; branch which includes a customized version of the CAMEL framework in the &lt;code&gt;owl/camel&lt;/code&gt; directory. This version contains enhanced toolkits with improved stability specifically optimized for the GAIA benchmark compared to the standard CAMEL installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check if Python is installed
python --version

# If not installed, download and install from https://www.python.org/downloads/
# For macOS users with Homebrew:
brew install python@3.10

# For Ubuntu/Debian:
sudo apt update
sudo apt install python3.10 python3.10-venv python3-pip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;strong&gt;Installation Options&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;OWL supports multiple installation methods to fit your workflow preferences.&lt;/p&gt; 
&lt;h3&gt;Option 1: Using uv (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Install uv if you don't have it already
pip install uv

# Create a virtual environment and install dependencies
uv venv .venv --python=3.10

# Activate the virtual environment
# For macOS/Linux
source .venv/bin/activate
# For Windows
.venv\Scripts\activate

# Install CAMEL with all dependencies
uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Using venv and pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Create a virtual environment
# For Python 3.10 (also works with 3.11, 3.12)
python3.10 -m venv .venv

# Activate the virtual environment
# For macOS/Linux
source .venv/bin/activate
# For Windows
.venv\Scripts\activate

# Install from requirements.txt
pip install -r requirements.txt --use-pep517
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 3: Using conda&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Create a conda environment
conda create -n owl python=3.10

# Activate the conda environment
conda activate owl

# Option 1: Install as a package (recommended)
pip install -e .

# Option 2: Install from requirements.txt
pip install -r requirements.txt --use-pep517
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 4: Using Docker&lt;/h3&gt; 
&lt;h4&gt;&lt;strong&gt;Using Pre-built Image (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# This option downloads a ready-to-use image from Docker Hub
# Fastest and recommended for most users
docker compose up -d

# Run OWL inside the container
docker compose exec owl bash
cd .. &amp;amp;&amp;amp; source .venv/bin/activate
playwright install-deps
xvfb-python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Building Image Locally&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For users who need to customize the Docker image or cannot access Docker Hub:
# 1. Open docker-compose.yml
# 2. Comment out the "image: mugglejinx/owl:latest" line
# 3. Uncomment the "build:" section and its nested properties
# 4. Then run:
docker compose up -d --build

# Run OWL inside the container
docker compose exec owl bash
cd .. &amp;amp;&amp;amp; source .venv/bin/activate
playwright install-deps
xvfb-python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Using Convenience Scripts&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to container directory
cd .container

# Make the script executable and build the Docker image
chmod +x build_docker.sh
./build_docker.sh

# Run OWL with your question
./run_in_docker.sh "your question"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;strong&gt;Setup Environment Variables&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;OWL requires various API keys to interact with different services.&lt;/p&gt; 
&lt;h3&gt;Setting Environment Variables Directly&lt;/h3&gt; 
&lt;p&gt;You can set environment variables directly in your terminal:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;macOS/Linux (Bash/Zsh)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY="your-openai-api-key-here"
# Add other required API keys as needed
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows (Command Prompt)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-batch"&gt;set OPENAI_API_KEY=your-openai-api-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows (PowerShell)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;$env:OPENAI_API_KEY = "your-openai-api-key-here"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Environment variables set directly in the terminal will only persist for the current session.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Alternative: Using a &lt;code&gt;.env&lt;/code&gt; File&lt;/h3&gt; 
&lt;p&gt;If you prefer using a &lt;code&gt;.env&lt;/code&gt; file instead, you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Copy and Rename the Template&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# For macOS/Linux
cd owl
cp .env_template .env

# For Windows
cd owl
copy .env_template .env
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can manually create a new file named &lt;code&gt;.env&lt;/code&gt; in the owl directory and copy the contents from &lt;code&gt;.env_template&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure Your API Keys&lt;/strong&gt;: Open the &lt;code&gt;.env&lt;/code&gt; file in your preferred text editor and insert your API keys in the corresponding fields.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For the minimal example (&lt;code&gt;examples/run_mini.py&lt;/code&gt;), you only need to configure the LLM API key (e.g., &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;MCP Desktop Commander Setup&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;If using MCP Desktop Commander within Docker, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx -y @wonderwhy-er/desktop-commander setup --force-file-protocol
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed Docker usage instructions, including cross-platform support, optimized configurations, and troubleshooting, please refer to &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/.container/DOCKER_README_en.md"&gt;DOCKER_README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;üöÄ Quick Start&lt;/h1&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;After installation and setting up your environment variables, you can start using OWL right away:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Running with Different Models&lt;/h2&gt; 
&lt;h3&gt;Model Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Calling&lt;/strong&gt;: OWL requires models with robust tool calling capabilities to interact with various toolkits. Models must be able to understand tool descriptions, generate appropriate tool calls, and process tool outputs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multimodal Understanding&lt;/strong&gt;: For tasks involving web interaction, image analysis, or video processing, models with multimodal capabilities are required to interpret visual content and context.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Supported Models&lt;/h4&gt; 
&lt;p&gt;For information on configuring AI models, please refer to our &lt;a href="https://docs.camel-ai.org/key_modules/models.html#supported-model-platforms-in-camel"&gt;CAMEL models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For optimal performance, we strongly recommend using OpenAI models (GPT-4 or later versions). Our experiments show that other models may result in significantly lower performance on complex tasks and benchmarks, especially those requiring advanced multi-modal understanding and tool use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;OWL supports various LLM backends, though capabilities may vary depending on the model's tool calling and multimodal abilities. You can use the following scripts to run with different models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run with Claude model
python examples/run_claude.py

# Run with Qwen model
python examples/run_qwen_zh.py

# Run with Deepseek model
python examples/run_deepseek_zh.py

# Run with other OpenAI-compatible models
python examples/run_openai_compatible_model.py

# Run with Gemini model
python examples/run_gemini.py

# Run with Azure OpenAI
python examples/run_azure_openai.py

# Run with Ollama
python examples/run_ollama.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a simpler version that only requires an LLM API key, you can try our minimal example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/run_mini.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can run OWL agent with your own task by modifying the &lt;code&gt;examples/run.py&lt;/code&gt; script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Define your own task
task = "Task description here."

society = construct_society(question)
answer, chat_history, token_count = run_society(society)

print(f"\033[94mAnswer: {answer}\033[0m")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For uploading files, simply provide the file path along with your question:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Task with a local file (e.g., file path: `tmp/example.docx`)
task = "What is in the given DOCX file? Here is the file path: tmp/example.docx"

society = construct_society(question)
answer, chat_history, token_count = run_society(society)
print(f"\033[94mAnswer: {answer}\033[0m")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OWL will then automatically invoke document-related tools to process the file and extract the answer.&lt;/p&gt; 
&lt;h3&gt;Example Tasks&lt;/h3&gt; 
&lt;p&gt;Here are some tasks you can try with OWL:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Find the latest stock price for Apple Inc."&lt;/li&gt; 
 &lt;li&gt;"Analyze the sentiment of recent tweets about climate change"&lt;/li&gt; 
 &lt;li&gt;"Help me debug this Python code: [your code here]"&lt;/li&gt; 
 &lt;li&gt;"Summarize the main points from this research paper: [paper URL]"&lt;/li&gt; 
 &lt;li&gt;"Create a data visualization for this dataset: [dataset path]"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üß∞ Toolkits and Capabilities&lt;/h1&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;OWL's MCP integration provides a standardized way for AI models to interact with various tools and data sources:&lt;/p&gt; 
&lt;p&gt;Before using MCP, you need to install Node.js first.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Install Node.js&lt;/strong&gt;&lt;/h3&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Download the official installer: &lt;a href="https://nodejs.org/en"&gt;Node.js&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Check "Add to PATH" option during installation.&lt;/p&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt update
sudo apt install nodejs npm -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;Install Playwright MCP Service&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @executeautomation/playwright-mcp-server
npx playwright install-deps
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try our comprehensive MCP examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;examples/run_mcp.py&lt;/code&gt; - Basic MCP functionality demonstration (local call, requires dependencies)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/run_mcp_sse.py&lt;/code&gt; - Example using the SSE protocol (Use remote services, no dependencies)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Available Toolkits&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Effective use of toolkits requires models with strong tool calling capabilities. For multimodal toolkits (Web, Image, Video), models must also have multimodal understanding abilities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;OWL supports various toolkits that can be customized by modifying the &lt;code&gt;tools&lt;/code&gt; list in your script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Configure toolkits
tools = [
    *BrowserToolkit(headless=False).get_tools(),  # Browser automation
    *VideoAnalysisToolkit(model=models["video"]).get_tools(),
    *AudioAnalysisToolkit().get_tools(),  # Requires OpenAI Key
    *CodeExecutionToolkit(sandbox="subprocess").get_tools(),
    *ImageAnalysisToolkit(model=models["image"]).get_tools(),
    SearchToolkit().search_duckduckgo,
    SearchToolkit().search_google,  # Comment out if unavailable
    SearchToolkit().search_wiki,
    SearchToolkit().search_bocha,
    SearchToolkit().search_baidu,
    *ExcelToolkit().get_tools(),
    *DocumentProcessingToolkit(model=models["document"]).get_tools(),
    *FileWriteToolkit(output_dir="./").get_tools(),
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Available Toolkits&lt;/h2&gt; 
&lt;p&gt;Key toolkits include:&lt;/p&gt; 
&lt;h3&gt;Multimodal Toolkits (Require multimodal model capabilities)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;BrowserToolkit&lt;/strong&gt;: Browser automation for web interaction and navigation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VideoAnalysisToolkit&lt;/strong&gt;: Video processing and content analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ImageAnalysisToolkit&lt;/strong&gt;: Image analysis and interpretation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Text-Based Toolkits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AudioAnalysisToolkit&lt;/strong&gt;: Audio processing (requires OpenAI API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CodeExecutionToolkit&lt;/strong&gt;: Python code execution and evaluation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SearchToolkit&lt;/strong&gt;: Web searches (Google, DuckDuckGo, Wikipedia)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DocumentProcessingToolkit&lt;/strong&gt;: Document parsing (PDF, DOCX, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additional specialized toolkits: ArxivToolkit, GitHubToolkit, GoogleMapsToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, RedditToolkit, WeatherToolkit, and more. For a complete list, see the &lt;a href="https://docs.camel-ai.org/key_modules/tools.html#built-in-toolkits"&gt;CAMEL toolkits documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Customizing Your Configuration&lt;/h2&gt; 
&lt;p&gt;To customize available tools:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# 1. Import toolkits
from camel.toolkits import BrowserToolkit, SearchToolkit, CodeExecutionToolkit

# 2. Configure tools list
tools = [
    *BrowserToolkit(headless=True).get_tools(),
    SearchToolkit().search_wiki,
    *CodeExecutionToolkit(sandbox="subprocess").get_tools(),
]

# 3. Pass to assistant agent
assistant_agent_kwargs = {"model": models["assistant"], "tools": tools}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Selecting only necessary toolkits optimizes performance and reduces resource usage.&lt;/p&gt; 
&lt;h1&gt;üåê Web Interface&lt;/h1&gt; 
&lt;div align="center" style="background-color: #f0f7ff; padding: 15px; border-radius: 10px; border: 2px solid #1e88e5; margin: 20px 0;"&gt; 
 &lt;h3 style="color: #1e88e5; margin: 0;"&gt; üöÄ Enhanced Web Interface Now Available! &lt;/h3&gt; 
 &lt;p style="margin: 10px 0;"&gt; Experience improved system stability and optimized performance with our latest update. Start exploring the power of OWL through our user-friendly interface! &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Starting the Web UI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the Chinese version
python owl/webapp_zh.py

# Start the English version
python owl/webapp.py

# Start the Japanese version
python owl/webapp_jp.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Model Selection&lt;/strong&gt;: Choose between different models (OpenAI, Qwen, DeepSeek, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environment Variable Management&lt;/strong&gt;: Configure your API keys and other settings directly from the UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Chat Interface&lt;/strong&gt;: Communicate with OWL agents through a user-friendly interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task History&lt;/strong&gt;: View the history and results of your interactions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The web interface is built using Gradio and runs locally on your machine. No data is sent to external servers beyond what's required for the model API calls you configure.&lt;/p&gt; 
&lt;h1&gt;üß™ Experiments&lt;/h1&gt; 
&lt;p&gt;To reproduce OWL's GAIA benchmark score: Furthermore, to ensure optimal performance on the GAIA benchmark, please note that our &lt;code&gt;gaia69&lt;/code&gt; branch includes a customized version of the CAMEL framework in the &lt;code&gt;owl/camel&lt;/code&gt; directory. This version contains enhanced toolkits with improved stability for gaia benchmark compared to the standard CAMEL installation.&lt;/p&gt; 
&lt;p&gt;When running the benchmark evaluation:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Switch to the &lt;code&gt;gaia69&lt;/code&gt; branch:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git checkout gaia69
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the evaluation script:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python run_gaia_workforce_claude.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will execute the same configuration that achieved our top-ranking performance on the GAIA benchmark.&lt;/p&gt; 
&lt;h1&gt;‚è±Ô∏è Future Plans&lt;/h1&gt; 
&lt;p&gt;We're continuously working to improve OWL. Here's what's on our roadmap:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Write a technical blog post detailing our exploration and insights in multi-agent collaboration in real-world tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Enhance the toolkit ecosystem with more specialized tools for domain-specific tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Develop more sophisticated agent interaction patterns and communication protocols&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Improve performance on complex multi-step reasoning tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üìÑ License&lt;/h1&gt; 
&lt;p&gt;The source code is licensed under Apache 2.0.&lt;/p&gt; 
&lt;h1&gt;ü§ù Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions from the community! Here's how you can help:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Read our &lt;a href="https://github.com/camel-ai/camel/raw/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check &lt;a href="https://github.com/camel-ai/camel/issues"&gt;open issues&lt;/a&gt; or create new ones&lt;/li&gt; 
 &lt;li&gt;Submit pull requests with your improvements&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Current Issues Open for Contribution:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1915"&gt;#1915&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2190"&gt;#2190&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2165"&gt;#2165&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2121"&gt;#2121&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1908"&gt;#1908&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1538"&gt;#1538&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1481"&gt;#1481&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To take on an issue, simply leave a comment stating your interest.&lt;/p&gt; 
&lt;h1&gt;üî• Community&lt;/h1&gt; 
&lt;p&gt;Join us (&lt;a href="https://discord.camel-ai.org/"&gt;&lt;em&gt;Discord&lt;/em&gt;&lt;/a&gt; or &lt;a href="https://ghli.org/camel/wechat.png"&gt;&lt;em&gt;WeChat&lt;/em&gt;&lt;/a&gt;) in pushing the boundaries of finding the scaling laws of agents.&lt;/p&gt; 
&lt;p&gt;Join us for further discussions!&lt;/p&gt; 
&lt;!-- ![](./assets/community.png) --&gt; 
&lt;img src="https://raw.githubusercontent.com/camel-ai/owl/main/assets/community_code.jpeg" width="50%" /&gt; 
&lt;h1&gt;‚ùì FAQ&lt;/h1&gt; 
&lt;h2&gt;General Questions&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why don't I see Chrome running locally after starting the example script?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: If OWL determines that a task can be completed using non-browser tools (such as search or code execution), the browser will not be launched. The browser window will only appear when OWL determines that browser-based interaction is necessary.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: OWL supports Python 3.10, 3.11, and 3.12.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: How can I contribute to the project?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: See our &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-contributing"&gt;Contributing&lt;/a&gt; section for details on how to get involved. We welcome contributions of all kinds, from code improvements to documentation updates.&lt;/p&gt; 
&lt;h2&gt;Experiment Questions&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Q: Which CAMEL version should I use for replicate the role playing result?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: We provide a modified version of CAMEL (owl/camel) in the gaia58.18 branch. Please make sure you use this CAMEL version for your experiments.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why are my experiment results lower than the reported numbers?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: Since the GAIA benchmark evaluates LLM agents in a realistic world, it introduces a significant amount of randomness. Based on user feedback, one of the most common issues for replication is, for example, agents being blocked on certain webpages due to network reasons. We have uploaded a keywords matching script to help quickly filter out these errors &lt;a href="https://github.com/camel-ai/owl/raw/gaia58.18/owl/filter_failed_cases.py"&gt;here&lt;/a&gt;. You can also check this &lt;a href="https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f?pvs=74"&gt;technical report&lt;/a&gt; for more details when evaluating LLM agents in realistic open-world environments.&lt;/p&gt; 
&lt;h1&gt;üìö Exploring CAMEL Dependency&lt;/h1&gt; 
&lt;p&gt;OWL is built on top of the &lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL&lt;/a&gt; Framework, here's how you can explore the CAMEL source code and understand how it works with OWL:&lt;/p&gt; 
&lt;h2&gt;Accessing CAMEL Source Code&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the CAMEL repository
git clone https://github.com/camel-ai/camel.git
cd camel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;üñäÔ∏è Cite&lt;/h1&gt; 
&lt;p&gt;If you find this repo useful, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{hu2025owl,
      title={OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation}, 
      author={Mengkang Hu and Yuhang Zhou and Wendong Fan and Yuzhou Nie and Bowei Xia and Tao Sun and Ziyu Ye and Zhaoxuan Jin and Yingru Li and Qiguang Chen and Zeyu Zhang and Yifeng Wang and Qianshuo Ye and Bernard Ghanem and Ping Luo and Guohao Li},
      year={2025},
      eprint={2505.23885},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.23885}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;‚≠ê Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#camel-ai/owl&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=camel-ai/owl&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;üìë PageIndex: Document Index for Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; ‚ó¶ &amp;nbsp;No Vector DB&amp;nbsp; ‚ó¶ &amp;nbsp;No Chunking&amp;nbsp; ‚ó¶ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;üñ•Ô∏è Chat Platform&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;üîå MCP&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;üìö Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h2&gt;üì¢ Latest Updates&lt;/h2&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;üî• Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework ‚Äî an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;üß™ Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìë Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity ‚â† relevance&lt;/strong&gt; ‚Äî what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; ‚Äî a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a ‚ÄúTable-of-Contents‚Äù &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;üìç Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host ‚Äî run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service ‚Äî try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; ‚Äî private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß™ Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook ‚Äî a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; ‚Äî no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üå≤ PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚öôÔ∏è Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# ‚òÅÔ∏è Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìà Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üß≠ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß™ &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;üîå &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚≠ê Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star üåü if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/VectifyAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;¬© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/VideoRAG</title>
      <link>https://github.com/HKUDS/VideoRAG</link>
      <description>&lt;p&gt;[KDD'2026] "VideoRAG: Chat with Your Videos"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/cover.png" width="80%" style="border: none; box-shadow: none;" alt="Vimo: Chat with Your Videos" /&gt; 
 &lt;/picture&gt; 
 &lt;h1&gt; &lt;strong&gt;VideoRAG: Chat with Your Videos&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;Vimo Desktop&lt;/strong&gt; &lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/16146" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/16146" alt="HKUDS%2FVideoRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://arxiv.org/abs/2502.01549"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2502.01549-b31b1b" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/VideoRAG/issues/1"&gt;&lt;img src="https://img.shields.io/badge/Áæ§ËÅä-wechat/feishu-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ZzU55kz3"&gt;&lt;img src="https://discordapp.com/api/guilds/1296348098003734629/widget.png?style=shield" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=D5vsxcp4QZI"&gt;&lt;img src="https://img.shields.io/badge/YouTube-Watch%20Demo-red?style=flat&amp;amp;logo=youtube" /&gt;&lt;/a&gt; &lt;a href="https://learnopencv.com/videorag-long-context-video-comprehension/"&gt;&lt;img src="https://img.shields.io/badge/Blog-LearnOpenCV-blue?style=flat&amp;amp;logo=data:image/png;base64,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" alt="Blog" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/platform-macOS%20%7C%20Windows%20%7C%20Linux-lightgrey.svg?sanitize=true" alt="Platform" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üé¨ Intelligent Video Conversations | Powered by Advanced AI | Extreme Long-Context Processing&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm/VideoRAG_cover.png" /&gt; 
&lt;p&gt;Vimo is a revolutionary desktop application that lets you &lt;strong&gt;chat with your videos&lt;/strong&gt; using cutting-edge AI technology. Built on the powerful &lt;a href="https://arxiv.org/abs/2502.01549"&gt;VideoRAG framework&lt;/a&gt;, Vimo can understand and analyze videos of any length - from short clips to hundreds of hours of content - and answer your questions with remarkable accuracy.&lt;/p&gt; 
&lt;h3&gt;üé• Watch Vimo in Action&lt;/h3&gt; 
&lt;p&gt;See how Vimo transforms video interaction with intelligent conversations and deep understanding capabilities.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=D5vsxcp4QZI"&gt; &lt;img src="https://img.youtube.com/vi/D5vsxcp4QZI/maxresdefault.jpg" width="80%" alt="Vimo Introduction Video" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;em&gt;üëÜ Click to watch the Vimo demo video&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;h3&gt;For Everyone&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Drag &amp;amp; Drop Upload&lt;/strong&gt;: Simply drag video files into Vimo&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Conversations&lt;/strong&gt;: Ask questions in natural language&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Format Support&lt;/strong&gt;: Works with MP4, MKV, AVI, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Platform&lt;/strong&gt;: Available on macOS, Windows, and Linux&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Power Users&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Extreme Long Videos&lt;/strong&gt;: Process videos up to hundreds of hours&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Video Analysis&lt;/strong&gt;: Compare and analyze multiple videos simultaneously&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Retrieval&lt;/strong&gt;: Find specific moments and scenes with precision&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Export Capabilities&lt;/strong&gt;: Save insights and references for later use&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Researchers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;VideoRAG Framework&lt;/strong&gt;: Access to cutting-edge retrieval-augmented generation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dataset&lt;/strong&gt;: LongerVideos benchmark with 134+ hours of content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Metrics&lt;/strong&gt;: Detailed evaluation against existing methods&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible Architecture&lt;/strong&gt;: Build upon our open-source foundation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåü Why Vimo?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;For Video Enthusiasts &amp;amp; Professionals:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Effortless Video Analysis&lt;/strong&gt;: Upload any video and start asking questions immediately&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Natural Conversations&lt;/strong&gt;: Chat with your videos as if talking to a human expert&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Length Limits&lt;/strong&gt;: Process everything from 30-second clips to 100+ hour documentaries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Understanding&lt;/strong&gt;: Combines visual content, audio, and context for comprehensive answers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;For Researchers &amp;amp; Developers:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;State-of-the-Art Algorithm&lt;/strong&gt;: Built on VideoRAG, featuring graph-driven knowledge indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Performance&lt;/strong&gt;: Evaluated on 134+ hours across lectures, documentaries, and entertainment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Full access to VideoRAG implementation and research findings&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Architecture&lt;/strong&gt;: Efficient processing with single GPU (RTX 3090) capability&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìã Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-key-features"&gt;‚ú® Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-videorag-algorithm"&gt;üî¨ VideoRAG Algorithm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#%EF%B8%8F-development-setup"&gt;üõ†Ô∏è Development Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-benchmarks--evaluation"&gt;üß™ Benchmarks &amp;amp; Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-citation"&gt;üìñ Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-contributing"&gt;ü§ù Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/#-acknowledgement"&gt;üôè Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quick Start of Vimo&lt;/h2&gt; 
&lt;h3&gt;Option 1: Download Vimo App (Coming Soon)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] We are preparing the &lt;strong&gt;Beta release&lt;/strong&gt; for macOS Apple Silicon first, with Windows and Linux versions coming soon!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="left"&gt; 
 &lt;a href="https://github.com/HKUDS/Vimo/releases"&gt; &lt;img src="https://img.shields.io/badge/Coming%20Soon-Mac%20Download-007ACC?style=for-the-badge&amp;amp;logo=apple&amp;amp;logoColor=white" alt="Coming Soon - Mac Release" height="50" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Option 2: Run from Source Code&lt;/h3&gt; 
&lt;p&gt;For detailed setup instructions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Vimo Desktop App&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/Vimo-desktop"&gt;Vimo-desktop&lt;/a&gt; for complete installation and configuration steps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Overview:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Set up the Python backend environment and start the VideoRAG server&lt;/li&gt; 
 &lt;li&gt;Launch the Electron frontend application&lt;/li&gt; 
 &lt;li&gt;Start chatting with your videos!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üî¨ VideoRAG Algorithm&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm/VideoRAG.png" alt="VideoRAG Architecture" width="80%" /&gt; &lt;/p&gt; 
&lt;p&gt;VideoRAG introduces a novel dual-channel architecture that combines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graph-Driven Knowledge Indexing&lt;/strong&gt;: Multi-modal knowledge graphs for structured video understanding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hierarchical Context Encoding&lt;/strong&gt;: Preserves spatiotemporal visual patterns across long sequences&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptive Retrieval&lt;/strong&gt;: Dynamic retrieval mechanisms optimized for video content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Video Understanding&lt;/strong&gt;: Semantic relationship modeling across multiple videos&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Technical Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Processing&lt;/strong&gt;: Handle hundreds of hours on a single RTX 3090 (24GB)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structured Indexing&lt;/strong&gt;: Distill long videos into concise knowledge representations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Modal Retrieval&lt;/strong&gt;: Align textual queries with visual and audio content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LongerVideos Benchmark&lt;/strong&gt;: 160+ videos, 134+ hours across diverse domains&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Comparison&lt;/h3&gt; 
&lt;p&gt;Our VideoRAG algorithm significantly outperforms existing methods in long-context video understanding:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/Vimo-desktop/figures/table.png" width="80%" alt="Performance Comparison" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Experiments and Evaluation&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm"&gt;VideoRAG-algorithm&lt;/a&gt; for detailed development setup including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Conda environment creation&lt;/li&gt; 
 &lt;li&gt;Model checkpoints download&lt;/li&gt; 
 &lt;li&gt;Dependencies installation&lt;/li&gt; 
 &lt;li&gt;Evaluation scripts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß™ LongerVideos Benchmark&lt;/h2&gt; 
&lt;p&gt;We created the LongerVideos benchmark to evaluate long-context video understanding:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Video Type&lt;/th&gt; 
   &lt;th&gt;#Collections&lt;/th&gt; 
   &lt;th&gt;#Videos&lt;/th&gt; 
   &lt;th&gt;#Queries&lt;/th&gt; 
   &lt;th&gt;Avg. Duration&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Lectures&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;135&lt;/td&gt; 
   &lt;td&gt;376&lt;/td&gt; 
   &lt;td&gt;~64.3 hours&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Documentaries&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;114&lt;/td&gt; 
   &lt;td&gt;~28.5 hours&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Entertainment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;112&lt;/td&gt; 
   &lt;td&gt;~41.9 hours&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;164&lt;/td&gt; 
   &lt;td&gt;602&lt;/td&gt; 
   &lt;td&gt;~134.6 hours&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For detailed evaluation instructions and reproduction scripts, see &lt;a href="https://raw.githubusercontent.com/HKUDS/VideoRAG/main/VideoRAG-algorithm/reproduce"&gt;VideoRAG-algorithm/reproduce&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;p&gt;If you find Vimo or VideoRAG helpful in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{VideoRAG,
  title={VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos},
  author={Ren, Xubin and Xu, Lingrui and Xia, Long and Wang, Shuaiqiang and Yin, Dawei and Huang, Chao},
  journal={arXiv preprint arXiv:2502.01549},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reporting bugs&lt;/strong&gt; or suggesting features for Vimo&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Improving VideoRAG algorithms&lt;/strong&gt; or adding new capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhancing documentation&lt;/strong&gt; or creating tutorials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Designing UI/UX improvements&lt;/strong&gt; for better user experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Feel free to submit issues and pull requests. Together, we're building the future of intelligent video interaction!&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Vimo builds upon the incredible work of the open-source community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2502.01549"&gt;VideoRAG&lt;/a&gt;&lt;/strong&gt;: The core algorithm powering Vimo's intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/gusye1234/nano-graphrag"&gt;nano-graphrag&lt;/a&gt;&lt;/strong&gt; &amp;amp; &lt;strong&gt;&lt;a href="https://github.com/HKUDS/LightRAG"&gt;LightRAG&lt;/a&gt;&lt;/strong&gt;: Graph-based retrieval foundations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/facebookresearch/ImageBind"&gt;ImageBind&lt;/a&gt;&lt;/strong&gt;: Multi-modal representation learning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/bytedance/UI-TARS-desktop"&gt;uitars-desktop&lt;/a&gt;&lt;/strong&gt;: Desktop application architecture inspiration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;üåü Transform how you interact with videos. Start your journey with Vimo today!&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;sub&gt;Built with ‚ù§Ô∏è by the VideoRAG@HKUDS team.&lt;/sub&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>icloud-photos-downloader/icloud_photos_downloader</title>
      <link>https://github.com/icloud-photos-downloader/icloud_photos_downloader</link>
      <description>&lt;p&gt;A command-line tool to download photos from iCloud&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;!!!! &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/issues/1305"&gt;Looking for MAINTAINER for this project&lt;/a&gt; !!!!&lt;/h1&gt; 
&lt;h1&gt;iCloud Photos Downloader &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/quality-checks.yml"&gt;&lt;img src="https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Quality%20Checks/badge.svg?sanitize=true" alt="Quality Checks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/produce-artifacts.yml"&gt;&lt;img src="https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Produce%20Artifacts/badge.svg?sanitize=true" alt="Build and Package" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="MIT License" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;A command-line tool to download all your iCloud photos.&lt;/li&gt; 
 &lt;li&gt;Works on Linux, Windows, and macOS; laptop, desktop, and NAS&lt;/li&gt; 
 &lt;li&gt;Available as an executable for direct downloading and through package managers/ecosystems (&lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#docker"&gt;Docker&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#pypi"&gt;PyPI&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#aur"&gt;AUR&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#npm"&gt;npm&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Developed and maintained by volunteers (we are always looking for &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/CONTRIBUTING.md"&gt;help&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/"&gt;Documentation&lt;/a&gt; for more details. Also, check &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/issues"&gt;Issues&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We aim to release new versions once a week (Friday), if there is something worth delivering.&lt;/p&gt; 
&lt;h2&gt;iCloud Prerequisites&lt;/h2&gt; 
&lt;p&gt;To make iCloud Photo Downloader work, ensure the iCloud account is configured with the following settings, otherwise Apple Servers will return an ACCESS_DENIED error:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enable Access iCloud Data on the Web:&lt;/strong&gt; On your iPhone / iPad, enable &lt;code&gt;Settings &amp;gt; Apple ID &amp;gt; iCloud &amp;gt; Access iCloud Data on the Web&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Disable Advanced Data Protection:&lt;/strong&gt; On your iPhone /iPad disable &lt;code&gt;Settings &amp;gt; Apple ID &amp;gt; iCloud &amp;gt; Advanced Data Protection&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install and Run&lt;/h2&gt; 
&lt;p&gt;There are three ways to run &lt;code&gt;icloudpd&lt;/code&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download executable for your platform from the GitHub &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/releases/tag/v1.32.2"&gt;Release&lt;/a&gt; and run it&lt;/li&gt; 
 &lt;li&gt;Use package manager to install, update, and, in some cases, run (&lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#docker"&gt;Docker&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#pypi"&gt;PyPI&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#aur"&gt;AUR&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#npm"&gt;npm&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Build and run from the source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html"&gt;Documentation&lt;/a&gt; for more details&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;!-- start features --&gt; 
&lt;ul&gt; 
 &lt;li&gt;Three modes of operation: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Copy&lt;/strong&gt; - download new photos from iCloud (default mode)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sync&lt;/strong&gt; - download new photos from iCloud and delete local files that were removed in iCloud (&lt;code&gt;--auto-delete&lt;/code&gt; option)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Move&lt;/strong&gt; - download new photos from iCloud and delete photos in iCloud (&lt;code&gt;--keep-icloud-recent-days&lt;/code&gt; option)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support for Live Photos (image and video as separate files) and RAW images (including RAW+JPEG)&lt;/li&gt; 
 &lt;li&gt;Automatic de-duplication of photos with the same name&lt;/li&gt; 
 &lt;li&gt;One time download and an option to monitor for iCloud changes continuously (&lt;code&gt;--watch-with-interval&lt;/code&gt; option)&lt;/li&gt; 
 &lt;li&gt;Optimizations for incremental runs (&lt;code&gt;--until-found&lt;/code&gt; and &lt;code&gt;--recent&lt;/code&gt; options)&lt;/li&gt; 
 &lt;li&gt;Photo metadata (EXIF) updates (&lt;code&gt;--set-exif-datetime&lt;/code&gt; option)&lt;/li&gt; 
 &lt;li&gt;... and many more (use &lt;code&gt;--help&lt;/code&gt; option to get full list)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- end features --&gt; 
&lt;h2&gt;Experimental Mode&lt;/h2&gt; 
&lt;p&gt;Some changes are added to the experimental mode before they graduate into the main package. &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/EXPERIMENTAL.md"&gt;Details&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;To keep your iCloud photo collection synchronized to your local system:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;icloudpd --directory /data --username my@email.address --watch-with-interval 3600
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] It is &lt;code&gt;icloudpd&lt;/code&gt;, not &lt;code&gt;icloud&lt;/code&gt; executable&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Synchronization logic can be adjusted with command-line parameters. Run &lt;code&gt;icloudpd --help&lt;/code&gt; to get full list.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To independently create and authorize a session (and complete 2SA/2FA validation if needed) on your local system:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;icloudpd --username my@email.address --password my_password --auth-only
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] This feature can also be used to check and verify that the session is still authenticated.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to contribute to iCloud Photos Downloader? Awesome! Check out the &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to get involved.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/ChatDev</title>
      <link>https://github.com/OpenBMB/ChatDev</link>
      <description>&lt;p&gt;ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatDev 2.0 - DevAll&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/frontend/public/media/logo.png" alt="DevAll Logo" width="500" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;A Zero-Code Multi-Agent Platform for Developing Everything&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; „Äê&lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README-zh.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;p align="center"&gt; „Äêüìö &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developers&lt;/a&gt; | üë• &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#primary-contributors"&gt;Contributors&lt;/a&gt;ÔΩú‚≠êÔ∏è &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;ChatDev 1.0 (Legacy)&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;h2&gt;üìñ Overview&lt;/h2&gt; 
&lt;p&gt;ChatDev has evolved from a specialized software development multi-agent system into a comprehensive multi-agent orchestration platform.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/main"&gt;&lt;strong&gt;ChatDev 2.0 (DevAll)&lt;/strong&gt;&lt;/a&gt; is a &lt;strong&gt;Zero-Code Multi-Agent Platform&lt;/strong&gt; for "Developing Everything". It empowers users to rapidly build and execute customized multi-agent systems through simple configuration. No coding is required‚Äîusers can define agents, workflows, and tasks to orchestrate complex scenarios such as data visualization, 3D generation, and deep research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;strong&gt;ChatDev 1.0 (Legacy)&lt;/strong&gt;&lt;/a&gt; operates as a &lt;strong&gt;Virtual Software Company&lt;/strong&gt;. It utilizes various intelligent agents (e.g., CEO, CTO, Programmer) participating in specialized functional seminars to automate the entire software development life cycle‚Äîincluding designing, coding, testing, and documenting. It serves as the foundational paradigm for communicative agent collaboration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéâ News&lt;/h2&gt; 
&lt;p&gt;‚Ä¢ &lt;strong&gt;Jan 07, 2026: üöÄ We are excited to announce the official release of ChatDev 2.0 (DevAll)!&lt;/strong&gt; This version introduces a zero-code multi-agent orchestration platform. The classic ChatDev (v1.x) has been moved to the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;code&gt;chatdev1.0&lt;/code&gt;&lt;/a&gt; branch for maintenance. More details about ChatDev 2.0 can be found on &lt;a href="https://x.com/OpenBMB/status/2008916790399701335"&gt;our official post&lt;/a&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Old News&lt;/summary&gt; 
 &lt;p&gt;‚Ä¢Sep 24, 2025: üéâ Our paper &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt; has been accepted to NeurIPS 2025. The implementation is available in the &lt;code&gt;puppeteer&lt;/code&gt; branch of this repository.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢May 26, 2025: üéâ We propose a novel puppeteer-style paradigm for multi-agent collaboration among large language model based agents. By leveraging a learnable central orchestrator optimized with reinforcement learning, our method dynamically activates and sequences agents to construct efficient, context-aware reasoning paths. This approach not only improves reasoning quality but also reduces computational costs, enabling scalable and adaptable multi-agent cooperation in complex tasks. See our paper in &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/puppeteer.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢June 25, 2024: üéâTo foster development in LLM-powered multi-agent collaborationü§ñü§ñ and related fields, the ChatDev team has curated a collection of seminal papersüìÑ presented in a &lt;a href="https://github.com/OpenBMB/ChatDev/tree/main/MultiAgentEbook"&gt;open-source&lt;/a&gt; interactive e-booküìö format. Now you can explore the latest advancements on the &lt;a href="https://thinkwee.top/multiagent_ebook"&gt;Ebook Website&lt;/a&gt; and download the &lt;a href="https://github.com/OpenBMB/ChatDev/raw/main/MultiAgentEbook/papers.csv"&gt;paper list&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ebook.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢June 12, 2024: We introduced Multi-Agent Collaboration Networks (MacNet) üéâ, which utilize directed acyclic graphs to facilitate effective task-oriented collaboration among agents through linguistic interactions ü§ñü§ñ. MacNet supports co-operation across various topologies and among more than a thousand agents without exceeding context limits. More versatile and scalable, MacNet can be considered as a more advanced version of ChatDev's chain-shaped topology. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2406.07155"&gt;https://arxiv.org/abs/2406.07155&lt;/a&gt;. This technique has been incorporated into the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/macnet"&gt;macnet&lt;/a&gt; branch, enhancing support for diverse organizational structures and offering richer solutions beyond software development (e.g., logical reasoning, data analysis, story generation, and more).&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/macnet.png" width="500" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ May 07, 2024, we introduced "Iterative Experience Refinement" (IER), a novel method where instructor and assistant agents enhance shortcut-oriented experiences to efficiently adapt to new tasks. This approach encompasses experience acquisition, utilization, propagation and elimination across a series of tasks and making the pricess shorter and efficient. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2405.04219"&gt;https://arxiv.org/abs/2405.04219&lt;/a&gt;, and this technique will soon be incorporated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ier.png" width="220" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ January 25, 2024: We have integrated Experiential Co-Learning Module into ChatDev. Please see the &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#co-tracking"&gt;Experiential Co-Learning Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ December 28, 2023: We present Experiential Co-Learning, an innovative approach where instructor and assistant agents accumulate shortcut-oriented experiences to effectively solve new tasks, reducing repetitive errors and enhancing efficiency. Check out our preprint paper at &lt;a href="https://arxiv.org/abs/2312.17025"&gt;https://arxiv.org/abs/2312.17025&lt;/a&gt; and this technique will soon be integrated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ecl.png" width="860" /&gt; &lt;/p&gt; ‚Ä¢ November 15, 2023: We launched ChatDev as a SaaS platform that enables software developers and innovative entrepreneurs to build software efficiently at a very low cost and remove the barrier to entry. Try it out at https://chatdev.modelbest.cn/. 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/saas.png" width="560" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ November 2, 2023: ChatDev is now supported with a new feature: incremental development, which allows agents to develop upon existing codes. Try &lt;code&gt;--config "incremental" --path "[source_code_directory_path]"&lt;/code&gt; to start it.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/increment.png" width="700" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ October 26, 2023: ChatDev is now supported with Docker for safe execution (thanks to contribution from &lt;a href="https://github.com/ManindraDeMel"&gt;ManindraDeMel&lt;/a&gt;). Please see &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#docker-start"&gt;Docker Start Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/docker.png" width="400" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 25, 2023: The &lt;strong&gt;Git&lt;/strong&gt; mode is now available, enabling the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt; to utilize Git for version control. To enable this feature, simply set &lt;code&gt;"git_management"&lt;/code&gt; to &lt;code&gt;"True"&lt;/code&gt; in &lt;code&gt;ChatChainConfig.json&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#git-mode"&gt;guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/github.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 20, 2023: The &lt;strong&gt;Human-Agent-Interaction&lt;/strong&gt; mode is now available! You can get involved with the ChatDev team by playing the role of reviewer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/reviewer.png" height="20" /&gt; and making suggestions to the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt;; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Human"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#human-agent-interaction"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/Gomoku_HumanAgentInteraction_20230920135038"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/Human_intro.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;‚Ä¢ September 1, 2023: The &lt;strong&gt;Art&lt;/strong&gt; mode is available now! You can activate the designer agent &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/designer.png" height="20" /&gt; to generate images used in the software; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Art"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#art"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/gomokugameArtExample_THUNLP_20230831122822"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ August 28, 2023: The system is publicly available.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ August 17, 2023: The v1.0.0 version was ready for release.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ July 30, 2023: Users can customize ChatChain, Phasea and Role settings. Additionally, both online Log mode and replay mode are now supported.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ July 16, 2023: The &lt;a href="https://arxiv.org/abs/2307.07924"&gt;preprint paper&lt;/a&gt; associated with this project was published.&lt;/p&gt; 
 &lt;p&gt;‚Ä¢ June 30, 2023: The initial version of the ChatDev repository was released.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üìã Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: macOS / Linux / WSL / Windows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Node.js&lt;/strong&gt;: 18+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Package Manager&lt;/strong&gt;: &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì¶ Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backend Dependencies&lt;/strong&gt; (Python managed by &lt;code&gt;uv&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend Dependencies&lt;/strong&gt; (Vite + Vue 3):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend &amp;amp;&amp;amp; npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;‚ö°Ô∏è Run the Application&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Backend&lt;/strong&gt; :&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Run from the project root
uv run python server_main.py --port 6400 --reload
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Frontend&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
VITE_API_BASE_URL=http://localhost:6400 npm run dev
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Then access the Web Console at &lt;strong&gt;&lt;a href="http://localhost:5173"&gt;http://localhost:5173&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üîë Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt;: Create a &lt;code&gt;.env&lt;/code&gt; file in the project root.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Keys&lt;/strong&gt;: Set &lt;code&gt;API_KEY&lt;/code&gt; and &lt;code&gt;BASE_URL&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt; for your LLM provider.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;YAML placeholders&lt;/strong&gt;: Use &lt;code&gt;${VAR}&lt;/code&gt;Ôºàe.g., &lt;code&gt;${API_KEY}&lt;/code&gt;Ôºâin configuration files to reference these variables.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° How to Use&lt;/h2&gt; 
&lt;h3&gt;üñ•Ô∏è Web Console&lt;/h3&gt; 
&lt;p&gt;The DevAll interface provides a seamless experience for both construction and execution&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;: Comprehensive step-by-step guides and documentation integrated directly into the platform to help you get started quickly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/tutorial-en.png" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: A visual canvas to design your multi-agent systems. Configure node parameters, define context flows, and orchestrate complex agent interactions with drag-and-drop ease.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/workflow.gif" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Launch&lt;/strong&gt;: Initiate workflows, monitor real-time logs, inspect intermediate artifacts, and provide human-in-the-loop feedback.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/launch.gif" /&gt; 
&lt;h3&gt;üß∞ Python SDK&lt;/h3&gt; 
&lt;p&gt;For automation and batch processing, use our lightweight Python SDK to execute workflows programmatically and retrieve results directly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from runtime.sdk import run_workflow

# Execute a workflow and get the final node message
result = run_workflow(
    yaml_file="yaml_instance/demo.yaml",
    task_prompt="Summarize the attached document in one sentence.",
    attachments=["/path/to/document.pdf"],
    variables={"API_KEY": "sk-xxxx"} # Override .env variables if needed
)

if result.final_message:
    print(f"Output: {result.final_message.text_content()}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a id="developers"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚öôÔ∏è For Developers&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;For secondary development and extensions, please proceed with this section.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Extend DevAll with new nodes, providers, and tools. The project is organized into a modular structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Core Systems&lt;/strong&gt;: &lt;code&gt;server/&lt;/code&gt; hosts the FastAPI backend, while &lt;code&gt;runtime/&lt;/code&gt; manages agent abstraction and tool execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Orchestration&lt;/strong&gt;: &lt;code&gt;workflow/&lt;/code&gt; handles the multi-agent logic, driven by configurations in &lt;code&gt;entity/&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: &lt;code&gt;frontend/&lt;/code&gt; contains the Vue 3 Web Console.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensibility&lt;/strong&gt;: &lt;code&gt;functions/&lt;/code&gt; is the place for custom Python tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Relevant reference documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/index.md"&gt;Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core Modules&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/workflow_authoring.md"&gt;Workflow Authoring&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/memory.md"&gt;Memory&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/tooling/index.md"&gt;Tooling&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåü Featured Workflows&lt;/h2&gt; 
&lt;p&gt;We provide robust, out-of-the-box templates for common scenarios. All runnable workflow configs are located in &lt;code&gt;yaml_instance/&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Demos&lt;/strong&gt;: Files named &lt;code&gt;demo_*.yaml&lt;/code&gt; showcase specific features or modules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Implementations&lt;/strong&gt;: Files named directly (e.g., &lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;) are full in-house or recreated workflows. As follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìã Workflow Collection&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Category&lt;/th&gt; 
   &lt;th align="left"&gt;Workflow&lt;/th&gt; 
   &lt;th align="left"&gt;Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üìà Data Visualization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;data_visualization_basic.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;data_visualization_enhanced.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/data_analysis/data_analysis.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Create 4‚Äì6 high-quality PNG charts for my large real-estate transactions dataset."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üõ†Ô∏è 3D Generation&lt;/strong&gt;&lt;br /&gt;&lt;em&gt;(Requires &lt;a href="https://www.blender.org/"&gt;Blender&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ahujasid/blender-mcp"&gt;blender-mcp&lt;/a&gt;)&lt;/em&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;blender_3d_builder_simple.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_3d_builder_hub.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_scientific_illustration.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/3d_generation/3d.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please build a Christmas tree."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üéÆ Game Dev&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;GameDev_v1.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/game_development/game.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please help me design and develop a Tank Battle game."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üìö Deep Research&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;deep_research_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/deep_research/deep_research.gif" width="85%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Research about recent advances in the field of LLM-based agent RL"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;üéì Teach Video&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;teach_video.yaml&lt;/code&gt; (Please run command &lt;code&gt;uv add manim&lt;/code&gt; before running this workflow)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/video_generation/video.gif" width="140%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"ËÆ≤‰∏Ä‰∏ã‰ªÄ‰πàÊòØÂá∏‰ºòÂåñ"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üí° Usage Guide&lt;/h3&gt; 
&lt;p&gt;For those implementations, you can use the &lt;strong&gt;Launch&lt;/strong&gt; tab to execute them.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Select&lt;/strong&gt;: Choose a workflow in the &lt;strong&gt;Launch&lt;/strong&gt; tab.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt;: Upload necessary files (e.g., &lt;code&gt;.csv&lt;/code&gt; for data analysis) if required.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: Enter your request (e.g., &lt;em&gt;"Visualize the sales trends"&lt;/em&gt; or &lt;em&gt;"Design a snake game"&lt;/em&gt;).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding new workflow templates, or sharing high-quality cases/artifacts produced by DevAll, your help is much appreciated. Feel free to contribute by submitting &lt;strong&gt;Issues&lt;/strong&gt; or &lt;strong&gt;Pull Requests&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;By contributing to DevAll, you'll be recognized in our &lt;strong&gt;Contributors&lt;/strong&gt; list below. Check out our &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developer Guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;h3&gt;üë• Contributors&lt;/h3&gt; 
&lt;h4&gt;Primary Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/NA-Wen"&gt;&lt;img src="https://github.com/NA-Wen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;NA-Wen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/zxrys"&gt;&lt;img src="https://github.com/zxrys.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;zxrys&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/swugi"&gt;&lt;img src="https://github.com/swugi.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;swugi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/huatl98"&gt;&lt;img src="https://github.com/huatl98.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;huatl98&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h4&gt;Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/shiowen"&gt;&lt;img src="https://github.com/shiowen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shiowen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/kilo2127"&gt;&lt;img src="https://github.com/kilo2127.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;kilo2127&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/AckerlyLau"&gt;&lt;img src="https://github.com/AckerlyLau.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AckerlyLau&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ü§ù Acknowledgments&lt;/h2&gt; 
&lt;p&gt;&lt;a href="http://nlp.csai.tsinghua.edu.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/thunlp.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://modelbest.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/modelbest.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/AgentVerse/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/agentverse.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/RepoAgent"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/repoagent.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://app.commanddash.io/agent?github=https://github.com/OpenBMB/ChatDev"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/CommandDash.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/www.teachmaster.cn"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/teachmaster.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://github.com/OpenBMB/AppCopilot"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/appcopilot.png" height="50pt" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üîé Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@article{chatdev,
    title = {ChatDev: Communicative Agents for Software Development},
    author = {Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2307.07924},
    url = {https://arxiv.org/abs/2307.07924},
    year = {2023}
}

@article{colearning,
    title = {Experiential Co-Learning of Software-Developing Agents},
    author = {Chen Qian and Yufan Dang and Jiahao Li and Wei Liu and Zihao Xie and Yifei Wang and Weize Chen and Cheng Yang and Xin Cong and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2312.17025},
    url = {https://arxiv.org/abs/2312.17025},
    year = {2023}
}

@article{macnet,
    title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
    author={Chen Qian and Zihao Xie and Yifei Wang and Wei Liu and Yufan Dang and Zhuoyun Du and Weize Chen and Cheng Yang and Zhiyuan Liu and Maosong Sun}
    journal={arXiv preprint arXiv:2406.07155},
    url = {https://arxiv.org/abs/2406.07155},
    year={2024}
}

@article{iagents,
    title={Autonomous Agents for Collaborative Task under Information Asymmetry},
    author={Wei Liu and Chenxi Wang and Yifei Wang and Zihao Xie and Rennai Qiu and Yufan Dnag and Zhuoyun Du and Weize Chen and Cheng Yang and Chen Qian},
    journal={arXiv preprint arXiv:2406.14928},
    url = {https://arxiv.org/abs/2406.14928},
    year={2024}
}

@article{puppeteer,
      title={Multi-Agent Collaboration via Evolving Orchestration}, 
      author={Yufan Dang and Chen Qian and Xueheng Luo and Jingru Fan and Zihao Xie and Ruijie Shi and Weize Chen and Cheng Yang and Xiaoyin Che and Ye Tian and Xuantang Xiong and Lei Han and Zhiyuan Liu and Maosong Sun},
      journal={arXiv preprint arXiv:2505.19591},
      url={https://arxiv.org/abs/2505.19591},
      year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üì¨ Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:qianc62@gmail.com"&gt;qianc62@gmail.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hacksider/Deep-Live-Cam</title>
      <link>https://github.com/hacksider/Deep-Live-Cam</link>
      <description>&lt;p&gt;real time face swap and one-click video deepfake with only a single image&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Deep-Live-Cam 2.0.1c&lt;/h1&gt; 
&lt;p align="center"&gt; Real-time face swap and video deepfake with a single click and only a single image. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/11395" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11395" alt="hacksider%2FDeep-Live-Cam | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/demo.gif" alt="Demo GIF" width="800" /&gt; &lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.&lt;/p&gt; 
&lt;p&gt;We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ethical Use: Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.&lt;/p&gt; 
&lt;p&gt;Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.&lt;/p&gt; 
&lt;h2&gt;Exclusive v2.4 Quick Start - Pre-built (Windows/Mac Silicon)&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/Download.png" width="285" height="77" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;h5&gt;This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you'll receive special priority support.&lt;/h5&gt; &lt;h6&gt;These Pre-builts are perfect for non-technical users or those who don't have time to, or can't manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually.&lt;/h6&gt; &lt;h2&gt;TLDR; Live Deepfake in just 3 Clicks&lt;/h2&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6" alt="easysteps" /&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Select a face&lt;/li&gt; 
  &lt;li&gt;Select which camera to use&lt;/li&gt; 
  &lt;li&gt;Press live!&lt;/li&gt; 
 &lt;/ol&gt; &lt;h2&gt;Features &amp;amp; Uses - Everything is in real-time&lt;/h2&gt; &lt;h3&gt;Mouth Mask&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Retain your original mouth for accurate movement using Mouth Mask&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/ludwig.gif" alt="resizable-gif" /&gt; &lt;/p&gt; &lt;h3&gt;Face Mapping&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Use different faces on multiple subjects simultaneously&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/streamers.gif" alt="face_mapping_source" /&gt; &lt;/p&gt; &lt;h3&gt;Your Movie, Your Face&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Watch movies with any face in real-time&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/movie.gif" alt="movie" /&gt; &lt;/p&gt; &lt;h3&gt;Live Show&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Run Live shows and performances&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/live_show.gif" alt="show" /&gt; &lt;/p&gt; &lt;h3&gt;Memes&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Create Your Most Viral Meme Yet&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/meme.gif" alt="show" width="450" /&gt; &lt;br /&gt; &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt; &lt;/p&gt; &lt;h3&gt;Omegle&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Surprise people on Omegle&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; 
  &lt;video src="https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0" width="450" controls&gt;&lt;/video&gt; &lt;/p&gt; &lt;h2&gt;Installation (Manual)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.&lt;/strong&gt;&lt;/p&gt; &lt;/a&gt;
&lt;details&gt;
 &lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;summary&gt;Click to see the process&lt;/summary&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;This is more likely to work on your computer but will be slower as it utilizes the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Set up Your Platform&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python (3.11 recommended)&lt;/li&gt; 
   &lt;li&gt;pip&lt;/li&gt; 
   &lt;li&gt;git&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=OlNWCpFdVMA"&gt;ffmpeg&lt;/a&gt; - &lt;code&gt;iex (irm ffmpeg.tc.ht)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/"&gt;Visual Studio 2022 Runtimes (Windows)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;strong&gt;2. Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Download the Models&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth"&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx"&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Place these files in the "&lt;strong&gt;models&lt;/strong&gt;" folder.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4. Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We highly recommend using a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; 
 &lt;p&gt;For Windows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Linux:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) requires specific setup:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;** In case something goes wrong and you need to reinstall the virtual environment **&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Run:&lt;/strong&gt; If you don't have a GPU, you can run Deep-Live-Cam using &lt;code&gt;python run.py&lt;/code&gt;. Note that initial execution will download models (~300MB).&lt;/p&gt; 
 &lt;h3&gt;GPU Acceleration&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;CUDA Execution Provider (Nvidia)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/cuda-12-8-0-download-archive"&gt;CUDA Toolkit 12.8.0&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/rdp/cudnn-archive"&gt;cuDNN v8.9.7 for CUDA 12.x&lt;/a&gt; (required for onnxruntime-gpu): 
   &lt;ul&gt; 
    &lt;li&gt;Download cuDNN v8.9.7 for CUDA 12.x&lt;/li&gt; 
    &lt;li&gt;Make sure the cuDNN bin directory is in your system PATH&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider cuda
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Silicon)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) specific installation:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Make sure you've completed the macOS setup above using Python 3.10.&lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage (important: specify Python 3.10):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3.10 run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important Notes for macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You &lt;strong&gt;must&lt;/strong&gt; use Python 3.10, not newer versions like 3.11 or 3.13&lt;/li&gt; 
  &lt;li&gt;Always run with &lt;code&gt;python3.10&lt;/code&gt; command not just &lt;code&gt;python&lt;/code&gt; if you have multiple Python versions installed&lt;/li&gt; 
  &lt;li&gt;If you get error about &lt;code&gt;_tkinter&lt;/code&gt; missing, reinstall the tkinter package: &lt;code&gt;brew reinstall python-tk@3.10&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;If you get model loading errors, check that your models are in the correct folder&lt;/li&gt; 
  &lt;li&gt;If you encounter conflicts with other Python versions, consider uninstalling them: &lt;pre&gt;&lt;code class="language-bash"&gt;# List all installed Python versions
brew list | grep python

# Uninstall conflicting versions if needed
brew uninstall --ignore-dependencies python@3.11 python@3.13

# Keep only Python 3.11
brew cleanup
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Legacy)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;DirectML Execution Provider (Windows)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider directml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;OpenVINO‚Ñ¢ Execution Provider (Intel)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider openvino
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. Image/Video Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose a source face image and a target image/video.&lt;/li&gt; 
 &lt;li&gt;Click "Start".&lt;/li&gt; 
 &lt;li&gt;The output will be saved in a directory named after the target video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Webcam Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Select a source face image.&lt;/li&gt; 
 &lt;li&gt;Click "Live".&lt;/li&gt; 
 &lt;li&gt;Wait for the preview to appear (10-30 seconds).&lt;/li&gt; 
 &lt;li&gt;Use a screen capture tool like OBS to stream.&lt;/li&gt; 
 &lt;li&gt;To change the face, select a new source image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Command Line Arguments (Unmaintained)&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program's version number and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; 
&lt;h2&gt;Press&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We are always open to criticism and are ready to improve, that's why we didn't cherry-pick anything.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/"&gt;&lt;em&gt;"Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger"&lt;/em&gt;&lt;/a&gt; - Ars Technica&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/"&gt;&lt;em&gt;"Thanks Deep Live Cam, shapeshifters are among us now"&lt;/em&gt;&lt;/a&gt; - Dataconomy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story"&gt;&lt;em&gt;"This free AI tool lets you become anyone during video-calls"&lt;/em&gt;&lt;/a&gt; - NewsBytes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying"&gt;&lt;em&gt;"OK, this viral AI live stream software is truly terrifying"&lt;/em&gt;&lt;/a&gt; - Creative Bloq&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/"&gt;&lt;em&gt;"Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo"&lt;/em&gt;&lt;/a&gt; - PetaPixel&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.techeblog.com/deep-live-cam-ai-transform-face/"&gt;&lt;em&gt;"Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included"&lt;/em&gt;&lt;/a&gt; - TechEBlog&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/"&gt;&lt;em&gt;"An AI tool that "makes you look like anyone" during a video call is going viral online"&lt;/em&gt;&lt;/a&gt; - Telegrafi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts"&gt;&lt;em&gt;"This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts"&lt;/em&gt;&lt;/a&gt; - Emerge&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/"&gt;&lt;em&gt;"New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces"&lt;/em&gt;&lt;/a&gt; - Digital Music News&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/"&gt;&lt;em&gt;"This real-time webcam deepfake tool raises alarms about the future of identity theft"&lt;/em&gt;&lt;/a&gt; - DIYPhotography&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?time_continue=1074&amp;amp;v=py4Tc-Y8BcY"&gt;&lt;em&gt;"That's Crazy, Oh God. That's Fucking Freaky Dude... That's So Wild Dude"&lt;/em&gt;&lt;/a&gt; - SomeOrdinaryGamers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;amp;t=2686"&gt;&lt;em&gt;"Alright look look look, now look chat, we can do any face we want to look like chat"&lt;/em&gt;&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wnCghLjqv3s&amp;amp;t=551s"&gt;&lt;em&gt;"They do a pretty good job matching poses, expression and even the lighting"&lt;/em&gt;&lt;/a&gt; - TechLinked (LTT)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html"&gt;&lt;em&gt;"Als Sean Connery an der Redaktionskonferenz teilnahm"&lt;/em&gt;&lt;/a&gt; - Golem.de (German)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/JbUPRmXRUtE?t=3964"&gt;&lt;em&gt;"What the F&lt;/em&gt;**! Why do I look like Vinny Jr? I look exactly like Vinny Jr!? No, this shit is crazy! Bro This is F*** Crazy! "*&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;: for making video-related operations easy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/henryruhs"&gt;Henry&lt;/a&gt;: One of the major contributor in this repo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deepinsight"&gt;deepinsight&lt;/a&gt;: for their &lt;a href="https://github.com/deepinsight/insightface"&gt;insightface&lt;/a&gt; project which provided a well-made library and models. Please be reminded that the &lt;a href="https://github.com/deepinsight/insightface?tab=readme-ov-file#license"&gt;use of the model is for non-commercial research purposes only&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/havok2-htwo"&gt;havok2-htwo&lt;/a&gt;: for sharing the code for webcam&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GosuDRM"&gt;GosuDRM&lt;/a&gt;: for the open version of roop&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pereiraroland26"&gt;pereiraroland26&lt;/a&gt;: Multiple faces support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vic4key"&gt;vic4key&lt;/a&gt;: For supporting/contributing to this project&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kier007"&gt;kier007&lt;/a&gt;: for improving the user experience&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/qitianai"&gt;qitianai&lt;/a&gt;: for multi-lingual support&lt;/li&gt; 
 &lt;li&gt;and &lt;a href="https://github.com/hacksider/Deep-Live-Cam/graphs/contributors"&gt;all developers&lt;/a&gt; behind libraries used in this project.&lt;/li&gt; 
 &lt;li&gt;Footnote: Please be informed that the base author of the code is &lt;a href="https://github.com/s0md3v/roop"&gt;s0md3v&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All the wonderful users who helped make this project go viral by starring the repo ‚ù§Ô∏è&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/hacksider/Deep-Live-Cam/stargazers"&gt;&lt;img src="https://reporoster.com/stars/hacksider/Deep-Live-Cam" alt="Stargazers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Stars to the Moon üöÄ&lt;/h2&gt; 
&lt;a href="https://star-history.com/#hacksider/deep-live-cam&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>crewAIInc/crewAI</title>
      <link>https://github.com/crewAIInc/crewAI</link>
      <description>&lt;p&gt;Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/crewai_logo.png" width="600px" alt="Open source Multi-AI Agent orchestration framework" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"&gt; &lt;a href="https://trendshift.io/repositories/11239" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/11239" alt="crewAIInc%2FcrewAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://crewai.com"&gt;Homepage&lt;/a&gt; ¬∑ &lt;a href="https://docs.crewai.com"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://app.crewai.com"&gt;Start Cloud Trial&lt;/a&gt; ¬∑ &lt;a href="https://blog.crewai.com"&gt;Blog&lt;/a&gt; ¬∑ &lt;a href="https://community.crewai.com"&gt;Forum&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://img.shields.io/github/stars/crewAIInc/crewAI" alt="GitHub Repo stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/network/members"&gt; &lt;img src="https://img.shields.io/github/forks/crewAIInc/crewAI" alt="GitHub forks" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/issues"&gt; &lt;img src="https://img.shields.io/github/issues/crewAIInc/crewAI" alt="GitHub issues" /&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/pulls"&gt; &lt;img src="https://img.shields.io/github/issues-pr/crewAIInc/crewAI" alt="GitHub pull requests" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="License: MIT" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/v/crewai" alt="PyPI version" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/dm/crewai" alt="PyPI downloads" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/crewAIInc"&gt; &lt;img src="https://img.shields.io/twitter/follow/crewAIInc?style=social" alt="Twitter Follow" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Fast and Flexible Multi-Agent Automation Framework&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;CrewAI is a lean, lightning-fast Python framework built entirely from scratch‚Äîcompletely &lt;strong&gt;independent of LangChain or other agent frameworks&lt;/strong&gt;. It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Crews&lt;/strong&gt;: Optimize for autonomy and collaborative intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Flows&lt;/strong&gt;: The &lt;strong&gt;enterprise and production architecture&lt;/strong&gt; for building and deploying multi-agent systems. Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With over 100,000 developers certified through our community courses at &lt;a href="https://learn.crewai.com"&gt;learn.crewai.com&lt;/a&gt;, CrewAI is rapidly becoming the standard for enterprise-ready AI automation.&lt;/p&gt; 
&lt;h1&gt;CrewAI AMP Suite&lt;/h1&gt; 
&lt;p&gt;CrewAI AMP Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.&lt;/p&gt; 
&lt;p&gt;You can try one part of the suite the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane for free&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Crew Control Plane Key Features:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tracing &amp;amp; Observability&lt;/strong&gt;: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified Control Plane&lt;/strong&gt;: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integrations&lt;/strong&gt;: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Security&lt;/strong&gt;: Built-in robust security and compliance measures ensuring safe deployment and management.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Actionable Insights&lt;/strong&gt;: Real-time analytics and reporting to optimize performance and decision-making.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;24/7 Support&lt;/strong&gt;: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-premise and Cloud Deployment Options&lt;/strong&gt;: Deploy CrewAI AMP on-premise or in the cloud, depending on your security and compliance requirements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI AMP is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient, intelligent automations.&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#why-crewai"&gt;Why CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#understanding-flows-and-crews"&gt;Understanding Flows and Crews&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;CrewAI vs LangGraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#examples"&gt;Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#quick-tutorial"&gt;Quick Tutorial&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#write-job-descriptions"&gt;Write Job Descriptions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#trip-planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#stock-analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#using-crews-and-flows-together"&gt;Using Crews and Flows Together&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#connecting-your-crew-to-a-model"&gt;Connecting Your Crew to a Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;How CrewAI Compares&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#frequently-asked-questions-faq"&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#telemetry"&gt;Telemetry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why CrewAI?&lt;/h2&gt; 
&lt;div align="center" style="margin-bottom: 30px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/asset.png" alt="CrewAI Logo" width="100%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone Framework&lt;/strong&gt;: Built from scratch, independent of LangChain or any other agent framework.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Optimized for speed and minimal resource usage, enabling faster execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Low Level Customization&lt;/strong&gt;: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ideal for Every Use Case&lt;/strong&gt;: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Robust Community&lt;/strong&gt;: Backed by a rapidly growing community of over &lt;strong&gt;100,000 certified&lt;/strong&gt; developers offering comprehensive support and resources.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Setup and run your first CrewAI agents by following this tutorial.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-kSOTtYzgEw" title="CrewAI Getting Started Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg" alt="CrewAI Getting Started Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;p&gt;Learning Resources&lt;/p&gt; 
&lt;p&gt;Learn CrewAI through our comprehensive courses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/"&gt;Multi AI Agent Systems with CrewAI&lt;/a&gt; - Master the fundamentals of multi-agent systems&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/"&gt;Practical Multi AI Agents and Advanced Use Cases&lt;/a&gt; - Deep dive into advanced implementations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Understanding Flows and Crews&lt;/h3&gt; 
&lt;p&gt;CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Crews&lt;/strong&gt;: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Natural, autonomous decision-making between agents&lt;/li&gt; 
   &lt;li&gt;Dynamic task delegation and collaboration&lt;/li&gt; 
   &lt;li&gt;Specialized roles with defined goals and expertise&lt;/li&gt; 
   &lt;li&gt;Flexible problem-solving approaches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flows&lt;/strong&gt;: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fine-grained control over execution paths for real-world scenarios&lt;/li&gt; 
   &lt;li&gt;Secure, consistent state management between tasks&lt;/li&gt; 
   &lt;li&gt;Clean integration of AI agents with production Python code&lt;/li&gt; 
   &lt;li&gt;Conditional branching for complex business logic&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build complex, production-grade applications&lt;/li&gt; 
 &lt;li&gt;Balance autonomy with precise control&lt;/li&gt; 
 &lt;li&gt;Handle sophisticated real-world scenarios&lt;/li&gt; 
 &lt;li&gt;Maintain clean, maintainable code structure&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting Started with Installation&lt;/h3&gt; 
&lt;p&gt;To get started with CrewAI, follow these simple steps:&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;Ensure you have Python &amp;gt;=3.10 &amp;lt;3.14 installed on your system. CrewAI uses &lt;a href="https://docs.astral.sh/uv/"&gt;UV&lt;/a&gt; for dependency management and package handling, offering a seamless setup and execution experience.&lt;/p&gt; 
&lt;p&gt;First, install CrewAI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to install the 'crewai' package along with its optional features that include additional tools for agents, you can do so by using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command above installs the basic package and also adds extra components which require more dependencies to function.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting Dependencies&lt;/h3&gt; 
&lt;p&gt;If you encounter issues during installation or usage, here are some common solutions:&lt;/p&gt; 
&lt;h4&gt;Common Issues&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ModuleNotFoundError: No module named 'tiktoken'&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Install tiktoken explicitly: &lt;code&gt;uv pip install 'crewai[embeddings]'&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If using embedchain or other tools: &lt;code&gt;uv pip install 'crewai[tools]'&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Failed building wheel for tiktoken&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure Rust compiler is installed (see installation steps above)&lt;/li&gt; 
   &lt;li&gt;For Windows: Verify Visual C++ Build Tools are installed&lt;/li&gt; 
   &lt;li&gt;Try upgrading pip: &lt;code&gt;uv pip install --upgrade pip&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If issues persist, use a pre-built wheel: &lt;code&gt;uv pip install tiktoken --prefer-binary&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. Setting Up Your Crew with the YAML Configuration&lt;/h3&gt; 
&lt;p&gt;To create a new CrewAI project, run the following CLI (Command Line Interface) command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew &amp;lt;project_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command creates a new project folder with the following structure:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my_project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ my_project/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ main.py
        ‚îú‚îÄ‚îÄ crew.py
        ‚îú‚îÄ‚îÄ tools/
        ‚îÇ   ‚îú‚îÄ‚îÄ custom_tool.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ config/
            ‚îú‚îÄ‚îÄ agents.yaml
            ‚îî‚îÄ‚îÄ tasks.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now start developing your crew by editing the files in the &lt;code&gt;src/my_project&lt;/code&gt; folder. The &lt;code&gt;main.py&lt;/code&gt; file is the entry point of the project, the &lt;code&gt;crew.py&lt;/code&gt; file is where you define your crew, the &lt;code&gt;agents.yaml&lt;/code&gt; file is where you define your agents, and the &lt;code&gt;tasks.yaml&lt;/code&gt; file is where you define your tasks.&lt;/p&gt; 
&lt;h4&gt;To customize your project, you can:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/agents.yaml&lt;/code&gt; to define your agents.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/tasks.yaml&lt;/code&gt; to define your tasks.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/crew.py&lt;/code&gt; to add your own logic, tools, and specific arguments.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/main.py&lt;/code&gt; to add custom inputs for your agents and tasks.&lt;/li&gt; 
 &lt;li&gt;Add your environment variables into the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Example of a simple crew with a sequential process:&lt;/h4&gt; 
&lt;p&gt;Instantiate your crew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew latest-ai-development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modify the files as needed to fit your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;agents.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/agents.yaml
researcher:
  role: &amp;gt;
    {topic} Senior Data Researcher
  goal: &amp;gt;
    Uncover cutting-edge developments in {topic}
  backstory: &amp;gt;
    You're a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &amp;gt;
    {topic} Reporting Analyst
  goal: &amp;gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &amp;gt;
    You're a meticulous analyst with a keen eye for detail. You're known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;tasks.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/tasks.yaml
research_task:
  description: &amp;gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &amp;gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &amp;gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &amp;gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without '```'
  agent: reporting_analyst
  output_file: report.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;crew.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	"""LatestAiDevelopment crew"""
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['researcher'],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['reporting_analyst'],
			verbose=True
		)

	@task
	def research_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['research_task'],
		)

	@task
	def reporting_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['reporting_task'],
			output_file='report.md'
		)

	@crew
	def crew(self) -&amp;gt; Crew:
		"""Creates the LatestAiDevelopment crew"""
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;main.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    """
    Run the crew.
    """
    inputs = {
        'topic': 'AI Agents'
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Running Your Crew&lt;/h3&gt; 
&lt;p&gt;Before running your crew, make sure you have the following keys set as environment variables in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An &lt;a href="https://platform.openai.com/account/api-keys"&gt;OpenAI API key&lt;/a&gt; (or other LLM API key): &lt;code&gt;OPENAI_API_KEY=sk-...&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; API key: &lt;code&gt;SERPER_API_KEY=YOUR_KEY_HERE&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd my_project
crewai install (Optional)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run your crew, execute the following command in the root of your project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python src/my_project/main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If an error happens due to the usage of poetry, please run the following command to update your crewai package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai update
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the output in the console and the &lt;code&gt;report.md&lt;/code&gt; file should be created in the root of your project with the full final report.&lt;/p&gt; 
&lt;p&gt;In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. &lt;a href="https://docs.crewai.com/core-concepts/Processes/"&gt;See more about the processes here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise control‚Äîfree from the complexity and limitations found in other agent frameworks.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone &amp;amp; Lean&lt;/strong&gt;: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible &amp;amp; Precise&lt;/strong&gt;: Easily orchestrate autonomous agents through intuitive &lt;a href="https://docs.crewai.com/concepts/crews"&gt;Crews&lt;/a&gt; or precise &lt;a href="https://docs.crewai.com/concepts/flows"&gt;Flows&lt;/a&gt;, achieving perfect balance for your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt;: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Customization&lt;/strong&gt;: Tailor every aspect‚Äîfrom high-level workflows down to low-level internal prompts and agent behaviors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Performance&lt;/strong&gt;: Consistent results across simple tasks and complex, enterprise-level automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Thriving Community&lt;/strong&gt;: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;You can test different real life examples of AI crews in the &lt;a href="https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file"&gt;CrewAI-examples repo&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/landing_page_generator"&gt;Landing Page Generator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.crewai.com/how-to/Human-Input-on-Execution"&gt;Having Human input on the execution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Tutorial&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tnejrr-0a94" title="CrewAI Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg" alt="CrewAI Tutorial" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Write Job Descriptions&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/job-posting"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=u98wEMz-9to" title="Jobs postings"&gt;&lt;img src="https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg" alt="Jobs postings" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Trip Planner&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=xis7rWp-hjs" title="Trip Planner"&gt;&lt;img src="https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg" alt="Trip Planner" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Stock Analysis&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=e0Uj4yWdaAg" title="Stock Analysis"&gt;&lt;img src="https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg" alt="Stock Analysis" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Using Crews and Flows Together&lt;/h3&gt; 
&lt;p&gt;CrewAI's power truly shines when combining Crews with Flows to create sophisticated automation pipelines. CrewAI flows support logical operators like &lt;code&gt;or_&lt;/code&gt; and &lt;code&gt;and_&lt;/code&gt; to combine multiple conditions. This can be used with &lt;code&gt;@start&lt;/code&gt;, &lt;code&gt;@listen&lt;/code&gt;, or &lt;code&gt;@router&lt;/code&gt; decorators to create complex triggering conditions.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;or_&lt;/code&gt;: Triggers when any of the specified conditions are met.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;and_&lt;/code&gt;Triggers when all of the specified conditions are met.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here's how you can orchestrate multiple Crews within a Flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = "neutral"
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = "analyzing"
        return {"sector": "tech", "timeframe": "1W"}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role="Senior Market Analyst",
            goal="Conduct deep market analysis with expert insight",
            backstory="You're a veteran analyst known for identifying subtle market patterns"
        )
        researcher = Agent(
            role="Data Researcher",
            goal="Gather and validate supporting market data",
            backstory="You excel at finding and correlating multiple data sources"
        )

        analysis_task = Task(
            description="Analyze {sector} sector data for the past {timeframe}",
            expected_output="Detailed market analysis with confidence score",
            agent=analyst
        )
        research_task = Task(
            description="Find supporting data to validate the analysis",
            expected_output="Corroborating evidence and potential contradictions",
            agent=researcher
        )

        # Demonstrate crew autonomy
        analysis_crew = Crew(
            agents=[analyst, researcher],
            tasks=[analysis_task, research_task],
            process=Process.sequential,
            verbose=True
        )
        return analysis_crew.kickoff(inputs=market_data)  # Pass market_data as named inputs

    @router(analyze_with_crew)
    def determine_next_steps(self):
        # Show flow control with conditional routing
        if self.state.confidence &amp;gt; 0.8:
            return "high_confidence"
        elif self.state.confidence &amp;gt; 0.5:
            return "medium_confidence"
        return "low_confidence"

    @listen("high_confidence")
    def execute_strategy(self):
        # Demonstrate complex decision making
        strategy_crew = Crew(
            agents=[
                Agent(role="Strategy Expert",
                      goal="Develop optimal market strategy")
            ],
            tasks=[
                Task(description="Create detailed strategy based on analysis",
                     expected_output="Step-by-step action plan")
            ]
        )
        return strategy_crew.kickoff()

    @listen(or_("medium_confidence", "low_confidence"))
    def request_additional_analysis(self):
        self.state.recommendations.append("Gather more data")
        return "Additional analysis required"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This example demonstrates how to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use Python code for basic data operations&lt;/li&gt; 
 &lt;li&gt;Create and execute Crews as steps in your workflow&lt;/li&gt; 
 &lt;li&gt;Use Flow decorators to manage the sequence of operations&lt;/li&gt; 
 &lt;li&gt;Implement conditional branching based on Crew results&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Connecting Your Crew to a Model&lt;/h2&gt; 
&lt;p&gt;CrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several other ways to allow your agents to connect to models. For example, you can configure your agents to use a local model via the Ollama tool.&lt;/p&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;Connect CrewAI to LLMs&lt;/a&gt; page for details on configuring your agents' connections to models.&lt;/p&gt; 
&lt;h2&gt;How CrewAI Compares&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CrewAI's Advantage&lt;/strong&gt;: CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture. The framework excels at both high-level orchestration and low-level customization, enabling complex, production-grade systems with granular control.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: While LangGraph provides a foundation for building agent workflows, its approach requires significant boilerplate code and complex state management patterns. The framework's tight coupling with LangChain can limit flexibility when implementing custom agent behaviors or integrating with external systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;P.S. CrewAI demonstrates significant performance advantages over LangGraph, executing 5.76x faster in certain cases like this QA task example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/QA%20Agent"&gt;see comparison&lt;/a&gt;) while achieving higher evaluation scores with faster completion times in certain coding tasks, like in this example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/raw/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/Coding%20Assistant/coding_assistant_eval.ipynb"&gt;detailed analysis&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Autogen&lt;/strong&gt;: While Autogen excels at creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents' interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ChatDev&lt;/strong&gt;: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;CrewAI is open-source and we welcome contributions. If you're looking to contribute, please:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fork the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your feature.&lt;/li&gt; 
 &lt;li&gt;Add your feature or improvement.&lt;/li&gt; 
 &lt;li&gt;Send a pull request.&lt;/li&gt; 
 &lt;li&gt;We appreciate your input!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing Dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv lock
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Virtual Env&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit hooks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Tests&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running static type checks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx mypy src
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Packaging&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing Locally&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install dist/*.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;CrewAI uses anonymous telemetry to collect usage data with the main purpose of helping us improve the library by focusing our efforts on the most used features, integrations and tools.&lt;/p&gt; 
&lt;p&gt;It's pivotal to understand that &lt;strong&gt;NO data is collected&lt;/strong&gt; concerning prompts, task descriptions, agents' backstories or goals, usage of tools, API calls, responses, any data processed by the agents, or secrets and environment variables, with the exception of the conditions mentioned. When the &lt;code&gt;share_crew&lt;/code&gt; feature is enabled, detailed data including task descriptions, agents' backstories or goals, and other specific attributes are collected to provide deeper insights while respecting user privacy. Users can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.&lt;/p&gt; 
&lt;p&gt;Data collected includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Version of CrewAI 
  &lt;ul&gt; 
   &lt;li&gt;So we can understand how many users are using the latest version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Version of Python 
  &lt;ul&gt; 
   &lt;li&gt;So we can decide on what versions to better support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;General OS (e.g. number of CPUs, macOS/Windows/Linux) 
  &lt;ul&gt; 
   &lt;li&gt;So we know what OS we should focus on and if we could build specific OS related features&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Number of agents and tasks in a crew 
  &lt;ul&gt; 
   &lt;li&gt;So we make sure we are testing internally with similar use cases and educate people on the best practices&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Crew Process being used 
  &lt;ul&gt; 
   &lt;li&gt;Understand where we should focus our efforts&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Agents are using memory or allowing delegation 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we improved the features or maybe even drop them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Tasks are being executed in parallel or sequentially 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we should focus more on parallel execution&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Language model being used 
  &lt;ul&gt; 
   &lt;li&gt;Improved support on most used languages&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Roles of agents in a crew 
  &lt;ul&gt; 
   &lt;li&gt;Understand high level use cases so we can build better tools, integrations and examples about it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Tools names available 
  &lt;ul&gt; 
   &lt;li&gt;Understand out of the publicly available tools, which ones are being used the most so we can improve them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Users can opt-in to Further Telemetry, sharing the complete telemetry data by setting the &lt;code&gt;share_crew&lt;/code&gt; attribute to &lt;code&gt;True&lt;/code&gt; on their Crews. Enabling &lt;code&gt;share_crew&lt;/code&gt; results in the collection of detailed crew and task execution data, including &lt;code&gt;goal&lt;/code&gt;, &lt;code&gt;backstory&lt;/code&gt;, &lt;code&gt;context&lt;/code&gt;, and &lt;code&gt;output&lt;/code&gt; of tasks. This enables a deeper insight into usage patterns while respecting the user's choice to share.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;CrewAI is released under the &lt;a href="https://github.com/crewAIInc/crewAI/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Frequently Asked Questions (FAQ)&lt;/h2&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-exactly-is-crewai"&gt;What exactly is CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-do-i-install-crewai"&gt;How do I install CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-depend-on-langchain"&gt;Does CrewAI depend on LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-open-source"&gt;Is CrewAI open-source?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-collect-data-from-users"&gt;Does CrewAI collect data from users?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Features and Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-crewai-handle-complex-use-cases"&gt;Can CrewAI handle complex use cases?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-use-crewai-with-local-ai-models"&gt;Can I use CrewAI with local AI models?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-makes-crews-different-from-flows"&gt;What makes Crews different from Flows?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-is-crewai-better-than-langchain"&gt;How is CrewAI better than LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-support-fine-tuning-or-training-custom-models"&gt;Does CrewAI support fine-tuning or training custom models?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resources and Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-where-can-i-find-real-world-crewai-examples"&gt;Where can I find real-world CrewAI examples?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-can-i-contribute-to-crewai"&gt;How can I contribute to CrewAI?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-additional-features-does-crewai-amp-offer"&gt;What additional features does CrewAI AMP offer?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-amp-available-for-cloud-and-on-premise-deployments"&gt;Is CrewAI AMP available for cloud and on-premise deployments?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-try-crewai-amp-for-free"&gt;Can I try CrewAI AMP for free?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: What exactly is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is a standalone, lean, and fast Python framework built specifically for orchestrating autonomous AI agents. Unlike frameworks like LangChain, CrewAI does not rely on external dependencies, making it leaner, faster, and simpler.&lt;/p&gt; 
&lt;h3&gt;Q: How do I install CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Install CrewAI using pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For additional tools, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Q: Does CrewAI depend on LangChain?&lt;/h3&gt; 
&lt;p&gt;A: No. CrewAI is built entirely from the ground up, with no dependencies on LangChain or other agent frameworks. This ensures a lean, fast, and flexible experience.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI handle complex use cases?&lt;/h3&gt; 
&lt;p&gt;A: Yes. CrewAI excels at both simple and highly complex real-world scenarios, offering deep customization options at both high and low levels, from internal prompts to sophisticated workflow orchestration.&lt;/p&gt; 
&lt;h3&gt;Q: Can I use CrewAI with local AI models?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI supports various language models, including local ones. Tools like Ollama and LM Studio allow seamless integration. Check the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;LLM Connections documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Q: What makes Crews different from Flows?&lt;/h3&gt; 
&lt;p&gt;A: Crews provide autonomous agent collaboration, ideal for tasks requiring flexible decision-making and dynamic interaction. Flows offer precise, event-driven control, ideal for managing detailed execution paths and secure state management. You can seamlessly combine both for maximum effectiveness.&lt;/p&gt; 
&lt;h3&gt;Q: How is CrewAI better than LangChain?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI provides simpler, more intuitive APIs, faster execution speeds, more reliable and consistent results, robust documentation, and an active community‚Äîaddressing common criticisms and limitations associated with LangChain.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI open-source?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is open-source and actively encourages community contributions and collaboration.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI collect data from users?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI collects anonymous telemetry data strictly for improvement purposes. Sensitive data such as prompts, tasks, or API responses are never collected unless explicitly enabled by the user.&lt;/p&gt; 
&lt;h3&gt;Q: Where can I find real-world CrewAI examples?&lt;/h3&gt; 
&lt;p&gt;A: Check out practical examples in the &lt;a href="https://github.com/crewAIInc/crewAI-examples"&gt;CrewAI-examples repository&lt;/a&gt;, covering use cases like trip planners, stock analysis, and job postings.&lt;/p&gt; 
&lt;h3&gt;Q: How can I contribute to CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Contributions are warmly welcomed! Fork the repository, create your branch, implement your changes, and submit a pull request. See the Contribution section of the README for detailed guidelines.&lt;/p&gt; 
&lt;h3&gt;Q: What additional features does CrewAI AMP offer?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI AMP provides advanced features such as a unified control plane, real-time observability, secure integrations, advanced security, actionable insights, and dedicated 24/7 enterprise support.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI AMP available for cloud and on-premise deployments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI AMP supports both cloud-based and on-premise deployment options, allowing enterprises to meet their specific security and compliance requirements.&lt;/p&gt; 
&lt;h3&gt;Q: Can I try CrewAI AMP for free?&lt;/h3&gt; 
&lt;p&gt;A: Yes, you can explore part of the CrewAI AMP Suite by accessing the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane&lt;/a&gt; for free.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI support fine-tuning or training custom models?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI can integrate with custom-trained or fine-tuned models, allowing you to enhance your agents with domain-specific knowledge and accuracy.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI agents interact with external tools and APIs?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI agents can easily integrate with external tools, APIs, and databases, empowering them to leverage real-world data and resources.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI suitable for production environments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is explicitly designed with production-grade standards, ensuring reliability, stability, and scalability for enterprise deployments.&lt;/p&gt; 
&lt;h3&gt;Q: How scalable is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is highly scalable, supporting simple automations and large-scale enterprise workflows involving numerous agents and complex tasks simultaneously.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer debugging and monitoring tools?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI AMP includes advanced debugging, tracing, and real-time observability features, simplifying the management and troubleshooting of your automations.&lt;/p&gt; 
&lt;h3&gt;Q: What programming languages does CrewAI support?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is primarily Python-based but easily integrates with services and APIs written in any programming language through its flexible API integration capabilities.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer educational resources for beginners?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI provides extensive beginner-friendly tutorials, courses, and documentation through learn.crewai.com, supporting developers at all skill levels.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI automate human-in-the-loop workflows?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI fully supports human-in-the-loop workflows, allowing seamless collaboration between human experts and AI agents for enhanced decision-making.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>