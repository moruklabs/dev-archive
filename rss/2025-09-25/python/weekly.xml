<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Wed, 24 Sep 2025 01:47:47 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>ml-explore/mlx-lm</title>
      <link>https://github.com/ml-explore/mlx-lm</link>
      <description>&lt;p&gt;Run LLMs with MLX&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;MLX LM&lt;/h2&gt; 
&lt;p&gt;MLX LM is a Python package for generating text and fine-tuning large language models on Apple silicon with MLX.&lt;/p&gt; 
&lt;p&gt;Some key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integration with the Hugging Face Hub to easily use thousands of LLMs with a single command.&lt;/li&gt; 
 &lt;li&gt;Support for quantizing and uploading models to the Hugging Face Hub.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/LORA.md"&gt;Low-rank and full model fine-tuning&lt;/a&gt; with support for quantized models.&lt;/li&gt; 
 &lt;li&gt;Distributed inference and fine-tuning with &lt;code&gt;mx.distributed&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The easiest way to get started is to install the &lt;code&gt;mlx-lm&lt;/code&gt; package:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;To generate text with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate --prompt "How tall is Mt Everest?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To chat with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.chat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will give you a chat REPL that you can use to interact with the LLM. The chat context is preserved during the lifetime of the REPL.&lt;/p&gt; 
&lt;p&gt;Commands in &lt;code&gt;mlx-lm&lt;/code&gt; typically take command line options which let you specify the model, sampling parameters, and more. Use &lt;code&gt;-h&lt;/code&gt; to see a list of available options for a command, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default model for generation and chat is &lt;code&gt;mlx-community/Llama-3.2-3B-Instruct-4bit&lt;/code&gt;. You can specify any MLX-compatible model with the &lt;code&gt;--model&lt;/code&gt; flag. Thousands are available in the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Community&lt;/a&gt; Hugging Face organization.&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;You can use &lt;code&gt;mlx-lm&lt;/code&gt; as a module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

text = generate(model, tokenizer, prompt=prompt, verbose=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(generate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/generate_response.py"&gt;generation example&lt;/a&gt; to see how to use the API in more detail. Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/batch_generate_response.py"&gt;batch generation example&lt;/a&gt; to see how to efficiently generate continuations for a batch of prompts.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;mlx-lm&lt;/code&gt; package also comes with functionality to quantize and optionally upload models to the Hugging Face Hub.&lt;/p&gt; 
&lt;p&gt;You can convert models using the Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import convert

repo = "mistralai/Mistral-7B-Instruct-v0.3"
upload_repo = "mlx-community/My-Mistral-7B-Instruct-v0.3-4bit"

convert(repo, quantize=True, upload_repo=upload_repo)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will generate a 4-bit quantized Mistral 7B and upload it to the repo &lt;code&gt;mlx-community/My-Mistral-7B-Instruct-v0.3-4bit&lt;/code&gt;. It will also save the converted model in the path &lt;code&gt;mlx_model&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(convert)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Streaming&lt;/h4&gt; 
&lt;p&gt;For streaming generation, use the &lt;code&gt;stream_generate&lt;/code&gt; function. This yields a generation response object.&lt;/p&gt; 
&lt;p&gt;For example,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, stream_generate

repo = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
model, tokenizer = load(repo)

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

for response in stream_generate(model, tokenizer, prompt, max_tokens=512):
    print(response.text, end="", flush=True)
print()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Sampling&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;generate&lt;/code&gt; and &lt;code&gt;stream_generate&lt;/code&gt; functions accept &lt;code&gt;sampler&lt;/code&gt; and &lt;code&gt;logits_processors&lt;/code&gt; keyword arguments. A sampler is any callable which accepts a possibly batched logits array and returns an array of sampled tokens. The &lt;code&gt;logits_processors&lt;/code&gt; must be a list of callables which take the token history and current logits as input and return the processed logits. The logits processors are applied in order.&lt;/p&gt; 
&lt;p&gt;Some standard sampling functions and logits processors are provided in &lt;code&gt;mlx_lm.sample_utils&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;p&gt;You can also use &lt;code&gt;mlx-lm&lt;/code&gt; from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt "hello"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will download a Mistral 7B model from the Hugging Face Hub and generate text using the given prompt.&lt;/p&gt; 
&lt;p&gt;For a full list of options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To quantize a model from the command line run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can upload new models to Hugging Face by specifying &lt;code&gt;--upload-repo&lt;/code&gt; to &lt;code&gt;convert&lt;/code&gt;. For example, to upload a quantized Mistral-7B model to the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Hugging Face community&lt;/a&gt; you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert \
    --hf-path mistralai/Mistral-7B-Instruct-v0.3 \
    -q \
    --upload-repo mlx-community/my-4bit-mistral
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models can also be converted and quantized directly in the &lt;a href="https://huggingface.co/spaces/mlx-community/mlx-my-repo"&gt;mlx-my-repo&lt;/a&gt; Hugging Face Space.&lt;/p&gt; 
&lt;h3&gt;Long Prompts and Generations&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; has some tools to scale efficiently to long prompts and generations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A rotating fixed-size key-value cache.&lt;/li&gt; 
 &lt;li&gt;Prompt caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use the rotating key-value cache pass the argument &lt;code&gt;--max-kv-size n&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; can be any integer. Smaller values like &lt;code&gt;512&lt;/code&gt; will use very little RAM but result in worse quality. Larger values like &lt;code&gt;4096&lt;/code&gt; or higher will use more RAM but have better quality.&lt;/p&gt; 
&lt;p&gt;Caching prompts can substantially speedup reusing the same long context with different queries. To cache a prompt use &lt;code&gt;mlx_lm.cache_prompt&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat prompt.txt | mlx_lm.cache_prompt \
  --model mistralai/Mistral-7B-Instruct-v0.3 \
  --prompt - \
  --prompt-cache-file mistral_prompt.safetensors
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then use the cached prompt with &lt;code&gt;mlx_lm.generate&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate \
    --prompt-cache-file mistral_prompt.safetensors \
    --prompt "\nSummarize the above text."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The cached prompt is treated as a prefix to the supplied prompt. Also notice when using a cached prompt, the model to use is read from the cache and need not be supplied explicitly.&lt;/p&gt; 
&lt;p&gt;Prompt caching can also be used in the Python API in order to avoid recomputing the prompt. This is useful in multi-turn dialogues or across requests that use the same context. See the &lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/examples/chat.py"&gt;example&lt;/a&gt; for more usage details.&lt;/p&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; supports thousands of Hugging Face format LLMs. If the model you want to run is not supported, file an &lt;a href="https://github.com/ml-explore/mlx-lm/issues/new"&gt;issue&lt;/a&gt; or better yet, submit a pull request.&lt;/p&gt; 
&lt;p&gt;Here are a few examples of Hugging Face models that work with this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-7B-v0.1"&gt;mistralai/Mistral-7B-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-2-7b-hf"&gt;meta-llama/Llama-2-7b-hf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct"&gt;deepseek-ai/deepseek-coder-6.7b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/01-ai/Yi-6B-Chat"&gt;01-ai/Yi-6B-Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/phi-2"&gt;microsoft/phi-2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"&gt;mistralai/Mixtral-8x7B-Instruct-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-7B"&gt;Qwen/Qwen-7B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b"&gt;pfnet/plamo-13b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b-instruct"&gt;pfnet/plamo-13b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b"&gt;stabilityai/stablelm-2-zephyr-1_6b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/internlm/internlm2-7b"&gt;internlm/internlm2-7b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/tiiuae/falcon-mamba-7b-instruct"&gt;tiiuae/falcon-mamba-7b-instruct&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mistral&amp;amp;sort=trending"&gt;Mistral&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=llama&amp;amp;sort=trending"&gt;Llama&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=phi&amp;amp;sort=trending"&gt;Phi-2&lt;/a&gt;, and &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mixtral&amp;amp;sort=trending"&gt;Mixtral&lt;/a&gt; style models should work out of the box.&lt;/p&gt; 
&lt;p&gt;For some models (such as &lt;code&gt;Qwen&lt;/code&gt; and &lt;code&gt;plamo&lt;/code&gt;) the tokenizer requires you to enable the &lt;code&gt;trust_remote_code&lt;/code&gt; option. You can do this by passing &lt;code&gt;--trust-remote-code&lt;/code&gt; in the command line. If you don't specify the flag explicitly, you will be prompted to trust remote code in the terminal when running the model.&lt;/p&gt; 
&lt;p&gt;For &lt;code&gt;Qwen&lt;/code&gt; models you must also specify the &lt;code&gt;eos_token&lt;/code&gt;. You can do this by passing &lt;code&gt;--eos-token "&amp;lt;|endoftext|&amp;gt;"&lt;/code&gt; in the command line.&lt;/p&gt; 
&lt;p&gt;These options can also be set in the Python API. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model, tokenizer = load(
    "qwen/Qwen-7B",
    tokenizer_config={"eos_token": "&amp;lt;|endoftext|&amp;gt;", "trust_remote_code": True},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Large Models&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This requires macOS 15.0 or higher to work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Models which are large relative to the total RAM available on the machine can be slow. &lt;code&gt;mlx-lm&lt;/code&gt; will attempt to make them faster by wiring the memory occupied by the model and cache. This requires macOS 15 or higher to work.&lt;/p&gt; 
&lt;p&gt;If you see the following warning message:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[WARNING] Generating with a model that requires ...&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;then the model will likely be slow on the given machine. If the model fits in RAM then it can often be sped up by increasing the system wired memory limit. To increase the limit, set the following &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo sysctl iogpu.wired_limit_mb=N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The value &lt;code&gt;N&lt;/code&gt; should be larger than the size of the model in megabytes but smaller than the memory size of the machine.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/adk-samples</title>
      <link>https://github.com/google/adk-samples</link>
      <description>&lt;p&gt;A collection of sample agents built with Agent Development (ADK)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Development Kit (ADK) Samples&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://github.com/google/adk-docs/raw/main/docs/assets/agent-development-kit.png" alt="Agent Development Kit Logo" width="150" /&gt; 
&lt;p&gt;Welcome to the ADK Sample Agents repository! This collection provides ready-to-use agents built on top of the &lt;a href="https://google.github.io/adk-docs/"&gt;Agent Development Kit&lt;/a&gt;, designed to accelerate your development process. These agents cover a range of common use cases and complexities, from simple conversational bots to complex multi-agent workflows.&lt;/p&gt; 
&lt;h2&gt;‚ú® Getting Started&lt;/h2&gt; 
&lt;p&gt;This repo contains ADK sample agents for both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;Java.&lt;/strong&gt; Navigate to the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/python/"&gt;Python&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/java/"&gt;Java&lt;/a&gt;&lt;/strong&gt; subfolders to see language-specific setup instructions, and learn more about the available sample agents.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The agents in this repository are built using the &lt;strong&gt;Agent Development Kit (ADK)&lt;/strong&gt;. Before you can run any of the samples, you must have the ADK installed. For instructions, please refer to the &lt;a href="https://google.github.io/adk-docs/get-started/installation"&gt;&lt;strong&gt;ADK Installation Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To learn more, check out the &lt;a href="https://google.github.io/adk-docs/"&gt;ADK Documentation&lt;/a&gt;, and the GitHub repositories for &lt;a href="https://github.com/google/adk-python"&gt;ADK Python&lt;/a&gt; and &lt;a href="https://github.com/google/adk-java"&gt;ADK Java&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üå≥ Repository Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;‚îú‚îÄ‚îÄ java
‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ agents
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ software-bug-assistant
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ time-series-forecasting
‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ python
‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ agents
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ academic-research
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ blog-writer
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ brand-search-optimization
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ camel
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ customer-service
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ data-engineering
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ data-science
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ financial-advisor
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ fomc-research
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini-fullstack
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google-trends-agent
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ image-scoring
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm-auditor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ machine-learning-engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ marketing-agency
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ medical-pre-authorization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ personalized-shopping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ realtime-conversational-agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ safety-plugins
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ software-bug-assistant  
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ travel-concierge
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ÑπÔ∏è Getting help&lt;/h2&gt; 
&lt;p&gt;If you have any questions or if you found any problems with this repository, please report through &lt;a href="https://github.com/google/adk-samples/issues"&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our &lt;a href="https://github.com/google/adk-samples/raw/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guidelines&lt;/strong&gt;&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://github.com/google/adk-samples/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Disclaimers&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. This project is not eligible for the &lt;a href="https://bughunters.google.com/open-source-security"&gt;Google Open Source Software Vulnerability Rewards Program&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is intended for demonstration purposes only. It is not intended for use in a production environment.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Alibaba-NLP/DeepResearch</title>
      <link>https://github.com/Alibaba-NLP/DeepResearch</link>
      <description>&lt;p&gt;Tongyi Deep Research, the Leading Open-source Deep Research Agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/logo.png" width="100%" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;&lt;img src="https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="MODELS" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Alibaba-NLP/DeepResearch"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p align="center"&gt; ü§ó &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;HuggingFace&lt;/a&gt; ÔΩú &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;ModelScope&lt;/a&gt; | üí¨ &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/wechat.jpg"&gt;WeChat(ÂæÆ‰ø°)&lt;/a&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14895" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14895" alt="Alibaba-NLP%2FDeepResearch | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;We present &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;strong&gt;Tongyi DeepResearch&lt;/strong&gt;, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for &lt;strong&gt;long-horizon, deep information-seeking&lt;/strong&gt; tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tongyi DeepResearch builds upon our previous work on the &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/"&gt;WebAgent&lt;/a&gt; project.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;More details can be found in our üì∞&amp;nbsp;&lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;Tech Blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/performance.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Fully automated synthetic data generation pipeline&lt;/strong&gt;: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Large-scale continual pre-training on agentic data&lt;/strong&gt;: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.&lt;/li&gt; 
 &lt;li&gt;üîÅ &lt;strong&gt;End-to-end reinforcement learning&lt;/strong&gt;: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Agent Inference Paradigm Compatibility&lt;/strong&gt;: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Model Download&lt;/h1&gt; 
&lt;p&gt;You can directly download the model by following the links below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Download Links&lt;/th&gt; 
   &lt;th align="center"&gt;Model Size&lt;/th&gt; 
   &lt;th align="center"&gt;Context Length&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Tongyi-DeepResearch-30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B"&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;128K&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;p&gt;[2025/09/20]üöÄ Tongyi-DeepResearch-30B-A3B is now on &lt;a href="https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b"&gt;OpenRouter&lt;/a&gt;! Follow the &lt;a href="https://github.com/Alibaba-NLP/DeepResearch?tab=readme-ov-file#6-you-can-use-openrouters-api-to-call-our-model"&gt;Quick-start&lt;/a&gt; guide.&lt;/p&gt; 
&lt;p&gt;[2025/09/17]üî• We have released &lt;strong&gt;Tongyi-DeepResearch-30B-A3B&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;Deep Research Benchmark Results&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This guide provides instructions for setting up the environment and running inference scripts located in the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/inference/"&gt;inference&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recommended Python version: &lt;strong&gt;3.10.0&lt;/strong&gt; (using other versions may cause dependency issues).&lt;/li&gt; 
 &lt;li&gt;It is strongly advised to create an isolated environment using &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with Conda
conda create -n react_infer_env python=3.10.0
conda activate react_infer_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install the required dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Environment Configuration and Prepare Evaluation Data&lt;/h3&gt; 
&lt;h4&gt;Environment Configuration&lt;/h4&gt; 
&lt;p&gt;Configure your API keys and settings by copying the example environment file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy the example environment file
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Edit the &lt;code&gt;.env&lt;/code&gt; file and provide your actual API keys and configuration values:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SERPER_KEY_ID&lt;/strong&gt;: Get your key from &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; for web search and Google Scholar&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JINA_API_KEYS&lt;/strong&gt;: Get your key from &lt;a href="https://jina.ai/"&gt;Jina.ai&lt;/a&gt; for web page reading&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API_KEY/API_BASE&lt;/strong&gt;: OpenAI-compatible API for page summarization from &lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DASHSCOPE_API_KEY&lt;/strong&gt;: Get your key from &lt;a href="https://dashscope.aliyun.com/"&gt;Dashscope&lt;/a&gt; for file parsing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SANDBOX_FUSION_ENDPOINT&lt;/strong&gt;: Python interpreter sandbox endpoints (see &lt;a href="https://github.com/bytedance/SandboxFusion"&gt;SandboxFusion&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MODEL_PATH&lt;/strong&gt;: Path to your model weights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DATASET&lt;/strong&gt;: Name of your evaluation dataset&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OUTPUT_PATH&lt;/strong&gt;: Directory for saving results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;.env&lt;/code&gt; file is gitignored, so your secrets will not be committed to the repository.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Prepare Evaluation Data&lt;/h4&gt; 
&lt;p&gt;The system supports two input file formats: &lt;strong&gt;JSON&lt;/strong&gt; and &lt;strong&gt;JSONL&lt;/strong&gt;.&lt;/p&gt; 
&lt;h4&gt;Supported File Formats:&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: JSONL Format (recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.jsonl&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.jsonl&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Each line must be a valid JSON object with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class="language-json"&gt;{"question": "What is the capital of France?", "answer": "Paris"}
{"question": "Explain quantum computing", "answer": ""}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: JSON Format&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.json&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.json&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;File must contain a JSON array of objects, each with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class="language-json"&gt;[
  {"question": "What is the capital of France?", "answer": "Paris"},
  {"question": "Explain quantum computing", "answer": ""}
]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; The &lt;code&gt;answer&lt;/code&gt; field contains the &lt;strong&gt;ground truth/reference answer&lt;/strong&gt; used for evaluation. The system generates its own responses to the questions, and these reference answers are used to automatically judge the quality of the generated responses during benchmark evaluation.&lt;/p&gt; 
&lt;h4&gt;File References for Document Processing:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;If using the &lt;em&gt;file parser&lt;/em&gt; tool, &lt;strong&gt;prepend the filename to the &lt;code&gt;question&lt;/code&gt; field&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Place referenced files in &lt;code&gt;eval_data/file_corpus/&lt;/code&gt; directory&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;{"question": "report.pdf What are the key findings?", "answer": "..."}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;File Organization:&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;project_root/
‚îú‚îÄ‚îÄ eval_data/
‚îÇ   ‚îú‚îÄ‚îÄ my_questions.jsonl          # Your evaluation data
‚îÇ   ‚îî‚îÄ‚îÄ file_corpus/                # Referenced documents
‚îÇ       ‚îú‚îÄ‚îÄ report.pdf
‚îÇ       ‚îî‚îÄ‚îÄ data.xlsx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Configure the Inference Script&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open &lt;code&gt;run_react_infer.sh&lt;/code&gt; and modify the following variables as instructed in the comments: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - path to the local or remote model weights.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;DATASET&lt;/code&gt; - full path to your evaluation file, e.g. &lt;code&gt;eval_data/my_questions.jsonl&lt;/code&gt; or &lt;code&gt;/path/to/my_questions.json&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OUTPUT_PATH&lt;/code&gt; - path for saving the prediction results, e.g. &lt;code&gt;./outputs&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required &lt;code&gt;API_KEY&lt;/code&gt;, &lt;code&gt;BASE_URL&lt;/code&gt;, or other credentials. Each key is explained inline in the bash script.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Run the Inference Script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash run_react_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;With these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.&lt;/p&gt; 
&lt;h3&gt;6. You can use OpenRouter's API to call our model&lt;/h3&gt; 
&lt;p&gt;Tongyi-DeepResearch-30B-A3B is now available at &lt;a href="https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b"&gt;OpenRouter&lt;/a&gt;. You can run the inference without any GPUs.&lt;/p&gt; 
&lt;p&gt;You need to modify the following in the file &lt;a href="https://github.com/Alibaba-NLP/DeepResearch/raw/main/inference/react_agent.py"&gt;inference/react_agent.py&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the call_server function: Set the API key and URL to your OpenRouter account‚Äôs API and URL.&lt;/li&gt; 
 &lt;li&gt;Change the model name to alibaba/tongyi-deepresearch-30b-a3b.&lt;/li&gt; 
 &lt;li&gt;Adjust the content concatenation way as described in the comments on lines &lt;strong&gt;88‚Äì90.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide benchmark evaluation scripts for various datasets. Please refer to the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/evaluation/"&gt;evaluation scripts&lt;/a&gt; directory for more details.&lt;/p&gt; 
&lt;h2&gt;Deep Research Agent Family&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/family.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:&lt;/p&gt; 
&lt;p&gt;[1] &lt;a href="https://arxiv.org/pdf/2501.07572"&gt;WebWalker: Benchmarking LLMs in Web Traversal&lt;/a&gt; (ACL 2025)&lt;br /&gt; [2] &lt;a href="https://arxiv.org/pdf/2505.22648"&gt;WebDancer: Towards Autonomous Information Seeking Agency&lt;/a&gt; (NeurIPS 2025)&lt;br /&gt; [3] &lt;a href="https://arxiv.org/pdf/2507.02592"&gt;WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/a&gt;&lt;br /&gt; [4] &lt;a href="https://arxiv.org/pdf/2507.15061"&gt;WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/a&gt;&lt;br /&gt; [5] &lt;a href="https://arxiv.org/pdf/2508.05748"&gt;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&lt;/a&gt;&lt;br /&gt; [6] &lt;a href="https://arxiv.org/pdf/2509.13309"&gt;WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents&lt;/a&gt;&lt;br /&gt; [7] &lt;a href="https://arxiv.org/pdf/2509.13313"&gt;ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization&lt;/a&gt;&lt;br /&gt; [8] &lt;a href="https://arxiv.org/pdf/2509.13312"&gt;WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research&lt;/a&gt;&lt;br /&gt; [9] &lt;a href="https://arxiv.org/pdf/2509.13305"&gt;WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning&lt;/a&gt;&lt;br /&gt; [10] &lt;a href="https://arxiv.org/pdf/2509.13310"&gt;Scaling Agents via Continual Pre-training&lt;/a&gt;&lt;br /&gt; [11] &lt;a href="https://arxiv.org/pdf/2509.13311"&gt;Towards General Agentic Intelligence via Environment Scaling&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üåü Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#Alibaba-NLP/DeepResearch&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üö© Talent Recruitment&lt;/h2&gt; 
&lt;p&gt;üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)&lt;/p&gt; 
&lt;p&gt;üìö &lt;strong&gt;Research Area&lt;/strong&gt;ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; 
&lt;p&gt;‚òéÔ∏è &lt;strong&gt;Contact&lt;/strong&gt;Ôºö&lt;a href=""&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href="mailto:yongjiang.jy@alibaba-inc.com"&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi-DeepResearch},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;‚å®Ô∏è Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;üñ•Ô∏è Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;‚å®Ô∏è Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03‚ÄØPM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dortania/OpenCore-Legacy-Patcher</title>
      <link>https://github.com/dortania/OpenCore-Legacy-Patcher</link>
      <description>&lt;p&gt;Experience macOS just like before&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/dortania/OpenCore-Legacy-Patcher/main/docs/images/OC-Patcher.png" alt="OpenCore Patcher Logo" width="256" /&gt; 
 &lt;h1&gt;OpenCore Legacy Patcher&lt;/h1&gt; 
&lt;/div&gt; 
&lt;p&gt;A Python-based project revolving around &lt;a href="https://github.com/acidanthera/OpenCorePkg"&gt;Acidanthera's OpenCorePkg&lt;/a&gt; and &lt;a href="https://github.com/acidanthera/Lilu"&gt;Lilu&lt;/a&gt; for both running and unlocking features in macOS on supported and unsupported Macs.&lt;/p&gt; 
&lt;p&gt;Our project's main goal is to breathe new life into Macs no longer supported by Apple, allowing for the installation and usage of macOS Big Sur and newer on machines as old as 2007.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/downloads/dortania/OpenCore-Legacy-Patcher/total?color=white&amp;amp;style=plastic" alt="GitHub all releases" /&gt; &lt;img src="https://img.shields.io/github/languages/top/dortania/OpenCore-Legacy-Patcher?color=4B8BBE&amp;amp;style=plastic" alt="GitHub top language" /&gt; &lt;img src="https://img.shields.io/discord/417165963327176704?color=7289da&amp;amp;label=discord&amp;amp;style=plastic" alt="Discord" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Noteworthy features of OpenCore Legacy Patcher:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for macOS Big Sur, Monterey, Ventura, Sonoma and Sequoia&lt;/li&gt; 
 &lt;li&gt;Native Over the Air (OTA) System Updates&lt;/li&gt; 
 &lt;li&gt;Supports Penryn and newer Macs&lt;/li&gt; 
 &lt;li&gt;Full support for WPA Wi-Fi and Personal Hotspot on BCM943224 and newer wireless chipsets&lt;/li&gt; 
 &lt;li&gt;System Integrity Protection, FileVault 2, .im4m Secure Boot and Vaulting&lt;/li&gt; 
 &lt;li&gt;Recovery OS, Safe Mode and Single-user Mode booting on non-native OSes&lt;/li&gt; 
 &lt;li&gt;Unlocks features such as Sidecar and AirPlay to Mac even on native Macs&lt;/li&gt; 
 &lt;li&gt;Enables enhanced SATA and NVMe power management on non-Apple storage devices&lt;/li&gt; 
 &lt;li&gt;Zero firmware patching required (ie. APFS ROM patching)&lt;/li&gt; 
 &lt;li&gt;Graphics acceleration for both Metal and non-Metal GPUs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;Note: Only clean-installs and upgrades are supported. macOS Big Sur installs already patched with other patchers, such as &lt;a href="https://github.com/BenSova/Patched-Sur"&gt;Patched Sur&lt;/a&gt; or &lt;a href="https://github.com/StarPlayrX/bigmac"&gt;bigmac&lt;/a&gt;, cannot be used due to broken file integrity with APFS snapshots and SIP.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can, however, reinstall macOS with this patcher and retain your original data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note 2: Currently, OpenCore Legacy Patcher officially supports patching to run macOS Big Sur through Sonoma installs. For older OSes, OpenCore may function; however, support is currently not provided from Dortania.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For macOS Mojave and Catalina support, we recommend the use of &lt;a href="http://dosdude1.com"&gt;dosdude1's patchers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To start using the project, please see our in-depth guide:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dortania.github.io/OpenCore-Legacy-Patcher/"&gt;OpenCore Legacy Patcher Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;This project is offered on an AS-IS basis, we do not guarantee support for any issues that may arise. However, there is a community server with other passionate users and developers that can aid you:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/rqdPgH8xSN"&gt;OpenCore Patcher Paradise Discord Server&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Keep in mind that the Discord server is maintained by the community, so we ask everyone to be respectful.&lt;/li&gt; 
   &lt;li&gt;Please review our docs on &lt;a href="https://dortania.github.io/OpenCore-Legacy-Patcher/DEBUG.html"&gt;how to debug with OpenCore&lt;/a&gt; to gather important information to help others with troubleshooting.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running from source&lt;/h2&gt; 
&lt;p&gt;To run the project from source, see here: &lt;a href="https://raw.githubusercontent.com/dortania/OpenCore-Legacy-Patcher/main/SOURCE.md"&gt;Build and run from source&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Acidanthera"&gt;Acidanthera&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;OpenCorePkg, as well as many of the core kexts and tools&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DhinakG"&gt;DhinakG&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Main co-author&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Khronokernel"&gt;Khronokernel&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Main co-author&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Ausdauersportler"&gt;Ausdauersportler&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;iMacs Metal GPUs Upgrade Patch set and documentation&lt;/li&gt; 
   &lt;li&gt;Great amounts of help with debugging, and code suggestions&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vit9696"&gt;vit9696&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Endless amount of help troubleshooting, determining fixes and writing patches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/covasedu"&gt;EduCovas&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/non-metal-frameworks"&gt;non-Metal patch set&lt;/a&gt; for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/3802-Metal-15"&gt;3802 Metal patch set&lt;/a&gt; and &lt;a href="https://github.com/dortania/MetallibSupportPkg"&gt;MetallibSupportPkg&lt;/a&gt; for nVidia Kepler and Intel Core 3rd/4th Generation GPUs&lt;/li&gt; 
   &lt;li&gt;Metal bundle patches and shims for &lt;a href="https://github.com/moraea/misc-patches/tree/main/Kepler%2013%2B"&gt;nVidia Kepler&lt;/a&gt;, &lt;a href="https://github.com/moraea/misc-patches/tree/main/GCN%2013%2B"&gt;AMD GCN 1 - 4&lt;/a&gt;, and &lt;a href="https://github.com/moraea/misc-patches/tree/main/vega%2013%2B"&gt;AMD GCN 5 (Vega)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/Sonoma%2014.4%20IOSurface"&gt;IOSurface offset patches&lt;/a&gt; for nVidia Kepler, AMD GCN 1 - 5, and Intel Core 3rd - 6th Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/unsupported-wifi-patches"&gt;legacy Wi-Fi patch set&lt;/a&gt; restores functionality for Wi-Fi cards in all 2007 - 2017 models&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/T1-Patch"&gt;T1 patch set&lt;/a&gt; restores Touch ID, Apple Pay, and other secure functionality in 2016 - 2017 models&lt;/li&gt; 
   &lt;li&gt;AppleGVA downgrade for accelerated video decoding on 2012 - 2016 models&lt;/li&gt; 
   &lt;li&gt;OpenCL and OpenGL downgrade for AMD GCN&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/IOUSBHostFamily-14.4"&gt;USB 1 patch&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moosethegoose2213"&gt;ASentientHedgehog&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/non-metal-frameworks"&gt;non-Metal patch set&lt;/a&gt; for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ASentientBot"&gt;ASentientBot&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/non-metal-frameworks"&gt;non-Metal patch set&lt;/a&gt; for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/sequoia%2031001%20interposer"&gt;Metal bundle interposer&lt;/a&gt; for AMD GCN 1 - 5 and Intel Core 5th/6th Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/dsce"&gt;dsce&lt;/a&gt; and &lt;a href="https://github.com/moraea/moraea-common"&gt;shared code&lt;/a&gt; used by some other patches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cdf"&gt;cdf&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Mac Pro on OpenCore Patch set and documentation&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/cdf/Innie"&gt;Innie&lt;/a&gt; and &lt;a href="https://github.com/cdf/NightShiftEnabler"&gt;NightShiftEnabler&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forums.macrumors.com/members/syncretic.1173816/"&gt;Syncretic&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://forums.macrumors.com/threads/mp3-1-others-sse-4-2-emulation-to-enable-amd-metal-driver.2206682/"&gt;AAAMouSSE&lt;/a&gt;, &lt;a href="https://forums.macrumors.com/threads/mp3-1-others-sse-4-2-emulation-to-enable-amd-metal-driver.2206682/post-28447707"&gt;telemetrap&lt;/a&gt; and &lt;a href="https://github.com/reenigneorcim/SurPlus"&gt;SurPlus&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dosdude1"&gt;dosdude1&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Main author of the &lt;a href="https://github.com/dortania/OCLP-GUI"&gt;original GUI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Development of previous patchers, laying out much of what needs to be patched&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/parrotgeek1"&gt;parrotgeek1&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/dortania/OpenCore-Legacy-Patcher/raw/4a8f61a01da72b38a4b2250386cc4b497a31a839/payloads/Config/config.plist#L1222-L1281"&gt;VMM Patch Set&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BarryKN"&gt;BarryKN&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Development of previous patchers, laying out much of what needs to be patched&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mariobrostech"&gt;mario_bros_tech&lt;/a&gt; and the rest of the Unsupported Mac Discord 
  &lt;ul&gt; 
   &lt;li&gt;Catalyst that started OpenCore Legacy Patcher&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arter97/"&gt;arter97&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/arter97/SimpleMSR/"&gt;SimpleMSR&lt;/a&gt; to disable firmware throttling in Nehalem+ MacBooks without batteries&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mrmacintosh.com"&gt;Mr.Macintosh&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Endless hours helping architect and troubleshoot many portions of the project&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/flagersgit"&gt;flagers&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Aid with Nvidia Web Driver research and development&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/non-metal-frameworks"&gt;non-Metal patch set&lt;/a&gt; for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/moraea/misc-patches/tree/main/sequoia%2031001%20interposer"&gt;Metal bundle interposer&lt;/a&gt; for AMD GCN 1 - 5 and Intel Core 5th/6th Generation GPUs&lt;/li&gt; 
   &lt;li&gt;LegacyRVPL, SnapshotIsKill, etc. to aid in rapid testing and development&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/joevt"&gt;joevt&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/joevt/joevtApps"&gt;FixPCIeLinkrate&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Jazzzny"&gt;Jazzzny&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Research and various contributions to the project&lt;/li&gt; 
   &lt;li&gt;UEFI Legacy XHCI research and development&lt;/li&gt; 
   &lt;li&gt;NVIDIA OpenCL research and development&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;MacBook5,2&lt;/code&gt; research and development 
    &lt;ul&gt; 
     &lt;li&gt;LegacyKeyboardInjector&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Pre-Ivy Bridge Aquantia Ethernet Patch&lt;/li&gt; 
   &lt;li&gt;Non-Metal Photo Booth Patch for Monterey+&lt;/li&gt; 
   &lt;li&gt;GUI and Backend Development 
    &lt;ul&gt; 
     &lt;li&gt;Updater UI&lt;/li&gt; 
     &lt;li&gt;macOS Downloader UI&lt;/li&gt; 
     &lt;li&gt;Downloader UI&lt;/li&gt; 
     &lt;li&gt;USB Top Case probing&lt;/li&gt; 
     &lt;li&gt;Developer root patching&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Vaulting implementation&lt;/li&gt; 
   &lt;li&gt;macOS 15 3802 Helios Research&lt;/li&gt; 
   &lt;li&gt;UEFI bootx64.efi research&lt;/li&gt; 
   &lt;li&gt;universal2 build research&lt;/li&gt; 
   &lt;li&gt;Various documentation contributions&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Amazing users who've graciously donate hardware: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://forums.macrumors.com/members/johnd.53633/"&gt;JohnD&lt;/a&gt; - 2013 Mac Pro&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/SpiGAndromeda"&gt;SpiGAndromeda&lt;/a&gt; - AMD Vega 64&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/turbomacs"&gt;turbomacs&lt;/a&gt; - 2014 5k iMac&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://forums.macrumors.com/members/vinaypundith.1212357/"&gt;vinaypundith&lt;/a&gt; - MacBook7,1&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/ThatStella7922"&gt;ThatStella7922&lt;/a&gt; - 2017 13" MacBook Pro (A1708)&lt;/li&gt; 
   &lt;li&gt;zephar - 2008 Mac Pro&lt;/li&gt; 
   &lt;li&gt;jazo97 - 2011 15" MacBook Pro&lt;/li&gt; 
   &lt;li&gt;And others (reach out if we forgot you!)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;MacRumors and Unsupported Mac Communities 
  &lt;ul&gt; 
   &lt;li&gt;Endless testing and reporting issues&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Apple 
  &lt;ul&gt; 
   &lt;li&gt;for macOS and many of the kexts, frameworks and other binaries we reimplemented into newer OSes&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/detectron2</title>
      <link>https://github.com/facebookresearch/detectron2</link>
      <description>&lt;p&gt;Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.&lt;/p&gt;&lt;hr&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/detectron2/main/.github/Detectron2-Logo-Horz.svg?sanitize=true" width="300" /&gt; 
&lt;p&gt;Detectron2 is Facebook AI Research's next generation library that provides state-of-the-art detection and segmentation algorithms. It is the successor of &lt;a href="https://github.com/facebookresearch/Detectron/"&gt;Detectron&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/maskrcnn-benchmark/"&gt;maskrcnn-benchmark&lt;/a&gt;. It supports a number of computer vision research projects and production applications in Facebook.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Learn More about Detectron2&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Includes new capabilities such as panoptic segmentation, Densepose, Cascade R-CNN, rotated bounding boxes, PointRend, DeepLab, ViTDet, MViTv2 etc.&lt;/li&gt; 
 &lt;li&gt;Used as a library to support building &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/"&gt;research projects&lt;/a&gt; on top of it.&lt;/li&gt; 
 &lt;li&gt;Models can be exported to TorchScript format or Caffe2 format for deployment.&lt;/li&gt; 
 &lt;li&gt;It &lt;a href="https://detectron2.readthedocs.io/notes/benchmarks.html"&gt;trains much faster&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://ai.meta.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/"&gt;blog post&lt;/a&gt; to see more demos. See this &lt;a href="https://ai.meta.com/blog/detectron-everingham-prize/"&gt;interview&lt;/a&gt; to learn more about the stories behind detectron2.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://detectron2.readthedocs.io/tutorials/install.html"&gt;installation instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://detectron2.readthedocs.io/tutorials/getting_started.html"&gt;Getting Started with Detectron2&lt;/a&gt;, and the &lt;a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5"&gt;Colab Notebook&lt;/a&gt; to learn about basic usage.&lt;/p&gt; 
&lt;p&gt;Learn more at our &lt;a href="https://detectron2.readthedocs.org"&gt;documentation&lt;/a&gt;. And see &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/projects/"&gt;projects/&lt;/a&gt; for some projects that are built on top of detectron2.&lt;/p&gt; 
&lt;h2&gt;Model Zoo and Baselines&lt;/h2&gt; 
&lt;p&gt;We provide a large set of baseline results and trained models available for download in the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md"&gt;Detectron2 Model Zoo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Detectron2 is released under the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing Detectron2&lt;/h2&gt; 
&lt;p&gt;If you use Detectron2 in your research or wish to refer to the baseline results published in the &lt;a href="https://raw.githubusercontent.com/facebookresearch/detectron2/main/MODEL_ZOO.md"&gt;Model Zoo&lt;/a&gt;, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-research/timesfm</title>
      <link>https://github.com/google-research/timesfm</link>
      <description>&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; 
&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2310.10688"&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, ICML 2024.&lt;/li&gt; 
 &lt;li&gt;All checkpoints: &lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;TimesFM Hugging Face Collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/"&gt;Google Research blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/docs/timesfm-model"&gt;TimesFM in BigQuery&lt;/a&gt;: an official Google product.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This open version is not an officially supported Google product.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Latest Model Version:&lt;/strong&gt; TimesFM 2.5&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Archived Model Versions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1.0 and 2.0: relevant code archived in the sub directory &lt;code&gt;v1&lt;/code&gt;. You can &lt;code&gt;pip install timesfm==1.3.0&lt;/code&gt; to install an older version of this package to load them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Update - Sept. 15, 2025&lt;/h2&gt; 
&lt;p&gt;TimesFM 2.5 is out!&lt;/p&gt; 
&lt;p&gt;Comparing to TimesFM 2.0, this new 2.5 model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;uses 200M parameters, down from 500M.&lt;/li&gt; 
 &lt;li&gt;supports up to 16k context length, up from 2048.&lt;/li&gt; 
 &lt;li&gt;supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.&lt;/li&gt; 
 &lt;li&gt;gets rid of the &lt;code&gt;frequency&lt;/code&gt; indicator.&lt;/li&gt; 
 &lt;li&gt;has a couple of new forecasting flags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;add support for an upcoming Flax version of the model (faster inference).&lt;/li&gt; 
 &lt;li&gt;add back covariate support.&lt;/li&gt; 
 &lt;li&gt;populate more docstrings, docs and notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;p&gt;TODO(siriuz42): Package timesfm==2.0.0 and upload to PyPI .&lt;/p&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/google-research/timesfm.git
cd timesfm
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Code Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import timesfm
model = timesfm.TimesFM_2p5_200M_torch()
model.load_checkpoint()
model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>EbookFoundation/free-programming-books</title>
      <link>https://github.com/EbookFoundation/free-programming-books</link>
      <description>&lt;p&gt;üìö Freely available programming books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;List of Free Learning Resources In Many Languages&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg?sanitize=true" alt="License: CC BY 4.0" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2023-10-01..2023-10-31"&gt;&lt;img src="https://img.shields.io/github/hacktoberfest/2023/EbookFoundation/free-programming-books?label=Hacktoberfest+2023" alt="Hacktoberfest 2023 stats" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Search the list at &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;https://ebookfoundation.github.io/free-programming-books-search/&lt;/a&gt; &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Dynamic%20search%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F" alt="https://ebookfoundation.github.io/free-programming-books-search/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This page is available as an easy-to-read website. Access it by clicking on &lt;a href="https://ebookfoundation.github.io/free-programming-books/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Static%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F" alt="https://ebookfoundation.github.io/free-programming-books/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;form action="https://ebookfoundation.github.io/free-programming-books-search"&gt; 
  &lt;input type="text" id="fpbSearch" name="search" required placeholder="Search Book or Author" /&gt; 
  &lt;label for="submit"&gt; &lt;/label&gt; 
  &lt;input type="submit" id="submit" name="submit" value="Search" /&gt; 
 &lt;/form&gt; 
&lt;/div&gt; 
&lt;h2&gt;Intro&lt;/h2&gt; 
&lt;p&gt;This list was originally a clone of &lt;a href="https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926"&gt;StackOverflow - List of Freely Available Programming Books&lt;/a&gt; with contributions from Karan Bhangui and George Stocker.&lt;/p&gt; 
&lt;p&gt;The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of &lt;a href="https://octoverse.github.com/"&gt;GitHub's most popular repositories&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/network"&gt;&lt;img src="https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Forks" alt="GitHub repo forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars" alt="GitHub repo stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Contributors" alt="GitHub repo contributors" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/sponsors/EbookFoundation"&gt;&lt;img src="https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Sponsors" alt="GitHub org sponsors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers" alt="GitHub repo watchers" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip"&gt;&lt;img src="https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size" alt="GitHub repo size" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;a href="https://ebookfoundation.org"&gt;Free Ebook Foundation&lt;/a&gt; now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. &lt;a href="https://ebookfoundation.org/contributions.html"&gt;Donations&lt;/a&gt; to the Free Ebook Foundation are tax-deductible in the US.&lt;/p&gt; 
&lt;h2&gt;How To Contribute&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;. If you're new to GitHub, &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;welcome&lt;/a&gt;! Remember to abide by our adapted from &lt;img src="https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg?sanitize=true" alt="Contributor Covenant 1.3" /&gt; &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; too (&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/#translations"&gt;translations&lt;/a&gt; also available).&lt;/p&gt; 
&lt;p&gt;Click on these badges to see how you might be able to help:&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/issues"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=red&amp;amp;label=Issues" alt="GitHub repo Issues" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Good%20First%20issues" alt="GitHub repo Good Issues for newbies" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20issues" alt="GitHub Help Wanted issues" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=orange&amp;amp;label=PRs" alt="GitHub repo PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged"&gt;&lt;img src="https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Merged%20PRs&amp;amp;query=is%3Amerged" alt="GitHub repo Merged PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20PRs" alt="GitHub Help Wanted PRs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;How To Share&lt;/h2&gt; 
&lt;div align="left" markdown="1"&gt; 
 &lt;a href="https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;amp;p%5Bimages%5D%5B0%5D=&amp;amp;p%5Btitle%5D=Free%20Programming%20Books&amp;amp;p%5Bsummary%5D="&gt;Share on Facebook&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on LinkedIn&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://toot.kytta.dev/?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on Mastodon/Fediverse&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Telegram&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books"&gt;Share on ùïè (Twitter)&lt;/a&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;This project lists books and other resources grouped by genres:&lt;/p&gt; 
&lt;h3&gt;Books&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-langs.md"&gt;English, By Programming Language&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-subjects.md"&gt;English, By Subject&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Other Languages&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ar.md"&gt;Arabic / al arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hy.md"&gt;Armenian / ’Ä’°’µ’•÷Ä’•’∂&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-az.md"&gt;Azerbaijani / –ê–∑”ô—Ä–±–∞—ò“π–∞–Ω –¥–∏–ª–∏ / ÿ¢ÿ∞ÿ±ÿ®ÿßŸäÿ¨ÿßŸÜÿ¨ÿß ÿØŸäŸÑŸä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bn.md"&gt;Bengali / ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bg.md"&gt;Bulgarian / –±—ä–ª–≥–∞—Ä—Å–∫–∏&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-my.md"&gt;Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-cs.md"&gt;Czech / ƒçe≈°tina / ƒçesk√Ω jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ca.md"&gt;Catalan / catalan/ catal√†&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-da.md"&gt;Danish / dansk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-et.md"&gt;Estonian / eesti keel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fr.md"&gt;French / fran√ßais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-el.md"&gt;Greek / ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-he.md"&gt;Hebrew / ◊¢◊ë◊®◊ô◊™&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hi.md"&gt;Hindi / ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hu.md"&gt;Hungarian / magyar / magyar nyelv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ja.md"&gt;Japanese / Êó•Êú¨Ë™û&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ko.md"&gt;Korean / ÌïúÍµ≠Ïñ¥&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-lv.md"&gt;Latvian / Latvie≈°u&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ml.md"&gt;Malayalam / ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fa_IR.md"&gt;Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pl.md"&gt;Polish / polski / jƒôzyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ro.md"&gt;Romanian (Romania) / limba rom√¢nƒÉ / rom√¢n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ru.md"&gt;Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sr.md"&gt;Serbian / —Å—Ä–ø—Å–∫–∏ —ò–µ–∑–∏–∫ / srpski jezik&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sk.md"&gt;Slovak / slovenƒçina&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-es.md"&gt;Spanish / espa√±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ta.md"&gt;Tamil / ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-te.md"&gt;Telugu / ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-th.md"&gt;Thai / ‡πÑ‡∏ó‡∏¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-tr.md"&gt;Turkish / T√ºrk√ße&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-uk.md"&gt;Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-vi.md"&gt;Vietnamese / Ti·∫øng Vi·ªát&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cheat Sheets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-cheatsheets.md"&gt;All Languages&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Free Online Courses&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ar.md"&gt;Arabic / al arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bn.md"&gt;Bengali / ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bg.md"&gt;Bulgarian / –±—ä–ª–≥–∞—Ä—Å–∫–∏&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-my.md"&gt;Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fr.md"&gt;French / fran√ßais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-el.md"&gt;Greek / ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-he.md"&gt;Hebrew / ◊¢◊ë◊®◊ô◊™&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-hi.md"&gt;Hindi / ‡§π‡§ø‡§Ç‡§¶‡•Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ja.md"&gt;Japanese / Êó•Êú¨Ë™û&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kn.md"&gt;Kannada/‡≤ï‡≤®‡≥ç‡≤®‡≤°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kk.md"&gt;Kazakh / “õ–∞–∑–∞“õ—à–∞&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-km.md"&gt;Khmer / ·ûó·û∂·ûü·û∂·ûÅ·üí·ûò·üÇ·ûö&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ko.md"&gt;Korean / ÌïúÍµ≠Ïñ¥&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ml.md"&gt;Malayalam / ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-mr.md"&gt;Marathi / ‡§Æ‡§∞‡§æ‡§†‡•Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ne.md"&gt;Nepali / ‡§®‡•á‡§™‡§æ‡§≤‡•Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fa_IR.md"&gt;Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pl.md"&gt;Polish / polski / jƒôzyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ru.md"&gt;Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-si.md"&gt;Sinhala / ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-es.md"&gt;Spanish / espa√±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-sv.md"&gt;Swedish / svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ta.md"&gt;Tamil / ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-te.md"&gt;Telugu / ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-th.md"&gt;Thai / ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-tr.md"&gt;Turkish / T√ºrk√ße&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-uk.md"&gt;Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ur.md"&gt;Urdu / ÿßÿ±ÿØŸà&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-vi.md"&gt;Vietnamese / Ti·∫øng Vi·ªát&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Interactive Programming Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ja.md"&gt;Japanese / Êó•Êú¨Ë™û&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ru.md"&gt;Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Problem Sets and Competitive Programming&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/problem-sets-competitive-programming.md"&gt;Problem Sets&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Podcast - Screencast&lt;/h3&gt; 
&lt;p&gt;Free Podcasts and Screencasts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ar.md"&gt;Arabic / al Arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-my.md"&gt;Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-cs.md"&gt;Czech / ƒçe≈°tina / ƒçesk√Ω jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fi.md"&gt;Finnish / Suomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fr.md"&gt;French / fran√ßais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-he.md"&gt;Hebrew / ◊¢◊ë◊®◊ô◊™&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fa_IR.md"&gt;Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pl.md"&gt;Polish / polski / jƒôzyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ru.md"&gt;Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-si.md"&gt;Sinhala / ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-es.md"&gt;Spanish / espa√±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-tr.md"&gt;Turkish / T√ºrk√ße&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-uk.md"&gt;Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Programming Playgrounds&lt;/h3&gt; 
&lt;p&gt;Write, compile, and run your code within a browser. Try it out!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;How-to&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;... &lt;em&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;More languages&lt;/a&gt;&lt;/em&gt; ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might notice that there are &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;some missing translations here&lt;/a&gt; - perhaps you would like to help out by &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md#help-out-by-contributing-a-translation"&gt;contributing a translation&lt;/a&gt;?&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Each file included in this repository is licensed under the &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/LICENSE"&gt;CC BY License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;üî• Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÁÆ±Âç≥Áî®&lt;/strong&gt;: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êòì‰∫éÈõÜÊàê&lt;/strong&gt;: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ÂèØÊéß&lt;/strong&gt;: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Â∑•‰ΩúÂéüÁêÜ&lt;/h2&gt; 
&lt;img width="1105" height="577" alt="system-arch" src="https://github.com/user-attachments/assets/462603fc-980b-4b8b-a6d4-a821c070a048" /&gt; 
&lt;h2&gt;Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;ÂÆâË£ÖÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÂÆâË£ÖÂ•Ω &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/file:/opt/sqlbot/data/file \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‰Ω†‰πüÂèØ‰ª•ÈÄöËøá &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel Â∫îÁî®ÂïÜÂ∫ó&lt;/a&gt; Âø´ÈÄüÈÉ®ÁΩ≤ SQLBot„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊòØÂÜÖÁΩëÁéØÂ¢ÉÔºå‰Ω†ÂèØ‰ª•ÈÄöËøá &lt;a href="https://community.fit2cloud.com/#/products/sqlbot/downloads"&gt;Á¶ªÁ∫øÂÆâË£ÖÂåÖÊñπÂºè&lt;/a&gt; ÈÉ®ÁΩ≤ SQLBot„ÄÇ&lt;/p&gt; 
&lt;h3&gt;ËÆøÈóÆÊñπÂºè&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&amp;lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;Áî®Êà∑Âêç: admin&lt;/li&gt; 
 &lt;li&gt;ÂØÜÁ†Å: SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÅîÁ≥ªÊàë‰ª¨&lt;/h3&gt; 
&lt;p&gt;Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ&lt;/p&gt; 
&lt;img width="180" height="180" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI Â±ïÁ§∫&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1Panel-dev/CordysCRM"&gt;Cordys CRM&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫ê AI CRM Á≥ªÁªü&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ªìÂ∫ìÈÅµÂæ™ &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ&lt;/p&gt; 
&lt;p&gt;‰Ω†ÂèØ‰ª•Âü∫‰∫é SQLBot ÁöÑÊ∫ê‰ª£Á†ÅËøõË°å‰∫åÊ¨°ÂºÄÂèëÔºå‰ΩÜÊòØÈúÄË¶ÅÈÅµÂÆà‰ª•‰∏ãËßÑÂÆöÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‰∏çËÉΩÊõøÊç¢Âíå‰øÆÊîπ SQLBot ÁöÑ Logo ÂíåÁâàÊùÉ‰ø°ÊÅØÔºõ&lt;/li&gt; 
 &lt;li&gt;‰∫åÊ¨°ÂºÄÂèëÂêéÁöÑË°çÁîü‰ΩúÂìÅÂøÖÈ°ªÈÅµÂÆà GPL V3 ÁöÑÂºÄÊ∫ê‰πâÂä°„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Â¶ÇÈúÄÂïÜ‰∏öÊéàÊùÉÔºåËØ∑ËÅîÁ≥ª &lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt; „ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>unslothai/unsloth</title>
      <link>https://github.com/unslothai/unsloth</link>
      <description>&lt;p&gt;Fine-tuning &amp; Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with 70% less VRAM.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://unsloth.ai"&gt;
   &lt;picture&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png" /&gt; 
    &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" /&gt; 
    &lt;img alt="unsloth logo" src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png" height="110" style="max-width: 100%;" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png" width="154" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/unsloth"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png" width="165" /&gt;&lt;/a&gt; &lt;a href="https://docs.unsloth.ai"&gt;&lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png" width="137" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;Finetune gpt-oss, Gemma 3n, Qwen3, Llama 4, &amp;amp; Mistral 2x faster with 80% less VRAM!&lt;/h3&gt; 
 &lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt="" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® Finetune for Free&lt;/h2&gt; 
&lt;p&gt;Notebooks are beginner friendly. Read our &lt;a href="https://docs.unsloth.ai/get-started/fine-tuning-guide"&gt;guide&lt;/a&gt;. Add your dataset, click "Run All", and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Unsloth supports&lt;/th&gt; 
   &lt;th&gt;Free Notebooks&lt;/th&gt; 
   &lt;th&gt;Performance&lt;/th&gt; 
   &lt;th&gt;Memory use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;gpt-oss (20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3n (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (4B): GRPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5-VL (7B): GSPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_5_7B_VL_GRPO.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;80% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 Vision (11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2x faster&lt;/td&gt; 
   &lt;td&gt;70% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.2x faster&lt;/td&gt; 
   &lt;td&gt;75% less&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Orpheus-TTS (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb"&gt;‚ñ∂Ô∏è Start for free&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5x faster&lt;/td&gt; 
   &lt;td&gt;50% less&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;See all our notebooks for: &lt;a href="https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks"&gt;Kaggle&lt;/a&gt;, &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks"&gt;GRPO&lt;/a&gt;, &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#text-to-speech-tts-notebooks"&gt;TTS&lt;/a&gt;&lt;/strong&gt; &amp;amp; &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-multimodal-notebooks"&gt;Vision&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;all our models&lt;/a&gt; and &lt;a href="https://github.com/unslothai/notebooks"&gt;all our notebooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See detailed documentation for Unsloth &lt;a href="https://docs.unsloth.ai/"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Quickstart&lt;/h2&gt; 
&lt;h3&gt;Linux or WSL&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;For Windows, &lt;code&gt;pip install unsloth&lt;/code&gt; works only if you have Pytorch installed. For more info, read our &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation"&gt;Windows Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;Use our official &lt;a href="https://hub.docker.com/r/unsloth/unsloth"&gt;Unsloth Docker image&lt;/a&gt; &lt;code&gt;unsloth/unsloth&lt;/code&gt; container. Read our &lt;a href="https://docs.unsloth.ai/get-started/install-and-update/docker"&gt;Docker Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Blackwell&lt;/h3&gt; 
&lt;p&gt;For RTX 50x, B200, 6000 GPUs, simply do &lt;code&gt;pip install unsloth&lt;/code&gt;. Read our &lt;a href="https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth"&gt;Blackwell Guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;ü¶• Unsloth.ai News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;Vision RL&lt;/strong&gt; You can now train VLMs with GRPO or GSPO in Unsloth! &lt;a href="https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl"&gt;Read guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;Memory-efficient RL&lt;/strong&gt; We're introducing even better RL. Our new kernels &amp;amp; algos allows faster RL with 50% less VRAM &amp;amp; 10√ó more context. &lt;a href="https://docs.unsloth.ai/new/memory-efficient-rl"&gt;Read blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;gpt-oss&lt;/strong&gt; by OpenAI: For details on &lt;a href="https://docs.unsloth.ai/new/long-context-gpt-oss-training"&gt;Unsloth Flex Attention&lt;/a&gt;, long-context training, bug fixes, &lt;a href="https://docs.unsloth.ai/basics/gpt-oss"&gt;Read our Guide&lt;/a&gt;. 20B works on a 14GB GPU and 120B on 65GB VRAM. &lt;a href="https://huggingface.co/collections/unsloth/gpt-oss-6892433695ce0dee42f31681"&gt;gpt-oss uploads&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;Gemma 3n&lt;/strong&gt; by Google: &lt;a href="https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune"&gt;Read Blog&lt;/a&gt;. We &lt;a href="https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339"&gt;uploaded GGUFs, 4-bit models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;Text-to-Speech (TTS)&lt;/a&gt;&lt;/strong&gt; is now supported, including &lt;code&gt;sesame/csm-1b&lt;/code&gt; and STT &lt;code&gt;openai/whisper-large-v3&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune"&gt;Qwen3&lt;/a&gt;&lt;/strong&gt; is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.&lt;/li&gt; 
 &lt;li&gt;üì£ Introducing &lt;strong&gt;&lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs"&gt;Dynamic 2.0&lt;/a&gt;&lt;/strong&gt; quants that set new benchmarks on 5-shot MMLU &amp;amp; KL Divergence.&lt;/li&gt; 
 &lt;li&gt;üì£ &lt;a href="https://unsloth.ai/blog/gemma3#everything"&gt;&lt;strong&gt;EVERYTHING&lt;/strong&gt; is now supported&lt;/a&gt; - all models (BERT, diffusion, Cohere, Mamba), FFT, etc. MultiGPU coming soon. Enable FFT with &lt;code&gt;full_finetuning = True&lt;/code&gt;, 8-bit with &lt;code&gt;load_in_8bit = True&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for more news&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;DeepSeek-R1&lt;/a&gt; - run or fine-tune them &lt;a href="https://unsloth.ai/blog/deepseek-r1"&gt;with our guide&lt;/a&gt;. All model uploads: &lt;a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ Introducing Long-context &lt;a href="https://unsloth.ai/blog/grpo"&gt;Reasoning (GRPO)&lt;/a&gt; in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ Introducing Unsloth &lt;a href="https://unsloth.ai/blog/dynamic-4bit"&gt;Dynamic 4-bit Quantization&lt;/a&gt;! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using &amp;lt;10% more VRAM than BnB 4-bit. See our collection on &lt;a href="https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7"&gt;Hugging Face here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;strong&gt;&lt;a href="https://unsloth.ai/blog/llama4"&gt;Llama 4&lt;/a&gt;&lt;/strong&gt; by Meta, including Scout &amp;amp; Maverick are now supported.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;a href="https://unsloth.ai/blog/phi4"&gt;Phi-4&lt;/a&gt; by Microsoft: We also &lt;a href="https://unsloth.ai/blog/phi4"&gt;fixed bugs&lt;/a&gt; in Phi-4 and &lt;a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa"&gt;uploaded GGUFs, 4-bit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;a href="https://unsloth.ai/blog/vision"&gt;Vision models&lt;/a&gt; now supported! &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb"&gt;Llama 3.2 Vision (11B)&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb"&gt;Qwen 2.5 VL (7B)&lt;/a&gt; and &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb"&gt;Pixtral (12B) 2409&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ &lt;a href="https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f"&gt;Llama 3.3 (70B)&lt;/a&gt;, Meta's latest model is supported.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ We worked with Apple to add &lt;a href="https://arxiv.org/abs/2411.09009"&gt;Cut Cross Entropy&lt;/a&gt;. Unsloth now supports 89K context for Meta's Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ We found and helped fix a &lt;a href="https://unsloth.ai/blog/gradient"&gt;gradient accumulation bug&lt;/a&gt;! Please update Unsloth and transformers.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;üì£ We cut memory usage by a &lt;a href="https://unsloth.ai/blog/long-context"&gt;further 30%&lt;/a&gt; and now support &lt;a href="https://unsloth.ai/blog/long-context"&gt;4x longer context windows&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üîó Links and Resources&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Links&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üìö &lt;strong&gt;Documentation &amp;amp; Wiki&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai"&gt;Read Our Docs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="16" src="https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg?sanitize=true" /&gt;&amp;nbsp; &lt;strong&gt;Twitter (aka X)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitter.com/unslothai"&gt;Follow us on X&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üíæ &lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;Pip install&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîÆ &lt;strong&gt;Our Models&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth Releases&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úçÔ∏è &lt;strong&gt;Blog&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://unsloth.ai/blog"&gt;Read our Blogs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width="15" src="https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png" /&gt;&amp;nbsp; &lt;strong&gt;Reddit&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://reddit.com/r/unsloth"&gt;Join our Reddit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚≠ê Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports &lt;strong&gt;full-finetuning&lt;/strong&gt;, pretraining, 4b-bit, 16-bit and &lt;strong&gt;8-bit&lt;/strong&gt; training&lt;/li&gt; 
 &lt;li&gt;Supports &lt;strong&gt;all transformer-style models&lt;/strong&gt; including &lt;a href="https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning"&gt;TTS, STT&lt;/a&gt;, multimodal, diffusion, &lt;a href="https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks"&gt;BERT&lt;/a&gt; and more!&lt;/li&gt; 
 &lt;li&gt;All kernels written in &lt;a href="https://openai.com/index/triton/"&gt;OpenAI's Triton&lt;/a&gt; language. &lt;strong&gt;Manual backprop engine&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;0% loss in accuracy&lt;/strong&gt; - no approximation methods - all exact.&lt;/li&gt; 
 &lt;li&gt;No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) &lt;a href="https://developer.nvidia.com/cuda-gpus"&gt;Check your GPU!&lt;/a&gt; GTX 1070, 1080 works, but is slow.&lt;/li&gt; 
 &lt;li&gt;Works on &lt;strong&gt;Linux&lt;/strong&gt; and &lt;strong&gt;Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;If you trained a model with ü¶•Unsloth, you can use this cool sticker! &amp;nbsp; &lt;img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png" width="200" align="center" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üíæ Install Unsloth&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!warning] Python 3.14 does not support Unsloth. Use 3.13 or lower.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can also see our documentation for more detailed installation and updating instructions &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Install with pip (recommended) for Linux devices:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To update Unsloth:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation"&gt;here&lt;/a&gt; for advanced pip install instructions.&lt;/p&gt; 
&lt;h3&gt;Windows Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA Video Driver:&lt;/strong&gt; You should install the latest version of your GPUs driver. Download drivers here: &lt;a href="https://www.nvidia.com/Download/index.aspx"&gt;NVIDIA GPU Drive&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Visual Studio C++:&lt;/strong&gt; You will need Visual Studio, with C++ installed. By default, C++ is not installed with &lt;a href="https://visualstudio.microsoft.com/vs/community/"&gt;Visual Studio&lt;/a&gt;, so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see &lt;a href="https://docs.unsloth.ai/get-started/installing-+-updating"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install CUDA Toolkit:&lt;/strong&gt; Follow the instructions to install &lt;a href="https://developer.nvidia.com/cuda-toolkit-archive"&gt;CUDA Toolkit&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install PyTorch:&lt;/strong&gt; You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully. &lt;a href="https://pytorch.org/get-started/locally/"&gt;Install PyTorch&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Unsloth:&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Notes&lt;/h4&gt; 
&lt;p&gt;To run Unsloth directly on Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install Triton from this Windows fork and follow the instructions &lt;a href="https://github.com/woct0rdho/triton-windows"&gt;here&lt;/a&gt; (be aware that the Windows fork requires PyTorch &amp;gt;= 2.4 and CUDA 12)&lt;/li&gt; 
 &lt;li&gt;In the &lt;code&gt;SFTConfig&lt;/code&gt;, set &lt;code&gt;dataset_num_proc=1&lt;/code&gt; to avoid a crashing issue:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;SFTConfig(
    dataset_num_proc=1,
    ...
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Advanced/Troubleshooting&lt;/h4&gt; 
&lt;p&gt;For &lt;strong&gt;advanced installation instructions&lt;/strong&gt; or if you see weird errors during installations:&lt;/p&gt; 
&lt;p&gt;First try using an isolated environment via then &lt;code&gt;pip install unsloth&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv unsloth
source unsloth/bin/activate
pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;code&gt;torch&lt;/code&gt; and &lt;code&gt;triton&lt;/code&gt;. Go to &lt;a href="https://pytorch.org"&gt;https://pytorch.org&lt;/a&gt; to install it. For example &lt;code&gt;pip install torch torchvision torchaudio triton&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Confirm if CUDA is installed correctly. Try &lt;code&gt;nvcc&lt;/code&gt;. If that fails, you need to install &lt;code&gt;cudatoolkit&lt;/code&gt; or CUDA drivers.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;xformers&lt;/code&gt; manually via:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install ninja
pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs and ignore `xformers`
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;For GRPO runs, you can try installing &lt;code&gt;vllm&lt;/code&gt; and seeing if &lt;code&gt;pip install vllm&lt;/code&gt; succeeds.&lt;/li&gt; 
 &lt;li&gt;Double check that your versions of Python, CUDA, CUDNN, &lt;code&gt;torch&lt;/code&gt;, &lt;code&gt;triton&lt;/code&gt;, and &lt;code&gt;xformers&lt;/code&gt; are compatible with one another. The &lt;a href="https://github.com/pytorch/pytorch/raw/main/RELEASE.md#release-compatibility-matrix"&gt;PyTorch Compatibility Matrix&lt;/a&gt; may be useful.&lt;/li&gt; 
 &lt;li&gt;Finally, install &lt;code&gt;bitsandbytes&lt;/code&gt; and check it with &lt;code&gt;python -m bitsandbytes&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Conda Installation (Optional)&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;‚ö†Ô∏èOnly use Conda if you have it. If not, use Pip&lt;/code&gt;. Select either &lt;code&gt;pytorch-cuda=11.8,12.1&lt;/code&gt; for CUDA 11.8 or CUDA 12.1. We support &lt;code&gt;python=3.10,3.11,3.12&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create --name unsloth_env \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate unsloth_env

pip install unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you're looking to install Conda in a Linux environment, &lt;a href="https://docs.anaconda.com/miniconda/"&gt;read here&lt;/a&gt;, or run the below üîΩ&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm -rf ~/miniconda3/miniconda.sh
~/miniconda3/bin/conda init bash
~/miniconda3/bin/conda init zsh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Pip Installation&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;‚ö†Ô∏èDo **NOT** use this if you have Conda.&lt;/code&gt; Pip is a bit more complex since there are dependency issues. The pip command is different for &lt;code&gt;torch 2.2,2.3,2.4,2.5&lt;/code&gt; and CUDA versions.&lt;/p&gt; 
&lt;p&gt;For other torch versions, we support &lt;code&gt;torch211&lt;/code&gt;, &lt;code&gt;torch212&lt;/code&gt;, &lt;code&gt;torch220&lt;/code&gt;, &lt;code&gt;torch230&lt;/code&gt;, &lt;code&gt;torch240&lt;/code&gt; and for CUDA versions, we support &lt;code&gt;cu118&lt;/code&gt; and &lt;code&gt;cu121&lt;/code&gt; and &lt;code&gt;cu124&lt;/code&gt;. For Ampere devices (A100, H100, RTX3090) and above, use &lt;code&gt;cu118-ampere&lt;/code&gt; or &lt;code&gt;cu121-ampere&lt;/code&gt; or &lt;code&gt;cu124-ampere&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you have &lt;code&gt;torch 2.4&lt;/code&gt; and &lt;code&gt;CUDA 12.1&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Another example, if you have &lt;code&gt;torch 2.5&lt;/code&gt; and &lt;code&gt;CUDA 12.4&lt;/code&gt;, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install "unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And other examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git"

pip install "unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git"
pip install "unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below in a terminal to get the &lt;strong&gt;optimal&lt;/strong&gt; pip installation command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, run the below manually in a Python REPL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;try: import torch
except: raise ImportError('Install torch via `pip install torch`')
from packaging.version import Version as V
import re
v = V(re.match(r"[0-9\.]{3,}", torch.__version__).group(0))
cuda = str(torch.version.cuda)
is_ampere = torch.cuda.get_device_capability()[0] &amp;gt;= 8
USE_ABI = torch._C._GLIBCXX_USE_CXX11_ABI
if cuda not in ("11.8", "12.1", "12.4", "12.6", "12.8"): raise RuntimeError(f"CUDA = {cuda} not supported!")
if   v &amp;lt;= V('2.1.0'): raise RuntimeError(f"Torch = {v} too old!")
elif v &amp;lt;= V('2.1.1'): x = 'cu{}{}-torch211'
elif v &amp;lt;= V('2.1.2'): x = 'cu{}{}-torch212'
elif v  &amp;lt; V('2.3.0'): x = 'cu{}{}-torch220'
elif v  &amp;lt; V('2.4.0'): x = 'cu{}{}-torch230'
elif v  &amp;lt; V('2.5.0'): x = 'cu{}{}-torch240'
elif v  &amp;lt; V('2.5.1'): x = 'cu{}{}-torch250'
elif v &amp;lt;= V('2.5.1'): x = 'cu{}{}-torch251'
elif v  &amp;lt; V('2.7.0'): x = 'cu{}{}-torch260'
elif v  &amp;lt; V('2.7.9'): x = 'cu{}{}-torch270'
elif v  &amp;lt; V('2.8.0'): x = 'cu{}{}-torch271'
elif v  &amp;lt; V('2.8.9'): x = 'cu{}{}-torch280'
else: raise RuntimeError(f"Torch = {v} too new!")
if v &amp;gt; V('2.6.9') and cuda not in ("11.8", "12.6", "12.8"): raise RuntimeError(f"CUDA = {cuda} not supported!")
x = x.format(cuda.replace(".", ""), "-ampere" if is_ampere else "")
print(f'pip install --upgrade pip &amp;amp;&amp;amp; pip install "unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git"')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker Installation&lt;/h3&gt; 
&lt;p&gt;You can use our pre-built Docker container with all dependencies to use Unsloth instantly with no setup required. &lt;a href="https://docs.unsloth.ai/get-started/install-and-update/docker"&gt;Read our guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;This container requires installing &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"&gt;NVIDIA's Container Toolkit&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -e JUPYTER_PASSWORD="mypassword" \
  -p 8888:8888 -p 2222:22 \
  -v $(pwd)/work:/workspace/work \
  --gpus all \
  unsloth/unsloth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access Jupyter Lab at &lt;code&gt;http://localhost:8888&lt;/code&gt; and start fine-tuning!&lt;/p&gt; 
&lt;h2&gt;üìú Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Go to our official &lt;a href="https://docs.unsloth.ai"&gt;Documentation&lt;/a&gt; for saving to GGUF, checkpointing, evaluation and more!&lt;/li&gt; 
 &lt;li&gt;We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!&lt;/li&gt; 
 &lt;li&gt;We're in ü§óHugging Face's official docs! Check out the &lt;a href="https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth"&gt;SFT docs&lt;/a&gt; and &lt;a href="https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth"&gt;DPO docs&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;If you want to download models from the ModelScope community, please use an environment variable: &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt;, and install the modelscope library by: &lt;code&gt;pip install modelscope -U&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;unsloth_cli.py also supports &lt;code&gt;UNSLOTH_USE_MODELSCOPE=1&lt;/code&gt; to download models and datasets. please remember to use the model and dataset id in the ModelScope community.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from unsloth import FastLanguageModel, FastModel
import torch
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
max_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!
# Get LAION dataset
url = "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl"
dataset = load_dataset("json", data_files = {"train" : url}, split = "train")

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 2x faster
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # 4bit for 405b!
    "unsloth/Mistral-Small-Instruct-2409",     # Mistral 22b 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!

    "unsloth/Llama-3.2-1B-bnb-4bit",           # NEW! Llama 3.2 models
    "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
    "unsloth/Llama-3.2-3B-bnb-4bit",
    "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",

    "unsloth/Llama-3.3-70B-Instruct-bnb-4bit" # NEW! Llama 3.3 70B!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/gemma-3-4B-it",
    max_seq_length = 2048, # Choose any for long context!
    load_in_4bit = True,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = "hf_...", # use one if using gated models
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

trainer = SFTTrainer(
    model = model,
    train_dataset = dataset,
    tokenizer = tokenizer,
    args = SFTConfig(
        max_seq_length = max_seq_length,
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 10,
        max_steps = 60,
        logging_steps = 1,
        output_dir = "outputs",
        optim = "adamw_8bit",
        seed = 3407,
    ),
)
trainer.train()

# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like
# (1) Saving to GGUF / merging to 16bit for vLLM
# (2) Continued training from a saved LoRA adapter
# (3) Adding an evaluation loop / OOMs
# (4) Customized chat templates
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a name="RL"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üí° Reinforcement Learning&lt;/h2&gt; 
&lt;p&gt;RL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth. We're in ü§óHugging Face's official docs! We're on the &lt;a href="https://huggingface.co/learn/nlp-course/en/chapter12/6"&gt;GRPO docs&lt;/a&gt; and the &lt;a href="https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth"&gt;DPO docs&lt;/a&gt;! List of RL notebooks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Advanced Qwen3 GRPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ORPO notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DPO Zephyr notebook: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;KTO notebook: &lt;a href="https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SimPO notebook: &lt;a href="https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing"&gt;Link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click for DPO code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # Optional set GPU device ID

from unsloth import FastLanguageModel
import torch
from trl import DPOTrainer, DPOConfig
max_seq_length = 2048

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/zephyr-sft-bnb-4bit",
    max_seq_length = max_seq_length,
    load_in_4bit = True,
)

# Do model patching and add fast LoRA weights
model = FastLanguageModel.get_peft_model(
    model,
    r = 64,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 64,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    max_seq_length = max_seq_length,
)

dpo_trainer = DPOTrainer(
    model = model,
    ref_model = None,
    train_dataset = YOUR_DATASET_HERE,
    # eval_dataset = YOUR_DATASET_HERE,
    tokenizer = tokenizer,
    args = DPOConfig(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 8,
        warmup_ratio = 0.1,
        num_train_epochs = 3,
        logging_steps = 1,
        optim = "adamw_8bit",
        seed = 42,
        output_dir = "outputs",
        max_length = 1024,
        max_prompt_length = 512,
        beta = 0.1,
    ),
)
dpo_trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;ü•á Performance Benchmarking&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For our most detailed benchmarks, read our &lt;a href="https://unsloth.ai/blog/llama3-3"&gt;Llama 3.3 Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Benchmarking of Unsloth was also conducted by &lt;a href="https://huggingface.co/blog/unsloth-trl"&gt;ü§óHugging Face&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We tested using the Alpaca Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;VRAM&lt;/th&gt; 
   &lt;th&gt;ü¶• Unsloth speed&lt;/th&gt; 
   &lt;th&gt;ü¶• VRAM reduction&lt;/th&gt; 
   &lt;th&gt;ü¶• Longer context&lt;/th&gt; 
   &lt;th&gt;üòä Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.3 (70B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;75%&lt;/td&gt; 
   &lt;td&gt;13x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;2x&lt;/td&gt; 
   &lt;td&gt;&amp;gt;70%&lt;/td&gt; 
   &lt;td&gt;12x longer&lt;/td&gt; 
   &lt;td&gt;1x&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Context length benchmarks&lt;/h3&gt; 
&lt;h4&gt;Llama 3.1 (8B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;ü¶•Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8 GB&lt;/td&gt; 
   &lt;td&gt;2,972&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12 GB&lt;/td&gt; 
   &lt;td&gt;21,848&lt;/td&gt; 
   &lt;td&gt;932&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16 GB&lt;/td&gt; 
   &lt;td&gt;40,724&lt;/td&gt; 
   &lt;td&gt;2,551&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24 GB&lt;/td&gt; 
   &lt;td&gt;78,475&lt;/td&gt; 
   &lt;td&gt;5,789&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40 GB&lt;/td&gt; 
   &lt;td&gt;153,977&lt;/td&gt; 
   &lt;td&gt;12,264&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;191,728&lt;/td&gt; 
   &lt;td&gt;15,502&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;342,733&lt;/td&gt; 
   &lt;td&gt;28,454&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Llama 3.3 (70B) max. context length&lt;/h4&gt; 
&lt;p&gt;We tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;GPU VRAM&lt;/th&gt; 
   &lt;th&gt;ü¶•Unsloth context length&lt;/th&gt; 
   &lt;th&gt;Hugging Face + FA2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48 GB&lt;/td&gt; 
   &lt;td&gt;12,106&lt;/td&gt; 
   &lt;td&gt;OOM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;80 GB&lt;/td&gt; 
   &lt;td&gt;89,389&lt;/td&gt; 
   &lt;td&gt;6,916&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://i.ibb.co/sJ7RhGG/image-41.png" alt="" /&gt; &lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;p&gt;You can cite the Unsloth repo as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{unsloth,
  author = {Daniel Han, Michael Han and Unsloth team},
  title = {Unsloth},
  url = {http://github.com/unslothai/unsloth},
  year = {2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Thank You to&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp library&lt;/a&gt; that lets users save models with Unsloth&lt;/li&gt; 
 &lt;li&gt;The Hugging Face team and their &lt;a href="https://github.com/huggingface/trl"&gt;TRL library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/erikwijmans"&gt;Erik&lt;/a&gt; for his help adding &lt;a href="https://github.com/apple/ml-cross-entropy"&gt;Apple's ML Cross Entropy&lt;/a&gt; in Unsloth&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Etherll"&gt;Etherl&lt;/a&gt; for adding support for &lt;a href="https://github.com/unslothai/notebooks/pull/34"&gt;TTS, diffusion and BERT models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;And of course for every single person who has contributed or has used Unsloth!&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>ccxt/ccxt</title>
      <link>https://github.com/ccxt/ccxt</link>
      <description>&lt;p&gt;A cryptocurrency trading API with more than 100 exchanges in JavaScript / TypeScript / Python / C# / PHP / Go&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CCXT ‚Äì CryptoCurrency eXchange Trading Library&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.npmjs.com/package/ccxt"&gt;&lt;img src="https://img.shields.io/npm/dy/ccxt.svg?sanitize=true" alt="NPM Downloads" /&gt;&lt;/a&gt; &lt;a href="https://npmjs.com/package/ccxt"&gt;&lt;img src="https://img.shields.io/npm/v/ccxt.svg?sanitize=true" alt="npm" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/ccxt"&gt;&lt;img src="https://img.shields.io/pypi/v/ccxt.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://www.nuget.org/packages/ccxt"&gt;&lt;img src="https://img.shields.io/nuget/v/ccxt" alt="NuGet version" /&gt;&lt;/a&gt; &lt;a href="https://godoc.org/github.com/ccxt/ccxt/go/v4"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/ccxt/ccxt/go/v4?utm_source=godoc" alt="GoDoc" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ccxt"&gt;&lt;img src="https://img.shields.io/discord/690203284119617602?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ccxt/ccxt/wiki/Exchange-Markets"&gt;&lt;img src="https://img.shields.io/badge/exchanges-106-blue.svg?sanitize=true" alt="Supported Exchanges" /&gt;&lt;/a&gt; &lt;a href="https://x.com/ccxt_official"&gt;&lt;img src="https://img.shields.io/twitter/follow/ccxt_official.svg?style=social&amp;amp;label=CCXT" alt="Follow CCXT at x.com" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A cryptocurrency trading API with more than 100 exchanges in JavaScript / TypeScript / Python / C# / PHP / Go.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#install"&gt;Install&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#usage"&gt;Usage&lt;/a&gt; ¬∑ &lt;a href="https://github.com/ccxt/ccxt/wiki"&gt;Manual&lt;/a&gt; ¬∑ &lt;a href="https://github.com/ccxt/ccxt/wiki/FAQ"&gt;FAQ&lt;/a&gt; ¬∑ &lt;a href="https://github.com/ccxt/ccxt/tree/master/examples"&gt;Examples&lt;/a&gt; ¬∑ &lt;a href="https://github.com/ccxt/ccxt/raw/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#disclaimer"&gt;Disclaimer&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/#social"&gt;Social&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;The &lt;strong&gt;CCXT&lt;/strong&gt; library is used to connect and trade with cryptocurrency exchanges and payment processing services worldwide. It provides quick access to market data for storage, analysis, visualization, indicator development, algorithmic trading, strategy backtesting, bot programming, and related software engineering.&lt;/p&gt; 
&lt;p&gt;It is intended to be used by &lt;strong&gt;coders, developers, technically-skilled traders, data-scientists and financial analysts&lt;/strong&gt; for building trading algorithms.&lt;/p&gt; 
&lt;p&gt;Current feature list:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;support for many cryptocurrency exchanges ‚Äî more coming soon&lt;/li&gt; 
 &lt;li&gt;fully implemented public and private APIs&lt;/li&gt; 
 &lt;li&gt;optional normalized data for cross-exchange analytics and arbitrage&lt;/li&gt; 
 &lt;li&gt;an out of the box unified API that is extremely easy to integrate&lt;/li&gt; 
 &lt;li&gt;works in Node 10.4+, Python 3, PHP 8.1+, netstandard2.0/2.1, Go 1.20+ and web browsers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;See Also&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://tab-trader.com/?utm_source=ccxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/66755907-9c3e8880-eea1-11e9-846e-0bff349ceb87.png" alt="TabTrader" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://tab-trader.com/?utm_source=ccxt"&gt;TabTrader&lt;/a&gt;&lt;/strong&gt; ‚Äì trading on all exchanges in one app. Available on &lt;strong&gt;&lt;a href="https://play.google.com/store/apps/details?id=com.tabtrader.android&amp;amp;referrer=utm_source%3Dccxt"&gt;Android&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://itunes.apple.com/app/apple-store/id1095716562?mt=8"&gt;iOS&lt;/a&gt;&lt;/strong&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://www.freqtrade.io"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/114340585-8e35fa80-9b60-11eb-860f-4379125e2db6.png" alt="Freqtrade" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://www.freqtrade.io"&gt;Freqtrade&lt;/a&gt;&lt;/strong&gt; ‚Äì leading opensource cryptocurrency algorithmic trading software!&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://www.octobot.online"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/132113722-007fc092-7530-4b41-b929-b8ed380b7b2e.png" alt="OctoBot" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://www.octobot.online"&gt;OctoBot&lt;/a&gt;&lt;/strong&gt; ‚Äì cryptocurrency trading bot with an advanced web interface.&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://tokenbot.com/?utm_source=github&amp;amp;utm_medium=ccxt&amp;amp;utm_campaign=algodevs"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152720975-0522b803-70f0-4f18-a305-3c99b37cd990.png" alt="TokenBot" /&gt;&lt;/a&gt;&lt;/sub&gt; &lt;strong&gt;&lt;a href="https://tokenbot.com/?utm_source=github&amp;amp;utm_medium=ccxt&amp;amp;utm_campaign=algodevs"&gt;TokenBot&lt;/a&gt;&lt;/strong&gt; ‚Äì discover and copy the best algorithmic traders in the world.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Certified Cryptocurrency Exchanges&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;logo&lt;/th&gt; 
   &lt;th&gt;id&lt;/th&gt; 
   &lt;th&gt;name&lt;/th&gt; 
   &lt;th align="center"&gt;ver&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;certified&lt;/th&gt; 
   &lt;th align="center"&gt;pro&lt;/th&gt; 
   &lt;th&gt;discount&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/e9419b93-ccb0-46aa-9bff-c883f096274b" alt="binance" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binance&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developers.binance.com/en"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with Binance using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/871cbea7-eebb-4b28-b260-c1c91df0487a" alt="binanceusdm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binanceusdm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance USD‚ìà-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/futures/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with Binance USD‚ìà-M using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/387cfc4e-5f33-48cd-8f5c-cd4854dabf0c" alt="binancecoinm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binancecoinm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance COIN-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/delivery/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with Binance COIN-M using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/register?affiliate_id=35953"&gt;&lt;img src="https://github.com/user-attachments/assets/97a5d0b3-de10-423d-90e1-6620960025ed" alt="bybit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bybit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/register?affiliate_id=35953"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bybit-exchange.github.io/docs/inverse/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="okx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.okx.com/docs-v5/en/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange" alt="Sign up with OKX using CCXT's referral link for a 20% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;&lt;img src="https://github.com/user-attachments/assets/64f988c5-07b6-4652-b5c1-679a6bf67c85" alt="gate" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;gate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;Gate.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.gate.io/docs/developers/apiv4/en/"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d20%25&amp;amp;color=orange" alt="Sign up with Gate.io using CCXT's referral link for a 20% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg" alt="kucoin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;KuCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg" alt="kucoinfutures" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoinfutures&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;KuCoin Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com/futures"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;&lt;img src="https://github.com/user-attachments/assets/fbaa10cc-a277-441d-a5b7-997dd9a87658" alt="bitget" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitget&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;Bitget&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitget.com/api-doc/common/intro"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/b371bc6c-4a8c-489f-87f4-20a913dd8d4b" alt="hyperliquid" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hyperliquid&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hyperliquid.gitbook.io/hyperliquid-docs/for-developers/api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;&lt;img src="https://github.com/user-attachments/assets/c78425ab-78d5-49d6-bd14-db7734798f04" alt="bitmex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitmex.com/app/apiOverview"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d10%25&amp;amp;color=orange" alt="Sign up with BitMEX using CCXT's referral link for a 10% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;&lt;img src="https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg" alt="bingx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bingx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;BingX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bingx-api.github.io/docs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg" alt="htx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;htx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;HTX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huobiapi.github.io/docs/spot/v1/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d15%25&amp;amp;color=orange" alt="Sign up with HTX using CCXT's referral link for a 15% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/137283979-8b2a818d-8633-461b-bfca-de89e8c446b2.jpg" alt="mexc" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mexc&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;MEXC Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://mexcdevelop.github.io/apidocs/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;&lt;img src="https://github.com/user-attachments/assets/0623e9c4-f50e-48c9-82bd-65c3908c3a14" alt="bitmart" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmart&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;BitMart&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developer-pro.bitmart.com/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d30%25&amp;amp;color=orange" alt="Sign up with BitMart using CCXT's referral link for a 30% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg" alt="cryptocom" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cryptocom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;Crypto.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://exchange-docs.crypto.com/exchange/v1/rest-ws/index.html"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d75%25&amp;amp;color=orange" alt="Sign up with Crypto.com using CCXT's referral link for a 75% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182089-1e05fa00-c2ec-11ea-8da9-cc73b45abbbc.jpg" alt="coinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;CoinEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.coinex.com/api/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;&lt;img src="https://github.com/user-attachments/assets/6dd6127b-cc19-4a13-9b29-a98d81f80e98" alt="hashkey" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hashkey&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;HashKey Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hashkeyglobal-apidoc.readme.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/150730761-1a00e5e0-d28c-480f-9e65-089ce3e6ef3b.jpg" alt="woo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;WOO X&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.woox.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d35%25&amp;amp;color=orange" alt="Sign up with WOO X using CCXT's referral link for a 35% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;&lt;img src="https://github.com/user-attachments/assets/9ba21b8a-a9c7-4770-b7f1-ce3bcbde68c1" alt="woofipro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woofipro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;WOOFI PRO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://orderly.network/docs/build-on-evm/building-on-evm"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;&lt;img src="https://img.shields.io/static/v1?label=Fee&amp;amp;message=%2d5%25&amp;amp;color=orange" alt="Sign up with WOOFI PRO using CCXT's referral link for a 5% discount!" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Supported Cryptocurrency Exchanges&lt;/h2&gt; 
&lt;!-- init list --&gt;The CCXT library currently supports the following 102 cryptocurrency exchange markets and trading APIs: 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;logo&lt;/th&gt; 
   &lt;th&gt;id&lt;/th&gt; 
   &lt;th&gt;name&lt;/th&gt; 
   &lt;th align="center"&gt;ver&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;certified&lt;/th&gt; 
   &lt;th&gt;pro&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://alpaca.markets"&gt;&lt;img src="https://github.com/user-attachments/assets/e9476df8-a450-4c3e-ab9a-1a7794219e1b" alt="alpaca" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://alpaca.markets"&gt;Alpaca&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://alpaca.markets/docs/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://omni.apex.exchange/trade"&gt;&lt;img src="https://github.com/user-attachments/assets/fef8f2f7-4265-46aa-965e-33a91881cb00" alt="apex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;apex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://omni.apex.exchange/trade"&gt;Apex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api-docs.pro.apex.exchange"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ascendex.com/en-us/register?inviteCode=EL6BXBQM"&gt;&lt;img src="https://github.com/user-attachments/assets/55bab6b9-d4ca-42a8-a0e6-fac81ae557f1" alt="ascendex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ascendex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ascendex.com/en-us/register?inviteCode=EL6BXBQM"&gt;AscendEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://ascendex.github.io/ascendex-pro-api/#ascendex-pro-api-documentation"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://backpack.exchange/join/ib8qxwyl"&gt;&lt;img src="https://github.com/user-attachments/assets/cc04c278-679f-4554-9f72-930dd632b80f" alt="backpack" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;backpack&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://backpack.exchange/join/ib8qxwyl"&gt;Backpack&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.backpack.exchange/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bequant.io/referral/dd104e3bee7634ec"&gt;&lt;img src="https://github.com/user-attachments/assets/0583ef1f-29fe-4b7c-8189-63565a0e2867" alt="bequant" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bequant&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bequant.io/referral/dd104e3bee7634ec"&gt;Bequant&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.bequant.io/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://b1.run/users/new?code=D3LLBVFT"&gt;&lt;img src="https://github.com/user-attachments/assets/4e5cfd53-98cc-4b90-92cd-0d7b512653d1" alt="bigone" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bigone&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://b1.run/users/new?code=D3LLBVFT"&gt;BigONE&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://open.big.one/docs/api.html"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/e9419b93-ccb0-46aa-9bff-c883f096274b" alt="binance" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binance&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developers.binance.com/en"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/387cfc4e-5f33-48cd-8f5c-cd4854dabf0c" alt="binancecoinm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binancecoinm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance COIN-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/delivery/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.binance.us/?ref=35005074"&gt;&lt;img src="https://github.com/user-attachments/assets/a9667919-b632-4d52-a832-df89f8a35e8c" alt="binanceus" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binanceus&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.binance.us/?ref=35005074"&gt;Binance US&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/binance-us/binance-official-api-docs"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;&lt;img src="https://github.com/user-attachments/assets/871cbea7-eebb-4b28-b260-c1c91df0487a" alt="binanceusdm" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;binanceusdm&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://accounts.binance.com/en/register?ref=D7YA7CLY"&gt;Binance USD‚ìà-M&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://binance-docs.github.io/apidocs/futures/en/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;&lt;img src="https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg" alt="bingx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bingx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bingx.com/invite/OHETOM"&gt;BingX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bingx-api.github.io/docs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bit2c.co.il/Aff/63bfed10-e359-420c-ab5a-ad368dab0baf"&gt;&lt;img src="https://github.com/user-attachments/assets/db0bce50-6842-4c09-a1d5-0c87d22118aa" alt="bit2c" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bit2c&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bit2c.co.il/Aff/63bfed10-e359-420c-ab5a-ad368dab0baf"&gt;Bit2C&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bit2c.co.il/home/api"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitbank.cc/"&gt;&lt;img src="https://github.com/user-attachments/assets/9d616de0-8a88-4468-8e38-d269acab0348" alt="bitbank" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitbank&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitbank.cc/"&gt;bitbank&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.bitbank.cc/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ref.bitbns.com/1090961"&gt;&lt;img src="https://github.com/user-attachments/assets/a5b9a562-cdd8-4bea-9fa7-fd24c1dad3d9" alt="bitbns" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitbns&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ref.bitbns.com/1090961"&gt;Bitbns&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bitbns.com/trade/#/api-trading/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitfinex.com"&gt;&lt;img src="https://github.com/user-attachments/assets/4a8e947f-ab46-481a-a8ae-8b20e9b03178" alt="bitfinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitfinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitfinex.com"&gt;Bitfinex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.bitfinex.com/v2/docs/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitflyer.com"&gt;&lt;img src="https://github.com/user-attachments/assets/d0217747-e54d-4533-8416-0d553dca74bb" alt="bitflyer" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitflyer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitflyer.com"&gt;bitFlyer&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://lightning.bitflyer.com/docs?lang=en"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;&lt;img src="https://github.com/user-attachments/assets/fbaa10cc-a277-441d-a5b7-997dd9a87658" alt="bitget" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitget&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitget.com/expressly?languageType=0&amp;amp;channelCode=ccxt&amp;amp;vipCode=tg9j"&gt;Bitget&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitget.com/api-doc/common/intro"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bithumb.com"&gt;&lt;img src="https://github.com/user-attachments/assets/c9e0eefb-4777-46b9-8f09-9d7f7c4af82d" alt="bithumb" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bithumb&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bithumb.com"&gt;Bithumb&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://apidocs.bithumb.com"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;&lt;img src="https://github.com/user-attachments/assets/0623e9c4-f50e-48c9-82bd-65c3908c3a14" alt="bitmart" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmart&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://www.bitmart.com/?r=rQCFLh"&gt;BitMart&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developer-pro.bitmart.com/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;&lt;img src="https://github.com/user-attachments/assets/c78425ab-78d5-49d6-bd14-db7734798f04" alt="bitmex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitmex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitmex.com/app/register/NZTR1q"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitmex.com/app/apiOverview"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitopro.com"&gt;&lt;img src="https://github.com/user-attachments/assets/affc6337-b95a-44bf-aacd-04f9722364f6" alt="bitopro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitopro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitopro.com"&gt;BitoPro&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/bitoex/bitopro-offical-api-docs/raw/master/v3-1/rest-1/rest.md"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitrue.com/affiliate/landing?cn=600000&amp;amp;inviteCode=EZWETQE"&gt;&lt;img src="https://github.com/user-attachments/assets/67abe346-1273-461a-bd7c-42fa32907c8e" alt="bitrue" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitrue&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitrue.com/affiliate/landing?cn=600000&amp;amp;inviteCode=EZWETQE"&gt;Bitrue&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Bitrue-exchange/bitrue-official-api-docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitso.com/?ref=itej"&gt;&lt;img src="https://github.com/user-attachments/assets/178c8e56-9054-4107-b192-5e5053d4f975" alt="bitso" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitso&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitso.com/?ref=itej"&gt;Bitso&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bitso.com/api_info"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bitstamp.net"&gt;&lt;img src="https://github.com/user-attachments/assets/d5480572-1fee-43cb-b900-d38c522d0024" alt="bitstamp" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitstamp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bitstamp.net"&gt;Bitstamp&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.bitstamp.net/api"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bit.team/auth/sign-up?ref=bitboy2023"&gt;&lt;img src="https://github.com/user-attachments/assets/b41b5e0d-98e5-4bd3-8a6e-aeb230a4a135" alt="bitteam" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitteam&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bit.team/auth/sign-up?ref=bitboy2023"&gt;BIT.TEAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bit.team/trade/api/documentation"&gt;&lt;img src="https://img.shields.io/badge/2.0.6-lightgray" alt="API Version 2.0.6" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bittrade.co.jp/register/?invite_code=znnq3"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/85734211-85755480-b705-11ea-8b35-0b7f1db33a2f.jpg" alt="bittrade" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bittrade&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bittrade.co.jp/register/?invite_code=znnq3"&gt;BitTrade&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api-doc.bittrade.co.jp"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://bitvavo.com/?a=24F34952F7"&gt;&lt;img src="https://github.com/user-attachments/assets/d213155c-8c71-4701-9bd5-45351febc2a8" alt="bitvavo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bitvavo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitvavo.com/?a=24F34952F7"&gt;Bitvavo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.bitvavo.com/"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://blockchain.com"&gt;&lt;img src="https://github.com/user-attachments/assets/975e3054-3399-4363-bcee-ec3c6d63d4e8" alt="blockchaincom" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;blockchaincom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://blockchain.com"&gt;Blockchain.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.blockchain.com/v3"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://blofin.com/register?referral_code=f79EsS"&gt;&lt;img src="https://github.com/user-attachments/assets/518cdf80-f05d-4821-a3e3-d48ceb41d73b" alt="blofin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;blofin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://blofin.com/register?referral_code=f79EsS"&gt;BloFin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://blofin.com/docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://btc-alpha.com/?r=123788"&gt;&lt;img src="https://github.com/user-attachments/assets/dce49f3a-61e5-4ba0-a2fe-41d192fd0e5d" alt="btcalpha" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcalpha&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://btc-alpha.com/?r=123788"&gt;BTC-Alpha&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://btc-alpha.github.io/api-docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.btcbox.co.jp/"&gt;&lt;img src="https://github.com/user-attachments/assets/1e2cb499-8d0f-4f8f-9464-3c015cfbc76b" alt="btcbox" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcbox&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.btcbox.co.jp/"&gt;BtcBox&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://blog.btcbox.jp/en/archives/8762"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://btcmarkets.net"&gt;&lt;img src="https://github.com/user-attachments/assets/8c8d6907-3873-4cc4-ad20-e22fba28247e" alt="btcmarkets" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcmarkets&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://btcmarkets.net"&gt;BTC Markets&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.btcmarkets.net/doc/v3"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.btcturk.com"&gt;&lt;img src="https://github.com/user-attachments/assets/10e0a238-9f60-4b06-9dda-edfc7602f1d6" alt="btcturk" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;btcturk&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.btcturk.com"&gt;BTCTurk&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/BTCTrader/broker-api-docs"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/register?affiliate_id=35953"&gt;&lt;img src="https://github.com/user-attachments/assets/97a5d0b3-de10-423d-90e1-6620960025ed" alt="bybit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;bybit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bybit.com/register?affiliate_id=35953"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://bybit-exchange.github.io/docs/inverse/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://cex.io/r/0/up105393824/0/"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766442-8ddc33b0-5ed8-11e7-8b98-f786aef0f3c9.jpg" alt="cex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cex.io/r/0/up105393824/0/"&gt;CEX.IO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://trade.cex.io/docs/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinbase.com/join/58cbe25a355148797479dbd2"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/40811661-b6eceae2-653a-11e8-829e-10bfadb078cf.jpg" alt="coinbase" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinbase&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinbase.com/join/58cbe25a355148797479dbd2"&gt;Coinbase Advanced&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://developers.coinbase.com/api/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coinbase.com/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/34a65553-88aa-4a38-a714-064bd228b97e" alt="coinbaseexchange" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinbaseexchange&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coinbase.com/"&gt;Coinbase Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.cloud.coinbase.com/exchange/docs/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://international.coinbase.com"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/866ae638-6ab5-4ebf-ab2c-cdcce9545625" alt="coinbaseinternational" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinbaseinternational&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://international.coinbase.com"&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.cloud.coinbase.com/intx/docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://partner.coincatch.cc/bg/92hy70391729607848548"&gt;&lt;img src="https://github.com/user-attachments/assets/3d49065f-f05d-4573-88a2-1b5201ec6ff3" alt="coincatch" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coincatch&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://partner.coincatch.cc/bg/92hy70391729607848548"&gt;CoinCatch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coincatch.github.io/github.io/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coincheck.com"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182088-1d6d6380-c2ec-11ea-9c64-8ab9f9b289f5.jpg" alt="coincheck" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coincheck&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coincheck.com"&gt;coincheck&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coincheck.com/documents/exchange/api"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182089-1e05fa00-c2ec-11ea-8da9-cc73b45abbbc.jpg" alt="coinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinex.com/register?refer_code=yw5fz"&gt;CoinEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.coinex.com/api/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coinmate.io?referral=YTFkM1RsOWFObVpmY1ZjMGREQmpTRnBsWjJJNVp3PT0"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87460806-1c9f3f00-c616-11ea-8c46-a77018a8f3f4.jpg" alt="coinmate" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinmate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coinmate.io?referral=YTFkM1RsOWFObVpmY1ZjMGREQmpTRnBsWjJJNVp3PT0"&gt;CoinMate&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coinmate.docs.apiary.io"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://go.coinmetro.com/?ref=crypto24"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/e86f87ec-6ba3-4410-962b-f7988c5db539" alt="coinmetro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinmetro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://go.coinmetro.com/?ref=crypto24"&gt;Coinmetro&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://documenter.getpostman.com/view/3653795/SVfWN6KS"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coinone.co.kr"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/38003300-adc12fba-323f-11e8-8525-725f53c4a659.jpg" alt="coinone" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinone&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coinone.co.kr"&gt;CoinOne&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.coinone.co.kr"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://coins.ph/"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/225719995-48ab2026-4ddb-496c-9da7-0d7566617c9b.jpg" alt="coinsph" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinsph&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coins.ph/"&gt;Coins.ph&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://coins-docs.github.io/rest-api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.coinspot.com.au/register?code=PJURCU"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/28208429-3cacdf9a-6896-11e7-854e-4c79a772a30f.jpg" alt="coinspot" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;coinspot&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.coinspot.com.au/register?code=PJURCU"&gt;CoinSpot&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.coinspot.com.au/api"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg" alt="cryptocom" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cryptocom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://crypto.com/exch/kdacthrnxt"&gt;Crypto.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://exchange-docs.crypto.com/exchange/v1/rest-ws/index.html"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.cryptomus.com/signup/?ref=JRP4yj"&gt;&lt;img src="https://github.com/user-attachments/assets/8e0b1c48-7c01-4177-9224-f1b01d89d7e7" alt="cryptomus" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;cryptomus&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.cryptomus.com/signup/?ref=JRP4yj"&gt;Cryptomus&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.cryptomus.com/personal"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.defx.com/join/6I2CZ7"&gt;&lt;img src="https://github.com/user-attachments/assets/4e92bace-d7a9-45ea-92be-122168dc87e4" alt="defx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;defx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.defx.com/join/6I2CZ7"&gt;Defx X&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.defx.com/docs"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.delta.exchange/app/signup/?code=IULYNB"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/99450025-3be60a00-2931-11eb-9302-f4fd8d8589aa.jpg" alt="delta" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;delta&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.delta.exchange/app/signup/?code=IULYNB"&gt;Delta Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.delta.exchange"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.deribit.com/reg-1189.4038"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/41933112-9e2dd65a-798b-11e8-8440-5bab2959fcb8.jpg" alt="deribit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;deribit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.deribit.com/reg-1189.4038"&gt;Deribit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.deribit.com/v2"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.derive.xyz/invite/3VB0B"&gt;&lt;img src="https://github.com/user-attachments/assets/f835b95f-033a-43dd-b6bb-24e698fc498c" alt="derive" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;derive&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.derive.xyz/invite/3VB0B"&gt;derive&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.derive.xyz/docs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.digifinex.com/en-ww/from/DhOzBg?channelCode=ljaUPp"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87443315-01283a00-c5fe-11ea-8628-c2a0feaf07ac.jpg" alt="digifinex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;digifinex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.digifinex.com/en-ww/from/DhOzBg?channelCode=ljaUPp"&gt;DigiFinex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.digifinex.com"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://exmo.me/?ref=131685"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766491-1b0ea956-5eda-11e7-9225-40d67b481b8d.jpg" alt="exmo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;exmo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://exmo.me/?ref=131685"&gt;EXMO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://exmo.me/en/api_doc?ref=131685"&gt;&lt;img src="https://img.shields.io/badge/1.1-lightgray" alt="API Version 1.1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://fmfw.io/referral/da948b21d6c92d69"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/159177712-b685b40c-5269-4cea-ac83-f7894c49525d.jpg" alt="fmfwio" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;fmfwio&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://fmfw.io/referral/da948b21d6c92d69"&gt;FMFW.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.fmfw.io/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.foxbit.com.br"&gt;&lt;img src="https://github.com/user-attachments/assets/1f8faca2-ae2f-4222-b33e-5671e7d873dd" alt="foxbit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;foxbit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.foxbit.com.br"&gt;Foxbit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.foxbit.com.br"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;&lt;img src="https://github.com/user-attachments/assets/64f988c5-07b6-4652-b5c1-679a6bf67c85" alt="gate" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;gate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gate.io/signup/2436035"&gt;Gate.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.gate.io/docs/developers/apiv4/en/"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://gemini.com/"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27816857-ce7be644-6096-11e7-82d6-3c257263229c.jpg" alt="gemini" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;gemini&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://gemini.com/"&gt;Gemini&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.gemini.com/rest-api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;&lt;img src="https://github.com/user-attachments/assets/6dd6127b-cc19-4a13-9b29-a98d81f80e98" alt="hashkey" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hashkey&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN"&gt;HashKey Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hashkeyglobal-apidoc.readme.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/hibachi.xyz/r/ZBL2YFWIHU"&gt;&lt;img src="https://github.com/user-attachments/assets/7301bbb1-4f27-4167-8a55-75f74b14e973" alt="hibachi" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hibachi&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/hibachi.xyz/r/ZBL2YFWIHU"&gt;Hibachi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/undefined"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hitbtc.com/?ref_id=5a5d39a65d466"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766555-8eaec20e-5edc-11e7-9c5b-6dc69fc42f5e.jpg" alt="hitbtc" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hitbtc&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hitbtc.com/?ref_id=5a5d39a65d466"&gt;HitBTC&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.hitbtc.com"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://pro.hollaex.com/signup?affiliation_code=QSWA6G"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/75841031-ca375180-5ddd-11ea-8417-b975674c23cb.jpg" alt="hollaex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hollaex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pro.hollaex.com/signup?affiliation_code=QSWA6G"&gt;HollaEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://apidocs.hollaex.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg" alt="htx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;htx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223"&gt;HTX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huobiapi.github.io/docs/spot/v1/en/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/b371bc6c-4a8c-489f-87f4-20a913dd8d4b" alt="hyperliquid" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;hyperliquid&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.hyperliquid.xyz/"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://hyperliquid.gitbook.io/hyperliquid-docs/for-developers/api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.independentreserve.com"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87182090-1e9e9080-c2ec-11ea-8e49-563db9a38f37.jpg" alt="independentreserve" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;independentreserve&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.independentreserve.com"&gt;Independent Reserve&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.independentreserve.com/API"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://indodax.com/ref/testbitcoincoid/1"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87070508-9358c880-c221-11ea-8dc5-5391afbbb422.jpg" alt="indodax" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;indodax&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://indodax.com/ref/testbitcoincoid/1"&gt;INDODAX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/btcid/indodax-official-api-docs"&gt;&lt;img src="https://img.shields.io/badge/2.0-lightgray" alt="API Version 2.0" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kraken.com"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/76173629-fc67fb00-61b1-11ea-84fe-f2de582f58a3.jpg" alt="kraken" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kraken&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kraken.com"&gt;Kraken&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kraken.com/rest/"&gt;&lt;img src="https://img.shields.io/badge/0-lightgray" alt="API Version 0" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://futures.kraken.com/"&gt;&lt;img src="https://user-images.githubusercontent.com/24300605/81436764-b22fd580-9172-11ea-9703-742783e6376d.jpg" alt="krakenfutures" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;krakenfutures&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://futures.kraken.com/"&gt;Kraken Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kraken.com/api/docs/futures-api/trading/market-data/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg" alt="kucoin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kucoin.com/ucenter/signup?rcode=E5wkqe"&gt;KuCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg" alt="kucoinfutures" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;kucoinfutures&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://futures.kucoin.com/?rcode=E5wkqe"&gt;KuCoin Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.kucoin.com/futures"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://latoken.com/invite?r=mvgp2djk"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/61511972-24c39f00-aa01-11e9-9f7c-471f1d6e5214.jpg" alt="latoken" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;latoken&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://latoken.com/invite?r=mvgp2djk"&gt;Latoken&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.latoken.com"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.lbank.com/login/?icode=7QCY"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/38063602-9605e28a-3302-11e8-81be-64b1e53c4cfb.jpg" alt="lbank" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;lbank&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.lbank.com/login/?icode=7QCY"&gt;LBank&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.lbank.com/en-US/docs/index.html"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.luno.com/invite/44893A"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766607-8c1a69d8-5ede-11e7-930c-540b5eb9be24.jpg" alt="luno" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;luno&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.luno.com/invite/44893A"&gt;luno&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.luno.com/en/api"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mercadobitcoin.com.br"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27837060-e7c58714-60ea-11e7-9192-f05e86adb83f.jpg" alt="mercado" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mercado&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.mercadobitcoin.com.br"&gt;Mercado Bitcoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.mercadobitcoin.com.br/api-doc"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/137283979-8b2a818d-8633-461b-bfca-de89e8c446b2.jpg" alt="mexc" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mexc&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.mexc.com/register?inviteCode=mexc-1FQ1GNu1"&gt;MEXC Global&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://mexcdevelop.github.io/apidocs/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://trade.mode.network?ref=MODETRADE"&gt;&lt;img src="https://github.com/user-attachments/assets/cec2b7f1-3b2b-4502-971b-447ee1937d6b" alt="modetrade" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;modetrade&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://trade.mode.network?ref=MODETRADE"&gt;Mode Trade&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/ccxt/ccxt/master/undefined"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.my.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="myokx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;myokx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.my.okx.com/join/CCXT2023"&gt;MyOKX (EEA)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://my.okx.com/docs-v5/en/#overview"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://one.ndax.io/bfQiSL"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/108623144-67a3ef00-744e-11eb-8140-75c6b851e945.jpg" alt="ndax" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ndax&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://one.ndax.io/bfQiSL"&gt;NDAX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://apidoc.ndax.io/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.novadax.com.br/?s=ccxt"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/92337550-2b085500-f0b3-11ea-98e7-5794fb07dd3b.jpg" alt="novadax" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;novadax&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.novadax.com.br/?s=ccxt"&gt;NovaDAX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.novadax.com/pt-BR/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://oceanex.pro/signup?referral=VE24QX"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/58385970-794e2d80-8001-11e9-889c-0567cd79b78e.jpg" alt="oceanex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;oceanex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://oceanex.pro/signup?referral=VE24QX"&gt;OceanEx&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api.oceanex.pro/doc/v1"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.okcoin.com/account/register?flag=activity&amp;amp;channelId=600001513"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87295551-102fbf00-c50e-11ea-90a9-462eebba5829.jpg" alt="okcoin" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okcoin&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okcoin.com/account/register?flag=activity&amp;amp;channelId=600001513"&gt;OKCoin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.okcoin.com/docs/en/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="okx" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okx&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.okx.com/join/CCXT2023"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.okx.com/docs-v5/en/"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.app.okx.com/join/CCXT2023"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/152485636-38b19e4a-bece-4dec-979a-5982859ffc04.jpg" alt="okxus" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;okxus&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.app.okx.com/join/CCXT2023"&gt;OKX (US)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://app.okx.com/docs-v5/en/#overview"&gt;&lt;img src="https://img.shields.io/badge/5-lightgray" alt="API Version 5" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://onetrading.com/"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/bdbc26fd-02f2-4ca7-9f1e-17333690bb1c" alt="onetrading" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;onetrading&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://onetrading.com/"&gt;One Trading&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.onetrading.com"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ox.fun/register?shareAccountId=5ZUD4a7G"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/6a196124-c1ee-4fae-8573-962071b61a85" alt="oxfun" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;oxfun&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ox.fun/register?shareAccountId=5ZUD4a7G"&gt;OXFUN&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.ox.fun/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://p2pb2b.com?referral=ee784c53"&gt;&lt;img src="https://github.com/ccxt/ccxt/assets/43336371/8da13a80-1f0a-49be-bb90-ff8b25164755" alt="p2b" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;p2b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://p2pb2b.com?referral=ee784c53"&gt;p2b&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/P2B-team/p2b-api-docs/raw/master/api-doc.md"&gt;&lt;img src="https://img.shields.io/badge/2-lightgray" alt="API Version 2" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.paradex.trade/r/ccxt24"&gt;&lt;img src="https://github.com/user-attachments/assets/84628770-784e-4ec4-a759-ec2fbb2244ea" alt="paradex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;paradex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.paradex.trade/r/ccxt24"&gt;Paradex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.api.testnet.paradex.trade/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.paymium.com/page/sign-up?referral=eDAzPoRQFMvaAB8sf-qj"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/87153930-f0f02200-c2c0-11ea-9c0a-40337375ae89.jpg" alt="paymium" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;paymium&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.paymium.com/page/sign-up?referral=eDAzPoRQFMvaAB8sf-qj"&gt;Paymium&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Paymium/api-documentation"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://phemex.com/register?referralCode=EDNVJ"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/85225056-221eb600-b3d7-11ea-930d-564d2690e3f6.jpg" alt="phemex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;phemex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://phemex.com/register?referralCode=EDNVJ"&gt;Phemex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://phemex-docs.github.io/#overview"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://poloniex.com/signup?c=UBFZJRPJ"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766817-e9456312-5ee6-11e7-9b3c-b628ca5626a5.jpg" alt="poloniex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;poloniex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://poloniex.com/signup?c=UBFZJRPJ"&gt;Poloniex&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://api-docs.poloniex.com/spot/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.probit.com/r/34608773"&gt;&lt;img src="https://user-images.githubusercontent.com/51840849/79268032-c4379480-7ea2-11ea-80b3-dd96bb29fd0d.jpg" alt="probit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;probit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.probit.com/r/34608773"&gt;ProBit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs-en.probit.com"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://timex.io/?refcode=1x27vNkTbP1uwkCck"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/70423869-6839ab00-1a7f-11ea-8f94-13ae72c31115.jpg" alt="timex" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;timex&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://timex.io/?refcode=1x27vNkTbP1uwkCck"&gt;TimeX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://plasma-relay-backend.timex.io/swagger-ui/index.html"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://tokocrypto.com"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/183870484-d3398d0c-f6a1-4cce-91b8-d58792308716.jpg" alt="tokocrypto" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;tokocrypto&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tokocrypto.com"&gt;Tokocrypto&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.tokocrypto.com/apidocs/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://upbit.com"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/49245610-eeaabe00-f423-11e8-9cba-4b0aed794799.jpg" alt="upbit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;upbit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://upbit.com"&gt;Upbit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.upbit.com/docs/%EC%9A%94%EC%B2%AD-%EC%88%98-%EC%A0%9C%ED%95%9C"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://wx.network"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/84547058-5fb27d80-ad0b-11ea-8711-78ac8b3c7f31.jpg" alt="wavesexchange" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;wavesexchange&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://wx.network"&gt;Waves.Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.wx.network"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://whitebit.com/referral/d9bdf40e-28f2-4b52-b2f9-cd1415d82963"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/66732963-8eb7dd00-ee66-11e9-849b-10d9282bb9e0.jpg" alt="whitebit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;whitebit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://whitebit.com/referral/d9bdf40e-28f2-4b52-b2f9-cd1415d82963"&gt;WhiteBit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/whitebit-exchange/api-docs"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/150730761-1a00e5e0-d28c-480f-9e65-089ce3e6ef3b.jpg" alt="woo" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://woox.io/register?ref=DIJT0CNL"&gt;WOO X&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.woox.io/"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;&lt;img src="https://github.com/user-attachments/assets/9ba21b8a-a9c7-4770-b7f1-ce3bcbde68c1" alt="woofipro" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;woofipro&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dex.woo.org/en/trade?ref=CCXT"&gt;WOOFI PRO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://orderly.network/docs/build-on-evm/building-on-evm"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/DEX-blue.svg?sanitize=true" alt="DEX - Distributed EXchange" title="DEX - Distributed EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ccxt/ccxt/wiki/Certification"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Certified-green.svg?sanitize=true" alt="CCXT Certified" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.xt.com/en/accounts/register?ref=9PTM9VW"&gt;&lt;img src="https://user-images.githubusercontent.com/14319357/232636712-466df2fc-560a-4ca4-aab2-b1d954a58e24.jpg" alt="xt" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;xt&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.xt.com/en/accounts/register?ref=9PTM9VW"&gt;XT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://doc.xt.com/"&gt;&lt;img src="https://img.shields.io/badge/4-lightgray" alt="API Version 4" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ccxt.pro"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Pro-black" alt="CCXT Pro" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.yobit.net"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766910-cdcbfdae-5eea-11e7-9859-03fea873272d.jpg" alt="yobit" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;yobit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.yobit.net"&gt;YoBit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://www.yobit.net/en/api/"&gt;&lt;img src="https://img.shields.io/badge/3-lightgray" alt="API Version 3" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://zaif.jp"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/27766927-39ca2ada-5eeb-11e7-972f-1b4199518ca6.jpg" alt="zaif" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;zaif&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://zaif.jp"&gt;Zaif&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://techbureau-api-document.readthedocs.io/ja/latest/index.html"&gt;&lt;img src="https://img.shields.io/badge/1-lightgray" alt="API Version 1" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://auth.zondaglobal.com/ref/jHlbB4mIkdS1"&gt;&lt;img src="https://user-images.githubusercontent.com/1294454/159202310-a0e38007-5e7c-4ba9-a32f-c8263a0291fe.jpg" alt="zonda" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;zonda&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://auth.zondaglobal.com/ref/jHlbB4mIkdS1"&gt;Zonda&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://docs.zondacrypto.exchange/"&gt;&lt;img src="https://img.shields.io/badge/*-lightgray" alt="API Version *" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/badge/CEX-green.svg?sanitize=true" alt="CEX ‚Äì Centralized EXchange" title="CEX ‚Äì Centralized EXchange" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- end list --&gt; 
&lt;p&gt;The list above is updated frequently, new crypto markets, exchanges, bug fixes, and API endpoints are introduced on a regular basis. See the &lt;a href="https://github.com/ccxt/ccxt/wiki/"&gt;Manual&lt;/a&gt; for more details. If you can't find a cryptocurrency exchange in the list above and want it to be added, post a link to it by opening an issue here on GitHub or send us an email.&lt;/p&gt; 
&lt;p&gt;The library is under &lt;a href="https://github.com/ccxt/ccxt/raw/master/LICENSE.txt"&gt;MIT license&lt;/a&gt;, that means it's absolutely free for any developer to build commercial and opensource software on top of it, but use it at your own risk with no warranties, as is.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;The easiest way to install the CCXT library is to use a package manager:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.npmjs.com/package/ccxt"&gt;ccxt in &lt;strong&gt;NPM&lt;/strong&gt;&lt;/a&gt; (JavaScript / Node v7.6+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.python.org/pypi/ccxt"&gt;ccxt in &lt;strong&gt;PyPI&lt;/strong&gt;&lt;/a&gt; (Python 3.7.0+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://packagist.org/packages/ccxt/ccxt"&gt;ccxt in &lt;strong&gt;Packagist/Composer&lt;/strong&gt;&lt;/a&gt; (PHP 8.1+)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nuget.org/packages/ccxt"&gt;ccxt in &lt;strong&gt;Nuget&lt;/strong&gt;&lt;/a&gt; (netstandard 2.0)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pkg.go.dev/github.com/ccxt/ccxt/go/v4"&gt;ccxt in &lt;strong&gt;GO&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library is shipped as an all-in-one module implementation with minimalistic dependencies and requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/js/"&gt;js/&lt;/a&gt; in JavaScript&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/python/"&gt;python/&lt;/a&gt; in Python (generated from TS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/php/"&gt;php/&lt;/a&gt; in PHP (generated from TS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/cs/"&gt;cs/&lt;/a&gt; in C# (generated from TS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ccxt/ccxt/raw/master/go/"&gt;go/&lt;/a&gt; in Go (generated from TS)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also clone it into your project directory from &lt;a href="https://github.com/ccxt/ccxt"&gt;ccxt GitHub repository&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/ccxt/ccxt.git  # including 1GB of commit history

# or

git clone https://github.com/ccxt/ccxt.git --depth 1  # avoid downloading 1GB of commit history
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;JavaScript (NPM)&lt;/h3&gt; 
&lt;p&gt;JavaScript version of CCXT works in both Node and web browsers. Requires ES6 and &lt;code&gt;async/await&lt;/code&gt; syntax support (Node 7.6.0+). When compiling with Webpack and Babel, make sure it is &lt;a href="https://github.com/ccxt/ccxt/issues/225#issuecomment-331905178"&gt;not excluded&lt;/a&gt; in your &lt;code&gt;babel-loader&lt;/code&gt; config.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.npmjs.com/package/ccxt"&gt;ccxt in &lt;strong&gt;NPM&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;//cjs
var ccxt = require ('ccxt')
console.log (ccxt.exchanges) // print all available exchanges
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-Javascript"&gt;//esm
import {version, exchanges} from 'ccxt';
console.log(version, Object.keys(exchanges));
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;JavaScript (for use with the &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag):&lt;/h3&gt; 
&lt;p&gt;All-in-one browser bundle (dependencies included), served from a CDN of your choice:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;jsDelivr: &lt;a href="https://cdn.jsdelivr.net/npm/ccxt@4.5.5/dist/ccxt.browser.min.js"&gt;https://cdn.jsdelivr.net/npm/ccxt@4.5.5/dist/ccxt.browser.min.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;unpkg: &lt;a href="https://unpkg.com/ccxt@4.5.5/dist/ccxt.browser.min.js"&gt;https://unpkg.com/ccxt@4.5.5/dist/ccxt.browser.min.js&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CDNs are not updated in real-time and may have delays. Defaulting to the most recent version without specifying the version number is not recommended. Please, keep in mind that we are not responsible for the correct operation of those CDN servers.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-HTML"&gt;&amp;lt;script type="text/javascript" src="https://cdn.jsdelivr.net/npm/ccxt@4.5.5/dist/ccxt.browser.min.js"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Creates a global &lt;code&gt;ccxt&lt;/code&gt; object:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;console.log (ccxt.exchanges) // print all available exchanges
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://pypi.python.org/pypi/ccxt"&gt;ccxt in &lt;strong&gt;PyPI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-Python"&gt;import ccxt
print(ccxt.exchanges) # print a list of all available exchange classes
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The library supports concurrent asynchronous mode with asyncio and async/await in Python 3.7.0+&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Python"&gt;import ccxt.async_support as ccxt # link against the asynchronous version of ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;orjson support&lt;/h4&gt; 
&lt;p&gt;CCXT also supports &lt;code&gt;orjson&lt;/code&gt; for parsing JSON since it is much faster than the builtin library. This is especially important when using websockets because some exchanges return big messages that need to be parsed and dispatched as quickly as possible.&lt;/p&gt; 
&lt;p&gt;However, &lt;code&gt;orjson&lt;/code&gt; is not enabled by default because it is not supported by every python interpreter. If you want to opt-in, you just need to install it (&lt;code&gt;pip install orjson&lt;/code&gt;) on your local environment. CCXT will detect the installion and pick it up automatically.&lt;/p&gt; 
&lt;h3&gt;PHP&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://packagist.org/packages/ccxt/ccxt"&gt;ccxt in PHP with &lt;strong&gt;Packagist/Composer&lt;/strong&gt;&lt;/a&gt; (PHP 8.1+)&lt;/p&gt; 
&lt;p&gt;It requires common PHP modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;cURL&lt;/li&gt; 
 &lt;li&gt;mbstring (using UTF-8 is highly recommended)&lt;/li&gt; 
 &lt;li&gt;PCRE&lt;/li&gt; 
 &lt;li&gt;iconv&lt;/li&gt; 
 &lt;li&gt;gmp&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-PHP"&gt;include "ccxt.php";
var_dump (\ccxt\Exchange::$exchanges); // print a list of all available exchange classes
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The library supports concurrent asynchronous mode using tools from &lt;a href="https://reactphp.org/"&gt;ReactPHP&lt;/a&gt; in PHP 8.1+. Read the &lt;a href="https://github.com/ccxt/ccxt/wiki/"&gt;Manual&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;.net/C#&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.nuget.org/packages/ccxt"&gt;ccxt in C# with &lt;strong&gt;Nuget&lt;/strong&gt;&lt;/a&gt; (netstandard 2.0 and netstandard 2.1)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-c#"&gt;using ccxt;
Console.WriteLine(ccxt.Exchanges) // check this later
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Go&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://pkg.go.dev/github.com/ccxt/ccxt/go/v4"&gt;ccxt in GO with &lt;strong&gt;PKG&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;go install github.com/ccxt/ccxt/go/v4@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;import "ccxt"
fmt.Println(ccxt.Exchanges)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;You can get CCXT installed in a container along with all the supported languages and dependencies. This may be useful if you want to contribute to CCXT (e.g. run the build scripts and tests ‚Äî please see the &lt;a href="https://github.com/ccxt/ccxt/raw/master/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; document for the details on that).&lt;/p&gt; 
&lt;p&gt;Using &lt;code&gt;docker-compose&lt;/code&gt; (in the cloned CCXT repository):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker-compose run --rm ccxt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You don't need the Docker image if you're not going to develop CCXT. If you just want to use CCXT ‚Äì&amp;nbsp;just install it as a regular package into your project.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Intro&lt;/h3&gt; 
&lt;p&gt;The CCXT library consists of a public part and a private part. Anyone can use the public part immediately after installation. Public APIs provide unrestricted access to public information for all exchange markets without the need to register a user account or have an API key.&lt;/p&gt; 
&lt;p&gt;Public APIs include the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;market data&lt;/li&gt; 
 &lt;li&gt;instruments/trading pairs&lt;/li&gt; 
 &lt;li&gt;price feeds (exchange rates)&lt;/li&gt; 
 &lt;li&gt;order books&lt;/li&gt; 
 &lt;li&gt;trade history&lt;/li&gt; 
 &lt;li&gt;tickers&lt;/li&gt; 
 &lt;li&gt;OHLC(V) for charting&lt;/li&gt; 
 &lt;li&gt;other public endpoints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In order to trade with private APIs you need to obtain API keys from an exchange's website. It usually means signing up to the exchange and creating API keys for your account. Some exchanges require personal info or identification. Sometimes verification may be necessary as well. In this case you will need to register yourself, this library will not create accounts or API keys for you. Some exchanges expose API endpoints for registering an account, but most exchanges don't. You will have to sign up and create API keys on their websites.&lt;/p&gt; 
&lt;p&gt;Private APIs allow the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;manage personal account info&lt;/li&gt; 
 &lt;li&gt;query account balances&lt;/li&gt; 
 &lt;li&gt;trade by making market and limit orders&lt;/li&gt; 
 &lt;li&gt;deposit and withdraw fiat and crypto funds&lt;/li&gt; 
 &lt;li&gt;query personal orders&lt;/li&gt; 
 &lt;li&gt;get ledger history&lt;/li&gt; 
 &lt;li&gt;transfer funds between accounts&lt;/li&gt; 
 &lt;li&gt;use merchant services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library implements full public and private REST and WebSocket APIs for all exchanges in TypeScript, JavaScript, PHP and Python.&lt;/p&gt; 
&lt;p&gt;The CCXT library supports both camelcase notation (preferred in TypeScript and JavaScript) and underscore notation (preferred in Python and PHP), therefore all methods can be called in either notation or coding style in any language.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;// both of these notations work in JavaScript/Python/PHP
exchange.methodName ()  // camelcase pseudocode
exchange.method_name () // underscore pseudocode
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read the &lt;a href="https://github.com/ccxt/ccxt/wiki/"&gt;Manual&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;JavaScript&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;CCXT now supports ESM and CJS modules&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;CJS&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-JavaScript"&gt;// cjs example
'use strict';
const ccxt = require ('ccxt');

(async function () {
    let kraken    = new ccxt.kraken ()
    let bitfinex  = new ccxt.bitfinex ({ verbose: true })
    let huobipro  = new ccxt.huobipro ()
    let okcoinusd = new ccxt.okcoin ({
        apiKey: 'YOUR_PUBLIC_API_KEY',
        secret: 'YOUR_SECRET_PRIVATE_KEY',
    })

    const exchangeId = 'binance'
        , exchangeClass = ccxt[exchangeId]
        , exchange = new exchangeClass ({
            'apiKey': 'YOUR_API_KEY',
            'secret': 'YOUR_SECRET',
        })

    console.log (kraken.id,    await kraken.loadMarkets ())
    console.log (bitfinex.id,  await bitfinex.loadMarkets  ())
    console.log (huobipro.id,  await huobipro.loadMarkets ())

    console.log (kraken.id,    await kraken.fetchOrderBook (kraken.symbols[0]))
    console.log (bitfinex.id,  await bitfinex.fetchTicker ('BTC/USD'))
    console.log (huobipro.id,  await huobipro.fetchTrades ('ETH/USDT'))

    console.log (okcoinusd.id, await okcoinusd.fetchBalance ())

    // sell 1 BTC/USD for market price, sell a bitcoin for dollars immediately
    console.log (okcoinusd.id, await okcoinusd.createMarketSellOrder ('BTC/USD', 1))

    // buy 1 BTC/USD for $2500, you pay $2500 and receive ‡∏ø1 when the order is closed
    console.log (okcoinusd.id, await okcoinusd.createLimitBuyOrder ('BTC/USD', 1, 2500.00))

    // pass/redefine custom exchange-specific order params: type, amount, price or whatever
    // use a custom order type
    bitfinex.createLimitSellOrder ('BTC/USD', 1, 10, { 'type': 'trailing-stop' })

}) ();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ESM&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-Javascript"&gt;//esm example
import {version, binance} from 'ccxt';

console.log(version);
const exchange = new binance();
const ticker = await exchange.fetchTicker('BTC/USDT');
console.log(ticker);
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-Python"&gt;# coding=utf-8

import ccxt

hitbtc   = ccxt.hitbtc({'verbose': True})
bitmex   = ccxt.bitmex()
huobipro = ccxt.huobipro()
exmo     = ccxt.exmo({
    'apiKey': 'YOUR_PUBLIC_API_KEY',
    'secret': 'YOUR_SECRET_PRIVATE_KEY',
})
kraken = ccxt.kraken({
    'apiKey': 'YOUR_PUBLIC_API_KEY',
    'secret': 'YOUR_SECRET_PRIVATE_KEY',
})

exchange_id = 'binance'
exchange_class = getattr(ccxt, exchange_id)
exchange = exchange_class({
    'apiKey': 'YOUR_API_KEY',
    'secret': 'YOUR_SECRET',
})

hitbtc_markets = hitbtc.load_markets()

print(hitbtc.id, hitbtc_markets)
print(bitmex.id, bitmex.load_markets())
print(huobipro.id, huobipro.load_markets())

print(hitbtc.fetch_order_book(hitbtc.symbols[0]))
print(bitmex.fetch_ticker('BTC/USD'))
print(huobipro.fetch_trades('LTC/USDT'))

print(exmo.fetch_balance())

# sell one ‡∏ø for market price and receive $ right now
print(exmo.id, exmo.create_market_sell_order('BTC/USD', 1))

# limit buy BTC/EUR, you pay ‚Ç¨2500 and receive ‡∏ø1  when the order is closed
print(exmo.id, exmo.create_limit_buy_order('BTC/EUR', 1, 2500.00))

# pass/redefine custom exchange-specific order params: type, amount, price, flags, etc...
kraken.create_market_buy_order('BTC/USD', 1, {'trading_agreement': 'agree'})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;PHP&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-PHP"&gt;include 'ccxt.php';

$poloniex = new \ccxt\poloniex ();
$bittrex  = new \ccxt\bittrex  (array ('verbose' =&amp;gt; true));
$quoinex  = new \ccxt\quoinex   ();
$zaif     = new \ccxt\zaif     (array (
    'apiKey' =&amp;gt; 'YOUR_PUBLIC_API_KEY',
    'secret' =&amp;gt; 'YOUR_SECRET_PRIVATE_KEY',
));
$hitbtc   = new \ccxt\hitbtc   (array (
    'apiKey' =&amp;gt; 'YOUR_PUBLIC_API_KEY',
    'secret' =&amp;gt; 'YOUR_SECRET_PRIVATE_KEY',
));

$exchange_id = 'binance';
$exchange_class = "\\ccxt\\$exchange_id";
$exchange = new $exchange_class (array (
    'apiKey' =&amp;gt; 'YOUR_API_KEY',
    'secret' =&amp;gt; 'YOUR_SECRET',
));

$poloniex_markets = $poloniex-&amp;gt;load_markets ();

var_dump ($poloniex_markets);
var_dump ($bittrex-&amp;gt;load_markets ());
var_dump ($quoinex-&amp;gt;load_markets ());

var_dump ($poloniex-&amp;gt;fetch_order_book ($poloniex-&amp;gt;symbols[0]));
var_dump ($bittrex-&amp;gt;fetch_trades ('BTC/USD'));
var_dump ($quoinex-&amp;gt;fetch_ticker ('ETH/EUR'));
var_dump ($zaif-&amp;gt;fetch_ticker ('BTC/JPY'));

var_dump ($zaif-&amp;gt;fetch_balance ());

// sell 1 BTC/JPY for market price, you pay ¬• and receive ‡∏ø immediately
var_dump ($zaif-&amp;gt;id, $zaif-&amp;gt;create_market_sell_order ('BTC/JPY', 1));

// buy BTC/JPY, you receive ‡∏ø1 for ¬•285000 when the order closes
var_dump ($zaif-&amp;gt;id, $zaif-&amp;gt;create_limit_buy_order ('BTC/JPY', 1, 285000));

// set a custom user-defined id to your order
$hitbtc-&amp;gt;create_order ('BTC/USD', 'limit', 'buy', 1, 3000, array ('clientOrderId' =&amp;gt; '123'));
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;.net/C#&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-C#"&gt;using ccxt; // importing ccxt
namespace Project;
class Project {
    public async static Task CreateOrder() {
        var exchange = new Binance();
        exchange.apiKey = "my api key";
        exchange.secret = "my secret";
        // always use the capitalized method (CreateOrder instead of createOrder)
        var order = await exchange.CreateOrder("BTC/USDT", "limit", "buy", 1, 50);
        Console.WriteLine("Placed Order, order id: " + order.id);
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Go&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;package main
import (
	"github.com/ccxt/ccxt/go/v4/go"
	"fmt"
)

func main() {
	exchange := ccxt.NewBinance(map[string]interface{}{
		"apiKey": "MY KEY",
		"secret": "MY SECRET",
	})
	orderParams := map[string]interface{}{
		"clientOrderId": "myOrderId68768678",
	}

    &amp;lt;-exchange.LoadMarkets()

	order, err := exchange.CreateOrder("BTC/USDT", "limit", "buy", 0.001, ccxt.WithCreateOrderPrice(6000), ccxt.WithCreateOrderParams(orderParams))
	if err != nil {
		if ccxtError, ok := err.(*ccxt.Error); ok {
			if ccxtError.Type == "InvalidOrder" {
				fmt.Println("Invalid order")
			} else {
				fmt.Println("Some other error")
			}
		}
	} else {
		fmt.Println(*order.Id)
	}


    // fetching OHLCV
	ohlcv, err := exchange.FetchOHLCV("BTC/USDT", ccxt.WithFetchOHLCVTimeframe("5m"), ccxt.WithFetchOHLCVLimit(100))

	if err != nil {
		fmt.Println("Error: ", err)
	} else {
		fmt.Println("Got OHLCV!")
	}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Optional parameters&lt;/h4&gt; 
&lt;p&gt;Unlike Javascript/Python/PHP/C# Go does not support "traditional" optional parameters like &lt;code&gt;function a(optional = false)&lt;/code&gt;. However, the CCXT language and structure have some methods with optional params, and since the Go language is transpiled from the Typescript source, we had to find a way of representing them.&lt;/p&gt; 
&lt;p&gt;We have decided to "go" (pun intended) with Option structs and the &lt;code&gt;WithX&lt;/code&gt; methods.&lt;/p&gt; 
&lt;p&gt;For example, this function &lt;code&gt;FetchMyTrades&lt;/code&gt; supports 4 different "optional" parameters, symbol, since, limit, and params.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Golang"&gt;func (this *Binance) FetchMyTrades(options ...FetchMyTradesOptions) ([]Trade, error)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And we can provide them by doing&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Golang"&gt;trades, error := exchange.FetchMyTrades(ccxt.withFetchMyTradesSymbol("BTC/USDT"), ccxt.WithFetchOHLCVLimit(5), ccxt.WithFetchMyTradesParams(orderParams))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Lastly, just because the signature dictates that some argument like &lt;code&gt;symbol&lt;/code&gt; is optional, it will depend from exchange to exchange and you might need to provide it to avoid getting a &lt;code&gt;SymbolRequired&lt;/code&gt; error.&lt;/p&gt; 
&lt;p&gt;You can check different examples in the &lt;code&gt;examples/go&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h2&gt;CCXT CLI&lt;/h2&gt; 
&lt;p&gt;Read the documentation for more information and details: &lt;a href="https://github.com/ccxt/ccxt/tree/master/cli/README.md"&gt;docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;CCXT also provides a command-line interface (CLI) that enables direct interaction with any supported exchange from the terminal. You can quickly check balances, place orders, or fetch trade data‚Äîwithout the need to write or execute custom code. This is especially useful for simple or time-sensitive tasks that don‚Äôt warrant the overhead of building a full application.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The CLI is available as a npm package and can be installed by doing&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;npm i ccxt-cli -g
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;You can use the &lt;code&gt;--help&lt;/code&gt; option to view a general overview of how the CLI works. The tool allows you to invoke any CCXT method by specifying the exchange id, the methodName, and any required arguments.&lt;/p&gt; 
&lt;p&gt;Examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ccxt binance createOrder BTC/USDT market buy 0.1 // places an order
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are not sure which arguments should be provided you can always use the &lt;code&gt;explain&lt;/code&gt; command.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ccxt explain createOrder
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;result:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Method: createOrder
Usage:
  binance createOrder &amp;lt;symbol&amp;gt; &amp;lt;type&amp;gt; &amp;lt;side&amp;gt; &amp;lt;amount&amp;gt; [price] [params]

Arguments:
  - symbol       (required) ‚Äî Market symbol e.g., BTC/USDT
  - type         (required) ‚Äî (no description available)
  - side         (required) ‚Äî order side e.g., buy or sell
  - amount       (required) ‚Äî (no description available)
  - price        (optional) ‚Äî Price per unit of asset e.g., 26000.50
  - params       (optional) ‚Äî Extra parameters for the exchange e.g., { "recvWindow": 5000 }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can easily provide API keys by setting them as environment varibales eg: &lt;code&gt;BINANCE_APIKEY="XXXX"&lt;/code&gt; or adding them to the config file located at &lt;code&gt;$CACHE/config.json&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read the &lt;a href="https://github.com/ccxt/ccxt/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; document before making changes that you would like adopted in the code. Also, read the &lt;a href="https://github.com/ccxt/ccxt/wiki"&gt;Manual&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Support Developer Team&lt;/h2&gt; 
&lt;p&gt;We are investing a significant amount of time into the development of this library. If CCXT made your life easier and you want to help us improve it further, or if you want to speed up development of new features and exchanges, please support us with a tip. We appreciate all contributions!&lt;/p&gt; 
&lt;h3&gt;Sponsors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://vitalitycrypto.com/"&gt;&lt;img src="https://github.com/user-attachments/assets/0981aae2-3e12-4b57-8d2f-c5ae2b3b8b1c" alt="Vitality" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Support this project by becoming a sponsor.&lt;/p&gt; 
&lt;p&gt;[&lt;a href="https://opencollective.com/ccxt#sponsor"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/ccxt/tiers/sponsor/0/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/0/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/1/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/1/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/2/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/2/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/3/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/3/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/4/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/4/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/5/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/5/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/6/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/6/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/7/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/7/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/8/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/8/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/sponsor/9/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/sponsor/9/avatar.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Supporters&lt;/h3&gt; 
&lt;p&gt;Support this project by becoming a supporter. Your avatar will show up here with a link to your website.&lt;/p&gt; 
&lt;p&gt;[&lt;a href="https://opencollective.com/ccxt#supporter"&gt;Become a supporter&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/ccxt/tiers/supporter/0/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/0/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/1/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/1/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/2/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/2/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/3/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/3/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/4/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/4/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/5/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/5/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/6/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/6/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/7/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/7/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/8/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/8/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/ccxt/tiers/supporter/9/website" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/supporter/9/avatar.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Backers&lt;/h3&gt; 
&lt;p&gt;Thank you to all our backers! [&lt;a href="https://opencollective.com/ccxt#backer"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/ccxt#backers" target="_blank"&gt;&lt;img src="https://opencollective.com/ccxt/tiers/backer.svg?width=890" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thank you!&lt;/p&gt; 
&lt;h2&gt;Social&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://twitter.com/ccxt_official"&gt;&lt;img src="https://img.shields.io/twitter/follow/ccxt_official?style=social" alt="Twitter" /&gt;&lt;/a&gt;&lt;/sub&gt; Follow us on Twitter&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://medium.com/@ccxt"&gt;&lt;img src="https://img.shields.io/badge/read-our%20blog-black?logo=medium" alt="Medium" /&gt;&lt;/a&gt;&lt;/sub&gt; Read our blog on Medium&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://discord.gg/dhzSKYU"&gt;&lt;img src="https://img.shields.io/discord/690203284119617602?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/sub&gt; Join our Discord&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://t.me/ccxt_announcements"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Channel-blue?logo=telegram" alt="Telegram Announcements" /&gt;&lt;/a&gt;&lt;/sub&gt; CCXT Channel on Telegram (important announcements)&lt;/li&gt; 
 &lt;li&gt;&lt;sub&gt;&lt;a href="https://t.me/ccxt_chat"&gt;&lt;img src="https://img.shields.io/badge/CCXT-Chat-blue?logo=telegram" alt="Telegram Chat" /&gt;&lt;/a&gt;&lt;/sub&gt; CCXT Chat on Telegram (technical support)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#ccxt/ccxt&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=ccxt/ccxt&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;CCXT is not a service nor a server. CCXT is a software. &lt;strong&gt;CCXT is a free open source non-custodian API broker software under MIT license&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Non-custodian&lt;/strong&gt; means CCXT is not an intermediary in trading, it does not hold traders' money at any point in time, traders install CCXT and use CCXT to talk to exchanges directly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MIT license&lt;/strong&gt; means CCXT can be used for any purpose, but use at your own risk without any warranties.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API broker&lt;/strong&gt; means CCXT is funded with rebates from exchanges' API broker programs and it is an official API broker with many exchanges, all rebates and related fees are handled by the exchanges solely in accordance with exchanges' respective terms and conditions established by each partner exchange.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Free software&lt;/strong&gt; means CCXT is free to use and has no hidden fees, with CCXT traders pay the same trading fees they would pay to the exchanges directly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source&lt;/strong&gt; means anyone is allowed to use it, to look inside the code and to change everything, including other brokers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;For business inquiries: &lt;a href="mailto:info@ccxt.trade"&gt;info@ccxt.trade&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>odoo/odoo</title>
      <link>https://github.com/odoo/odoo</link>
      <description>&lt;p&gt;Odoo. Open Source Apps To Grow Your Business.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Odoo&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://runbot.odoo.com/runbot"&gt;&lt;img src="https://runbot.odoo.com/runbot/badge/flat/1/master.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://www.odoo.com/documentation/master"&gt;&lt;img src="https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&amp;amp;colorA=8F8F8F" alt="Tech Doc" /&gt;&lt;/a&gt; &lt;a href="https://www.odoo.com/forum/help-1"&gt;&lt;img src="https://img.shields.io/badge/master-help-875A7B.svg?style=flat&amp;amp;colorA=8F8F8F" alt="Help" /&gt;&lt;/a&gt; &lt;a href="https://nightly.odoo.com/"&gt;&lt;img src="https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&amp;amp;colorA=8F8F8F" alt="Nightly Builds" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Odoo is a suite of web based open source business apps.&lt;/p&gt; 
&lt;p&gt;The main Odoo Apps include an &lt;a href="https://www.odoo.com/page/crm"&gt;Open Source CRM&lt;/a&gt;, &lt;a href="https://www.odoo.com/app/website"&gt;Website Builder&lt;/a&gt;, &lt;a href="https://www.odoo.com/app/ecommerce"&gt;eCommerce&lt;/a&gt;, &lt;a href="https://www.odoo.com/app/inventory"&gt;Warehouse Management&lt;/a&gt;, &lt;a href="https://www.odoo.com/app/project"&gt;Project Management&lt;/a&gt;, &lt;a href="https://www.odoo.com/app/accounting"&gt;Billing &amp;amp; Accounting&lt;/a&gt;, &lt;a href="https://www.odoo.com/app/point-of-sale-shop"&gt;Point of Sale&lt;/a&gt;, &lt;a href="https://www.odoo.com/app/employees"&gt;Human Resources&lt;/a&gt;, &lt;a href="https://www.odoo.com/app/social-marketing"&gt;Marketing&lt;/a&gt;, &lt;a href="https://www.odoo.com/app/manufacturing"&gt;Manufacturing&lt;/a&gt;, &lt;a href="https://www.odoo.com/"&gt;...&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get a full-featured &lt;a href="https://www.odoo.com"&gt;Open Source ERP&lt;/a&gt; when you install several Apps.&lt;/p&gt; 
&lt;h2&gt;Getting started with Odoo&lt;/h2&gt; 
&lt;p&gt;For a standard installation please follow the &lt;a href="https://www.odoo.com/documentation/master/administration/install/install.html"&gt;Setup instructions&lt;/a&gt; from the documentation.&lt;/p&gt; 
&lt;p&gt;To learn the software, we recommend the &lt;a href="https://www.odoo.com/slides"&gt;Odoo eLearning&lt;/a&gt;, or &lt;a href="https://www.odoo.com/page/scale-up-business-game"&gt;Scale-up, the business game&lt;/a&gt;. Developers can start with &lt;a href="https://www.odoo.com/documentation/master/developer/howtos.html"&gt;the developer tutorials&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;If you believe you have found a security issue, check our &lt;a href="https://www.odoo.com/security-report"&gt;Responsible Disclosure page&lt;/a&gt; for details and get in touch with us via email.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CorentinJ/Real-Time-Voice-Cloning</title>
      <link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link>
      <description>&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; 
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href="https://matheo.uliege.be/handle/2268.2/6801"&gt;master's thesis&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA"&gt;&lt;img src="https://i.imgur.com/8lFUlgz.png" alt="Toolbox demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Papers implemented&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;Designation&lt;/th&gt; 
   &lt;th&gt;Title&lt;/th&gt; 
   &lt;th&gt;Implementation source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf"&gt;1802.08435&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; 
   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1703.10135.pdf"&gt;1703.10135&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; 
   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf"&gt;1710.10467&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GE2E (encoder)&lt;/td&gt; 
   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Heads up&lt;/h2&gt; 
&lt;p&gt;Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out &lt;a href="https://paperswithcode.com/task/speech-synthesis/"&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;Chatterbox&lt;/a&gt; for a similar project up to date with the 2025 SOTA in voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running the toolbox&lt;/h2&gt; 
&lt;p&gt;Both Windows and Linux are supported.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;a href="https://ffmpeg.org/download.html#get-packages"&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files. Check if it's installed by running in a command line&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install uv for python package management&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;# On Windows:
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
# On Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Alternatively, on any platform if you have pip installed you can do
pip install -U uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Run one of the following commands&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;# Run the toolbox if you have an NVIDIA GPU
uv run --extra cuda demo_toolbox.py
# Use this if you don't
uv run --extra cpu demo_toolbox.py

# Run in command line if you don't want the GUI
uv run --extra cuda demo_cli.py
uv run --extra cpu demo_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Uv will automatically create a .venv directory for you with an appropriate python environment. &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues"&gt;Open an issue&lt;/a&gt; if this fails for you&lt;/p&gt; 
&lt;h3&gt;(Optional) Download Pretrained Models&lt;/h3&gt; 
&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;(Optional) Download Datasets&lt;/h3&gt; 
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="https://www.openslr.org/resources/12/train-clean-100.tar.gz"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 80+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt; &lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-80+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;‚Äîpowering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;50,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, and OmniParser&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/organization/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%96_Demo_on_ModelScope-purple" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-purple.svg?logo=huggingface" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 ‚Äî Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 ‚Äî Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 ‚Äî Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;üì£ Recent updates&lt;/h2&gt; 
&lt;h3&gt;üî•üî•2025.08.21: Release of PaddleOCR 3.2.0, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
   &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
   &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
   &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
   &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languages‚ÄîC++, Java, Go, C#, Node.js, and PHP‚Äîfor pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;üî•üî•2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üåê Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;‚úçÔ∏è Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;üéØ &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing ‚Äì Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üßÆ &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;üß† Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding ‚Äì Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;üî• &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;üíª Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;ü§ù Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞ --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["È©æÈ©∂ÂÆ§ÂáÜ‰πò‰∫∫Êï∞"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß© More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚õ∞Ô∏è Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîÑ Quick Overview of Execution Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/blue_v3.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ú® Stay Tuned&lt;/h2&gt; 
&lt;p&gt;‚≠ê &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; ‚≠ê&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üòÉ Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! üíó A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR ‚Äî whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üë©‚Äçüë©‚Äçüëß‚Äçüë¶ Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üåü Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üéì Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/AI-Researcher</title>
      <link>https://github.com/HKUDS/AI-Researcher</link>
      <description>&lt;p&gt;[NeurIPS2025] "AI-Researcher: Autonomous Scientific Innovation" -- A production-ready version: https://novix.science/chat&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/ai-researcher.png" alt="Logo" width="400" /&gt; 
 &lt;h1 align="center"&gt;"AI-Researcher: Autonomous Scientific Innovation" &lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14638" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14638" alt="HKUDS%2FAI-Researcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://autoresearcher.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white" alt="Project Page" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Slack community" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/zBNYTk5q2g"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Discord community" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://autoresearcher.github.io/docs"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2505.18705"&gt;&lt;img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Paper" /&gt;&lt;/a&gt; 
 &lt;a href="https://autoresearcher.github.io/leaderboard"&gt;&lt;img src="https://img.shields.io/badge/DATASETS-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge" alt="Benchmark" /&gt;&lt;/a&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to &lt;strong&gt;AI-Researcher&lt;/strong&gt;ü§ó AI-Researcher introduces a revolutionary breakthrough in &lt;strong&gt;Automated Scientific Discovery&lt;/strong&gt;üî¨, presenting a new system that fundamentally &lt;strong&gt;Reshapes the Traditional Research Paradigm&lt;/strong&gt;. This state-of-the-art platform empowers researchers with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Full Autonomy&lt;/strong&gt;: Complete end-to-end research automation&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Seamless Orchestration&lt;/strong&gt;: From concept to publication&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;Advanced AI Integration&lt;/strong&gt;: Powered by cutting-edge AI agents&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Research Acceleration&lt;/strong&gt;: Streamlined scientific innovation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;‚ú® The AI-Researcher system accepts user input queries at two distinct levels ‚ú®&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 1: Detailed Idea Description&lt;/strong&gt; &lt;br /&gt; At this level, users provide comprehensive descriptions of their specific research ideas. The system processes these detailed inputs to develop implementation strategies based on the user's explicit requirements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Level 2: Reference-Based Ideation&lt;/strong&gt; &lt;br /&gt; This simpler level involves users submitting reference papers without a specific idea in mind. The user query typically follows the format: "I have some reference papers, please come up with an innovative idea and implement it with these papers." The system then analyzes the provided references to generate and develop novel research concepts.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;üåü&lt;strong&gt;Core Capabilities &amp;amp; Integration&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;AI-Researcher&lt;/strong&gt; delivers a &lt;strong&gt;Comprehensive Research Ecosystem&lt;/strong&gt; through seamless integration of critical components:&lt;/p&gt; 
&lt;p&gt;üöÄ&lt;strong&gt;Primary Research Functions&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Literature Review&lt;/strong&gt;: Conducts comprehensive analysis and synthesis of existing research.&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Idea Generation&lt;/strong&gt;: Systematically gathers, organizes, and formulates novel research directions.&lt;/li&gt; 
 &lt;li&gt;üß™ &lt;strong&gt;Algorithm Design and Implementation&lt;/strong&gt;: Develops methodologies and transforms ideas into functional implementations.&lt;/li&gt; 
 &lt;li&gt;üíª &lt;strong&gt;Algorithm Validation and Refinement&lt;/strong&gt;: Automates testing, performance evaluation, and iterative optimization.&lt;/li&gt; 
 &lt;li&gt;üìà &lt;strong&gt;Result Analysis&lt;/strong&gt;: Delivers advanced interpretation of experimental data and insights.&lt;/li&gt; 
 &lt;li&gt;‚úçÔ∏è &lt;strong&gt;Manuscript Creation&lt;/strong&gt;: Automatically generates polished, full-length academic papers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AI-Researchernew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/AI-Researcher-Framework.png" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;br /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Quick Overview of AI-Researcher.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;span id="news"&gt;&lt;/span&gt; 
&lt;h2&gt;üî• News&lt;/h2&gt; 
&lt;div class="scrollable"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025. 09]&lt;/strong&gt;: &amp;nbsp; üéØüéØüì¢üì¢ Exciting News! We are thrilled to announce that our üåüAI-Researcherüåü has been accepted as a Spotlight paper at NeurIPS 2025! üéâüéâ Thanks to all the team members ü§ó  &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025. 05]&lt;/strong&gt;: &amp;nbsp;üéâüéâ &lt;b&gt;Major Release! AI-Researcher Comprehensive Upgrade!&lt;/b&gt; üöÄ &lt;br /&gt;We are excited to announce a significant milestone for AI-Researcher: 
   &lt;ul&gt; 
    &lt;li&gt;üìÑ &lt;b&gt;&lt;a href="https://arxiv.org/abs/2505.18705"&gt;Academic Paper Release&lt;/a&gt;&lt;/b&gt;: Detailed exposition of our innovative methods and experimental results&lt;/li&gt; 
    &lt;li&gt;üìä &lt;b&gt;&lt;a href="https://autoresearcher.github.io/leaderboard"&gt;Benchmark Suite&lt;/a&gt;&lt;/b&gt;: Comprehensive evaluation framework and datasets&lt;/li&gt; 
    &lt;li&gt;üñ•Ô∏è &lt;b&gt;Web GUI Interface&lt;/b&gt;: User-friendly graphical interface making research more convenient&lt;/li&gt; 
   &lt;/ul&gt; &lt;b&gt;ü§ù Join Us!&lt;/b&gt; We welcome researchers, developers, and AI enthusiasts to contribute together and advance AI research development. Whether it's code contributions, bug reports, feature suggestions, or documentation improvements, every contribution is valuable! &lt;br /&gt;üí° &lt;i&gt;Let's build a smarter AI research assistant together!&lt;/i&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Mar 04]&lt;/strong&gt;: &amp;nbsp;üéâüéâWe've launched &lt;b&gt;AI-Researcher!&lt;/b&gt;, The release includes the complete framework, datasets, benchmark construction pipeline, and much more. Stay tuned‚Äîthere's plenty more to come! üöÄ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;span id="table-of-contents"&gt;&lt;/span&gt; 
&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#news"&gt;üî• News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#quick-start"&gt;‚ö° Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#api-keys-setup"&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#examples"&gt;‚¨áÔ∏è Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-it-works"&gt;‚ú® How AI-Researcher works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#how-to-use"&gt;üîç How to use AI-Researcher&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#documentation"&gt;üìñ Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#community"&gt;ü§ù Join the Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#acknowledgements"&gt;üôè Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/#cite"&gt;üåü Cite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="quick-start"&gt;&lt;/span&gt; 
&lt;h2&gt;‚ö° Quick Start&lt;/h2&gt; 
&lt;span id="installation"&gt;&lt;/span&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;AI Installation&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We recommend to use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage packages in our project (Much more faster than conda)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc

# clone the project
git clone https://github.com/HKUDS/AI-Researcher.git
cd AI-Researcher

# install and activate enviroment
uv venv --python 3.11
source ./.venv/bin/activate
uv pip install -e .
playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Installation&lt;/h4&gt; 
&lt;p&gt;To set up the agent-interactive environment, we use Docker for containerization. Please ensure you have &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; installed on your system before proceeding. For running the research agent, we utilize the Docker image 'tjbtech1/airesearcher:v1t'. You can pull this image by executing the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull tjbtech1/airesearcher:v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or you can build the docker image from our provided &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/docker/Dockerfile"&gt;Dockerfile&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ./docker &amp;amp;&amp;amp; docker build -t tjbtech1/airesearcher:v1 .
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="api-keys-setup"&gt;&lt;/span&gt; 
&lt;h3&gt;API Keys Setup&lt;/h3&gt; 
&lt;p&gt;Create an environment variable file based on the provided '.env.template' file. In this file, you should set the configuration including api key, instance id of the test case.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
# ================ container configuration ================
# workplace of the research agent
DOCKER_WORKPLACE_NAME=workplace_paper
# base image of the research agent
BASE_IMAGES=tjbtech1/airesearcher:v1
# completion model name, configuration details see: https://docs.litellm.ai/docs/
COMPLETION_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# cheep model name, configuration details see: https://docs.litellm.ai/docs/
CHEEP_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# specific gpu of the research agent, can be: 
# '"device=0"' using the first gpu
# '"device=0,1"' using the first and second gpu
# '"all"' using all gpus
# None for no gpu
GPUS='"device=0"'
# name of the container
CONTAINER_NAME=paper_eval
# name of the workplace
WORKPLACE_NAME=workplace
# path of the cache
CACHE_PATH=cache
# port of the research agent
PORT=7020
# platform of the research agent
PLATFORM=linux/amd64

# ================ llm configuration ================
# github ai token of the research agent
GITHUB_AI_TOKEN=your_github_ai_token
# openrouter api key of the research agent
OPENROUTER_API_KEY=your_openrouter_api_key
# openrouter api base url of the research agent
OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# ================ task configuration ================
# category of the research agent, based on: ./benchmark/final. Can be: 
# diffu_flow
# gnn
# reasoning
# recommendation
# vq
# example: ./benchmark/final/vq
CATEGORY=vq
# instance id of the research agent, example: ./benchmark/final/vq/one_layer_vq.json
INSTANCE_ID=one_layer_vq
# task level of the research agent, can be: 
# task1
# task2
TASK_LEVEL=task1
# maximum iteration times of the research agent
MAX_ITER_TIMES=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üî• Web GUI&lt;/h3&gt; 
&lt;p&gt;We add a webgui based on gradio. Just run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_ai_researcher.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135137558.png" alt="image-20250606135137558" /&gt;&lt;/p&gt; 
&lt;p&gt;You can configure the environment variables in the following tab:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135325373.png" alt="image-20250606135325373" /&gt;&lt;/p&gt; 
&lt;p&gt;Select the following example to run our AI-Researcher:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/assets/webgui/image-20250606135507970.png" alt="image-20250606135507970" style="zoom:67%;" /&gt; 
&lt;span id="examples"&gt;&lt;/span&gt; 
&lt;h2&gt;‚¨áÔ∏è Examples&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;ALERT&lt;/strong&gt;: The GIFs below are large files and may &lt;strong&gt;take some time to load&lt;/strong&gt;. &lt;strong&gt;Please be patient while they render completely&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Example 1 (Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model designed in this paper is designed to improve the performance of Vector Quantized Variational AutoEncoders (VQ-VAEs) by addressing issues with gradient propagation through the non-differentiable vector quantization layer.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core methodologies utilized include: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Rotation and Rescaling Transformation&lt;/strong&gt;: A linear transformation that alters the encoder output to align it with the nearest codebook vector without changing the forward pass output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation Method&lt;/strong&gt;: The proposed model ensures that gradients flow from the decoder to the encoder while preserving the angle between the gradient and codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Codebook Management&lt;/strong&gt;: Utilizes the connection between the encoder output and the corresponding codebook vectors to mitigate codebook collapse and improve utilization.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The primary functions of these components are: 
    &lt;ul&gt; 
     &lt;li&gt;The rotation and rescaling transformation modifies how the encoder output is quantized and how information is retained during backpropagation, enabling gradients to reflect the true positioning of the encoder output relative to the codebook vectors.&lt;/li&gt; 
     &lt;li&gt;The gradient propagation method redefines how gradients are transported back to the encoder, allowing for an enhanced and nuanced movement through the quantization layer, which leads to a better performance during training.&lt;/li&gt; 
     &lt;li&gt;Codebook management practices help in maintaining a diverse set of codebook vectors throughout training, avoiding scenarios where multiple vectors become redundant or unused.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Codebook size should be configured based on the complexity of the dataset (e.g., 1024 or 8192).&lt;/li&gt; 
       &lt;li&gt;Commitment loss coefficient (Œ≤) is typically set within [0.25, 2].&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Input to the encoder is a continuous high-dimensional vector, while the output is a corresponding quantized vector from the codebook.&lt;/li&gt; 
       &lt;li&gt;The output for reconstruction is generated using the decoder applied to the transformed codebook vectors.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Important Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure that the codebook is updated correctly with an exponential moving average procedure, and treat both rotation and rescaling during the forward pass as constants with respect to the gradient.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Input the data vector into the encoder to obtain the continuous representation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Identify the nearest codebook vector to the encoder output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Compute the rotation matrix that aligns the encoder output to the codebook vector.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Apply the rotation and rescaling transformation to obtain the modified output for the decoder (i.e., `Àú q`).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Feed `Àú q` into the decoder to produce the reconstructed output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 6&lt;/strong&gt;: Compute the loss using the reconstruction and apply backpropagation.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 7&lt;/strong&gt;: During backpropagation, modify the gradient transfer process to maintain the angle using the proposed model, replacing traditional shortcuts in gradient computation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of rotation matrix calculation should ensure computational efficiency‚Äîusing Householder transformations to minimize resource demands.&lt;/li&gt; 
     &lt;li&gt;The deployment of the stop-gradient technique effectively turns off the back-propagation through the quantization layer, which is essential to reflect the intended change without inducing undesired noise in the gradient updates.&lt;/li&gt; 
     &lt;li&gt;Monitor the codebook usage regularly during training to detect any potential collapse early and adjust the training dynamics (e.g., learning rate) accordingly to maintain effective utilization throughout the training period.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Estimating or propagating gradients through stochastic neurons for conditional computation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-resolution image synthesis with latent diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Finite scalar quantization: Vq-vae made simple&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Elements of information theory&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Vector-quantized image modeling with improved vqgan&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Uvim: A unified modeling approach for vision with learned guiding codes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Auto-encoding variational bayes&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/rotation_vq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 2 (Category: Vector Quantized)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on discrete representation learning for tasks such as image generation, depth estimation, colorization, and segmentation using the proposed approach integrated into architectures like autoregressive transformers.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Simplified Quantization&lt;/strong&gt;: Use a simplified quantization approach utilizing scalar quantization instead of VQ.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Dimensionality Projection&lt;/strong&gt;: Define a function to project the encoder output to a manageable dimensionality (typically between 3 to 10).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Gradient Propagation&lt;/strong&gt;: Implement the Straight-Through Estimator (STE) for gradient propagation through the quantization operation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Technical Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Bounding Function&lt;/strong&gt;: This compresses data dimensionality and confines values to a desired range. Use a function like \(f(z) = \left\lfloor \frac{L}{2} \right\rfloor \tanh(z)\) to project the data, where \(L\) is the number of quantization levels.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Quantization process&lt;/strong&gt;: Round each bounded dimension to its nearest integer to yield the quantized output.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt;: Operate under a reconstruction loss paradigm typical in VAEs to optimize the proposed model parameters.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of dimensions \(d\) and levels \(L\) per dimension should be defined based on the codebook size you aim to replicate (e.g., set \(L_i \geq 5\) for all \(i\)).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Input/Output Specifications&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;The input to the bounding function will be the output from the final encoder layer; the output after quantization will be in the format \(\hat{z}\), with shape matching the original \(z\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Ensure all inputs are preprocessed adequately to be within the functioning range of the bounding function.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Integration: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Train a standard VAE model and obtain its encoder output \(z\).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Apply the bounding function \(f\) on \(z\) to limit the output dimensions to usable values.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Quantize the resultant bounded \(z\) using the rounding procedure to generate \( \hat{z} \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Use the original \(z\) and \(\hat{z}\) in conjunction with the reconstruction loss to backpropagate through the network using the STE for gradient calculation.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the rounding process is correctly differentiable; utilize the STE to maintain gradient flow during backpropagation.&lt;/li&gt; 
     &lt;li&gt;Maintain high codebook utilization by selecting optimal dimensions and levels based on empirical trials, and monitor performance to refine the parameters if needed.&lt;/li&gt; 
     &lt;li&gt;Adjust the proposed model configurations (number of epochs, batch size) based on the structures laid out in this paper, ensuring hyperparameters match those recommended for the proposed approach integration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Neural discrete representation learning&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Conditional probability models for deep image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-fidelity generative image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;End-to-end optimized image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Taming transformers for high-resolution image generation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;An algorithm for vector quantizer design&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Joint autoregressive and hierarchical priors for learned image compression&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Assessing generative models via precision and recall&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Variational bayes on discrete representation with self-annealed stochastic quantization&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High quality monocular depth estimation via transfer learning&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/fsq/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 3 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model aims to improve user-item interaction predictions in recommendation systems by leveraging heterogeneous relational information.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques/Algorithms: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Neural Networks (GNNs)&lt;/strong&gt;: Used for embedding initialization and message propagation across different types of user-item and user-user/item-item graphs.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Specifically, a cross-view contrastive learning framework is utilized to enhance representation learning by aligning embeddings from auxiliary views with user-item interaction embeddings.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Networks&lt;/strong&gt;: Employed to extract personalized knowledge and facilitate customized knowledge transfer between auxiliary views and the user-item interaction view.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose and Function of Each Major Component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: Encodes user and item relationships into embeddings that capture the semantics of various interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Provides self-supervision signals to enhance the robustness of learned representations, allowing the proposed model to distinguish between relevant and irrelevant interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: Models personalized characteristics to facilitate adaptive knowledge transfer, ensuring that the influence of auxiliary information is tailored to individual users and items.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Heterogeneous GNN&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use Xavier initializer for embedding initialization; set the hidden dimensionality &lt;code&gt;d&lt;/code&gt;.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Take adjacency matrices for user-item, user-user, and item-item graphs as input; output relation-aware embeddings.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure that the GNN can handle varying types of nodes and relations.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Use cosine similarity as the similarity function; define a temperature coefficient for handling negative samples.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input embeddings from the meta network and user/item views; output contrastive loss values.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Maintain diverse representations to avoid overfitting.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Meta Network&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Key Parameters&lt;/strong&gt;: Set up fully connected layers with PReLU activation to generate personalized transformation matrices.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Input/Output&lt;/strong&gt;: Input user and item embeddings; output transformed embeddings for personalized knowledge transfer.&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;: Ensure low-rank decomposition of transformation matrices to reduce parameter count.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Initialize user and item embeddings using a heterogeneous GNN.&lt;/li&gt; 
     &lt;li&gt;Perform heterogeneous message propagation to refine embeddings iteratively across user-item, user-user, and item-item graphs.&lt;/li&gt; 
     &lt;li&gt;Aggregate the refined embeddings from various views using a mean pooling function to retain heterogeneous semantics.&lt;/li&gt; 
     &lt;li&gt;Extract meta knowledge from the learned embeddings to create personalized mapping functions using the meta network.&lt;/li&gt; 
     &lt;li&gt;Apply contrastive learning to align embeddings from auxiliary views with the user-item interaction embeddings, generating a contrastive loss.&lt;/li&gt; 
     &lt;li&gt;Combine the contrastive loss with a pairwise loss function (like Bayesian Personalized Ranking) to optimize the proposed model.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Choose appropriate hyperparameters such as embedding size, learning rate, and the number of GNN layers through systematic experimentation.&lt;/li&gt; 
     &lt;li&gt;Monitor the proposed model for signs of overfitting, especially when increasing the number of GNN layers or embedding dimensions.&lt;/li&gt; 
     &lt;li&gt;Ensure diverse user-item interaction patterns are captured through sufficient training data and effective augmentation techniques.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Neural Networks for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Knowledge-aware Coupled Graph Neural Network for Social Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Heterogeneous Graph Transformer&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sequential Recommendation with Graph Neural Networks&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/hgcl/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 4 (Category: Recommendation)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on collaborative filtering for recommendation systems by leveraging graph neural networks (GNNs) and contrastive learning to address the issue of sparse user-item interactions.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Core Techniques: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Neural Networks&lt;/strong&gt;: Utilize GNNs for message passing to learn user and item embeddings from the interaction graph.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Disentangled Representations&lt;/strong&gt;: Implement a mechanism to model multiple latent intent factors driving user-item interactions.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Contrastive Learning&lt;/strong&gt;: Use contrastive learning techniques to generate adaptive self-supervised signals from augmented views of user-item interactions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Purpose of Components: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Layers&lt;/strong&gt;: Capture high-order interactions among users and items through iterative message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Encoding&lt;/strong&gt;: Differentiate latent intents to improve the representation of user preferences.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Adaptive Augmentation&lt;/strong&gt;: Generate contrastive views that account for both local and global dependencies to enhance robustness against noise.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Graph Construction&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: User-item interaction matrix \( A \) of size \( I \times J \) (where \( I \) is the number of users and \( J \) is the number of items).&lt;/li&gt; 
       &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Normalized adjacency matrix \( \bar{A} \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;GNN Configuration&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of layers \( L \): Choose based on your dataset, typically 2 or 3 layers.&lt;/li&gt; 
       &lt;li&gt;Dimensionality \( d \) of embeddings: Start with \( d = 32 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Intent Prototypes&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Number of intents \( K \): Experiment with values from {32, 64, 128, 256}, starting with \( K = 128 \).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;: Use Adam optimizer with a learning rate around \( 1e-3 \).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Loss Functions&lt;/strong&gt;: 
      &lt;ul&gt; 
       &lt;li&gt;Use Bayesian Personalized Ranking (BPR) loss for the recommendation task.&lt;/li&gt; 
       &lt;li&gt;Implement InfoNCE loss for contrastive learning, incorporating both local and global augmented views.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-Step Interaction: 
    &lt;ul&gt; 
     &lt;li&gt;Construct the interaction graph from the user-item matrix.&lt;/li&gt; 
     &lt;li&gt;For each GNN layer: 
      &lt;ul&gt; 
       &lt;li&gt;Compute the aggregated embeddings \( Z(u) \) and \( Z(v) \) using the normalized adjacency matrix.&lt;/li&gt; 
       &lt;li&gt;Update user and item embeddings using residual connections to prevent over-smoothing.&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;Generate intent-aware representations by aggregating embeddings over the latent intents.&lt;/li&gt; 
     &lt;li&gt;Apply the learned parameterized masks for adaptive augmentation during message passing to create multiple contrastive views.&lt;/li&gt; 
     &lt;li&gt;Calculate contrastive learning signals using the generated augmented representations and optimize using the combined loss function.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure that the augmentation matrices are learned adaptively based on the current user-item embeddings to differentiate the importance of interactions.&lt;/li&gt; 
     &lt;li&gt;Monitor the performance with different numbers of latent intents \( K \) to find an optimal balance between expressiveness and noise.&lt;/li&gt; 
     &lt;li&gt;Regularly assess the proposed model for over-smoothing by checking the Mean Average Distance (MAD) metric on the embeddings.&lt;/li&gt; 
     &lt;li&gt;Tune hyperparameters \( \lambda_1, \lambda_2, \lambda_3 \) for the multi-task loss to balance the contribution of the self-supervised learning signals.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Lightgcn: Simplifying and powering graph convolution network for recommendation&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Neural collaborative filtering&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled contrastive learning on graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Curriculum Disentangled Recommendation with Noisy Multi-feedback&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Disentangled heterogeneous graph attention network for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning intents behind interactions with knowledge graph for recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Self-supervised graph learning for recommendation&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/dccf/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 5 (Category: Diffusion and Flow Matching)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model presented in this paper focuses on the task of generative modeling through the framework of Continuous Normalizing Flows (CNFs) to define straight flows between noise and data samples.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Architecture: 
    &lt;ul&gt; 
     &lt;li&gt;Implement a neural network to parameterize the velocity field \( v_{\theta}(t, x) \) that maps from noise to data distributions.&lt;/li&gt; 
     &lt;li&gt;Use architectures suitable for continuous functions, such as feedforward or convolutional networks.&lt;/li&gt; 
     &lt;li&gt;Each layer should have non-linear activation functions (e.g., ReLU, Tanh).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Loss Functions: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Velocity Consistency Loss&lt;/strong&gt;: This should be structured as: \[ L_{\theta} = E_{t \sim U} E_{x_t, x_{t+\Delta t}} \| f_{\theta}(t, x_t) - f_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 + \alpha \| v_{\theta}(t, x_t) - v_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 \] where \( f_{\theta}(t, x_t) = x_t + (1 - t) v_{\theta}(t, x_t) \). Choose \( \alpha \) based on cross-validation performance. &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Training Procedure: 
    &lt;ul&gt; 
     &lt;li&gt;Sample \( x_0 \) from the noise distribution \( p_0 \).&lt;/li&gt; 
     &lt;li&gt;For multiple time segments, define intervals and compute velocity fields iteratively.&lt;/li&gt; 
     &lt;li&gt;Use the weights of the proposed approach in an exponential moving average to stabilize training.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Sampling Process: 
    &lt;ul&gt; 
     &lt;li&gt;For single-step or multi-step generation, heuristically sample from the noise distribution and use the learned velocity field as follows: \[ x_{i/k} = x_{(i-1)/k} + \frac{1}{k} v_{i\theta}((i-1)/k, x_{(i-1)/k}) \] &lt;/li&gt; 
     &lt;li&gt;Apply the Euler method for iterative updates: \[ x_{t + \Delta t} = x_t + \Delta t v_i(t, x_t) \] where \( t \in [i/k, (i + 1)/k - \Delta t] \). &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Key Implementation Details: 
    &lt;ul&gt; 
     &lt;li&gt;Ensure the network is equipped with a suitable optimizer such as Adam with a learning rate around \( 2 \times 10^{-4} \).&lt;/li&gt; 
     &lt;li&gt;The batch size should be appropriately set (e.g., 512 for CIFAR-10).&lt;/li&gt; 
     &lt;li&gt;Employ an ODE solver, suggested as Euler's method, during the training and sampling processes.&lt;/li&gt; 
     &lt;li&gt;Maintain a uniform distribution for sampling time intervals \( U \).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance Considerations: 
    &lt;ul&gt; 
     &lt;li&gt;Monitor convergence rates and empirically validate parameter configurations through experiments. Start with fewer segments and gradually increase to capture complex distributions better.&lt;/li&gt; 
     &lt;li&gt;Adjust the decay rate for the EMA based on the stability of convergence (commonly around 0.999).&lt;/li&gt; 
     &lt;li&gt;Analyze the trade-offs between sampling efficiency and sample quality, ensuring a balance during proposed model development.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Flow matching for generative modeling&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Consistency models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Denoising diffusion probabilistic models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Optimal flow matching: Learning straight trajectories in just one step&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Maximum likelihood training of score-based diffusion models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/con_flowmatching/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 6 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed model focuses on the task of node classification in large graphs, addressing challenges like scalability, heterophily, long-range dependencies, and the absence of edges.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques used in this study include a kernelized Gumbel-Softmax operator for all-pair message passing, which reduces computational complexity to linear (O(N)), and a Transformer-style network architecture designed for layer-wise learning of latent graph structures.&lt;/li&gt; 
   &lt;li&gt;The purpose of the kernelized Gumbel-Softmax operator is to enable differentiable learning of discrete graph structures by approximating categorical distributions. The Transformer-style architecture facilitates information propagation between arbitrary pairs of nodes through learned latent graphs.&lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Kernelized Gumbel-Softmax Operator&lt;/strong&gt;: Set the temperature parameter (œÑ) to a range typically between 0.25 and 0.4 for training. It operates on node feature representations (D-dimensional feature vectors). The output of this operator is a distribution over node connections, facilitating the selection of neighbors for message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Node Feature Input&lt;/strong&gt;: Each node input should be represented as a feature vector (e.g., {x_u} ‚àà R^D), and the output is an updated representation of the node embedding after message passing.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Relational Bias (if applicable)&lt;/strong&gt;: Introduces activation (e.g., sigmoid) to adjust the message passing weights based on an observed adjacency matrix, which enhances weight assignment for connected nodes.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Edge Regularization Loss&lt;/strong&gt;: Combines categorical edge probabilities with a supervised classification loss, encouraging the network to maintain predicted edges consistent with observed edges.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The step-by-step interaction of these components includes: 
    &lt;ul&gt; 
     &lt;li&gt;Begin with an input matrix of node embeddings (X) and, if available, an adjacency matrix (A).&lt;/li&gt; 
     &lt;li&gt;Apply the kernelized Gumbel-Softmax operator to the embedding matrix to generate a probability distribution over neighbor selection for each node.&lt;/li&gt; 
     &lt;li&gt;Use these probabilities to sample neighbors, allowing for message passing where each node aggregates information from its selected neighbors.&lt;/li&gt; 
     &lt;li&gt;Update the node embeddings using an attention mechanism, which can be enhanced by relational bias if edges are available.&lt;/li&gt; 
     &lt;li&gt;After K iterations of neighbor sampling, apply loss functions comprising a supervised classification loss and, if applicable, edge-level regularization loss to optimize the embedding representations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details affecting performance involve: 
    &lt;ul&gt; 
     &lt;li&gt;Careful tuning of the temperature parameter (œÑ) in the Gumbel-Softmax operator, as it significantly influences the proposed approach's capacity to capture the discrete nature of graph structures.&lt;/li&gt; 
     &lt;li&gt;Utilizing appropriate batch sizes for large-scale graphs, ensuring enough memory is available while also maintaining computational efficiency.&lt;/li&gt; 
     &lt;li&gt;Choosing the correct dimensionality for random features in the kernel approximation, balancing model expressiveness and training stability.&lt;/li&gt; 
     &lt;li&gt;The use of dropout or other regularization techniques such as edge-level regularization can influence the proposed model's generalization capabilities on unseen data.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;On the bottleneck of graph neural networks and its practical implications&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Categorical reparameterization with gumbel-softmax&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph attention networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph structure learning for robust graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geom-gcn: Geometric graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New benchmarks for learning on non-homophilous graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Latent patient network learning for automatic diagnosis&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Few-shot learning with graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The graph neural network model&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Characteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Beyond homophily in graph neural networks: Current limitations and effective designs&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_nodeformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Example 7 (Category: Graph Neural Networks)&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Prompt&lt;/b&gt;&lt;br /&gt;&lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt;
  &lt;ol&gt; 
   &lt;li&gt;The proposed approach works on the task of uncovering data dependencies and learning instance representations from datasets that may not have complete or reliable relationships, particularly in semi-supervised contexts like node classification, image/text classification, and spatial-temporal dynamics prediction.&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;!-- &lt;p&gt;I have some reference papers, please implement the following idea with these papers:&lt;/p&gt; --&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;The core techniques/algorithms used in this paper include an energy-constrained diffusion model represented as a partial differential equation (PDE), an explicit Euler scheme for numerical solutions, and a form of adaptive diffusivity function based on the energy function. The proposed architecture utilizes a diffusion-based Transformer framework that allows for all-pair feature propagation among instances.&lt;/li&gt; 
   &lt;li&gt;The major technical components serve the following purposes: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process:&lt;/strong&gt; Encodes instances into evolving states by modeling information flow, where instance representations evolve according to a PDE illuminating the relationships among the instances.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Provides constraints to regularize the diffusion process and guide the proposed model towards desired low-energy embeddings, enhancing the quality of representations.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusivity Function:&lt;/strong&gt; Specifies the strength of information flow between instances, adapting based on the instance states, and allows for flexible and efficient propagation strategies.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Implementation details for each component: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Input:&lt;/strong&gt; Requires a batch of instances represented as a matrix of size \(N \times D\), where \(N\) is the number of instances and \(D\) is the input feature dimension.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Diffusion Process Output:&lt;/strong&gt; Produces the updated instance representations after \(K\) propagation steps. The step size \(\tau\) should be set within the range (0, 1).&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Energy Function:&lt;/strong&gt; Implemented as \(E(Z, k; \delta) = ||Z - Z^{(k)}||^2_F + \lambda \sum_{i,j} \delta(||z_i - z_j||^2_2)\), with \(\delta\) being a non-decreasing, concave function.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Key Parameters:&lt;/strong&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Step size \(\tau\)&lt;/li&gt; 
       &lt;li&gt;Layer number \(K\) (number of diffusion propagation steps)&lt;/li&gt; 
       &lt;li&gt;Regularization weight \(\lambda\).&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Step-by-step description of interactions: 
    &lt;ul&gt; 
     &lt;li&gt;Start by initializing the instance representations.&lt;/li&gt; 
     &lt;li&gt;For each layer of diffusion, compute the diffusivity \(S(k)\) based on current embeddings through a function \(f\) which can be defined differently depending on the proposed model implementation.&lt;/li&gt; 
     &lt;li&gt;Update the instance representations using the defined diffusion equations, ensuring to conserve states and introduce propagation according to the computed diffusivity.&lt;/li&gt; 
     &lt;li&gt;After \(K\) layers of diffusion, apply a final output layer to produce logits for predictions.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Critical implementation details that affect performance: 
    &lt;ul&gt; 
     &lt;li&gt;The choice of diffusivity function \(f\) greatly impacts the proposed model's capacity to learn complex dependencies, where specific formulations (like linear or logistic) yield different abilities in capturing inter-instance relationships.&lt;/li&gt; 
     &lt;li&gt;Ensure that the values of \(\tau\) and \(\lambda\) are set appropriately to balance convergence speed and representation quality; using a smaller \(\tau\) may require deeper layers to learn effectively.&lt;/li&gt; 
     &lt;li&gt;Optimization parameters like learning rate and early stopping criteria are essential, particularly for large-scale datasets where convergence behavior can vary widely depending on architecture size and complexity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Input:Reference Papers&lt;/b&gt;
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt;...
  &lt;/ol&gt;&lt;/summary&gt; 
 &lt;div&gt; 
  &lt;ol start="2"&gt; 
   &lt;!-- &lt;li&gt;&lt;strong&gt;Diffusion-convolutional neural networks&lt;/strong&gt;&lt;/li&gt; --&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised classification with graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Manifold regularization: A geometric framework for learning from labeled and unlabeled examples&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Geometric deep learning: going beyond euclidean data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Artificial neural networks for solving ordinary and partial differential equations&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Scaling graph neural networks with approximate pagerank&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Learning discrete structures for graph neural networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Semi-supervised learning using gaussian fields and harmonic functions&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph convolutional networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Deep learning via semi-supervised embedding&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;A generalization of transformer networks to graphs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Graph Convolution and Quadratic Time Complexity&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bayesian graph convolutional neural networks for semi-supervised classification&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Do transformers really perform bad for graph representation?&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Big bird: Transformers for longer sequences&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Adaptive graph diffusion networks&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Transformers are RNNs&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Collective classification in network data&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;NodeFormer: A scalable graph structure learning transformer for node classification&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.pdf" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/paper.gif" alt="PDF Document" width="100%" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Paper (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/project" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/examples/gnn_difformer/scrolling_code.gif" alt="profiles" width="100%" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Self-Organized Workplace, &lt;b&gt;take time to load&lt;/b&gt; (fully-generated by AI-Researcher, click to view).&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="how-it-works"&gt;&lt;/span&gt; 
&lt;h2&gt;‚ú®How AI-Researcher works&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;End-to-End Scientific Research Automation System&lt;/strong&gt; &lt;br /&gt;Our &lt;strong&gt;AI-Researcher&lt;/strong&gt; provides comprehensive automation for the complete scientific research lifecycle through an integrated pipeline. The system orchestrates research activities across three strategic phases: 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Literature Review &amp;amp; Idea Generation&lt;/strong&gt; üìöüí°&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;üîç &lt;strong&gt;Resource Collector&lt;/strong&gt;: Systematically gathers comprehensive research materials across multiple scientific domains through automated collection from major academic databases (e.g., arXiv, IEEE Xplore, ACM Digital Library, and Google Scholar), code platforms (e.g., GitHub, Hugging Face), and open datasets across scientific domains.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Resource Filter&lt;/strong&gt;: Evaluates and selects high-impact papers, well-maintained code implementations, and benchmark datasets through quality metrics (e.g., citation count, code maintenance, data completeness) and relevance assessment.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;üí≠ &lt;strong&gt;Idea Generator&lt;/strong&gt;: Leveraging the identified research resources, including high-impact papers and code repositories, the Idea Generator systematically formulates novel research directions through comprehensive analysis. It automatedly evaluates current methodological limitations, map emerging technological trends, and explore uncharted research territories.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;New Algorithm Design, Implementation &amp;amp; Validation&lt;/strong&gt; üß™üíª &lt;br /&gt;&lt;strong&gt;Design ‚Üí Implementation ‚Üí Validation ‚Üí Refinement&lt;/strong&gt;&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;üìù&lt;strong&gt;Design Phase&lt;/strong&gt;: The initial phase focuses on conceptual development, where novel algorithmic ideas are formulated and theoretical foundations are established. During this stage, we carefully plan the implementation strategy, ensuring the proposed solution advances beyond existing approaches while maintaining practical feasibility.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;‚öôÔ∏è&lt;strong&gt;Implementation Phase&lt;/strong&gt;: proceed to transform abstract concepts into concrete code implementations. This phase involves developing functional modules, establishing a robust testing environment, and creating necessary infrastructure for experimental validation.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;üî¨&lt;strong&gt;Validation Phase&lt;/strong&gt;: Systematic experimentation forms the core of our validation process. We execute comprehensive tests to evaluate algorithm performance, collect metrics, and document all findings. This phase ensures rigorous implementation verification with practical requirements.&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;üîß&lt;strong&gt;Refinement Phase&lt;/strong&gt; üî¨: Based on validation results, we enter an iterative refinement cycle. This phase involves identifying areas for improvement, optimizing code efficiency, and implementing necessary enhancements. We carefully analyze performance bottlenecks and plan strategic improvements for the next development iteration.&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Paper Writing&lt;/strong&gt; ‚úçÔ∏èüìù&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Writer Agent&lt;/strong&gt; üìÑ: Automatically generates full-length academic papers by integrating research ideas, motivations, newly designed algorithm frameworks, and algorithm validation performance. Leveraging a hierarchical writing approach, it creates polished manuscripts with precision and clarity.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üöÄ This fully automated system removes the need for manual intervention across the entire research lifecycle, enabling effortless and seamless scientific discovery‚Äîfrom initial concept to final publication. üöÄ It serves as an excellent research assistant, aiding researchers in achieving their goals efficiently and effectively.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üî¨ &lt;strong&gt;Comprehensive Benchmark Suite&lt;/strong&gt; &lt;br /&gt;We have developed a comprehensive and standardized evaluation framework to objectively assess the academic capabilities of AI researchers and the quality of their scholarly work, integrating several key innovations to ensure thorough and reliable evaluation.&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;üë®‚Äçüî¨ &lt;strong&gt;Expert-Level Ground Truth&lt;/strong&gt;: TThe benchmark leverages human expert-written papers as ground truth references, establishing a high-quality standard for comparison and validation.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üåà &lt;strong&gt;Multi-Domain Coverage&lt;/strong&gt;: Our benchmark is designed to comprehensively span 4 major research domains, ensuring broad applicability: Computer Vision (CV), Nature Language Processing (NLP), Data Mining (DM), and Information Retrieval (IR).&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üåê &lt;strong&gt;Fully Open-Source Benchmark Construction&lt;/strong&gt;: We have fully open-sourced the methodology and process for building the benchmark, including complete access to processed datasets, data collection pipelines, and processing code. This ensures &lt;strong&gt;Transparency in Evaluation&lt;/strong&gt; while empowering the community to customize and construct benchmarks tailored to their specific domains for testing AI researchers.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üìä &lt;strong&gt;Comprehensive Evaluation Metrics&lt;/strong&gt;: Our evaluation framework adopts a hierarchical and systematic approach, where tasks are organized into two levels based on the extent of idea provision. Leveraging specialized &lt;strong&gt;Evaluator Agents&lt;/strong&gt;, the framework conducts thorough assessments across multiple dimensions, ensuring a robust and comprehensive evaluation. Key evaluation metrics include: 1) &lt;strong&gt;Novelty&lt;/strong&gt;: Assessing the innovation and uniqueness of the research work. 2) &lt;strong&gt;Experimental Comprehensiveness&lt;/strong&gt;: Evaluating the design, execution, and rigor of the experiments. 3) &lt;strong&gt;Theoretical Foundation&lt;/strong&gt;: Measuring the strength of the theoretical background and foundations. 4) &lt;strong&gt;Result Analysis&lt;/strong&gt;: Analyzing the depth and accuracy of result interpretation. 5) &lt;strong&gt;Writing Quality&lt;/strong&gt;: Reviewing the clarity, coherence, and structure of the written report.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üöÄ &lt;strong&gt;Advancing Research Automation&lt;/strong&gt;. This benchmark suite provides an objective framework for assessing research automation capabilities. It is designed to evolve continuously, incorporating new advancements and expanding its scope to meet the growing demands of the research community.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üåü &lt;strong&gt;Easy-to-Use AI Research Assistant&lt;/strong&gt; &lt;br /&gt;&lt;strong&gt;AI-Researcher&lt;/strong&gt;E delivers a truly seamless and accessible experience for research automation, empowering users to focus on innovation without technical barriers. Key features include:&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt; &lt;p&gt;üåê &lt;strong&gt;Multi-LLM Provider Support&lt;/strong&gt;: Effortlessly integrates with leading language model providers such as Claude, OpenAI, Deepseek, and more. Researchers can select the most suitable AI capabilities for their specific needs.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üìö &lt;strong&gt;Effortless Research Kickoff&lt;/strong&gt;: Kickstart your research journey with unparalleled ease! Simply provide a list of relevant papers, and &lt;strong&gt;AI-Researcher&lt;/strong&gt; takes care of the rest‚Äîno need to upload files, contribute initial ideas, or navigate complex configurations. It's the ultimate tool to help you jumpstart your research process efficiently and effectively.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Minimal Domain Expertise Needed&lt;/strong&gt;: AI-Researcher simplifies the research process by autonomously identifying critical research gaps, proposing innovative approaches, and executing the entire research pipeline. While some domain understanding can enhance results, the tool is designed to empower users of all expertise levels to achieve impactful outcomes with ease.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;üì¶ &lt;strong&gt;Out-of-the-Box Functionality&lt;/strong&gt;: Experience seamless research automation right from the start. AI-Researcher is ready to use with minimal setup, giving you instant access to advanced capabilities. Skip the hassle of complex configurations and dive straight into accelerating your research process with ease and efficiency.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="how-to-use"&gt;&lt;/span&gt; 
&lt;h2&gt;üîç How to use AI-Researcher&lt;/h2&gt; 
&lt;h3&gt;1. Research Agent&lt;/h3&gt; 
&lt;p&gt;If you want to use research agent with the given idea (Level 1 tasks), conducting extensive survey and experiments, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_1.sh"&gt;&lt;code&gt;research_agent/run_infer_level_1.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_plan.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --task_level task1 --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to just give the reference papers, and let the research agent to generate the idea then conduct the experiments (Level 2 tasks), you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/research_agent/run_infer_level_2.sh"&gt;&lt;code&gt;research_agent/run_infer_level_2.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_idea.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Paper Writing Agent&lt;/h3&gt; 
&lt;p&gt;If you want to generate the paper after the research agent has conducted the research, you can use the following command in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/paper_agent/run_paper.sh"&gt;&lt;code&gt;paper_agent/run_infer.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#!/bin/bash

cd path/to/AI-Researcher/paper_agent

export OPENAI_API_KEY=sk-SKlupNntta4WPmvDCRo7uuPbYGwOnUQcb25Twn8c718tPpXN


research_field=vq
instance_id=rotated_vq

python path/to/AI-Researcher/paper_agent/writing.py --research_field ${research_field} --instance_id ${instance_id}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Benchmark Data and Collection&lt;/h3&gt; 
&lt;p&gt;Our benchmark is also fully-open-sourced:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detailed benchmark data is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark"&gt;&lt;code&gt;benchmark&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
 &lt;li&gt;Detailed benchmark collection process is available in the &lt;a href="https://raw.githubusercontent.com/HKUDS/AI-Researcher/main/benchmark_collection"&gt;&lt;code&gt;benchmark_collection&lt;/code&gt;&lt;/a&gt; folder.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="documentation"&gt;&lt;/span&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation is on its way üöÄ! Stay tuned for updates on our &lt;a href="https://auto-researcher.github.io/docs"&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; 
&lt;span id="community"&gt;&lt;/span&gt; 
&lt;h2&gt;ü§ù Join the Community&lt;/h2&gt; 
&lt;p&gt;We aim to build a vibrant community around AI-Researcher and warmly invite everyone to join us. Here's how you can become part of our community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/ghSnKGkq"&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/issues"&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we're working on, or add your own ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/stargazers"&gt;&lt;img src="https://reporoster.com/stars/HKUDS/AI-Researcher" alt="Stargazers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AI-Researcher/network/members"&gt;&lt;img src="https://reporoster.com/forks/HKUDS/AI-Researcher" alt="Forkers repo roster for @HKUDS/AI-Researcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#HKUDS/AI-Researcher&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=HKUDS/AI-Researcher&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;span id="cite"&gt;&lt;/span&gt; 
&lt;h2&gt;üåü Cite&lt;/h2&gt; 
&lt;p&gt;A more detailed technical report will be released soon. üöÄ:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;@misc{airesearcher,
      title={{AI-Researcher: Autonomous Scientific Innovation}},
      author={Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang},
      year={2025},
      eprint={2505.18705},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.18705},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>sentient-agi/ROMA</title>
      <link>https://github.com/sentient-agi/ROMA</link>
      <description>&lt;p&gt;Recursive-Open-Meta-Agent v0.1 (Beta). A meta-agent framework to build high-performance multi-agent systems.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/sentient-logo-new-M.png" alt="alt text" width="60%" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;ROMA: Recursive Open Meta-Agents&lt;/h1&gt; 
 &lt;p align="center"&gt; &lt;strong&gt;Building hierarchical high-performance multi-agent systems made easy! (Beta) &lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14848" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14848" alt="sentient-agi%2FROMA | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://sentient.xyz/" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Homepage" src="https://img.shields.io/badge/Sentient-Homepage-%23EAEAEA?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%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%2BPC9zdmc%2B&amp;amp;link=https%3A%2F%2Fhuggingface.co%2FSentientagi" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://github.com/sentient-agi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="GitHub" src="https://img.shields.io/badge/Github-sentient_agi-181717?logo=github" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://huggingface.co/Sentientagi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SentientAGI-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;/div&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://discord.gg/sentientfoundation" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord-SentientAGI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;a href="https://x.com/SentientAGI" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/badge/-SentientAGI-grey?logo=x&amp;amp;link=https%3A%2F%2Fx.com%2FSentientAGI%2F" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.sentient.xyz/blog/recursive-open-meta-agent"&gt;Technical Blog&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/"&gt;Paper (Coming soon)&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.sentient.xyz/"&gt;Build Agents for $$$&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt;  
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/INTRODUCTION.md"&gt;üöÄ Introduction&lt;/a&gt;&lt;/strong&gt; - Understand the vision and architecture behind ROMA&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;üì¶ Setup&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;ü§ñ Agents Guide&lt;/a&gt;&lt;/strong&gt; - Learn how to create and customize your own agents&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/CONFIGURATION.md"&gt;‚öôÔ∏è Configuration&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/ROADMAP.md"&gt;üó∫Ô∏è Roadmap&lt;/a&gt;&lt;/strong&gt; - See what's coming next for ROMA&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéØ What is ROMA?&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/roma_run.gif" alt="alt text" width="80%" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; is a &lt;strong&gt;meta-agent framework&lt;/strong&gt; that uses recursive hierarchical structures to solve complex problems. By breaking down tasks into parallelizable components, ROMA enables agents to tackle sophisticated reasoning challenges while maintaining transparency that makes context-engineering and iteration straightforward. The framework offers &lt;strong&gt;parallel problem solving&lt;/strong&gt; where agents work simultaneously on different parts of complex tasks, &lt;strong&gt;transparent development&lt;/strong&gt; with a clear structure for easy debugging, and &lt;strong&gt;proven performance&lt;/strong&gt; demonstrated through our search agent's strong benchmark results. We've shown the framework's effectiveness, but this is just the beginning. As an &lt;strong&gt;open-source and extensible&lt;/strong&gt; platform, ROMA is designed for community-driven development, allowing you to build and customize agents for your specific needs while benefiting from the collective improvements of the community.&lt;/p&gt; 
&lt;h2&gt;üèóÔ∏è How It Works&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; framework processes tasks through a recursive &lt;strong&gt;plan‚Äìexecute loop&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def solve(task):
    if is_atomic(task):                 # Step 1: Atomizer
        return execute(task)            # Step 2: Executor
    else:
        subtasks = plan(task)           # Step 2: Planner
        results = []
        for subtask in subtasks:
            results.append(solve(subtask))  # Recursive call
        return aggregate(results)       # Step 3: Aggregator

# Entry point:
answer = solve(initial_request)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Atomizer&lt;/strong&gt; ‚Äì Decides whether a request is &lt;strong&gt;atomic&lt;/strong&gt; (directly executable) or requires &lt;strong&gt;planning&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Planner&lt;/strong&gt; ‚Äì If planning is needed, the task is broken into smaller &lt;strong&gt;subtasks&lt;/strong&gt;. Each subtask is fed back into the &lt;strong&gt;Atomizer&lt;/strong&gt;, making the process recursive.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Executors&lt;/strong&gt; ‚Äì Handle atomic tasks. Executors can be &lt;strong&gt;LLMs, APIs, or even other agents&lt;/strong&gt; ‚Äî as long as they implement an &lt;code&gt;agent.execute()&lt;/code&gt; interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aggregator&lt;/strong&gt; ‚Äì Collects and integrates results from subtasks. Importantly, the Aggregator produces the &lt;strong&gt;answer to the original parent task&lt;/strong&gt;, not just raw child outputs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;üìê Information Flow&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Top-down:&lt;/strong&gt; Tasks are decomposed into subtasks recursively.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bottom-up:&lt;/strong&gt; Subtask results are aggregated upwards into solutions for parent tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Left-to-right:&lt;/strong&gt; If a subtask depends on the output of a previous one, it waits until that subtask completes before execution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This structure makes the system flexible, recursive, and dependency-aware ‚Äî capable of decomposing complex problems into smaller steps while ensuring results are integrated coherently.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view the system flow diagram&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TB
    A[Your Request] --&amp;gt; B{Atomizer}
    B --&amp;gt;|Plan Needed| C[Planner]
    B --&amp;gt;|Atomic Task| D[Executor]

    %% Planner spawns subtasks
    C --&amp;gt; E[Subtasks]
    E --&amp;gt; G[Aggregator]

    %% Recursion
    E -.-&amp;gt; B  

    %% Execution + Aggregation
    D --&amp;gt; F[Final Result]
    G --&amp;gt; F

    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#d1c4e9
    style G fill:#c5cae9

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt;
&lt;br /&gt; 
&lt;h3&gt;üöÄ 30-Second Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sentient-agi/ROMA.git
cd ROMA

# Run the automated setup
./setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Choose between:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Setup&lt;/strong&gt; (Recommended) - One-command setup with isolation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native Setup&lt;/strong&gt; - Direct installation for development&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üõ†Ô∏è Technical Stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: Built on &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/your/agnoagents%5D(https://github.com/agno-agi/agno)"&gt;AgnoAgents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Python 3.12+ with FastAPI/Flask&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + TypeScript with real-time WebSocket&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Support&lt;/strong&gt;: Any provider via LiteLLM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Persistence&lt;/strong&gt;: Enterprise S3 mounting with security validation 
  &lt;ul&gt; 
   &lt;li&gt;üîí &lt;strong&gt;goofys FUSE mounting&lt;/strong&gt; for zero-latency file access&lt;/li&gt; 
   &lt;li&gt;üõ°Ô∏è &lt;strong&gt;Path injection protection&lt;/strong&gt; with comprehensive validation&lt;/li&gt; 
   &lt;li&gt;üîê &lt;strong&gt;AWS credentials verification&lt;/strong&gt; before operations&lt;/li&gt; 
   &lt;li&gt;üìÅ &lt;strong&gt;Dynamic Docker Compose&lt;/strong&gt; with secure volume mounting&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Execution&lt;/strong&gt;: E2B sandboxes with unified S3 integration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Production-grade validation and error handling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt;: Multi-modal, tools, MCP, hooks, caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì¶ Installation Options&lt;/h2&gt; 
&lt;h3&gt;Quick Start (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Main setup (choose Docker or Native)
./setup.sh

# Optional: Setup E2B sandbox integration
./setup.sh --e2b

# Test E2B integration  
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Command Line Options&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./setup.sh --docker     # Run Docker setup directly
./setup.sh --docker-from-scratch  # Rebuild Docker images/containers from scratch (down -v, no cache)
./setup.sh --native     # Run native setup directly (macOS/Ubuntu/Debian)
./setup.sh --e2b        # Setup E2B template (requires E2B_API_KEY + AWS creds)
./setup.sh --test-e2b   # Test E2B template integration
./setup.sh --help       # Show all available options
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual Installation&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;setup docs&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h3&gt;üèóÔ∏è Optional: E2B Sandbox Integration&lt;/h3&gt; 
&lt;p&gt;For secure code execution capabilities, optionally set up E2B sandboxes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# After main setup, configure E2B (requires E2B_API_KEY and AWS credentials in .env)
./setup.sh --e2b

# Test E2B integration
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;E2B Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Secure Code Execution&lt;/strong&gt; - Run untrusted code in isolated sandboxes&lt;/li&gt; 
 &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;S3 Integration&lt;/strong&gt; - Automatic data sync between local and sandbox environments&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;goofys Mounting&lt;/strong&gt; - High-performance S3 filesystem mounting&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;AWS Credentials&lt;/strong&gt; - Passed securely via Docker build arguments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ñ Pre-built Agents&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; These agents are demonstrations built using ROMA's framework through simple vibe-prompting and minimal manual tuning. They showcase how easily you can create high-performance agents with ROMA, rather than being production-final solutions. Our mission is to empower the community to build, share, and get rewarded for creating innovative agent recipes and use-cases.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ROMA comes with example agents that demonstrate the framework's capabilities:&lt;/p&gt; 
&lt;h3&gt;üîç General Task Solver&lt;/h3&gt; 
&lt;p&gt;A versatile agent powered by ChatGPT Search Preview for handling diverse tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Search&lt;/strong&gt;: Leverages OpenAI's latest search capabilities for real-time information&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Planning&lt;/strong&gt;: Adapts task decomposition based on query complexity&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Domain&lt;/strong&gt;: Handles everything from technical questions to creative projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quick Prototyping&lt;/strong&gt;: Perfect for testing ROMA's capabilities without domain-specific setup&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: General research, fact-checking, exploratory analysis, quick information gathering&lt;/p&gt; 
&lt;h3&gt;üî¨ Deep Research Agent&lt;/h3&gt; 
&lt;p&gt;A comprehensive research system that breaks down complex research questions into manageable sub-tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Task Decomposition&lt;/strong&gt;: Automatically splits research topics into search, analysis, and synthesis phases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Information Gathering&lt;/strong&gt;: Executes multiple searches simultaneously for faster results&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Source Integration&lt;/strong&gt;: Combines results from web search, Wikipedia, and specialized APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Synthesis&lt;/strong&gt;: Aggregates findings into coherent, well-structured reports&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Academic research, market analysis, competitive intelligence, technical documentation&lt;/p&gt; 
&lt;h3&gt;üíπ Crypto Analytics Agent&lt;/h3&gt; 
&lt;p&gt;Specialized financial analysis agent with deep blockchain and DeFi expertise:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Market Data&lt;/strong&gt;: Integrates with Binance, CoinGecko, and DefiLlama APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-Chain Analytics&lt;/strong&gt;: Access to Arkham Intelligence for wallet tracking and token flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technical Analysis&lt;/strong&gt;: Advanced charting with OHLC data and market indicators&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeFi Metrics&lt;/strong&gt;: TVL tracking, yield analysis, protocol comparisons&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Execution&lt;/strong&gt;: Runs analysis in E2B sandboxes with data persistence&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Token research, portfolio analysis, DeFi protocol evaluation, market trend analysis&lt;/p&gt; 
&lt;p&gt;All three agents demonstrate ROMA's recursive architecture in action, showing how complex queries that would overwhelm single-pass systems can be elegantly decomposed and solved. They serve as templates and inspiration for building your own specialized agents.&lt;/p&gt; 
&lt;h3&gt;Your First Agent in 5 Minutes&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;./setup.sh  # Automated setup with Docker or native installation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access all the pre-defined agents through the frontend on &lt;code&gt;localhost:3000&lt;/code&gt; after setting up the backend on &lt;code&gt;localhost:5000&lt;/code&gt;. Please checkout &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;Setup&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;Agents guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/agent_customization.png" alt="alt text" width="60%" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Your first agent in 3 lines
from sentientresearchagent import SentientAgent

agent = SentientAgent.create()
result = await agent.run("Create a podcast about AI safety")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìä Benchmarks&lt;/h2&gt; 
&lt;p&gt;We evaluate our simple implementation of a search system using ROMA, called ROMA-Search across three benchmarks: &lt;strong&gt;SEAL-0&lt;/strong&gt;, &lt;strong&gt;FRAMES&lt;/strong&gt;, and &lt;strong&gt;SimpleQA&lt;/strong&gt;.&lt;br /&gt; Below are the performance graphs for each benchmark.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/vtllms/sealqa"&gt;SEAL-0&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;SealQA is a new challenging benchmark for evaluating Search-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/seal-0-full.001.jpeg" alt="SEAL-0 Results" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/google/frames-benchmark"&gt;FRAMES&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;A comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/FRAMES-full.001.jpeg" alt="FRAMES Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://openai.com/index/introducing-simpleqa/"&gt;SimpleQA&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;Factuality benchmark that measures the ability for language models to answer short, fact-seeking questions.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/simpleQAFull.001.jpeg" alt="SimpleQA Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîÑ &lt;strong&gt;Recursive Task Decomposition&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Automatically breaks down complex tasks into manageable subtasks with intelligent dependency management. Runs independent sub-tasks in &lt;strong&gt;parallel&lt;/strong&gt;.&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ü§ñ &lt;strong&gt;Agent Agnostic&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Works with any provider (OpenAI, Anthropic, Google, local models) through unified interface, as long as it has an &lt;code&gt;agent.run()&lt;/code&gt; command, then you can use it!&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîç &lt;strong&gt;Complete Transparency&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Stage tracing shows exactly what happens at each step - debug and optimize with full visibility&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîå Connect Any Tool&lt;/h3&gt; &lt;p&gt;Seamlessly integrate external tools and protocols with configurable intervention points. Already includes production-grade connectors such as E2B, file-read-write, and more.&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;This framework would not have been possible if it wasn't for these amazing open-source contributions!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inspired by the hierarchical planning approach described in &lt;a href="https://arxiv.org/abs/2503.08275"&gt;"Beyond Outlining: Heterogeneous Recursive Planning"&lt;/a&gt; by Xiong et al.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt; - Data validation using Python type annotations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/agno-ai/agno%5D(https://github.com/agno-agi/agno)"&gt;Agno&lt;/a&gt; - Framework for building AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/e2b-dev/e2b"&gt;E2B&lt;/a&gt; - Cloud runtime for AI agents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìö Citation&lt;/h2&gt; 
&lt;p&gt;If you use the ROMA repo in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{al_zubi_2025_17052592,
  author       = {Al-Zubi, Salah and
                  Nama, Baran and
                  Kaz, Arda and
                  Oh, Sewoong},
  title        = {SentientResearchAgent: A Hierarchical AI Agent
                   Framework for Research and Analysis
                  },
  month        = sep,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {ROMA},
  doi          = {10.5281/zenodo.17052592},
  url          = {https://doi.org/10.5281/zenodo.17052592},
  swhid        = {swh:1:dir:69cd1552103e0333dd0c39fc4f53cb03196017ce
                   ;origin=https://doi.org/10.5281/zenodo.17052591;vi
                   sit=swh:1:snp:f50bf99634f9876adb80c027361aec9dff97
                   3433;anchor=swh:1:rel:afa7caa843ce1279f5b4b29b5d3d
                   5e3fe85edc95;path=salzubi401-ROMA-b31c382
                  },
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üåü Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#sentient-agi/roma&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=sentient-agi/roma&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lightricks/LTX-Video</title>
      <link>https://github.com/Lightricks/LTX-Video</link>
      <description>&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;LTX-Video&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://www.lightricks.com/ltxv"&gt;&lt;img src="https://img.shields.io/badge/Website-LTXV-181717?logo=google-chrome" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface" alt="Model" /&gt;&lt;/a&gt; &lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;&lt;img src="https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel" alt="Demo" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.00103"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;&lt;img src="https://img.shields.io/badge/LTXV-Trainer-9146FF?logo=github" alt="Trainer" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Mn8BRgUKKy"&gt;&lt;img src="https://img.shields.io/badge/Join-Discord-5865F2?logo=discord" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news"&gt;What's new&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#models"&gt;Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide"&gt;Quick Start Guide&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-inference"&gt;Online demo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally"&gt;Run locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference"&gt;Inference&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration"&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide"&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution"&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#training"&gt;Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#control-models"&gt;Control Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#join-us-"&gt;Join Us!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216√ó704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; 
&lt;p&gt;The model supports image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; 
&lt;h3&gt;Image-to-video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00001.gif" alt="example1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00002.gif" alt="example2" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00003.gif" alt="example3" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00004.gif" alt="example4" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00005.gif" alt="example5" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00006.gif" alt="example6" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00007.gif" alt="example7" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00008.gif" alt="example8" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00009.gif" alt="example9" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Controlled video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00000.gif" alt="control0" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00001.gif" alt="control1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00002.gif" alt="control2" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00003.gif" alt="control3" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00004.gif" alt="control4" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;h2&gt;July, 16th, 2025: New Distilled models v0.9.8 with up to 60 seconds of video:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Long shot generation in LTXV-13B! 
  &lt;ul&gt; 
   &lt;li&gt;LTX-Video now supports up to 60 seconds of video.&lt;/li&gt; 
   &lt;li&gt;Compatible also with the official IC-LoRAs.&lt;/li&gt; 
   &lt;li&gt;Try now in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-i2v-long-multi-prompt.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new distilled models: 
  &lt;ul&gt; 
   &lt;li&gt;13B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Both models are distilled from the same base model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev&lt;/a&gt; and are compatible for use together in the same multiscale pipeline.&lt;/li&gt; 
   &lt;li&gt;Improved prompt understanding and detail generation&lt;/li&gt; 
   &lt;li&gt;Includes corresponding FP8 weights and workflows.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new detailer model &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8"&gt;LTX-Video-ICLoRA-detailer-13B-0.9.8&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Available in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-upscale.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;July, 8th, 2025: New Control Models Released!&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Released three new control models for LTX-Video on HuggingFace: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Depth Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-depth-13b-0.9.7"&gt;LTX-Video-ICLoRA-depth-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Pose Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7"&gt;LTX-Video-ICLoRA-pose-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Canny Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-canny-13b-0.9.7"&gt;LTX-Video-ICLoRA-canny-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 14th, 2025: New distilled model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors"&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
   &lt;li&gt;Also released a LoRA version of the distilled model, &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors"&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Requires only 1GB of VRAM&lt;/li&gt; 
     &lt;li&gt;Can be used with the full 13B model for fast inference&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new quantized distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors"&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/a&gt; for &lt;em&gt;real-time&lt;/em&gt; generation (on H100) with even less VRAM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors"&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Release a new quantized model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors"&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRam&lt;/li&gt; 
 &lt;li&gt;Release a new upscalers 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors"&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors"&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; 
 &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors"&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; 
 &lt;li&gt;Release a new distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors"&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; 
 &lt;li&gt;New default resolution and FPS: 1216 √ó 704 pixels at 30 FPS 
  &lt;ul&gt; 
   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; 
   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;New license for commercial use (&lt;a href="https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt"&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support keyframes and video extension&lt;/li&gt; 
 &lt;li&gt;Support higher resolutions&lt;/li&gt; 
 &lt;li&gt;Improved prompt understanding&lt;/li&gt; 
 &lt;li&gt;Improved VAE&lt;/li&gt; 
 &lt;li&gt;New online web app in &lt;a href="https://app.ltx.studio/ltx-video"&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; 
 &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; 
 &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Add &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release the &lt;a href="https://arxiv.org/abs/2501.00103"&gt;research paper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support for STG / PAG&lt;/li&gt; 
 &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; 
 &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; 
 &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; 
 &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; 
 &lt;li&gt;Relax transformers dependency&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Models &amp;amp; Workflows&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
   &lt;th&gt;inference.py config&lt;/th&gt; 
   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev&lt;/td&gt; 
   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json"&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;ltxv-13b-0.9.8-mix&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json"&gt;ltxv-13b-i2v-mixed-multiscale.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json"&gt;ltxv-13b-dist-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled&lt;/td&gt; 
   &lt;td&gt;Smaller model, slight quality reduction compared to 13b distilled. Ideal for fast generation with light VRAM usage&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml"&gt;ltxv-13b-0.9.8-dev-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json"&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml"&gt;ltxv-13b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json"&gt;ltxv-13b-dist-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-2b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml"&gt;ltxv-2b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6&lt;/td&gt; 
   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml"&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json"&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6-distilled&lt;/td&gt; 
   &lt;td&gt;15√ó faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml"&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json"&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Quick Start Guide&lt;/h1&gt; 
&lt;h2&gt;Online inference&lt;/h2&gt; 
&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;LTX-Studio image-to-video (13B-mix)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;LTX-Studio image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video"&gt;Fal.ai image-to-video (13B full)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video"&gt;Fal.ai image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/lightricks/ltx-video"&gt;Replicate image-to-video&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run locally&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macOS, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference\]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FP8 Kernels (optional)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/LTXVideo-Q8-Kernels"&gt;FP8 kernels&lt;/a&gt; developed for LTX-Video provide performance boost on supported graphics cards (Ada architecture and later). To install FP8 kernels, follow the instructions in that repository.&lt;/p&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI&lt;/a&gt; workflow. We're working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; 
&lt;p&gt;To use our model, please follow the inference code in &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py"&gt;inference.py&lt;/a&gt;:&lt;/p&gt; 
&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extending a video:&lt;/h4&gt; 
&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; 
&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using as a library&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config="configs/ltxv-13b-0.9.8-distilled.yaml",
        prompt=PROMPT,
        height=HEIGHT,
        width=WIDTH,
        num_frames=NUM_FRAMES,
        output_path="output.mp4",
    )
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/"&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Diffusers Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8"&gt;see details below&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Model User Guide&lt;/h1&gt; 
&lt;h2&gt;üìù Prompt Engineering&lt;/h2&gt; 
&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; 
 &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; 
 &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; 
 &lt;li&gt;Include background and environment details&lt;/li&gt; 
 &lt;li&gt;Specify camera angles and movements&lt;/li&gt; 
 &lt;li&gt;Describe lighting and colors&lt;/li&gt; 
 &lt;li&gt;Note any changes or sudden events&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; 
&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;üéÆ Parameter Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; 
 &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; 
 &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; 
 &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìù For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Community Contribution&lt;/h2&gt; 
&lt;h3&gt;ComfyUI-LTXTricks üõ†Ô∏è&lt;/h3&gt; 
&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üîÑ &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href="https://rf-inversion.github.io/"&gt;RF-Inversion&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;‚úÇÔ∏è &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href="https://github.com/wangjiangshan0725/RF-Solver-Edit"&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üåä &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href="https://github.com/fallenshock/FlowEdit"&gt;FlowEdit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üé• &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;‚ú® &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href="https://junhahyung.github.io/STGuidance/"&gt;STGuidance&lt;/a&gt;. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;üñºÔ∏è &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LTX-VideoQ8 üé± &lt;a id="ltx-videoq8"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/KONAKONA666/LTX-Video"&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üöÄ Up to 3X speed-up with no accuracy loss&lt;/li&gt; 
   &lt;li&gt;üé• Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; 
   &lt;li&gt;üõ†Ô∏è Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/"&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href="https://github.com/sayakpaul/q8-ltx-video"&gt;Details here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;TeaCache for LTX-Video üçµ &lt;a id="TeaCache"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video"&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üöÄ Speeds up LTX-Video inference.&lt;/li&gt; 
   &lt;li&gt;üìä Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; 
   &lt;li&gt;üõ†Ô∏è No retraining required: Works directly with existing models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Your Contribution&lt;/h3&gt; 
&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; 
&lt;h1&gt;‚ö°Ô∏è Training&lt;/h1&gt; 
&lt;p&gt;We provide an open-source repository for fine-tuning the LTX-Video model: &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;LTX-Video-Trainer&lt;/a&gt;. This repository supports both the 2B and 13B model variants, enabling full fine-tuning as well as LoRA (Low-Rank Adaptation) fine-tuning for more efficient training. This includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Control LoRAs&lt;/strong&gt;: Train custom control models like depth, pose, and canny control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Effect LoRAs&lt;/strong&gt;: Create specialized effects and transformations for video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Explore the repository to customize the model for your specific use cases! More information and training instructions can be found in the &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer/raw/main/README.md"&gt;README&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;üé¨ Control Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo"&gt;ComfyUI-LTXVideo&lt;/a&gt; repository now contains workflows and models for 3 specialized models that enable precise control over LTX-Video generation:&lt;/p&gt; 
&lt;p&gt;Pose Control, Depth Control and Canny Control&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example ComfyUI Workflow (for all control types):&lt;/strong&gt; &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ic_lora/ic-lora.json"&gt;ic-lora.json&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üöÄ Join Us&lt;/h1&gt; 
&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; 
&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we're revolutionizing how visual content is created.&lt;/p&gt; 
&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; 
&lt;p&gt;Please visit our &lt;a href="https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D"&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h1&gt;Acknowledgement&lt;/h1&gt; 
&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/DiT"&gt;DiT&lt;/a&gt; and &lt;a href="https://github.com/PixArt-alpha/PixArt-alpha"&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;üìÑ Our tech report is out! If you find our work helpful, please ‚≠êÔ∏è star the repository and cite our paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,
  title={LTX-Video: Realtime Video Latent Diffusion},
  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  journal={arXiv preprint arXiv:2501.00103},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>