<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Fri, 30 Jan 2026 01:52:47 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>Free-TV/IPTV</title>
      <link>https://github.com/Free-TV/IPTV</link>
      <description>&lt;p&gt;M3U Playlist for free TV channels&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Free TV&lt;/h1&gt; 
&lt;p&gt;This is an M3U playlist for free TV channels around the World.&lt;/p&gt; 
&lt;p&gt;Either free locally (over the air):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/usa.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/us.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/canada.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ca.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/uk.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/gb.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/ireland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ie.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/australia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/au.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/india.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/in.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/japan.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/jp.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/china.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cn.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/hong_kong.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/hk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/macau.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mo.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/taiwan.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/tw.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/north_korea.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/kp.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/korea.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/kr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/denmark.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/dk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/faroe_islands.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/fo.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/greenland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/gl.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/finland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/fi.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/iceland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/is.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/norway.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/no.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/sweden.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/se.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/estonia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ee.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/latvia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/lv.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/lithuania.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/lt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/belgium.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/be.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/netherlands.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/nl.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/luxembourg.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/lu.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/germany.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/de.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/austria.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/at.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/switzerland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ch.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/poland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/pl.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/czech_republic.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cz.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/slovakia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/sk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/hungary.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/hu.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/romania.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ro.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/moldova.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/md.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/bulgaria.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/bg.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/france.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/fr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/italy.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/it.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/portugal.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/pt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/spain.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/es.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/russia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ru.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/belarus.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/by.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/ukraine.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ua.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/armenia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/am.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/azerbaijan.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/az.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/georgia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ge.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/bosnia_and_herzegovina.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ba.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/croatia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/hr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/montenegro.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/me.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/north_macedonia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/serbia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/rs.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/slovenia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/si.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/albania.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/al.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/kosovo.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/xk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/greece.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/gr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/cyprus.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cy.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/andorra.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ad.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/malta.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/monaco.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mc.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/san_marino.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/sm.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/iran.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ir.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/iraq.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/iq.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/israel.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/il.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/qatar.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/qa.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/turkey.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/tr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/united_arab_emirates.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ae.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/argentina.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ar.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/costa_rica.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/dominican_republic.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/do.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/mexico.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mx.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/paraguay.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/py.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/peru.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/pe.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/venezuela.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ve.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/brazil.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/br.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/trinidad.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/tt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/chad.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/td.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/somalia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/so.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/indonesia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/id.svg?sanitize=true" width="24" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Or free on the Internet:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Plex TV&lt;/li&gt; 
 &lt;li&gt;Pluto TV (English, Spanish, French, Italian)&lt;/li&gt; 
 &lt;li&gt;Redbox Live TV&lt;/li&gt; 
 &lt;li&gt;Roku TV&lt;/li&gt; 
 &lt;li&gt;Samsung TV Plus&lt;/li&gt; 
 &lt;li&gt;Youtube live channels&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use it point your IPTV player to &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8"&gt;https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Philosophy&lt;/h1&gt; 
&lt;p&gt;The main goals for this playlist are listed below.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quality over quantity&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The less channels we support the better.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All channels should work well.&lt;/li&gt; 
 &lt;li&gt;As much as possible channels should be in HD, not SD.&lt;/li&gt; 
 &lt;li&gt;Only one URL per channel (no +1, no alternate feeds, no regional declinations)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Only free channels&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If a channel is normally only available via commercial subscriptions it has nothing to do in this playlist. If on the other hand it is provided for free to everybody in a particular country, then it should be in this playlist.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No paid channels&lt;/li&gt; 
 &lt;li&gt;Only channels which are officially provided for free (via DVB-S, DVB-T, analog, etc..)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Only mainstream channels&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This is a playlist for everybody.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No adult channels&lt;/li&gt; 
 &lt;li&gt;No channels dedicated to any particular religion&lt;/li&gt; 
 &lt;li&gt;No channels dedicated to any particular political party&lt;/li&gt; 
 &lt;li&gt;No channels made for a country and funded by a different country&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Feed sources&lt;/h1&gt; 
&lt;p&gt;It can be quite hard to find up to date URLs, here's a list of sources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iptv-org/iptv/tree/master/streams"&gt;https://github.com/iptv-org/iptv/tree/master/streams&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Youtube: As long as the channel is live and its URL doesn't change (check the age of the stream, the number of viewers..)&lt;/li&gt; 
 &lt;li&gt;Dailymotion: Same criteria as for youtube&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Format&lt;/h1&gt; 
&lt;p&gt;The m3u8 playlist is generated by &lt;code&gt;make_playlist.py&lt;/code&gt;, using the &lt;code&gt;.md&lt;/code&gt; files located in &lt;code&gt;lists&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Each .md file represesnts a group. The &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt; line is used as the group title.&lt;/p&gt; 
&lt;p&gt;Only channels which URL column starts with &lt;code&gt;[&amp;gt;]&lt;/code&gt; are included in the playlist.&lt;/p&gt; 
&lt;p&gt;Channels which are not in HD are marked with an &lt;code&gt;‚ìà&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Channels which use GeoIP blocking are marked with a &lt;code&gt;‚íº&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Channels which are live Youtube channels are marked with a &lt;code&gt;‚ìé&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Issues&lt;/h1&gt; 
&lt;p&gt;Only create issues for bugs and feature requests.&lt;/p&gt; 
&lt;p&gt;Do not create issues to add/edit or to remove channels. If you want to add/edit/remove channels, create a pull request directly.&lt;/p&gt; 
&lt;h1&gt;Pull Requests&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Only modify .md files&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If your Pull Request modifies channels, only modify .md files. Do not modify m3u8 files in your pull request.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Adding a new Channel&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To add a new channel, make a Pull Request.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In your Pull Request you need to provide information to show that the channel is free.&lt;/li&gt; 
 &lt;li&gt;Use imgur.com to host the channel logo and point to it.&lt;/li&gt; 
 &lt;li&gt;If you have a valid stream, add it and put &lt;code&gt;[&amp;gt;]&lt;/code&gt; in front of it.&lt;/li&gt; 
 &lt;li&gt;If you don't have an stream for the channel, add &lt;code&gt;[x]()&lt;/code&gt; in the url column and place your channel in the Invalid category.&lt;/li&gt; 
 &lt;li&gt;If you have a stream but it doesn't work well, put the channel in the Invalid category and put &lt;code&gt;[x]&lt;/code&gt; in front of the url.&lt;/li&gt; 
 &lt;li&gt;If you're adding geoblocked URLs specify it in your PR and specify which country they're working in. The PR will only be merged if these URLs can be tested.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Removing a Channel&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To remove a channel, make a Pull Request.&lt;/p&gt; 
&lt;p&gt;In your Pull Request you need to provide information to show that the channel is only available via a private paid subscription.&lt;/p&gt; 
&lt;p&gt;Note: Public taxes (whether national or regional, whether called TV License or not) do not constitute a private paid subscription.&lt;/p&gt; 
&lt;p&gt;If a stream is broken, simply move the channel to the invalid category and replace &lt;code&gt;[&amp;gt;]&lt;/code&gt; with &lt;code&gt;[x]&lt;/code&gt; in the url column.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>IAmTomShaw/f1-race-replay</title>
      <link>https://github.com/IAmTomShaw/f1-race-replay</link>
      <description>&lt;p&gt;An interactive Formula 1 race visualisation and data analysis tool built with Python! üèéÔ∏è&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;F1 Race Replay üèéÔ∏è üèÅ&lt;/h1&gt; 
&lt;p&gt;A Python application for visualizing Formula 1 race telemetry and replaying race events with interactive controls and a graphical interface.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/resources/preview.png" alt="Race Replay Preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Race Replay Visualization:&lt;/strong&gt; Watch the race unfold with real-time driver positions on a rendered track.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leaderboard:&lt;/strong&gt; See live driver positions and current tyre compounds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lap &amp;amp; Time Display:&lt;/strong&gt; Track the current lap and total race time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Driver Status:&lt;/strong&gt; Drivers who retire or go out are marked as "OUT" on the leaderboard.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Controls:&lt;/strong&gt; Pause, rewind, fast forward, and adjust playback speed using on-screen buttons or keyboard shortcuts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Legend:&lt;/strong&gt; On-screen legend explains all controls.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Driver Telemetry Insights:&lt;/strong&gt; View speed, gear, DRS status, and current lap for selected drivers when selected on the leaderboard.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Controls&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pause/Resume:&lt;/strong&gt; SPACE or Pause button&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rewind/Fast Forward:&lt;/strong&gt; ‚Üê / ‚Üí or Rewind/Fast Forward buttons&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Playback Speed:&lt;/strong&gt; ‚Üë / ‚Üì or Speed button (cycles through 0.5x, 1x, 2x, 4x)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set Speed Directly:&lt;/strong&gt; Keys 1‚Äì4&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Restart&lt;/strong&gt;: &lt;strong&gt;R&lt;/strong&gt; to restart replay&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toggle DRS Zone&lt;/strong&gt;: &lt;strong&gt;D&lt;/strong&gt; to hide/showi DRS Zone&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toggle Progress Bar&lt;/strong&gt;: &lt;strong&gt;B&lt;/strong&gt; to hide/show progress bar&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toggle Driver Names&lt;/strong&gt;: &lt;strong&gt;L&lt;/strong&gt; to hide/show driver names on track&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Select driver/drivers&lt;/strong&gt;: Click to select driver or shift click to select multiple drivers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Qualifying Session Support (in development)&lt;/h2&gt; 
&lt;p&gt;Recently added support for Qualifying session replays with telemetry visualization including speed, gear, throttle, and brake over the lap distance. This feature is still being refined.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.11+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/theOehrly/Fast-F1"&gt;FastF1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://api.arcade.academy/en/latest/"&gt;Arcade&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;numpy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;FastF1 cache folder will be created automatically on first run. If it is not created, you can manually create a folder named &lt;code&gt;.fastf1-cache&lt;/code&gt; in the project root.&lt;/p&gt; 
&lt;h2&gt;Environment Setup&lt;/h2&gt; 
&lt;p&gt;To get started with this project locally, you can follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the Repository:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/IAmTomShaw/f1-race-replay
 cd f1-race-replay
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create a Virtual Environment:&lt;/strong&gt; This process differs based on your operating system.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;On macOS/Linux: &lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv venv
source venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;On Windows: &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
.\venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the Application:&lt;/strong&gt; You can now run the application using the instructions in the Usage section below.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;DEFAULT GUI MENU:&lt;/strong&gt; To use the new GUI menu system, you can simply run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/resources/gui-menu.png" alt="GUI Menu Preview" /&gt;&lt;/p&gt; 
&lt;p&gt;This will open a graphical interface where you can select the year and round of the race weekend you want to replay. This is still a new feature, so please report any issues you encounter.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OPTIONAL CLI MENU:&lt;/strong&gt; To use the CLI menu system, you can simply run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/resources/cli-menu.gif" alt="CLI Menu Preview" /&gt;&lt;/p&gt; 
&lt;p&gt;This will prompt you with series of questions and a list of options to make your choice from using the arrow keys and enter key.&lt;/p&gt; 
&lt;p&gt;If you would already know the year and round number of the session you would like to watch, you run the commands directly as follows:&lt;/p&gt; 
&lt;p&gt;Run the main script and specify the year and round:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run without HUD:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --no-hud
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run a Sprint session (if the event has one), add &lt;code&gt;--sprint&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --sprint
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The application will load a pre-computed telemetry dataset if you have run it before for the same event. To force re-computation of telemetry data, use the &lt;code&gt;--refresh-data&lt;/code&gt; flag:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --refresh-data
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qualifying Session Replay&lt;/h3&gt; 
&lt;p&gt;To run a Qualifying session replay, use the &lt;code&gt;--qualifying&lt;/code&gt; flag:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --qualifying
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run a Sprint Qualifying session (if the event has one), add &lt;code&gt;--sprint&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --qualifying --sprint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;f1-race-replay/
‚îú‚îÄ‚îÄ main.py                    # Entry point, handles session loading and starts the replay
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ README.md                  # Project documentation
‚îú‚îÄ‚îÄ roadmap.md                 # Planned features and project vision
‚îú‚îÄ‚îÄ resources/
‚îÇ   ‚îî‚îÄ‚îÄ preview.png           # Race replay preview image
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ f1_data.py            # Telemetry loading, processing, and frame generation
‚îÇ   ‚îú‚îÄ‚îÄ arcade_replay.py      # Visualization and UI logic
‚îÇ   ‚îî‚îÄ‚îÄ ui_components.py      # UI components like buttons and leaderboard
‚îÇ   ‚îú‚îÄ‚îÄ interfaces/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ qualifying.py     # Qualifying session interface and telemetry visualization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ race_replay.py    # Race replay interface and telemetry visualization
‚îÇ   ‚îî‚îÄ‚îÄ lib/
‚îÇ       ‚îî‚îÄ‚îÄ tyres.py          # Type definitions for telemetry data structures
‚îÇ       ‚îî‚îÄ‚îÄ time.py           # Time formatting utilities
‚îî‚îÄ‚îÄ .fastf1-cache/            # FastF1 cache folder (created automatically upon first run)
‚îî‚îÄ‚îÄ computed_data/            # Computed telemetry data (created automatically upon first run)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Customization&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Change track width, colors, and UI layout in &lt;code&gt;src/arcade_replay.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Adjust telemetry processing in &lt;code&gt;src/f1_data.py&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;There have been several contributions from the community that have helped enhance this project. I have added a &lt;a href="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/contributors.md"&gt;contributors.md&lt;/a&gt; file to acknowledge those who have contributed features and improvements.&lt;/p&gt; 
&lt;p&gt;If you would like to contribute, feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open pull requests for UI improvements or new features.&lt;/li&gt; 
 &lt;li&gt;Report issues on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please see &lt;a href="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/roadmap.md"&gt;roadmap.md&lt;/a&gt; for planned features and project vision.&lt;/p&gt; 
&lt;h1&gt;Known Issues&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;The leaderboard appears to be inaccurate for the first few corners of the race. The leaderboard is also temporarily affected by a driver going in the pits. At the end of the race, the leaderboard is sometimes affected by the drivers' final x,y positions being further ahead than other drivers. These are known issues caused by inaccuracies in the telemetry and are being worked on for future releases. It's likely that these issues will be fixed in stages as improving the leaderboard accuracy is a complex task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìù License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License.&lt;/p&gt; 
&lt;h2&gt;‚ö†Ô∏è Disclaimer&lt;/h2&gt; 
&lt;p&gt;No copyright infringement intended. Formula 1 and related trademarks are the property of their respective owners. All data used is sourced from publicly available APIs and is used for educational and non-commercial purposes only.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Built with ‚ù§Ô∏è by &lt;a href="https://tomshaw.dev"&gt;Tom Shaw&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ManimCommunity/manim</title>
      <link>https://github.com/ManimCommunity/manim</link>
      <description>&lt;p&gt;A community-maintained Python framework for creating mathematical animations.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://www.manim.community/"&gt;&lt;img src="https://raw.githubusercontent.com/ManimCommunity/manim/main/logo/cropped.png" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://pypi.org/project/manim/"&gt;&lt;img src="https://img.shields.io/pypi/v/manim.svg?style=flat&amp;amp;logo=pypi" alt="PyPI Latest Release" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/manimcommunity/manim"&gt;&lt;img src="https://img.shields.io/docker/v/manimcommunity/manim?color=%23099cec&amp;amp;label=docker%20image&amp;amp;logo=docker" alt="Docker image" /&gt; &lt;/a&gt; &lt;a href="https://mybinder.org/v2/gh/ManimCommunity/jupyter_examples/HEAD?filepath=basic_example_scenes.ipynb"&gt;&lt;img src="https://mybinder.org/badge_logo.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="http://choosealicense.com/licenses/mit/"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-red.svg?style=flat" alt="MIT License" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/manim/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=orange&amp;amp;label=reddit&amp;amp;logo=reddit" alt="Reddit" href /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/manimcommunity/"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&amp;amp;label=Follow%20%40manimcommunity" alt="Twitter" /&gt; &lt;/a&gt;&lt;a href="https://manim.community/discord/"&gt;&lt;img src="https://img.shields.io/discord/581738731934056449.svg?label=discord&amp;amp;color=yellow&amp;amp;logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://docs.manim.community/"&gt;&lt;img src="https://readthedocs.org/projects/manimce/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;img src="https://github.com/ManimCommunity/manim/workflows/CI/badge.svg?sanitize=true" alt="CI" /&gt; &lt;br /&gt; &lt;br /&gt; &lt;i&gt;An animation engine for explanatory math videos&lt;/i&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Manim is an animation engine for explanatory math videos. It's used to create precise animations programmatically, as demonstrated in the videos of &lt;a href="https://www.3blue1brown.com/"&gt;3Blue1Brown&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The community edition of Manim (ManimCE) is a version maintained and developed by the community. It was forked from 3b1b/manim, a tool originally created and open-sourced by Grant Sanderson, also creator of the 3Blue1Brown educational math videos. While Grant Sanderson continues to maintain his own repository, we recommend this version for its continued development, improved features, enhanced documentation, and more active community-driven maintenance. If you would like to study how Grant makes his videos, head over to his repository (&lt;a href="https://github.com/3b1b/manim"&gt;3b1b/manim&lt;/a&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Table of Contents:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#usage"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#help-with-manim"&gt;Help with Manim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] These instructions are for the community version &lt;em&gt;only&lt;/em&gt;. Trying to use these instructions to install &lt;a href="https://github.com/3b1b/manim"&gt;3b1b/manim&lt;/a&gt; or instructions there to install this version will cause problems. Read &lt;a href="https://docs.manim.community/en/stable/faq/installation.html#why-are-there-different-versions-of-manim"&gt;this&lt;/a&gt; and decide which version you wish to install, then only follow the instructions for your desired version.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Manim requires a few dependencies that must be installed prior to using it. If you want to try it out first before installing it locally, you can do so &lt;a href="https://try.manim.community/"&gt;in our online Jupyter environment&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For local installation, please visit the &lt;a href="https://docs.manim.community/en/stable/installation.html"&gt;Documentation&lt;/a&gt; and follow the appropriate instructions for your operating system.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Manim is an extremely versatile package. The following is an example &lt;code&gt;Scene&lt;/code&gt; you can construct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from manim import *


class SquareToCircle(Scene):
    def construct(self):
        circle = Circle()
        square = Square()
        square.flip(RIGHT)
        square.rotate(-3 * TAU / 8)
        circle.set_fill(PINK, opacity=0.5)

        self.play(Create(square))
        self.play(Transform(square, circle))
        self.play(FadeOut(square))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In order to view the output of this scene, save the code in a file called &lt;code&gt;example.py&lt;/code&gt;. Then, run the following in a terminal window:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;manim -p -ql example.py SquareToCircle
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see your native video player program pop up and play a simple scene in which a square is transformed into a circle. You may find some more simple examples within this &lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/example_scenes"&gt;GitHub repository&lt;/a&gt;. You can also visit the &lt;a href="https://docs.manim.community/en/stable/examples.html"&gt;official gallery&lt;/a&gt; for more advanced examples.&lt;/p&gt; 
&lt;p&gt;Manim also ships with a &lt;code&gt;%%manim&lt;/code&gt; IPython magic which allows to use it conveniently in JupyterLab (as well as classic Jupyter) notebooks. See the &lt;a href="https://docs.manim.community/en/stable/reference/manim.utils.ipython_magic.ManimMagic.html"&gt;corresponding documentation&lt;/a&gt; for some guidance and &lt;a href="https://mybinder.org/v2/gh/ManimCommunity/jupyter_examples/HEAD?filepath=basic_example_scenes.ipynb"&gt;try it out online&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Command line arguments&lt;/h2&gt; 
&lt;p&gt;The general usage of Manim is as follows:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ManimCommunity/manim/main/docs/source/_static/command.png" alt="manim-illustration" /&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;-p&lt;/code&gt; flag in the command above is for previewing, meaning the video file will automatically open when it is done rendering. The &lt;code&gt;-ql&lt;/code&gt; flag is for a faster rendering at a lower quality.&lt;/p&gt; 
&lt;p&gt;Some other useful flags include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-s&lt;/code&gt; to skip to the end and just show the final frame.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n &amp;lt;number&amp;gt;&lt;/code&gt; to skip ahead to the &lt;code&gt;n&lt;/code&gt;'th animation of a scene.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-f&lt;/code&gt; show the file in the file browser.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For a thorough list of command line arguments, visit the &lt;a href="https://docs.manim.community/en/stable/guides/configuration.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Documentation is in progress at &lt;a href="https://docs.manim.community/"&gt;ReadTheDocs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;The community also maintains a docker image (&lt;code&gt;manimcommunity/manim&lt;/code&gt;), which can be found &lt;a href="https://hub.docker.com/r/manimcommunity/manim"&gt;on DockerHub&lt;/a&gt;. Instructions on how to install and use it can be found in our &lt;a href="https://docs.manim.community/en/stable/installation/docker.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Help with Manim&lt;/h2&gt; 
&lt;p&gt;If you need help installing or using Manim, feel free to reach out to our &lt;a href="https://www.manim.community/discord/"&gt;Discord Server&lt;/a&gt; or &lt;a href="https://www.reddit.com/r/manim"&gt;Reddit Community&lt;/a&gt;. If you would like to submit a bug report or feature request, please open an issue.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions to Manim are always welcome. In particular, there is a dire need for tests and documentation. For contribution guidelines, please see the &lt;a href="https://docs.manim.community/en/stable/contributing.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;However, please note that Manim is currently undergoing a major refactor. In general, contributions implementing new features will not be accepted in this period. The contribution guide may become outdated quickly; we highly recommend joining our &lt;a href="https://www.manim.community/discord/"&gt;Discord server&lt;/a&gt; to discuss any potential contributions and keep up to date with the latest developments.&lt;/p&gt; 
&lt;p&gt;Most developers on the project use &lt;code&gt;uv&lt;/code&gt; for management. You'll want to have uv installed and available in your environment. Learn more about &lt;code&gt;uv&lt;/code&gt; at its &lt;a href="https://docs.astral.sh/uv/"&gt;documentation&lt;/a&gt; and find out how to install manim with uv at the &lt;a href="https://docs.manim.community/en/latest/contributing/development.html"&gt;manim dev-installation guide&lt;/a&gt; in the manim documentation.&lt;/p&gt; 
&lt;h2&gt;How to Cite Manim&lt;/h2&gt; 
&lt;p&gt;We acknowledge the importance of good software to support research, and we note that research becomes more valuable when it is communicated effectively. To demonstrate the value of Manim, we ask that you cite Manim in your work. Currently, the best way to cite Manim is to go to our &lt;a href="https://github.com/ManimCommunity/manim"&gt;repository page&lt;/a&gt; (if you aren't already) and click the "cite this repository" button on the right sidebar. This will generate a citation in your preferred format, and will also integrate well with citation managers.&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;Our full code of conduct, and how we enforce it, can be read on &lt;a href="https://docs.manim.community/en/stable/conduct.html"&gt;our website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The software is double-licensed under the MIT license, with copyright by 3blue1brown LLC (see LICENSE), and copyright by Manim Community Developers (see LICENSE.community).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=githubpages" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/TTS-Report-red?logo=arxiv" alt="TTS Report" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2601.18184"&gt;&lt;img src="https://img.shields.io/badge/ASR-Report-yellow?logo=arxiv" alt="ASR Report" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb"&gt;&lt;img src="https://img.shields.io/badge/StreamingTTS-Colab-green?logo=googlecolab" alt="Colab" /&gt;&lt;/a&gt; &lt;a href="https://aka.ms/vibevoice-asr"&gt;&lt;img src="https://img.shields.io/badge/ASR-Playground-6F42C1?logo=gradio" alt="ASR Playground" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;2026-01-21: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/a&gt;, a unified speech-to-text model designed to handle 60-minute long-form audio in a single pass, generating structured transcriptions containing Who (Speaker), When (Timestamps), and What (Content), with support for User-Customized Context. Try it in &lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‚≠êÔ∏è VibeVoice-ASR is natively multilingual, supporting over 50 languages ‚Äî check the &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md#language-distribution"&gt;supported languages&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;li&gt;üî• The VibeVoice-ASR &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;finetuning code&lt;/a&gt; is now available!&lt;/li&gt; 
  &lt;li&gt;‚ö°Ô∏è &lt;strong&gt;vLLM inference&lt;/strong&gt; is now supported for faster inference; see &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-vllm-asr.md"&gt;vllm-asr&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üìë &lt;a href="https://arxiv.org/pdf/2601.18184"&gt;VibeVoice-ASR Technique Report&lt;/a&gt; is available.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;2025-12-16: üì£ We added experimental speakers to &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt; for exploration, including multilingual voices in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) and 11 distinct English style voices. &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices"&gt;Try it&lt;/a&gt;. More speaker types will be added over time.&lt;/p&gt; 
 &lt;p&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have removed the VibeVoice-TTS code from this repository.&lt;/p&gt; 
 &lt;p&gt;2025-08-25: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;&lt;strong&gt;VibeVoice-TTS&lt;/strong&gt;&lt;/a&gt;, a long-form multi-speaker text-to-speech model that can synthesize speech up to 90 minutes long with up to 4 distinct speakers.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;VibeVoice is a &lt;strong&gt;family of open-source frontier voice AI models&lt;/strong&gt; that includes both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) models.&lt;/p&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of &lt;strong&gt;7.5 Hz&lt;/strong&gt;. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p&gt;For more information, demos, and examples, please visit our &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Weight&lt;/th&gt; 
    &lt;th&gt;Quick Try&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-ASR-7B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-TTS-1.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Disabled&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-Realtime-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;h3&gt;1. üìñ &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;VibeVoice-ASR&lt;/a&gt; - Long-form Speech Recognition&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt; is a unified speech-to-text model designed to handle &lt;strong&gt;60-minute long-form audio&lt;/strong&gt; in a single pass, generating structured transcriptions containing &lt;strong&gt;Who (Speaker), When (Timestamps), and What (Content)&lt;/strong&gt;, with support for &lt;strong&gt;Customized Hotwords&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üïí 60-minute Single-Pass Processing&lt;/strong&gt;: Unlike conventional ASR models that slice audio into short chunks (often losing global context), VibeVoice ASR accepts up to &lt;strong&gt;60 minutes&lt;/strong&gt; of continuous audio input within 64K token length. This ensures consistent speaker tracking and semantic coherence across the entire hour.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë§ Customized Hotwords&lt;/strong&gt;: Users can provide customized hotwords (e.g., specific names, technical terms, or background info) to guide the recognition process, significantly improving accuracy on domain-specific content.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Rich Transcription (Who, When, What)&lt;/strong&gt;: The model jointly performs ASR, diarization, and timestamping, producing a structured output that indicates &lt;em&gt;who&lt;/em&gt; said &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;when&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://aka.ms/vibevoice-asr"&gt;üéÆ Playground&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;üõ†Ô∏è Finetuning&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/VibeVoice-ASR-Report.pdf"&gt;üìä Paper&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/DER.jpg" alt="DER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/cpWER.jpg" alt="cpWER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/tcpWER.jpg" alt="tcpWER" width="50%" /&gt; &lt;/p&gt; 
&lt;div align="center" id="vibevoice-asr"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa"&gt;https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;2. üéôÔ∏è &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;VibeVoice-TTS&lt;/a&gt; - Long-form Multi-speaker TTS&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Best for&lt;/strong&gt;: Long-form conversational audio, podcasts, multi-speaker dialogues&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚è±Ô∏è 90-minute Long-form Generation&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; in a single pass, maintaining speaker consistency and semantic coherence throughout.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë• Multi-speaker Support&lt;/strong&gt;: Supports up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt; in a single conversation, with natural turn-taking and speaker consistency across long dialogues.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üé≠ Expressive Speech&lt;/strong&gt;: Generates expressive, natural-sounding speech that captures conversational dynamics and emotional nuances.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Multi-lingual Support&lt;/strong&gt;: Supports English, Chinese and other languages.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;üìä Paper&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice-TTS-results.jpg" alt="VibeVoice Results" width="80%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;3. ‚ö° &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;VibeVoice-Streaming&lt;/a&gt; - Real-time Streaming TTS&lt;/h3&gt; 
&lt;p&gt;VibeVoice-Realtime is a &lt;strong&gt;lightweight real‚Äëtime&lt;/strong&gt; text-to-speech model supporting &lt;strong&gt;streaming text input&lt;/strong&gt; and &lt;strong&gt;robust long-form speech generation&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Parameter size: 0.5B (deployment-friendly)&lt;/li&gt; 
 &lt;li&gt;Real-time TTS (~300 milliseconds first audible latency)&lt;/li&gt; 
 &lt;li&gt;Streaming text input&lt;/li&gt; 
 &lt;li&gt;Robust long-form speech generation (~10 minutes)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;üöÄ Colab&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center" id="generated-example-audio-vibevoice-realtime"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please see &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for detailed contribution guidelines.&lt;/p&gt; 
&lt;h2&gt;‚ö†Ô∏è Risks and Limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;h1&gt;PageIndex: Vectorless, Reasoning-based RAG&lt;/h1&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; ‚ó¶ &amp;nbsp;No Vector DB&amp;nbsp; ‚ó¶ &amp;nbsp;No Chunking&amp;nbsp; ‚ó¶ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;üñ•Ô∏è Chat Platform&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;üîå MCP&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;üìö Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;üì¢ Latest Updates&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;üî• Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework ‚Äî an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;üß™ Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìë Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity ‚â† relevance&lt;/strong&gt; ‚Äî what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; ‚Äî a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a ‚ÄúTable-of-Contents‚Äù &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Core Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;üìç Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host ‚Äî run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service ‚Äî try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; ‚Äî private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß™ Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook ‚Äî a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; ‚Äî no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üå≤ PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚öôÔ∏è Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# ‚òÅÔ∏è Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìà Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üß≠ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß™ &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;üîå &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚≠ê Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star üåü if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/PageIndexAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;¬© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/UltraRAG</title>
      <link>https://github.com/OpenBMB/UltraRAG</link>
      <description>&lt;p&gt;UltraRAG v3: A Low-Code MCP Framework for Building Complex and Innovative RAG Pipelines&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="./docs/ultrarag_dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="./docs/ultrarag.svg" /&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/ultrarag.svg?sanitize=true" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Less Code, Lower Barrier, Faster Deployment &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://modelscope.cn/datasets/UltraRAG/UltraRAG_Benchmark"&gt;&lt;b&gt;Dataset&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;&lt;b&gt;Paper Daily&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/README_zh.md"&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; | &lt;b&gt;English&lt;/b&gt; | &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; üî•&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2026.01.23] üéâ UltraRAG 3.0 Released: Say no to "black box" development‚Äîmake every line of reasoning logic clearly visible üëâ|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/ultrarag3_0.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
 &lt;li&gt;[2026.01.20] üéâ AgentCPM-Report Model Released! DeepResearch is finally localized: 8B on-device writing agent AgentCPM-Report is open-sourced üëâ |&lt;a href="https://huggingface.co/openbmb/AgentCPM-Report"&gt;ü§ó Model&lt;/a&gt;|&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025.11.11] üéâ UltraRAG 2.1 Released: Enhanced knowledge ingestion &amp;amp; multimodal support, with a more complete unified evaluation system!&lt;/li&gt; 
  &lt;li&gt;[2025.09.23] New daily RAG paper digest, updated every day üëâ |&lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;üìñ Papers&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.09] Released a Lightweight DeepResearch Pipeline local setup tutorial üëâ |&lt;a href="https://www.bilibili.com/video/BV1p8JfziEwM"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/01_build_light_deepresearch.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.01] Released a step-by-step UltraRAG installation and full RAG walkthrough video üëâ |&lt;a href="https://www.bilibili.com/video/BV1B9apz4E7K/?share_source=copy_web&amp;amp;vd_source=7035ae721e76c8149fb74ea7a2432710"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/00_Installing_and_Running_RAG.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.08.28] üéâ UltraRAG 2.0 Released! UltraRAG 2.0 is fully upgraded: build a high-performance RAG with just a few dozen lines of code, empowering researchers to focus on ideas and innovation! We have preserved the UltraRAG v2 code, which can be viewed at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v2"&gt;v2&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025.01.23] UltraRAG Released! Enabling large models to better comprehend and utilize knowledge bases. The UltraRAG 1.0 code is still available at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v1"&gt;v1&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About UltraRAG&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/18747" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/18747" alt="OpenBMB%2FUltraRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;UltraRAG is the first lightweight RAG development framework based on the &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;Model Context Protocol (MCP)&lt;/a&gt; architecture design, jointly launched by &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt; at Tsinghua University, &lt;a href="https://neuir.github.io"&gt;NEUIR&lt;/a&gt; at Northeastern University, &lt;a href="https://www.openbmb.cn/home"&gt;OpenBMB&lt;/a&gt;, and &lt;a href="https://github.com/AI9Stars"&gt;AI9stars&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Designed for research exploration and industrial prototyping, UltraRAG standardizes core RAG components (Retriever, Generation, etc.) as independent &lt;strong&gt;MCP Servers&lt;/strong&gt;, combined with the powerful workflow orchestration capabilities of the &lt;strong&gt;MCP Client&lt;/strong&gt;. Developers can achieve precise orchestration of complex control structures such as conditional branches and loops simply through YAML configuration.&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/architecture.png" width="90%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3&gt;UltraRAG UI&lt;/h3&gt; 
&lt;p&gt;UltraRAG UI transcends the boundaries of traditional chat interfaces, evolving into a visual RAG Integrated Development Environment (IDE) that combines orchestration, debugging, and demonstration.&lt;/p&gt; 
&lt;p&gt;The system features a powerful built-in Pipeline Builder that supports bidirectional real-time synchronization between "Canvas Construction" and "Code Editing," allowing for granular online adjustments of pipeline parameters and prompts. Furthermore, it introduces an Intelligent AI Assistant to empower the entire development lifecycle, from pipeline structural design to parameter tuning and prompt generation. Once constructed, logic flows can be converted into interactive dialogue systems with a single click. The system seamlessly integrates Knowledge Base Management components, enabling users to build custom knowledge bases for document Q&amp;amp;A. This truly realizes a one-stop closed loop, spanning from underlying logic construction and data governance to final application deployment.&lt;/p&gt; 
&lt;!-- &lt;p align="center"&gt;
  &lt;picture&gt;
    &lt;img alt="UltraRAG_UI" src="./docs/chat_menu.png" width=80%&gt;
  &lt;/picture&gt;
&lt;/p&gt; --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9"&gt;https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Key Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Low-Code Orchestration of Complex Workflows&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Inference Orchestration&lt;/strong&gt;: Natively supports control structures such as sequential, loop, and conditional branches. Developers only need to write YAML configuration files to implement complex iterative RAG logic in dozens of lines of code.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ö° &lt;strong&gt;Modular Extension and Reproduction&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Atomic Servers&lt;/strong&gt;: Based on the MCP architecture, functions are decoupled into independent Servers. New features only need to be registered as function-level Tools to seamlessly integrate into workflows, achieving extremely high reusability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìä &lt;strong&gt;Unified Evaluation and Benchmark Comparison&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Research Efficiency&lt;/strong&gt;: Built-in standardized evaluation workflows, ready-to-use mainstream research benchmarks. Through unified metric management and baseline integration, significantly improves experiment reproducibility and comparison efficiency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ú® &lt;strong&gt;Rapid Interactive Prototype Generation&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;One-Click Delivery&lt;/strong&gt;: Say goodbye to tedious UI development. With just one command, Pipeline logic can be instantly converted into an interactive conversational Web UI, shortening the distance from algorithm to demonstration.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We provide two installation methods: local source code installation (recommended using &lt;code&gt;uv&lt;/code&gt; for package management) and Docker container deployment&lt;/p&gt; 
&lt;h3&gt;Method 1: Source Code Installation&lt;/h3&gt; 
&lt;p&gt;We strongly recommend using &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; to manage Python environments and dependencies, as it can greatly improve installation speed.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Prepare Environment&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you haven't installed uv yet, please execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;## Direct installation
pip install uv
## Download
curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Download Source Code&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Choose one of the following modes to install dependencies based on your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A: Create a New Environment&lt;/strong&gt; Use &lt;code&gt;uv sync&lt;/code&gt; to automatically create a virtual environment and synchronize dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Core dependencies: If you only need to run basic core functions, such as only using UltraRAG UI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Full installation: If you want to fully experience UltraRAG's retrieval, generation, corpus processing, and evaluation functions, please run:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;On-demand installation: If you only need to run specific modules, keep the corresponding &lt;code&gt;--extra&lt;/code&gt; as needed, for example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --extra retriever   # Retrieval module only
uv sync --extra generation  # Generation module only
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once installed, activate the virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Windows CMD
.venv\Scripts\activate.bat

# Windows Powershell
.venv\Scripts\Activate.ps1

# macOS / Linux
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;B: Install into an Existing Environment&lt;/strong&gt; To install UltraRAG into your currently active Python environment, use &lt;code&gt;uv pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Core dependencies
uv pip install -e .

# Full installation
uv pip install -e ".[all]"

# On-demand installation
uv pip install -e ".[retriever]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Method 2: Docker Container Deployment&lt;/h3&gt; 
&lt;p&gt;If you prefer not to configure a local Python environment, you can deploy using Docker.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Get Code and Images&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# 1. Clone the repository
git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG

# 2. Prepare the image (choose one)
# Option A: Pull from Docker Hub
docker pull hdxin2002/ultrarag:v0.3.0-base-cpu # Base version (CPU)
docker pull hdxin2002/ultrarag:v0.3.0-base-gpu # Base version (GPU)
docker pull hdxin2002/ultrarag:v0.3.0          # Full version (GPU)

# Option B: Build locally
docker build -t ultrarag:v0.3.0 .

# 3. Start container (port 5050 is automatically mapped)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Start the Container&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Start the container (Port 5050 is mapped by default)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: After the container starts, UltraRAG UI will run automatically. You can directly access &lt;code&gt;http://localhost:5050&lt;/code&gt; in your browser to use it.&lt;/p&gt; 
&lt;h3&gt;Verify Installation&lt;/h3&gt; 
&lt;p&gt;After installation, run the following example command to check if the environment is normal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ultrarag run examples/sayhello.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you see the following output, the installation is successful:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Hello, UltraRAG v3!
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;We provide complete tutorial examples from beginner to advanced. Whether you are conducting academic research or building industrial applications, you can find guidance here. Welcome to visit the &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;Documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Research Experiments&lt;/h3&gt; 
&lt;p&gt;Designed for researchers, providing data, experimental workflows, and visualization analysis tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/quick_start"&gt;Getting Started&lt;/a&gt;: Learn how to quickly run standard RAG experimental workflows based on UltraRAG.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/dataset"&gt;Evaluation Data&lt;/a&gt;: Download the most commonly used public evaluation datasets in the RAG field and large-scale retrieval corpora, directly for research benchmark testing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/case_study"&gt;Case Analysis&lt;/a&gt;: Provides a visual Case Study interface to deeply track each intermediate output of the workflow, assisting in analysis and error attribution.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/code_integration"&gt;Code Integration&lt;/a&gt;: Learn how to directly call UltraRAG components in Python code to achieve more flexible customized development.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Demo Systems&lt;/h3&gt; 
&lt;p&gt;Designed for developers and end users, providing complete UI interaction and complex application cases.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/start"&gt;Quick Start&lt;/a&gt;: Learn how to start UltraRAG UI and familiarize yourself with various advanced configurations in administrator mode.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/prepare"&gt;Deployment Guide&lt;/a&gt;: Detailed production environment deployment tutorials, covering the setup of Retriever, Generation models (LLM), and Milvus vector database.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/demo/deepresearch"&gt;Deep Research&lt;/a&gt;: Flagship case, deploy a Deep Research Pipeline. Combined with the AgentCPM-Report model, it can automatically perform multi-step retrieval and integration to generate tens of thousands of words of survey reports.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thanks to the following contributors for their code submissions and testing. We also welcome new members to join us in collectively building a comprehensive RAG ecosystem!&lt;/p&gt; 
&lt;p&gt;You can contribute by following the standard process: &lt;strong&gt;Fork this repository ‚Üí Submit Issues ‚Üí Create Pull Requests (PRs)&lt;/strong&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBMB/UltraRAG/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=OpenBMB/UltraRAG&amp;amp;nocache=true" /&gt; &lt;/a&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;If you find this repository helpful for your research, please consider giving us a ‚≠ê to show your support.&lt;/p&gt; 
&lt;a href="https://star-history.com/#OpenBMB/UltraRAG&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical issues and feature requests, please use &lt;a href="https://github.com/OpenBMB/UltraRAG/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For questions about usage, feedback, or any discussions related to RAG technologies, you are welcome to join our &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/wechat_qr.png"&gt;WeChat group&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/feishu_qr.png"&gt;Feishu group&lt;/a&gt;, and &lt;a href="https://discord.gg/yRFFjjJnnS"&gt;Discord&lt;/a&gt; to exchange ideas with us.&lt;/li&gt; 
 &lt;li&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:yanyk.thu@gmail.com"&gt;yanyk.thu@gmail.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/wechat_qr.png" alt="WeChat Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;WeChat Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/feishu_qr.png" alt="Feishu Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;Feishu Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://discord.gg/yRFFjjJnnS"&gt; &lt;img src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt; &lt;/a&gt;&lt;br /&gt; &lt;b&gt;Discord&lt;/b&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;üåê Make websites accessible for AI agents. Automate tasks online with ease.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" " /&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/774a46d5-27a0-490c-b7d0-e65fcbbfa358" /&gt; 
 &lt;img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" width="full" /&gt; 
&lt;/picture&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" " /&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/6797d09b-8ac3-4cb9-ba07-b289e080765a" /&gt; 
  &lt;img alt="The AI browser agent." src="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" width="400" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/package" height="48" alt="Browser-Use Package Download Statistics" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/#demos"&gt;&lt;img src="https://media.browser-use.tools/badges/demos" alt="Demos" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://docs.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/docs" alt="Docs" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browser-use.com/posts"&gt;&lt;img src="https://media.browser-use.tools/badges/blog" alt="Blog" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browsermerch.com"&gt;&lt;img src="https://media.browser-use.tools/badges/merch" alt="Merch" /&gt;&lt;/a&gt; 
 &lt;img width="100" height="1" alt="" /&gt; 
 &lt;a href="https://github.com/browser-use/browser-use"&gt;&lt;img src="https://media.browser-use.tools/badges/github" alt="Github Stars" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://x.com/intent/user?screen_name=browser_use"&gt;&lt;img src="https://media.browser-use.tools/badges/twitter" alt="Twitter" /&gt;&lt;/a&gt; 
 &lt;img width="4 height=" 1" alt="" /&gt; 
 &lt;a href="https://link.browser-use.com/discord"&gt;&lt;img src="https://media.browser-use.tools/badges/discord" alt="Discord" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/cloud" height="48" alt="Browser-Use Cloud" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;üå§Ô∏è Want to skip the setup? Use our &lt;b&gt;&lt;a href="https://cloud.browser-use.com"&gt;cloud&lt;/a&gt;&lt;/b&gt; for faster, scalable, stealth-enabled browser automation!&lt;/p&gt; 
&lt;h1&gt;ü§ñ LLM Quickstart&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;Direct your favorite coding agent (Cursor, Claude Code, etc) to &lt;a href="https://docs.browser-use.com/llms-full.txt"&gt;Agents.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Prompt away!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;h1&gt;üëã Human Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;1. Create environment with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; (Python&amp;gt;=3.11):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. Install Browser-Use package:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#  We ship every day - use the latest version!
uv add browser-use
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Get your API key from &lt;a href="https://cloud.browser-use.com/new-api-key"&gt;Browser Use Cloud&lt;/a&gt; and add it to your &lt;code&gt;.env&lt;/code&gt; file (new signups get $10 free credits):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# .env
BROWSER_USE_API_KEY=your-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. Install Chromium browser:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. Run your first agent:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Agent, Browser, ChatBrowserUse
import asyncio

async def example():
    browser = Browser(
        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud
    )

    llm = ChatBrowserUse()

    agent = Agent(
        task="Find the number of stars of the browser-use repo",
        llm=llm,
        browser=browser,
    )

    history = await agent.run()
    return history

if __name__ == "__main__":
    history = asyncio.run(example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://docs.browser-use.com"&gt;library docs&lt;/a&gt; and the &lt;a href="https://docs.cloud.browser-use.com"&gt;cloud docs&lt;/a&gt; for more!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üî• Deploy on Sandboxes&lt;/h1&gt; 
&lt;p&gt;We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Browser, sandbox, ChatBrowserUse
from browser_use.agent.service import Agent
import asyncio

@sandbox()
async def my_task(browser: Browser):
    agent = Agent(task="Find the top HN post", browser=browser, llm=ChatBrowserUse())
    await agent.run()

# Just call it like any async function
asyncio.run(my_task())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://docs.browser-use.com/production"&gt;Going to Production&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üöÄ Template Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Want to get started even faster?&lt;/strong&gt; Generate a ready-to-run template:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;browser_use_default.py&lt;/code&gt; file with a working example. Available templates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;default&lt;/code&gt; - Minimal setup to get started quickly&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;advanced&lt;/code&gt; - All configuration options with detailed comments&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tools&lt;/code&gt; - Examples of custom tools and extending the agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also specify a custom output path:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default --output my_agent.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;üíª CLI&lt;/h1&gt; 
&lt;p&gt;Fast, persistent browser automation from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;browser-use open https://example.com    # Navigate to URL
browser-use state                       # See clickable elements
browser-use click 5                     # Click element by index
browser-use type "Hello"                # Type text
browser-use screenshot page.png         # Take screenshot
browser-use close                       # Close browser
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The CLI keeps the browser running between commands for fast iteration. See &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/browser_use/skill_cli/README.md"&gt;CLI docs&lt;/a&gt; for all commands.&lt;/p&gt; 
&lt;h3&gt;Claude Code Skill&lt;/h3&gt; 
&lt;p&gt;For &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;, install the skill to enable AI-assisted browser automation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/.claude/skills/browser-use
curl -o ~/.claude/skills/browser-use/SKILL.md \
  https://raw.githubusercontent.com/browser-use/browser-use/main/skills/browser-use/SKILL.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;h3&gt;üìã Form-Filling&lt;/h3&gt; 
&lt;h4&gt;Task = "Fill in this job application with my resume and information."&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/57865ee6-6004-49d5-b2c2-6dff39ec2ba9" alt="Job Application Demo" /&gt; &lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/apply_to_job.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üçé Grocery-Shopping&lt;/h3&gt; 
&lt;h4&gt;Task = "Put this list of items into my instacart."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850"&gt;https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/buy_groceries.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üíª Personal-Assistant.&lt;/h3&gt; 
&lt;h4&gt;Task = "Help me find parts for a custom PC."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06"&gt;https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/pcpartpicker.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üí°See &lt;a href="https://docs.browser-use.com/examples"&gt;more examples here ‚Üó&lt;/a&gt; and give us a star!&lt;/h3&gt; 
&lt;br /&gt; 
&lt;h2&gt;Integrations, hosting, custom tools, MCP, and more on our &lt;a href="https://docs.browser-use.com"&gt;Docs ‚Üó&lt;/a&gt;&lt;/h2&gt; 
&lt;br /&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;What's the best model to use?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;We optimized &lt;strong&gt;ChatBrowserUse()&lt;/strong&gt; specifically for browser automation tasks. On avg it completes tasks 3-5x faster than other models with SOTA accuracy.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Pricing (per 1M tokens):&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Input tokens: $0.20&lt;/li&gt; 
  &lt;li&gt;Cached input tokens: $0.02&lt;/li&gt; 
  &lt;li&gt;Output tokens: $2.00&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For other LLM providers, see our &lt;a href="https://docs.browser-use.com/supported-models"&gt;supported models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use custom tools with the agent?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! You can add custom tools to extend the agent's capabilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Tools

tools = Tools()

@tools.action(description='Description of what this tool does.')
def custom_tool(param: str) -&amp;gt; str:
    return f"Result: {param}"

agent = Agent(
    task="Your task",
    llm=llm,
    browser=browser,
    tools=tools,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use this for free?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! Browser-Use is open source and free to use. You only need to choose an LLM provider (like OpenAI, Google, ChatBrowserUse, or run local models with Ollama).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I handle authentication?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Check out our authentication examples:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py"&gt;Using real browser profiles&lt;/a&gt; - Reuse your existing Chrome profile with saved logins&lt;/li&gt; 
  &lt;li&gt;If you want to use temporary accounts with inbox, choose AgentMail&lt;/li&gt; 
  &lt;li&gt;To sync your auth profile with the remote browser, run &lt;code&gt;curl -fsSL https://browser-use.com/profile.sh | BROWSER_USE_API_KEY=XXXX sh&lt;/code&gt; (replace XXXX with your API key)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These examples show how to maintain sessions and handle authentication seamlessly.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I solve CAPTCHAs?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;For CAPTCHA handling, you need better browser fingerprinting and proxies. Use &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud&lt;/a&gt; which provides stealth browsers designed to avoid detection and CAPTCHA challenges.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I go into production?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Chrome can consume a lot of memory, and running many agents in parallel can be tricky to manage.&lt;/p&gt; 
 &lt;p&gt;For production use cases, use our &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud API&lt;/a&gt; which handles:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Scalable browser infrastructure&lt;/li&gt; 
  &lt;li&gt;Memory management&lt;/li&gt; 
  &lt;li&gt;Proxy rotation&lt;/li&gt; 
  &lt;li&gt;Stealth browser fingerprinting&lt;/li&gt; 
  &lt;li&gt;High-performance parallel execution&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Tell your computer what to do, and it gets it done.&lt;/strong&gt;&lt;/p&gt; 
 &lt;img src="https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f" width="400" /&gt; 
 &lt;p&gt;&lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; ‚ÄÉ‚ÄÉ‚ÄÉ &lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  Made with ‚ù§Ô∏è in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from &lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; , &lt;img src="https://cdn.simpleicons.org/anthropic" alt="anthropic logo" width="25" height="15" /&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/googlegemini" alt="google logo" width="25" height="18" /&gt;&lt;strong&gt;Google&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/x" alt="X logo" width="25" height="15" /&gt;&lt;strong&gt;xAI&lt;/strong&gt; and open-source models like &lt;img src="https://cdn.simpleicons.org/alibabacloud" alt="alibaba logo" width="25" height="15" /&gt;&lt;strong&gt;Qwen&lt;/strong&gt; or &lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt;&lt;strong&gt;Llama&lt;/strong&gt; that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Thanks to our sponsors&lt;/h2&gt; 
&lt;table align="center" cellpadding="16" cellspacing="12"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" title="Tiger Data"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/tigerdata.png" alt="Tiger Data" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Tiger Data MCP &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" title="Speechmatics"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/speechmatics.png" alt="Speechmatics" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Speechmatics &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" title="Okara"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/okara.png" alt="Okara" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Okara AI &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sponsorunwindai.com/" title="Become a Sponsor"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsor_awesome_llm_apps.png" alt="Become a Sponsor" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sponsorunwindai.com/" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Become a Sponsor &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scraping AI Agent (Local &amp;amp; Cloud SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent"&gt;üèöÔ∏è üçå AI Home Renovation Agent with Nano Banana Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team"&gt;üìä AI VC Due Diligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api"&gt;üî¨ AI Research Planner &amp;amp; Executor (Google Interactions API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team"&gt;üë®üèª‚Äçüíº AI Sales Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/accomplish-ai/openwork"&gt;üåê Openwork - Open Browser Automation Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/"&gt;üé® üçå Multimodal UI/UX Feedback Agent Team with Nano Banana&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akshayaggarwal99/jarvis-ai-assistant"&gt;üéôÔ∏è OpenSource Voice Dictation Agent (like Wispr Flow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/modelcontextprotocol" alt="mcp logo" width="25" height="20" /&gt; MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_embedding_gemma"&gt;üî• Agentic RAG with Embedding Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/contextualai_rag_agent/"&gt;üîÑ Contextual AI RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéØ LLM Optimization Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/"&gt;üéØ Toonify Token Optimization&lt;/a&gt; - Reduce LLM API costs by 30-60% using TOON format&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/headroom_context_optimization/"&gt;üß† Headroom Context Optimization&lt;/a&gt; - Reduce LLM API costs by 50-90% through intelligent context compression for AI agents (includes persistent memory &amp;amp; MCP support)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="20" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/"&gt;Gemma 3 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üßë‚Äçüè´ AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; model‚Äëagnostic (OpenAI, Claude)&lt;/li&gt; 
 &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty, MCP tools&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
 &lt;li&gt;Simple multi‚Äëagent; Multi‚Äëagent patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/openai_sdk_crash_course/"&gt;OpenAI Agents SDK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; function calling; structured outputs&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty integrations&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; evaluation&lt;/li&gt; 
 &lt;li&gt;Multi‚Äëagent patterns; agent handoffs&lt;/li&gt; 
 &lt;li&gt;Swarm orchestration; routing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/github" alt="github logo" width="25" height="20" /&gt; Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/skills</title>
      <link>https://github.com/anthropics/skills</link>
      <description>&lt;p&gt;Public repository for Agent Skills&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This repository contains Anthropic's implementation of skills for Claude. For information about the Agent Skills standard, see &lt;a href="http://agentskills.io"&gt;agentskills.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Skills&lt;/h1&gt; 
&lt;p&gt;Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that's creating documents with your company's brand guidelines, analyzing data using your organization's specific workflows, or automating personal tasks.&lt;/p&gt; 
&lt;p&gt;For more information, check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512176-what-are-skills"&gt;What are skills?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude"&gt;Using skills in Claude&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills"&gt;Equipping agents for the real world with Agent Skills&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;About This Repository&lt;/h1&gt; 
&lt;p&gt;This repository contains skills that demonstrate what's possible with Claude's skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).&lt;/p&gt; 
&lt;p&gt;Each skill is self-contained in its own folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.&lt;/p&gt; 
&lt;p&gt;Many skills in this repo are open source (Apache 2.0). We've also included the document creation &amp;amp; editing skills that power &lt;a href="https://www.anthropic.com/news/create-files"&gt;Claude's document capabilities&lt;/a&gt; under the hood in the &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/docx"&gt;&lt;code&gt;skills/docx&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pdf"&gt;&lt;code&gt;skills/pdf&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pptx"&gt;&lt;code&gt;skills/pptx&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/xlsx"&gt;&lt;code&gt;skills/xlsx&lt;/code&gt;&lt;/a&gt; subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;These skills are provided for demonstration and educational purposes only.&lt;/strong&gt; While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.&lt;/p&gt; 
&lt;h1&gt;Skill Sets&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills"&gt;./skills&lt;/a&gt;: Skill examples for Creative &amp;amp; Design, Development &amp;amp; Technical, Enterprise &amp;amp; Communication, and Document Skills&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/spec"&gt;./spec&lt;/a&gt;: The Agent Skills specification&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/template"&gt;./template&lt;/a&gt;: Skill template&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Try in Claude Code, Claude.ai, and the API&lt;/h1&gt; 
&lt;h2&gt;Claude Code&lt;/h2&gt; 
&lt;p&gt;You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin marketplace add anthropics/skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, to install a specific set of skills:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Select &lt;code&gt;Browse and install plugins&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;anthropic-agent-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;document-skills&lt;/code&gt; or &lt;code&gt;example-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;Install now&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Alternatively, directly install either Plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the &lt;code&gt;document-skills&lt;/code&gt; plugin from the marketplace, you can ask Claude Code to do something like: "Use the PDF skill to extract the form fields from &lt;code&gt;path/to/some-file.pdf&lt;/code&gt;"&lt;/p&gt; 
&lt;h2&gt;Claude.ai&lt;/h2&gt; 
&lt;p&gt;These example skills are all already available to paid plans in Claude.ai.&lt;/p&gt; 
&lt;p&gt;To use any skill from this repository or upload custom skills, follow the instructions in &lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b"&gt;Using skills in Claude&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Claude API&lt;/h2&gt; 
&lt;p&gt;You can use Anthropic's pre-built skills, and upload custom skills, via the Claude API. See the &lt;a href="https://docs.claude.com/en/api/skills-guide#creating-a-skill"&gt;Skills API Quickstart&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h1&gt;Creating a Basic Skill&lt;/h1&gt; 
&lt;p&gt;Skills are simple to create - just a folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing YAML frontmatter and instructions. You can use the &lt;strong&gt;template-skill&lt;/strong&gt; in this repository as a starting point:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontmatter requires only two fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt; - A unique identifier for your skill (lowercase, hyphens for spaces)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;description&lt;/code&gt; - A complete description of what the skill does and when to use it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see &lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Partner Skills&lt;/h1&gt; 
&lt;p&gt;Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Notion&lt;/strong&gt; - &lt;a href="https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0"&gt;Notion Skills for Claude&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>MoonshotAI/kimi-cli</title>
      <link>https://github.com/MoonshotAI/kimi-cli</link>
      <description>&lt;p&gt;Kimi Code CLI is your next CLI agent.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kimi Code CLI&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli" alt="Commit Activity" /&gt;&lt;/a&gt; &lt;a href="https://github.com/MoonshotAI/kimi-cli/actions"&gt;&lt;img src="https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main" alt="Checks" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/kimi-cli/"&gt;&lt;img src="https://img.shields.io/pypi/v/kimi-cli" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/kimi-cli"&gt;&lt;img src="https://img.shields.io/pypi/dw/kimi-cli" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/MoonshotAI/kimi-cli"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.kimi.com/code/"&gt;Kimi Code&lt;/a&gt; | &lt;a href="https://moonshotai.github.io/kimi-cli/en/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://moonshotai.github.io/kimi-cli/zh/"&gt;ÊñáÊ°£&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Kimi Code CLI is an AI agent that runs in the terminal, helping you complete software development tasks and terminal operations. It can read and edit code, execute shell commands, search and fetch web pages, and autonomously plan and adjust actions during execution.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://moonshotai.github.io/kimi-cli/en/guides/getting-started.html"&gt;Getting Started&lt;/a&gt; for how to install and start using Kimi Code CLI.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;Shell command mode&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI is not only a coding agent, but also a shell. You can switch the shell command mode by pressing &lt;code&gt;Ctrl-X&lt;/code&gt;. In this mode, you can directly run shell commands without leaving Kimi Code CLI.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/docs/media/shell-mode.gif" alt="" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Built-in shell commands like &lt;code&gt;cd&lt;/code&gt; are not supported yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;IDE integration via ACP&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI supports &lt;a href="https://github.com/agentclientprotocol/agent-client-protocol"&gt;Agent Client Protocol&lt;/a&gt; out of the box. You can use it together with any ACP-compatible editor or IDE.&lt;/p&gt; 
&lt;p&gt;To use Kimi Code CLI with ACP clients, make sure to run Kimi Code CLI in the terminal and send &lt;code&gt;/login&lt;/code&gt; to complete the login first. Then, you can configure your ACP client to start Kimi Code CLI as an ACP agent server with command &lt;code&gt;kimi acp&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, to use Kimi Code CLI with &lt;a href="https://zed.dev/"&gt;Zed&lt;/a&gt; or &lt;a href="https://blog.jetbrains.com/ai/2025/12/bring-your-own-ai-agent-to-jetbrains-ides/"&gt;JetBrains&lt;/a&gt;, add the following configuration to your &lt;code&gt;~/.config/zed/settings.json&lt;/code&gt; or &lt;code&gt;~/.jetbrains/acp.json&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "agent_servers": {
    "Kimi Code CLI": {
      "command": "kimi",
      "args": ["acp"],
      "env": {}
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can create Kimi Code CLI threads in IDE's agent panel.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/docs/media/acp-integration.gif" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;Zsh integration&lt;/h3&gt; 
&lt;p&gt;You can use Kimi Code CLI together with Zsh, to empower your shell experience with AI agent capabilities.&lt;/p&gt; 
&lt;p&gt;Install the &lt;a href="https://github.com/MoonshotAI/zsh-kimi-cli"&gt;zsh-kimi-cli&lt;/a&gt; plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/MoonshotAI/zsh-kimi-cli.git \
  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin's README for installation instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Then add &lt;code&gt;kimi-cli&lt;/code&gt; to your Zsh plugin list in &lt;code&gt;~/.zshrc&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;plugins=(... kimi-cli)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After restarting Zsh, you can switch to agent mode by pressing &lt;code&gt;Ctrl-X&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;MCP support&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI supports MCP (Model Context Protocol) tools.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;code&gt;kimi mcp&lt;/code&gt; sub-command group&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can manage MCP servers with &lt;code&gt;kimi mcp&lt;/code&gt; sub-command group. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Add streamable HTTP server:
kimi mcp add --transport http context7 https://mcp.context7.com/mcp --header "CONTEXT7_API_KEY: ctx7sk-your-key"

# Add streamable HTTP server with OAuth authorization:
kimi mcp add --transport http --auth oauth linear https://mcp.linear.app/mcp

# Add stdio server:
kimi mcp add --transport stdio chrome-devtools -- npx chrome-devtools-mcp@latest

# List added MCP servers:
kimi mcp list

# Remove an MCP server:
kimi mcp remove chrome-devtools

# Authorize an MCP server:
kimi mcp auth linear
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Ad-hoc MCP configuration&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Kimi Code CLI also supports ad-hoc MCP server configuration via CLI option.&lt;/p&gt; 
&lt;p&gt;Given an MCP config file in the well-known MCP config format like the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "context7": {
      "url": "https://mcp.context7.com/mcp",
      "headers": {
        "CONTEXT7_API_KEY": "YOUR_API_KEY"
      }
    },
    "chrome-devtools": {
      "command": "npx",
      "args": ["-y", "chrome-devtools-mcp@latest"]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run &lt;code&gt;kimi&lt;/code&gt; with &lt;code&gt;--mcp-config-file&lt;/code&gt; option to connect to the specified MCP servers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;kimi --mcp-config-file /path/to/mcp.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;More&lt;/h3&gt; 
&lt;p&gt;See more features in the &lt;a href="https://moonshotai.github.io/kimi-cli/en/"&gt;Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To develop Kimi Code CLI, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/MoonshotAI/kimi-cli.git
cd kimi-cli

make prepare  # prepare the development environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can start working on Kimi Code CLI.&lt;/p&gt; 
&lt;p&gt;Refer to the following commands after you make changes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run kimi  # run Kimi Code CLI

make format  # format code
make check  # run linting and type checking
make test  # run tests
make test-kimi-cli  # run Kimi Code CLI tests only
make test-kosong  # run kosong tests only
make test-pykaos  # run pykaos tests only
make build-web  # build the web UI and sync it into the package (requires Node.js/npm)
make build  # build python packages
make build-bin  # build standalone binary
make help  # show all make targets
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: &lt;code&gt;make build&lt;/code&gt; and &lt;code&gt;make build-bin&lt;/code&gt; automatically run &lt;code&gt;make build-web&lt;/code&gt; to embed the web UI.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to Kimi Code CLI! Please refer to &lt;a href="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>KellerJordan/modded-nanogpt</title>
      <link>https://github.com/KellerJordan/modded-nanogpt</link>
      <description>&lt;p&gt;NanoGPT (124M) in 2 minutes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Modded-NanoGPT&lt;/h1&gt; 
&lt;p&gt;This repository hosts the &lt;em&gt;NanoGPT speedrun&lt;/em&gt;, in which we (collaboratively|competitively) search for the fastest algorithm to use 8 NVIDIA H100 GPUs to train a language model that attains 3.28 cross-entropy loss on the &lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb"&gt;FineWeb&lt;/a&gt; validation set.&lt;/p&gt; 
&lt;p&gt;The target (3.28 validation loss on FineWeb) follows Andrej Karpathy's &lt;a href="https://github.com/karpathy/llm.c/discussions/481#:~:text=By%20the%20end%20of%20the%20optimization%20we%27ll%20get%20to%20about%203.29"&gt;GPT-2 replication in llm.c, which attains that loss after running for 45 minutes&lt;/a&gt;. The speedrun code also descends from llm.c's &lt;a href="https://github.com/karpathy/llm.c/raw/master/train_gpt2.py"&gt;PyTorch trainer&lt;/a&gt;, which itself descends from NanoGPT, hence the name of the repo. Thanks to the efforts of many contributors, this repo now contains a training algorithm which attains the target performance in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Under 100 seconds on 8xH100 (the llm.c GPT-2 replication needed 45 minutes)&lt;/li&gt; 
 &lt;li&gt;under 500M tokens (the llm.c GPT-2 replication needed 10B)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This improvement in training speed has been brought about by the following techniques:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modernized architecture: Rotary embeddings, QK-Norm, and ReLU¬≤&lt;/li&gt; 
 &lt;li&gt;The Muon optimizer [&lt;a href="https://kellerjordan.github.io/posts/muon/"&gt;writeup&lt;/a&gt;] [&lt;a href="https://github.com/KellerJordan/Muon"&gt;repo&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;Use FP8 matmul for head, and asymmetric rescale and softcap logits&lt;/li&gt; 
 &lt;li&gt;Initialization of projections to zero (muP-like)&lt;/li&gt; 
 &lt;li&gt;Skip connections from embedding to every block as well as from block 3 to 6&lt;/li&gt; 
 &lt;li&gt;Extra embeddings which are mixed into the values in attention layers (inspired by Zhou et al. 2024)&lt;/li&gt; 
 &lt;li&gt;Flash Attention 3 with long-short sliding window attention pattern (inspired by Gemma 2) and window size warmup with YaRN&lt;/li&gt; 
 &lt;li&gt;Align training batch starts with EoS and set a max document length&lt;/li&gt; 
 &lt;li&gt;Accumulate gradients for 2 steps for embedding and lm_head before updating parameters&lt;/li&gt; 
 &lt;li&gt;Enable model to back out contributions from first 2/3 layers before prediction&lt;/li&gt; 
 &lt;li&gt;Polar Express implementation in Muon&lt;/li&gt; 
 &lt;li&gt;Smear module to enable 1 token look back&lt;/li&gt; 
 &lt;li&gt;Sparse attention gate&lt;/li&gt; 
 &lt;li&gt;NorMuon&lt;/li&gt; 
 &lt;li&gt;Cautious Weight Decay w/ schedule tied to LR&lt;/li&gt; 
 &lt;li&gt;Exponential decay of residual stream&lt;/li&gt; 
 &lt;li&gt;Batch size schedule&lt;/li&gt; 
 &lt;li&gt;Partial Key Offset&lt;/li&gt; 
 &lt;li&gt;Multi token prediction&lt;/li&gt; 
 &lt;li&gt;Untie embed and lm_head at 2/3 of training&lt;/li&gt; 
 &lt;li&gt;Additional gating on value embeddings and skip connection&lt;/li&gt; 
 &lt;li&gt;Paired head attention&lt;/li&gt; 
 &lt;li&gt;Bigram hash embedding&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As well as many systems optimizations.&lt;/p&gt; 
&lt;p&gt;Contributors list (growing with each new record): &lt;a href="https://x.com/bozavlado"&gt;@bozavlado&lt;/a&gt;; &lt;a href="https://x.com/brendanh0gan"&gt;@brendanh0gan&lt;/a&gt;; &lt;a href="https://bsky.app/profile/fernbear.bsky.social"&gt;@fernbear.bsky.social&lt;/a&gt;; &lt;a href="https://x.com/Grad62304977"&gt;@Grad62304977&lt;/a&gt;; &lt;a href="https://x.com/jxbz"&gt;@jxbz&lt;/a&gt;; &lt;a href="https://x.com/kellerjordan0"&gt;@kellerjordan0&lt;/a&gt;; &lt;a href="https://x.com/KoszarskyB"&gt;@KoszarskyB&lt;/a&gt;; &lt;a href="https://x.com/@leloykun"&gt;@leloykun&lt;/a&gt;; &lt;a href="https://x.com/YouJiacheng"&gt;@YouJiacheng&lt;/a&gt;; &lt;a href="https://x.com/jadenj3o"&gt;@jadenj3o&lt;/a&gt;; &lt;a href="https://github.com/KonstantinWilleke"&gt;@KonstantinWilleke&lt;/a&gt;, &lt;a href="https://github.com/alexrgilbert"&gt;@alexrgilbert&lt;/a&gt;, &lt;a href="https://github.com/adricarda"&gt;@adricarda&lt;/a&gt;, &lt;a href="https://github.com/tuttyfrutyee"&gt;@tuttyfrutyee&lt;/a&gt;, &lt;a href="https://github.com/vdlad"&gt;@vdlad&lt;/a&gt;; &lt;a href="https://x.com/ryanyang0"&gt;@ryanyang0&lt;/a&gt;, &lt;a href="https://github.com/vagrawal"&gt;@vagrawal&lt;/a&gt;, &lt;a href="https://x.com/classiclarryd"&gt;@classiclarryd&lt;/a&gt;, &lt;a href="https://github.com/byronxu99"&gt;@byronxu99&lt;/a&gt;, &lt;a href="https://x.com/varunneal"&gt;@varunneal&lt;/a&gt;, &lt;a href="https://github.com/EmelyanenkoK"&gt;@EmelyanenkoK&lt;/a&gt;, &lt;a href="https://github.com/bernard24"&gt;@bernard24&lt;/a&gt;/&lt;a href="https://www.hiverge.ai/"&gt;https://www.hiverge.ai/&lt;/a&gt;, &lt;a href="https://x.com/Gusarich"&gt;@Gusarich&lt;/a&gt;, &lt;a href="https://x.com/li_zichong"&gt;@li_zichong&lt;/a&gt;, &lt;a href="https://github.com/akash5474"&gt;@akash5474&lt;/a&gt;, &lt;a href="https://x.com/omouamoua"&gt;@snimu&lt;/a&gt;, &lt;a href="https://x.com/roeeshenberg"&gt;@roeeshenberg&lt;/a&gt;, &lt;a href="https://x.com/ChrisJMcCormick"&gt;@ChrisJMcCormick&lt;/a&gt;, &lt;a href="https://github.com/dominikkallusky"&gt;@dominikkallusky&lt;/a&gt;, &lt;a href="https://github.com/acutkosky"&gt;@acutkosky&lt;/a&gt;, &lt;a href="https://github.com/manikbhandari"&gt;@manikbhandari&lt;/a&gt;, &lt;a href="https://github.com/andrewbriand"&gt;@andrewbriand&lt;/a&gt;, &lt;a href="https://github.com/jrauvola"&gt;@jrauvola&lt;/a&gt;, &lt;a href="https://x.com/soren_dunn_"&gt;@soren_dunn_&lt;/a&gt;, &lt;a href="https://x.com/photon_mz"&gt;@photon_mz&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running the current record&lt;/h2&gt; 
&lt;p&gt;To run the current record, run the following commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;amp;&amp;amp; cd modded-nanogpt
pip install -r requirements.txt
pip install torch==2.10.0.dev20251210+cu126 --index-url https://download.pytorch.org/whl/nightly/cu126
# downloads only the first 900M training tokens to save time
python data/cached_fineweb10B.py 9
./run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add torchrun to path if ./run.sh gives error &lt;code&gt;torchrun: command not found&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note: torch.compile will add around 7 minutes of latency the first time you run the code.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Official records are timed on 8 NVIDIA H100 GPUs from &lt;a href="https://app.primeintellect.ai/"&gt;https://app.primeintellect.ai/&lt;/a&gt;. PrimeIntellect has generously sponsored recent validation runs.&lt;/p&gt; 
&lt;h2&gt;Alternative: Running with Docker (recommended for precise timing)&lt;/h2&gt; 
&lt;p&gt;For cases where CUDA or NCCL versions aren't compatible with your current system setup, Docker can be a helpful alternative. This approach standardizes versions for CUDA, NCCL, CUDNN, and Python, reducing dependency issues and simplifying setup. Note: an NVIDIA driver must already be installed on the system (useful if only the NVIDIA driver and Docker are available).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;amp;&amp;amp; cd modded-nanogpt
sudo docker build -t modded-nanogpt .
sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt python data/cached_fineweb10B.py 8
sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt sh run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get an interactive docker, you can use&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;World record history&lt;/h2&gt; 
&lt;p&gt;The following is the historical progression of world speed records for the following competitive task:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Train a neural network to ‚â§3.28 validation loss on FineWeb using 8x NVIDIA H100s.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Note: The 3.28 target was selected to match &lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;Andrej Karpathy's GPT-2 (small) reproduction&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Record time&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Log&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;45 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;llm.c baseline&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/28/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-13_llmc/main.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@karpathy, llm.c contributors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;31.4 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1798863559243513937"&gt;Tuned learning rate &amp;amp; rotary embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;06/06/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-06-06_AdamW/f66d43d7-e449-4029-8adf-e8537bab49ea.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;24.9 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1842300916864844014"&gt;Introduced the Muon optimizer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/04/24&lt;/td&gt; 
   &lt;td&gt;none&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0, @jxbz&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;22.3 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1844820919061287009"&gt;Muon improvements&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/11/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-10_Muon/eb5659d0-fb6a-49e5-a311-f1f89412f726.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0, @bozavlado&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;15.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1845865698532450646"&gt;Pad embeddings, ReLU¬≤, zero-init projections, QK-norm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/14/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;13.1 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1847291684016783746"&gt;Distributed the overhead of Muon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/18/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-17_DistributedMuon/22d24867-eb5a-4fcc-ae2c-263d0277dfd1.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;12.0 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1847358578686152764"&gt;Upgraded PyTorch 2.5.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/18/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-18_PyTorch25/d4bfb25f-688d-4da5-8743-33926fad4842.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;10.8 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1853188916704387239"&gt;Untied embedding and head&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/03/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-03_UntieEmbed/d6b50d71-f419-4d26-bb39-a60d55ae7a04.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;8.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1854296101303800108"&gt;Value and embedding skip connections, momentum warmup, logit softcap&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/06/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-06_ShortcutsTweaks/dd7304a6-cc43-4d5e-adb8-c070111464a1.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;7.8 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1855267054774865980"&gt;Bfloat16 activations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/08/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-08_CastBf16/a833bed8-2fa8-4cfe-af05-58c1cc48bc30.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;7.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1856053121103093922"&gt;U-net pattern skip connections &amp;amp; double lr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/10/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-10_UNetDoubleLr/c87bb826-797b-4f37-98c7-d3a5dad2de74.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@brendanh0gan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;5.03 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1859331370268623321"&gt;1024-ctx dense causal attention ‚Üí 64K-ctx FlexAttention&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/19/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-19_FlexAttention/8384493d-dba9-4991-b16b-8696953f5e6d.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;4.66 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/hi_tysam/status/1860851011797053450"&gt;Attention window warmup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/24/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-24_WindowWarmup/cf9e4571-c5fc-4323-abf3-a98d862ec6c8.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@fernbear.bsky.social&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;4.41 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/KoszarskyB/status/1864746625572257852"&gt;Value Embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/04/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-04_ValueEmbed"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;3.95 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1865761473886347747"&gt;U-net pattern value embeddings, assorted code optimizations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/08/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-08_UNetValueEmbedsTweaks"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun, @YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;3.80 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1866734331559071981"&gt;Split value embeddings, block sliding window, separate block mask&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/10/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-10_MFUTweaks"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;3.57 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1868938024731787640"&gt;Sparsify value embeddings, improve rotary embeddings, drop an attn layer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/17/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-17_SparsifyEmbeds"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;3.4 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1876048851158880624"&gt;Lower logit softcap from 30 to 15&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap/31d6c427-f1f7-4d8a-91be-a67b5dcd13fd.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;3.142 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1878827972519772241"&gt;FP8 head, offset logits, lr decay to 0.1 instead of 0.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-13_Fp8LmHead/c51969c2-d04c-40a7-bcea-c092c3c2d11a.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;2.992 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1880301753213809016"&gt;Merged QKV weights, long-short attention, attention scale, lower Adam epsilon, batched Muon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-16_Sub3Min/1d3bd93b-a69e-4118-aeb8-8184239d7566.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun, @fernbear.bsky.social, @YouJiacheng, @brendanh0gan, @scottjmaddox, @Grad62304977&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;2.933 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1885640350368420160"&gt;Reduced batch size&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/26/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-26_BatchSize/c44090cc-1b99-4c95-8624-38fb4b5834f9.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;2.997 minutes&lt;/td&gt; 
   &lt;td&gt;21st record with new timing&lt;/td&gt; 
   &lt;td&gt;02/01/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-02-01_RuleTweak/eff63a8c-2f7e-4fc5-97ce-7f600dae0bc7.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;not a new record, just re-timing #21 with the &lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/#timing-change-after-record-21"&gt;updated rules&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;3.014 minutes&lt;/td&gt; 
   &lt;td&gt;21st record with latest torch&lt;/td&gt; 
   &lt;td&gt;05/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-24_StableTorch/89d9f224-3b01-4581-966e-358d692335e0.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;not a new record, just re-timing #21 with latest torch&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;2.990 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/KonstantinWille/status/1927137223238909969"&gt;Faster gradient all-reduce&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-24_FasterReduce/23f40b75-06fb-4c3f-87a8-743524769a35.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad; The Enigma project&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;2.979 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1927460573098262616"&gt;Overlap computation and gradient communication&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/25/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-25_EvenFasterReduce/6ae86d05-5cb2-4e40-a512-63246fd08e45.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ryanyang0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;2.966 minutes&lt;/td&gt; 
   &lt;td&gt;Replace gradient all_reduce with reduce_scatter&lt;/td&gt; 
   &lt;td&gt;05/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-30_noallreduce/8054c239-3a18-499e-b0c8-dbd27cb4b3ab.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@vagrawal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;2.896 minutes&lt;/td&gt; 
   &lt;td&gt;Upgrade PyTorch to 2.9.0.dev20250713+cu126&lt;/td&gt; 
   &lt;td&gt;07/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-13_UpgradeTorch190/692f80e0-5e64-4819-97d4-0dc83b7106b9.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;2.863 minutes&lt;/td&gt; 
   &lt;td&gt;Align training batch starts with EoS, increase cooldown frac to .45&lt;/td&gt; 
   &lt;td&gt;07/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-12_BosAlign/c1fd8a38-bb9f-45c4-8af0-d37f70c993f3.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;2.817 minutes&lt;/td&gt; 
   &lt;td&gt;Transpose one of the MLP matrices + add Triton kernel for symmetric matmul&lt;/td&gt; 
   &lt;td&gt;07/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-18_TritonMuon/record.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/109"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@byronxu99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;2.812 minutes&lt;/td&gt; 
   &lt;td&gt;Sparse attention gate&lt;/td&gt; 
   &lt;td&gt;08/23/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-08-23_SparseAttnGate/020630eb-2191-4ba2-9ee4-4cdc94316943.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/117"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;2.731 minutes&lt;/td&gt; 
   &lt;td&gt;Flash Attention 3, 2048 max_doc_len, update ws schedule&lt;/td&gt; 
   &lt;td&gt;09/03/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-03_FA3/44fc1276-0510-4961-92c0-730c65e5feba.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/118"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td&gt;2.717 minutes&lt;/td&gt; 
   &lt;td&gt;Drop first MLP layer&lt;/td&gt; 
   &lt;td&gt;09/05/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-05_SkipMLPBlocks/07e7ae76-b7d0-4481-b149-01e7d81b5ad4.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/120"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@EmelyanenkoK&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;31&lt;/td&gt; 
   &lt;td&gt;2.656 minutes&lt;/td&gt; 
   &lt;td&gt;Dynamically incorporate YaRN during training and validation&lt;/td&gt; 
   &lt;td&gt;09/10/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-10_Yarn/0ecdb695-510b-4c3b-b030-09861a162ce8.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/122"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;2.625 minutes&lt;/td&gt; 
   &lt;td&gt;Optimize distributed training, improve skip connection gating, and enhance bfloat16 usage&lt;/td&gt; 
   &lt;td&gt;09/11/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-11_VectSigmoidBFloat16/0d0d9882-c34f-4d82-b961-a17d5659c988.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/125"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@bernard24 &amp;amp; AI system &lt;a href="https://www.hiverge.ai/"&gt;hiverge.ai&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;33&lt;/td&gt; 
   &lt;td&gt;2.565 minutes&lt;/td&gt; 
   &lt;td&gt;Asynchronously fetch and index data batches, extend final layer attention window for validation&lt;/td&gt; 
   &lt;td&gt;09/15/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-15_AsyncDataLoadAttnFinalWindow/25db37c7-2bab-4ef4-ae63-d593590ef823.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/127"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;34&lt;/td&gt; 
   &lt;td&gt;2.547 minutes&lt;/td&gt; 
   &lt;td&gt;Smear token embeddings 1 position forward&lt;/td&gt; 
   &lt;td&gt;09/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-18_Smear/18a1e5c7-947e-479d-bc3a-a57a61a98fc9.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/130"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;35&lt;/td&gt; 
   &lt;td&gt;2.527 minutes&lt;/td&gt; 
   &lt;td&gt;Drop first attn layer, extend all long windows for validation, update schedule&lt;/td&gt; 
   &lt;td&gt;09/21/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-21_DropAttn/01fc4a96-f2a0-47a1-8a6a-c7d10bac99fe.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/131"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;36&lt;/td&gt; 
   &lt;td&gt;2.495 minutes&lt;/td&gt; 
   &lt;td&gt;MuonCustomSizing, perform mlp and attn reduce scatter in shared call&lt;/td&gt; 
   &lt;td&gt;09/23/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-23_MuonCustomSizing/b067b4ac-72a6-4436-a6f8-ea51c1efeef3.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/132"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;37&lt;/td&gt; 
   &lt;td&gt;2.483 minutes&lt;/td&gt; 
   &lt;td&gt;Compute cross entropy in BF16 during training&lt;/td&gt; 
   &lt;td&gt;09/27/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-27_BF16CE/08c0770f-17fc-44cd-971d-734a7a28a3e3.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/133"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Gusarich&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;38&lt;/td&gt; 
   &lt;td&gt;2.476 minutes&lt;/td&gt; 
   &lt;td&gt;Polar Express, replacement for Newton-Schulz&lt;/td&gt; 
   &lt;td&gt;09/29/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-29_PolarExpress/0e3f0af5-ad08-47a6-813d-0c709b50d422.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/134"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;39&lt;/td&gt; 
   &lt;td&gt;2.447 minutes&lt;/td&gt; 
   &lt;td&gt;Only update Adam params every other step, reduce batch size&lt;/td&gt; 
   &lt;td&gt;09/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-30_CustomBatching/40b101b1-77ea-45ea-a089-1d3a647daa22.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/136"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40&lt;/td&gt; 
   &lt;td&gt;2.358 minutes&lt;/td&gt; 
   &lt;td&gt;Backout, misc hyperparameter tuning, optimize lambda padding&lt;/td&gt; 
   &lt;td&gt;10/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-04_Backout/514e7581-fbd4-4338-a3e4-e556f9c958ce.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/140"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;41&lt;/td&gt; 
   &lt;td&gt;2.345 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2510.05491"&gt;NorMuon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-24_NorMuon/088a77ee-9b67-475a-bbb9-3e92e4698799.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/144"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@li_zichong&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;42&lt;/td&gt; 
   &lt;td&gt;2.313 minutes&lt;/td&gt; 
   &lt;td&gt;Update NorMuon LR, Step Logic&lt;/td&gt; 
   &lt;td&gt;10/27/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-27_FixMuonLR/14afd380-d3d9-48d7-ad23-4c13cb96754b.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/146"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;43&lt;/td&gt; 
   &lt;td&gt;2.284 minutes&lt;/td&gt; 
   &lt;td&gt;Cautious Weight Decay w/ schedule&lt;/td&gt; 
   &lt;td&gt;11/10/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-11-10_CautiousWD/1aac0132-a891-4ed9-b358-0fd2abd1b019.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/154"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;44&lt;/td&gt; 
   &lt;td&gt;2.269 minutes&lt;/td&gt; 
   &lt;td&gt;Backward hooks on Adam, &lt;a href="https://blog.underfit.ai/profiling-101-nanogpt"&gt;Profiling 101&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-31_AdamSyncGradientHook/0c17cdfd-772c-4906-8d11-141b370599a0.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/149"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@akash5474&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;45&lt;/td&gt; 
   &lt;td&gt;2.248 minutes&lt;/td&gt; 
   &lt;td&gt;Refine skip arch, update exponential decay init&lt;/td&gt; 
   &lt;td&gt;11/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-11-18_RefineSkip/00f4e1e6-0044-4a08-b88a-3b7ec0624081.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/159"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;46&lt;/td&gt; 
   &lt;td&gt;2.203 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/1998212158770065844"&gt;Batch size schedule&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/29/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-11-29_BatchSizeSchedule/10e8f7c6-7175-4467-bdb0-a5de25d771a6.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/163"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;47&lt;/td&gt; 
   &lt;td&gt;2.193 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/1999630732814348451"&gt;Multiply attn lambda with weight instead of data, fix warmup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/10/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-10_SALambdaOnWeights/15ef5eaf-56e1-40e1-9ddf-af010027c9dd.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/166"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@roeeshenberg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48&lt;/td&gt; 
   &lt;td&gt;2.170 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2000272495644152317"&gt;Speed up Muon, additional pre-multiply lambda, reshape matrices, update lr, update NorMuon axis&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/11/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-11_NorMuonOptimsAndFixes/82edf6be-f343-475d-b93a-47c32acf4de2.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/168"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;49&lt;/td&gt; 
   &lt;td&gt;2.146 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2000841339299402142"&gt;Partial Key Offset&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/14/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-14_PartialKeyOffset/150d40bf-c20b-4568-aac9-26eb919e25fd.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/169"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;50&lt;/td&gt; 
   &lt;td&gt;2.128 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2002482925741486381"&gt;Extend Cautious Weight Decay to Adam parameters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-18_CautiousWDAdam/1981d492-bc65-4ba9-a0fa-2b30fc5c3eba.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/172"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@roeeshenberg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;51&lt;/td&gt; 
   &lt;td&gt;2.075 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2003167208483209668"&gt;Retie Embed to lm_head, retune fp8 scales&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/19/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-19_RetieLMHead/0828d309-ecfe-4442-9ee9-68fed3a4b599.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/175"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;52&lt;/td&gt; 
   &lt;td&gt;2.037 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2003863282613190656"&gt;Smooth scalars via beta increase, decrease smear gate lr, freeze scalars during transitions, adam all reduce&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/21/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-21_SmoothedScalars/12-21-Smoothed-Scalars/0bc6e909-8ee8-4ae3-ac62-0070e151a808.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/177"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;53&lt;/td&gt; 
   &lt;td&gt;1.988 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2004248941878296580"&gt;Multi-token prediction, untie embed/lm_head at 2/3 training, lr update, tweak CWD&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/22/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-22_MultiTokenPrediction/17aaf854-f338-4d0d-9767-a5db30fd7980.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/178"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal, feat. @classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;54&lt;/td&gt; 
   &lt;td&gt;1.940 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2004791008098480232"&gt;Asymmetric Logit Rescale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/26/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-26_LogitRescale/03e41c2d-2951-4546-a599-24cd723247fc.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/181"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;55&lt;/td&gt; 
   &lt;td&gt;1.918 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2005659526960492638"&gt;Gates on value embeds and skip connection&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/29/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-29_VeSkipGates/2851d7dc-d6a5-4e74-8623-57031425db16.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/186"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;56&lt;/td&gt; 
   &lt;td&gt;1.894 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2007882371576873445"&gt;Optimize and compile Adam, increase Adam buffer precision, move gates from Muon to Adam parameter banks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/31/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-31_GatesToCompiledAdam/12-31-gates-to-adam-20stps/219a5f2f-151e-4c56-ab91-3735ae4610b8.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/187"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;57&lt;/td&gt; 
   &lt;td&gt;1.878 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2008261904566022590"&gt;Bfloat16 attn/mlp weights, mixed precision Muon, interweave Adam/Muon, finer-grain Adam beta&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/04/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-04_MixedPrecisionInterweavedOptimizer/41f606b6-1b9c-46a3-b46e-2beff1521d18.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/190"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd, feat. @YouJiacheng, @ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;58&lt;/td&gt; 
   &lt;td&gt;1.820 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2008963501688324228"&gt;Paired Head Attention&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/07/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-07_PairedHeadAttention/2a5d5cde-db5f-4aab-a4a8-cc8e183ea671.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/191"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;59&lt;/td&gt; 
   &lt;td&gt;1.781 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2010545452832407943"&gt;Fused triton kernel for linear relu square MLP step&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/10/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-10_FusedLinearReLUSquare/3c47e63b-075e-4b5b-9c76-9dbe7bad9ad4.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/197"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@andrewbriand, @jrauvola&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;60&lt;/td&gt; 
   &lt;td&gt;1.765 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2012927211448516796"&gt;Fused triton kernel for softcapped multi-token prediction cross entropy step&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/16/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-16_FusedSoftcappedEntropy/45beba56-93e2-4995-bc5b-caff3cb2c1b5.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/199"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@soren_dunn_ &amp;amp; AI System &lt;a href="https://www.intology.ai/blog/previewing-locus"&gt;Locus&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;61&lt;/td&gt; 
   &lt;td&gt;1.748 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2013399457841160702"&gt;Unified Optimizers and Transposed LM Head&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/18/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-18_UnifiedOptimizers/unified-optimizer/2fc79469-a527-4bde-8540-8426ed3352d1.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/200"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;62&lt;/td&gt; 
   &lt;td&gt;1.655 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2013520088297558274"&gt;Bigram Hash Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/19/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-19_BigramHashEmbedding/40ec7bb6-14b3-46f8-90b7-bb5ed188faba.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/201"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;63&lt;/td&gt; 
   &lt;td&gt;1.650 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2016968386476200301"&gt;Untie Value Embeds&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/26/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-26-UntieValueEmbeddings/43955d93-6914-40cb-bdf8-786ace93784f.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/209"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@photo_mz&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Rules&lt;/h2&gt; 
&lt;p&gt;New records must:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Not modify the train or validation data pipelines. (You can change the batch size, sequence length, attention structure etc.; just don't change the underlying streams of tokens.)&lt;/li&gt; 
 &lt;li&gt;Attain ‚â§3.28 mean val loss. (Due to inter-run variance, submissions must provide enough run logs to attain a statistical significance level of p&amp;lt;0.01 that their mean val loss is ‚â§3.28. Example code to compute p-value can be found &lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap#softer-softcap"&gt;here&lt;/a&gt;. For submissions which improve speed by optimizing the systems performance, without touching the ML, this requirement is waived.)&lt;/li&gt; 
 &lt;li&gt;Not use any extra &lt;code&gt;torch._inductor.config&lt;/code&gt; or &lt;code&gt;torch.compile&lt;/code&gt; flags. (These can save a few seconds, but they can also make compilation take &amp;gt;30min. This rule was introduced after the 21st record.)&lt;/li&gt; 
 &lt;li&gt;Run faster than the prior record when baselined on the same hardware.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Discretionary reasons why a PR may not be accepted:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Disproportionately degrades the readability of the codebase. A 200 line kernel to drop 300ms is considered worthwhile. 500 lines that convolute the optimizer layout for a 50ms gain will likely be rejected.&lt;/li&gt; 
 &lt;li&gt;The current record is intentionally kept roughly 0.001-0.002 loss below 3.28 to make validation simpler. If a PR substantially consumes this buffer, it should do so in a way that outperforms a simple step count decrease, when measured at equivalent loss.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt; is allowed for GPT-2 Medium track (a.k.a. 2.92 track).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Other than that, anything and everything is fair game!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KellerJordan/modded-nanogpt/discussions/23?sort=new#discussioncomment-12109560"&gt;further clarifications&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Comment on the target metric&lt;/h3&gt; 
&lt;p&gt;The target metric is &lt;em&gt;cross-entropy loss on the FineWeb val set&lt;/em&gt;. To speak mathematically, the goal of the speedrun is *to obtain a probability model of language which assigns a probability of at least &lt;code&gt;math.exp(-3.28 * 10485760)&lt;/code&gt; to the first 10,485,760 tokens of the FineWeb valset. Hence, e.g., we allow evaluation at any sequence length, so long as we still have a valid probability model of language.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Timing change after record 21&lt;/h3&gt; 
&lt;p&gt;After the 21st record, we made two changes to the timing. First, there used to be an initial "grace period" of 10 untimed steps to allow kernel warmup. We replaced this with an explicit kernel-warmup section which is untimed and uses dummy data. This results in an extra runtime of 850ms from the 10 extra timed steps. Second, we banned the use of &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt;. This saves ~25min of untimed pre-run compilation, but results in an extra runtime of ~3s.&lt;/p&gt; 
&lt;!--Note: The original llm.c baseline is intended to be closer to a replication of GPT-2 than to an optimized LLM training.
So it's no surprise that there is room to improve; as @karpathy has said, 'llm.c still has a lot of pending optimizations.'
In addition, many of the techniques used in these records are completely standard, such as rotary embeddings.
The goal of this benchmark/speedrun is simply to find out which techniques actually work, and maybe come up with some new ones.--&gt; 
&lt;!--The goal of this benchmark is simply to find out all the techniques which actually work, because I'm going crazy reading all these
LLM training papers
which claim a huge benefit but then use their own idiosyncratic non-competitive benchmark and therefore no one in the community has any idea if it's legit for months.--&gt; 
&lt;!--[LLM](https://arxiv.org/abs/2305.14342) [training](https://arxiv.org/abs/2402.17764) [papers](https://arxiv.org/abs/2410.01131)--&gt; 
&lt;!--I mean hello??? We're in a completely empirical field; it is insane to not have a benchmark. Ideally everyone uses the same LLM training benchmark,
and then reviewing LLM training papers becomes as simple as checking if they beat the benchmark. It's not like this would be unprecedented, that's how things
were in the ImageNet days.
The only possible 'benefit' I can think of for any empirical field to abandon benchmarks is that it would make it easier to publish false results. Oh, I guess that's why it happened.
Hilarious to think about how, in the often-commented-upon and ongoing collapse of the peer review system, people blame the *reviewers* --
yeah, those guys doing free labor who everyone constantly musters all of their intelligence to lie to, it's *their* fault! My bad, you caught me monologuing.--&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Notable attempts &amp;amp; forks&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Notable runs:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://x.com/alexjc/status/1881410039639863622"&gt;@alexjc's 01/20/2025 2.77-minute TokenMonster-based record&lt;/a&gt;. This record is technically outside the rules of the speedrun, since we specified that the train/val tokens must be kept fixed. However, it's very interesting, and worth including. The run is not more data-efficient; rather, the speedup comes from the improved tokenizer allowing the vocabulary size to be reduced (nearly halved!) while preserving the same bytes-per-token, which saves lots of parameters and FLOPs in the head and embeddings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/205"&gt;@samacqua's 1/23/2026 test time training run&lt;/a&gt;. Sam found that prediction accuracy on the later portions of a given document could be improved by performing a training update on Adam parameters based on the early portion of the document. This 'parameter nudging' is repeated independently for each document. Interestingly, these gradient updates prove effective while only using ~500 tokens, substantially less than the over 200k tokens typically used on a normal training step. While technically a valid probability model, we are not allowing untimed backward passes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Notable forks:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BlinkDL/modded-nanogpt-rwkv"&gt;https://github.com/BlinkDL/modded-nanogpt-rwkv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nikhilvyas/modded-nanogpt-SOAP"&gt;https://github.com/nikhilvyas/modded-nanogpt-SOAP&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Speedrun track 2: GPT-2 Medium&lt;/h2&gt; 
&lt;p&gt;The target loss for this track is lowered from 3.28 to 2.92, as per Andrej Karpathy's 350M-parameter llm.c baseline. This baseline generates a model with performance similar to the original GPT-2 Medium, whereas the first track's baseline generates a model on par with GPT-2 Small. All other rules remain the same.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt; is turned on after the record 6 (*).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Record time&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Log&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;5.8 hours&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;llm.c baseline (350M parameters)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/28/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-01-18/main.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@karpathy, llm.c contributors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;29.3 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1881959719012847703"&gt;Initial record based on scaling up the GPT-2 small track speedrun&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-01-18/241dd7a7-3d76-4dce-85a4-7df60387f32a.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;28.1 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1888320690543284449"&gt;Added standard weight decay&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;02/08/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-02-08_WeightDecay/b01743db-605c-4326-b5b1-d388ee5bebc5.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;27.7 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1892793848163946799"&gt;Tuned Muon Newton-Schulz coefficients&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;02/14/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-02-14_OptCoeffs/1baa66b2-bff7-4850-aced-d63885ffb4b6.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;27.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt"&gt;Increased learning rate cooldown phase duration&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;03/06/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;25.95 minutes*&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1905861218138804534"&gt;2x MLP wd, qkv norm, all_reduce/opt.step() overlap, optimized skip pattern&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;03/25/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-25_ArchOptTweaks/train_gpt-20250329.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;25.29 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1912570883878842527"&gt;Remove FP8 head; ISRU logits softcap; New sharded mixed precision Muon; merge weights&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;04/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-04-16_Record7/223_3310d0b1-b24d-48ee-899f-d5c2a254a195.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;24.50 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/jadenj3o/status/1914893086276169754"&gt;Cubic sliding window size schedule, 2√ó max window size (24.84 minutes)&lt;/a&gt; &lt;a href="https://x.com/YouJiacheng/status/1915667616913645985"&gt;24.5min repro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;04/22/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-04-22_Record8/075_640429f2-e726-4e83-aa27-684626239ffc.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@jadenj3o&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;24.12 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://snimu.github.io/2025/10/07/modded-nanogpt-value-embeddings.html"&gt;Add two value embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;08/28/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-08-28_NewValemb/036_61ef4351-7b68-4897-b440-a99221a1a629.txt"&gt;log&lt;/a&gt;, &lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/119"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@snimu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;24.07 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://snimu.github.io/2025/10/10/modded-nanogpt-x0.html"&gt;Second input embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;09/11/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-09-11_SecondInputEmbed/000_592014ec-6781-4f59-b274-c4af68ccfe75.txt"&gt;log&lt;/a&gt;, &lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/124"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@snimu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;23.45 minutes&lt;/td&gt; 
   &lt;td&gt;Upgrade from torch 2.7 to torch==2.10.0.dev20251210+cu126&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;23.28 minutes&lt;/td&gt; 
   &lt;td&gt;Snoo Optimizer (Outer optimizer around Adam and Muon)&lt;/td&gt; 
   &lt;td&gt;09/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-09-16_Snoo/000_01db7a67-f715-4114-a7b5-6bfe23bac1b1.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/128"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@dominikkallusky&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;23.14 minutes&lt;/td&gt; 
   &lt;td&gt;EMA Wrapper on Muon&lt;/td&gt; 
   &lt;td&gt;09/17/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-09-17_UpdateSmoothing/001_8379f695-6bc3-4f76-b58b-8fadd3b6ebb0.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/129"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@acutkosky&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;23.08 minutes&lt;/td&gt; 
   &lt;td&gt;Combine both records 12 &amp;amp; 13&lt;/td&gt; 
   &lt;td&gt;09/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-09-30_SmoothedSnooMedium/101_5bc91cd0-cb46-428c-a5da-9d8d228f1f97.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/137"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@acutkosky&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;23.03 minutes&lt;/td&gt; 
   &lt;td&gt;Backout (Skip from 2/3 point to pre-lm_head)&lt;/td&gt; 
   &lt;td&gt;10/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-10-04_GPT2MediumLayerReuse/000_cc3943e4-02b5-4ae3-9441-839d32dfd9b2.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/139"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@snimu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;22.99 minutes&lt;/td&gt; 
   &lt;td&gt;Smear-MTP&lt;/td&gt; 
   &lt;td&gt;11/02/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-11-02-Smear-MTP/000_3b50518d-d542-44bc-8566-3abf633f83ad.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/151"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@snimu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;22.98 minutes&lt;/td&gt; 
   &lt;td&gt;Remove Redundant Mask Op&lt;/td&gt; 
   &lt;td&gt;11/12/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-11-12_BlockMaskRedundantOp/000_3b22a9d4-b52e-4916-99bf-3d48b38747a7.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/157/"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@manikbhandari&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;17.35 minutes&lt;/td&gt; 
   &lt;td&gt;Bulk transfer short track features&lt;/td&gt; 
   &lt;td&gt;12/31/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-12-31_BulkSmallTrackTransfer/354be270-7d41-44b7-8064-f040923f024f.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/188"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Q: What is the point of NanoGPT speedrunning?&lt;/h3&gt; 
&lt;p&gt;A: The officially stated goal of NanoGPT speedrunning is as follows: &lt;code&gt;gotta go fast&lt;/code&gt;. But for something a little more verbose involving an argument for good benchmarking, here's some kind of manifesto, adorned with a blessing from the master. &lt;a href="https://x.com/karpathy/status/1846790537262571739"&gt;https://x.com/karpathy/status/1846790537262571739&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Q: What makes "NanoGPT speedrunning" not just another idiosyncratic benchmark?&lt;/h3&gt; 
&lt;p&gt;A: Because it is a &lt;em&gt;competitive&lt;/em&gt; benchmark. In particular, if you attain a new speed record (using whatever method you want), there is an open invitation for you to post that record (on arXiv or X) and thereby vacuum up all the clout for yourself. I will even help you do it by reposting you as much as I can.&lt;/p&gt; 
&lt;!--On the contrary, for example, the benchmark used in the [Sophia](https://arxiv.org/abs/2305.14342) paper does *not* have this property.
There is no such open invitation for anyone to compete on the benchmark they used. In particular, if, for a random and definitely not weirdly specific example, you happen to find better AdamW hyperparameters for their training setup than
the ones they used which significantly close the gap between AdamW and their proposed optimizer,
then there is no clear path for you to publish that result in *any* form.
You could try posting it on X.com, but then you would be risking being perceived as aggressive/confrontational, which is *not a good look* in this racket.
So if you're rational, the result probably just dies with you and no one else learns anything
(unless you're in a frontier lab, in which case you can do a nice internal writeup. Boy I'd love to get my hands on those writeups).--&gt; 
&lt;p&gt;&lt;a href="https://www.argmin.net/p/too-much-information"&gt;"Artificial intelligence advances by inventing games and gloating to goad others to play" - Professor Ben Recht&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Q: NanoGPT speedrunning is cool and all, but meh it probably won't scale and is just overfitting to val loss&lt;/h3&gt; 
&lt;p&gt;A: This is hard to refute, since "at scale" is an infinite category (what if the methods stop working only for &amp;gt;100T models?), making it impossible to fully prove. Also, I would agree that some of the methods used in the speedrun are unlikely to scale, particularly those which &lt;em&gt;impose additional structure&lt;/em&gt; on the network, such as logit softcapping. But if the reader cares about 1.5B models, they might be convinced by this result:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Straightforwardly scaling up the speedrun (10/18/24 version) to 1.5B parameters yields a model with GPT-2 (1.5B)-level HellaSwag performance 2.5x more cheaply than &lt;a href="https://github.com/karpathy/llm.c/discussions/677"&gt;@karpathy's baseline&lt;/a&gt; ($233 instead of $576):&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/nanogpt_speedrun51.png" alt="" /&gt; [&lt;a href="https://github.com/KellerJordan/modded-nanogpt/raw/master/records/track_1_short/2024-10-20_ScaleUp1B/ad8d7ae5-7b2d-4ee9-bc52-f912e9174d7a.txt"&gt;reproducible log&lt;/a&gt;] &lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/nanogpt_speedrun52.png" alt="" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon optimizer&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Muon is defined as follows:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/algo_optimizer.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Where NewtonSchulz5 is the following Newton-Schulz iteration [2, 3], which approximately replaces &lt;code&gt;G&lt;/code&gt; with &lt;code&gt;U @ V.T&lt;/code&gt; where &lt;code&gt;U, S, V = G.svd()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@torch.compile
def zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16() / (G.norm() + eps)
    if G.size(0) &amp;gt; G.size(1):
        X = X.T 
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A
        X = a * X + B @ X
    if G.size(0) &amp;gt; G.size(1):
        X = X.T 
    return X.to(G.dtype)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For this training scenario, Muon has the following favorable properties:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Lower memory usage than Adam&lt;/li&gt; 
 &lt;li&gt;~1.5x better sample-efficiency&lt;/li&gt; 
 &lt;li&gt;&amp;lt;2% wallclock overhead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provenance&lt;/h3&gt; 
&lt;p&gt;Many of the choices made to generate this optimizer were obtained experimentally by our pursuit of &lt;a href="https://github.com/KellerJordan/cifar10-airbench"&gt;CIFAR-10 speedrunning&lt;/a&gt;. In particular, we experimentally obtained the following practices:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Using Nesterov momentum inside the update, with orthogonalization applied after momentum.&lt;/li&gt; 
 &lt;li&gt;Using a specifically quintic Newton-Schulz iteration as the method of orthogonalization.&lt;/li&gt; 
 &lt;li&gt;Using non-convergent coefficients for the quintic polynomial in order to maximize slope at zero, and thereby minimize the number of necessary Newton-Schulz iterations. It turns out that the variance doesn't actually matter that much, so we end up with a quintic that rapidly converges to the range 0.68, 1.13 upon repeated application, rather than converging more slowly to 1.&lt;/li&gt; 
 &lt;li&gt;Running the Newton-Schulz iteration in bfloat16 (whereas Shampoo implementations often depend on inverse-pth-roots run in fp32 or fp64).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Our use of a Newton-Schulz iteration for orthogonalization traces to &lt;a href="https://arxiv.org/abs/2409.20325"&gt;Bernstein &amp;amp; Newhouse (2024)&lt;/a&gt;, who suggested it as a way to compute Shampoo [5, 6] preconditioners, and theoretically explored Shampoo without preconditioner accumulation. In particular, Jeremy Bernstein @jxbz sent us the draft, which caused us to experiment with various Newton-Schulz iterations as the orthogonalization method for this optimizer. If we had used SVD instead of a Newton-Schulz iteration, this optimizer would have been too slow to be useful. Bernstein &amp;amp; Newhouse also pointed out that Shampoo without preconditioner accumulation is equivalent to steepest descent in the spectral norm, and therefore Shampoo can be thought of as a way to smooth out spectral steepest descent. The proposed optimizer can be thought of as a second way of smoothing spectral steepest descent, with a different set of memory and runtime tradeoffs compared to Shampoo.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running on fewer GPUs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;To run experiments on fewer GPUs, simply modify &lt;code&gt;run.sh&lt;/code&gt; to have a different &lt;code&gt;--nproc_per_node&lt;/code&gt;. This should not change the behavior of the training.&lt;/li&gt; 
 &lt;li&gt;If you're running out of memory, you may need to reduce the sequence length for FlexAttention (which does change the training. see &lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/38"&gt;here&lt;/a&gt; for a guide)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2406.17557"&gt;Guilherme Penedo et al. "The fineweb datasets: Decanting the web for the finest text data at scale." arXiv preprint arXiv:2406.17557 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Nicholas J. Higham. Functions of Matrices. Society for Industrial and Applied Mathematics (2008). Equation 5.22.&lt;/li&gt; 
 &lt;li&gt;G√É¬ºnther Schulz. Iterative Berechnung der reziproken Matrix. Z. Angew. Math. Mech., 13:57√¢¬Ä¬ì59 (1933).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2409.20325"&gt;Jeremy Bernstein and Laker Newhouse. "Old Optimizer, New Norm: An Anthology." arxiv preprint arXiv:2409.20325 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/1802.09568"&gt;Vineet Gupta, Tomer Koren, and Yoram Singer. "Shampoo: Preconditioned stochastic tensor optimization." International Conference on Machine Learning. PMLR, 2018.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2002.09018"&gt;Rohan Anil et al. "Scalable second order optimization for deep learning." arXiv preprint arXiv:2002.09018 (2020).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.18392"&gt;Alexander H√É¬§gele et al. "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations." arXiv preprint arXiv:2405.18392 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2410.17897"&gt;Zhanchao Zhou et al. "Value Residual Learning For Alleviating Attention Concentration In Transformers." arXiv preprint arXiv:2410.17897 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2408.00118"&gt;Team, Gemma, et al. "Gemma 2: Improving open language models at a practical size." arXiv preprint arXiv:2408.00118 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"&gt;Alec Radford et al. "Language models are unsupervised multitask learners." OpenAI blog 1.8 (2019).&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{modded_nanogpt_2024,
  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and
                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and
                  Franz Cesista and Braden Koszarsky and @Grad62304977},
  title        = {modded-nanogpt: Speedrunning the NanoGPT baseline},
  year         = {2024},
  url          = {https://github.com/KellerJordan/modded-nanogpt}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/dofa.jpg" alt="itsover_wereback" style="width:100%;" /&gt;</description>
    </item>
    
    <item>
      <title>Epodonios/v2ray-configs</title>
      <link>https://github.com/Epodonios/v2ray-configs</link>
      <description>&lt;p&gt;Free vless-vmess-shadowsocks-trojan-xray-V2ray Configs Updating Every 5 minutes&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://img.shields.io/github/last-commit/barry-far/V2ray-Configs.svg?sanitize=true" alt="GitHub last commit" /&gt; &lt;a href="https://lbesson.mit-license.org/"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true" alt="MIT license" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Epodonios/V2ray-Configs/actions/workflows/main.yml"&gt;&lt;img src="https://github.com/barry-far/V2ray-Configs/actions/workflows/main.yml/badge.svg?sanitize=true" alt="Update Configs" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/repo-size/Epodonios/V2ray-Configs" alt="GitHub repo size" /&gt;&lt;/p&gt; 
&lt;a href="https://t.me/+IOG0nSifAV03ZmY0" target="_blank"&gt; &lt;img src="https://cdn-icons-png.flaticon.com/512/2111/2111646.png" alt="Telegram" width="500" height="500" /&gt; contact us &lt;/a&gt; 
&lt;h1&gt;Bulk V2ray Configs&lt;/h1&gt; 
&lt;p&gt;üíª This repository contains a collection of free V2ray configuration files that you can use with your V2ray client to access the internet securely and anonymously. This script collects several thousand V2ray configurations every five minutes, and you can receive and use the protocol in base 64, normal, or split format.&lt;/p&gt; 
&lt;h3&gt;Supported Protocols:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vmess&lt;/li&gt; 
 &lt;li&gt;Vless&lt;/li&gt; 
 &lt;li&gt;Trojan&lt;/li&gt; 
 &lt;li&gt;Tuic&lt;/li&gt; 
 &lt;li&gt;Shadowsocks&lt;/li&gt; 
 &lt;li&gt;ShadowsocksR&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;You can use a v2ray client to use these subscription links:&lt;/h3&gt; 
&lt;h4&gt;Android:&lt;/h4&gt; 
&lt;p&gt;v2rayng&lt;/p&gt; 
&lt;h4&gt;IOS:&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;fair&lt;/li&gt; 
 &lt;li&gt;streisand&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Windows and Linux:&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;hiddify-next&lt;/li&gt; 
 &lt;li&gt;nekoray&lt;/li&gt; 
 &lt;li&gt;v2rayn&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Subscriptions Links&lt;/h2&gt; 
&lt;h3&gt;Here are the subscription links at your disposal:&lt;/h3&gt; 
&lt;p&gt;All collected configs:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/All_Configs_Sub.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the above link doesn't work, try the base 64 configurations:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/All_Configs_base64_Sub.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Splited by protocol:&lt;/h3&gt; 
&lt;p&gt;Vless:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vless.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Vmess:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vmess.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ss:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/ss.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ssr:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/ssr.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Trojan:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/trojan.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Splited in 250 count of configs:&lt;/h3&gt; 
&lt;p&gt;Config List 1:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub1.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub2.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 3:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub3.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 4:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub4.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 5:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub5.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 6:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub6.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 7:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub7.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 8:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub8.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 9:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub9.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 10:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub10.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 11:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub11.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 12:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub12.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 13:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub13.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Config List 14:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/Sub14.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage:&lt;/h3&gt; 
&lt;p&gt;Mobile and pc:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Copy the links provided and go to your v2ray clients subscription setting and paste metioned link and save that.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Occasionally use the subscription update function in your v2ray client to stay up-to-date ü§ù.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;I hope u use this configs very well.&lt;/p&gt; 
&lt;h1&gt;V2Ray Config Scanner&lt;/h1&gt; 
&lt;p&gt;A lightweight Python script that scans and pings a list of V2Ray configuration links (vmess, vless, etc.), and outputs their protocol and latency. Useful for testing and sorting multiple V2Ray configs based on performance.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports &lt;code&gt;vmess&lt;/code&gt;, vless, and other V2Ray protocols&lt;/li&gt; 
 &lt;li&gt;Measures latency (ping) for each config&lt;/li&gt; 
 &lt;li&gt;Sorts or filters results based on protocol and responsiveness&lt;/li&gt; 
 &lt;li&gt;Simple, fast, and dependency-free (only requires Python)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.x (no external packages required)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure Python 3 is installed on your system.&lt;/li&gt; 
 &lt;li&gt;Download the sub*.txt files from this repository (they contain lists of V2Ray subscription links).&lt;/li&gt; 
 &lt;li&gt;Run the script and provide the path to one or more sub*.txt files as arguments.&lt;/li&gt; 
 &lt;li&gt;The script will start scanning and show you the protocol and ping for each config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Sample Output&lt;/p&gt; 
&lt;p&gt;[vmess] node1.example.com - 42 ms [vless] node2.example.net - timeout [shadowsocks] fastnode.org - 35 ms&lt;/p&gt; 
&lt;h2&gt;Tunnel entire system:&lt;/h2&gt; 
&lt;p&gt;For better use and tunneling the entire system, you can use a proxy program. The usage steps are as follows:&lt;/p&gt; 
&lt;h3&gt;Usage Instructions:&lt;/h3&gt; 
&lt;p&gt;1-First, install the Proxifier program.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://proxifier.com/download/"&gt;https://proxifier.com/download/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2-Activate the program:&lt;/p&gt; 
&lt;p&gt;Activation keys:&lt;/p&gt; 
&lt;p&gt;Portable Edition:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;L6Z8A-XY2J4-BTZ3P-ZZ7DF-A2Q9C
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Standard Edition:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  5EZ8G-C3WL5-B56YG-SCXM9-6QZAP
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Mac OS:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; P427L-9Y552-5433E-8DSR3-58Z68
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;3-Go to the Profile section and select the Proxy Server. In the displayed section, click on Add.&lt;/p&gt; 
&lt;p&gt;4-Enter the following information:&lt;/p&gt; 
&lt;p&gt;IP: Enter 127.0.0.1&lt;/p&gt; 
&lt;p&gt;Port: Depending on the version you are using, enter:&lt;/p&gt; 
&lt;p&gt;V2rayN: 10808&lt;/p&gt; 
&lt;p&gt;Netch: 2801&lt;/p&gt; 
&lt;p&gt;SSR: 1080&lt;/p&gt; 
&lt;p&gt;Mac V2rayU: 1086&lt;/p&gt; 
&lt;p&gt;Protocol: Select SOCKS5&lt;/p&gt; 
&lt;p&gt;5-Enjoy!&lt;/p&gt; 
&lt;p&gt;Some installed programs on the system, like Spotube, might not fully tunnel. This issue can be resolved with this method.&lt;/p&gt; 
&lt;p&gt;Your friend, EPODONIOS&lt;/p&gt; 
&lt;h2&gt;u can use this feature with another way it no needs any program set by system tools&lt;/h2&gt; 
&lt;h3&gt;instruction:&lt;/h3&gt; 
&lt;p&gt;1- open your OS setting&lt;/p&gt; 
&lt;p&gt;2- go to proxy section&lt;/p&gt; 
&lt;p&gt;3- in proxy section set this values : ip : 127.0.0.1&lt;/p&gt; 
&lt;p&gt;port : 10809&lt;/p&gt; 
&lt;p&gt;local host :&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;localhost;127.*;10.*;172.16.*;172.17.*;172.18.*;172.19.*;172.20.*;172.21.*;172.22.*;172.23.*;172.24.*;172.25.*;172.26.*;172.27.*;172.28.*;172.29.*;172.30.*;172.31.*;192.168.*
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;4- then set it up with ON key&lt;/p&gt; 
&lt;p&gt;5- back to v2rayn and after set your config turn it to set system proxy&lt;/p&gt; 
&lt;p&gt;6- now your system tunneled entirely&lt;/p&gt; 
&lt;p&gt;ur friend,EPODONIOS&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>business-science/ai-data-science-team</title>
      <link>https://github.com/business-science/ai-data-science-team</link>
      <description>&lt;p&gt;An AI-powered data science team of agents to help you perform common data science tasks 10X faster.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt; 
  &lt;picture&gt; 
   &lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/ai_data_science_logo.png" alt="AI Data Science Team" width="360" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;em&gt;AI Data Science Team + AI Pipeline Studio&lt;/em&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.python.org/pypi/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/v/ai-data-science-team.svg?style=for-the-badge" alt="PyPI" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ai-data-science-team.svg?style=for-the-badge" alt="versions" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/business-science/ai-data-science-team.svg?style=for-the-badge" alt="license" /&gt;&lt;/a&gt; 
 &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/business-science/ai-data-science-team?style=for-the-badge" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;AI Data Science Team&lt;/h1&gt; 
&lt;p&gt;AI Data Science Team is a Python library of specialized agents for common data science workflows, plus a flagship app: &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt;. The Studio turns your work into a visual, reproducible pipeline, while the AI team handles data loading, cleaning, visualization, and modeling.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Beta. Breaking changes may occur until 0.1.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;strong&gt;Please ‚≠ê us on GitHub (it takes 2 seconds and means a lot).&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;AI Pipeline Studio (Flagship App)&lt;/h2&gt; 
&lt;p&gt;AI Pipeline Studio is the main example of the AI Data Science Team in action.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/apps/ai_pipeline_studio_app.jpg" alt="AI Pipeline Studio" /&gt;&lt;/p&gt; 
&lt;p&gt;Highlights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pipeline-first workspace: Visual Editor, Table, Chart, EDA, Code, Model, Predictions, MLflow&lt;/li&gt; 
 &lt;li&gt;Manual + AI steps with lineage and reproducible scripts&lt;/li&gt; 
 &lt;li&gt;Multi-dataset handling and merge workflows&lt;/li&gt; 
 &lt;li&gt;Project saves: metadata-only or full-data&lt;/li&gt; 
 &lt;li&gt;Storage footprint controls and rehydrate workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Full app docs: &lt;code&gt;apps/ai-pipeline-studio-app/README.md&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;OpenAI API key (or Ollama for local models)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install the app and library&lt;/h3&gt; 
&lt;p&gt;Clone the repo and install in editable mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run the AI Pipeline Studio app&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Library Overview&lt;/h2&gt; 
&lt;p&gt;The repository includes both the &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt; app and the underlying &lt;strong&gt;AI Data Science Team&lt;/strong&gt; library. The library provides agent building blocks and multi-agent workflows for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data loading and inspection&lt;/li&gt; 
 &lt;li&gt;Cleaning, wrangling, and feature engineering&lt;/li&gt; 
 &lt;li&gt;Visualization and EDA&lt;/li&gt; 
 &lt;li&gt;Modeling and evaluation (H2O + MLflow tools)&lt;/li&gt; 
 &lt;li&gt;SQL database interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Agents (Snapshot)&lt;/h3&gt; 
&lt;p&gt;Agent examples live in &lt;code&gt;examples/&lt;/code&gt;. Notable agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data Loader Tools Agent&lt;/li&gt; 
 &lt;li&gt;Data Wrangling Agent&lt;/li&gt; 
 &lt;li&gt;Data Cleaning Agent&lt;/li&gt; 
 &lt;li&gt;Data Visualization Agent&lt;/li&gt; 
 &lt;li&gt;EDA Tools Agent&lt;/li&gt; 
 &lt;li&gt;Feature Engineering Agent&lt;/li&gt; 
 &lt;li&gt;SQL Database Agent&lt;/li&gt; 
 &lt;li&gt;H2O ML Agent&lt;/li&gt; 
 &lt;li&gt;MLflow Tools Agent&lt;/li&gt; 
 &lt;li&gt;Multi-agent workflows (e.g., Pandas Data Analyst, SQL Data Analyst)&lt;/li&gt; 
 &lt;li&gt;Supervisor Agent (oversees other agents)&lt;/li&gt; 
 &lt;li&gt;Custom tools for data science tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Apps&lt;/h2&gt; 
&lt;p&gt;See all apps in &lt;code&gt;apps/&lt;/code&gt;. Notable apps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Pipeline Studio: &lt;code&gt;apps/ai-pipeline-studio-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;EDA Explorer App: &lt;code&gt;apps/exploratory-copilot-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Pandas Data Analyst App: &lt;code&gt;apps/pandas-data-analyst-app/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use OpenAI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    model_name="gpt-4.1-mini",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Ollama (Local LLM)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama serve
ollama pull llama3.1:8b
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.1:8b",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Next-Gen AI Agentic Workshop&lt;/h2&gt; 
&lt;p&gt;Want to learn how to build AI agents and AI apps for real data science workflows? Join my next‚Äëgen AI workshop: &lt;a href="https://learn.business-science.io/ai-register"&gt;https://learn.business-science.io/ai-register&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NevaMind-AI/memU</title>
      <link>https://github.com/NevaMind-AI/memU</link>
      <description>&lt;p&gt;Memory for 24/7 proactive agents like moltbot (clawdbot).&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/banner.png" alt="MemU Banner" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;memU&lt;/h1&gt; 
 &lt;h3&gt;Always-On Proactive Memory for AI Agents&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/memu-py"&gt;&lt;img src="https://badge.fury.io/py/memu-py.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.13+-blue.svg?sanitize=true" alt="Python 3.13+" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/memu"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/memU_ai"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/17374" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/17374" alt="NevaMind-AI%2FmemU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_en.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_zh.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_fr.md"&gt;Fran√ßais&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;memU is a memory framework built for &lt;strong&gt;24/7 proactive agents&lt;/strong&gt;. It is designed for long-running use and greatly &lt;strong&gt;reduces the LLM token cost&lt;/strong&gt; of keeping agents always online, making always-on, evolving agents practical in production systems. memU &lt;strong&gt;continuously captures and understands user intent&lt;/strong&gt;. Even without a command, the agent can tell what you are about to do and act on it by itself.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ñ &lt;a href="https://memu.bot"&gt;memU bot&lt;/a&gt;&lt;/h2&gt; 
&lt;img width="100%" src="https://github.com/NevaMind-AI/memU/raw/main/assets/memUbot.png" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Download-and-use and simple&lt;/strong&gt; to get started.&lt;/li&gt; 
 &lt;li&gt;Builds long-term memory to &lt;strong&gt;understand user intent&lt;/strong&gt; and act proactively.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cuts LLM token cost&lt;/strong&gt; with smaller context.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Try now: &lt;a href="https://memu.bot"&gt;memU bot&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠êÔ∏è Star the repository&lt;/h2&gt; 
&lt;img width="100%" src="https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif" /&gt; If you find memU useful or interesting, a GitHub Star ‚≠êÔ∏è would be greatly appreciated. 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® Core Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Capability&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ü§ñ &lt;strong&gt;24/7 Proactive Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Always-on memory agent that works continuously in the background‚Äînever sleeps, never forgets&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üéØ &lt;strong&gt;User Intention Capture&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Understands and remembers user goals, preferences, and context across sessions automatically&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üí∞ &lt;strong&gt;Cost Efficient&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reduces long-running token costs by caching insights and avoiding redundant LLM calls&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîÑ How Proactive Memory Works&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
cd examples/proactive
python proactive.py

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Proactive Memory Lifecycle&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. USER INITIAL QUERY                          ‚îÇ
‚îÇ  ‚îî‚îÄ User input, context, or any trigger event   ‚îÇ
‚îÇ     Conversation starts here                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  2. AGENT PLANNING / ACTIONS                    ‚îÇ
    ‚îÇ  ‚îî‚îÄ Analyze request, execute tasks              ‚îÇ
    ‚îÇ     Retrieve relevant memories for context      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  3. MEMORIZE &amp;amp; UPDATE TODOLIST                  ‚îÇ
    ‚îÇ  ‚îî‚îÄ Store new insights, facts, preferences      ‚îÇ
    ‚îÇ     Modify task list based on progress          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  4. PREDICT USER INTENT                         ‚îÇ
    ‚îÇ  ‚îî‚îÄ Anticipate next steps and needs             ‚îÇ
    ‚îÇ     Proactively prepare relevant context        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  5. LOOP (2 ‚Üí 4)                                ‚îÇ
    ‚îÇ  ‚îî‚îÄ Continuous iteration until task complete    ‚îÇ
    ‚îÇ     Agent-driven proactive workflow             ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ Proactive Use Cases&lt;/h2&gt; 
&lt;h3&gt;1. &lt;strong&gt;Information Recommendation&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Agent monitors interests and proactively surfaces relevant content&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# User has been researching AI topics
MemU tracks: reading history, saved articles, search queries

# When new content arrives:
Agent: "I found 3 new papers on RAG optimization that align with
        your recent research on retrieval systems. One author
        (Dr. Chen) you've cited before published yesterday."

# Proactive behaviors:
- Learns topic preferences from browsing patterns
- Tracks author/source credibility preferences
- Filters noise based on engagement history
- Times recommendations for optimal attention
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. &lt;strong&gt;Email Management&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Agent learns communication patterns and handles routine correspondence&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# MemU observes email patterns over time:
- Response templates for common scenarios
- Priority contacts and urgent keywords
- Scheduling preferences and availability
- Writing style and tone variations

# Proactive email assistance:
Agent: "You have 12 new emails. I've drafted responses for 3 routine
        requests and flagged 2 urgent items from your priority contacts.
        Should I also reschedule tomorrow's meeting based on the
        conflict John mentioned?"

# Autonomous actions:
‚úì Draft context-aware replies
‚úì Categorize and prioritize inbox
‚úì Detect scheduling conflicts
‚úì Summarize long threads with key decisions
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. &lt;strong&gt;Trading &amp;amp; Financial Monitoring&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Agent tracks market context and user investment behavior&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# MemU learns trading preferences:
- Risk tolerance from historical decisions
- Preferred sectors and asset classes
- Response patterns to market events
- Portfolio rebalancing triggers

# Proactive alerts:
Agent: "NVDA dropped 5% in after-hours trading. Based on your past
        behavior, you typically buy tech dips above 3%. Your current
        allocation allows for $2,000 additional exposure while
        maintaining your 70/30 equity-bond target."

# Continuous monitoring:
- Track price alerts tied to user-defined thresholds
- Correlate news events with portfolio impact
- Learn from executed vs. ignored recommendations
- Anticipate tax-loss harvesting opportunities
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;...&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üóÇÔ∏è Hierarchical Memory Architecture&lt;/h2&gt; 
&lt;p&gt;MemU's three-layer system enables both &lt;strong&gt;reactive queries&lt;/strong&gt; and &lt;strong&gt;proactive context loading&lt;/strong&gt;:&lt;/p&gt; 
&lt;img width="100%" alt="structure" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Layer&lt;/th&gt; 
   &lt;th&gt;Reactive Use&lt;/th&gt; 
   &lt;th&gt;Proactive Use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Resource&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct access to original data&lt;/td&gt; 
   &lt;td&gt;Background monitoring for new patterns&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Item&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Targeted fact retrieval&lt;/td&gt; 
   &lt;td&gt;Real-time extraction from ongoing interactions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Summary-level overview&lt;/td&gt; 
   &lt;td&gt;Automatic context assembly for anticipation&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Auto-categorization&lt;/strong&gt;: New memories self-organize into topics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pattern Detection&lt;/strong&gt;: System identifies recurring themes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context Prediction&lt;/strong&gt;: Anticipates what information will be needed next&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Option 1: Cloud Version&lt;/h3&gt; 
&lt;p&gt;Experience proactive memory instantly:&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://memu.so"&gt;memu.so&lt;/a&gt;&lt;/strong&gt; - Hosted service with 7√ó24 continuous learning&lt;/p&gt; 
&lt;p&gt;For enterprise deployment with custom proactive workflows, contact &lt;strong&gt;&lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Cloud API (v3)&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;https://api.memu.so&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Auth&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Authorization: Bearer YOUR_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Endpoint&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register continuous learning task&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize/status/{task_id}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Check real-time processing status&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/categories&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List auto-generated categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/retrieve&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Query memory (supports proactive context loading)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;üìö &lt;strong&gt;&lt;a href="https://memu.pro/docs#cloud-version"&gt;Full API Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option 2: Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Basic Example&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Python 3.13+ and an OpenAI API key&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Test Continuous Learning&lt;/strong&gt; (in-memory):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Test with Persistent Storage&lt;/strong&gt; (PostgreSQL):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run continuous learning test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Both examples demonstrate &lt;strong&gt;proactive memory workflows&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Continuous Ingestion&lt;/strong&gt;: Process multiple files sequentially&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Auto-Extraction&lt;/strong&gt;: Immediate memory creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Proactive Retrieval&lt;/strong&gt;: Context-aware memory surfacing&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_inmemory.py"&gt;&lt;code&gt;tests/test_inmemory.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_postgres.py"&gt;&lt;code&gt;tests/test_postgres.py&lt;/code&gt;&lt;/a&gt; for implementation details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Custom LLM and Embedding Providers&lt;/h3&gt; 
&lt;p&gt;MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via &lt;code&gt;llm_profiles&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        "default": {
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": "your_api_key",
            "chat_model": "qwen3-max",
            "client_backend": "sdk"  # "sdk" or "http"
        },
        # Separate profile for embeddings
        "embedding": {
            "base_url": "https://api.voyageai.com/v1",
            "api_key": "your_voyage_api_key",
            "embed_model": "voyage-3.5-lite"
        }
    },
    # ... other configuration
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;MemU supports &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt; as a model provider, giving you access to multiple LLM providers through a single API.&lt;/p&gt; 
&lt;h4&gt;Configuration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemoryService

service = MemoryService(
    llm_profiles={
        "default": {
            "provider": "openrouter",
            "client_backend": "httpx",
            "base_url": "https://openrouter.ai",
            "api_key": "your_openrouter_api_key",
            "chat_model": "anthropic/claude-3.5-sonnet",  # Any OpenRouter model
            "embed_model": "openai/text-embedding-3-small",  # Embedding model
        },
    },
    database_config={
        "metadata_store": {"provider": "inmemory"},
    },
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Your OpenRouter API key from &lt;a href="https://openrouter.ai/keys"&gt;openrouter.ai/keys&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Supported Features&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chat Completions&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Works with any OpenRouter chat model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Embeddings&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use OpenAI embedding models via OpenRouter&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use vision-capable models (e.g., &lt;code&gt;openai/gpt-4o&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Running OpenRouter Tests&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENROUTER_API_KEY=your_api_key

# Full workflow test (memorize + retrieve)
python tests/test_openrouter.py

# Embedding-specific tests
python tests/test_openrouter_embedding.py

# Vision-specific tests
python tests/test_openrouter_vision.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/examples/example_4_openrouter_memory.py"&gt;&lt;code&gt;examples/example_4_openrouter_memory.py&lt;/code&gt;&lt;/a&gt; for a complete working example.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Core APIs&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;memorize()&lt;/code&gt; - Continuous Learning Pipeline&lt;/h3&gt; 
&lt;p&gt;Processes inputs in real-time and immediately updates memory:&lt;/p&gt; 
&lt;img width="100%" alt="memorize" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png" /&gt; ```python result = await service.memorize( resource_url="path/to/file.json", # File path or URL modality="conversation", # conversation | document | image | video | audio user={"user_id": "123"} # Optional: scope to a user ) 
&lt;h1&gt;Returns immediately with extracted memory:&lt;/h1&gt; 
&lt;p&gt;{ "resource": {...}, # Stored resource metadata "items": [...], # Extracted memory items (available instantly) "categories": [...] # Auto-updated category structure }&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;
**Proactive Features:**
- Zero-delay processing‚Äîmemories available immediately
- Automatic categorization without manual tagging
- Cross-reference with existing memories for pattern detection

### `retrieve()` - Dual-Mode Intelligence

MemU supports both **proactive context loading** and **reactive querying**:

&amp;lt;img width="100%" alt="retrieve" src="assets/retrieve.png" /&amp;gt;

#### RAG-based Retrieval (`method="rag"`)

Fast **proactive context assembly** using embeddings:

- ‚úÖ **Instant context**: Sub-second memory surfacing
- ‚úÖ **Background monitoring**: Can run continuously without LLM costs
- ‚úÖ **Similarity scoring**: Identifies most relevant memories automatically

#### LLM-based Retrieval (`method="llm"`)

Deep **anticipatory reasoning** for complex contexts:

- ‚úÖ **Intent prediction**: LLM infers what user needs before they ask
- ‚úÖ **Query evolution**: Automatically refines search as context develops
- ‚úÖ **Early termination**: Stops when sufficient context is gathered

#### Comparison

| Aspect | RAG (Fast Context) | LLM (Deep Reasoning) |
|--------|-------------------|---------------------|
| **Speed** | ‚ö° Milliseconds | üê¢ Seconds |
| **Cost** | üí∞ Embedding only | üí∞üí∞ LLM inference |
| **Proactive use** | Continuous monitoring | Triggered context loading |
| **Best for** | Real-time suggestions | Complex anticipation |

#### Usage
```python
# Proactive retrieval with context history
result = await service.retrieve(
    queries=[
        {"role": "user", "content": {"text": "What are their preferences?"}},
        {"role": "user", "content": {"text": "Tell me about work habits"}}
    ],
    where={"user_id": "123"},  # Optional: scope filter
    method="rag"  # or "llm" for deeper reasoning
)

# Returns context-aware results:
{
    "categories": [...],     # Relevant topic areas (auto-prioritized)
    "items": [...],          # Specific memory facts
    "resources": [...],      # Original sources for traceability
    "next_step_query": "..." # Predicted follow-up context
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Filtering&lt;/strong&gt;: Use &lt;code&gt;where&lt;/code&gt; to scope continuous monitoring:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;where={"user_id": "123"}&lt;/code&gt; - User-specific context&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;where={"agent_id__in": ["1", "2"]}&lt;/code&gt; - Multi-agent coordination&lt;/li&gt; 
 &lt;li&gt;Omit &lt;code&gt;where&lt;/code&gt; for global context awareness&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìö &lt;strong&gt;For complete API documentation&lt;/strong&gt;, see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/docs/SERVICE_API.md"&gt;SERVICE_API.md&lt;/a&gt; - includes proactive workflow patterns, pipeline configuration, and real-time update handling.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Proactive Scenarios&lt;/h2&gt; 
&lt;h3&gt;Example 1: Always-Learning Assistant&lt;/h3&gt; 
&lt;p&gt;Continuously learns from every interaction without explicit memory commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automatically extracts preferences from casual mentions&lt;/li&gt; 
 &lt;li&gt;Builds relationship models from interaction patterns&lt;/li&gt; 
 &lt;li&gt;Surfaces relevant context in future conversations&lt;/li&gt; 
 &lt;li&gt;Adapts communication style based on learned preferences&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Personal AI assistants, customer support that remembers, social chatbots&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 2: Self-Improving Agent&lt;/h3&gt; 
&lt;p&gt;Learns from execution logs and proactively suggests optimizations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Monitors agent actions and outcomes continuously&lt;/li&gt; 
 &lt;li&gt;Identifies patterns in successes and failures&lt;/li&gt; 
 &lt;li&gt;Auto-generates skill guides from experience&lt;/li&gt; 
 &lt;li&gt;Proactively suggests strategies for similar future tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; DevOps automation, agent self-improvement, knowledge capture&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 3: Multimodal Context Builder&lt;/h3&gt; 
&lt;p&gt;Unifies memory across different input types for comprehensive context:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cross-references text, images, and documents automatically&lt;/li&gt; 
 &lt;li&gt;Builds unified understanding across modalities&lt;/li&gt; 
 &lt;li&gt;Surfaces visual context when discussing related topics&lt;/li&gt; 
 &lt;li&gt;Anticipates information needs by combining multiple sources&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Documentation systems, learning platforms, research assistants&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìä Performance&lt;/h2&gt; 
&lt;p&gt;MemU achieves &lt;strong&gt;92.09% average accuracy&lt;/strong&gt; on the Locomo benchmark across all reasoning tasks, demonstrating reliable proactive memory operations.&lt;/p&gt; 
&lt;img width="100%" alt="benchmark" src="https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9" /&gt; 
&lt;p&gt;View detailed experimental data: &lt;a href="https://github.com/NevaMind-AI/memU-experiment"&gt;memU-experiment&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üß© Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Repository&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Proactive Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU"&gt;memU&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Core proactive memory engine&lt;/td&gt; 
   &lt;td&gt;7√ó24 learning pipeline, auto-categorization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-server"&gt;memU-server&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Backend with continuous sync&lt;/td&gt; 
   &lt;td&gt;Real-time memory updates, webhook triggers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-ui"&gt;memU-ui&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Visual memory dashboard&lt;/td&gt; 
   &lt;td&gt;Live memory evolution monitoring&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ &lt;a href="https://app.memu.so/quick-start"&gt;Try MemU Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìö &lt;a href="https://memu.pro/docs"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://discord.gg/memu"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Partners&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework"&gt;&lt;img src="https://avatars.githubusercontent.com/u/113095513?s=200&amp;amp;v=4" alt="Ten" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://openagents.org"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/openagents.png" alt="OpenAgents" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/milvus-io/milvus"&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png" alt="Milvus" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://xroute.ai/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/xroute.png" alt="xRoute" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://jaaz.app/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/jazz.png" alt="Jazz" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Buddie-AI/Buddie"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/buddie.png" alt="Buddie" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/bytebase/bytebase"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/bytebase.png" alt="Bytebase" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/LazyLLM.png" alt="LazyLLM" height="40" style="margin: 10px;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù How to Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;p&gt;To start contributing to MemU, you'll need to set up your development environment:&lt;/p&gt; 
&lt;h4&gt;Prerequisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.13+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; (Python package manager)&lt;/li&gt; 
 &lt;li&gt;Git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Setup Development Environment&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/memU.git
cd memU

# 2. Install development dependencies
make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make install&lt;/code&gt; command will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a virtual environment using &lt;code&gt;uv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install all project dependencies&lt;/li&gt; 
 &lt;li&gt;Set up pre-commit hooks for code quality checks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running Quality Checks&lt;/h4&gt; 
&lt;p&gt;Before submitting your contribution, ensure your code passes all quality checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make check&lt;/code&gt; command runs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lock file verification&lt;/strong&gt;: Ensures &lt;code&gt;pyproject.toml&lt;/code&gt; consistency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-commit hooks&lt;/strong&gt;: Lints code with Ruff, formats with Black&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type checking&lt;/strong&gt;: Runs &lt;code&gt;mypy&lt;/code&gt; for static type analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency analysis&lt;/strong&gt;: Uses &lt;code&gt;deptry&lt;/code&gt; to find obsolete dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing Guidelines&lt;/h3&gt; 
&lt;p&gt;For detailed contribution guidelines, code standards, and development practices, please see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick tips:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new branch for each feature or bug fix&lt;/li&gt; 
 &lt;li&gt;Write clear commit messages&lt;/li&gt; 
 &lt;li&gt;Add tests for new functionality&lt;/li&gt; 
 &lt;li&gt;Update documentation as needed&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make check&lt;/code&gt; before pushing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/LICENSE.txt"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåç Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/NevaMind-AI/memU/issues"&gt;Report bugs &amp;amp; request features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.com/invite/hQZntfGsbJ"&gt;Join the community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;X (Twitter)&lt;/strong&gt;: &lt;a href="https://x.com/memU_ai"&gt;Follow @memU_ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contact&lt;/strong&gt;: &lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‚≠ê &lt;strong&gt;Star us on GitHub&lt;/strong&gt; to get notified about new releases!&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/ViMax</title>
      <link>https://github.com/HKUDS/ViMax</link>
      <description>&lt;p&gt;"ViMax: Agentic Video Generation (Director, Screenwriter, Producer, and Video Generator All-in-One)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/ViMax/main/assets/vimax.png" /&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://trendshift.io/repositories/15299" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15299" alt="HKUDS%2FViMax | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;h1 align="center"&gt;ViMax: Agentic Video Generation&lt;/h1&gt; 
 &lt;div align="center"&gt; 
 &lt;/div&gt; 
 &lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.12-00d9ff?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;img src="https://img.shields.io/badge/‚ö°uv-Ready-ff6b6b?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/Communication.md"&gt;&lt;img src="https://img.shields.io/badge/üí¨Feishu-Group-07c160?style=for-the-badge&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/Communication.md"&gt;&lt;img src="https://img.shields.io/badge/WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.youtube.com/@AI-Creator-is-here"&gt;&lt;img src="https://badges.aleen42.com/src/youtube.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/readme.md"&gt;&lt;img src="https://img.shields.io/badge/English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/README_ZH.md"&gt;&lt;img src="https://img.shields.io/badge/‰∏≠ÊñáÁâà-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-FFC107?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üö® Current Video Generation Limitations:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ùå &lt;strong&gt;Limited to Short Clips&lt;/strong&gt; - Most AI tools generate only seconds of footage. &lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;‚ùå &lt;strong&gt;Consistency Chaos&lt;/strong&gt; - Characters and scenes change unpredictably across frames. &lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;‚ùå &lt;strong&gt;Visual-Only Focus&lt;/strong&gt; - Missing scripts, audio, narrative structure, and storytelling depth. &lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí° ViMax Solution:&lt;/h3&gt; 
&lt;p&gt;üé¨ &lt;strong&gt;Director&lt;/strong&gt;, &lt;strong&gt;Screenwriter&lt;/strong&gt;, &lt;strong&gt;Producer&lt;/strong&gt;, and &lt;strong&gt;Video Generator&lt;/strong&gt; &lt;strong&gt;All-in-One&lt;/strong&gt;! We're exploring a future where AI becomes a complete creative powerhouse. üí° Simply input your concept. ViMax autonomously handles the rest. It orchestrates scriptwriting, storyboarding, character creation, and final video generation‚Äîall end-to-end. üöÄ&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5bad46b2-8276-4e1d-9480-3522640744b2"&gt;https://github.com/user-attachments/assets/5bad46b2-8276-4e1d-9480-3522640744b2&lt;/a&gt;&lt;/p&gt;  
&lt;hr /&gt; 
&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#key-features"&gt;üí° Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#Video-Demos-Generated-from-Scratch"&gt;üîÆ Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#%EF%B8%8F-architecture"&gt;üèóÔ∏è Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/ViMax/main/#quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí°Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="25%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üåü &lt;strong&gt;Idea2Video&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/IDEA-GENERATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;From Spark to Screen&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Transform &lt;strong&gt; raw ideas &lt;/strong&gt; into complete video stories through intelligent multi-agent workflows automating &lt;strong&gt; storytelling, character design, and production &lt;/strong&gt;. &lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="25%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üé® &lt;strong&gt;Novel2Video&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/NOVEL-ADAPTATION-4ecdc4?style=for-the-badge&amp;amp;logo=book&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Smart Literary Adaptation Engine&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Transform &lt;strong&gt;complete novels&lt;/strong&gt; into &lt;strong&gt;episodic video content&lt;/strong&gt; with intelligent narrative compression, character tracking, and scene-by-scene visual adaptation&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="25%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;‚öôÔ∏è &lt;strong&gt;Script2Video&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/SCRIPT-ADAPTATION-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Unlimited Screenplay Video Creation&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Unleash your creativity by writing &lt;strong&gt;any screenplay&lt;/strong&gt; from personal stories to epic adventures, giving you complete control over every aspect of your visual storytelling.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="25%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;ü§≥ &lt;strong&gt;AutoCameo&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/INTERACTIVE-GENERATION-FFC107?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Generate Video from Your Photo&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt; &lt;strong&gt;Create your own cameo&lt;/strong&gt; video, transforming yourself/pet into a guest star who appears across limitless creative scripts, cinematic sequences, and interactive storylines.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîÆVideo Demos Generated from Scratch&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/c2fb27b0-218c-4976-b3d6-2abf8ea06be7" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/bfa566a8-688d-4d53-a9e2-6cedeb4a399d" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/49f61134-4f78-4285-9a9e-bb5e3e0c4abf" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/a950f449-a15c-449b-a1b8-c393951aa9be" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/bb3ff0fd-9433-4806-886a-3f77b61d06ec" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/2624a3f0-9f66-4fa4-b527-45c0ea0353fc" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/5dbb80f7-aff0-4211-940c-a898f91fb80c" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/cc0b0bcd-e7db-4839-950b-0b03949637bd" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/85919b59-80f0-461a-af7e-a93d3fb412fc" controls width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;End-to-End Video Creation Engine&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üåÖ &lt;strong&gt;Reference Images&lt;/strong&gt;: Time-consuming acquisition, organization, and alignment of reference frames that accurately capture characters, objects, positions, and environments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ü´† &lt;strong&gt;Consistency Check&lt;/strong&gt;: Sometimes, the image generator may generate unusable images even if it is given the correct characters, position, environment reference image and prompts.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìÑ &lt;strong&gt;Scripts Generation&lt;/strong&gt;: Professional and high-quality videos need to have rich information density and structured design.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìù &lt;strong&gt;Storyboard Design&lt;/strong&gt;: Converting stories into visual narratives requires expertise in cinematography, scene composition, and visual storytelling that most creators lack.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé¨ &lt;strong&gt;Shot Design&lt;/strong&gt;: Creating coherent camera sequences with proper angles, transitions, and pacing while maintaining narrative flow across complex scenes.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé® &lt;strong&gt;Development Delays&lt;/strong&gt;: Ensuring character appearances, environments, and artistic style remain consistent across hundreds of shots in long-form content.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚è±Ô∏è &lt;strong&gt;Production Efficiency&lt;/strong&gt;: Traditional video creation involves multiple specialists and lengthy workflows, creating barriers for independent creators and rapid prototyping.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé• &lt;strong&gt;Scaling AI Generated Video&lt;/strong&gt;: AI-generated videos are usually only a few seconds long, high-quality long videos at the minute or even hour level require complex cross-scene continuity and multi-storyboards design and processing capabilities.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ViMAX&lt;/strong&gt;: eliminates these production bottlenecks by automating the entire video creation pipeline from narrative input to final video output.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üî• &lt;strong&gt;Why ViMax?&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;üß† &lt;strong&gt;Effortless Production&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;üöÄ &lt;strong&gt;Complete Creative Freedom&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;üîä &lt;strong&gt;Audio and Video Binding&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;üé® &lt;strong&gt;Professional Quality&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;ü§© &lt;strong&gt;Interactive Video&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;One-Prompt to Finished Video&lt;/td&gt; 
   &lt;td align="center"&gt;From Any Narrative to Reality&lt;/td&gt; 
   &lt;td align="center"&gt;Synchronized Storytelling&lt;/td&gt; 
   &lt;td align="center"&gt;Movie-Grade Output&lt;/td&gt; 
   &lt;td align="center"&gt;Make Your Own Cameo Video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Skip the technical complexity‚Äîjust describe your vision and let ViMax handle script generation, storyboarding, shot design, reference management, and consistency validation&lt;/td&gt; 
   &lt;td align="center"&gt;No creative limits‚Äîwhether it's a trailer, short story, novel chapter, or original concept, ViMax intelligently structures narratives and designs cinematography to bring any idea to life&lt;/td&gt; 
   &lt;td align="center"&gt;Seamlessly integrate character voice, and sound effects with visual content to create immersive experiences where audio and video work in perfect harmony&lt;/td&gt; 
   &lt;td align="center"&gt;Automated quality control ensures character consistency, proper scene composition, and professional visual standards across every frame of your video&lt;/td&gt; 
   &lt;td align="center"&gt;Interact in your own short stories by uploading your photo‚ÄîViMax intelligently integrates you as a character with consistent appearance and natural interactions throughout the entire video&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;‚òÑÔ∏è &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üë®‚Äçüíª &lt;strong&gt;Google AI Studio API config‚úÖ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üìπ &lt;strong&gt;Dev mode branch&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ü§≥ &lt;strong&gt;AutoCameo integrate&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üì∫ &lt;strong&gt;More demos&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üéûÔ∏è &lt;strong&gt;Shot planning&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;New features&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;h3&gt;üìä &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;ViMax&lt;/strong&gt; is a multi-agent video framework that enables automated multi-shot video generation while ensuring character and scene consistency. Our system seamlessly translates your ideas into corresponding videos, allowing you to focus on storytelling rather than technical implementation.&lt;/p&gt; 
&lt;p&gt;üéØ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;üß¨ &lt;strong&gt;Intelligent Long Script Generation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;RAG-based long script design engine that intelligently analyzes lengthy, novel-like stories and automatically segments them into a multi-scene script format. The process meticulously ensures that all key plot developments and character dialogues are accurately retained within the new structure.&lt;/p&gt; 
&lt;p&gt;ü™Ñ &lt;strong&gt;Expressive Storyboard Design&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Shot-level storyboard design system that create expressive storyboards through cinematography language based on user requirements and target audiences, which establishs the narrative rhythm for subsequent video generation.&lt;/p&gt; 
&lt;p&gt;üîÆ &lt;strong&gt;Multi-camera Filming Simulation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Simulates multi-camera filming to deliver an immersive viewing experience while maintaining consistent character positioning and backgrounds within the same scene.&lt;/p&gt; 
&lt;p&gt;üß∏ &lt;strong&gt;Intelligent Reference Images Selection&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Intelligently select the reference image required for the first frame of the current video, including the storyboards that occurred in the previous timeline, to ensure the accuracy of multiple characters and environmental elements as the video becomes longer.&lt;/p&gt; 
&lt;p&gt;‚öôÔ∏è &lt;strong&gt;Automated Images Generation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Based on the selected reference image and the visual logical order on the previous timeline, the prompt of the image generator is automatically generated to reasonably arrange the spatial interaction position between the character and the environment.&lt;/p&gt; 
&lt;p&gt;‚úÖ &lt;strong&gt;Automated Image Generation Consistency Check&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Generate multiple images in parallel and select the best consistent image as the first frame through MLLM/VLM to imitate the workflow of human creators.&lt;/p&gt; 
&lt;p&gt;‚ö° &lt;strong&gt;High-efficiency Parallel Shot Generation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Parallel processing for sequential shots captured from the same camera enables highly efficient video production.&lt;/p&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Agent Video Generation Pipeline&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; üß† &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; üìù Idea &amp;amp; Scripts &amp;amp; Novels ‚Ä¢ üí≠ Natural Language Prompts ‚Ä¢ üñºÔ∏è Reference Images ‚Ä¢ üé® Style Directives ‚Ä¢ üß© Configs &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üß≠ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Agent Scheduling ‚Ä¢ Stage Transitions ‚Ä¢ Resource Management ‚Ä¢ Retry/Fallback Logic &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; üßæ &lt;strong&gt;SCRIPT UNDERSTANDING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Character/Environment Extraction ‚Ä¢ Scene Boundaries ‚Ä¢ Style Intent&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; üé• &lt;strong&gt;SCENE &amp;amp; SHOT PLANNING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Storyboard Steps ‚Ä¢ Shot List ‚Ä¢ Key Frames &amp;amp; Beats&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üß™ &lt;strong&gt;VISUAL ASSET PLANNING&lt;/strong&gt;&lt;br /&gt; Reference Image Selection ‚Ä¢ Look/Style Guidance ‚Ä¢ Prompt Conditioning &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;!-- Swapped: ASSET INDEXING is now on the left --&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; üóÇÔ∏è &lt;strong&gt;ASSET INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Frames/Refs Catalog ‚Ä¢ Embeddings ‚Ä¢ Retrieval for Reuse&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;!-- Swapped: CONSISTENCY &amp; CONTINUITY is now on the right --&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; ‚ôªÔ∏è &lt;strong&gt;CONSISTENCY &amp;amp; CONTINUITY&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Character/Environment Tracking ‚Ä¢ Ref Matching ‚Ä¢ Temporal Coherence&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ‚úÇÔ∏è &lt;strong&gt;VISUAL SYNTHESIS &amp;amp; ASSEMBLY&lt;/strong&gt;&lt;br /&gt; Image Generation ‚Ä¢ Best-Frame Selection ‚Ä¢ First/Last-Frame‚ÜíVideo ‚Ä¢ Cut &amp;amp; Timeline Assembly &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; üöÄ &lt;strong&gt;OUTPUT LAYER&lt;/strong&gt;&lt;br /&gt; üñºÔ∏è Frames ‚Ä¢ üéûÔ∏è Clips &amp;amp; Final Videos ‚Ä¢ üìú Logs ‚Ä¢ üì¶ Working Directory Artifacts &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üöÄQuick Start&lt;/h2&gt; 
&lt;h3&gt;üñ•Ô∏è &lt;strong&gt;Environment&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;OS: Linux, Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üì• &lt;strong&gt;Clone and Install&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We use uv to manage the environment. For uv installation, please refer to the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;https://docs.astral.sh/uv/getting-started/installation/&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/ViMax.git
cd ViMax
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Usage&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;main_idea2video.py is used to convert your ideas into videos. You need to configure the model and API key information in the configs/idea2video.yaml file, including three parts‚Äîthe chat model, the image generator, and the video generator, as shown below&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;chat_model:
  init_args:
    model: google/gemini-2.5-flash-lite-preview-09-2025
    model_provider: openai
    api_key: &amp;lt;YOUR_API_KEY&amp;gt;
    base_url: https://openrouter.ai/api/v1

image_generator:
  class_path: tools.ImageGeneratorNanobananaGoogleAPI
  init_args:
    api_key: &amp;lt;YOUR_API_KEY&amp;gt;

video_generator:
  class_path: tools.VideoGeneratorVeoGoogleAPI
  init_args:
    api_key: &amp;lt;YOUR_API_KEY&amp;gt;

working_dir: .working_dir/idea2video
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, provide a simple yet thoughtful idea and the corresponding creative requirements in main_idea2video.py.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;idea = \
"""
If a cat and a dog are best friends, what would happen when they meet a new cat?
"""
user_requirement = \
"""
For children, do not exceed 3 scenes.
"""
style = "Cartoon"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;main_script2video.py generates a video based on a specific script. You similarly need to set up the API configuration in configs/script2video.yaml file. Then, provide a scene script and the corresponding creative requirements in main_script2video.py, as shown below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;script = \
"""
EXT. SCHOOL GYM - DAY
A group of students are practicing basketball in the gym. The gym is large and open, with a basketball hoop at one end and a large crowd of spectators at the other end. John (18, male, tall, athletic) is the star player, and he is practicing his dribble and shot. Jane (17, female, short, athletic) is the assistant coach, and she is helping John with his practice. The other students are watching the practice and cheering for John.
John: (dribbling the ball) I'm going to score a basket!
Jane: (smiling) Good job, John!
John: (shooting the ball) Yes!
...
"""
user_requirement = \
"""
Fast-paced with no more than 20 shots.
"""
style = "Animate Style"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;üåü If this project helps you, please give us a Star!&lt;/strong&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt; ‚ù§Ô∏è Thanks for visiting ‚ú® ViMax!&lt;/em&gt;&lt;br /&gt;&lt;br /&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GetStream/Vision-Agents</title>
      <link>https://github.com/GetStream/Vision-Agents</link>
      <description>&lt;p&gt;Open Vision Agents by Stream. Build Vision Agents quickly with any model or video provider. Uses Stream's edge network for ultra-low latency.&lt;/p&gt;&lt;hr&gt;&lt;img width="1280" height="360" alt="Readme" src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/repo_image.png" /&gt; 
&lt;h1&gt;Open Vision Agents by Stream&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/GetStream/Vision-Agents/actions"&gt;&lt;img src="https://github.com/GetStream/Vision-Agents/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="build" /&gt;&lt;/a&gt; &lt;a href="http://badge.fury.io/py/vision-agents"&gt;&lt;img src="https://badge.fury.io/py/vision-agents.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/vision-agents.svg?sanitize=true" alt="PyPI - Python Version" /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/GetStream/Vision-Agents" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RkhX9PxMS6"&gt;&lt;img src="https://img.shields.io/discord/1108586339550638090" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Build Real-Time Vision AI Agents&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9778ab9-938d-4101-8605-ff879c29b0e4"&gt;https://github.com/user-attachments/assets/d9778ab9-938d-4101-8605-ff879c29b0e4&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Multi-modal AI agents that watch, listen, and understand video.&lt;/h3&gt; 
&lt;p&gt;Vision Agents give you the building blocks to create intelligent, low-latency video experiences powered by your models, your infrastructure, and your use cases.&lt;/p&gt; 
&lt;h3&gt;Key Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Video AI:&lt;/strong&gt; Built for real-time video AI. Combine YOLO, Roboflow, and others with Gemini/OpenAI in real-time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Low Latency:&lt;/strong&gt; Join quickly (500ms) and maintain audio/video latency under 30ms using &lt;a href="https://getstream.io/video/"&gt;Stream's edge network&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open:&lt;/strong&gt; Built by Stream, but works with any video edge network.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native APIs:&lt;/strong&gt; Native SDK methods from OpenAI (&lt;code&gt;create response&lt;/code&gt;), Gemini (&lt;code&gt;generate&lt;/code&gt;), and Claude ( &lt;code&gt;create message&lt;/code&gt;) ‚Äî always access the latest LLM capabilities.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SDKs:&lt;/strong&gt; SDKs for React, Android, iOS, Flutter, React Native, and Unity, powered by Stream's ultra-low-latency network.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d66587ea-7af4-40c4-9966-5c04fbcf467c"&gt;https://github.com/user-attachments/assets/d66587ea-7af4-40c4-9966-5c04fbcf467c&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;See It In Action&lt;/h2&gt; 
&lt;h3&gt;Sports Coaching&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d1258ac2-ca98-4019-80e4-41ec5530117e"&gt;https://github.com/user-attachments/assets/d1258ac2-ca98-4019-80e4-41ec5530117e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This example shows you how to build golf coaching AI with YOLO and Gemini Live. Combining a fast object detection model (like YOLO) with a full realtime AI is useful for many different video AI use cases. For example: Drone fire detection, sports/video game coaching, physical therapy, workout coaching, just dance style games etc.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# partial example, full example: examples/02_golf_coach_example/golf_coach_example.py
agent = Agent(
    edge=getstream.Edge(),
    agent_user=agent_user,
    instructions="Read @golf_coach.md",
    llm=gemini.Realtime(fps=10),
    # llm=openai.Realtime(fps=1), # Careful with FPS can get expensive
    processors=[ultralytics.YOLOPoseProcessor(model_path="yolo11n-pose.pt", device="cuda")],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Security Camera with Package Theft Detection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/92a2cdd8-909c-46d8-aab7-039a90efc186"&gt;https://github.com/user-attachments/assets/92a2cdd8-909c-46d8-aab7-039a90efc186&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This example shows a security camera system that detects faces, tracks packages and detects when a package is stolen. It automatically generates "WANTED" posters, posting them to X in real-time.&lt;/p&gt; 
&lt;p&gt;It combines face recognition, YOLOv11 object detection, Nano Banana and Gemini for a complete security workflow with voice interaction.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# partial example, full example: examples/04_security_camera_example/security_camera_example.py
security_processor = SecurityCameraProcessor(
    fps=5,
    model_path="weights_custom.pt",  # YOLOv11 for package detection
    package_conf_threshold=0.7,
)

agent = Agent(
    edge=getstream.Edge(),
    agent_user=User(name="Security AI", id="agent"),
    instructions="Read @instructions.md",
    processors=[security_processor],
    llm=gemini.LLM("gemini-2.5-flash-lite"),
    tts=elevenlabs.TTS(),
    stt=deepgram.STT(),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Cluely style Invisible Assistant (coming soon)&lt;/h3&gt; 
&lt;p&gt;Apps like Cluely offer realtime coaching via an invisible overlay. This example shows you how you can build your own invisible assistant. It combines Gemini realtime (to watch your screen and audio), and doesn't broadcast audio (only text). This approach is quite versatile and can be used for: Sales coaching, job interview cheating, physical world/ on the job coaching with glasses&lt;/p&gt; 
&lt;p&gt;Demo video&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;agent = Agent(
    edge=StreamEdge(),  # low latency edge. clients for React, iOS, Android, RN, Flutter etc.
    agent_user=agent_user,  # the user object for the agent (name, image etc)
    instructions="You are silently helping the user pass this interview. See @interview_coach.md",
    # gemini realtime, no need to set tts, or sst (though that's also supported)
    llm=gemini.Realtime()
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Step 1: Install via uv&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;uv add vision-agents&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 2: (Optional) Install with extra integrations&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;uv add "vision-agents[getstream, openai, elevenlabs, deepgram]"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 3: Obtain your Stream API credentials&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Get a free API key from &lt;a href="https://getstream.io/"&gt;Stream&lt;/a&gt;. Developers receive &lt;strong&gt;333,000 participant minutes&lt;/strong&gt; per month, plus extra credits via the Maker Program.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;True real-time via WebRTC&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Stream directly to model providers that support it for instant visual understanding.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Interval/processor pipeline&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;For providers without WebRTC, process frames with pluggable video processors (e.g., YOLO, Roboflow, or custom PyTorch/ONNX) before/after model calls.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Turn detection &amp;amp; diarization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Keep conversations natural; know when the agent should speak or stay quiet and who's talking.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice activity detection (VAD)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Trigger actions intelligently and use resources efficiently.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speech‚ÜîText‚ÜîSpeech&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Enable low-latency loops for smooth, conversational voice UX.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tool/function calling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Execute arbitrary code and APIs mid-conversation. Create Linear issues, query weather, trigger telephony, or hit internal services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Built-in memory via Stream Chat&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Agents recall context naturally across turns and sessions.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Text back-channel&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Message the agent silently during a call.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phone and RAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Interact with the Agent via inbound or outbound phone calls using Twilio and Turbopuffer&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Out-of-the-Box Integrations&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Plugin Name&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Docs Link&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Realtime speech-to-speech plugin using Amazon Nova models with automatic reconnection&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/aws-bedrock"&gt;AWS&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Polly&lt;/td&gt; 
   &lt;td&gt;TTS plugin using Amazon's cloud-based service with natural-sounding voices and neural engine support&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/aws-polly"&gt;AWS Polly&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Cartesia&lt;/td&gt; 
   &lt;td&gt;TTS plugin for realistic voice synthesis in real-time voice applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/cartesia"&gt;Cartesia&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Decart&lt;/td&gt; 
   &lt;td&gt;Real-time AI video transformation service for applying artistic styles and effects to video streams&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/decart"&gt;Decart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deepgram&lt;/td&gt; 
   &lt;td&gt;STT plugin for fast, accurate real-time transcription with speaker diarization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/deepgram"&gt;Deepgram&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;TTS plugin with highly realistic and expressive voices for conversational agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/elevenlabs"&gt;ElevenLabs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fast-Whisper&lt;/td&gt; 
   &lt;td&gt;High-performance STT plugin using OpenAI's Whisper model with CTranslate2 for fast inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/fast-whisper"&gt;Fast-Whisper&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fish Audio&lt;/td&gt; 
   &lt;td&gt;STT and TTS plugin with automatic language detection and voice cloning capabilities&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/fish"&gt;Fish Audio&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Realtime API for building conversational agents with support for both voice and video&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/gemini"&gt;Gemini&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HeyGen&lt;/td&gt; 
   &lt;td&gt;Realtime interactive avatars powered by &lt;a href="https://heygen.com/"&gt;HeyGen&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/heygen"&gt;HeyGen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hugging Face&lt;/td&gt; 
   &lt;td&gt;LLM plugin providing access to many open-source language models hosted on the Hugging Face Hub and powered by external providers (Cerebras, Together, Groq, etc.)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/huggingface"&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inworld&lt;/td&gt; 
   &lt;td&gt;TTS plugin with high-quality streaming voices for real-time conversational AI agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/inworld"&gt;Inworld&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kokoro&lt;/td&gt; 
   &lt;td&gt;Local TTS engine for offline voice synthesis with low latency&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/kokoro"&gt;Kokoro&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Moondream&lt;/td&gt; 
   &lt;td&gt;Moondream provides realtime detection and VLM capabilities. Developers can choose from using the hosted API or running locally on their CUDA devices. Vision Agents supports Moondream's Detect, Caption and VQA skills out-of-the-box.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/moondream"&gt;Moondream&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA Cosmos 2&lt;/td&gt; 
   &lt;td&gt;VLM plugin using NVIDIA's Cosmos 2 models for video understanding with automatic frame buffering and streaming responses&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/nvidia"&gt;NVIDIA&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;Realtime API for building conversational agents with out of the box support for real-time video directly over WebRTC, LLMs and Open AI TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/openai"&gt;OpenAI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;LLM plugin providing access to multiple providers (Anthropic, Google, OpenAI) through a unified API&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/openrouter"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen&lt;/td&gt; 
   &lt;td&gt;Realtime audio plugin using Alibaba's Qwen3 with native audio output and built-in speech recognition&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/qwen"&gt;Qwen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Roboflow&lt;/td&gt; 
   &lt;td&gt;Object detection processor using Roboflow's hosted API or local RF-DETR models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/roboflow"&gt;Roboflow&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Smart Turn&lt;/td&gt; 
   &lt;td&gt;Advanced turn detection system combining Silero VAD, Whisper, and neural models for natural conversation flow&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/smart-turn"&gt;Smart Turn&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TurboPuffer&lt;/td&gt; 
   &lt;td&gt;RAG plugin using TurboPuffer for hybrid search (vector + BM25) with Gemini embeddings for retrieval augmented generation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/guides/rag"&gt;TurboPuffer&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twilio&lt;/td&gt; 
   &lt;td&gt;Voice call integration plugin enabling bidirectional audio streaming via Twilio Media Streams with call registry and audio conversion&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/03_phone_and_rag_example"&gt;Twilio&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ultralytics&lt;/td&gt; 
   &lt;td&gt;Real-time pose detection processor using YOLO models with skeleton overlays&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/ultralytics"&gt;Ultralytics&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vogent&lt;/td&gt; 
   &lt;td&gt;Neural turn detection system for intelligent turn-taking in voice conversations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/vogent"&gt;Vogent&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wizper&lt;/td&gt; 
   &lt;td&gt;STT plugin with real-time translation capabilities powered by Whisper v3&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/wizper"&gt;Wizper&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;LLM plugin using xAI's Grok models with advanced reasoning and real-time knowledge&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/xai"&gt;xAI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Processors&lt;/h2&gt; 
&lt;p&gt;Processors let your agent &lt;strong&gt;manage state&lt;/strong&gt; and &lt;strong&gt;handle audio/video&lt;/strong&gt; in real-time.&lt;/p&gt; 
&lt;p&gt;They take care of the hard stuff, like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running smaller models&lt;/li&gt; 
 &lt;li&gt;Making API calls&lt;/li&gt; 
 &lt;li&gt;Transforming media&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚Ä¶ so you can focus on your agent logic.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Check out our getting started guide at &lt;a href="https://visionagents.ai/"&gt;VisionAgents.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Quickstart:&lt;/strong&gt; &lt;a href="https://visionagents.ai/introduction/voice-agents"&gt;Building a Voice AI app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quickstart:&lt;/strong&gt; &lt;a href="https://visionagents.ai/introduction/video-agents"&gt;Building a Video AI app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial:&lt;/strong&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/01_simple_agent_example"&gt;Building a real-time meeting assistant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial:&lt;/strong&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/02_golf_coach_example"&gt;Building real-time sports coaching&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;üîÆ Demo Applications&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Cartesia&lt;/h3&gt;Using Cartesia's Sonic 3 model to visually look at what's in the frame and tell a story with emotion.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-time visual understanding&lt;br /&gt;‚Ä¢ Emotional storytelling&lt;br /&gt;‚Ä¢ Frame-by-frame analysis&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/plugins/cartesia/example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/cartesia.gif" width="320" alt="Cartesia Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Realtime Stable Diffusion&lt;/h3&gt;Realtime stable diffusion using Vision Agents and Decart's Mirage 2 model to create interactive scenes and stories.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-time video restyling&lt;br /&gt;‚Ä¢ Interactive scene generation&lt;br /&gt;‚Ä¢ Stable diffusion integration&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/plugins/decart/example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/mirage.gif" width="320" alt="Mirage Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Golf Coach&lt;/h3&gt;Using Gemini Live together with Vision Agents and Ultralytics YOLO, we're able to track the user's pose and provide realtime actionable feedback on their golf game.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-time pose tracking&lt;br /&gt;‚Ä¢ Actionable coaching feedback&lt;br /&gt;‚Ä¢ YOLO pose detection&lt;br /&gt;‚Ä¢ Gemini Live integration&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/02_golf_coach_example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/golf.gif" width="320" alt="Golf Coach Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;GeoGuesser&lt;/h3&gt;Together with OpenAI Realtime and Vision Agents, we can take GeoGuesser to the next level by asking it to identify places in our real world surroundings.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-world location identification&lt;br /&gt;‚Ä¢ OpenAI Realtime integration&lt;br /&gt;‚Ä¢ Visual scene understanding&lt;br /&gt;&lt;br /&gt; &lt;a href="https://visionagents.ai/integrations/openai#openai-realtime"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/geoguesser.gif" width="320" alt="GeoGuesser Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Phone and RAG&lt;/h3&gt;Interact with your Agent over the phone using Twilio. This example demonstrates how to use TurboPuffer for Retrieval Augmented Generation (RAG) to give your agent specialized knowledge.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Inbound/Outbound telephony&lt;br /&gt;‚Ä¢ Twilio Media Streams integration&lt;br /&gt;‚Ä¢ Vector search with TurboPuffer&lt;br /&gt;‚Ä¢ Retrieval Augmented Generation&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/03_phone_and_rag_example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/va_phone.png" width="320" alt="Phone and RAG Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Security Camera&lt;/h3&gt;A security camera with face recognition, package detection and automated theft response. Generates WANTED posters with Nano Banana and posts them to X when packages disappear.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Face detection &amp;amp; named recognition&lt;br /&gt;‚Ä¢ YOLOv11 package detection&lt;br /&gt;‚Ä¢ Automated WANTED poster generation&lt;br /&gt;‚Ä¢ Real-time X posting&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/04_security_camera_example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/security_camera.gif" width="320" alt="Security Camera Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/DEVELOPMENT.md"&gt;DEVELOPMENT.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Open Platform&lt;/h2&gt; 
&lt;p&gt;Want to add your platform or provider? Reach out to &lt;strong&gt;&lt;a href="mailto:nash@getstream.io"&gt;nash@getstream.io&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Awesome Video AI&lt;/h2&gt; 
&lt;p&gt;Our favorite people &amp;amp; projects to follow for vision AI&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/demishassabis"&gt;&lt;img src="https://github.com/user-attachments/assets/9149e871-cfe8-4169-a4ce-4073417e645c" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/OfficialLoganK"&gt;&lt;img src="https://github.com/user-attachments/assets/2e1335d3-58af-4988-b879-1db8d862cd34" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/ultralytics"&gt;&lt;img src="https://github.com/user-attachments/assets/c9249ae9-e66a-4a70-9393-f6fe4ab5c0b0" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/demishassabis"&gt;@demishassabis&lt;/a&gt;&lt;br /&gt;CEO @ Google DeepMind&lt;br /&gt;&lt;sub&gt;Won a Nobel prize&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/OfficialLoganK"&gt;@OfficialLoganK&lt;/a&gt;&lt;br /&gt;Product Lead @ Gemini&lt;br /&gt;&lt;sub&gt;Posts about robotics vision&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/ultralytics"&gt;@ultralytics&lt;/a&gt;&lt;br /&gt;Various fast vision AI models&lt;br /&gt;&lt;sub&gt;Pose, detect, segment, classify&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/skalskip92"&gt;&lt;img src="https://github.com/user-attachments/assets/c1fe873d-6f41-4155-9be1-afc287ca9ac7" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/moondreamai"&gt;&lt;img src="https://github.com/user-attachments/assets/43359165-c23d-4d5d-a5a6-1de58d71fabd" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/kwindla"&gt;&lt;img src="https://github.com/user-attachments/assets/490d349c-7152-4dfb-b705-04e57bb0a4ca" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/skalskip92"&gt;@skalskip92&lt;/a&gt;&lt;br /&gt;Open Source Lead @ Roboflow&lt;br /&gt;&lt;sub&gt;Building tools for vision AI&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/moondreamai"&gt;@moondreamai&lt;/a&gt;&lt;br /&gt;The tiny vision model that could&lt;br /&gt;&lt;sub&gt;Lightweight, fast, efficient&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/kwindla"&gt;@kwindla&lt;/a&gt;&lt;br /&gt;Pipecat / Daily&lt;br /&gt;&lt;sub&gt;Sharing AI and vision insights&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/juberti"&gt;&lt;img src="https://github.com/user-attachments/assets/d7ade584-781f-4dac-95b8-1acc6db4a7c4" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/romainhuet"&gt;&lt;img src="https://github.com/user-attachments/assets/00a1ed37-3620-426d-b47d-07dd59c19b28" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/thorwebdev"&gt;&lt;img src="https://github.com/user-attachments/assets/eb5928c7-83b9-4aaa-854f-1d4f641426f2" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/juberti"&gt;@juberti&lt;/a&gt;&lt;br /&gt;Head of Realtime AI @ OpenAI&lt;br /&gt;&lt;sub&gt;Realtime AI systems&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/romainhuet"&gt;@romainhuet&lt;/a&gt;&lt;br /&gt;Head of DX @ OpenAI&lt;br /&gt;&lt;sub&gt;Developer tooling &amp;amp; APIs&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/thorwebdev"&gt;@thorwebdev&lt;/a&gt;&lt;br /&gt;Eleven Labs&lt;br /&gt;&lt;sub&gt;Voice and AI experiments&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/mervenoyann"&gt;&lt;img src="https://github.com/user-attachments/assets/ab5ef918-7c97-4c6d-be10-2e2aeefec015" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/stash_pomichter"&gt;&lt;img src="https://github.com/user-attachments/assets/af936e13-22cf-4000-a35b-bfe30d44c320" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/Mentraglass"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1893061651152121856/Op4W8mza_400x400.jpg" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/mervenoyann"&gt;@mervenoyann&lt;/a&gt;&lt;br /&gt;Hugging Face&lt;br /&gt;&lt;sub&gt;Posts extensively about Video AI&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/stash_pomichter"&gt;@stash_pomichter&lt;/a&gt;&lt;br /&gt;Spatial memory for robots&lt;br /&gt;&lt;sub&gt;Robotics &amp;amp; AI navigation&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/Mentraglass"&gt;@Mentraglass&lt;/a&gt;&lt;br /&gt;Open-source smart glasses&lt;br /&gt;&lt;sub&gt;Open-Source, hackable AR glasses with AI capabilities built in&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/vikhyatk"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1664559115581145088/UMD1vtMw_400x400.jpg" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/vikhyatk"&gt;@vikhyatk&lt;/a&gt;&lt;br /&gt;AI Engineer&lt;br /&gt;&lt;sub&gt;Open-source AI projects, Creator of Moondream AI&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Inspiration&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Livekit Agents: Great syntax, Livekit only&lt;/li&gt; 
 &lt;li&gt;Pipecat: Flexible, but more verbose.&lt;/li&gt; 
 &lt;li&gt;OpenAI Agents: Focused on openAI only&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;h3&gt;0.1 ‚Äì First Release - Oct&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Working TTS, Gemini &amp;amp; OpenAI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;0.2 - Simplification - Nov&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Simplified the library &amp;amp; improved code quality&lt;/li&gt; 
 &lt;li&gt;Deepgram Nova 3, Elevenlabs Scribe 2, Fish, Moondream, QWen3, Smart turn, Vogent, Inworld, Heygen, AWS and more&lt;/li&gt; 
 &lt;li&gt;Improved openAI &amp;amp; Gemini realtime performance&lt;/li&gt; 
 &lt;li&gt;Audio &amp;amp; Video utilities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;0.3 - Examples and Deploys - Jan&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Production-grade HTTP API for agent deployment (&lt;code&gt;uv run &amp;lt;agent.py&amp;gt; serve&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Metrics &amp;amp; Observability stack&lt;/li&gt; 
 &lt;li&gt;Phone/voice integration with RAG capabilities&lt;/li&gt; 
 &lt;li&gt;10 new LLM plugins (&lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/aws"&gt;AWS Nova 2&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/qwen"&gt;Qwen 3 Realtime&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/nvidia"&gt;NVIDIA Cosmos 2&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/pocket"&gt;Pocket TTS&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/deepgram"&gt;Deepgram TTS&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/openrouter"&gt;OpenRouter&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/huggingface"&gt;HuggingFace Inference&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/roboflow"&gt;Roboflow&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/twilio"&gt;Twilio&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/turbopuffer"&gt;Turbopuffer&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Real-world examples (&lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/05_security_camera_example"&gt;security camera&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/03_phone_and_rag_example"&gt;phone integration&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/04_football_commentator_example"&gt;football commentator&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/07_deploy_example"&gt;Docker deployment with GPU support&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/08_agent_server_example"&gt;agent server&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Stability: Fixes for participant sync, video frame handling, agent lifecycle, and screen sharing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;0.4 Documentation/polish&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Excellence on documentation/polish&lt;/li&gt; 
 &lt;li&gt;Better Roboflow annotation docs&lt;/li&gt; 
 &lt;li&gt;Automated workflows for maintenance&lt;/li&gt; 
 &lt;li&gt;Local camera/audio support AND/OR WebRTC connection&lt;/li&gt; 
 &lt;li&gt;Embedded/robotics examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Vision AI limitations&lt;/h2&gt; 
&lt;p&gt;Video AI is the frontier of AI. The state of the art is changing daily to help models understand live video. While building the integrations, here are the limitations we've noticed (Dec 2025)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video AI struggles with small text. If you want the AI to read the score in a game it will often get it wrong and hallucinate&lt;/li&gt; 
 &lt;li&gt;Longer videos can cause the AI to lose context. For instance if it's watching a soccer match it will get confused after 30 seconds&lt;/li&gt; 
 &lt;li&gt;Most applications require a combination of small specialized models like Yolo/Roboflow/Moondream, API calls to get more context and larger models like gemini/openAI&lt;/li&gt; 
 &lt;li&gt;Image size &amp;amp; FPS need to stay relatively low due to performance constraints&lt;/li&gt; 
 &lt;li&gt;Video doesn‚Äôt trigger responses in realtime models. You always need to send audio/text to trigger a response.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;We are hiring&lt;/h2&gt; 
&lt;p&gt;Join the team behind this project - we‚Äôre hiring a Staff Python Engineer to architect, build, and maintain a powerful toolkit for developers integrating voice and video AI into their products.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://jobs.ashbyhq.com/stream/3bea7dba-54e1-4c71-aa02-712a075842df?utm_source=Jmv9QOkznl"&gt;Apply here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#GetStream/vision-agents&amp;amp;type=timeline&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=GetStream/vision-agents&amp;amp;type=timeline&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Blaizzy/mlx-audio</title>
      <link>https://github.com/Blaizzy/mlx-audio</link>
      <description>&lt;p&gt;A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX-Audio&lt;/h1&gt; 
&lt;p&gt;The best audio processing library built on Apple's MLX framework, providing fast and efficient text-to-speech (TTS), speech-to-text (STT), and speech-to-speech (STS) on Apple Silicon.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fast inference optimized for Apple Silicon (M series chips)&lt;/li&gt; 
 &lt;li&gt;Multiple model architectures for TTS, STT, and STS&lt;/li&gt; 
 &lt;li&gt;Multilingual support across models&lt;/li&gt; 
 &lt;li&gt;Voice customization and cloning capabilities&lt;/li&gt; 
 &lt;li&gt;Adjustable speech speed control&lt;/li&gt; 
 &lt;li&gt;Interactive web interface with 3D audio visualization&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible REST API&lt;/li&gt; 
 &lt;li&gt;Quantization support (3-bit, 4-bit, 6-bit, 8-bit, and more) for optimized performance&lt;/li&gt; 
 &lt;li&gt;Swift package for iOS/macOS integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Using pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mlx-audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using uv to install only the command line tools&lt;/h3&gt; 
&lt;p&gt;Latest release from pypi:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force mlx-audio --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Latest code from github:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force git+https://github.com/Blaizzy/mlx-audio.git --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For development or web interface:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Blaizzy/mlx-audio.git
cd mlx-audio
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic TTS generation
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello, world!' --lang_code a

# With voice selection and speed adjustment
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --voice af_heart --speed 1.2 --lang_code a

# Play audio immediately
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --play  --lang_code a

# Save to a specific directory
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --output_path ./my_audio  --lang_code a
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

# Load model
model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate speech
for result in model.generate("Hello from MLX-Audio!", voice="af_heart"):
    print(f"Generated {result.audio.shape[0]} samples")
    # result.audio contains the waveform as mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;h3&gt;Text-to-Speech (TTS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Kokoro&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fast, high-quality multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, JA, ZH, FR, ES, IT, PT, HI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Kokoro-82M-bf16"&gt;mlx-community/Kokoro-82M-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alibaba's multilingual TTS with voice design&lt;/td&gt; 
   &lt;td&gt;ZH, EN, JA, KO, + more&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16"&gt;mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CSM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational Speech Model with voice cloning&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/csm-1b"&gt;mlx-community/csm-1b&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Dia&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dialogue-focused TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Dia-1.6B-bf16"&gt;mlx-community/Dia-1.6B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OuteTTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient TTS model&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/OuteTTS-0.2-500M"&gt;mlx-community/OuteTTS-0.2-500M&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;SparkTTS model&lt;/td&gt; 
   &lt;td&gt;EN, ZH&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/SparkTTS-0.5B-bf16"&gt;mlx-community/SparkTTS-0.5B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chatterbox&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Expressive multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, ES, FR, DE, IT, PT, PL, TR, RU, NL, CS, AR, ZH, JA, HU, KO&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Chatterbox-bf16"&gt;mlx-community/Chatterbox-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Soprano&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;High-quality TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Soprano-bf16"&gt;mlx-community/Soprano-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Text (STT)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI's robust STT model&lt;/td&gt; 
   &lt;td&gt;99+ languages&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/whisper-large-v3-turbo-asr-fp16"&gt;mlx-community/whisper-large-v3-turbo-asr-fp16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-ASR&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alibaba's multilingual ASR&lt;/td&gt; 
   &lt;td&gt;ZH, EN, JA, KO, + more&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-ASR-1.7B-8bit"&gt;mlx-community/Qwen3-ASR-1.7B-8bit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-ForcedAligner&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Word-level audio alignment&lt;/td&gt; 
   &lt;td&gt;ZH, EN, JA, KO, + more&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-ForcedAligner-0.6B-8bit"&gt;mlx-community/Qwen3-ForcedAligner-0.6B-8bit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Parakeet&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NVIDIA's accurate STT&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2"&gt;mlx-community/parakeet-tdt-0.6b-v2&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voxtral&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Mistral's speech model&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16"&gt;mlx-community/Voxtral-Mini-3B-2507-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Microsoft's 9B ASR with diarization &amp;amp; timestamps&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/VibeVoice-ASR-bf16"&gt;mlx-community/VibeVoice-ASR-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Speech (STS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SAM-Audio&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text-guided source separation&lt;/td&gt; 
   &lt;td&gt;Extract specific sounds&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/sam-audio-large"&gt;mlx-community/sam-audio-large&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Liquid2.5-Audio&lt;/strong&gt;*&lt;/td&gt; 
   &lt;td&gt;Speech-to-Speech, Text-to-Speech and Speech-to-Text&lt;/td&gt; 
   &lt;td&gt;Speech interactions&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-8bit"&gt;mlx-community/LFM2.5-Audio-1.5B-8bit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MossFormer2 SE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Speech enhancement&lt;/td&gt; 
   &lt;td&gt;Noise removal&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/starkdmi/MossFormer2_SE_48K_MLX"&gt;starkdmi/MossFormer2_SE_48K_MLX&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Examples&lt;/h2&gt; 
&lt;h3&gt;Kokoro TTS&lt;/h3&gt; 
&lt;p&gt;Kokoro is a fast, multilingual TTS model with 54 voice presets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate with different voices
for result in model.generate(
    text="Welcome to MLX-Audio!",
    voice="af_heart",  # American female
    speed=1.0,
    lang_code="a"  # American English
):
    audio = result.audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Voices:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;American English: &lt;code&gt;af_heart&lt;/code&gt;, &lt;code&gt;af_bella&lt;/code&gt;, &lt;code&gt;af_nova&lt;/code&gt;, &lt;code&gt;af_sky&lt;/code&gt;, &lt;code&gt;am_adam&lt;/code&gt;, &lt;code&gt;am_echo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;British English: &lt;code&gt;bf_alice&lt;/code&gt;, &lt;code&gt;bf_emma&lt;/code&gt;, &lt;code&gt;bm_daniel&lt;/code&gt;, &lt;code&gt;bm_george&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Japanese: &lt;code&gt;jf_alpha&lt;/code&gt;, &lt;code&gt;jm_kumo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Chinese: &lt;code&gt;zf_xiaobei&lt;/code&gt;, &lt;code&gt;zm_yunxi&lt;/code&gt;, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Language Codes:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Code&lt;/th&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;a&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;American English&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;b&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;British English&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;j&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Japanese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[ja]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;z&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mandarin Chinese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[zh]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Spanish&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;f&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;French&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Qwen3-TTS&lt;/h3&gt; 
&lt;p&gt;Alibaba's state-of-the-art multilingual TTS with voice cloning, emotion control, and voice design capabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16")
results = list(model.generate(
    text="Hello, welcome to MLX-Audio!",
    voice="Chelsie",
    language="English",
))

audio = results[0].audio  # mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/mlx_audio/tts/models/qwen3_tts/README.md"&gt;Qwen3-TTS README&lt;/a&gt; for voice cloning, CustomVoice, VoiceDesign, and all available models.&lt;/p&gt; 
&lt;h3&gt;CSM (Voice Cloning)&lt;/h3&gt; 
&lt;p&gt;Clone any voice using a reference audio sample:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_audio.tts.generate \
    --model mlx-community/csm-1b \
    --text "Hello from Sesame." \
    --ref_audio ./reference_voice.wav \
    --play
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Whisper STT&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.generate import generate_transcription

result = generate_transcription(
    model="mlx-community/whisper-large-v3-turbo-asr-fp16",
    audio="audio.wav",
)
print(result.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qwen3-ASR &amp;amp; ForcedAligner&lt;/h3&gt; 
&lt;p&gt;Alibaba's multilingual speech models for transcription and word-level alignment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt import load

# Speech recognition
model = load("mlx-community/Qwen3-ASR-0.6B-8bit")
result = model.generate("audio.wav", language="English")
print(result.text)

# Word-level forced alignment
aligner = load("mlx-community/Qwen3-ForcedAligner-0.6B-8bit")
result = aligner.generate("audio.wav", text="I have a dream", language="English")
for item in result:
    print(f"[{item.start_time:.2f}s - {item.end_time:.2f}s] {item.text}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/mlx_audio/stt/models/qwen3_asr/README.md"&gt;Qwen3-ASR README&lt;/a&gt; for CLI usage, all models, and more examples.&lt;/p&gt; 
&lt;h3&gt;VibeVoice-ASR&lt;/h3&gt; 
&lt;p&gt;Microsoft's 9B parameter speech-to-text model with speaker diarization and timestamps. Supports long-form audio (up to 60 minutes) and outputs structured JSON.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.utils import load

model = load("mlx-community/VibeVoice-ASR-bf16")

# Basic transcription
result = model.generate(audio="meeting.wav", max_tokens=8192, temperature=0.0)
print(result.text)
# [{"Start":0,"End":5.2,"Speaker":0,"Content":"Hello everyone, let's begin."},
#  {"Start":5.5,"End":9.8,"Speaker":1,"Content":"Thanks for joining today."}]

# Access parsed segments
for seg in result.segments:
    print(f"[{seg['start_time']:.1f}-{seg['end_time']:.1f}] Speaker {seg['speaker_id']}: {seg['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Streaming transcription:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Stream tokens as they are generated
for text in model.stream_transcribe(audio="speech.wav", max_tokens=4096):
    print(text, end="", flush=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With context (hotwords/metadata):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = model.generate(
    audio="technical_talk.wav",
    context="MLX, Apple Silicon, PyTorch, Transformer",
    max_tokens=8192,
    temperature=0.0,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CLI usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic transcription
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio meeting.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --verbose

# With context/hotwords
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio technical_talk.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --context "MLX, Apple Silicon, PyTorch, Transformer" \
    --verbose
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;SAM-Audio (Source Separation)&lt;/h3&gt; 
&lt;p&gt;Separate specific sounds from audio using text prompts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import SAMAudio, SAMAudioProcessor, save_audio

model = SAMAudio.from_pretrained("mlx-community/sam-audio-large")
processor = SAMAudioProcessor.from_pretrained("mlx-community/sam-audio-large")

batch = processor(
    descriptions=["A person speaking"],
    audios=["mixed_audio.wav"],
)

result = model.separate_long(
    batch.audios,
    descriptions=batch.descriptions,
    anchors=batch.anchor_ids,
    chunk_seconds=10.0,
    overlap_seconds=3.0,
    ode_opt={"method": "midpoint", "step_size": 2/32},
)

save_audio(result.target[0], "voice.wav")
save_audio(result.residual[0], "background.wav")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MossFormer2 (Speech Enhancement)&lt;/h3&gt; 
&lt;p&gt;Remove noise from speech recordings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import MossFormer2SEModel, save_audio

model = MossFormer2SEModel.from_pretrained("starkdmi/MossFormer2_SE_48K_MLX")
enhanced = model.enhance("noisy_speech.wav")
save_audio(enhanced, "clean.wav", 48000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Web Interface &amp;amp; API Server&lt;/h2&gt; 
&lt;p&gt;MLX-Audio includes a modern web interface and OpenAI-compatible API.&lt;/p&gt; 
&lt;h3&gt;Starting the Server&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start API server
mlx_audio.server --host 0.0.0.0 --port 8000

# Start web UI (in another terminal)
cd mlx_audio/ui
npm install &amp;amp;&amp;amp; npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;API Endpoints&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; (OpenAI-compatible):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{"model": "mlx-community/Kokoro-82M-bf16", "input": "Hello!", "voice": "af_heart"}' \
  --output speech.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.wav" \
  -F "model=mlx-community/whisper-large-v3-turbo-asr-fp16"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quantization&lt;/h2&gt; 
&lt;p&gt;Reduce model size and improve performance with quantization using the convert script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert and quantize to 4-bit
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-4bit \
    --quantize \
    --q-bits 4 \
    --upload-repo username/Kokoro-82M-4bit (optional: if you want to upload the model to Hugging Face)

# Convert with specific dtype (bfloat16)
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-bf16 \
    --dtype bfloat16 \
    --upload-repo username/Kokoro-82M-bf16 (optional: if you want to upload the model to Hugging Face)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--hf-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source Hugging Face model or local path&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--mlx-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Output directory for converted model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-q, --quantize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable quantization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-bits&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bits per weight (4, 6, or 8)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-group-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group size for quantization (default: 64)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--dtype&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Weight dtype: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;bfloat16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--upload-repo&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Upload converted model to HF Hub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Swift&lt;/h2&gt; 
&lt;p&gt;Looking for Swift/iOS support? Check out &lt;a href="https://github.com/Blaizzy/mlx-audio-swift"&gt;mlx-audio-swift&lt;/a&gt; for on-device TTS using MLX on macOS and iOS.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Apple Silicon Mac (M1/M2/M3/M4)&lt;/li&gt; 
 &lt;li&gt;MLX framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ffmpeg&lt;/strong&gt; (required for MP3/FLAC audio encoding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing ffmpeg&lt;/h3&gt; 
&lt;p&gt;ffmpeg is required for saving audio in MP3 or FLAC format. Install it using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS (using Homebrew)
brew install ffmpeg

# Ubuntu/Debian
sudo apt install ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;WAV format works without ffmpeg.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mlx-audio,
  author = {Canuma, Prince},
  title = {MLX Audio},
  year = {2025},
  howpublished = {\url{https://github.com/Blaizzy/mlx-audio}},
  note = {Audio processing library for Apple Silicon with TTS, STT, and STS capabilities.}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx"&gt;Apple MLX Team&lt;/a&gt; for the MLX framework&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>yt-dlp/yt-dlp</title>
      <link>https://github.com/yt-dlp/yt-dlp</link>
      <description>&lt;p&gt;A feature-rich command-line audio/video downloader&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#readme"&gt;&lt;img src="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg?sanitize=true" alt="YT-DLP" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#installation" title="Installation"&gt;&lt;img src="https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;amp;label=Download&amp;amp;style=for-the-badge" alt="Release version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/yt-dlp" title="PyPI"&gt;&lt;img src="https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;amp;labelColor=555555&amp;amp;style=for-the-badge" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/Maintainers.md#maintainers" title="Donate"&gt;&lt;img src="https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;amp;labelColor=555555&amp;amp;style=for-the-badge" alt="Donate" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/H5MNcFW63r" title="Discord"&gt;&lt;img src="https://img.shields.io/discord/807245652072857610?color=blue&amp;amp;labelColor=555555&amp;amp;label=&amp;amp;logo=discord&amp;amp;style=for-the-badge" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/supportedsites.md" title="Supported Sites"&gt;&lt;img src="https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge" alt="Supported Sites" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/LICENSE" title="License"&gt;&lt;img src="https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge" alt="License: Unlicense" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/actions" title="CI Status"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;amp;label=Tests&amp;amp;style=for-the-badge" alt="CI Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/commits" title="Commit History"&gt;&lt;img src="https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;amp;style=for-the-badge" alt="Commits" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/pulse/monthly" title="Last activity"&gt;&lt;img src="https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;amp;style=for-the-badge&amp;amp;display_timestamp=committer" alt="Last Commit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;yt-dlp is a feature-rich command-line audio/video downloader with support for &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/supportedsites.md"&gt;thousands of sites&lt;/a&gt;. The project is a fork of &lt;a href="https://github.com/ytdl-org/youtube-dl"&gt;youtube-dl&lt;/a&gt; based on the now inactive &lt;a href="https://github.com/blackjack4494/yt-dlc"&gt;youtube-dlc&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- MANPAGE: MOVE "USAGE AND OPTIONS" SECTION HERE --&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#installation"&gt;INSTALLATION&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation"&gt;Detailed instructions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#release-files"&gt;Release Files&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#update"&gt;Update&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#dependencies"&gt;Dependencies&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#compile"&gt;Compile&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#usage-and-options"&gt;USAGE AND OPTIONS&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#general-options"&gt;General Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#network-options"&gt;Network Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#geo-restriction"&gt;Geo-restriction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#video-selection"&gt;Video Selection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#download-options"&gt;Download Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#filesystem-options"&gt;Filesystem Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#thumbnail-options"&gt;Thumbnail Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#internet-shortcut-options"&gt;Internet Shortcut Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#verbosity-and-simulation-options"&gt;Verbosity and Simulation Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#workarounds"&gt;Workarounds&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#video-format-options"&gt;Video Format Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#subtitle-options"&gt;Subtitle Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#authentication-options"&gt;Authentication Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#post-processing-options"&gt;Post-processing Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sponsorblock-options"&gt;SponsorBlock Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#extractor-options"&gt;Extractor Options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#preset-aliases"&gt;Preset Aliases&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;CONFIGURATION&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration-file-encoding"&gt;Configuration file encoding&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#authentication-with-netrc"&gt;Authentication with netrc&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#notes-about-environment-variables"&gt;Notes about environment variables&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;OUTPUT TEMPLATE&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template-examples"&gt;Output template examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection"&gt;FORMAT SELECTION&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#filtering-formats"&gt;Filtering Formats&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats"&gt;Sorting Formats&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection-examples"&gt;Format Selection examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#modifying-metadata"&gt;MODIFYING METADATA&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#modifying-metadata-examples"&gt;Modifying metadata examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#extractor-arguments"&gt;EXTRACTOR ARGUMENTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#plugins"&gt;PLUGINS&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#installing-plugins"&gt;Installing Plugins&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#developing-plugins"&gt;Developing Plugins&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#embedding-yt-dlp"&gt;EMBEDDING YT-DLP&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#embedding-examples"&gt;Embedding examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#changes-from-youtube-dl"&gt;CHANGES FROM YOUTUBE-DL&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#new-features"&gt;New features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#differences-in-default-behavior"&gt;Differences in default behavior&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#deprecated-options"&gt;Deprecated options&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#contributing-to-yt-dlp"&gt;CONTRIBUTING&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#opening-an-issue"&gt;Opening an Issue&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#developer-instructions"&gt;Developer Instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/wiki"&gt;WIKI&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/FAQ"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;h1&gt;INSTALLATION&lt;/h1&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;p&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe"&gt;&lt;img src="https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;amp;logo=windows" alt="Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp"&gt;&lt;img src="https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;amp;logo=linux" alt="Unix" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos"&gt;&lt;img src="https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;amp;logo=apple" alt="MacOS" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/yt-dlp"&gt;&lt;img src="https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;amp;labelColor=555555&amp;amp;style=for-the-badge" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz"&gt;&lt;img src="https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge" alt="Source Tarball" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#release-files"&gt;&lt;img src="https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge" alt="Other variants" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yt-dlp/yt-dlp/releases"&gt;&lt;img src="https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge" alt="All versions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;You can install yt-dlp using &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#release-files"&gt;the binaries&lt;/a&gt;, &lt;a href="https://pypi.org/project/yt-dlp"&gt;pip&lt;/a&gt; or one using a third-party package manager. See &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation"&gt;the wiki&lt;/a&gt; for detailed instructions&lt;/p&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;h2&gt;RELEASE FILES&lt;/h2&gt; 
&lt;h4&gt;Recommended&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;File&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp"&gt;yt-dlp&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Platform-independent &lt;a href="https://docs.python.org/3/library/zipimport.html"&gt;zipimport&lt;/a&gt; binary. Needs Python (recommended for &lt;strong&gt;Linux/BSD&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe"&gt;yt-dlp.exe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Windows (Win8+) standalone x64 binary (recommended for &lt;strong&gt;Windows&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos"&gt;yt-dlp_macos&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Universal MacOS (10.15+) standalone executable (recommended for &lt;strong&gt;MacOS&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Alternatives&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;File&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux"&gt;yt-dlp_linux&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Linux (glibc 2.17+) standalone x86_64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip"&gt;yt-dlp_linux.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (glibc 2.17+) x86_64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64"&gt;yt-dlp_linux_aarch64&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Linux (glibc 2.17+) standalone aarch64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64.zip"&gt;yt-dlp_linux_aarch64.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (glibc 2.17+) aarch64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l.zip"&gt;yt-dlp_linux_armv7l.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (glibc 2.31+) armv7l executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux"&gt;yt-dlp_musllinux&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Linux (musl 1.2+) standalone x86_64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux.zip"&gt;yt-dlp_musllinux.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (musl 1.2+) x86_64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64"&gt;yt-dlp_musllinux_aarch64&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Linux (musl 1.2+) standalone aarch64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64.zip"&gt;yt-dlp_musllinux_aarch64.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Linux (musl 1.2+) aarch64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe"&gt;yt-dlp_x86.exe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Windows (Win8+) standalone x86 (32-bit) binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_x86.zip"&gt;yt-dlp_win_x86.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Windows (Win8+) x86 (32-bit) executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_arm64.exe"&gt;yt-dlp_arm64.exe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Windows (Win10+) standalone ARM64 binary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_arm64.zip"&gt;yt-dlp_win_arm64.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Windows (Win10+) ARM64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip"&gt;yt-dlp_win.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged Windows (Win8+) x64 executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip"&gt;yt-dlp_macos.zip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Unpackaged MacOS (10.15+) executable (no auto-update)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Misc&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;File&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz"&gt;yt-dlp.tar.gz&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Source tarball&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS"&gt;SHA2-512SUMS&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;GNU-style SHA512 sums&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig"&gt;SHA2-512SUMS.sig&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;GPG signature file for SHA512 sums&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS"&gt;SHA2-256SUMS&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;GNU-style SHA256 sums&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig"&gt;SHA2-256SUMS.sig&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;GPG signature file for SHA256 sums&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The public key that can be used to verify the GPG signatures is &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/public.key"&gt;available here&lt;/a&gt; Example usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Licensing&lt;/h4&gt; 
&lt;p&gt;While yt-dlp is licensed under the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/LICENSE"&gt;Unlicense&lt;/a&gt;, many of the release files contain code from other projects with different licenses.&lt;/p&gt; 
&lt;p&gt;Most notably, the PyInstaller-bundled executables include GPLv3+ licensed code, and as such the combined work is licensed under &lt;a href="https://www.gnu.org/licenses/gpl-3.0.html"&gt;GPLv3+&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The zipimport Unix executable (&lt;code&gt;yt-dlp&lt;/code&gt;) contains &lt;a href="https://github.com/meriyah/meriyah/raw/main/LICENSE.md"&gt;ISC&lt;/a&gt; licensed code from &lt;a href="https://github.com/meriyah/meriyah"&gt;&lt;code&gt;meriyah&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/davidbonnet/astring/raw/main/LICENSE"&gt;MIT&lt;/a&gt; licensed code from &lt;a href="https://github.com/davidbonnet/astring"&gt;&lt;code&gt;astring&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/THIRD_PARTY_LICENSES.txt"&gt;THIRD_PARTY_LICENSES.txt&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;The git repository, the source tarball (&lt;code&gt;yt-dlp.tar.gz&lt;/code&gt;), the PyPI source distribution and the PyPI built distribution (wheel) only contain code licensed under the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/LICENSE"&gt;Unlicense&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The manpages, shell completion (autocomplete) files etc. are available inside the &lt;a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz"&gt;source tarball&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;UPDATE&lt;/h2&gt; 
&lt;p&gt;You can use &lt;code&gt;yt-dlp -U&lt;/code&gt; to update if you are using the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#release-files"&gt;release binaries&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip"&gt;installed with pip&lt;/a&gt;, simply re-run the same command that was used to install the program&lt;/p&gt; 
&lt;p&gt;For other third-party package managers, see &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers"&gt;the wiki&lt;/a&gt; or refer to their documentation&lt;/p&gt; 
&lt;p&gt;&lt;a id="update-channels"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;There are currently three release channels for binaries: &lt;code&gt;stable&lt;/code&gt;, &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;master&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt; is the default channel, and many of its changes have been tested by users of the &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;master&lt;/code&gt; channels.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;nightly&lt;/code&gt; channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project's new patches and changes. This is the &lt;strong&gt;recommended channel for regular users&lt;/strong&gt; of yt-dlp. The &lt;code&gt;nightly&lt;/code&gt; releases are available from &lt;a href="https://github.com/yt-dlp/yt-dlp-nightly-builds/releases"&gt;yt-dlp/yt-dlp-nightly-builds&lt;/a&gt; or as development releases of the &lt;code&gt;yt-dlp&lt;/code&gt; PyPI package (which can be installed with pip's &lt;code&gt;--pre&lt;/code&gt; flag).&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;master&lt;/code&gt; channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from &lt;a href="https://github.com/yt-dlp/yt-dlp-master-builds/releases"&gt;yt-dlp/yt-dlp-master-builds&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When using &lt;code&gt;--update&lt;/code&gt;/&lt;code&gt;-U&lt;/code&gt;, a release binary will only update to its current channel. &lt;code&gt;--update-to CHANNEL&lt;/code&gt; can be used to switch to a different channel when a newer version is available. &lt;code&gt;--update-to [CHANNEL@]TAG&lt;/code&gt; can also be used to upgrade or downgrade to specific tags from a channel.&lt;/p&gt; 
&lt;p&gt;You may also use &lt;code&gt;--update-to &amp;lt;repository&amp;gt;&lt;/code&gt; (&lt;code&gt;&amp;lt;owner&amp;gt;/&amp;lt;repository&amp;gt;&lt;/code&gt;) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.&lt;/p&gt; 
&lt;p&gt;Example usage:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;yt-dlp --update-to master&lt;/code&gt; switch to the &lt;code&gt;master&lt;/code&gt; channel and update to its latest release&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;yt-dlp --update-to stable@2023.07.06&lt;/code&gt; upgrade/downgrade to release to &lt;code&gt;stable&lt;/code&gt; channel tag &lt;code&gt;2023.07.06&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;yt-dlp --update-to 2023.10.07&lt;/code&gt; upgrade/downgrade to tag &lt;code&gt;2023.10.07&lt;/code&gt; if it exists on the current channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;yt-dlp --update-to example/yt-dlp@2023.09.24&lt;/code&gt; upgrade/downgrade to the release from the &lt;code&gt;example/yt-dlp&lt;/code&gt; repository, tag &lt;code&gt;2023.09.24&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Any user experiencing an issue with the &lt;code&gt;stable&lt;/code&gt; release should install or update to the &lt;code&gt;nightly&lt;/code&gt; release before submitting a bug report:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python -m pip install -U --pre "yt-dlp[default]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version. You can suppress this warning by adding &lt;code&gt;--no-update&lt;/code&gt; to your command or configuration file.&lt;/p&gt; 
&lt;h2&gt;DEPENDENCIES&lt;/h2&gt; 
&lt;p&gt;Python versions 3.10+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.&lt;/p&gt; 
&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt; 
&lt;p&gt;While all the other dependencies are optional, &lt;code&gt;ffmpeg&lt;/code&gt;, &lt;code&gt;ffprobe&lt;/code&gt;, &lt;code&gt;yt-dlp-ejs&lt;/code&gt; and a supported JavaScript runtime/engine are highly recommended&lt;/p&gt; 
&lt;h3&gt;Strongly recommended&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.ffmpeg.org"&gt;&lt;strong&gt;ffmpeg&lt;/strong&gt; and &lt;strong&gt;ffprobe&lt;/strong&gt;&lt;/a&gt; - Required for &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection"&gt;merging separate video and audio files&lt;/a&gt;, as well as for various &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#post-processing-options"&gt;post-processing&lt;/a&gt; tasks. License &lt;a href="https://www.ffmpeg.org/legal.html"&gt;depends on the build&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide &lt;a href="https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds"&gt;custom builds&lt;/a&gt; with patches for some of these issues at &lt;a href="https://github.com/yt-dlp/FFmpeg-Builds"&gt;yt-dlp/FFmpeg-Builds&lt;/a&gt;. See &lt;a href="https://github.com/yt-dlp/FFmpeg-Builds#patches-applied"&gt;the readme&lt;/a&gt; for details on the specific issues solved by these builds&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: What you need is ffmpeg &lt;em&gt;binary&lt;/em&gt;, &lt;strong&gt;NOT&lt;/strong&gt; &lt;a href="https://pypi.org/project/ffmpeg"&gt;the Python package of the same name&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/yt-dlp/ejs"&gt;&lt;strong&gt;yt-dlp-ejs&lt;/strong&gt;&lt;/a&gt; - Required for full YouTube support. Licensed under &lt;a href="https://github.com/yt-dlp/ejs/raw/main/LICENSE"&gt;Unlicense&lt;/a&gt;, bundles &lt;a href="https://github.com/davidbonnet/astring/raw/main/LICENSE"&gt;MIT&lt;/a&gt; and &lt;a href="https://github.com/meriyah/meriyah/raw/main/LICENSE.md"&gt;ISC&lt;/a&gt; components.&lt;/p&gt; &lt;p&gt;A JavaScript runtime/engine like &lt;a href="https://deno.land"&gt;&lt;strong&gt;deno&lt;/strong&gt;&lt;/a&gt; (recommended), &lt;a href="https://nodejs.org"&gt;&lt;strong&gt;node.js&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://bun.sh"&gt;&lt;strong&gt;bun&lt;/strong&gt;&lt;/a&gt;, or &lt;a href="https://bellard.org/quickjs/"&gt;&lt;strong&gt;QuickJS&lt;/strong&gt;&lt;/a&gt; is also required to run yt-dlp-ejs. See &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/EJS"&gt;the wiki&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Networking&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/certifi/python-certifi"&gt;&lt;strong&gt;certifi&lt;/strong&gt;&lt;/a&gt;* - Provides Mozilla's root certificate bundle. Licensed under &lt;a href="https://github.com/certifi/python-certifi/raw/master/LICENSE"&gt;MPLv2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/brotli"&gt;&lt;strong&gt;brotli&lt;/strong&gt;&lt;/a&gt;* or &lt;a href="https://github.com/python-hyper/brotlicffi"&gt;&lt;strong&gt;brotlicffi&lt;/strong&gt;&lt;/a&gt; - &lt;a href="https://en.wikipedia.org/wiki/Brotli"&gt;Brotli&lt;/a&gt; content encoding support. Both licensed under MIT &lt;sup&gt;&lt;a href="https://github.com/google/brotli/raw/master/LICENSE"&gt;1&lt;/a&gt; &lt;a href="https://github.com/python-hyper/brotlicffi/raw/master/LICENSE"&gt;2&lt;/a&gt; &lt;/sup&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aaugustin/websockets"&gt;&lt;strong&gt;websockets&lt;/strong&gt;&lt;/a&gt;* - For downloading over websocket. Licensed under &lt;a href="https://github.com/aaugustin/websockets/raw/main/LICENSE"&gt;BSD-3-Clause&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/psf/requests"&gt;&lt;strong&gt;requests&lt;/strong&gt;&lt;/a&gt;* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under &lt;a href="https://github.com/psf/requests/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Impersonation&lt;/h4&gt; 
&lt;p&gt;The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lexiforest/curl_cffi"&gt;&lt;strong&gt;curl_cffi&lt;/strong&gt;&lt;/a&gt; (recommended) - Python binding for &lt;a href="https://github.com/lexiforest/curl-impersonate"&gt;curl-impersonate&lt;/a&gt;. Provides impersonation targets for Chrome, Edge and Safari. Licensed under &lt;a href="https://github.com/lexiforest/curl_cffi/raw/main/LICENSE"&gt;MIT&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Can be installed with the &lt;code&gt;curl-cffi&lt;/code&gt; extra, e.g. &lt;code&gt;pip install "yt-dlp[default,curl-cffi]"&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Currently included in most builds &lt;em&gt;except&lt;/em&gt; &lt;code&gt;yt-dlp&lt;/code&gt; (Unix zipimport binary), &lt;code&gt;yt-dlp_x86&lt;/code&gt; (Windows 32-bit) and &lt;code&gt;yt-dlp_musllinux_aarch64&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Metadata&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/quodlibet/mutagen"&gt;&lt;strong&gt;mutagen&lt;/strong&gt;&lt;/a&gt;* - For &lt;code&gt;--embed-thumbnail&lt;/code&gt; in certain formats. Licensed under &lt;a href="https://github.com/quodlibet/mutagen/raw/master/COPYING"&gt;GPLv2+&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/wez/atomicparsley"&gt;&lt;strong&gt;AtomicParsley&lt;/strong&gt;&lt;/a&gt; - For &lt;code&gt;--embed-thumbnail&lt;/code&gt; in &lt;code&gt;mp4&lt;/code&gt;/&lt;code&gt;m4a&lt;/code&gt; files when &lt;code&gt;mutagen&lt;/code&gt;/&lt;code&gt;ffmpeg&lt;/code&gt; cannot. Licensed under &lt;a href="https://github.com/wez/atomicparsley/raw/master/COPYING"&gt;GPLv2+&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xattr/xattr"&gt;&lt;strong&gt;xattr&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/iustin/pyxattr"&gt;&lt;strong&gt;pyxattr&lt;/strong&gt;&lt;/a&gt; or &lt;a href="http://savannah.nongnu.org/projects/attr"&gt;&lt;strong&gt;setfattr&lt;/strong&gt;&lt;/a&gt; - For writing xattr metadata (&lt;code&gt;--xattrs&lt;/code&gt;) on &lt;strong&gt;Mac&lt;/strong&gt; and &lt;strong&gt;BSD&lt;/strong&gt;. Licensed under &lt;a href="https://github.com/xattr/xattr/raw/master/LICENSE.txt"&gt;MIT&lt;/a&gt;, &lt;a href="https://github.com/iustin/pyxattr/raw/master/COPYING"&gt;LGPL2.1&lt;/a&gt; and &lt;a href="http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING"&gt;GPLv2+&lt;/a&gt; respectively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Misc&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Legrandin/pycryptodome"&gt;&lt;strong&gt;pycryptodomex&lt;/strong&gt;&lt;/a&gt;* - For decrypting AES-128 HLS streams and various other data. Licensed under &lt;a href="https://github.com/Legrandin/pycryptodome/raw/master/LICENSE.rst"&gt;BSD-2-Clause&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ariya/phantomjs"&gt;&lt;strong&gt;phantomjs&lt;/strong&gt;&lt;/a&gt; - Used in some extractors where JavaScript needs to be run. No longer used for YouTube. To be deprecated in the near future. Licensed under &lt;a href="https://github.com/ariya/phantomjs/raw/master/LICENSE.BSD"&gt;BSD-3-Clause&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mitya57/secretstorage"&gt;&lt;strong&gt;secretstorage&lt;/strong&gt;&lt;/a&gt;* - For &lt;code&gt;--cookies-from-browser&lt;/code&gt; to access the &lt;strong&gt;Gnome&lt;/strong&gt; keyring while decrypting cookies of &lt;strong&gt;Chromium&lt;/strong&gt;-based browsers on &lt;strong&gt;Linux&lt;/strong&gt;. Licensed under &lt;a href="https://github.com/mitya57/secretstorage/raw/master/LICENSE"&gt;BSD-3-Clause&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Any external downloader that you want to use with &lt;code&gt;--downloader&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deprecated&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://rtmpdump.mplayerhq.hu"&gt;&lt;strong&gt;rtmpdump&lt;/strong&gt;&lt;/a&gt; - For downloading &lt;code&gt;rtmp&lt;/code&gt; streams. ffmpeg can be used instead with &lt;code&gt;--downloader ffmpeg&lt;/code&gt;. Licensed under &lt;a href="http://rtmpdump.mplayerhq.hu"&gt;GPLv2+&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mplayerhq.hu/design7/info.html"&gt;&lt;strong&gt;mplayer&lt;/strong&gt;&lt;/a&gt; or &lt;a href="https://mpv.io"&gt;&lt;strong&gt;mpv&lt;/strong&gt;&lt;/a&gt; - For downloading &lt;code&gt;rstp&lt;/code&gt;/&lt;code&gt;mms&lt;/code&gt; streams. ffmpeg can be used instead with &lt;code&gt;--downloader ffmpeg&lt;/code&gt;. Licensed under &lt;a href="https://github.com/mpv-player/mpv/raw/master/Copyright"&gt;GPLv2+&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use or redistribute the dependencies, you must agree to their respective licensing terms.&lt;/p&gt; 
&lt;p&gt;The standalone release binaries are built with the Python interpreter and the packages marked with &lt;strong&gt;*&lt;/strong&gt; included.&lt;/p&gt; 
&lt;p&gt;If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the &lt;code&gt;--verbose&lt;/code&gt; output&lt;/p&gt; 
&lt;h2&gt;COMPILE&lt;/h2&gt; 
&lt;h3&gt;Standalone PyInstaller Builds&lt;/h3&gt; 
&lt;p&gt;To build the standalone executable, you must have Python and &lt;code&gt;pyinstaller&lt;/code&gt; (plus any of yt-dlp's &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#dependencies"&gt;optional dependencies&lt;/a&gt; if needed). The executable will be built for the same CPU architecture as the Python used.&lt;/p&gt; 
&lt;p&gt;You can run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python devscripts/install_deps.py --include-extra pyinstaller
python devscripts/make_lazy_extractors.py
python -m bundle.pyinstaller
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On some systems, you may need to use &lt;code&gt;py&lt;/code&gt; or &lt;code&gt;python3&lt;/code&gt; instead of &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python -m bundle.pyinstaller&lt;/code&gt; accepts any arguments that can be passed to &lt;code&gt;pyinstaller&lt;/code&gt;, such as &lt;code&gt;--onefile/-F&lt;/code&gt; or &lt;code&gt;--onedir/-D&lt;/code&gt;, which is further &lt;a href="https://pyinstaller.org/en/stable/usage.html#what-to-generate"&gt;documented here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Pyinstaller versions below 4.4 &lt;a href="https://github.com/pyinstaller/pyinstaller#requirements-and-tested-platforms"&gt;do not support&lt;/a&gt; Python installed from the Windows store without using a virtual environment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Running &lt;code&gt;pyinstaller&lt;/code&gt; directly &lt;strong&gt;instead of&lt;/strong&gt; using &lt;code&gt;python -m bundle.pyinstaller&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; officially supported. This may or may not work correctly.&lt;/p&gt; 
&lt;h3&gt;Platform-independent Binary (UNIX)&lt;/h3&gt; 
&lt;p&gt;You will need the build tools &lt;code&gt;python&lt;/code&gt; (3.10+), &lt;code&gt;zip&lt;/code&gt;, &lt;code&gt;make&lt;/code&gt; (GNU), &lt;code&gt;pandoc&lt;/code&gt;* and &lt;code&gt;pytest&lt;/code&gt;*.&lt;/p&gt; 
&lt;p&gt;After installing these, simply run &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can also run &lt;code&gt;make yt-dlp&lt;/code&gt; instead to compile only the binary without updating any of the additional files. (The build tools marked with &lt;strong&gt;*&lt;/strong&gt; are not needed for this)&lt;/p&gt; 
&lt;h3&gt;Related scripts&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/install_deps.py&lt;/code&gt;&lt;/strong&gt; - Install dependencies for yt-dlp.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/update-version.py&lt;/code&gt;&lt;/strong&gt; - Update the version number based on the current date.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/set-variant.py&lt;/code&gt;&lt;/strong&gt; - Set the build variant of the executable.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/make_changelog.py&lt;/code&gt;&lt;/strong&gt; - Create a markdown changelog using short commit messages and update &lt;code&gt;CONTRIBUTORS&lt;/code&gt; file.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;devscripts/make_lazy_extractors.py&lt;/code&gt;&lt;/strong&gt; - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable &lt;code&gt;YTDLP_NO_LAZY_EXTRACTORS&lt;/code&gt; to something nonempty to forcefully disable lazy extractor loading.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note: See their &lt;code&gt;--help&lt;/code&gt; for more info.&lt;/p&gt; 
&lt;h3&gt;Forking the project&lt;/h3&gt; 
&lt;p&gt;If you fork the project on GitHub, you can run your fork's &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/workflows/build.yml"&gt;build workflow&lt;/a&gt; to automatically build the selected version(s) as artifacts. Alternatively, you can run the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/workflows/release.yml"&gt;release workflow&lt;/a&gt; or enable the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/workflows/release-nightly.yml"&gt;nightly workflow&lt;/a&gt; to create full (pre-)releases.&lt;/p&gt; 
&lt;h1&gt;USAGE AND OPTIONS&lt;/h1&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;pre&gt;&lt;code&gt;yt-dlp [OPTIONS] [--] URL [URL...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Tip: Use &lt;code&gt;CTRL&lt;/code&gt;+&lt;code&gt;F&lt;/code&gt; (or &lt;code&gt;Command&lt;/code&gt;+&lt;code&gt;F&lt;/code&gt;) to search by keywords&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;!-- Auto generated --&gt; 
&lt;h2&gt;General Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-h, --help                      Print this help text and exit
--version                       Print program version and exit
-U, --update                    Update this program to the latest version
--no-update                     Do not check for updates (default)
--update-to [CHANNEL]@[TAG]     Upgrade/downgrade to a specific version.
                                CHANNEL can be a repository as well. CHANNEL
                                and TAG default to "stable" and "latest"
                                respectively if omitted; See "UPDATE" for
                                details. Supported channels: stable,
                                nightly, master
-i, --ignore-errors             Ignore download and postprocessing errors.
                                The download will be considered successful
                                even if the postprocessing fails
--no-abort-on-error             Continue with next video on download errors;
                                e.g. to skip unavailable videos in a
                                playlist (default)
--abort-on-error                Abort downloading of further videos if an
                                error occurs (Alias: --no-ignore-errors)
--list-extractors               List all supported extractors and exit
--extractor-descriptions        Output descriptions of all supported
                                extractors and exit
--use-extractors NAMES          Extractor names to use separated by commas.
                                You can also use regexes, "all", "default"
                                and "end" (end URL matching); e.g. --ies
                                "holodex.*,end,youtube". Prefix the name
                                with a "-" to exclude it, e.g. --ies
                                default,-generic. Use --list-extractors for
                                a list of extractor names. (Alias: --ies)
--default-search PREFIX         Use this prefix for unqualified URLs. E.g.
                                "gvsearch2:python" downloads two videos from
                                google videos for the search term "python".
                                Use the value "auto" to let yt-dlp guess
                                ("auto_warning" to emit a warning when
                                guessing). "error" just throws an error. The
                                default value "fixup_error" repairs broken
                                URLs, but emits an error if this is not
                                possible instead of searching
--ignore-config                 Don't load any more configuration files
                                except those given to --config-locations.
                                For backward compatibility, if this option
                                is found inside the system configuration
                                file, the user configuration is not loaded.
                                (Alias: --no-config)
--no-config-locations           Do not load any custom configuration files
                                (default). When given inside a configuration
                                file, ignore all previous --config-locations
                                defined in the current file
--config-locations PATH         Location of the main configuration file;
                                either the path to the config or its
                                containing directory ("-" for stdin). Can be
                                used multiple times and inside other
                                configuration files
--plugin-dirs DIR               Path to an additional directory to search
                                for plugins. This option can be used
                                multiple times to add multiple directories.
                                Use "default" to search the default plugin
                                directories (default)
--no-plugin-dirs                Clear plugin directories to search,
                                including defaults and those provided by
                                previous --plugin-dirs
--js-runtimes RUNTIME[:PATH]    Additional JavaScript runtime to enable,
                                with an optional location for the runtime
                                (either the path to the binary or its
                                containing directory). This option can be
                                used multiple times to enable multiple
                                runtimes. Supported runtimes are (in order
                                of priority, from highest to lowest): deno,
                                node, quickjs, bun. Only "deno" is enabled
                                by default. The highest priority runtime
                                that is both enabled and available will be
                                used. In order to use a lower priority
                                runtime when "deno" is available, --no-js-
                                runtimes needs to be passed before enabling
                                other runtimes
--no-js-runtimes                Clear JavaScript runtimes to enable,
                                including defaults and those provided by
                                previous --js-runtimes
--remote-components COMPONENT   Remote components to allow yt-dlp to fetch
                                when required. This option is currently not
                                needed if you are using an official
                                executable or have the requisite version of
                                the yt-dlp-ejs package installed. You can
                                use this option multiple times to allow
                                multiple components. Supported values:
                                ejs:npm (external JavaScript components from
                                npm), ejs:github (external JavaScript
                                components from yt-dlp-ejs GitHub). By
                                default, no remote components are allowed
--no-remote-components          Disallow fetching of all remote components,
                                including any previously allowed by
                                --remote-components or defaults.
--flat-playlist                 Do not extract a playlist's URL result
                                entries; some entry metadata may be missing
                                and downloading may be bypassed
--no-flat-playlist              Fully extract the videos of a playlist
                                (default)
--live-from-start               Download livestreams from the start.
                                Currently experimental and only supported
                                for YouTube and Twitch
--no-live-from-start            Download livestreams from the current time
                                (default)
--wait-for-video MIN[-MAX]      Wait for scheduled streams to become
                                available. Pass the minimum number of
                                seconds (or range) to wait between retries
--no-wait-for-video             Do not wait for scheduled streams (default)
--mark-watched                  Mark videos watched (even with --simulate)
--no-mark-watched               Do not mark videos watched (default)
--color [STREAM:]POLICY         Whether to emit color codes in output,
                                optionally prefixed by the STREAM (stdout or
                                stderr) to apply the setting to. Can be one
                                of "always", "auto" (default), "never", or
                                "no_color" (use non color terminal
                                sequences). Use "auto-tty" or "no_color-tty"
                                to decide based on terminal support only.
                                Can be used multiple times
--compat-options OPTS           Options that can help keep compatibility
                                with youtube-dl or youtube-dlc
                                configurations by reverting some of the
                                changes made in yt-dlp. See "Differences in
                                default behavior" for details
--alias ALIASES OPTIONS         Create aliases for an option string. Unless
                                an alias starts with a dash "-", it is
                                prefixed with "--". Arguments are parsed
                                according to the Python string formatting
                                mini-language. E.g. --alias get-audio,-X "-S
                                aext:{0},abr -x --audio-format {0}" creates
                                options "--get-audio" and "-X" that takes an
                                argument (ARG0) and expands to "-S
                                aext:ARG0,abr -x --audio-format ARG0". All
                                defined aliases are listed in the --help
                                output. Alias options can trigger more
                                aliases; so be careful to avoid defining
                                recursive options. As a safety measure, each
                                alias may be triggered a maximum of 100
                                times. This option can be used multiple times
-t, --preset-alias PRESET       Applies a predefined set of options. e.g.
                                --preset-alias mp3. The following presets
                                are available: mp3, aac, mp4, mkv, sleep.
                                See the "Preset Aliases" section at the end
                                for more info. This option can be used
                                multiple times
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Network Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--proxy URL                     Use the specified HTTP/HTTPS/SOCKS proxy. To
                                enable SOCKS proxy, specify a proper scheme,
                                e.g. socks5://user:pass@127.0.0.1:1080/.
                                Pass in an empty string (--proxy "") for
                                direct connection
--socket-timeout SECONDS        Time to wait before giving up, in seconds
--source-address IP             Client-side IP address to bind to
--impersonate CLIENT[:OS]       Client to impersonate for requests. E.g.
                                chrome, chrome-110, chrome:windows-10. Pass
                                --impersonate="" to impersonate any client.
                                Note that forcing impersonation for all
                                requests may have a detrimental impact on
                                download speed and stability
--list-impersonate-targets      List available clients to impersonate.
-4, --force-ipv4                Make all connections via IPv4
-6, --force-ipv6                Make all connections via IPv6
--enable-file-urls              Enable file:// URLs. This is disabled by
                                default for security reasons.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Geo-restriction:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--geo-verification-proxy URL    Use this proxy to verify the IP address for
                                some geo-restricted sites. The default proxy
                                specified by --proxy (or none, if the option
                                is not present) is used for the actual
                                downloading
--xff VALUE                     How to fake X-Forwarded-For HTTP header to
                                try bypassing geographic restriction. One of
                                "default" (only when known to be useful),
                                "never", an IP block in CIDR notation, or a
                                two-letter ISO 3166-2 country code
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Video Selection:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-I, --playlist-items ITEM_SPEC  Comma-separated playlist_index of the items
                                to download. You can specify a range using
                                "[START]:[STOP][:STEP]". For backward
                                compatibility, START-STOP is also supported.
                                Use negative indices to count from the right
                                and negative STEP to download in reverse
                                order. E.g. "-I 1:3,7,-5::2" used on a
                                playlist of size 15 will download the items
                                at index 1,2,3,7,11,13,15
--min-filesize SIZE             Abort download if filesize is smaller than
                                SIZE, e.g. 50k or 44.6M
--max-filesize SIZE             Abort download if filesize is larger than
                                SIZE, e.g. 50k or 44.6M
--date DATE                     Download only videos uploaded on this date.
                                The date can be "YYYYMMDD" or in the format 
                                [now|today|yesterday][-N[day|week|month|year]].
                                E.g. "--date today-2weeks" downloads only
                                videos uploaded on the same day two weeks ago
--datebefore DATE               Download only videos uploaded on or before
                                this date. The date formats accepted are the
                                same as --date
--dateafter DATE                Download only videos uploaded on or after
                                this date. The date formats accepted are the
                                same as --date
--match-filters FILTER          Generic video filter. Any "OUTPUT TEMPLATE"
                                field can be compared with a number or a
                                string using the operators defined in
                                "Filtering Formats". You can also simply
                                specify a field to match if the field is
                                present, use "!field" to check if the field
                                is not present, and "&amp;amp;" to check multiple
                                conditions. Use a "\" to escape "&amp;amp;" or
                                quotes if needed. If used multiple times,
                                the filter matches if at least one of the
                                conditions is met. E.g. --match-filters
                                !is_live --match-filters "like_count&amp;gt;?100 &amp;amp;
                                description~='(?i)\bcats \&amp;amp; dogs\b'" matches
                                only videos that are not live OR those that
                                have a like count more than 100 (or the like
                                field is not available) and also has a
                                description that contains the phrase "cats &amp;amp;
                                dogs" (caseless). Use "--match-filters -" to
                                interactively ask whether to download each
                                video
--no-match-filters              Do not use any --match-filters (default)
--break-match-filters FILTER    Same as "--match-filters" but stops the
                                download process when a video is rejected
--no-break-match-filters        Do not use any --break-match-filters (default)
--no-playlist                   Download only the video, if the URL refers
                                to a video and a playlist
--yes-playlist                  Download the playlist, if the URL refers to
                                a video and a playlist
--age-limit YEARS               Download only videos suitable for the given
                                age
--download-archive FILE         Download only videos not listed in the
                                archive file. Record the IDs of all
                                downloaded videos in it
--no-download-archive           Do not use archive file (default)
--max-downloads NUMBER          Abort after downloading NUMBER files
--break-on-existing             Stop the download process when encountering
                                a file that is in the archive supplied with
                                the --download-archive option
--no-break-on-existing          Do not stop the download process when
                                encountering a file that is in the archive
                                (default)
--break-per-input               Alters --max-downloads, --break-on-existing,
                                --break-match-filters, and autonumber to
                                reset per input URL
--no-break-per-input            --break-on-existing and similar options
                                terminates the entire download queue
--skip-playlist-after-errors N  Number of allowed failures until the rest of
                                the playlist is skipped
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Download Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-N, --concurrent-fragments N    Number of fragments of a dash/hlsnative
                                video that should be downloaded concurrently
                                (default is 1)
-r, --limit-rate RATE           Maximum download rate in bytes per second,
                                e.g. 50K or 4.2M
--throttled-rate RATE           Minimum download rate in bytes per second
                                below which throttling is assumed and the
                                video data is re-extracted, e.g. 100K
-R, --retries RETRIES           Number of retries (default is 10), or
                                "infinite"
--file-access-retries RETRIES   Number of times to retry on file access
                                error (default is 3), or "infinite"
--fragment-retries RETRIES      Number of retries for a fragment (default is
                                10), or "infinite" (DASH, hlsnative and ISM)
--retry-sleep [TYPE:]EXPR       Time to sleep between retries in seconds
                                (optionally) prefixed by the type of retry
                                (http (default), fragment, file_access,
                                extractor) to apply the sleep to. EXPR can
                                be a number, linear=START[:END[:STEP=1]] or
                                exp=START[:END[:BASE=2]]. This option can be
                                used multiple times to set the sleep for the
                                different retry types, e.g. --retry-sleep
                                linear=1::2 --retry-sleep fragment:exp=1:20
--skip-unavailable-fragments    Skip unavailable fragments for DASH,
                                hlsnative and ISM downloads (default)
                                (Alias: --no-abort-on-unavailable-fragments)
--abort-on-unavailable-fragments
                                Abort download if a fragment is unavailable
                                (Alias: --no-skip-unavailable-fragments)
--keep-fragments                Keep downloaded fragments on disk after
                                downloading is finished
--no-keep-fragments             Delete downloaded fragments after
                                downloading is finished (default)
--buffer-size SIZE              Size of download buffer, e.g. 1024 or 16K
                                (default is 1024)
--resize-buffer                 The buffer size is automatically resized
                                from an initial value of --buffer-size
                                (default)
--no-resize-buffer              Do not automatically adjust the buffer size
--http-chunk-size SIZE          Size of a chunk for chunk-based HTTP
                                downloading, e.g. 10485760 or 10M (default
                                is disabled). May be useful for bypassing
                                bandwidth throttling imposed by a webserver
                                (experimental)
--playlist-random               Download playlist videos in random order
--lazy-playlist                 Process entries in the playlist as they are
                                received. This disables n_entries,
                                --playlist-random and --playlist-reverse
--no-lazy-playlist              Process videos in the playlist only after
                                the entire playlist is parsed (default)
--hls-use-mpegts                Use the mpegts container for HLS videos;
                                allowing some players to play the video
                                while downloading, and reducing the chance
                                of file corruption if download is
                                interrupted. This is enabled by default for
                                live streams
--no-hls-use-mpegts             Do not use the mpegts container for HLS
                                videos. This is default when not downloading
                                live streams
--download-sections REGEX       Download only chapters that match the
                                regular expression. A "*" prefix denotes
                                time-range instead of chapter. Negative
                                timestamps are calculated from the end.
                                "*from-url" can be used to download between
                                the "start_time" and "end_time" extracted
                                from the URL. Needs ffmpeg. This option can
                                be used multiple times to download multiple
                                sections, e.g. --download-sections
                                "*10:15-inf" --download-sections "intro"
--downloader [PROTO:]NAME       Name or path of the external downloader to
                                use (optionally) prefixed by the protocols
                                (http, ftp, m3u8, dash, rstp, rtmp, mms) to
                                use it for. Currently supports native,
                                aria2c, axel, curl, ffmpeg, httpie, wget.
                                You can use this option multiple times to
                                set different downloaders for different
                                protocols. E.g. --downloader aria2c
                                --downloader "dash,m3u8:native" will use
                                aria2c for http/ftp downloads, and the
                                native downloader for dash/m3u8 downloads
                                (Alias: --external-downloader)
--downloader-args NAME:ARGS     Give these arguments to the external
                                downloader. Specify the downloader name and
                                the arguments separated by a colon ":". For
                                ffmpeg, arguments can be passed to different
                                positions using the same syntax as
                                --postprocessor-args. You can use this
                                option multiple times to give different
                                arguments to different downloaders (Alias:
                                --external-downloader-args)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Filesystem Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-a, --batch-file FILE           File containing URLs to download ("-" for
                                stdin), one URL per line. Lines starting
                                with "#", ";" or "]" are considered as
                                comments and ignored
--no-batch-file                 Do not read URLs from batch file (default)
-P, --paths [TYPES:]PATH        The paths where the files should be
                                downloaded. Specify the type of file and the
                                path separated by a colon ":". All the same
                                TYPES as --output are supported.
                                Additionally, you can also provide "home"
                                (default) and "temp" paths. All intermediary
                                files are first downloaded to the temp path
                                and then the final files are moved over to
                                the home path after download is finished.
                                This option is ignored if --output is an
                                absolute path
-o, --output [TYPES:]TEMPLATE   Output filename template; see "OUTPUT
                                TEMPLATE" for details
--output-na-placeholder TEXT    Placeholder for unavailable fields in
                                --output (default: "NA")
--restrict-filenames            Restrict filenames to only ASCII characters,
                                and avoid "&amp;amp;" and spaces in filenames
--no-restrict-filenames         Allow Unicode characters, "&amp;amp;" and spaces in
                                filenames (default)
--windows-filenames             Force filenames to be Windows-compatible
--no-windows-filenames          Sanitize filenames only minimally
--trim-filenames LENGTH         Limit the filename length (excluding
                                extension) to the specified number of
                                characters
-w, --no-overwrites             Do not overwrite any files
--force-overwrites              Overwrite all video and metadata files. This
                                option includes --no-continue
--no-force-overwrites           Do not overwrite the video, but overwrite
                                related files (default)
-c, --continue                  Resume partially downloaded files/fragments
                                (default)
--no-continue                   Do not resume partially downloaded
                                fragments. If the file is not fragmented,
                                restart download of the entire file
--part                          Use .part files instead of writing directly
                                into output file (default)
--no-part                       Do not use .part files - write directly into
                                output file
--mtime                         Use the Last-modified header to set the file
                                modification time
--no-mtime                      Do not use the Last-modified header to set
                                the file modification time (default)
--write-description             Write video description to a .description file
--no-write-description          Do not write video description (default)
--write-info-json               Write video metadata to a .info.json file
                                (this may contain personal information)
--no-write-info-json            Do not write video metadata (default)
--write-playlist-metafiles      Write playlist metadata in addition to the
                                video metadata when using --write-info-json,
                                --write-description etc. (default)
--no-write-playlist-metafiles   Do not write playlist metadata when using
                                --write-info-json, --write-description etc.
--clean-info-json               Remove some internal metadata such as
                                filenames from the infojson (default)
--no-clean-info-json            Write all fields to the infojson
--write-comments                Retrieve video comments to be placed in the
                                infojson. The comments are fetched even
                                without this option if the extraction is
                                known to be quick (Alias: --get-comments)
--no-write-comments             Do not retrieve video comments unless the
                                extraction is known to be quick (Alias:
                                --no-get-comments)
--load-info-json FILE           JSON file containing the video information
                                (created with the "--write-info-json" option)
--cookies FILE                  Netscape formatted file to read cookies from
                                and dump cookie jar in
--no-cookies                    Do not read/dump cookies from/to file
                                (default)
--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]
                                The name of the browser to load cookies
                                from. Currently supported browsers are:
                                brave, chrome, chromium, edge, firefox,
                                opera, safari, vivaldi, whale. Optionally,
                                the KEYRING used for decrypting Chromium
                                cookies on Linux, the name/path of the
                                PROFILE to load cookies from, and the
                                CONTAINER name (if Firefox) ("none" for no
                                container) can be given with their
                                respective separators. By default, all
                                containers of the most recently accessed
                                profile are used. Currently supported
                                keyrings are: basictext, gnomekeyring,
                                kwallet, kwallet5, kwallet6
--no-cookies-from-browser       Do not load cookies from browser (default)
--cache-dir DIR                 Location in the filesystem where yt-dlp can
                                store some downloaded information (such as
                                client ids and signatures) permanently. By
                                default ${XDG_CACHE_HOME}/yt-dlp
--no-cache-dir                  Disable filesystem caching
--rm-cache-dir                  Delete all filesystem cache files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Thumbnail Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-thumbnail               Write thumbnail image to disk
--no-write-thumbnail            Do not write thumbnail image to disk (default)
--write-all-thumbnails          Write all thumbnail image formats to disk
--list-thumbnails               List available thumbnails of each video.
                                Simulate unless --no-simulate is used
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Internet Shortcut Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-link                    Write an internet shortcut file, depending
                                on the current platform (.url, .webloc or
                                .desktop). The URL may be cached by the OS
--write-url-link                Write a .url Windows internet shortcut. The
                                OS caches the URL based on the file path
--write-webloc-link             Write a .webloc macOS internet shortcut
--write-desktop-link            Write a .desktop Linux internet shortcut
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Verbosity and Simulation Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-q, --quiet                     Activate quiet mode. If used with --verbose,
                                print the log to stderr
--no-quiet                      Deactivate quiet mode. (Default)
--no-warnings                   Ignore warnings
-s, --simulate                  Do not download the video and do not write
                                anything to disk
--no-simulate                   Download the video even if printing/listing
                                options are used
--ignore-no-formats-error       Ignore "No video formats" error. Useful for
                                extracting metadata even if the videos are
                                not actually available for download
                                (experimental)
--no-ignore-no-formats-error    Throw error when no downloadable video
                                formats are found (default)
--skip-download                 Do not download the video but write all
                                related files (Alias: --no-download)
-O, --print [WHEN:]TEMPLATE     Field name or output template to print to
                                screen, optionally prefixed with when to
                                print it, separated by a ":". Supported
                                values of "WHEN" are the same as that of
                                --use-postprocessor (default: video).
                                Implies --quiet. Implies --simulate unless
                                --no-simulate or later stages of WHEN are
                                used. This option can be used multiple times
--print-to-file [WHEN:]TEMPLATE FILE
                                Append given template to the file. The
                                values of WHEN and TEMPLATE are the same as
                                that of --print. FILE uses the same syntax
                                as the output template. This option can be
                                used multiple times
-j, --dump-json                 Quiet, but print JSON information for each
                                video. Simulate unless --no-simulate is
                                used. See "OUTPUT TEMPLATE" for a
                                description of available keys
-J, --dump-single-json          Quiet, but print JSON information for each
                                URL or infojson passed. Simulate unless
                                --no-simulate is used. If the URL refers to
                                a playlist, the whole playlist information
                                is dumped in a single line
--force-write-archive           Force download archive entries to be written
                                as far as no errors occur, even if -s or
                                another simulation option is used (Alias:
                                --force-download-archive)
--newline                       Output progress bar as new lines
--no-progress                   Do not print progress bar
--progress                      Show progress bar, even if in quiet mode
--console-title                 Display progress in console titlebar
--progress-template [TYPES:]TEMPLATE
                                Template for progress outputs, optionally
                                prefixed with one of "download:" (default),
                                "download-title:" (the console title),
                                "postprocess:",  or "postprocess-title:".
                                The video's fields are accessible under the
                                "info" key and the progress attributes are
                                accessible under "progress" key. E.g.
                                --console-title --progress-template
                                "download-title:%(info.id)s-%(progress.eta)s"
--progress-delta SECONDS        Time between progress output (default: 0)
-v, --verbose                   Print various debugging information
--dump-pages                    Print downloaded pages encoded using base64
                                to debug problems (very verbose)
--write-pages                   Write downloaded intermediary pages to files
                                in the current directory to debug problems
--print-traffic                 Display sent and read HTTP traffic
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Workarounds:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--encoding ENCODING             Force the specified encoding (experimental)
--legacy-server-connect         Explicitly allow HTTPS connection to servers
                                that do not support RFC 5746 secure
                                renegotiation
--no-check-certificates         Suppress HTTPS certificate validation
--prefer-insecure               Use an unencrypted connection to retrieve
                                information about the video (Currently
                                supported only for YouTube)
--add-headers FIELD:VALUE       Specify a custom HTTP header and its value,
                                separated by a colon ":". You can use this
                                option multiple times
--bidi-workaround               Work around terminals that lack
                                bidirectional text support. Requires bidiv
                                or fribidi executable in PATH
--sleep-requests SECONDS        Number of seconds to sleep between requests
                                during data extraction
--sleep-interval SECONDS        Number of seconds to sleep before each
                                download. This is the minimum time to sleep
                                when used along with --max-sleep-interval
                                (Alias: --min-sleep-interval)
--max-sleep-interval SECONDS    Maximum number of seconds to sleep. Can only
                                be used along with --min-sleep-interval
--sleep-subtitles SECONDS       Number of seconds to sleep before each
                                subtitle download
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Video Format Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-f, --format FORMAT             Video format code, see "FORMAT SELECTION"
                                for more details
-S, --format-sort SORTORDER     Sort the formats by the fields given, see
                                "Sorting Formats" for more details
--format-sort-reset             Disregard previous user specified sort order
                                and reset to the default
--format-sort-force             Force user specified sort order to have
                                precedence over all fields, see "Sorting
                                Formats" for more details (Alias: --S-force)
--no-format-sort-force          Some fields have precedence over the user
                                specified sort order (default)
--video-multistreams            Allow multiple video streams to be merged
                                into a single file
--no-video-multistreams         Only one video stream is downloaded for each
                                output file (default)
--audio-multistreams            Allow multiple audio streams to be merged
                                into a single file
--no-audio-multistreams         Only one audio stream is downloaded for each
                                output file (default)
--prefer-free-formats           Prefer video formats with free containers
                                over non-free ones of the same quality. Use
                                with "-S ext" to strictly prefer free
                                containers irrespective of quality
--no-prefer-free-formats        Don't give any special preference to free
                                containers (default)
--check-formats                 Make sure formats are selected only from
                                those that are actually downloadable
--check-all-formats             Check all formats for whether they are
                                actually downloadable
--no-check-formats              Do not check that the formats are actually
                                downloadable
-F, --list-formats              List available formats of each video.
                                Simulate unless --no-simulate is used
--merge-output-format FORMAT    Containers that may be used when merging
                                formats, separated by "/", e.g. "mp4/mkv".
                                Ignored if no merge is required. (currently
                                supported: avi, flv, mkv, mov, mp4, webm)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Subtitle Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--write-subs                    Write subtitle file
--no-write-subs                 Do not write subtitle file (default)
--write-auto-subs               Write automatically generated subtitle file
                                (Alias: --write-automatic-subs)
--no-write-auto-subs            Do not write auto-generated subtitles
                                (default) (Alias: --no-write-automatic-subs)
--list-subs                     List available subtitles of each video.
                                Simulate unless --no-simulate is used
--sub-format FORMAT             Subtitle format; accepts formats preference
                                separated by "/", e.g. "srt" or "ass/srt/best"
--sub-langs LANGS               Languages of the subtitles to download (can
                                be regex) or "all" separated by commas, e.g.
                                --sub-langs "en.*,ja" (where "en.*" is a
                                regex pattern that matches "en" followed by
                                0 or more of any character). You can prefix
                                the language code with a "-" to exclude it
                                from the requested languages, e.g. --sub-
                                langs all,-live_chat. Use --list-subs for a
                                list of available language tags
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Authentication Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-u, --username USERNAME         Login with this account ID
-p, --password PASSWORD         Account password. If this option is left
                                out, yt-dlp will ask interactively
-2, --twofactor TWOFACTOR       Two-factor authentication code
-n, --netrc                     Use .netrc authentication data
--netrc-location PATH           Location of .netrc authentication data;
                                either the path or its containing directory.
                                Defaults to ~/.netrc
--netrc-cmd NETRC_CMD           Command to execute to get the credentials
                                for an extractor.
--video-password PASSWORD       Video-specific password
--ap-mso MSO                    Adobe Pass multiple-system operator (TV
                                provider) identifier, use --ap-list-mso for
                                a list of available MSOs
--ap-username USERNAME          Multiple-system operator account login
--ap-password PASSWORD          Multiple-system operator account password.
                                If this option is left out, yt-dlp will ask
                                interactively
--ap-list-mso                   List all supported multiple-system operators
--client-certificate CERTFILE   Path to client certificate file in PEM
                                format. May include the private key
--client-certificate-key KEYFILE
                                Path to private key file for client
                                certificate
--client-certificate-password PASSWORD
                                Password for client certificate private key,
                                if encrypted. If not provided, and the key
                                is encrypted, yt-dlp will ask interactively
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Post-Processing Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;-x, --extract-audio             Convert video files to audio-only files
                                (requires ffmpeg and ffprobe)
--audio-format FORMAT           Format to convert the audio to when -x is
                                used. (currently supported: best (default),
                                aac, alac, flac, m4a, mp3, opus, vorbis,
                                wav). You can specify multiple rules using
                                similar syntax as --remux-video
--audio-quality QUALITY         Specify ffmpeg audio quality to use when
                                converting the audio with -x. Insert a value
                                between 0 (best) and 10 (worst) for VBR or a
                                specific bitrate like 128K (default 5)
--remux-video FORMAT            Remux the video into another container if
                                necessary (currently supported: avi, flv,
                                gif, mkv, mov, mp4, webm, aac, aiff, alac,
                                flac, m4a, mka, mp3, ogg, opus, vorbis,
                                wav). If the target container does not
                                support the video/audio codec, remuxing will
                                fail. You can specify multiple rules; e.g.
                                "aac&amp;gt;m4a/mov&amp;gt;mp4/mkv" will remux aac to m4a,
                                mov to mp4 and anything else to mkv
--recode-video FORMAT           Re-encode the video into another format if
                                necessary. The syntax and supported formats
                                are the same as --remux-video
--postprocessor-args NAME:ARGS  Give these arguments to the postprocessors.
                                Specify the postprocessor/executable name
                                and the arguments separated by a colon ":"
                                to give the argument to the specified
                                postprocessor/executable. Supported PP are:
                                Merger, ModifyChapters, SplitChapters,
                                ExtractAudio, VideoRemuxer, VideoConvertor,
                                Metadata, EmbedSubtitle, EmbedThumbnail,
                                SubtitlesConvertor, ThumbnailsConvertor,
                                FixupStretched, FixupM4a, FixupM3u8,
                                FixupTimestamp and FixupDuration. The
                                supported executables are: AtomicParsley,
                                FFmpeg and FFprobe. You can also specify
                                "PP+EXE:ARGS" to give the arguments to the
                                specified executable only when being used by
                                the specified postprocessor. Additionally,
                                for ffmpeg/ffprobe, "_i"/"_o" can be
                                appended to the prefix optionally followed
                                by a number to pass the argument before the
                                specified input/output file, e.g. --ppa
                                "Merger+ffmpeg_i1:-v quiet". You can use
                                this option multiple times to give different
                                arguments to different postprocessors.
                                (Alias: --ppa)
-k, --keep-video                Keep the intermediate video file on disk
                                after post-processing
--no-keep-video                 Delete the intermediate video file after
                                post-processing (default)
--post-overwrites               Overwrite post-processed files (default)
--no-post-overwrites            Do not overwrite post-processed files
--embed-subs                    Embed subtitles in the video (only for mp4,
                                webm and mkv videos)
--no-embed-subs                 Do not embed subtitles (default)
--embed-thumbnail               Embed thumbnail in the video as cover art
--no-embed-thumbnail            Do not embed thumbnail (default)
--embed-metadata                Embed metadata to the video file. Also
                                embeds chapters/infojson if present unless
                                --no-embed-chapters/--no-embed-info-json are
                                used (Alias: --add-metadata)
--no-embed-metadata             Do not add metadata to file (default)
                                (Alias: --no-add-metadata)
--embed-chapters                Add chapter markers to the video file
                                (Alias: --add-chapters)
--no-embed-chapters             Do not add chapter markers (default) (Alias:
                                --no-add-chapters)
--embed-info-json               Embed the infojson as an attachment to
                                mkv/mka video files
--no-embed-info-json            Do not embed the infojson as an attachment
                                to the video file
--parse-metadata [WHEN:]FROM:TO
                                Parse additional metadata like title/artist
                                from other fields; see "MODIFYING METADATA"
                                for details. Supported values of "WHEN" are
                                the same as that of --use-postprocessor
                                (default: pre_process)
--replace-in-metadata [WHEN:]FIELDS REGEX REPLACE
                                Replace text in a metadata field using the
                                given regex. This option can be used
                                multiple times. Supported values of "WHEN"
                                are the same as that of --use-postprocessor
                                (default: pre_process)
--xattrs                        Write metadata to the video file's xattrs
                                (using Dublin Core and XDG standards)
--concat-playlist POLICY        Concatenate videos in a playlist. One of
                                "never", "always", or "multi_video"
                                (default; only when the videos form a single
                                show). All the video files must have the
                                same codecs and number of streams to be
                                concatenable. The "pl_video:" prefix can be
                                used with "--paths" and "--output" to set
                                the output filename for the concatenated
                                files. See "OUTPUT TEMPLATE" for details
--fixup POLICY                  Automatically correct known faults of the
                                file. One of never (do nothing), warn (only
                                emit a warning), detect_or_warn (the
                                default; fix the file if we can, warn
                                otherwise), force (try fixing even if the
                                file already exists)
--ffmpeg-location PATH          Location of the ffmpeg binary; either the
                                path to the binary or its containing directory
--exec [WHEN:]CMD               Execute a command, optionally prefixed with
                                when to execute it, separated by a ":".
                                Supported values of "WHEN" are the same as
                                that of --use-postprocessor (default:
                                after_move). The same syntax as the output
                                template can be used to pass any field as
                                arguments to the command. If no fields are
                                passed, %(filepath,_filename|)q is appended
                                to the end of the command. This option can
                                be used multiple times
--no-exec                       Remove any previously defined --exec
--convert-subs FORMAT           Convert the subtitles to another format
                                (currently supported: ass, lrc, srt, vtt).
                                Use "--convert-subs none" to disable
                                conversion (default) (Alias: --convert-
                                subtitles)
--convert-thumbnails FORMAT     Convert the thumbnails to another format
                                (currently supported: jpg, png, webp). You
                                can specify multiple rules using similar
                                syntax as "--remux-video". Use "--convert-
                                thumbnails none" to disable conversion
                                (default)
--split-chapters                Split video into multiple files based on
                                internal chapters. The "chapter:" prefix can
                                be used with "--paths" and "--output" to set
                                the output filename for the split files. See
                                "OUTPUT TEMPLATE" for details
--no-split-chapters             Do not split video based on chapters (default)
--remove-chapters REGEX         Remove chapters whose title matches the
                                given regular expression. The syntax is the
                                same as --download-sections. This option can
                                be used multiple times
--no-remove-chapters            Do not remove any chapters from the file
                                (default)
--force-keyframes-at-cuts       Force keyframes at cuts when
                                downloading/splitting/removing sections.
                                This is slow due to needing a re-encode, but
                                the resulting video may have fewer artifacts
                                around the cuts
--no-force-keyframes-at-cuts    Do not force keyframes around the chapters
                                when cutting/splitting (default)
--use-postprocessor NAME[:ARGS]
                                The (case-sensitive) name of plugin
                                postprocessors to be enabled, and
                                (optionally) arguments to be passed to it,
                                separated by a colon ":". ARGS are a
                                semicolon ";" delimited list of NAME=VALUE.
                                The "when" argument determines when the
                                postprocessor is invoked. It can be one of
                                "pre_process" (after video extraction),
                                "after_filter" (after video passes filter),
                                "video" (after --format; before
                                --print/--output), "before_dl" (before each
                                video download), "post_process" (after each
                                video download; default), "after_move"
                                (after moving the video file to its final
                                location), "after_video" (after downloading
                                and processing all formats of a video), or
                                "playlist" (at end of playlist). This option
                                can be used multiple times to add different
                                postprocessors
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;SponsorBlock Options:&lt;/h2&gt; 
&lt;p&gt;Make chapter entries for, or remove various segments (sponsor, introductions, etc.) from downloaded YouTube videos using the &lt;a href="https://sponsor.ajay.app"&gt;SponsorBlock API&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--sponsorblock-mark CATS        SponsorBlock categories to create chapters
                                for, separated by commas. Available
                                categories are sponsor, intro, outro,
                                selfpromo, preview, filler, interaction,
                                music_offtopic, hook, poi_highlight,
                                chapter, all and default (=all). You can
                                prefix the category with a "-" to exclude
                                it. See [1] for descriptions of the
                                categories. E.g. --sponsorblock-mark
                                all,-preview
                                [1] https://wiki.sponsor.ajay.app/w/Segment_Categories
--sponsorblock-remove CATS      SponsorBlock categories to be removed from
                                the video file, separated by commas. If a
                                category is present in both mark and remove,
                                remove takes precedence. The syntax and
                                available categories are the same as for
                                --sponsorblock-mark except that "default"
                                refers to "all,-filler" and poi_highlight,
                                chapter are not available
--sponsorblock-chapter-title TEMPLATE
                                An output template for the title of the
                                SponsorBlock chapters created by
                                --sponsorblock-mark. The only available
                                fields are start_time, end_time, category,
                                categories, name, category_names. Defaults
                                to "[SponsorBlock]: %(category_names)l"
--no-sponsorblock               Disable both --sponsorblock-mark and
                                --sponsorblock-remove
--sponsorblock-api URL          SponsorBlock API location, defaults to
                                https://sponsor.ajay.app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Extractor Options:&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;--extractor-retries RETRIES     Number of retries for known extractor errors
                                (default is 3), or "infinite"
--allow-dynamic-mpd             Process dynamic DASH manifests (default)
                                (Alias: --no-ignore-dynamic-mpd)
--ignore-dynamic-mpd            Do not process dynamic DASH manifests
                                (Alias: --no-allow-dynamic-mpd)
--hls-split-discontinuity       Split HLS playlists to different formats at
                                discontinuities such as ad breaks
--no-hls-split-discontinuity    Do not split HLS playlists into different
                                formats at discontinuities such as ad breaks
                                (default)
--extractor-args IE_KEY:ARGS    Pass ARGS arguments to the IE_KEY extractor.
                                See "EXTRACTOR ARGUMENTS" for details. You
                                can use this option multiple times to give
                                arguments for different extractors
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Preset Aliases:&lt;/h2&gt; 
&lt;p&gt;Predefined aliases for convenience and ease of use. Note that future versions of yt-dlp may add or adjust presets, but the existing preset names will not be changed or removed&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-t mp3                          -f 'ba[acodec^=mp3]/ba/b' -x --audio-format
                                mp3

-t aac                          -f
                                'ba[acodec^=aac]/ba[acodec^=mp4a.40.]/ba/b'
                                -x --audio-format aac

-t mp4                          --merge-output-format mp4 --remux-video mp4
                                -S vcodec:h264,lang,quality,res,fps,hdr:12,a
                                codec:aac

-t mkv                          --merge-output-format mkv --remux-video mkv

-t sleep                        --sleep-subtitles 5 --sleep-requests 0.75
                                --sleep-interval 10 --max-sleep-interval 20
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;CONFIGURATION&lt;/h1&gt; 
&lt;p&gt;You can configure yt-dlp by placing any supported command line option in a configuration file. The configuration is loaded from the following locations:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Main Configuration&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The file given to &lt;code&gt;--config-locations&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Portable Configuration&lt;/strong&gt;: (Recommended for portable installations)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;If using a binary, &lt;code&gt;yt-dlp.conf&lt;/code&gt; in the same directory as the binary&lt;/li&gt; 
   &lt;li&gt;If running from source-code, &lt;code&gt;yt-dlp.conf&lt;/code&gt; in the parent directory of &lt;code&gt;yt_dlp&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Home Configuration&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;yt-dlp.conf&lt;/code&gt; in the home path given to &lt;code&gt;-P&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If &lt;code&gt;-P&lt;/code&gt; is not given, the current directory is searched&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;User Configuration&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp.conf&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp/config&lt;/code&gt; (recommended on Linux/macOS)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp/config.txt&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp.conf&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp/config&lt;/code&gt; (recommended on Windows)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp/config.txt&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;~/yt-dlp.conf&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;~/yt-dlp.conf.txt&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;~/.yt-dlp/config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;~/.yt-dlp/config.txt&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;See also: &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#notes-about-environment-variables"&gt;Notes about environment variables&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;System Configuration&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;/etc/yt-dlp.conf&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;/etc/yt-dlp/config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;/etc/yt-dlp/config.txt&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;E.g. with the following configuration file, yt-dlp will always extract the audio, copy the mtime, use a proxy and save all videos under &lt;code&gt;YouTube&lt;/code&gt; directory in your home directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Lines starting with # are comments

# Always extract audio
-x

# Copy the mtime
--mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under YouTube directory in your home directory
-o ~/YouTube/%(title)s.%(ext)s
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Options in a configuration file are just the same options aka switches used in regular command line calls; thus there &lt;strong&gt;must be no whitespace&lt;/strong&gt; after &lt;code&gt;-&lt;/code&gt; or &lt;code&gt;--&lt;/code&gt;, e.g. &lt;code&gt;-o&lt;/code&gt; or &lt;code&gt;--proxy&lt;/code&gt; but not &lt;code&gt;- o&lt;/code&gt; or &lt;code&gt;-- proxy&lt;/code&gt;. They must also be quoted when necessary, as if it were a UNIX shell.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;--ignore-config&lt;/code&gt; if you want to disable all configuration files for a particular yt-dlp run. If &lt;code&gt;--ignore-config&lt;/code&gt; is found inside any configuration file, no further configuration will be loaded. For example, having the option in the portable configuration file prevents loading of home, user, and system configurations. Additionally, (for backward compatibility) if &lt;code&gt;--ignore-config&lt;/code&gt; is found inside the system configuration file, the user configuration is not loaded.&lt;/p&gt; 
&lt;h3&gt;Configuration file encoding&lt;/h3&gt; 
&lt;p&gt;The configuration files are decoded according to the UTF BOM if present, and in the encoding from system locale otherwise.&lt;/p&gt; 
&lt;p&gt;If you want your file to be decoded differently, add &lt;code&gt;# coding: ENCODING&lt;/code&gt; to the beginning of the file (e.g. &lt;code&gt;# coding: shift-jis&lt;/code&gt;). There must be no characters before that, even spaces or BOM.&lt;/p&gt; 
&lt;h3&gt;Authentication with netrc&lt;/h3&gt; 
&lt;p&gt;You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with &lt;code&gt;--username&lt;/code&gt; and &lt;code&gt;--password&lt;/code&gt;) in order not to pass credentials as command line arguments on every yt-dlp execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a &lt;a href="https://stackoverflow.com/tags/.netrc/info"&gt;&lt;code&gt;.netrc&lt;/code&gt; file&lt;/a&gt; on a per-extractor basis. For that, you will need to create a &lt;code&gt;.netrc&lt;/code&gt; file in &lt;code&gt;--netrc-location&lt;/code&gt; and restrict permissions to read/write by only you:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;touch ${HOME}/.netrc
chmod a-rwx,u+rw ${HOME}/.netrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After that, you can add credentials for an extractor in the following format, where &lt;em&gt;extractor&lt;/em&gt; is the name of the extractor in lowercase:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;machine &amp;lt;extractor&amp;gt; login &amp;lt;username&amp;gt; password &amp;lt;password&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;E.g.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To activate authentication with the &lt;code&gt;.netrc&lt;/code&gt; file you should pass &lt;code&gt;--netrc&lt;/code&gt; to yt-dlp or place it in the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;configuration file&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The default location of the .netrc file is &lt;code&gt;~&lt;/code&gt; (see below).&lt;/p&gt; 
&lt;p&gt;As an alternative to using the &lt;code&gt;.netrc&lt;/code&gt; file, which has the disadvantage of keeping your passwords in a plain text file, you can configure a custom shell command to provide the credentials for an extractor. This is done by providing the &lt;code&gt;--netrc-cmd&lt;/code&gt; parameter, it shall output the credentials in the netrc format and return &lt;code&gt;0&lt;/code&gt; on success, other values will be treated as an error. &lt;code&gt;{}&lt;/code&gt; in the command will be replaced by the name of the extractor to make it possible to select the credentials for the right extractor.&lt;/p&gt; 
&lt;p&gt;E.g. To use an encrypted &lt;code&gt;.netrc&lt;/code&gt; file stored as &lt;code&gt;.authinfo.gpg&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;yt-dlp --netrc-cmd 'gpg --decrypt ~/.authinfo.gpg' 'https://www.youtube.com/watch?v=BaW_jenozKc'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Notes about environment variables&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Environment variables are normally specified as &lt;code&gt;${VARIABLE}&lt;/code&gt;/&lt;code&gt;$VARIABLE&lt;/code&gt; on UNIX and &lt;code&gt;%VARIABLE%&lt;/code&gt; on Windows; but is always shown as &lt;code&gt;${VARIABLE}&lt;/code&gt; in this documentation&lt;/li&gt; 
 &lt;li&gt;yt-dlp also allows using UNIX-style variables on Windows for path-like options; e.g. &lt;code&gt;--output&lt;/code&gt;, &lt;code&gt;--config-locations&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;If unset, &lt;code&gt;${XDG_CONFIG_HOME}&lt;/code&gt; defaults to &lt;code&gt;~/.config&lt;/code&gt; and &lt;code&gt;${XDG_CACHE_HOME}&lt;/code&gt; to &lt;code&gt;~/.cache&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;On Windows, &lt;code&gt;~&lt;/code&gt; points to &lt;code&gt;${HOME}&lt;/code&gt; if present; or, &lt;code&gt;${USERPROFILE}&lt;/code&gt; or &lt;code&gt;${HOMEDRIVE}${HOMEPATH}&lt;/code&gt; otherwise&lt;/li&gt; 
 &lt;li&gt;On Windows, &lt;code&gt;${USERPROFILE}&lt;/code&gt; generally points to &lt;code&gt;C:\Users\&amp;lt;user name&amp;gt;&lt;/code&gt; and &lt;code&gt;${APPDATA}&lt;/code&gt; to &lt;code&gt;${USERPROFILE}\AppData\Roaming&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;OUTPUT TEMPLATE&lt;/h1&gt; 
&lt;p&gt;The &lt;code&gt;-o&lt;/code&gt; option is used to indicate a template for the output file names while &lt;code&gt;-P&lt;/code&gt; option is used to specify the path each type of file should be saved to.&lt;/p&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;The simplest usage of &lt;code&gt;-o&lt;/code&gt; is not to set any template arguments when downloading a single file, like in &lt;code&gt;yt-dlp -o funny_video.flv "https://some/video"&lt;/code&gt; (hard-coding file extension like this is &lt;em&gt;not&lt;/em&gt; recommended and could break some post-processing).&lt;/p&gt; 
&lt;p&gt;It may however also contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to &lt;a href="https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting"&gt;Python string formatting operations&lt;/a&gt;, e.g. &lt;code&gt;%(NAME)s&lt;/code&gt; or &lt;code&gt;%(NAME)05d&lt;/code&gt;. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations.&lt;/p&gt; 
&lt;p&gt;The field names themselves (the part inside the parenthesis) can also have some special formatting:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Object traversal&lt;/strong&gt;: The dictionaries and lists available in metadata can be traversed by using a dot &lt;code&gt;.&lt;/code&gt; separator; e.g. &lt;code&gt;%(tags.0)s&lt;/code&gt;, &lt;code&gt;%(subtitles.en.-1.ext)s&lt;/code&gt;. You can do Python slicing with colon &lt;code&gt;:&lt;/code&gt;; E.g. &lt;code&gt;%(id.3:7)s&lt;/code&gt;, &lt;code&gt;%(id.6:2:-1)s&lt;/code&gt;, &lt;code&gt;%(formats.:.format_id)s&lt;/code&gt;. Curly braces &lt;code&gt;{}&lt;/code&gt; can be used to build dictionaries with only specific keys; e.g. &lt;code&gt;%(formats.:.{format_id,height})#j&lt;/code&gt;. An empty field name &lt;code&gt;%()s&lt;/code&gt; refers to the entire infodict; e.g. &lt;code&gt;%(.{id,title})s&lt;/code&gt;. Note that all the fields that become available using this method are not listed below. Use &lt;code&gt;-j&lt;/code&gt; to see such fields&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Arithmetic&lt;/strong&gt;: Simple arithmetic can be done on numeric fields using &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;*&lt;/code&gt;. E.g. &lt;code&gt;%(playlist_index+10)03d&lt;/code&gt;, &lt;code&gt;%(n_entries+1-playlist_index)d&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Date/time Formatting&lt;/strong&gt;: Date/time fields can be formatted according to &lt;a href="https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes"&gt;strftime formatting&lt;/a&gt; by specifying it separated from the field name using a &lt;code&gt;&amp;gt;&lt;/code&gt;. E.g. &lt;code&gt;%(duration&amp;gt;%H-%M-%S)s&lt;/code&gt;, &lt;code&gt;%(upload_date&amp;gt;%Y-%m-%d)s&lt;/code&gt;, &lt;code&gt;%(epoch-3600&amp;gt;%H-%M-%S)s&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternatives&lt;/strong&gt;: Alternate fields can be specified separated with a &lt;code&gt;,&lt;/code&gt;. E.g. &lt;code&gt;%(release_date&amp;gt;%Y,upload_date&amp;gt;%Y|Unknown)s&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Replacement&lt;/strong&gt;: A replacement value can be specified using a &lt;code&gt;&amp;amp;&lt;/code&gt; separator according to the &lt;a href="https://docs.python.org/3/library/string.html#format-specification-mini-language"&gt;&lt;code&gt;str.format&lt;/code&gt; mini-language&lt;/a&gt;. If the field is &lt;em&gt;not&lt;/em&gt; empty, this replacement value will be used instead of the actual field content. This is done after alternate fields are considered; thus the replacement is used if &lt;em&gt;any&lt;/em&gt; of the alternative fields is &lt;em&gt;not&lt;/em&gt; empty. E.g. &lt;code&gt;%(chapters&amp;amp;has chapters|no chapters)s&lt;/code&gt;, &lt;code&gt;%(title&amp;amp;TITLE={:&amp;gt;20}|NO TITLE)s&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Default&lt;/strong&gt;: A literal default value can be specified for when the field is empty using a &lt;code&gt;|&lt;/code&gt; separator. This overrides &lt;code&gt;--output-na-placeholder&lt;/code&gt;. E.g. &lt;code&gt;%(uploader|Unknown)s&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;More Conversions&lt;/strong&gt;: In addition to the normal format types &lt;code&gt;diouxXeEfFgGcrs&lt;/code&gt;, yt-dlp additionally supports converting to &lt;code&gt;B&lt;/code&gt; = &lt;strong&gt;B&lt;/strong&gt;ytes, &lt;code&gt;j&lt;/code&gt; = &lt;strong&gt;j&lt;/strong&gt;son (flag &lt;code&gt;#&lt;/code&gt; for pretty-printing, &lt;code&gt;+&lt;/code&gt; for Unicode), &lt;code&gt;h&lt;/code&gt; = HTML escaping, &lt;code&gt;l&lt;/code&gt; = a comma-separated &lt;strong&gt;l&lt;/strong&gt;ist (flag &lt;code&gt;#&lt;/code&gt; for &lt;code&gt;\n&lt;/code&gt; newline-separated), &lt;code&gt;q&lt;/code&gt; = a string &lt;strong&gt;q&lt;/strong&gt;uoted for the terminal (flag &lt;code&gt;#&lt;/code&gt; to split a list into different arguments), &lt;code&gt;D&lt;/code&gt; = add &lt;strong&gt;D&lt;/strong&gt;ecimal suffixes (e.g. 10M) (flag &lt;code&gt;#&lt;/code&gt; to use 1024 as factor), and &lt;code&gt;S&lt;/code&gt; = &lt;strong&gt;S&lt;/strong&gt;anitize as filename (flag &lt;code&gt;#&lt;/code&gt; for restricted)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unicode normalization&lt;/strong&gt;: The format type &lt;code&gt;U&lt;/code&gt; can be used for NFC &lt;a href="https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize"&gt;Unicode normalization&lt;/a&gt;. The alternate form flag (&lt;code&gt;#&lt;/code&gt;) changes the normalization to NFD and the conversion flag &lt;code&gt;+&lt;/code&gt; can be used for NFKC/NFKD compatibility equivalence normalization. E.g. &lt;code&gt;%(title)+.100U&lt;/code&gt; is NFKC&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To summarize, the general syntax for a field is:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;%(name[.keys][addition][&amp;gt;strf][,alternate][&amp;amp;replacement][|default])[flags][width][.precision][length]type
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additionally, you can set different output templates for the various metadata files separately from the general output template by specifying the type of file followed by the template separated by a colon &lt;code&gt;:&lt;/code&gt;. The different file types supported are &lt;code&gt;subtitle&lt;/code&gt;, &lt;code&gt;thumbnail&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, &lt;code&gt;annotation&lt;/code&gt; (deprecated), &lt;code&gt;infojson&lt;/code&gt;, &lt;code&gt;link&lt;/code&gt;, &lt;code&gt;pl_thumbnail&lt;/code&gt;, &lt;code&gt;pl_description&lt;/code&gt;, &lt;code&gt;pl_infojson&lt;/code&gt;, &lt;code&gt;chapter&lt;/code&gt;, &lt;code&gt;pl_video&lt;/code&gt;. E.g. &lt;code&gt;-o "%(title)s.%(ext)s" -o "thumbnail:%(title)s\%(title)s.%(ext)s"&lt;/code&gt; will put the thumbnails in a folder with the same name as the video. If any of the templates is empty, that type of file will not be written. E.g. &lt;code&gt;--write-thumbnail -o "thumbnail:"&lt;/code&gt; will write thumbnails only for playlists and not for video.&lt;/p&gt; 
&lt;p&gt;&lt;a id="outtmpl-postprocess-note"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Due to post-processing (i.e. merging etc.), the actual output filename might differ. Use &lt;code&gt;--print after_move:filepath&lt;/code&gt; to get the name after all post-processing is complete.&lt;/p&gt; 
&lt;p&gt;The available fields are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;id&lt;/code&gt; (string): Video identifier&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;title&lt;/code&gt; (string): Video title&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fulltitle&lt;/code&gt; (string): Video title ignoring live timestamp and generic title&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt; (string): Video filename extension&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;alt_title&lt;/code&gt; (string): A secondary title of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;description&lt;/code&gt; (string): The description of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;display_id&lt;/code&gt; (string): An alternative identifier for the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader&lt;/code&gt; (string): Full name of the video uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader_id&lt;/code&gt; (string): Nickname or id of the video uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;uploader_url&lt;/code&gt; (string): URL to the video uploader's profile&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;license&lt;/code&gt; (string): License name the video is licensed under&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;creators&lt;/code&gt; (list): The creators of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;creator&lt;/code&gt; (string): The creators of the video; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video became available&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;upload_date&lt;/code&gt; (string): Video upload date in UTC (YYYYMMDD)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video was released&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_date&lt;/code&gt; (string): The date (YYYYMMDD) when the video was released in UTC&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;release_year&lt;/code&gt; (numeric): Year (YYYY) when the video or album was released&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;modified_timestamp&lt;/code&gt; (numeric): UNIX timestamp of the moment the video was last modified&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;modified_date&lt;/code&gt; (string): The date (YYYYMMDD) when the video was last modified in UTC&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel&lt;/code&gt; (string): Full name of the channel the video is uploaded on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_id&lt;/code&gt; (string): Id of the channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_url&lt;/code&gt; (string): URL of the channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_follower_count&lt;/code&gt; (numeric): Number of followers of the channel&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channel_is_verified&lt;/code&gt; (boolean): Whether the channel is verified on the platform&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;location&lt;/code&gt; (string): Physical location where the video was filmed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;duration&lt;/code&gt; (numeric): Length of the video in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;duration_string&lt;/code&gt; (string): Length of the video (HH:mm:ss)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;view_count&lt;/code&gt; (numeric): How many users have watched the video on the platform&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;concurrent_view_count&lt;/code&gt; (numeric): How many users are currently watching the video on the platform.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;like_count&lt;/code&gt; (numeric): Number of positive ratings of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dislike_count&lt;/code&gt; (numeric): Number of negative ratings of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;repost_count&lt;/code&gt; (numeric): Number of reposts of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;average_rating&lt;/code&gt; (numeric): Average rating given by users, the scale used depends on the webpage&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;comment_count&lt;/code&gt; (numeric): Number of comments on the video (For some extractors, comments are only downloaded at the end, and so this field cannot be used)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;save_count&lt;/code&gt; (numeric): Number of times the video has been saved or bookmarked&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;age_limit&lt;/code&gt; (numeric): Age restriction for the video (years)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;live_status&lt;/code&gt; (string): One of "not_live", "is_live", "is_upcoming", "was_live", "post_live" (was live, but VOD is not yet processed)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;is_live&lt;/code&gt; (boolean): Whether this video is a live stream or a fixed-length video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;was_live&lt;/code&gt; (boolean): Whether this video was originally a live stream&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playable_in_embed&lt;/code&gt; (string): Whether this video is allowed to play in embedded players on other sites&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;availability&lt;/code&gt; (string): Whether the video is "private", "premium_only", "subscriber_only", "needs_auth", "unlisted" or "public"&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;media_type&lt;/code&gt; (string): The type of media as classified by the site, e.g. "episode", "clip", "trailer"&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;start_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should start, as specified in the URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;end_time&lt;/code&gt; (numeric): Time in seconds where the reproduction should end, as specified in the URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;extractor&lt;/code&gt; (string): Name of the extractor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;extractor_key&lt;/code&gt; (string): Key name of the extractor&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;epoch&lt;/code&gt; (numeric): Unix epoch of when the information extraction was completed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;autonumber&lt;/code&gt; (numeric): Number that will be increased with each download, starting at &lt;code&gt;--autonumber-start&lt;/code&gt;, padded with leading zeros to 5 digits&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;video_autonumber&lt;/code&gt; (numeric): Number that will be increased with each video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;n_entries&lt;/code&gt; (numeric): Total number of extracted items in the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_id&lt;/code&gt; (string): Identifier of the playlist that contains the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_title&lt;/code&gt; (string): Name of the playlist that contains the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist&lt;/code&gt; (string): &lt;code&gt;playlist_title&lt;/code&gt; if available or else &lt;code&gt;playlist_id&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_count&lt;/code&gt; (numeric): Total number of items in the playlist. May not be known if entire playlist is not extracted&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_index&lt;/code&gt; (numeric): Index of the video in the playlist padded with leading zeros according the final index&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_autonumber&lt;/code&gt; (numeric): Position of the video in the playlist download queue padded with leading zeros according to the total length of the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_uploader&lt;/code&gt; (string): Full name of the playlist uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_uploader_id&lt;/code&gt; (string): Nickname or id of the playlist uploader&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_channel&lt;/code&gt; (string): Display name of the channel that uploaded the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_channel_id&lt;/code&gt; (string): Identifier of the channel that uploaded the playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_webpage_url&lt;/code&gt; (string): URL of the playlist webpage&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webpage_url&lt;/code&gt; (string): A URL to the video webpage which, if given to yt-dlp, should yield the same result again&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webpage_url_basename&lt;/code&gt; (string): The basename of the webpage URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webpage_url_domain&lt;/code&gt; (string): The domain of the webpage URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;original_url&lt;/code&gt; (string): The URL given by the user (or the same as &lt;code&gt;webpage_url&lt;/code&gt; for playlist entries)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (list): List of categories the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tags&lt;/code&gt; (list): List of tags assigned to the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cast&lt;/code&gt; (list): List of cast members&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All the fields in &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#filtering-formats"&gt;Filtering Formats&lt;/a&gt; can also be used&lt;/p&gt; 
&lt;p&gt;Available for the video that belongs to some logical chapter or section:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;chapter&lt;/code&gt; (string): Name or title of the chapter the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;chapter_number&lt;/code&gt; (numeric): Number of the chapter the video belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;chapter_id&lt;/code&gt; (string): Id of the chapter the video belongs to&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the video that is an episode of some series or program:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;series&lt;/code&gt; (string): Title of the series or program the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;series_id&lt;/code&gt; (string): Id of the series or program the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season&lt;/code&gt; (string): Title of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season_number&lt;/code&gt; (numeric): Number of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;season_id&lt;/code&gt; (string): Id of the season the video episode belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode&lt;/code&gt; (string): Title of the video episode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode_number&lt;/code&gt; (numeric): Number of the video episode within a season&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;episode_id&lt;/code&gt; (string): Id of the video episode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available for the media that is a track or a part of a music album:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;track&lt;/code&gt; (string): Title of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;track_number&lt;/code&gt; (numeric): Number of the track within an album or a disc&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;track_id&lt;/code&gt; (string): Id of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;artists&lt;/code&gt; (list): Artist(s) of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;artist&lt;/code&gt; (string): Artist(s) of the track; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;genres&lt;/code&gt; (list): Genre(s) of the track&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;genre&lt;/code&gt; (string): Genre(s) of the track; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;composers&lt;/code&gt; (list): Composer(s) of the piece&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;composer&lt;/code&gt; (string): Composer(s) of the piece; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album&lt;/code&gt; (string): Title of the album the track belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_type&lt;/code&gt; (string): Type of the album&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_artists&lt;/code&gt; (list): All artists appeared on the album&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;album_artist&lt;/code&gt; (string): All artists appeared on the album; comma-separated&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;disc_number&lt;/code&gt; (numeric): Number of the disc or other physical medium the track belongs to&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available only when using &lt;code&gt;--download-sections&lt;/code&gt; and for &lt;code&gt;chapter:&lt;/code&gt; prefix when using &lt;code&gt;--split-chapters&lt;/code&gt; for videos with internal chapters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;section_title&lt;/code&gt; (string): Title of the chapter&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;section_number&lt;/code&gt; (numeric): Number of the chapter within the file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;section_start&lt;/code&gt; (numeric): Start time of the chapter in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;section_end&lt;/code&gt; (numeric): End time of the chapter in seconds&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available only when used in &lt;code&gt;--print&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;urls&lt;/code&gt; (string): The URLs of all requested formats, one in each line&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filename&lt;/code&gt; (string): Name of the video file. Note that the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#outtmpl-postprocess-note"&gt;actual filename may differ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;formats_table&lt;/code&gt; (table): The video format table as printed by &lt;code&gt;--list-formats&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;thumbnails_table&lt;/code&gt; (table): The thumbnail format table as printed by &lt;code&gt;--list-thumbnails&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;subtitles_table&lt;/code&gt; (table): The subtitle format table as printed by &lt;code&gt;--list-subs&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;automatic_captions_table&lt;/code&gt; (table): The automatic subtitle format table as printed by &lt;code&gt;--list-subs&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available only after the video is downloaded (&lt;code&gt;post_process&lt;/code&gt;/&lt;code&gt;after_move&lt;/code&gt;):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;filepath&lt;/code&gt;: Actual path of downloaded video file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Available only in &lt;code&gt;--sponsorblock-chapter-title&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;start_time&lt;/code&gt; (numeric): Start time of the chapter in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;end_time&lt;/code&gt; (numeric): End time of the chapter in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;categories&lt;/code&gt; (list): The &lt;a href="https://wiki.sponsor.ajay.app/w/Types#Category"&gt;SponsorBlock categories&lt;/a&gt; the chapter belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;category&lt;/code&gt; (string): The smallest SponsorBlock category the chapter belongs to&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;category_names&lt;/code&gt; (list): Friendly names of the categories&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt; (string): Friendly name of the smallest category&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;type&lt;/code&gt; (string): The &lt;a href="https://wiki.sponsor.ajay.app/w/Types#Action_Type"&gt;SponsorBlock action type&lt;/a&gt; of the chapter&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. E.g. for &lt;code&gt;-o %(title)s-%(id)s.%(ext)s&lt;/code&gt; and an mp4 video with title &lt;code&gt;yt-dlp test video&lt;/code&gt; and id &lt;code&gt;BaW_jenozKc&lt;/code&gt;, this will result in a &lt;code&gt;yt-dlp test video-BaW_jenozKc.mp4&lt;/code&gt; file created in the current directory.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Some of the sequences are not guaranteed to be present, since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with placeholder value provided with &lt;code&gt;--output-na-placeholder&lt;/code&gt; (&lt;code&gt;NA&lt;/code&gt; by default).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: Look at the &lt;code&gt;-j&lt;/code&gt; output to identify which fields are available for the particular URL&lt;/p&gt; 
&lt;p&gt;For numeric sequences, you can use &lt;a href="https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting"&gt;numeric related formatting&lt;/a&gt;; e.g. &lt;code&gt;%(view_count)05d&lt;/code&gt; will result in a string with view count padded with zeros up to 5 characters, like in &lt;code&gt;00042&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Output templates can also contain arbitrary hierarchical path, e.g. &lt;code&gt;-o "%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s"&lt;/code&gt; which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.&lt;/p&gt; 
&lt;p&gt;To use percent literals in an output template use &lt;code&gt;%%&lt;/code&gt;. To output to stdout use &lt;code&gt;-o -&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The current default template is &lt;code&gt;%(title)s [%(id)s].%(ext)s&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In some cases, you don't want special characters such as ‰∏≠, spaces, or &amp;amp;, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the &lt;code&gt;--restrict-filenames&lt;/code&gt; flag to get a shorter title.&lt;/p&gt; 
&lt;h4&gt;Output template examples&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ yt-dlp --print filename -o "test video.%(ext)s" BaW_jenozKc
test video.webm    # Literal name with correct extension

$ yt-dlp --print filename -o "%(title)s.%(ext)s" BaW_jenozKc
youtube-dl test video ''_√§‚Ü≠ùïê.webm    # All kinds of weird characters

$ yt-dlp --print filename -o "%(title)s.%(ext)s" BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.webm    # Restricted file name

# Download YouTube playlist videos in separate directory indexed by video order in a playlist
$ yt-dlp -o "%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s" "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"

# Download YouTube playlist videos in separate directories according to their uploaded year
$ yt-dlp -o "%(upload_date&amp;gt;%Y)s/%(title)s.%(ext)s" "https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re"

# Prefix playlist index with " - " separator, but only if it is available
$ yt-dlp -o "%(playlist_index&amp;amp;{} - |)s%(title)s.%(ext)s" BaW_jenozKc "https://www.youtube.com/user/TheLinuxFoundation/playlists"

# Download all playlists of YouTube channel/user keeping each playlist in separate directory:
$ yt-dlp -o "%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s" "https://www.youtube.com/user/TheLinuxFoundation/playlists"

# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home
$ yt-dlp -u user -p password -P "~/MyVideos" -o "%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s" "https://www.udemy.com/java-tutorial"

# Download entire series season keeping each series and each season in separate directory under C:/MyVideos
$ yt-dlp -P "C:/MyVideos" -o "%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s" "https://videomore.ru/kino_v_detalayah/5_sezon/367617"

# Download video as "C:\MyVideos\uploader\title.ext", subtitles as "C:\MyVideos\subs\uploader\title.ext"
# and put all temporary files in "C:\MyVideos\tmp"
$ yt-dlp -P "C:/MyVideos" -P "temp:tmp" -P "subtitle:subs" -o "%(uploader)s/%(title)s.%(ext)s" BaW_jenozKc --write-subs

# Download video as "C:\MyVideos\uploader\title.ext" and subtitles as "C:\MyVideos\uploader\subs\title.ext"
$ yt-dlp -P "C:/MyVideos" -o "%(uploader)s/%(title)s.%(ext)s" -o "subtitle:%(uploader)s/subs/%(title)s.%(ext)s" BaW_jenozKc --write-subs

# Stream the video being downloaded to stdout
$ yt-dlp -o - BaW_jenozKc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;FORMAT SELECTION&lt;/h1&gt; 
&lt;p&gt;By default, yt-dlp tries to download the best available quality if you &lt;strong&gt;don't&lt;/strong&gt; pass any options. This is generally equivalent to using &lt;code&gt;-f bestvideo*+bestaudio/best&lt;/code&gt;. However, if multiple audiostreams is enabled (&lt;code&gt;--audio-multistreams&lt;/code&gt;), the default format changes to &lt;code&gt;-f bestvideo+bestaudio/best&lt;/code&gt;. Similarly, if ffmpeg is unavailable, or if you use yt-dlp to stream to &lt;code&gt;stdout&lt;/code&gt; (&lt;code&gt;-o -&lt;/code&gt;), the default becomes &lt;code&gt;-f best/bestvideo+bestaudio&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deprecation warning&lt;/strong&gt;: Latest versions of yt-dlp can stream multiple formats to the stdout simultaneously using ffmpeg. So, in future versions, the default for this will be set to &lt;code&gt;-f bv*+ba/b&lt;/code&gt; similar to normal downloads. If you want to preserve the &lt;code&gt;-f b/bv+ba&lt;/code&gt; setting, it is recommended to explicitly specify it in the configuration options.&lt;/p&gt; 
&lt;p&gt;The general syntax for format selection is &lt;code&gt;-f FORMAT&lt;/code&gt; (or &lt;code&gt;--format FORMAT&lt;/code&gt;) where &lt;code&gt;FORMAT&lt;/code&gt; is a &lt;em&gt;selector expression&lt;/em&gt;, i.e. an expression that describes format or formats you would like to download.&lt;/p&gt; 
&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt; 
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection-examples"&gt;navigate me to examples&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt; 
&lt;p&gt;The simplest case is requesting a specific format; e.g. with &lt;code&gt;-f 22&lt;/code&gt; you can download the format with format code equal to 22. You can get the list of available format codes for particular video using &lt;code&gt;--list-formats&lt;/code&gt; or &lt;code&gt;-F&lt;/code&gt;. Note that these format codes are extractor specific.&lt;/p&gt; 
&lt;p&gt;You can also use a file extension (currently &lt;code&gt;3gp&lt;/code&gt;, &lt;code&gt;aac&lt;/code&gt;, &lt;code&gt;flv&lt;/code&gt;, &lt;code&gt;m4a&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, &lt;code&gt;mp4&lt;/code&gt;, &lt;code&gt;ogg&lt;/code&gt;, &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;webm&lt;/code&gt; are supported) to download the best quality format of a particular file extension served as a single file, e.g. &lt;code&gt;-f webm&lt;/code&gt; will download the best quality format with the &lt;code&gt;webm&lt;/code&gt; extension served as a single file.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;-f -&lt;/code&gt; to interactively provide the format selector &lt;em&gt;for each video&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;You can also use special names to select particular edge case formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;all&lt;/code&gt;: Select &lt;strong&gt;all formats&lt;/strong&gt; separately&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mergeall&lt;/code&gt;: Select and &lt;strong&gt;merge all formats&lt;/strong&gt; (Must be used with &lt;code&gt;--audio-multistreams&lt;/code&gt;, &lt;code&gt;--video-multistreams&lt;/code&gt; or both)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;b*&lt;/code&gt;, &lt;code&gt;best*&lt;/code&gt;: Select the best quality format that &lt;strong&gt;contains either&lt;/strong&gt; a video or an audio or both (i.e.; &lt;code&gt;vcodec!=none or acodec!=none&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;b&lt;/code&gt;, &lt;code&gt;best&lt;/code&gt;: Select the best quality format that &lt;strong&gt;contains both&lt;/strong&gt; video and audio. Equivalent to &lt;code&gt;best*[vcodec!=none][acodec!=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;bv&lt;/code&gt;, &lt;code&gt;bestvideo&lt;/code&gt;: Select the best quality &lt;strong&gt;video-only&lt;/strong&gt; format. Equivalent to &lt;code&gt;best*[acodec=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;bv*&lt;/code&gt;, &lt;code&gt;bestvideo*&lt;/code&gt;: Select the best quality format that &lt;strong&gt;contains video&lt;/strong&gt;. It may also contain audio. Equivalent to &lt;code&gt;best*[vcodec!=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ba&lt;/code&gt;, &lt;code&gt;bestaudio&lt;/code&gt;: Select the best quality &lt;strong&gt;audio-only&lt;/strong&gt; format. Equivalent to &lt;code&gt;best*[vcodec=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ba*&lt;/code&gt;, &lt;code&gt;bestaudio*&lt;/code&gt;: Select the best quality format that &lt;strong&gt;contains audio&lt;/strong&gt;. It may also contain video. Equivalent to &lt;code&gt;best*[acodec!=none]&lt;/code&gt; (&lt;a href="https://github.com/yt-dlp/yt-dlp/issues/979#issuecomment-919629354"&gt;Do not use!&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;w*&lt;/code&gt;, &lt;code&gt;worst*&lt;/code&gt;: Select the worst quality format that contains either a video or an audio&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;w&lt;/code&gt;, &lt;code&gt;worst&lt;/code&gt;: Select the worst quality format that contains both video and audio. Equivalent to &lt;code&gt;worst*[vcodec!=none][acodec!=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wv&lt;/code&gt;, &lt;code&gt;worstvideo&lt;/code&gt;: Select the worst quality video-only format. Equivalent to &lt;code&gt;worst*[acodec=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wv*&lt;/code&gt;, &lt;code&gt;worstvideo*&lt;/code&gt;: Select the worst quality format that contains video. It may also contain audio. Equivalent to &lt;code&gt;worst*[vcodec!=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wa&lt;/code&gt;, &lt;code&gt;worstaudio&lt;/code&gt;: Select the worst quality audio-only format. Equivalent to &lt;code&gt;worst*[vcodec=none]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wa*&lt;/code&gt;, &lt;code&gt;worstaudio*&lt;/code&gt;: Select the worst quality format that contains audio. It may also contain video. Equivalent to &lt;code&gt;worst*[acodec!=none]&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, to download the worst quality video-only format you can use &lt;code&gt;-f worstvideo&lt;/code&gt;. It is, however, recommended not to use &lt;code&gt;worst&lt;/code&gt; and related options. When your format selector is &lt;code&gt;worst&lt;/code&gt;, the format which is worst in all respects is selected. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use &lt;code&gt;-S +size&lt;/code&gt; or more rigorously, &lt;code&gt;-S +size,+br,+res,+fps&lt;/code&gt; instead of &lt;code&gt;-f worst&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats"&gt;Sorting Formats&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;You can select the n'th best format of a type by using &lt;code&gt;best&amp;lt;type&amp;gt;.&amp;lt;n&amp;gt;&lt;/code&gt;. For example, &lt;code&gt;best.2&lt;/code&gt; will select the 2nd best combined format. Similarly, &lt;code&gt;bv*.3&lt;/code&gt; will select the 3rd best format that contains a video stream.&lt;/p&gt; 
&lt;p&gt;If you want to download multiple videos, and they don't have the same formats available, you can specify the order of preference using slashes. Note that formats on the left hand side are preferred; e.g. &lt;code&gt;-f 22/17/18&lt;/code&gt; will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.&lt;/p&gt; 
&lt;p&gt;If you want to download several formats of the same video use a comma as a separator, e.g. &lt;code&gt;-f 22,17,18&lt;/code&gt; will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: &lt;code&gt;-f 136/137/mp4/bestvideo,140/m4a/bestaudio&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can merge the video and audio of multiple formats into a single file using &lt;code&gt;-f &amp;lt;format1&amp;gt;+&amp;lt;format2&amp;gt;+...&lt;/code&gt; (requires ffmpeg installed); e.g. &lt;code&gt;-f bestvideo+bestaudio&lt;/code&gt; will download the best video-only format, the best audio-only format and mux them together with ffmpeg.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deprecation warning&lt;/strong&gt;: Since the &lt;em&gt;below&lt;/em&gt; described behavior is complex and counter-intuitive, this will be removed and multistreams will be enabled by default in the future. A new operator will be instead added to limit formats to single audio/video&lt;/p&gt; 
&lt;p&gt;Unless &lt;code&gt;--video-multistreams&lt;/code&gt; is used, all formats with a video stream except the first one are ignored. Similarly, unless &lt;code&gt;--audio-multistreams&lt;/code&gt; is used, all formats with an audio stream except the first one are ignored. E.g. &lt;code&gt;-f bestvideo+best+bestaudio --video-multistreams --audio-multistreams&lt;/code&gt; will download and merge all 3 given formats. The resulting file will have 2 video streams and 2 audio streams. But &lt;code&gt;-f bestvideo+best+bestaudio --no-video-multistreams&lt;/code&gt; will download and merge only &lt;code&gt;bestvideo&lt;/code&gt; and &lt;code&gt;bestaudio&lt;/code&gt;. &lt;code&gt;best&lt;/code&gt; is ignored since another format containing a video stream (&lt;code&gt;bestvideo&lt;/code&gt;) has already been selected. The order of the formats is therefore important. &lt;code&gt;-f best+bestaudio --no-audio-multistreams&lt;/code&gt; will download only &lt;code&gt;best&lt;/code&gt; while &lt;code&gt;-f bestaudio+best --no-audio-multistreams&lt;/code&gt; will ignore &lt;code&gt;best&lt;/code&gt; and download only &lt;code&gt;bestaudio&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Filtering Formats&lt;/h2&gt; 
&lt;p&gt;You can also filter the video formats by putting a condition in brackets, as in &lt;code&gt;-f "best[height=720]"&lt;/code&gt; (or &lt;code&gt;-f "[filesize&amp;gt;10M]"&lt;/code&gt; since filters without a selector are interpreted as &lt;code&gt;best&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;The following numeric meta fields can be used with comparisons &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;!=&lt;/code&gt; (not equals):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;filesize&lt;/code&gt;: The number of bytes, if known in advance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filesize_approx&lt;/code&gt;: An estimate for the number of bytes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;width&lt;/code&gt;: Width of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;height&lt;/code&gt;: Height of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;aspect_ratio&lt;/code&gt;: Aspect ratio of the video, if known&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tbr&lt;/code&gt;: Average bitrate of audio and video in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;abr&lt;/code&gt;: Average audio bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vbr&lt;/code&gt;: Average video bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;asr&lt;/code&gt;: Audio sampling rate in Hertz&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fps&lt;/code&gt;: Frame rate&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;audio_channels&lt;/code&gt;: The number of audio channels&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stretched_ratio&lt;/code&gt;: &lt;code&gt;width:height&lt;/code&gt; of the video's pixels, if not square&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also filtering work for comparisons &lt;code&gt;=&lt;/code&gt; (equals), &lt;code&gt;^=&lt;/code&gt; (starts with), &lt;code&gt;$=&lt;/code&gt; (ends with), &lt;code&gt;*=&lt;/code&gt; (contains), &lt;code&gt;~=&lt;/code&gt; (matches regex) and following string meta fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;url&lt;/code&gt;: Video URL&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: File extension&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;acodec&lt;/code&gt;: Name of the audio codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: Name of the video codec in use&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;container&lt;/code&gt;: Name of the container format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;protocol&lt;/code&gt;: The protocol that will be used for the actual download, lower-case (&lt;code&gt;http&lt;/code&gt;, &lt;code&gt;https&lt;/code&gt;, &lt;code&gt;rtsp&lt;/code&gt;, &lt;code&gt;rtmp&lt;/code&gt;, &lt;code&gt;rtmpe&lt;/code&gt;, &lt;code&gt;mms&lt;/code&gt;, &lt;code&gt;f4m&lt;/code&gt;, &lt;code&gt;ism&lt;/code&gt;, &lt;code&gt;http_dash_segments&lt;/code&gt;, &lt;code&gt;m3u8&lt;/code&gt;, or &lt;code&gt;m3u8_native&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;language&lt;/code&gt;: Language code&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dynamic_range&lt;/code&gt;: The dynamic range of the video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_id&lt;/code&gt;: A short description of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format&lt;/code&gt;: A human-readable description of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;format_note&lt;/code&gt;: Additional info about the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resolution&lt;/code&gt;: Textual description of width and height&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Any string comparison may be prefixed with negation &lt;code&gt;!&lt;/code&gt; in order to produce an opposite comparison, e.g. &lt;code&gt;!*=&lt;/code&gt; (does not contain). The comparand of a string comparison needs to be quoted with either double or single quotes if it contains spaces or special characters other than &lt;code&gt;._-&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: None of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by the particular extractor, i.e. the metadata offered by the website. Any other field made available by the extractor can also be used for filtering.&lt;/p&gt; 
&lt;p&gt;Formats for which the value is not known are excluded unless you put a question mark (&lt;code&gt;?&lt;/code&gt;) after the operator. You can combine format filters, so &lt;code&gt;-f "bv[height&amp;lt;=?720][tbr&amp;gt;500]"&lt;/code&gt; selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 kbps. You can also use the filters with &lt;code&gt;all&lt;/code&gt; to download all formats that satisfy the filter, e.g. &lt;code&gt;-f "all[vcodec=none]"&lt;/code&gt; selects all audio-only formats.&lt;/p&gt; 
&lt;p&gt;Format selectors can also be grouped using parentheses; e.g. &lt;code&gt;-f "(mp4,webm)[height&amp;lt;480]"&lt;/code&gt; will download the best pre-merged mp4 and webm formats with a height lower than 480.&lt;/p&gt; 
&lt;h2&gt;Sorting Formats&lt;/h2&gt; 
&lt;p&gt;You can change the criteria for being considered the &lt;code&gt;best&lt;/code&gt; by using &lt;code&gt;-S&lt;/code&gt; (&lt;code&gt;--format-sort&lt;/code&gt;). The general format for this is &lt;code&gt;--format-sort field1,field2...&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The available fields are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;hasvid&lt;/code&gt;: Gives priority to formats that have a video stream&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;hasaud&lt;/code&gt;: Gives priority to formats that have an audio stream&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ie_pref&lt;/code&gt;: The format preference&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;lang&lt;/code&gt;: The language preference as determined by the extractor (e.g. original language preferred over audio description)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;quality&lt;/code&gt;: The quality of the format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;source&lt;/code&gt;: The preference of the source&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;proto&lt;/code&gt;: Protocol used for download (&lt;code&gt;https&lt;/code&gt;/&lt;code&gt;ftps&lt;/code&gt; &amp;gt; &lt;code&gt;http&lt;/code&gt;/&lt;code&gt;ftp&lt;/code&gt; &amp;gt; &lt;code&gt;m3u8_native&lt;/code&gt;/&lt;code&gt;m3u8&lt;/code&gt; &amp;gt; &lt;code&gt;http_dash_segments&lt;/code&gt;&amp;gt; &lt;code&gt;websocket_frag&lt;/code&gt; &amp;gt; &lt;code&gt;mms&lt;/code&gt;/&lt;code&gt;rtsp&lt;/code&gt; &amp;gt; &lt;code&gt;f4f&lt;/code&gt;/&lt;code&gt;f4m&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: Video Codec (&lt;code&gt;av01&lt;/code&gt; &amp;gt; &lt;code&gt;vp9.2&lt;/code&gt; &amp;gt; &lt;code&gt;vp9&lt;/code&gt; &amp;gt; &lt;code&gt;h265&lt;/code&gt; &amp;gt; &lt;code&gt;h264&lt;/code&gt; &amp;gt; &lt;code&gt;vp8&lt;/code&gt; &amp;gt; &lt;code&gt;h263&lt;/code&gt; &amp;gt; &lt;code&gt;theora&lt;/code&gt; &amp;gt; other)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;acodec&lt;/code&gt;: Audio Codec (&lt;code&gt;flac&lt;/code&gt;/&lt;code&gt;alac&lt;/code&gt; &amp;gt; &lt;code&gt;wav&lt;/code&gt;/&lt;code&gt;aiff&lt;/code&gt; &amp;gt; &lt;code&gt;opus&lt;/code&gt; &amp;gt; &lt;code&gt;vorbis&lt;/code&gt; &amp;gt; &lt;code&gt;aac&lt;/code&gt; &amp;gt; &lt;code&gt;mp4a&lt;/code&gt; &amp;gt; &lt;code&gt;mp3&lt;/code&gt; &amp;gt; &lt;code&gt;ac4&lt;/code&gt; &amp;gt; &lt;code&gt;eac3&lt;/code&gt; &amp;gt; &lt;code&gt;ac3&lt;/code&gt; &amp;gt; &lt;code&gt;dts&lt;/code&gt; &amp;gt; other)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;codec&lt;/code&gt;: Equivalent to &lt;code&gt;vcodec,acodec&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vext&lt;/code&gt;: Video Extension (&lt;code&gt;mp4&lt;/code&gt; &amp;gt; &lt;code&gt;mov&lt;/code&gt; &amp;gt; &lt;code&gt;webm&lt;/code&gt; &amp;gt; &lt;code&gt;flv&lt;/code&gt; &amp;gt; other). If &lt;code&gt;--prefer-free-formats&lt;/code&gt; is used, &lt;code&gt;webm&lt;/code&gt; is preferred.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;aext&lt;/code&gt;: Audio Extension (&lt;code&gt;m4a&lt;/code&gt; &amp;gt; &lt;code&gt;aac&lt;/code&gt; &amp;gt; &lt;code&gt;mp3&lt;/code&gt; &amp;gt; &lt;code&gt;ogg&lt;/code&gt; &amp;gt; &lt;code&gt;opus&lt;/code&gt; &amp;gt; &lt;code&gt;webm&lt;/code&gt; &amp;gt; other). If &lt;code&gt;--prefer-free-formats&lt;/code&gt; is used, the order changes to &lt;code&gt;ogg&lt;/code&gt; &amp;gt; &lt;code&gt;opus&lt;/code&gt; &amp;gt; &lt;code&gt;webm&lt;/code&gt; &amp;gt; &lt;code&gt;mp3&lt;/code&gt; &amp;gt; &lt;code&gt;m4a&lt;/code&gt; &amp;gt; &lt;code&gt;aac&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: Equivalent to &lt;code&gt;vext,aext&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;filesize&lt;/code&gt;: Exact filesize, if known in advance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fs_approx&lt;/code&gt;: Approximate filesize&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;size&lt;/code&gt;: Exact filesize if available, otherwise approximate filesize&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;height&lt;/code&gt;: Height of video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;width&lt;/code&gt;: Width of video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;res&lt;/code&gt;: Video resolution, calculated as the smallest dimension.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fps&lt;/code&gt;: Framerate of video&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;hdr&lt;/code&gt;: The dynamic range of the video (&lt;code&gt;DV&lt;/code&gt; &amp;gt; &lt;code&gt;HDR12&lt;/code&gt; &amp;gt; &lt;code&gt;HDR10+&lt;/code&gt; &amp;gt; &lt;code&gt;HDR10&lt;/code&gt; &amp;gt; &lt;code&gt;HLG&lt;/code&gt; &amp;gt; &lt;code&gt;SDR&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;channels&lt;/code&gt;: The number of audio channels&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tbr&lt;/code&gt;: Total average bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vbr&lt;/code&gt;: Average video bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;abr&lt;/code&gt;: Average audio bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;br&lt;/code&gt;: Average bitrate in &lt;a href="##" title="1000 bits/sec"&gt;kbps&lt;/a&gt;, &lt;code&gt;tbr&lt;/code&gt;/&lt;code&gt;vbr&lt;/code&gt;/&lt;code&gt;abr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;asr&lt;/code&gt;: Audio sample rate in Hz&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Deprecation warning&lt;/strong&gt;: Many of these fields have (currently undocumented) aliases, that may be removed in a future version. It is recommended to use only the documented field names.&lt;/p&gt; 
&lt;p&gt;All fields, unless specified otherwise, are sorted in descending order. To reverse this, prefix the field with a &lt;code&gt;+&lt;/code&gt;. E.g. &lt;code&gt;+res&lt;/code&gt; prefers format with the smallest resolution. Additionally, you can suffix a preferred value for the fields, separated by a &lt;code&gt;:&lt;/code&gt;. E.g. &lt;code&gt;res:720&lt;/code&gt; prefers larger videos, but no larger than 720p and the smallest video if there are no videos less than 720p. For &lt;code&gt;codec&lt;/code&gt; and &lt;code&gt;ext&lt;/code&gt;, you can provide two preferred values, the first for video and the second for audio. E.g. &lt;code&gt;+codec:avc:m4a&lt;/code&gt; (equivalent to &lt;code&gt;+vcodec:avc,+acodec:m4a&lt;/code&gt;) sets the video codec preference to &lt;code&gt;h264&lt;/code&gt; &amp;gt; &lt;code&gt;h265&lt;/code&gt; &amp;gt; &lt;code&gt;vp9&lt;/code&gt; &amp;gt; &lt;code&gt;vp9.2&lt;/code&gt; &amp;gt; &lt;code&gt;av01&lt;/code&gt; &amp;gt; &lt;code&gt;vp8&lt;/code&gt; &amp;gt; &lt;code&gt;h263&lt;/code&gt; &amp;gt; &lt;code&gt;theora&lt;/code&gt; and audio codec preference to &lt;code&gt;mp4a&lt;/code&gt; &amp;gt; &lt;code&gt;aac&lt;/code&gt; &amp;gt; &lt;code&gt;vorbis&lt;/code&gt; &amp;gt; &lt;code&gt;opus&lt;/code&gt; &amp;gt; &lt;code&gt;mp3&lt;/code&gt; &amp;gt; &lt;code&gt;ac3&lt;/code&gt; &amp;gt; &lt;code&gt;dts&lt;/code&gt;. You can also make the sorting prefer the nearest values to the provided by using &lt;code&gt;~&lt;/code&gt; as the delimiter. E.g. &lt;code&gt;filesize~1G&lt;/code&gt; prefers the format with filesize closest to 1 GiB.&lt;/p&gt; 
&lt;p&gt;The fields &lt;code&gt;hasvid&lt;/code&gt; and &lt;code&gt;ie_pref&lt;/code&gt; are always given highest priority in sorting, irrespective of the user-defined order. This behavior can be changed by using &lt;code&gt;--format-sort-force&lt;/code&gt;. Apart from these, the default order used is: &lt;code&gt;lang,quality,res,fps,hdr:12,vcodec,channels,acodec,size,br,asr,proto,ext,hasaud,source,id&lt;/code&gt;. The extractors may override this default order, but they cannot override the user-provided order.&lt;/p&gt; 
&lt;p&gt;Note that the default for hdr is &lt;code&gt;hdr:12&lt;/code&gt;; i.e. Dolby Vision is not preferred. This choice was made since DV formats are not yet fully compatible with most devices. This may be changed in the future.&lt;/p&gt; 
&lt;p&gt;If your format selector is &lt;code&gt;worst&lt;/code&gt;, the last item is selected after sorting. This means it will select the format that is worst in all respects. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use &lt;code&gt;-f best -S +size,+br,+res,+fps&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you use the &lt;code&gt;-S&lt;/code&gt;/&lt;code&gt;--format-sort&lt;/code&gt; option multiple times, each subsequent sorting argument will be prepended to the previous one, and only the highest priority entry of any duplicated field will be preserved. E.g. &lt;code&gt;-S proto -S res&lt;/code&gt; is equivalent to &lt;code&gt;-S res,proto&lt;/code&gt;, and &lt;code&gt;-S res:720,fps -S vcodec,res:1080&lt;/code&gt; is equivalent to &lt;code&gt;-S vcodec,res:1080,fps&lt;/code&gt;. You can use &lt;code&gt;--format-sort-reset&lt;/code&gt; to disregard any previously passed &lt;code&gt;-S&lt;/code&gt;/&lt;code&gt;--format-sort&lt;/code&gt; arguments and reset to the default order.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: You can use the &lt;code&gt;-v -F&lt;/code&gt; to see how the formats have been sorted (worst to best).&lt;/p&gt; 
&lt;h2&gt;Format Selection examples&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download and merge the best video-only format and the best audio-only format,
# or download the best combined format if video-only format is not available
$ yt-dlp -f "bv+ba/b"

# Download best format that contains video,
# and if it doesn't already have an audio stream, merge it with best audio-only format
$ yt-dlp -f "bv*+ba/b"

# Same as above
$ yt-dlp

# Download the best video-only format and the best audio-only format without merging them
# For this case, an output template should be used since
# by default, bestvideo and bestaudio will have the same file name.
$ yt-dlp -f "bv,ba" -o "%(title)s.f%(format_id)s.%(ext)s"

# Download and merge the best format that has a video stream,
# and all audio-only formats into one file
$ yt-dlp -f "bv*+mergeall[vcodec=none]" --audio-multistreams

# Download and merge the best format that has a video stream,
# and the best 2 audio-only formats into one file
$ yt-dlp -f "bv*+ba+ba.2" --audio-multistreams


# The following examples show the old method (without -S) of format selection
# and how to use -S to achieve a similar but (generally) better result

# Download the worst video available (old method)
$ yt-dlp -f "wv*+wa/w"

# Download the best video available but with the smallest resolution
$ yt-dlp -S "+res"

# Download the smallest video available
$ yt-dlp -S "+size,+br"



# Download the best mp4 video available, or the best video if no mp4 available
$ yt-dlp -f "bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b"

# Download the best video with the best extension
# (For video, mp4 &amp;gt; mov &amp;gt; webm &amp;gt; flv. For audio, m4a &amp;gt; aac &amp;gt; mp3 ...)
$ yt-dlp -S "ext"



# Download the best video available but no better than 480p,
# or the worst video if there is no video under 480p
$ yt-dlp -f "bv*[height&amp;lt;=480]+ba/b[height&amp;lt;=480] / wv*+ba/w"

# Download the best video available with the largest height but no better than 480p,
# or the best video with the smallest resolution if there is no video under 480p
$ yt-dlp -S "height:480"

# Download the best video available with the largest resolution but no better than 480p,
# or the best video with the smallest resolution if there is no video under 480p
# Resolution is determined by using the smallest dimension.
# So this works correctly for vertical videos as well
$ yt-dlp -S "res:480"



# Download the best video (that also has audio) but no bigger than 50 MB,
# or the worst video (that also has audio) if there is no video under 50 MB
$ yt-dlp -f "b[filesize&amp;lt;50M] / w"

# Download the largest video (that also has audio) but no bigger than 50 MB,
# or the smallest video (that also has audio) if there is no video under 50 MB
$ yt-dlp -f "b" -S "filesize:50M"

# Download the best video (that also has audio) that is closest in size to 50 MB
$ yt-dlp -f "b" -S "filesize~50M"



# Download best video available via direct link over HTTP/HTTPS protocol,
# or the best video available via any protocol if there is no such video
$ yt-dlp -f "(bv*+ba/b)[protocol^=http][protocol!*=dash] / (bv*+ba/b)"

# Download best video available via the best protocol
# (https/ftps &amp;gt; http/ftp &amp;gt; m3u8_native &amp;gt; m3u8 &amp;gt; http_dash_segments ...)
$ yt-dlp -S "proto"



# Download the best video with either h264 or h265 codec,
# or the best video if there is no such video
$ yt-dlp -f "(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)"

# Download the best video with best codec no better than h264,
# or the best video with worst codec if there is no such video
$ yt-dlp -S "codec:h264"

# Download the best video with worst codec no worse than h264,
# or the best video with best codec if there is no such video
$ yt-dlp -S "+codec:h264"



# More complex examples

# Download the best video no better than 720p preferring framerate greater than 30,
# or the worst video (still preferring framerate greater than 30) if there is no such video
$ yt-dlp -f "((bv*[fps&amp;gt;30]/bv*)[height&amp;lt;=720]/(wv*[fps&amp;gt;30]/wv*)) + ba / (b[fps&amp;gt;30]/b)[height&amp;lt;=720]/(w[fps&amp;gt;30]/w)"

# Download the video with the largest resolution no better than 720p,
# or the video with the smallest resolution available if there is no such video,
# preferring larger framerate for formats with the same resolution
$ yt-dlp -S "res:720,fps"



# Download the video with smallest resolution no worse than 480p,
# or the video with the largest resolution available if there is no such video,
# preferring better codec and then larger total bitrate for the same resolution
$ yt-dlp -S "+res:480,codec,br"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;MODIFYING METADATA&lt;/h1&gt; 
&lt;p&gt;The metadata obtained by the extractors can be modified by using &lt;code&gt;--parse-metadata&lt;/code&gt; and &lt;code&gt;--replace-in-metadata&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--replace-in-metadata FIELDS REGEX REPLACE&lt;/code&gt; is used to replace text in any metadata field using &lt;a href="https://docs.python.org/3/library/re.html#regular-expression-syntax"&gt;Python regular expression&lt;/a&gt;. &lt;a href="https://docs.python.org/3/library/re.html?highlight=backreferences#re.sub"&gt;Backreferences&lt;/a&gt; can be used in the replace string for advanced use.&lt;/p&gt; 
&lt;p&gt;The general syntax of &lt;code&gt;--parse-metadata FROM:TO&lt;/code&gt; is to give the name of a field or an &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; to extract data from, and the format to interpret it as, separated by a colon &lt;code&gt;:&lt;/code&gt;. Either a &lt;a href="https://docs.python.org/3/library/re.html#regular-expression-syntax"&gt;Python regular expression&lt;/a&gt; with named capture groups, a single field name, or a similar syntax to the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; (only &lt;code&gt;%(field)s&lt;/code&gt; formatting is supported) can be used for &lt;code&gt;TO&lt;/code&gt;. The option can be used multiple times to parse and modify various fields.&lt;/p&gt; 
&lt;p&gt;Note that these options preserve their relative order, allowing replacements to be made in parsed fields and vice versa. Also, any field thus created can be used in the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; and will also affect the media file's metadata added when using &lt;code&gt;--embed-metadata&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;This option also has a few special uses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can download an additional URL based on the metadata of the currently downloaded video. To do this, set the field &lt;code&gt;additional_urls&lt;/code&gt; to the URL that you want to download. E.g. &lt;code&gt;--parse-metadata "description:(?P&amp;lt;additional_urls&amp;gt;https?://www\.vimeo\.com/\d+)"&lt;/code&gt; will download the first vimeo video found in the description&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can use this to change the metadata that is embedded in the media file. To do this, set the value of the corresponding field with a &lt;code&gt;meta_&lt;/code&gt; prefix. For example, any value you set to &lt;code&gt;meta_description&lt;/code&gt; field will be added to the &lt;code&gt;description&lt;/code&gt; field in the file - you can use this to set a different "description" and "synopsis". To modify the metadata of individual streams, use the &lt;code&gt;meta&amp;lt;n&amp;gt;_&lt;/code&gt; prefix (e.g. &lt;code&gt;meta1_language&lt;/code&gt;). Any value set to the &lt;code&gt;meta_&lt;/code&gt; field will overwrite all default values.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Metadata modification happens before format selection, post-extraction and other post-processing operations. Some fields may be added or changed during these steps, overriding your changes.&lt;/p&gt; 
&lt;p&gt;For reference, these are the fields yt-dlp adds by default to the file metadata:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Metadata fields&lt;/th&gt; 
   &lt;th align="left"&gt;From&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;title&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;track&lt;/code&gt; or &lt;code&gt;title&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;date&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;upload_date&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;description&lt;/code&gt;, &lt;code&gt;synopsis&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;purl&lt;/code&gt;, &lt;code&gt;comment&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;webpage_url&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;track&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;track_number&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;artist&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;artist&lt;/code&gt;, &lt;code&gt;artists&lt;/code&gt;, &lt;code&gt;creator&lt;/code&gt;, &lt;code&gt;creators&lt;/code&gt;, &lt;code&gt;uploader&lt;/code&gt; or &lt;code&gt;uploader_id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;composer&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;composer&lt;/code&gt; or &lt;code&gt;composers&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;genre&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;genre&lt;/code&gt;, &lt;code&gt;genres&lt;/code&gt;, &lt;code&gt;categories&lt;/code&gt; or &lt;code&gt;tags&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;album&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;album&lt;/code&gt; or &lt;code&gt;series&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;album_artist&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;album_artist&lt;/code&gt; or &lt;code&gt;album_artists&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;disc&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;disc_number&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;show&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;series&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;season_number&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;season_number&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;episode_id&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;episode&lt;/code&gt; or &lt;code&gt;episode_id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;episode_sort&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;episode_number&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;language&lt;/code&gt; of each stream&lt;/td&gt; 
   &lt;td align="left"&gt;the format's &lt;code&gt;language&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The file format may not support some of these fields&lt;/p&gt; 
&lt;h2&gt;Modifying metadata examples&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Interpret the title as "Artist - Title"
$ yt-dlp --parse-metadata "title:%(artist)s - %(title)s"

# Regex example
$ yt-dlp --parse-metadata "description:Artist - (?P&amp;lt;artist&amp;gt;.+)"

# Copy the episode field to the title field (with FROM and TO as single fields)
$ yt-dlp --parse-metadata "episode:title"

# Set title as "Series name S01E05"
$ yt-dlp --parse-metadata "%(series)s S%(season_number)02dE%(episode_number)02d:%(title)s"

# Prioritize uploader as the "artist" field in video metadata
$ yt-dlp --parse-metadata "%(uploader|)s:%(meta_artist)s" --embed-metadata

# Set "comment" field in video metadata using description instead of webpage_url,
# handling multiple lines correctly
$ yt-dlp --parse-metadata "description:(?s)(?P&amp;lt;meta_comment&amp;gt;.+)" --embed-metadata

# Do not set any "synopsis" in the video metadata
$ yt-dlp --parse-metadata ":(?P&amp;lt;meta_synopsis&amp;gt;)"

# Remove "formats" field from the infojson by setting it to an empty string
$ yt-dlp --parse-metadata "video::(?P&amp;lt;formats&amp;gt;)" --write-info-json

# Replace all spaces and "_" in title and uploader with a `-`
$ yt-dlp --replace-in-metadata "title,uploader" "[ _]" "-"

&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;EXTRACTOR ARGUMENTS&lt;/h1&gt; 
&lt;p&gt;Some extractors accept additional arguments which can be passed using &lt;code&gt;--extractor-args KEY:ARGS&lt;/code&gt;. &lt;code&gt;ARGS&lt;/code&gt; is a &lt;code&gt;;&lt;/code&gt; (semicolon) separated string of &lt;code&gt;ARG=VAL1,VAL2&lt;/code&gt;. E.g. &lt;code&gt;--extractor-args "youtube:player-client=tv,mweb;formats=incomplete" --extractor-args "twitter:api=syndication"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Note: In CLI, &lt;code&gt;ARG&lt;/code&gt; can use &lt;code&gt;-&lt;/code&gt; instead of &lt;code&gt;_&lt;/code&gt;; e.g. &lt;code&gt;youtube:player-client"&lt;/code&gt; becomes &lt;code&gt;youtube:player_client"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;The following extractors use this feature:&lt;/p&gt; 
&lt;h4&gt;youtube&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;lang&lt;/code&gt;: Prefer translated metadata (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt; etc) of this language code (case-sensitive). By default, the video primary language metadata is preferred, with a fallback to &lt;code&gt;en&lt;/code&gt; translated. See &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/415b4c9f955b1a0391204bd24a7132590e7b3bdb/yt_dlp/extractor/youtube/_base.py#L402-L409"&gt;youtube/_base.py&lt;/a&gt; for the list of supported content language codes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;skip&lt;/code&gt;: One or more of &lt;code&gt;hls&lt;/code&gt;, &lt;code&gt;dash&lt;/code&gt; or &lt;code&gt;translated_subs&lt;/code&gt; to skip extraction of the m3u8 manifests, dash manifests and &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/4090#issuecomment-1158102032"&gt;auto-translated subtitles&lt;/a&gt; respectively&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_client&lt;/code&gt;: Clients to extract video data from. The currently available clients are &lt;code&gt;web&lt;/code&gt;, &lt;code&gt;web_safari&lt;/code&gt;, &lt;code&gt;web_embedded&lt;/code&gt;, &lt;code&gt;web_music&lt;/code&gt;, &lt;code&gt;web_creator&lt;/code&gt;, &lt;code&gt;mweb&lt;/code&gt;, &lt;code&gt;ios&lt;/code&gt;, &lt;code&gt;ios_downgraded&lt;/code&gt;, &lt;code&gt;android&lt;/code&gt;, &lt;code&gt;android_vr&lt;/code&gt;, &lt;code&gt;tv&lt;/code&gt;, &lt;code&gt;tv_simply&lt;/code&gt;, &lt;code&gt;tv_downgraded&lt;/code&gt;, and &lt;code&gt;tv_embedded&lt;/code&gt;. By default, &lt;code&gt;android_vr,ios_downgraded,web,web_safari&lt;/code&gt; is used. If no JavaScript runtime/engine is available, then &lt;code&gt;android_vr,ios_downgraded&lt;/code&gt; is used. If logged-in cookies are passed to yt-dlp, then &lt;code&gt;tv_downgraded,web,web_safari&lt;/code&gt; is used for free accounts and &lt;code&gt;tv_downgraded,web_creator,web&lt;/code&gt; is used for premium accounts. The &lt;code&gt;web_music&lt;/code&gt; client is added for &lt;code&gt;music.youtube.com&lt;/code&gt; URLs when logged-in cookies are used. The &lt;code&gt;web_embedded&lt;/code&gt; client is added for age-restricted videos but only works if the video is embeddable. The &lt;code&gt;tv_embedded&lt;/code&gt; and &lt;code&gt;web_creator&lt;/code&gt; clients are added for age-restricted videos if account age-verification is required. Some clients, such as &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;web_music&lt;/code&gt;, require a &lt;code&gt;po_token&lt;/code&gt; for their formats to be downloadable. Some clients, such as &lt;code&gt;web_creator&lt;/code&gt;, will only work with authentication. Not all clients support authentication via cookies. You can use &lt;code&gt;default&lt;/code&gt; for the default clients, or you can use &lt;code&gt;all&lt;/code&gt; for all clients (not recommended). You can prefix a client with &lt;code&gt;-&lt;/code&gt; to exclude it, e.g. &lt;code&gt;youtube:player_client=default,-ios&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_skip&lt;/code&gt;: Skip some network requests that are generally needed for robust extraction. One or more of &lt;code&gt;configs&lt;/code&gt; (skip client configs), &lt;code&gt;webpage&lt;/code&gt; (skip initial webpage), &lt;code&gt;js&lt;/code&gt; (skip js player), &lt;code&gt;initial_data&lt;/code&gt; (skip initial data/next ep request). While these options can help reduce the number of requests needed or avoid some rate-limiting, they could cause issues such as missing formats or metadata. See &lt;a href="https://github.com/yt-dlp/yt-dlp/pull/860"&gt;#860&lt;/a&gt; and &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/12826"&gt;#12826&lt;/a&gt; for more details&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webpage_skip&lt;/code&gt;: Skip extraction of embedded webpage data. One or both of &lt;code&gt;player_response&lt;/code&gt;, &lt;code&gt;initial_data&lt;/code&gt;. These options are for testing purposes and don't skip any network requests&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_params&lt;/code&gt;: YouTube player parameters to use for player requests. Will overwrite any default ones set by yt-dlp.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_js_variant&lt;/code&gt;: The player javascript variant to use for n/sig deciphering. The known variants are: &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;tcc&lt;/code&gt;, &lt;code&gt;tce&lt;/code&gt;, &lt;code&gt;es5&lt;/code&gt;, &lt;code&gt;es6&lt;/code&gt;, &lt;code&gt;tv&lt;/code&gt;, &lt;code&gt;tv_es6&lt;/code&gt;, &lt;code&gt;phone&lt;/code&gt;, &lt;code&gt;tablet&lt;/code&gt;. The default is &lt;code&gt;main&lt;/code&gt;, and the others are for debugging purposes. You can use &lt;code&gt;actual&lt;/code&gt; to go with what is prescribed by the site&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;player_js_version&lt;/code&gt;: The player javascript version to use for n/sig deciphering, in the format of &lt;code&gt;signature_timestamp@hash&lt;/code&gt; (e.g. &lt;code&gt;20348@0004de42&lt;/code&gt;). The default is to use what is prescribed by the site, and can be selected with &lt;code&gt;actual&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;comment_sort&lt;/code&gt;: &lt;code&gt;top&lt;/code&gt; or &lt;code&gt;new&lt;/code&gt; (default) - choose comment sorting mode (on YouTube's side)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;max_comments&lt;/code&gt;: Limit the amount of comments to gather. Comma-separated list of integers representing &lt;code&gt;max-comments,max-parents,max-replies,max-replies-per-thread,max-depth&lt;/code&gt;. Default is &lt;code&gt;all,all,all,all,all&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A &lt;code&gt;max-depth&lt;/code&gt; value of &lt;code&gt;1&lt;/code&gt; will discard all replies, regardless of the &lt;code&gt;max-replies&lt;/code&gt; or &lt;code&gt;max-replies-per-thread&lt;/code&gt; values given&lt;/li&gt; 
   &lt;li&gt;E.g. &lt;code&gt;all,all,1000,10,2&lt;/code&gt; will get a maximum of 1000 replies total, with up to 10 replies per thread, and only 2 levels of depth (i.e. top-level comments plus their immediate replies). &lt;code&gt;1000,all,100&lt;/code&gt; will get a maximum of 1000 comments, with a maximum of 100 replies total&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;formats&lt;/code&gt;: Change the types of formats to return. &lt;code&gt;dashy&lt;/code&gt; (convert HTTP to DASH), &lt;code&gt;duplicate&lt;/code&gt; (identical content but different URLs or protocol; includes &lt;code&gt;dashy&lt;/code&gt;), &lt;code&gt;incomplete&lt;/code&gt; (cannot be downloaded completely - live dash and post-live m3u8), &lt;code&gt;missing_pot&lt;/code&gt; (include formats that require a PO Token but are missing one)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;innertube_host&lt;/code&gt;: Innertube API host to use for all API requests; e.g. &lt;code&gt;studio.youtube.com&lt;/code&gt;, &lt;code&gt;youtubei.googleapis.com&lt;/code&gt;. Note that cookies exported from one subdomain will not work on others&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;innertube_key&lt;/code&gt;: Innertube API key to use for all API requests. By default, no API key is used&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;raise_incomplete_data&lt;/code&gt;: &lt;code&gt;Incomplete Data Received&lt;/code&gt; raises an error instead of reporting a warning&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;data_sync_id&lt;/code&gt;: Overrides the account Data Sync ID used in Innertube API requests. This may be needed if you are using an account with &lt;code&gt;youtube:player_skip=webpage,configs&lt;/code&gt; or &lt;code&gt;youtubetab:skip=webpage&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;visitor_data&lt;/code&gt;: Overrides the Visitor Data used in Innertube API requests. This should be used with &lt;code&gt;player_skip=webpage,configs&lt;/code&gt; and without cookies. Note: this may have adverse effects if used improperly. If a session from a browser is wanted, you should pass cookies instead (which contain the Visitor ID)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;po_token&lt;/code&gt;: Proof of Origin (PO) Token(s) to use. Comma-separated list of PO Tokens in the format &lt;code&gt;CLIENT.CONTEXT+PO_TOKEN&lt;/code&gt;, e.g. &lt;code&gt;youtube:po_token=web.gvs+XXX,web.player=XXX,web_safari.gvs+YYY&lt;/code&gt;. Context can be any of &lt;code&gt;gvs&lt;/code&gt; (Google Video Server URLs), &lt;code&gt;player&lt;/code&gt; (Innertube player request) or &lt;code&gt;subs&lt;/code&gt; (Subtitles)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pot_trace&lt;/code&gt;: Enable debug logging for PO Token fetching. Either &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; (default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fetch_pot&lt;/code&gt;: Policy to use for fetching a PO Token from providers. One of &lt;code&gt;always&lt;/code&gt; (always try fetch a PO Token regardless if the client requires one for the given context), &lt;code&gt;never&lt;/code&gt; (never fetch a PO Token), or &lt;code&gt;auto&lt;/code&gt; (default; only fetch a PO Token if the client requires one for the given context)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jsc_trace&lt;/code&gt;: Enable debug logging for JS Challenge fetching. Either &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; (default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;use_ad_playback_context&lt;/code&gt;: Skip preroll ads to eliminate the mandatory wait period before download. Do NOT use this when passing premium account cookies to yt-dlp, as it will result in a loss of premium formats. Only effective with the &lt;code&gt;web&lt;/code&gt;, &lt;code&gt;web_safari&lt;/code&gt;, &lt;code&gt;web_music&lt;/code&gt; and &lt;code&gt;mweb&lt;/code&gt; player clients. Either &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; (default)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;youtube-ejs&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;jitless&lt;/code&gt;: Run supported Javascript engines in JIT-less mode. Supported runtimes are &lt;code&gt;deno&lt;/code&gt;, &lt;code&gt;node&lt;/code&gt; and &lt;code&gt;bun&lt;/code&gt;. Provides better security at the cost of performance/speed. Do note that &lt;code&gt;node&lt;/code&gt; and &lt;code&gt;bun&lt;/code&gt; are still considered insecure. Either &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; (default)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;youtubepot-webpo&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bind_to_visitor_id&lt;/code&gt;: Whether to use the Visitor ID instead of Visitor Data for caching WebPO tokens. Either &lt;code&gt;true&lt;/code&gt; (default) or &lt;code&gt;false&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;youtubetab (YouTube playlists, channels, feeds, etc.)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;skip&lt;/code&gt;: One or more of &lt;code&gt;webpage&lt;/code&gt; (skip initial webpage download), &lt;code&gt;authcheck&lt;/code&gt; (allow the download of playlists requiring authentication when no initial webpage is downloaded. This may cause unwanted behavior, see &lt;a href="https://github.com/yt-dlp/yt-dlp/pull/1122"&gt;#1122&lt;/a&gt; for more details)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;approximate_date&lt;/code&gt;: Extract approximate &lt;code&gt;upload_date&lt;/code&gt; and &lt;code&gt;timestamp&lt;/code&gt; in flat-playlist. This may cause date-based filters to be slightly off&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;generic&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;fragment_query&lt;/code&gt;: Passthrough any query in mpd/m3u8 manifest URLs to their fragments if no value is provided, or else apply the query string given as &lt;code&gt;fragment_query=VALUE&lt;/code&gt;. Note that if the stream has an HLS AES-128 key, then the query parameters will be passed to the key URI as well, unless the &lt;code&gt;key_query&lt;/code&gt; extractor-arg is passed, or unless an external key URI is provided via the &lt;code&gt;hls_key&lt;/code&gt; extractor-arg. Does not apply to ffmpeg&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;variant_query&lt;/code&gt;: Passthrough the master m3u8 URL query to its variant playlist URLs if no value is provided, or else apply the query string given as &lt;code&gt;variant_query=VALUE&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;key_query&lt;/code&gt;: Passthrough the master m3u8 URL query to its HLS AES-128 decryption key URI if no value is provided, or else apply the query string given as &lt;code&gt;key_query=VALUE&lt;/code&gt;. Note that this will have no effect if the key URI is provided via the &lt;code&gt;hls_key&lt;/code&gt; extractor-arg. Does not apply to ffmpeg&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;hls_key&lt;/code&gt;: An HLS AES-128 key URI &lt;em&gt;or&lt;/em&gt; key (as hex), and optionally the IV (as hex), in the form of &lt;code&gt;(URI|KEY)[,IV]&lt;/code&gt;; e.g. &lt;code&gt;generic:hls_key=ABCDEF1234567980,0xFEDCBA0987654321&lt;/code&gt;. Passing any of these values will force usage of the native HLS downloader and override the corresponding values found in the m3u8 playlist&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;is_live&lt;/code&gt;: Bypass live HLS detection and manually set &lt;code&gt;live_status&lt;/code&gt; - a value of &lt;code&gt;false&lt;/code&gt; will set &lt;code&gt;not_live&lt;/code&gt;, any other value (or no value) will set &lt;code&gt;is_live&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;impersonate&lt;/code&gt;: Target(s) to try and impersonate with the initial webpage request; e.g. &lt;code&gt;generic:impersonate=safari,chrome-110&lt;/code&gt;. Use &lt;code&gt;generic:impersonate&lt;/code&gt; to impersonate any available target, and use &lt;code&gt;generic:impersonate=false&lt;/code&gt; to disable impersonation (default)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;vikichannel&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;video_types&lt;/code&gt;: Types of videos to download - one or more of &lt;code&gt;episodes&lt;/code&gt;, &lt;code&gt;movies&lt;/code&gt;, &lt;code&gt;clips&lt;/code&gt;, &lt;code&gt;trailers&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;youtubewebarchive&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;check_all&lt;/code&gt;: Try to check more at the cost of more requests. One or more of &lt;code&gt;thumbnails&lt;/code&gt;, &lt;code&gt;captures&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;gamejolt&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;comment_sort&lt;/code&gt;: &lt;code&gt;hot&lt;/code&gt; (default), &lt;code&gt;you&lt;/code&gt; (cookies needed), &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;new&lt;/code&gt; - choose comment sorting mode (on GameJolt's side)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;hotstar&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;res&lt;/code&gt;: resolution to ignore - one or more of &lt;code&gt;sd&lt;/code&gt;, &lt;code&gt;hd&lt;/code&gt;, &lt;code&gt;fhd&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vcodec&lt;/code&gt;: vcodec to ignore - one or more of &lt;code&gt;h264&lt;/code&gt;, &lt;code&gt;h265&lt;/code&gt;, &lt;code&gt;dvh265&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dr&lt;/code&gt;: dynamic range to ignore - one or more of &lt;code&gt;sdr&lt;/code&gt;, &lt;code&gt;hdr10&lt;/code&gt;, &lt;code&gt;dv&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;instagram&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;app_id&lt;/code&gt;: The value of the &lt;code&gt;X-IG-App-ID&lt;/code&gt; header used for API requests. Default is the web app ID, &lt;code&gt;936619743392459&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;niconicochannelplus&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;max_comments&lt;/code&gt;: Maximum number of comments to extract - default is &lt;code&gt;120&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;tiktok&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;api_hostname&lt;/code&gt;: Hostname to use for mobile API calls, e.g. &lt;code&gt;api22-normal-c-alisg.tiktokv.com&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;app_name&lt;/code&gt;: Default app name to use with mobile API calls, e.g. &lt;code&gt;trill&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;app_version&lt;/code&gt;: Default app version to use with mobile API calls - should be set along with &lt;code&gt;manifest_app_version&lt;/code&gt;, e.g. &lt;code&gt;34.1.2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;manifest_app_version&lt;/code&gt;: Default numeric app version to use with mobile API calls, e.g. &lt;code&gt;2023401020&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;aid&lt;/code&gt;: Default app ID to use with mobile API calls, e.g. &lt;code&gt;1180&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;app_info&lt;/code&gt;: Enable mobile API extraction with one or more app info strings in the format of &lt;code&gt;&amp;lt;iid&amp;gt;/[app_name]/[app_version]/[manifest_app_version]/[aid]&lt;/code&gt;, where &lt;code&gt;iid&lt;/code&gt; is the unique app install ID. &lt;code&gt;iid&lt;/code&gt; is the only required value; all other values and their &lt;code&gt;/&lt;/code&gt; separators can be omitted, e.g. &lt;code&gt;tiktok:app_info=1234567890123456789&lt;/code&gt; or &lt;code&gt;tiktok:app_info=123,456/trill///1180,789//34.0.1/340001&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;device_id&lt;/code&gt;: Enable mobile API extraction with a genuine device ID to be used with mobile API calls. Default is a random 19-digit string&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;rokfinchannel&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;tab&lt;/code&gt;: Which tab to download - one of &lt;code&gt;new&lt;/code&gt;, &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;videos&lt;/code&gt;, &lt;code&gt;podcasts&lt;/code&gt;, &lt;code&gt;streams&lt;/code&gt;, &lt;code&gt;stacks&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;twitter&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;api&lt;/code&gt;: Select one of &lt;code&gt;graphql&lt;/code&gt; (default), &lt;code&gt;legacy&lt;/code&gt; or &lt;code&gt;syndication&lt;/code&gt; as the API for tweet extraction. Has no effect if logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;stacommu, wrestleuniverse&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;device_id&lt;/code&gt;: UUID value assigned by the website and used to enforce device limits for paid livestream content. Can be found in browser local storage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;twitch&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;client_id&lt;/code&gt;: Client ID value to be sent with GraphQL requests, e.g. &lt;code&gt;twitch:client_id=kimne78kx3ncx6brgo4mv6wki5h1ko&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;nhkradirulive (NHK „Çâ„Åò„Çã‚òÖ„Çâ„Åò„Çã LIVE)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;area&lt;/code&gt;: Which regional variation to extract. Valid areas are: &lt;code&gt;sapporo&lt;/code&gt;, &lt;code&gt;sendai&lt;/code&gt;, &lt;code&gt;tokyo&lt;/code&gt;, &lt;code&gt;nagoya&lt;/code&gt;, &lt;code&gt;osaka&lt;/code&gt;, &lt;code&gt;hiroshima&lt;/code&gt;, &lt;code&gt;matsuyama&lt;/code&gt;, &lt;code&gt;fukuoka&lt;/code&gt;. Defaults to &lt;code&gt;tokyo&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;nflplusreplay&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type&lt;/code&gt;: Type(s) of game replays to extract. Valid types are: &lt;code&gt;full_game&lt;/code&gt;, &lt;code&gt;full_game_spanish&lt;/code&gt;, &lt;code&gt;condensed_game&lt;/code&gt; and &lt;code&gt;all_22&lt;/code&gt;. You can use &lt;code&gt;all&lt;/code&gt; to extract all available replay types, which is the default&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;jiocinema&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;refresh_token&lt;/code&gt;: The &lt;code&gt;refreshToken&lt;/code&gt; UUID from browser local storage can be passed to extend the life of your login session when logging in with &lt;code&gt;token&lt;/code&gt; as username and the &lt;code&gt;accessToken&lt;/code&gt; from browser local storage as password&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;jiosaavn&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bitrate&lt;/code&gt;: Audio bitrates to request. One or more of &lt;code&gt;16&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt;, &lt;code&gt;64&lt;/code&gt;, &lt;code&gt;128&lt;/code&gt;, &lt;code&gt;320&lt;/code&gt;. Default is &lt;code&gt;128,320&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;afreecatvlive&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cdn&lt;/code&gt;: One or more CDN IDs to use with the API call for stream URLs, e.g. &lt;code&gt;gcp_cdn&lt;/code&gt;, &lt;code&gt;gs_cdn_pc_app&lt;/code&gt;, &lt;code&gt;gs_cdn_mobile_web&lt;/code&gt;, &lt;code&gt;gs_cdn_pc_web&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;soundcloud&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;formats&lt;/code&gt;: Formats to request from the API. Requested values should be in the format of &lt;code&gt;{protocol}_{codec}&lt;/code&gt;, e.g. &lt;code&gt;hls_opus,http_aac&lt;/code&gt;. The &lt;code&gt;*&lt;/code&gt; character functions as a wildcard, e.g. &lt;code&gt;*_mp3&lt;/code&gt;, and can be passed by itself to request all formats. Known protocols include &lt;code&gt;http&lt;/code&gt;, &lt;code&gt;hls&lt;/code&gt; and &lt;code&gt;hls-aes&lt;/code&gt;; known codecs include &lt;code&gt;aac&lt;/code&gt;, &lt;code&gt;opus&lt;/code&gt; and &lt;code&gt;mp3&lt;/code&gt;. Original &lt;code&gt;download&lt;/code&gt; formats are always extracted. Default is &lt;code&gt;http_aac,hls_aac,http_opus,hls_opus,http_mp3,hls_mp3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;orfon (orf:on)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;prefer_segments_playlist&lt;/code&gt;: Prefer a playlist of program segments instead of a single complete video when available. If individual segments are desired, use &lt;code&gt;--concat-playlist never --extractor-args "orfon:prefer_segments_playlist"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;bilibili&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;prefer_multi_flv&lt;/code&gt;: Prefer extracting flv formats over mp4 for older videos that still provide legacy formats&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;sonylivseries&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;sort_order&lt;/code&gt;: Episode sort order for series extraction - one of &lt;code&gt;asc&lt;/code&gt; (ascending, oldest first) or &lt;code&gt;desc&lt;/code&gt; (descending, newest first). Default is &lt;code&gt;asc&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;tver&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;backend&lt;/code&gt;: Backend API to use for extraction - one of &lt;code&gt;streaks&lt;/code&gt; (default) or &lt;code&gt;brightcove&lt;/code&gt; (deprecated)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;vimeo&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;client&lt;/code&gt;: Client to extract video data from. The currently available clients are &lt;code&gt;android&lt;/code&gt;, &lt;code&gt;ios&lt;/code&gt;, &lt;code&gt;macos&lt;/code&gt; and &lt;code&gt;web&lt;/code&gt;. Only one client can be used. The &lt;code&gt;macos&lt;/code&gt; client is used by default, but the &lt;code&gt;web&lt;/code&gt; client is used when logged-in. The &lt;code&gt;web&lt;/code&gt; client only works with account cookies or login credentials. The &lt;code&gt;android&lt;/code&gt; and &lt;code&gt;ios&lt;/code&gt; clients only work with previously cached OAuth tokens&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;original_format_policy&lt;/code&gt;: Policy for when to try extracting original formats. One of &lt;code&gt;always&lt;/code&gt;, &lt;code&gt;never&lt;/code&gt;, or &lt;code&gt;auto&lt;/code&gt;. The default &lt;code&gt;auto&lt;/code&gt; policy tries to avoid exceeding the web client's API rate-limit by only making an extra request when Vimeo publicizes the video's downloadability&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: These options may be changed/removed in the future without concern for backward compatibility&lt;/p&gt; 
&lt;!-- MANPAGE: MOVE "INSTALLATION" SECTION HERE --&gt; 
&lt;h1&gt;PLUGINS&lt;/h1&gt; 
&lt;p&gt;Note that &lt;strong&gt;all&lt;/strong&gt; plugins are imported even if not invoked, and that &lt;strong&gt;there are no checks&lt;/strong&gt; performed on plugin code. &lt;strong&gt;Use plugins at your own risk and only if you trust the code!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Plugins can be of &lt;code&gt;&amp;lt;type&amp;gt;&lt;/code&gt;s &lt;code&gt;extractor&lt;/code&gt; or &lt;code&gt;postprocessor&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Extractor plugins do not need to be enabled from the CLI and are automatically invoked when the input URL is suitable for it.&lt;/li&gt; 
 &lt;li&gt;Extractor plugins take priority over built-in extractors.&lt;/li&gt; 
 &lt;li&gt;Postprocessor plugins can be invoked using &lt;code&gt;--use-postprocessor NAME&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Plugins are loaded from the namespace packages &lt;code&gt;yt_dlp_plugins.extractor&lt;/code&gt; and &lt;code&gt;yt_dlp_plugins.postprocessor&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In other words, the file structure on the disk looks something like:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    yt_dlp_plugins/
        extractor/
            myplugin.py
        postprocessor/
            myplugin.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;yt-dlp looks for these &lt;code&gt;yt_dlp_plugins&lt;/code&gt; namespace folders in many locations (see below) and loads in plugins from &lt;strong&gt;all&lt;/strong&gt; of them. Set the environment variable &lt;code&gt;YTDLP_NO_PLUGINS&lt;/code&gt; to something nonempty to disable loading plugins entirely.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Plugins"&gt;wiki for some known plugins&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installing Plugins&lt;/h2&gt; 
&lt;p&gt;Plugins can be installed using various methods and locations.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configuration directories&lt;/strong&gt;: Plugin packages (containing a &lt;code&gt;yt_dlp_plugins&lt;/code&gt; namespace folder) can be dropped into the following standard &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;configuration locations&lt;/a&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;User Plugins&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp/plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt; (recommended on Linux/macOS)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp/plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt; (recommended on Windows)&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;${APPDATA}/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;~/.yt-dlp/plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;~/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;System Plugins&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;/etc/yt-dlp/plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;/etc/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Executable location&lt;/strong&gt;: Plugin packages can similarly be installed in a &lt;code&gt;yt-dlp-plugins&lt;/code&gt; directory under the executable location (recommended for portable installations):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Binary: where &lt;code&gt;&amp;lt;root-dir&amp;gt;/yt-dlp.exe&lt;/code&gt;, &lt;code&gt;&amp;lt;root-dir&amp;gt;/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Source: where &lt;code&gt;&amp;lt;root-dir&amp;gt;/yt_dlp/__main__.py&lt;/code&gt;, &lt;code&gt;&amp;lt;root-dir&amp;gt;/yt-dlp-plugins/&amp;lt;package name&amp;gt;/yt_dlp_plugins/&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pip and other locations in &lt;code&gt;PYTHONPATH&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Plugin packages can be installed and managed using &lt;code&gt;pip&lt;/code&gt;. See &lt;a href="https://github.com/yt-dlp/yt-dlp-sample-plugins"&gt;yt-dlp-sample-plugins&lt;/a&gt; for an example. 
    &lt;ul&gt; 
     &lt;li&gt;Note: plugin files between plugin packages installed with pip must have unique filenames.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Any path in &lt;code&gt;PYTHONPATH&lt;/code&gt; is searched in for the &lt;code&gt;yt_dlp_plugins&lt;/code&gt; namespace folder. 
    &lt;ul&gt; 
     &lt;li&gt;Note: This does not apply for Pyinstaller builds.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;.zip&lt;/code&gt;, &lt;code&gt;.egg&lt;/code&gt; and &lt;code&gt;.whl&lt;/code&gt; archives containing a &lt;code&gt;yt_dlp_plugins&lt;/code&gt; namespace folder in their root are also supported as plugin packages.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;e.g. &lt;code&gt;${XDG_CONFIG_HOME}/yt-dlp/plugins/mypluginpkg.zip&lt;/code&gt; where &lt;code&gt;mypluginpkg.zip&lt;/code&gt; contains &lt;code&gt;yt_dlp_plugins/&amp;lt;type&amp;gt;/myplugin.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run yt-dlp with &lt;code&gt;--verbose&lt;/code&gt; to check if the plugin has been loaded.&lt;/p&gt; 
&lt;h2&gt;Developing Plugins&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/yt-dlp/yt-dlp-sample-plugins"&gt;yt-dlp-sample-plugins&lt;/a&gt; repo for a template plugin package and the &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki/Plugin-Development"&gt;Plugin Development&lt;/a&gt; section of the wiki for a plugin development guide.&lt;/p&gt; 
&lt;p&gt;All public classes with a name ending in &lt;code&gt;IE&lt;/code&gt;/&lt;code&gt;PP&lt;/code&gt; are imported from each file for extractors and postprocessors respectively. This respects underscore prefix (e.g. &lt;code&gt;_MyBasePluginIE&lt;/code&gt; is private) and &lt;code&gt;__all__&lt;/code&gt;. Modules can similarly be excluded by prefixing the module name with an underscore (e.g. &lt;code&gt;_myplugin.py&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;To replace an existing extractor with a subclass of one, set the &lt;code&gt;plugin_name&lt;/code&gt; class keyword argument (e.g. &lt;code&gt;class MyPluginIE(ABuiltInIE, plugin_name='myplugin')&lt;/code&gt; will replace &lt;code&gt;ABuiltInIE&lt;/code&gt; with &lt;code&gt;MyPluginIE&lt;/code&gt;). Since the extractor replaces the parent, you should exclude the subclass extractor from being imported separately by making it private using one of the methods described above.&lt;/p&gt; 
&lt;p&gt;If you are a plugin author, add &lt;a href="https://github.com/topics/yt-dlp-plugins"&gt;yt-dlp-plugins&lt;/a&gt; as a topic to your repository for discoverability.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/CONTRIBUTING.md#developer-instructions"&gt;Developer Instructions&lt;/a&gt; on how to write and test an extractor.&lt;/p&gt; 
&lt;h1&gt;EMBEDDING YT-DLP&lt;/h1&gt; 
&lt;p&gt;yt-dlp makes the best effort to be a good command-line program, and thus should be callable from any programming language.&lt;/p&gt; 
&lt;p&gt;Your program should avoid parsing the normal stdout since they may change in future versions. Instead, they should use options such as &lt;code&gt;-J&lt;/code&gt;, &lt;code&gt;--print&lt;/code&gt;, &lt;code&gt;--progress-template&lt;/code&gt;, &lt;code&gt;--exec&lt;/code&gt; etc to create console output that you can reliably reproduce and parse.&lt;/p&gt; 
&lt;p&gt;From a Python program, you can embed yt-dlp in a more powerful fashion, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from yt_dlp import YoutubeDL

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']
with YoutubeDL() as ydl:
    ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Most likely, you'll want to use various options. For a list of options available, have a look at &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/yt_dlp/YoutubeDL.py#L183"&gt;&lt;code&gt;yt_dlp/YoutubeDL.py&lt;/code&gt;&lt;/a&gt; or &lt;code&gt;help(yt_dlp.YoutubeDL)&lt;/code&gt; in a Python shell. If you are already familiar with the CLI, you can use &lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/devscripts/cli_to_api.py"&gt;&lt;code&gt;devscripts/cli_to_api.py&lt;/code&gt;&lt;/a&gt; to translate any CLI switches to &lt;code&gt;YoutubeDL&lt;/code&gt; params.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: If you are porting your code from youtube-dl to yt-dlp, one important point to look out for is that we do not guarantee the return value of &lt;code&gt;YoutubeDL.extract_info&lt;/code&gt; to be json serializable, or even be a dictionary. It will be dictionary-like, but if you want to ensure it is a serializable dictionary, pass it through &lt;code&gt;YoutubeDL.sanitize_info&lt;/code&gt; as shown in the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#extracting-information"&gt;example below&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Embedding examples&lt;/h2&gt; 
&lt;h4&gt;Extracting information&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import json
import yt_dlp

URL = 'https://www.youtube.com/watch?v=BaW_jenozKc'

# ‚ÑπÔ∏è See help(yt_dlp.YoutubeDL) for a list of available options and public functions
ydl_opts = {}
with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    info = ydl.extract_info(URL, download=False)

    # ‚ÑπÔ∏è ydl.sanitize_info makes the info json-serializable
    print(json.dumps(ydl.sanitize_info(info)))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Download using an info-json&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

INFO_FILE = 'path/to/video.info.json'

with yt_dlp.YoutubeDL() as ydl:
    error_code = ydl.download_with_info_file(INFO_FILE)

print('Some videos failed to download' if error_code
      else 'All videos successfully downloaded')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extract audio&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

ydl_opts = {
    'format': 'm4a/bestaudio/best',
    # ‚ÑπÔ∏è See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments
    'postprocessors': [{  # Extract audio using ffmpeg
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'm4a',
    }]
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Filter videos&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

def longer_than_a_minute(info, *, incomplete):
    """Download only videos longer than a minute (or with unknown duration)"""
    duration = info.get('duration')
    if duration and duration &amp;lt; 60:
        return 'The video is too short'

ydl_opts = {
    'match_filter': longer_than_a_minute,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Adding logger and progress hook&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

class MyLogger:
    def debug(self, msg):
        # For compatibility with youtube-dl, both debug and info are passed into debug
        # You can distinguish them by the prefix '[debug] '
        if msg.startswith('[debug] '):
            pass
        else:
            self.info(msg)

    def info(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        print(msg)


# ‚ÑπÔ∏è See "progress_hooks" in help(yt_dlp.YoutubeDL)
def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading, now post-processing ...')


ydl_opts = {
    'logger': MyLogger(),
    'progress_hooks': [my_hook],
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Add a custom PostProcessor&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

# ‚ÑπÔ∏è See help(yt_dlp.postprocessor.PostProcessor)
class MyCustomPP(yt_dlp.postprocessor.PostProcessor):
    def run(self, info):
        self.to_screen('Doing stuff')
        return [], info


with yt_dlp.YoutubeDL() as ydl:
    # ‚ÑπÔ∏è "when" can take any value in yt_dlp.utils.POSTPROCESS_WHEN
    ydl.add_post_processor(MyCustomPP(), when='pre_process')
    ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Use a custom format selector&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

def format_selector(ctx):
    """ Select the best video and the best audio that won't result in an mkv.
    NOTE: This is just an example and does not handle all cases """

    # formats are already sorted worst to best
    formats = ctx.get('formats')[::-1]

    # acodec='none' means there is no audio
    best_video = next(f for f in formats
                      if f['vcodec'] != 'none' and f['acodec'] == 'none')

    # find compatible audio extension
    audio_ext = {'mp4': 'm4a', 'webm': 'webm'}[best_video['ext']]
    # vcodec='none' means there is no video
    best_audio = next(f for f in formats if (
        f['acodec'] != 'none' and f['vcodec'] == 'none' and f['ext'] == audio_ext))

    # These are the minimum required fields for a merged format
    yield {
        'format_id': f'{best_video["format_id"]}+{best_audio["format_id"]}',
        'ext': best_video['ext'],
        'requested_formats': [best_video, best_audio],
        # Must be + separated list of protocols
        'protocol': f'{best_video["protocol"]}+{best_audio["protocol"]}'
    }


ydl_opts = {
    'format': format_selector,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download(URLS)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;CHANGES FROM YOUTUBE-DL&lt;/h1&gt; 
&lt;h3&gt;New features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Forked from &lt;a href="https://github.com/blackjack4494/yt-dlc/commit/f9401f2a91987068139c5f757b12fc711d4c0cee"&gt;&lt;strong&gt;yt-dlc@f9401f2&lt;/strong&gt;&lt;/a&gt; and merged with &lt;a href="https://github.com/ytdl-org/youtube-dl/commit/a08f2b7e4567cdc50c0614ee0a4ffdff49b8b6e6"&gt;&lt;strong&gt;youtube-dl@a08f2b7&lt;/strong&gt;&lt;/a&gt; (&lt;a href="https://github.com/yt-dlp/yt-dlp/issues/21"&gt;exceptions&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sponsorblock-options"&gt;SponsorBlock Integration&lt;/a&gt;&lt;/strong&gt;: You can mark/remove sponsor sections in YouTube videos by utilizing the &lt;a href="https://sponsor.ajay.app"&gt;SponsorBlock&lt;/a&gt; API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats"&gt;Format Sorting&lt;/a&gt;&lt;/strong&gt;: The default format sorting options have been changed so that higher resolution and better codecs will be now preferred instead of simply using larger bitrate. Furthermore, you can now specify the sort order using &lt;code&gt;-S&lt;/code&gt;. This allows for much easier format selection than what is possible by simply using &lt;code&gt;--format&lt;/code&gt; (&lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection-examples"&gt;examples&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Merged with animelover1984/youtube-dl&lt;/strong&gt;: You get most of the features and improvements from &lt;a href="https://github.com/animelover1984/youtube-dl"&gt;animelover1984/youtube-dl&lt;/a&gt; including &lt;code&gt;--write-comments&lt;/code&gt;, &lt;code&gt;BiliBiliSearch&lt;/code&gt;, &lt;code&gt;BilibiliChannel&lt;/code&gt;, Embedding thumbnail in mp4/ogg/opus, playlist infojson etc. See &lt;a href="https://github.com/yt-dlp/yt-dlp/pull/31"&gt;#31&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;YouTube improvements&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Supports Clips, Stories (&lt;code&gt;ytstories:&amp;lt;channel UCID&amp;gt;&lt;/code&gt;), Search (including filters)&lt;strong&gt;*&lt;/strong&gt;, YouTube Music Search, Channel-specific search, Search prefixes (&lt;code&gt;ytsearch:&lt;/code&gt;, &lt;code&gt;ytsearchdate:&lt;/code&gt;)&lt;strong&gt;*&lt;/strong&gt;, Mixes, and Feeds (&lt;code&gt;:ytfav&lt;/code&gt;, &lt;code&gt;:ytwatchlater&lt;/code&gt;, &lt;code&gt;:ytsubs&lt;/code&gt;, &lt;code&gt;:ythistory&lt;/code&gt;, &lt;code&gt;:ytrec&lt;/code&gt;, &lt;code&gt;:ytnotif&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;Fix for &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/29326"&gt;n-sig based throttling&lt;/a&gt; &lt;strong&gt;*&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;Download livestreams from the start using &lt;code&gt;--live-from-start&lt;/code&gt; (&lt;em&gt;experimental&lt;/em&gt;)&lt;/li&gt; 
   &lt;li&gt;Channel URLs download all uploads of the channel, including shorts and live&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cookies from browser&lt;/strong&gt;: Cookies can be automatically extracted from all major web browsers using &lt;code&gt;--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Download time range&lt;/strong&gt;: Videos can be downloaded partially based on either timestamps or chapters using &lt;code&gt;--download-sections&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Split video by chapters&lt;/strong&gt;: Videos can be split into multiple files based on chapters using &lt;code&gt;--split-chapters&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-threaded fragment downloads&lt;/strong&gt;: Download multiple fragments of m3u8/mpd videos in parallel. Use &lt;code&gt;--concurrent-fragments&lt;/code&gt; (&lt;code&gt;-N&lt;/code&gt;) option to set the number of threads used&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Aria2c with HLS/DASH&lt;/strong&gt;: You can use &lt;code&gt;aria2c&lt;/code&gt; as the external downloader for DASH(mpd) and HLS(m3u8) formats&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;New and fixed extractors&lt;/strong&gt;: Many new extractors have been added and a lot of existing ones have been fixed. See the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/Changelog.md"&gt;changelog&lt;/a&gt; or the &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/supportedsites.md"&gt;list of supported sites&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MSOs&lt;/strong&gt;: Philo, Spectrum, SlingTV, Cablevision, RCN etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Subtitle extraction from manifests&lt;/strong&gt;: Subtitles can be extracted from streaming media manifests. See &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/be6202f12b97858b9d716e608394b51065d0419f"&gt;commit/be6202f&lt;/a&gt; for details&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple paths and output templates&lt;/strong&gt;: You can give different &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output templates&lt;/a&gt; and download paths for different types of files. You can also set a temporary path where intermediary files are downloaded to using &lt;code&gt;--paths&lt;/code&gt; (&lt;code&gt;-P&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Portable Configuration&lt;/strong&gt;: Configuration files are automatically loaded from the home and root directories. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;CONFIGURATION&lt;/a&gt; for details&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Output template improvements&lt;/strong&gt;: Output templates can now have date-time formatting, numeric offsets, object traversal etc. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; for details. Even more advanced operations can also be done with the help of &lt;code&gt;--parse-metadata&lt;/code&gt; and &lt;code&gt;--replace-in-metadata&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other new options&lt;/strong&gt;: Many new options have been added such as &lt;code&gt;--alias&lt;/code&gt;, &lt;code&gt;--print&lt;/code&gt;, &lt;code&gt;--concat-playlist&lt;/code&gt;, &lt;code&gt;--wait-for-video&lt;/code&gt;, &lt;code&gt;--retry-sleep&lt;/code&gt;, &lt;code&gt;--sleep-requests&lt;/code&gt;, &lt;code&gt;--convert-thumbnails&lt;/code&gt;, &lt;code&gt;--force-download-archive&lt;/code&gt;, &lt;code&gt;--force-overwrites&lt;/code&gt;, &lt;code&gt;--break-match-filters&lt;/code&gt; etc&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Improvements&lt;/strong&gt;: Regex and other operators in &lt;code&gt;--format&lt;/code&gt;/&lt;code&gt;--match-filters&lt;/code&gt;, multiple &lt;code&gt;--postprocessor-args&lt;/code&gt; and &lt;code&gt;--downloader-args&lt;/code&gt;, faster archive checking, more &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#format-selection"&gt;format selection options&lt;/a&gt;, merge multi-video/audio, multiple &lt;code&gt;--config-locations&lt;/code&gt;, &lt;code&gt;--exec&lt;/code&gt; at different stages, etc&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Plugins&lt;/strong&gt;: Extractors and PostProcessors can be loaded from an external file. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#plugins"&gt;plugins&lt;/a&gt; for details&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Self updater&lt;/strong&gt;: The releases can be updated using &lt;code&gt;yt-dlp -U&lt;/code&gt;, and downgraded using &lt;code&gt;--update-to&lt;/code&gt; if required&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automated builds&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#update-channels"&gt;Nightly/master builds&lt;/a&gt; can be used with &lt;code&gt;--update-to nightly&lt;/code&gt; and &lt;code&gt;--update-to master&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/Changelog.md"&gt;changelog&lt;/a&gt; or &lt;a href="https://github.com/yt-dlp/yt-dlp/commits"&gt;commits&lt;/a&gt; for the full list of changes&lt;/p&gt; 
&lt;p&gt;Features marked with a &lt;strong&gt;*&lt;/strong&gt; have been back-ported to youtube-dl&lt;/p&gt; 
&lt;h3&gt;Differences in default behavior&lt;/h3&gt; 
&lt;p&gt;Some of yt-dlp's default options are different from that of youtube-dl and youtube-dlc:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;yt-dlp supports only &lt;a href="##" title="Windows 8"&gt;Python 3.10+&lt;/a&gt;, and will remove support for more versions as they &lt;a href="https://devguide.python.org/versions/#python-release-cycle"&gt;become EOL&lt;/a&gt;; while &lt;a href="https://github.com/ytdl-org/youtube-dl/issues/30568#issue-1118238743"&gt;youtube-dl still supports Python 2.6+ and 3.2+&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The options &lt;code&gt;--auto-number&lt;/code&gt; (&lt;code&gt;-A&lt;/code&gt;), &lt;code&gt;--title&lt;/code&gt; (&lt;code&gt;-t&lt;/code&gt;) and &lt;code&gt;--literal&lt;/code&gt; (&lt;code&gt;-l&lt;/code&gt;), no longer work. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#Removed"&gt;removed options&lt;/a&gt; for details&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;avconv&lt;/code&gt; is not supported as an alternative to &lt;code&gt;ffmpeg&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;yt-dlp stores config files in slightly different locations to youtube-dl. See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#configuration"&gt;CONFIGURATION&lt;/a&gt; for a list of correct locations&lt;/li&gt; 
 &lt;li&gt;The default &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#output-template"&gt;output template&lt;/a&gt; is &lt;code&gt;%(title)s [%(id)s].%(ext)s&lt;/code&gt;. There is no real reason for this change. This was changed before yt-dlp was ever made public and now there are no plans to change it back to &lt;code&gt;%(title)s-%(id)s.%(ext)s&lt;/code&gt;. Instead, you may use &lt;code&gt;--compat-options filename&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;The default &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#sorting-formats"&gt;format sorting&lt;/a&gt; is different from youtube-dl and prefers higher resolution and better codecs rather than higher bitrates. You can use the &lt;code&gt;--format-sort&lt;/code&gt; option to change this to any order you prefer, or use &lt;code&gt;--compat-options format-sort&lt;/code&gt; to use youtube-dl's sorting order. Older versions of yt-dlp preferred VP9 due to its broader compatibility; you can use &lt;code&gt;--compat-options prefer-vp9-sort&lt;/code&gt; to revert to that format sorting preference. These two compat options cannot be used together&lt;/li&gt; 
 &lt;li&gt;The default format selector is &lt;code&gt;bv*+ba/b&lt;/code&gt;. This means that if a combined video + audio format that is better than the best video-only format is found, the former will be preferred. Use &lt;code&gt;-f bv+ba/b&lt;/code&gt; or &lt;code&gt;--compat-options format-spec&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;Unlike youtube-dlc, yt-dlp does not allow merging multiple audio/video streams into one file by default (since this conflicts with the use of &lt;code&gt;-f bv*+ba&lt;/code&gt;). If needed, this feature must be enabled using &lt;code&gt;--audio-multistreams&lt;/code&gt; and &lt;code&gt;--video-multistreams&lt;/code&gt;. You can also use &lt;code&gt;--compat-options multistreams&lt;/code&gt; to enable both&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-abort-on-error&lt;/code&gt; is enabled by default. Use &lt;code&gt;--abort-on-error&lt;/code&gt; or &lt;code&gt;--compat-options abort-on-error&lt;/code&gt; to abort on errors instead&lt;/li&gt; 
 &lt;li&gt;When writing metadata files such as thumbnails, description or infojson, the same information (if available) is also written for playlists. Use &lt;code&gt;--no-write-playlist-metafiles&lt;/code&gt; or &lt;code&gt;--compat-options no-playlist-metafiles&lt;/code&gt; to not write these files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--add-metadata&lt;/code&gt; attaches the &lt;code&gt;infojson&lt;/code&gt; to &lt;code&gt;mkv&lt;/code&gt; files in addition to writing the metadata when used with &lt;code&gt;--write-info-json&lt;/code&gt;. Use &lt;code&gt;--no-embed-info-json&lt;/code&gt; or &lt;code&gt;--compat-options no-attach-info-json&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;Some metadata are embedded into different fields when using &lt;code&gt;--add-metadata&lt;/code&gt; as compared to youtube-dl. Most notably, &lt;code&gt;comment&lt;/code&gt; field contains the &lt;code&gt;webpage_url&lt;/code&gt; and &lt;code&gt;synopsis&lt;/code&gt; contains the &lt;code&gt;description&lt;/code&gt;. You can &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/#modifying-metadata"&gt;use &lt;code&gt;--parse-metadata&lt;/code&gt;&lt;/a&gt; to modify this to your liking or use &lt;code&gt;--compat-options embed-metadata&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;playlist_index&lt;/code&gt; behaves differently when used with options like &lt;code&gt;--playlist-reverse&lt;/code&gt; and &lt;code&gt;--playlist-items&lt;/code&gt;. See &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/302"&gt;#302&lt;/a&gt; for details. You can use &lt;code&gt;--compat-options playlist-index&lt;/code&gt; if you want to keep the earlier behavior&lt;/li&gt; 
 &lt;li&gt;The output of &lt;code&gt;-F&lt;/code&gt; is listed in a new format. Use &lt;code&gt;--compat-options list-formats&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;Live chats (if available) are considered as subtitles. Use &lt;code&gt;--sub-langs all,-live_chat&lt;/code&gt; to download all subtitles except live chat. You can also use &lt;code&gt;--compat-options no-live-chat&lt;/code&gt; to prevent any live chat/danmaku from downloading&lt;/li&gt; 
 &lt;li&gt;YouTube channel URLs download all uploads of the channel. To download only the videos in a specific tab, pass the tab's URL. If the channel does not show the requested tab, an error will be raised. Also, &lt;code&gt;/live&lt;/code&gt; URLs raise an error if there are no live videos instead of silently downloading the entire channel. You may use &lt;code&gt;--compat-options no-youtube-channel-redirect&lt;/code&gt; to revert all these redirections&lt;/li&gt; 
 &lt;li&gt;Unavailable videos are also listed for YouTube playlists. Use &lt;code&gt;--compat-options no-youtube-unavailable-videos&lt;/code&gt; to remove this&lt;/li&gt; 
 &lt;li&gt;The upload dates extracted from YouTube are in UTC.&lt;/li&gt; 
 &lt;li&gt;If &lt;code&gt;ffmpeg&lt;/code&gt; is used as the downloader, the downloading and merging of formats happen in a single step when possible. Use &lt;code&gt;--compat-options no-direct-merge&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;Thumbnail embedding in &lt;code&gt;mp4&lt;/code&gt; is done with mutagen if possible. Use &lt;code&gt;--compat-options embed-thumbnail-atomicparsley&lt;/code&gt; to force the use of AtomicParsley instead&lt;/li&gt; 
 &lt;li&gt;Some internal metadata such as filenames are removed by default from the infojson. Use &lt;code&gt;--no-clean-infojson&lt;/code&gt; or &lt;code&gt;--compat-options no-clean-infojson&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;When &lt;code&gt;--embed-subs&lt;/code&gt; and &lt;code&gt;--write-subs&lt;/code&gt; are used together, the subtitles are written to disk and also embedded in the media file. You can use just &lt;code&gt;--embed-subs&lt;/code&gt; to embed the subs and automatically delete the separate file. See &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/630#issuecomment-893659460"&gt;#630 (comment)&lt;/a&gt; for more info. &lt;code&gt;--compat-options no-keep-subs&lt;/code&gt; can be used to revert this&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;certifi&lt;/code&gt; will be used for SSL root certificates, if installed. If you want to use system certificates (e.g. self-signed), use &lt;code&gt;--compat-options no-certifi&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;yt-dlp's sanitization of invalid characters in filenames is different/smarter than in youtube-dl. You can use &lt;code&gt;--compat-options filename-sanitization&lt;/code&gt; to revert to youtube-dl's behavior&lt;/li&gt; 
 &lt;li&gt;&lt;del&gt;yt-dlp tries to parse the external downloader outputs into the standard progress output if possible (Currently implemented: &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/5931"&gt;aria2c&lt;/a&gt;). You can use &lt;code&gt;--compat-options no-external-downloader-progress&lt;/code&gt; to get the downloader output as-is&lt;/del&gt;&lt;/li&gt; 
 &lt;li&gt;yt-dlp versions between 2021.09.01 and 2023.01.02 applies &lt;code&gt;--match-filters&lt;/code&gt; to nested playlists. This was an unintentional side-effect of &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/8f18aca8717bb0dd49054555af8d386e5eda3a88"&gt;8f18ac&lt;/a&gt; and is fixed in &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/d7b460d0e5fc710950582baed2e3fc616ed98a80"&gt;d7b460&lt;/a&gt;. Use &lt;code&gt;--compat-options playlist-match-filter&lt;/code&gt; to revert this&lt;/li&gt; 
 &lt;li&gt;yt-dlp versions between 2021.11.10 and 2023.06.21 estimated &lt;code&gt;filesize_approx&lt;/code&gt; values for fragmented/manifest formats. This was added for convenience in &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/f2fe69c7b0d208bdb1f6292b4ae92bc1e1a7444a"&gt;f2fe69&lt;/a&gt;, but was reverted in &lt;a href="https://github.com/yt-dlp/yt-dlp/commit/0dff8e4d1e6e9fb938f4256ea9af7d81f42fd54f"&gt;0dff8e&lt;/a&gt; due to the potentially extreme inaccuracy of the estimated values. Use &lt;code&gt;--compat-options manifest-filesize-approx&lt;/code&gt; to keep extracting the estimated values&lt;/li&gt; 
 &lt;li&gt;yt-dlp uses modern http client backends such as &lt;code&gt;requests&lt;/code&gt;. Use &lt;code&gt;--compat-options prefer-legacy-http-handler&lt;/code&gt; to prefer the legacy http handler (&lt;code&gt;urllib&lt;/code&gt;) to be used for standard http requests.&lt;/li&gt; 
 &lt;li&gt;The sub-modules &lt;code&gt;swfinterp&lt;/code&gt;, &lt;code&gt;casefold&lt;/code&gt; are removed.&lt;/li&gt; 
 &lt;li&gt;Passing &lt;code&gt;--simulate&lt;/code&gt; (or calling &lt;code&gt;extract_info&lt;/code&gt; with &lt;code&gt;download=False&lt;/code&gt;) no longer alters the default format selection. See &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/9843"&gt;#9843&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;yt-dlp no longer applies the server modified time to downloaded files by default. Use &lt;code&gt;--mtime&lt;/code&gt; or &lt;code&gt;--compat-options mtime-by-default&lt;/code&gt; to revert this.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For convenience, there are some compat option aliases available to use:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options all&lt;/code&gt;: Use all compat options (&lt;strong&gt;Do NOT use this!&lt;/strong&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options youtube-dl&lt;/code&gt;: Same as &lt;code&gt;--compat-options all,-multistreams,-playlist-match-filter,-manifest-filesize-approx,-allow-unsafe-ext,-prefer-vp9-sort&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options youtube-dlc&lt;/code&gt;: Same as &lt;code&gt;--compat-options all,-no-live-chat,-no-youtube-channel-redirect,-playlist-match-filter,-manifest-filesize-approx,-allow-unsafe-ext,-prefer-vp9-sort&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options 2021&lt;/code&gt;: Same as &lt;code&gt;--compat-options 2022,no-certifi,filename-sanitization&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options 2022&lt;/code&gt;: Same as &lt;code&gt;--compat-options 2023,playlist-match-filter,no-external-downloader-progress,prefer-legacy-http-handler,manifest-filesize-approx&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options 2023&lt;/code&gt;: Same as &lt;code&gt;--compat-options 2024,prefer-vp9-sort&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options 2024&lt;/code&gt;: Same as &lt;code&gt;--compat-options 2025,mtime-by-default&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--compat-options 2025&lt;/code&gt;: Currently does nothing. Use this to enable all future compat options&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using one of the yearly compat option aliases will pin yt-dlp's default behavior to what it was at the &lt;em&gt;end&lt;/em&gt; of that calendar year.&lt;/p&gt; 
&lt;p&gt;The following compat options restore vulnerable behavior from before security patches:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--compat-options allow-unsafe-ext&lt;/code&gt;: Allow files with any extension (including unsafe ones) to be downloaded (&lt;a href="https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-79w7-vh3h-8g4j"&gt;GHSA-79w7-vh3h-8g4j&lt;/a&gt;)&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Only use if a valid file download is rejected because its extension is detected as uncommon&lt;/p&gt; 
   &lt;p&gt;&lt;strong&gt;This option can enable remote code execution! Consider &lt;a href="https://github.com/yt-dlp/yt-dlp/issues/new/choose"&gt;opening an issue&lt;/a&gt; instead!&lt;/strong&gt;&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deprecated options&lt;/h3&gt; 
&lt;p&gt;These are all the deprecated options and the current alternative to achieve the same effect&lt;/p&gt; 
&lt;h4&gt;Almost redundant options&lt;/h4&gt; 
&lt;p&gt;While these options are almost the same as their new counterparts, there are some differences that prevents them being redundant&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-j, --dump-json                  --print "%()j"
-F, --list-formats               --print formats_table
--list-thumbnails                --print thumbnails_table --print playlist:thumbnails_table
--list-subs                      --print automatic_captions_table --print subtitles_table
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Redundant options&lt;/h4&gt; 
&lt;p&gt;While these options are redundant, they are still expected to be used due to their ease of use&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--get-description                --print description
--get-duration                   --print duration_string
--get-filename                   --print filename
--get-format                     --print format
--get-id                         --print id
--get-thumbnail                  --print thumbnail
-e, --get-title                  --print title
-g, --get-url                    --print urls
--match-title REGEX              --match-filters "title ~= (?i)REGEX"
--reject-title REGEX             --match-filters "title !~= (?i)REGEX"
--min-views COUNT                --match-filters "view_count &amp;gt;=? COUNT"
--max-views COUNT                --match-filters "view_count &amp;lt;=? COUNT"
--break-on-reject                Use --break-match-filters
--user-agent UA                  --add-headers "User-Agent:UA"
--referer URL                    --add-headers "Referer:URL"
--playlist-start NUMBER          -I NUMBER:
--playlist-end NUMBER            -I :NUMBER
--playlist-reverse               -I ::-1
--no-playlist-reverse            Default
--no-colors                      --color no_color
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Not recommended&lt;/h4&gt; 
&lt;p&gt;While these options still work, their use is not recommended since there are other alternatives to achieve the same&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--force-generic-extractor        --ies generic,default
--exec-before-download CMD       --exec "before_dl:CMD"
--no-exec-before-download        --no-exec
--all-formats                    -f all
--all-subs                       --sub-langs all --write-subs
--print-json                     -j --no-simulate
--autonumber-size NUMBER         Use string formatting, e.g. %(autonumber)03d
--autonumber-start NUMBER        Use internal field formatting like %(autonumber+NUMBER)s
--id                             -o "%(id)s.%(ext)s"
--metadata-from-title FORMAT     --parse-metadata "%(title)s:FORMAT"
--hls-prefer-native              --downloader "m3u8:native"
--hls-prefer-ffmpeg              --downloader "m3u8:ffmpeg"
--list-formats-old               --compat-options list-formats (Alias: --no-list-formats-as-table)
--list-formats-as-table          --compat-options -list-formats [Default]
--geo-bypass                     --xff "default"
--no-geo-bypass                  --xff "never"
--geo-bypass-country CODE        --xff CODE
--geo-bypass-ip-block IP_BLOCK   --xff IP_BLOCK
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Developer options&lt;/h4&gt; 
&lt;p&gt;These options are not intended to be used by the end-user&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--test                           Download only part of video for testing extractors
--load-pages                     Load pages dumped by --write-pages
--allow-unplayable-formats       List unplayable formats also
--no-allow-unplayable-formats    Default
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Old aliases&lt;/h4&gt; 
&lt;p&gt;These are aliases that are no longer documented for various reasons&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--clean-infojson                 --clean-info-json
--force-write-download-archive   --force-write-archive
--no-clean-infojson              --no-clean-info-json
--no-split-tracks                --no-split-chapters
--no-write-srt                   --no-write-subs
--prefer-unsecure                --prefer-insecure
--rate-limit RATE                --limit-rate RATE
--split-tracks                   --split-chapters
--srt-lang LANGS                 --sub-langs LANGS
--trim-file-names LENGTH         --trim-filenames LENGTH
--write-srt                      --write-subs
--yes-overwrites                 --force-overwrites
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Sponskrub Options&lt;/h4&gt; 
&lt;p&gt;Support for &lt;a href="https://github.com/faissaloo/SponSkrub"&gt;SponSkrub&lt;/a&gt; has been removed in favor of the &lt;code&gt;--sponsorblock&lt;/code&gt; options&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--sponskrub                      --sponsorblock-mark all
--no-sponskrub                   --no-sponsorblock
--sponskrub-cut                  --sponsorblock-remove all
--no-sponskrub-cut               --sponsorblock-remove -all
--sponskrub-force                Not applicable
--no-sponskrub-force             Not applicable
--sponskrub-location             Not applicable
--sponskrub-args                 Not applicable
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;No longer supported&lt;/h4&gt; 
&lt;p&gt;These options may no longer work as intended&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--prefer-avconv                  avconv is not officially supported by yt-dlp (Alias: --no-prefer-ffmpeg)
--prefer-ffmpeg                  Default (Alias: --no-prefer-avconv)
-C, --call-home                  Not implemented
--no-call-home                   Default
--include-ads                    No longer supported
--no-include-ads                 Default
--write-annotations              No supported site has annotations now
--no-write-annotations           Default
--avconv-location                Removed alias for --ffmpeg-location
--cn-verification-proxy URL      Removed alias for --geo-verification-proxy URL
--dump-headers                   Removed alias for --print-traffic
--dump-intermediate-pages        Removed alias for --dump-pages
--youtube-skip-dash-manifest     Removed alias for --extractor-args "youtube:skip=dash" (Alias: --no-youtube-include-dash-manifest)
--youtube-skip-hls-manifest      Removed alias for --extractor-args "youtube:skip=hls" (Alias: --no-youtube-include-hls-manifest)
--youtube-include-dash-manifest  Default (Alias: --no-youtube-skip-dash-manifest)
--youtube-include-hls-manifest   Default (Alias: --no-youtube-skip-hls-manifest)
--youtube-print-sig-code         Removed testing functionality
--dump-user-agent                No longer supported
--xattr-set-filesize             No longer supported
--compat-options seperate-video-versions  No longer needed
--compat-options no-youtube-prefer-utc-upload-date  No longer supported
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Removed&lt;/h4&gt; 
&lt;p&gt;These options were deprecated since 2014 and have now been entirely removed&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-A, --auto-number                -o "%(autonumber)s-%(id)s.%(ext)s"
-t, -l, --title, --literal       -o "%(title)s-%(id)s.%(ext)s"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;CONTRIBUTING&lt;/h1&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#contributing-to-yt-dlp"&gt;CONTRIBUTING.md&lt;/a&gt; for instructions on &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#opening-an-issue"&gt;Opening an Issue&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/CONTRIBUTING.md#developer-instructions"&gt;Contributing code to the project&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;WIKI&lt;/h1&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/yt-dlp/yt-dlp/wiki"&gt;Wiki&lt;/a&gt; for more information&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>xai-org/grok-1</title>
      <link>https://github.com/xai-org/grok-1</link>
      <description>&lt;p&gt;Grok open release&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
  </channel>
</rss>