<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Sun, 14 Dec 2025 01:48:25 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>anthropics/claude-quickstarts</title>
      <link>https://github.com/anthropics/claude-quickstarts</link>
      <description>&lt;p&gt;A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Quickstarts&lt;/h1&gt; 
&lt;p&gt;Claude Quickstarts is a collection of projects designed to help developers quickly get started with building applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To use these quickstarts, you'll need an Claude API key. If you don't have one yet, you can sign up for free at &lt;a href="https://console.anthropic.com"&gt;console.anthropic.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Available Quickstarts&lt;/h2&gt; 
&lt;h3&gt;Customer Support Agent&lt;/h3&gt; 
&lt;p&gt;A customer support agent powered by Claude. This project demonstrates how to leverage Claude's natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/customer-support-agent"&gt;Go to Customer Support Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Financial Data Analyst&lt;/h3&gt; 
&lt;p&gt;A financial data analyst powered by Claude. This project demonstrates how to leverage Claude's capabilities with interactive data visualization to analyze financial data via chat.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/financial-data-analyst"&gt;Go to Financial Data Analyst Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Computer Use Demo&lt;/h3&gt; 
&lt;p&gt;An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest &lt;code&gt;computer_use_20251124&lt;/code&gt; tool version with zoom actions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/computer-use-demo"&gt;Go to Computer Use Demo Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Autonomous Coding Agent&lt;/h3&gt; 
&lt;p&gt;An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/autonomous-coding"&gt;Go to Autonomous Coding Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General Usage&lt;/h2&gt; 
&lt;p&gt;Each quickstart project comes with its own README and setup instructions. Generally, you'll follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository&lt;/li&gt; 
 &lt;li&gt;Navigate to the specific quickstart directory&lt;/li&gt; 
 &lt;li&gt;Install the required dependencies&lt;/li&gt; 
 &lt;li&gt;Set up your Claude API key as an environment variable&lt;/li&gt; 
 &lt;li&gt;Run the quickstart application&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;To deepen your understanding of working with Claude and the Claude API, check out these resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.claude.com"&gt;Claude API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/claude-cookbooks"&gt;Claude Cookbooks&lt;/a&gt; - A collection of code snippets and guides for common tasks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals"&gt;Claude API Fundamentals Course&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.&lt;/p&gt; 
&lt;h2&gt;Community and Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join our &lt;a href="https://www.anthropic.com/discord"&gt;Anthropic Discord community&lt;/a&gt; for discussions and support&lt;/li&gt; 
 &lt;li&gt;Check out the &lt;a href="https://support.anthropic.com"&gt;Anthropic support documentation&lt;/a&gt; for additional help&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>srbhr/Resume-Matcher</title>
      <link>https://github.com/srbhr/Resume-Matcher</link>
      <description>&lt;p&gt;Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.resumematcher.fyi"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/page_2.png" alt="Resume Matcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h1&gt;Resume Matcher&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://dsc.gg/resume-matcher"&gt;ğ™¹ğš˜ğš’ğš— ğ™³ğš’ğšœğšŒğš˜ğš›ğš&lt;/a&gt; âœ¦ &lt;a href="https://resumematcher.fyi"&gt;ğš†ğšğš‹ğšœğš’ğšğš&lt;/a&gt; âœ¦ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#how-to-install"&gt;ğ™·ğš˜ğš  ğšğš˜ ğ™¸ğš—ğšœğšğšŠğš•ğš•&lt;/a&gt; âœ¦ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#contributors"&gt;ğ™²ğš˜ğš—ğšğš›ğš’ğš‹ğšğšğš˜ğš›ğšœ&lt;/a&gt; âœ¦ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#support-the-development-by-donating"&gt;ğ™³ğš˜ğš—ğšŠğšğš&lt;/a&gt; âœ¦ &lt;a href="https://twitter.com/ssrbhr"&gt;ğšƒğš ğš’ğšğšğšğš›/ğš‡&lt;/a&gt; âœ¦ &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;ğ™»ğš’ğš—ğš”ğšğšğ™¸ğš—&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Stop getting auto-rejected by ATS bots.&lt;/strong&gt; Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands.&lt;/p&gt; 
 &lt;p&gt;Hoping to make this, &lt;strong&gt;VS Code for making resumes&lt;/strong&gt;.&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/github/stars/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Stars" /&gt; &lt;img src="https://img.shields.io/github/license/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Apache 2.0" /&gt; &lt;img src="https://img.shields.io/github/forks/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Forks" /&gt; &lt;img src="https://img.shields.io/badge/Version-0.1%20Veridis%20Quo-FFF?labelColor=black&amp;amp;logo=LinkedIn&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="version" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://dsc.gg/resume-matcher"&gt;&lt;img src="https://img.shields.io/discord/1122069176962531400?labelColor=black&amp;amp;logo=discord&amp;amp;logoColor=c20a71&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://resumematcher.fyi"&gt;&lt;img src="https://img.shields.io/badge/website-Resume%20Matcher-FFF?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-Resume%20Matcher-FFF?labelColor=black&amp;amp;logo=LinkedIn&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/565" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/565" alt="srbhr%2FResume-Matcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://vercel.com/oss/program-badge.svg?sanitize=true" alt="Vercel OSS Program" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;/p&gt; 
 &lt;p&gt;This project is in active development. New features are being added continuously, and we welcome contributions from the community. There are some breaking changes on the &lt;code&gt;main&lt;/code&gt; branch. If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting started with Resume Matcher&lt;/h2&gt; 
&lt;p&gt;Resume Matcher is designed to help you optimize your resume with the aim to highlight your skills and experience in a way that resonates with potential employers.&lt;/p&gt; 
&lt;p&gt;We're actively working on improving the platform, building towards a &lt;strong&gt;VS Code for making resumes&lt;/strong&gt;, and adding new features. The best way to stay updated is to join the Discord discussion and be part of the active development community.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Join our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; community ğŸ‘‡ &lt;a href="https://dsc.gg/resume-matcher"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_discord.png" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Follow us on &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;LinkedIn&lt;/a&gt; âœ¨ &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_linkedin.png" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â­ Star Resume Matcher to support the development and get updates on GitHub. &lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/star_resume_matcher.png" alt="Star Resume Matcher" /&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_features.png" alt="resume_matcher_features" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Works locally&lt;/strong&gt;: No need to upload your resume to a server. Everything runs on your machine with open source AI models by Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ATS Compatibility&lt;/strong&gt;: Get a detailed analysis of your resume's compatibility with ATS systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant Match Score&lt;/strong&gt;: Upload resume &amp;amp; job description for a quick match score and key improvement areas.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Keyword Optimizer&lt;/strong&gt;: Align your resume with job keywords and identify critical content gaps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Guided Improvements&lt;/strong&gt;: Get clear suggestions to make your resume stand out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Roadmap&lt;/h3&gt; 
&lt;p&gt;If you have any suggestions or feature requests, please feel free to open an issue on GitHub. And discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visual keyword highlighting.&lt;/li&gt; 
 &lt;li&gt;AI Canvas, which can help to craft impactful, metric-driven resume content.&lt;/li&gt; 
 &lt;li&gt;Multi-job description optimization.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/how_to_install_resumematcher.png" alt="Installation" /&gt;&lt;/p&gt; 
&lt;p&gt;Follow the instructions in the &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/SETUP.md"&gt;SETUP.md&lt;/a&gt; file to set up the project locally. The setup script will install all the necessary dependencies and configure your environment.&lt;/p&gt; 
&lt;p&gt;The project is built using:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;FastAPI for the backend.&lt;/li&gt; 
 &lt;li&gt;Next.js for the frontend.&lt;/li&gt; 
 &lt;li&gt;Ollama for local AI model serving.&lt;/li&gt; 
 &lt;li&gt;Tailwind CSS for styling.&lt;/li&gt; 
 &lt;li&gt;SQLite for the database.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Technology&lt;/th&gt; 
   &lt;th&gt;Info/Version&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.12+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Next.js&lt;/td&gt; 
   &lt;td&gt;15+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;0.6.7&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Join Us and Contribute&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/how_to_contribute.png" alt="how to contribute" /&gt;&lt;/p&gt; 
&lt;p&gt;We welcome contributions from everyone! Whether you're a developer, designer, or just someone who wants to help out. All the contributors are listed in the &lt;a href="https://resumematcher.fyi/about"&gt;about page&lt;/a&gt; on our website and on the GitHub Readme here.&lt;/p&gt; 
&lt;p&gt;Check out the roadmap if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub and discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/contributors.png" alt="Contributors" /&gt;&lt;/p&gt; 
&lt;a href="https://github.com/srbhr/Resume-Matcher/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=srbhr/Resume-Matcher" /&gt; &lt;/a&gt; 
&lt;h2&gt;Support the Development by Donating&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/supporting_resume_matcher.png" alt="donate" /&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to support the development of Resume Matcher, you can do so by donating. Your contributions will help us keep the project alive and continue adding new features.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GitHub&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sponsors/srbhr"&gt;&lt;img src="https://img.shields.io/github/sponsors/srbhr?style=for-the-badge&amp;amp;color=c20a71&amp;amp;labelColor=black&amp;amp;logo=github" alt="GitHub Sponsors" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Buy Me a Coffee&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.buymeacoffee.com/srbhr"&gt;&lt;img src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;amp;logo=buy-me-a-coffee&amp;amp;color=c20a72&amp;amp;logoColor=white" alt="BuyMeACoffee" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;kbd&gt;Star History&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;amp;theme=dark&amp;amp;type=Date" /&gt; 
  &lt;img width="100%" src="https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;amp;theme=dark&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; 
&lt;/details&gt; 
&lt;h2&gt;Resume Matcher is a part of &lt;a href="https://vercel.com/oss"&gt;Vercel Open Source Program&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://vercel.com/oss/program-badge.svg?sanitize=true" alt="Vercel OSS Program" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GoogleCloudPlatform/agent-starter-pack</title>
      <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
      <description>&lt;p&gt;Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸš€ Agent Starter Pack&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/agent-starter-pack?color=blue" alt="Version" /&gt; &lt;a href="https://youtu.be/jHt-ZVD660g"&gt;&lt;img src="https://img.shields.io/badge/1--Minute%20Overview-gray" alt="1-Minute Video Overview" /&gt;&lt;/a&gt; &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;&lt;img src="https://img.shields.io/badge/Documentation-gray" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fagent_starter_pack%2Fresources%2Fidx"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://cdn.firebasestudio.dev/btn/try_light_20.svg" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://cdn.firebasestudio.dev/btn/try_dark_20.svg" /&gt; 
   &lt;img height="20" alt="Try in Firebase Studio" src="https://cdn.firebasestudio.dev/btn/try_blue_20.svg?sanitize=true" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;&lt;img src="https://img.shields.io/badge/Launch-in_Cloud_Shell-white" alt="Launch in Cloud Shell" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow" alt="Stars" /&gt;&lt;/p&gt; 
&lt;p&gt;A Python package that provides &lt;strong&gt;production-ready templates&lt;/strong&gt; for GenAI agents on Google Cloud.&lt;/p&gt; 
&lt;p&gt;Focus on your agent logicâ€”the starter pack provides everything else: infrastructure, CI/CD, observability, and security.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;âš¡ï¸ Launch&lt;/th&gt; 
   &lt;th&gt;ğŸ§ª Experiment&lt;/th&gt; 
   &lt;th&gt;âœ… Deploy&lt;/th&gt; 
   &lt;th&gt;ğŸ› ï¸ Customize&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/agent_starter_pack/agents/"&gt;Pre-built agent templates&lt;/a&gt; (ReAct, RAG, multi-agent, Live API).&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview"&gt;Vertex AI evaluation&lt;/a&gt; and an interactive playground.&lt;/td&gt; 
   &lt;td&gt;Production-ready infra with &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/observability"&gt;monitoring, observability&lt;/a&gt;, and &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;CI/CD&lt;/a&gt; on &lt;a href="https://cloud.google.com/run"&gt;Cloud Run&lt;/a&gt; or &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview"&gt;Agent Engine&lt;/a&gt;.&lt;/td&gt; 
   &lt;td&gt;Extend and customize templates according to your needs. ğŸ†• Now integrating with &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;âš¡ Get Started in 1 Minute&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;From zero to production-ready agent in 60 seconds using &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack create
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; âœ¨ Alternative: Using pip&lt;/summary&gt; 
 &lt;p&gt;If you don't have &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed, you can use pip:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a Python virtual environment
python -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; You now have a fully functional agent projectâ€”complete with backend, frontend, and deployment infrastructureâ€”ready for you to explore and customize.&lt;/p&gt; 
&lt;h3&gt;ğŸ”§ Enhance Existing Agents&lt;/h3&gt; 
&lt;p&gt;Already have an agent? Add production-ready deployment and infrastructure by running this command in your project's root folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack enhance
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; for more options, or try with zero setup in &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx"&gt;Firebase Studio&lt;/a&gt; or &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;Cloud Shell&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤– Agents&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using Google's &lt;a href="https://github.com/google/adk-python"&gt;Agent Development Kit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_a2a_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;An ADK agent with &lt;a href="https://a2a-protocol.org/"&gt;Agent2Agent (A2A) Protocol&lt;/a&gt; support for distributed agent communication and interoperability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;agentic_rag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A RAG agent for document retrieval and Q&amp;amp;A. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;langgraph_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using LangChain's &lt;a href="https://github.com/langchain-ai/langgraph"&gt;LangGraph&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_live&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;More agents are on the way!&lt;/strong&gt; We are continuously expanding our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;agent library&lt;/a&gt;. Have a specific agent type in mind? &lt;a href="https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement"&gt;Raise an issue as a feature request!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ” ADK Samples&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Looking to explore more ADK examples? Check out the &lt;a href="https://github.com/google/adk-samples"&gt;ADK Samples Repository&lt;/a&gt; for additional examples and use cases demonstrating ADK's capabilities.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸŒŸ Community Showcase&lt;/h2&gt; 
&lt;p&gt;Explore amazing projects built with the Agent Starter Pack!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase"&gt;View Community Showcase â†’&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;agent-starter-pack&lt;/code&gt; offers key features to accelerate and simplify the development of your agent:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”„ &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd"&gt;CI/CD Automation&lt;/a&gt;&lt;/strong&gt; - A single command to set up a complete CI/CD pipeline for all environments, supporting both &lt;strong&gt;Google Cloud Build&lt;/strong&gt; and &lt;strong&gt;GitHub Actions&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“¥ &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion"&gt;Data Pipeline for RAG with Terraform/CI-CD&lt;/a&gt;&lt;/strong&gt; - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/guide/remote-templating.md"&gt;Remote Templates&lt;/a&gt;&lt;/strong&gt;: Create and share your own agent starter packs templates from any Git repository.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– Gemini CLI Integration&lt;/strong&gt; - Use the &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt; and the included &lt;code&gt;GEMINI.md&lt;/code&gt; context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;High-Level Architecture&lt;/h2&gt; 
&lt;p&gt;This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/images/ags_high_level_architecture.png" alt="High Level Architecture" title="Architecture" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”§ Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/sdk/docs/install"&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.hashicorp.com/terraform/downloads"&gt;Terraform&lt;/a&gt; (for deployment)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/make/"&gt;Make&lt;/a&gt; (for development tasks)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;documentation site&lt;/a&gt; for comprehensive guides and references!&lt;/p&gt; 
&lt;p&gt;ğŸ” &lt;strong&gt;New to the codebase?&lt;/strong&gt; Explore the &lt;a href="https://codewiki.google/github.com/googlecloudplatform/agent-starter-pack"&gt;CodeWiki&lt;/a&gt; for AI-powered code understanding and navigation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started"&gt;Getting Started Guide&lt;/a&gt; - First steps with agent-starter-pack&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; - Setting up your environment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;Deployment Guide&lt;/a&gt; - Taking your agent to production&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;Agent Templates Overview&lt;/a&gt; - Explore available agent patterns&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/"&gt;CLI Reference&lt;/a&gt; - Command-line tool documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Video Walkthrough:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=9zqwym-N3lg"&gt;Exploring the Agent Starter Pack&lt;/a&gt;&lt;/strong&gt;: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;amp;t=2791"&gt;6-minute introduction&lt;/a&gt;&lt;/strong&gt; (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Looking for more examples and resources for Generative AI on Google Cloud? Check out the &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai"&gt;GoogleCloudPlatform/generative-ai&lt;/a&gt; repository for notebooks, code samples, and more!&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See the &lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feedback&lt;/h2&gt; 
&lt;p&gt;We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.&lt;/p&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;p&gt;If you encounter any issues or have specific suggestions, please first consider &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai/issues"&gt;raising an issue&lt;/a&gt; on our GitHub repository.&lt;/p&gt; 
&lt;h3&gt;Share Your Experience&lt;/h3&gt; 
&lt;p&gt;For other types of feedback, or if you'd like to share a positive experience or success story using this starter pack, we'd love to hear from you! You can reach out to us at &lt;a href="mailto:agent-starter-pack@google.com"&gt;&lt;/a&gt;&lt;a href="mailto:agent-starter-pack@google.com"&gt;agent-starter-pack@google.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for your contributions!&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This repository is for demonstrative purposes only and is not an officially supported Google product.&lt;/p&gt; 
&lt;h2&gt;Terms of Service&lt;/h2&gt; 
&lt;p&gt;The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you'll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the &lt;a href="https://cloud.google.com/terms/service-terms"&gt;Google Cloud Service Terms&lt;/a&gt; for details on the terms of service associated with these APIs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/adk-samples</title>
      <link>https://github.com/google/adk-samples</link>
      <description>&lt;p&gt;A collection of sample agents built with Agent Development Kit (ADK)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Development Kit (ADK) Samples&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://github.com/google/adk-docs/raw/main/docs/assets/agent-development-kit.png" alt="Agent Development Kit Logo" width="150" /&gt; 
&lt;p&gt;Welcome to the ADK Sample Agents repository! This collection provides ready-to-use agents built on top of the &lt;a href="https://google.github.io/adk-docs/"&gt;Agent Development Kit&lt;/a&gt;, designed to accelerate your development process. These agents cover a range of common use cases and complexities, from simple conversational bots to complex multi-agent workflows.&lt;/p&gt; 
&lt;h2&gt;âœ¨ Getting Started&lt;/h2&gt; 
&lt;p&gt;This repo contains ADK sample agents for &lt;strong&gt;Python&lt;/strong&gt;, &lt;strong&gt;Go&lt;/strong&gt; and &lt;strong&gt;Java.&lt;/strong&gt; Navigate to the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/python/"&gt;Python&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/go/"&gt;Go&lt;/a&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/java/"&gt;Java&lt;/a&gt;&lt;/strong&gt; subfolders to see language-specific setup instructions, and learn more about the available sample agents.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The agents in this repository are built using the &lt;strong&gt;Agent Development Kit (ADK)&lt;/strong&gt;. Before you can run any of the samples, you must have the ADK installed. For instructions, please refer to the &lt;a href="https://google.github.io/adk-docs/get-started"&gt;&lt;strong&gt;ADK Installation Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To learn more, check out the &lt;a href="https://google.github.io/adk-docs/"&gt;ADK Documentation&lt;/a&gt;, and the GitHub repositories for each language:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-python"&gt;ADK Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-go"&gt;ADK Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-java"&gt;ADK Java&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸŒ³ Repository Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;â”œâ”€â”€ go
â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ agents
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ llm-auditor
â”‚&amp;nbsp;&amp;nbsp; â””â”€â”€ README.md
â”œâ”€â”€ java
â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ agents
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ software-bug-assistant
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â””â”€â”€ time-series-forecasting
â”‚&amp;nbsp;&amp;nbsp; â””â”€â”€ README.md
â”œâ”€â”€ python
â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ agents
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ academic-research
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ antom-payment
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ blog-writer
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ brand-search-optimization
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ camel
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ customer-service
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ data-engineering
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ data-science
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ financial-advisor
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ fomc-research
â”‚   â”‚   â”œâ”€â”€ gemini-fullstack
â”‚   â”‚   â”œâ”€â”€ deep-search
â”‚   â”‚   â”œâ”€â”€ google-trends-agent
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ image-scoring
â”‚   â”‚   â”œâ”€â”€ llm-auditor
â”‚   â”‚   â”œâ”€â”€ machine-learning-engineering
â”‚   â”‚   â”œâ”€â”€ marketing-agency
â”‚   â”‚   â”œâ”€â”€ medical-pre-authorization
â”‚   â”‚   â”œâ”€â”€ personalized-shopping
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ plumber-data-engineering-assistant
â”‚   â”‚   â”œâ”€â”€ RAG
â”‚   â”‚   â”œâ”€â”€ realtime-conversational-agent
â”‚   â”‚   â”œâ”€â”€ safety-plugins
â”‚   â”‚   â”œâ”€â”€ short-movie-agents
â”‚   â”‚   â”œâ”€â”€ software-bug-assistant
â”‚   â”‚   â”œâ”€â”€ travel-concierge
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â„¹ï¸ Getting help&lt;/h2&gt; 
&lt;p&gt;If you have any questions or if you found any problems with this repository, please report through &lt;a href="https://github.com/google/adk-samples/issues"&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our &lt;a href="https://github.com/google/adk-samples/raw/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guidelines&lt;/strong&gt;&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://github.com/google/adk-samples/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Disclaimers&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. This project is not eligible for the &lt;a href="https://bughunters.google.com/open-source-security"&gt;Google Open Source Software Vulnerability Rewards Program&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is intended for demonstration purposes only. It is not intended for use in a production environment.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelscope/DiffSynth-Studio</title>
      <link>https://github.com/modelscope/DiffSynth-Studio</link>
      <description>&lt;p&gt;Enjoy the magic of Diffusion models!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DiffSynth-Studio&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/.github/workflows/logo.gif" title="Logo" style="max-width:100%;" width="55" /&gt;&lt;/a&gt; &lt;a href="https://trendshift.io/repositories/10946" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10946" alt="modelscope%2FDiffSynth-Studio | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/DiffSynth/"&gt;&lt;img src="https://img.shields.io/pypi/v/DiffSynth" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelscope/DiffSynth-Studio/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg?sanitize=true" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelscope/DiffSynth-Studio/issues"&gt;&lt;img src="https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg?sanitize=true" alt="open issues" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/modelscope/DiffSynth-Studio/pull/"&gt;&lt;img src="https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg?sanitize=true" alt="GitHub pull-requests" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/modelscope/DiffSynth-Studio/commit/"&gt;&lt;img src="https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio" alt="GitHub latest commit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/README_zh.md"&gt;åˆ‡æ¢åˆ°ä¸­æ–‡ç‰ˆ&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to the magical world of Diffusion models! DiffSynth-Studio is an open-source Diffusion model engine developed and maintained by the &lt;a href="https://www.modelscope.cn/"&gt;ModelScope Community&lt;/a&gt;. We hope to foster technological innovation through framework construction, aggregate the power of the open-source community, and explore the boundaries of generative model technology!&lt;/p&gt; 
&lt;p&gt;DiffSynth currently includes two open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt;: Focused on aggressive technical exploration, targeting academia, and providing cutting-edge model capability support.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt;: Focused on stable model deployment, targeting industry, and providing higher computational performance and more stable features.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt; and &lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt; are the core engines of the ModelScope AIGC zone. Welcome to experience our carefully crafted productized features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ModelScope AIGC Zone (for Chinese users): &lt;a href="https://modelscope.cn/aigc/home"&gt;https://modelscope.cn/aigc/home&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ModelScope Civision (for global users): &lt;a href="https://modelscope.ai/civision/home"&gt;https://modelscope.ai/civision/home&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;DiffSynth-Studio Documentation: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/zh/README.md"&gt;ä¸­æ–‡ç‰ˆ&lt;/a&gt;ã€&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/README.md"&gt;English version&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We believe that a well-developed open-source code framework can lower the threshold for technical exploration. We have achieved many &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#innovative-achievements"&gt;interesting technologies&lt;/a&gt; based on this codebase. Perhaps you also have many wild ideas, and with DiffSynth-Studio, you can quickly realize these ideas. For this reason, we have prepared detailed documentation for developers. We hope that through these documents, developers can understand the principles of Diffusion models, and we look forward to expanding the boundaries of technology together with you.&lt;/p&gt; 
&lt;h2&gt;Update History&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;DiffSynth-Studio has undergone major version updates, and some old features are no longer maintained. If you need to use old features, please switch to the &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3"&gt;last historical version&lt;/a&gt; before the major version update.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Currently, the development personnel of this project are limited, with most of the work handled by &lt;a href="https://github.com/Artiprocher"&gt;Artiprocher&lt;/a&gt;. Therefore, the progress of new feature development will be relatively slow, and the speed of responding to and resolving issues is limited. We apologize for this and ask developers to understand.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 9, 2025&lt;/strong&gt; We release a wild model based on DiffSynth-Studio 2.0: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-i2L"&gt;Qwen-Image-i2L&lt;/a&gt; (Image-to-LoRA). This model takes an image as input and outputs a LoRA. Although this version still has significant room for improvement in terms of generalization, detail preservation, and other aspects, we are open-sourcing these models to inspire more innovative research.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 4, 2025&lt;/strong&gt; DiffSynth-Studio 2.0 released! Many new features online&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/README.md"&gt;Documentation&lt;/a&gt; online: Our documentation is still continuously being optimized and updated&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Pipeline_Usage/VRAM_management.md"&gt;VRAM Management&lt;/a&gt; module upgraded, supporting layer-level disk offload, releasing both memory and VRAM simultaneously&lt;/li&gt; 
   &lt;li&gt;New model support 
    &lt;ul&gt; 
     &lt;li&gt;Z-Image Turbo: &lt;a href="https://www.modelscope.ai/models/Tongyi-MAI/Z-Image-Turbo"&gt;Model&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/Z-Image.md"&gt;Documentation&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/"&gt;Code&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;FLUX.2-dev: &lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev"&gt;Model&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/FLUX2.md"&gt;Documentation&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/"&gt;Code&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Training framework upgrade 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/zh/Training/Split_Training.md"&gt;Split Training&lt;/a&gt;: Supports automatically splitting the training process into two stages: data processing and training (even for training ControlNet or any other model). Computations that do not require gradient backpropagation, such as text encoding and VAE encoding, are performed during the data processing stage, while other computations are handled during the training stage. Faster speed, less VRAM requirement.&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/zh/Training/Differential_LoRA.md"&gt;Differential LoRA Training&lt;/a&gt;: This is a training technique we used in &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;ArtAug&lt;/a&gt;, now available for LoRA training of any model.&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/zh/Training/FP8_Precision.md"&gt;FP8 Training&lt;/a&gt;: FP8 can be applied to any non-training model during training, i.e., models with gradients turned off or gradients that only affect LoRA weights.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;November 4, 2025&lt;/strong&gt; Supported the &lt;a href="https://modelscope.cn/models/ByteDance/Video-As-Prompt-Wan2.1-14B"&gt;ByteDance/Video-As-Prompt-Wan2.1-14B&lt;/a&gt; model, which is trained based on Wan 2.1 and supports generating corresponding actions based on reference videos.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 30, 2025&lt;/strong&gt; Supported the &lt;a href="https://www.modelscope.cn/models/meituan-longcat/LongCat-Video"&gt;meituan-longcat/LongCat-Video&lt;/a&gt; model, which supports text-to-video, image-to-video, and video continuation. This model uses the Wan framework for inference and training in this project.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 27, 2025&lt;/strong&gt; Supported the &lt;a href="https://www.modelscope.cn/models/krea/krea-realtime-video"&gt;krea/krea-realtime-video&lt;/a&gt; model, adding another member to the Wan model ecosystem.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;September 23, 2025&lt;/strong&gt; &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-Poster"&gt;DiffSynth-Studio/Qwen-Image-EliGen-Poster&lt;/a&gt; released! This model was jointly developed and open-sourced by us and Taobao Experience Design Team. Built upon Qwen-Image, the model is specifically designed for e-commerce poster scenarios, supporting precise partition layout control. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen-Poster.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;September 9, 2025&lt;/strong&gt; Our training framework supports various training modes. Currently adapted for Qwen-Image, in addition to the standard SFT training mode, Direct Distill is now supported. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Distill-LoRA.sh"&gt;our sample code&lt;/a&gt;. This feature is experimental, and we will continue to improve it to support more comprehensive model training functions.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 28, 2025&lt;/strong&gt; We support Wan2.2-S2V, an audio-driven cinematic video generation model. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2025&lt;/strong&gt; &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-V2"&gt;DiffSynth-Studio/Qwen-Image-EliGen-V2&lt;/a&gt; released! Compared to the V1 version, the training dataset has been changed to &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset"&gt;Qwen-Image-Self-Generated-Dataset&lt;/a&gt;, so the generated images better conform to Qwen-Image's own image distribution and style. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-V2.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2025&lt;/strong&gt; We open-sourced the &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union"&gt;DiffSynth-Studio/Qwen-Image-In-Context-Control-Union&lt;/a&gt; structural control LoRA model, adopting the In Context technical route, supporting multiple categories of structural control conditions, including canny, depth, lineart, softedge, normal, and openpose. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-In-Context-Control-Union.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 20, 2025&lt;/strong&gt; We open-sourced the &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix"&gt;DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix&lt;/a&gt; model, improving the editing effect of Qwen-Image-Edit on low-resolution image inputs. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit-Lowres-Fix.py"&gt;our sample code&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 19, 2025&lt;/strong&gt; ğŸ”¥ Qwen-Image-Edit open-sourced, welcome a new member to the image editing model family!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 18, 2025&lt;/strong&gt; We trained and open-sourced the Qwen-Image inpainting ControlNet model &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint&lt;/a&gt;. The model structure adopts a lightweight design. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 15, 2025&lt;/strong&gt; We open-sourced the &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset"&gt;Qwen-Image-Self-Generated-Dataset&lt;/a&gt; dataset. This is an image dataset generated using the Qwen-Image model, containing 160,000 &lt;code&gt;1024 x 1024&lt;/code&gt; images. It includes general, English text rendering, and Chinese text rendering subsets. We provide annotations for image descriptions, entities, and structural control images for each image. Developers can use this dataset to train Qwen-Image models' ControlNet and EliGen models. We aim to promote technological development through open-sourcing!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 13, 2025&lt;/strong&gt; We trained and open-sourced the Qwen-Image ControlNet model &lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth&lt;/a&gt;. The model structure adopts a lightweight design. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 12, 2025&lt;/strong&gt; We trained and open-sourced the Qwen-Image ControlNet model &lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny&lt;/a&gt;. The model structure adopts a lightweight design. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 11, 2025&lt;/strong&gt; We open-sourced the distilled acceleration model &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-LoRA"&gt;DiffSynth-Studio/Qwen-Image-Distill-LoRA&lt;/a&gt; for Qwen-Image, following the same training process as &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full"&gt;DiffSynth-Studio/Qwen-Image-Distill-Full&lt;/a&gt;, but the model structure has been modified to LoRA, thus being better compatible with other open-source ecosystem models.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 7, 2025&lt;/strong&gt; We open-sourced the entity control LoRA model &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen"&gt;DiffSynth-Studio/Qwen-Image-EliGen&lt;/a&gt; for Qwen-Image. Qwen-Image-EliGen can achieve entity-level controlled text-to-image generation. Technical details can be found in &lt;a href="https://arxiv.org/abs/2501.01097"&gt;the paper&lt;/a&gt;. Training dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet"&gt;EliGenTrainSet&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 5, 2025&lt;/strong&gt; We open-sourced the distilled acceleration model &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full"&gt;DiffSynth-Studio/Qwen-Image-Distill-Full&lt;/a&gt; for Qwen-Image, achieving approximately 5x acceleration.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 4, 2025&lt;/strong&gt; ğŸ”¥ Qwen-Image open-sourced, welcome a new member to the image generation model family!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 1, 2025&lt;/strong&gt; &lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev"&gt;FLUX.1-Krea-dev&lt;/a&gt; open-sourced, a text-to-image model focused on aesthetic photography. We provided comprehensive support in a timely manner, including low VRAM layer-by-layer offload, LoRA training, and full training. For more details, please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/"&gt;./examples/flux/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;July 28, 2025&lt;/strong&gt; Wan 2.2 open-sourced. We provided comprehensive support in a timely manner, including low VRAM layer-by-layer offload, FP8 quantization, sequence parallelism, LoRA training, and full training. For more details, please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;July 11, 2025&lt;/strong&gt; We propose Nexus-Gen, a unified framework that combines the language reasoning capabilities of Large Language Models (LLMs) with the image generation capabilities of diffusion models. This framework supports seamless image understanding, generation, and editing tasks.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Paper: &lt;a href="https://arxiv.org/pdf/2504.21356"&gt;Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;GitHub Repository: &lt;a href="https://github.com/modelscope/Nexus-Gen"&gt;https://github.com/modelscope/Nexus-Gen&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/Nexus-GenV2"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Training Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset"&gt;ModelScope Dataset&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen"&gt;ModelScope Nexus-Gen Studio&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 15, 2025&lt;/strong&gt; ModelScope's official evaluation framework &lt;a href="https://github.com/modelscope/evalscope"&gt;EvalScope&lt;/a&gt; now supports text-to-image generation evaluation. Please refer to the &lt;a href="https://evalscope.readthedocs.io/zh-cn/latest/best_practice/t2i_eval.html"&gt;best practices&lt;/a&gt; guide to try it out.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 25, 2025&lt;/strong&gt; Our new open-source project &lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt; is now open-sourced! Focused on stable model deployment, targeting industry, providing better engineering support, higher computational performance, and more stable features.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 31, 2025&lt;/strong&gt; We support InfiniteYou, a face feature preservation method for FLUX. More details can be found in &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/InfiniteYou/"&gt;./examples/InfiniteYou/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 13, 2025&lt;/strong&gt; We support HunyuanVideo-I2V, the image-to-video generation version of Tencent's open-source HunyuanVideo. More details can be found in &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/"&gt;./examples/HunyuanVideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 25, 2025&lt;/strong&gt; We support Wan-Video, a series of state-of-the-art video synthesis models open-sourced by Alibaba. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 17, 2025&lt;/strong&gt; We support &lt;a href="https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary"&gt;StepVideo&lt;/a&gt;! Advanced video synthesis model! See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/stepvideo/"&gt;./examples/stepvideo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 31, 2024&lt;/strong&gt; We propose EliGen, a new framework for entity-level controlled text-to-image generation, supplemented with an inpainting fusion pipeline, extending its capabilities to image inpainting tasks. EliGen can seamlessly integrate existing community models such as IP-Adapter and In-Context LoRA, enhancing their versatility. For more details, see &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/EntityControl/"&gt;./examples/EntityControl&lt;/a&gt;.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.01097"&gt;EliGen: Entity-Level Controlled Image Generation with Regional Attention&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Eligen"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/EliGen"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen"&gt;ModelScope EliGen Studio&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Training Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet"&gt;EliGen Train Set&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 19, 2024&lt;/strong&gt; We implemented advanced VRAM management for HunyuanVideo, enabling video generation with resolutions of 129x720x1280 on 24GB VRAM or 129x512x384 on just 6GB VRAM. More details can be found in &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/"&gt;./examples/HunyuanVideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 18, 2024&lt;/strong&gt; We propose ArtAug, a method to improve text-to-image models through synthesis-understanding interaction. We trained an ArtAug enhancement module for FLUX.1-dev in LoRA format. This model incorporates the aesthetic understanding of Qwen2-VL-72B into FLUX.1-dev, thereby improving the quality of generated images.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2412.12888"&gt;https://arxiv.org/abs/2412.12888&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Example: &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug"&gt;https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Demo: &lt;a href="https://modelscope.cn/aigc/imageGeneration?tab=advanced&amp;amp;versionId=7228&amp;amp;modelType=LoRA&amp;amp;sdVersion=FLUX_1&amp;amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0"&gt;ModelScope&lt;/a&gt;, HuggingFace (coming soon)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 25, 2024&lt;/strong&gt; We provide extensive FLUX ControlNet support. This project supports many different ControlNet models and can be freely combined, even if their structures are different. Additionally, ControlNet models are compatible with high-resolution optimization and partition control technologies, enabling very powerful controllable image generation. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ControlNet/"&gt;&lt;code&gt;./examples/ControlNet/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 8, 2024&lt;/strong&gt; We released extended LoRAs based on CogVideoX-5B and ExVideo. You can download this model from &lt;a href="https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1"&gt;ModelScope&lt;/a&gt; or &lt;a href="https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1"&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024&lt;/strong&gt; This project now supports CogVideoX-5B. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/video_synthesis/"&gt;here&lt;/a&gt;. We provide several interesting features for this text-to-video model, including:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Text-to-video&lt;/li&gt; 
    &lt;li&gt;Video editing&lt;/li&gt; 
    &lt;li&gt;Self super-resolution&lt;/li&gt; 
    &lt;li&gt;Video interpolation&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024&lt;/strong&gt; We implemented an interesting brush feature that supports all text-to-image models. Now you can create stunning images with the assistance of AI using the brush!&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Use it in our &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#usage-in-webui"&gt;WebUI&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2024&lt;/strong&gt; DiffSynth-Studio now supports FLUX.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Enable CFG and high-resolution inpainting to improve visual quality. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/README.md"&gt;here&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;LoRA, ControlNet, and other addon models will be released soon.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 21, 2024&lt;/strong&gt; We propose ExVideo, a post-training fine-tuning technique aimed at enhancing the capabilities of video generation models. We extended Stable Video Diffusion to achieve long video generation of up to 128 frames.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://ecnu-cilab.github.io/ExVideoProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Source code has been released in this repository. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/"&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Model has been released at &lt;a href="https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Technical report has been released at &lt;a href="https://arxiv.org/abs/2406.14130"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;You can try ExVideo in this &lt;a href="https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1"&gt;demo&lt;/a&gt;!&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 13, 2024&lt;/strong&gt; DiffSynth Studio has migrated to ModelScope. The development team has also transitioned from "me" to "us". Of course, I will still participate in subsequent development and maintenance work.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;January 29, 2024&lt;/strong&gt; We propose Diffutoon, an excellent cartoon coloring solution.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Source code has been released in this project.&lt;/li&gt; 
    &lt;li&gt;Technical report (IJCAI 2024) has been released at &lt;a href="https://arxiv.org/abs/2401.16224"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 8, 2023&lt;/strong&gt; We decided to initiate a new project aimed at unleashing the potential of diffusion models, especially in video synthesis. The development work of this project officially began.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;November 15, 2023&lt;/strong&gt; We propose FastBlend, a powerful video deflickering algorithm.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;sd-webui extension has been released at &lt;a href="https://github.com/Artiprocher/sd-webui-fastblend"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Demonstration videos have been showcased on Bilibili, including three tasks: 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1d94y1W7PE"&gt;Video Deflickering&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1Lw411m71p"&gt;Video Interpolation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1RB4y1Z7LF"&gt;Image-Driven Video Rendering&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;Technical report has been released at &lt;a href="https://arxiv.org/abs/2311.09265"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Unofficial ComfyUI extensions developed by other users have been released at &lt;a href="https://github.com/AInseven/ComfyUI-fastblend"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 1, 2023&lt;/strong&gt; We released an early version of the project named FastSDXL. This was an initial attempt to build a diffusion engine.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Source code has been released at &lt;a href="https://github.com/Artiprocher/FastSDXL"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;FastSDXL includes a trainable OLSS scheduler to improve efficiency. 
     &lt;ul&gt; 
      &lt;li&gt;The original repository of OLSS is located &lt;a href="https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler"&gt;here&lt;/a&gt;.&lt;/li&gt; 
      &lt;li&gt;Technical report (CIKM 2023) has been released at &lt;a href="https://arxiv.org/abs/2305.14677"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
      &lt;li&gt;Demonstration video has been released at &lt;a href="https://www.bilibili.com/video/BV1w8411y7uj"&gt;Bilibili&lt;/a&gt;.&lt;/li&gt; 
      &lt;li&gt;Since OLSS requires additional training, we did not implement it in this project.&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 29, 2023&lt;/strong&gt; We propose DiffSynth, a video synthesis framework.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://ecnu-cilab.github.io/DiffSynth.github.io/"&gt;Project Page&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Source code has been released at &lt;a href="https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth"&gt;EasyNLP&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Technical report (ECML PKDD 2024) has been released at &lt;a href="https://arxiv.org/abs/2308.03463"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Install from source (recommended):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git  
cd DiffSynth-Studio
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation methods&lt;/summary&gt; 
 &lt;p&gt;Install from PyPI (version updates may be delayed; for latest features, install from source)&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;pip install diffsynth
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you meet problems during installation, they might be caused by upstream dependencies. Please check the docs of these packages:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pytorch.org/get-started/locally/"&gt;torch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;sentencepiece&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://cmake.org"&gt;cmake&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.cupy.dev/en/stable/install.html"&gt;cupy&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Basic Framework&lt;/h2&gt; 
&lt;p&gt;DiffSynth-Studio redesigns the inference and training pipelines for mainstream Diffusion models (including FLUX, Wan, etc.), enabling efficient memory management and flexible model training.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Environment Variable Configuration&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Before running model inference or training, you can configure settings such as the model download source via &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Pipeline_Usage/Environment_Variables.md"&gt;environment variables&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;By default, this project downloads models from ModelScope. For users outside China, you can configure the system to download models from the ModelScope international site as follows:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["MODELSCOPE_DOMAIN"] = "www.modelscope.ai"
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;To download models from other sources, please modify the environment variable &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Pipeline_Usage/Environment_Variables.md#diffsynth_download_source"&gt;DIFFSYNTH_DOWNLOAD_SOURCE&lt;/a&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h3&gt;Image Synthesis&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/c01258e2-f251-441a-aa1e-ebb22f02594d" alt="Image" /&gt;&lt;/p&gt; 
&lt;h4&gt;Z-Image: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/Z-Image.md"&gt;/docs/en/Model_Details/Z-Image.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo"&gt;Tongyi-MAI/Z-Image-Turbo&lt;/a&gt; model for inference. FP8 quantization significantly degrades image quality, so we do not recommend enabling any quantization for the Z-Image Turbo model. CPU offloading is recommended, and the model can run with as little as 8 GB of GPU memory.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffsynth.pipelines.z_image import ZImagePipeline, ModelConfig
import torch

vram_config = {
    "offload_dtype": torch.bfloat16,
    "offload_device": "cpu",
    "onload_dtype": torch.bfloat16,
    "onload_device": "cpu",
    "preparing_dtype": torch.bfloat16,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = ZImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="transformer/*.safetensors", **vram_config),
        ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="text_encoder/*.safetensors", **vram_config),
        ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="vae/diffusion_pytorch_model.safetensors", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="tokenizer/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)
prompt = "Young Chinese woman in red Hanfu, intricate embroidery. Impeccable makeup, red floral forehead pattern. Elaborate high bun, golden phoenix headdress, red flowers, beads. Holds round folding fan with lady, trees, bird. Neon lightning-bolt lamp (âš¡ï¸), bright yellow glow, above extended left palm. Soft-lit outdoor night background, silhouetted tiered pagoda (è¥¿å®‰å¤§é›å¡”), blurred colorful distant lights."
image = pipe(prompt=prompt, seed=42, rand_device="cuda")
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for Z-Image is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/"&gt;/examples/z_image/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low-VRAM Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Full Training Validation&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo"&gt;Tongyi-MAI/Z-Image-Turbo&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_inference/Z-Image-Turbo.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_inference_low_vram/Z-Image-Turbo.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_training/full/Z-Image-Turbo.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_training/validate_full/Z-Image-Turbo.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_training/lora/Z-Image-Turbo.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_training/validate_lora/Z-Image-Turbo.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;FLUX.2: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/FLUX2.md"&gt;/docs/en/Model_Details/FLUX2.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev"&gt;black-forest-labs/FLUX.2-dev&lt;/a&gt; model for inference. VRAM management is enabled, and the framework automatically loads model parameters based on available GPU memory. The model can run with as little as 10 GB of VRAM.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffsynth.pipelines.flux2_image import Flux2ImagePipeline, ModelConfig
import torch

vram_config = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": torch.float8_e4m3fn,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e4m3fn,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = Flux2ImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="text_encoder/*.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="transformer/*.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="vae/diffusion_pytorch_model.safetensors"),
    ],
    tokenizer_config=ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="tokenizer/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)
prompt = "High resolution. A dreamy underwater portrait of a serene young woman in a flowing blue dress. Her hair floats softly around her face, strands delicately suspended in the water. Clear, shimmering light filters through, casting gentle highlights, while tiny bubbles rise around her. Her expression is calm, her features finely detailedâ€”creating a tranquil, ethereal scene."
image = pipe(prompt, seed=42, rand_device="cuda", num_inference_steps=50)
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for FLUX.2 is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/"&gt;/examples/flux2/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low-VRAM Inference&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev"&gt;black-forest-labs/FLUX.2-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/model_inference/FLUX.2-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/model_inference_low_vram/FLUX.2-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/model_training/lora/FLUX.2-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/model_training/validate_lora/FLUX.2-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;Qwen-Image: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/Qwen-Image.md"&gt;/docs/en/Model_Details/Qwen-Image.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image"&gt;Qwen/Qwen-Image&lt;/a&gt; model for inference. VRAM management is enabled, and the framework automatically adjusts model parameter loading based on available GPU memory. The model can run with as little as 8 GB of VRAM.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig
import torch

vram_config = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": torch.float8_e4m3fn,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e4m3fn,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = QwenImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="transformer/diffusion_pytorch_model*.safetensors", **vram_config),
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="text_encoder/model*.safetensors", **vram_config),
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="vae/diffusion_pytorch_model.safetensors", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="tokenizer/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)
prompt = "ç²¾è‡´è‚–åƒï¼Œæ°´ä¸‹å°‘å¥³ï¼Œè“è£™é£˜é€¸ï¼Œå‘ä¸è½»æ‰¬ï¼Œå…‰å½±é€æ¾ˆï¼Œæ°”æ³¡ç¯ç»•ï¼Œé¢å®¹æ¬é™ï¼Œç»†èŠ‚ç²¾è‡´ï¼Œæ¢¦å¹»å”¯ç¾ã€‚"
image = pipe(prompt, seed=0, num_inference_steps=40)
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Model Lineage&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;graph LR;
    Qwen/Qwen-Image--&amp;gt;Qwen/Qwen-Image-Edit;
    Qwen/Qwen-Image-Edit--&amp;gt;Qwen/Qwen-Image-Edit-2509;
    Qwen/Qwen-Image--&amp;gt;EliGen-Series;
    EliGen-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-EliGen;
    DiffSynth-Studio/Qwen-Image-EliGen--&amp;gt;DiffSynth-Studio/Qwen-Image-EliGen-V2;
    EliGen-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-EliGen-Poster;
    Qwen/Qwen-Image--&amp;gt;Distill-Series;
    Distill-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Distill-Full;
    Distill-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Distill-LoRA;
    Qwen/Qwen-Image--&amp;gt;ControlNet-Series;
    ControlNet-Series--&amp;gt;Blockwise-ControlNet-Series;
    Blockwise-ControlNet-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny;
    Blockwise-ControlNet-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth;
    Blockwise-ControlNet-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint;
    ControlNet-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-In-Context-Control-Union;
    Qwen/Qwen-Image--&amp;gt;DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for Qwen-Image is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/"&gt;/examples/qwen_image/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low-VRAM Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Full Training Validation&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image"&gt;Qwen/Qwen-Image&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit"&gt;Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit-2509"&gt;Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit-2509.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit-2509.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Edit-2509.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Edit-2509.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Edit-2509.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Edit-2509.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen"&gt;DiffSynth-Studio/Qwen-Image-EliGen&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-EliGen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-V2"&gt;DiffSynth-Studio/Qwen-Image-EliGen-V2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen-V2.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-V2.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-EliGen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-Poster"&gt;DiffSynth-Studio/Qwen-Image-EliGen-Poster&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen-Poster.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-Poster.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-EliGen-Poster.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen-Poster.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full"&gt;DiffSynth-Studio/Qwen-Image-Distill-Full&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Distill-Full.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Distill-Full.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-LoRA"&gt;DiffSynth-Studio/Qwen-Image-Distill-LoRA&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Distill-LoRA.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Distill-LoRA.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Distill-LoRA.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Distill-LoRA.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Canny.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Canny.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Depth.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Depth.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Inpaint.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Inpaint.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union"&gt;DiffSynth-Studio/Qwen-Image-In-Context-Control-Union&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-In-Context-Control-Union.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-In-Context-Control-Union.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-In-Context-Control-Union.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-In-Context-Control-Union.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix"&gt;DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit-Lowres-Fix.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit-Lowres-Fix.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-i2L"&gt;DiffSynth-Studio/Qwen-Image-i2L&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-i2L.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-i2L.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;FLUX.1: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/FLUX.md"&gt;/docs/en/Model_Details/FLUX.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-dev"&gt;black-forest-labs/FLUX.1-dev&lt;/a&gt; model for inference. VRAM management is enabled, and the framework automatically adjusts model parameter loading based on available GPU memory. The model can run with as little as 8 GB of VRAM.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from diffsynth.pipelines.flux_image import FluxImagePipeline, ModelConfig

vram_config = {
    "offload_dtype": torch.float8_e4m3fn,
    "offload_device": "cpu",
    "onload_dtype": torch.float8_e4m3fn,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e4m3fn,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = FluxImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="flux1-dev.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder/model.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder_2/*.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="ae.safetensors", **vram_config),
    ],
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 1,
)
prompt = "CG, masterpiece, best quality, solo, long hair, wavy hair, silver hair, blue eyes, blue dress, medium breasts, dress, underwater, air bubble, floating hair, refraction, portrait. The girl's flowing silver hair shimmers with every color of the rainbow and cascades down, merging with the floating flora around her."
image = pipe(prompt=prompt, seed=0)
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Model Lineage&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;graph LR;
    FLUX.1-Series--&amp;gt;black-forest-labs/FLUX.1-dev;
    FLUX.1-Series--&amp;gt;black-forest-labs/FLUX.1-Krea-dev;
    FLUX.1-Series--&amp;gt;black-forest-labs/FLUX.1-Kontext-dev;
    black-forest-labs/FLUX.1-dev--&amp;gt;FLUX.1-dev-ControlNet-Series;
    FLUX.1-dev-ControlNet-Series--&amp;gt;alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta;
    FLUX.1-dev-ControlNet-Series--&amp;gt;InstantX/FLUX.1-dev-Controlnet-Union-alpha;
    FLUX.1-dev-ControlNet-Series--&amp;gt;jasperai/Flux.1-dev-Controlnet-Upscaler;
    black-forest-labs/FLUX.1-dev--&amp;gt;InstantX/FLUX.1-dev-IP-Adapter;
    black-forest-labs/FLUX.1-dev--&amp;gt;ByteDance/InfiniteYou;
    black-forest-labs/FLUX.1-dev--&amp;gt;DiffSynth-Studio/Eligen;
    black-forest-labs/FLUX.1-dev--&amp;gt;DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev;
    black-forest-labs/FLUX.1-dev--&amp;gt;DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev;
    black-forest-labs/FLUX.1-dev--&amp;gt;ostris/Flex.2-preview;
    black-forest-labs/FLUX.1-dev--&amp;gt;stepfun-ai/Step1X-Edit;
    Qwen/Qwen2.5-VL-7B-Instruct--&amp;gt;stepfun-ai/Step1X-Edit;
    black-forest-labs/FLUX.1-dev--&amp;gt;DiffSynth-Studio/Nexus-GenV2;
    Qwen/Qwen2.5-VL-7B-Instruct--&amp;gt;DiffSynth-Studio/Nexus-GenV2;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for FLUX.1 is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/"&gt;/examples/flux/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Extra Args&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low-VRAM Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Full Training Validation&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-dev"&gt;black-forest-labs/FLUX.1-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev"&gt;black-forest-labs/FLUX.1-Krea-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-Krea-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-Krea-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Kontext-dev"&gt;black-forest-labs/FLUX.1-Kontext-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;kontext_images&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-Kontext-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-Kontext-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta"&gt;alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Inpainting-Beta.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Inpainting-Beta.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/InstantX/FLUX.1-dev-Controlnet-Union-alpha"&gt;InstantX/FLUX.1-dev-Controlnet-Union-alpha&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Union-alpha.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Union-alpha.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/jasperai/Flux.1-dev-Controlnet-Upscaler"&gt;jasperai/Flux.1-dev-Controlnet-Upscaler&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Upscaler.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Upscaler.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/InstantX/FLUX.1-dev-IP-Adapter"&gt;InstantX/FLUX.1-dev-IP-Adapter&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;ipadapter_images&lt;/code&gt;, &lt;code&gt;ipadapter_scale&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-IP-Adapter.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-IP-Adapter.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/ByteDance/InfiniteYou"&gt;ByteDance/InfiniteYou&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;infinityou_id_image&lt;/code&gt;, &lt;code&gt;infinityou_guidance&lt;/code&gt;, &lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-InfiniteYou.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-InfiniteYou.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Eligen"&gt;DiffSynth-Studio/Eligen&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;eligen_entity_prompts&lt;/code&gt;, &lt;code&gt;eligen_entity_masks&lt;/code&gt;, &lt;code&gt;eligen_enable_on_negative&lt;/code&gt;, &lt;code&gt;eligen_enable_inpaint&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-EliGen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev"&gt;DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_encoder_inputs&lt;/code&gt;, &lt;code&gt;lora_encoder_scale&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-LoRA-Encoder.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-LoRA-Encoder.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-LoRA-Encoder.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-LoRA-Encoder.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev"&gt;DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/stepfun-ai/Step1X-Edit"&gt;stepfun-ai/Step1X-Edit&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;step1x_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/Step1X-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/Step1X-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/ostris/Flex.2-preview"&gt;ostris/Flex.2-preview&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;flex_inpaint_image&lt;/code&gt;, &lt;code&gt;flex_inpaint_mask&lt;/code&gt;, &lt;code&gt;flex_control_image&lt;/code&gt;, &lt;code&gt;flex_control_strength&lt;/code&gt;, &lt;code&gt;flex_control_stop&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLEX.2-preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLEX.2-preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2"&gt;DiffSynth-Studio/Nexus-GenV2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;nexus_gen_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/Nexus-Gen-Editing.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/Nexus-Gen-Editing.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/Nexus-Gen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/Nexus-Gen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/Nexus-Gen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/Nexus-Gen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Video Synthesis&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/1d66ae74-3b02-40a9-acc3-ea95fc039314"&gt;https://github.com/user-attachments/assets/1d66ae74-3b02-40a9-acc3-ea95fc039314&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Wan: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/Wan.md"&gt;/docs/en/Model_Details/Wan.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B"&gt;Wan-AI/Wan2.1-T2V-1.3B&lt;/a&gt; model for inference. VRAM management is enabled, and the framework automatically adjusts model parameter loading based on available GPU memory. The model can run with as little as 8 GB of VRAM.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from diffsynth.utils.data import save_video, VideoData
from diffsynth.pipelines.wan_video import WanVideoPipeline, ModelConfig

vram_config = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": torch.bfloat16,
    "onload_device": "cpu",
    "preparing_dtype": torch.bfloat16,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="diffusion_pytorch_model*.safetensors", **vram_config),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", **vram_config),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="Wan2.1_VAE.pth", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="google/umt5-xxl/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 2,
)

video = pipe(
    prompt="çºªå®æ‘„å½±é£æ ¼ç”»é¢ï¼Œä¸€åªæ´»æ³¼çš„å°ç‹—åœ¨ç»¿èŒµèŒµçš„è‰åœ°ä¸Šè¿…é€Ÿå¥”è·‘ã€‚å°ç‹—æ¯›è‰²æ£•é»„ï¼Œä¸¤åªè€³æœµç«‹èµ·ï¼Œç¥æƒ…ä¸“æ³¨è€Œæ¬¢å¿«ã€‚é˜³å…‰æ´’åœ¨å®ƒèº«ä¸Šï¼Œä½¿å¾—æ¯›å‘çœ‹ä¸Šå»æ ¼å¤–æŸ”è½¯è€Œé—ªäº®ã€‚èƒŒæ™¯æ˜¯ä¸€ç‰‡å¼€é˜”çš„è‰åœ°ï¼Œå¶å°”ç‚¹ç¼€ç€å‡ æœµé‡èŠ±ï¼Œè¿œå¤„éšçº¦å¯è§è“å¤©å’Œå‡ ç‰‡ç™½äº‘ã€‚é€è§†æ„Ÿé²œæ˜ï¼Œæ•æ‰å°ç‹—å¥”è·‘æ—¶çš„åŠ¨æ„Ÿå’Œå››å‘¨è‰åœ°çš„ç”Ÿæœºã€‚ä¸­æ™¯ä¾§é¢ç§»åŠ¨è§†è§’ã€‚",
    negative_prompt="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°",
    seed=0, tiled=True,
)
save_video(video, "video.mp4", fps=15, quality=5)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Model Lineage&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;graph LR;
    Wan-Series--&amp;gt;Wan2.1-Series;
    Wan-Series--&amp;gt;Wan2.2-Series;
    Wan2.1-Series--&amp;gt;Wan-AI/Wan2.1-T2V-1.3B;
    Wan2.1-Series--&amp;gt;Wan-AI/Wan2.1-T2V-14B;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.1-I2V-14B-480P;
    Wan-AI/Wan2.1-I2V-14B-480P--&amp;gt;Wan-AI/Wan2.1-I2V-14B-720P;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.1-FLF2V-14B-720P;
    Wan-AI/Wan2.1-T2V-1.3B--&amp;gt;iic/VACE-Wan2.1-1.3B-Preview;
    iic/VACE-Wan2.1-1.3B-Preview--&amp;gt;Wan-AI/Wan2.1-VACE-1.3B;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.1-VACE-14B;
    Wan-AI/Wan2.1-T2V-1.3B--&amp;gt;Wan2.1-Fun-1.3B-Series;
    Wan2.1-Fun-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-1.3B-InP;
    Wan2.1-Fun-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-1.3B-Control;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan2.1-Fun-14B-Series;
    Wan2.1-Fun-14B-Series--&amp;gt;PAI/Wan2.1-Fun-14B-InP;
    Wan2.1-Fun-14B-Series--&amp;gt;PAI/Wan2.1-Fun-14B-Control;
    Wan-AI/Wan2.1-T2V-1.3B--&amp;gt;Wan2.1-Fun-V1.1-1.3B-Series;
    Wan2.1-Fun-V1.1-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control;
    Wan2.1-Fun-V1.1-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-1.3B-InP;
    Wan2.1-Fun-V1.1-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan2.1-Fun-V1.1-14B-Series;
    Wan2.1-Fun-V1.1-14B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-14B-Control;
    Wan2.1-Fun-V1.1-14B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-14B-InP;
    Wan2.1-Fun-V1.1-14B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-14B-Control-Camera;
    Wan-AI/Wan2.1-T2V-1.3B--&amp;gt;DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;krea/krea-realtime-video;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;meituan-longcat/LongCat-Video;
    Wan-AI/Wan2.1-I2V-14B-720P--&amp;gt;ByteDance/Video-As-Prompt-Wan2.1-14B;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.2-Animate-14B;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.2-S2V-14B;
    Wan2.2-Series--&amp;gt;Wan-AI/Wan2.2-T2V-A14B;
    Wan2.2-Series--&amp;gt;Wan-AI/Wan2.2-I2V-A14B;
    Wan2.2-Series--&amp;gt;Wan-AI/Wan2.2-TI2V-5B;
    Wan-AI/Wan2.2-T2V-A14B--&amp;gt;Wan2.2-Fun-Series;
    Wan2.2-Fun-Series--&amp;gt;PAI/Wan2.2-VACE-Fun-A14B;
    Wan2.2-Fun-Series--&amp;gt;PAI/Wan2.2-Fun-A14B-InP;
    Wan2.2-Fun-Series--&amp;gt;PAI/Wan2.2-Fun-A14B-Control;
    Wan2.2-Fun-Series--&amp;gt;PAI/Wan2.2-Fun-A14B-Control-Camera;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for Wan is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;/examples/wanvideo/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Extra Args&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Full Training Validation&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B"&gt;Wan-AI/Wan2.1-T2V-1.3B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-T2V-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-T2V-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-T2V-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-T2V-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B"&gt;Wan-AI/Wan2.1-T2V-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-T2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-T2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-T2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-T2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P"&gt;Wan-AI/Wan2.1-I2V-14B-480P&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-I2V-14B-480P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-I2V-14B-480P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-480P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-480P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-480P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P"&gt;Wan-AI/Wan2.1-I2V-14B-720P&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-I2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-I2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P"&gt;Wan-AI/Wan2.1-FLF2V-14B-720P&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-FLF2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-FLF2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-FLF2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-FLF2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-FLF2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/iic/VACE-Wan2.1-1.3B-Preview"&gt;iic/VACE-Wan2.1-1.3B-Preview&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-VACE-1.3B-Preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B-Preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B-Preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B-Preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B-Preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-1.3B"&gt;Wan-AI/Wan2.1-VACE-1.3B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-VACE-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-14B"&gt;Wan-AI/Wan2.1-VACE-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-VACE-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-VACE-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-VACE-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-InP"&gt;PAI/Wan2.1-Fun-1.3B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-Control"&gt;PAI/Wan2.1-Fun-1.3B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-InP"&gt;PAI/Wan2.1-Fun-14B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-Control"&gt;PAI/Wan2.1-Fun-14B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control"&gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;, &lt;code&gt;reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control"&gt;PAI/Wan2.1-Fun-V1.1-14B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;, &lt;code&gt;reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP"&gt;PAI/Wan2.1-Fun-V1.1-1.3B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-InP"&gt;PAI/Wan2.1-Fun-V1.1-14B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera"&gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_camera_video&lt;/code&gt;, &lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control-Camera"&gt;PAI/Wan2.1-Fun-V1.1-14B-Control-Camera&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_camera_video&lt;/code&gt;, &lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1"&gt;DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;motion_bucket_id&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-1.3b-speedcontrol-v1.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-1.3b-speedcontrol-v1.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-1.3b-speedcontrol-v1.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-1.3b-speedcontrol-v1.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-1.3b-speedcontrol-v1.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/krea/krea-realtime-video"&gt;krea/krea-realtime-video&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/krea-realtime-video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/krea-realtime-video.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/krea-realtime-video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/krea-realtime-video.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/krea-realtime-video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/meituan-longcat/LongCat-Video"&gt;meituan-longcat/LongCat-Video&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;longcat_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/LongCat-Video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/LongCat-Video.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/LongCat-Video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/LongCat-Video.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/LongCat-Video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/ByteDance/Video-As-Prompt-Wan2.1-14B"&gt;ByteDance/Video-As-Prompt-Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vap_video&lt;/code&gt;, &lt;code&gt;vap_prompt&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Video-As-Prompt-Wan2.1-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Video-As-Prompt-Wan2.1-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Video-As-Prompt-Wan2.1-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Video-As-Prompt-Wan2.1-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Video-As-Prompt-Wan2.1-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.2-T2V-A14B"&gt;Wan-AI/Wan2.2-T2V-A14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-T2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-T2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-T2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-T2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-T2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B"&gt;Wan-AI/Wan2.2-I2V-A14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-I2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-I2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-I2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-I2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-I2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.2-TI2V-5B"&gt;Wan-AI/Wan2.2-TI2V-5B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-TI2V-5B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-TI2V-5B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-TI2V-5B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-TI2V-5B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-TI2V-5B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Wan-AI/Wan2.2-Animate-14B"&gt;Wan-AI/Wan2.2-Animate-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;animate_pose_video&lt;/code&gt;, &lt;code&gt;animate_face_video&lt;/code&gt;, &lt;code&gt;animate_inpaint_video&lt;/code&gt;, &lt;code&gt;animate_mask_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-Animate-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-Animate-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-Animate-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-Animate-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-Animate-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Wan-AI/Wan2.2-S2V-14B"&gt;Wan-AI/Wan2.2-S2V-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;input_audio&lt;/code&gt;, &lt;code&gt;audio_sample_rate&lt;/code&gt;, &lt;code&gt;s2v_pose_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-S2V-14B_multi_clips.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-S2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-S2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-S2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-S2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/PAI/Wan2.2-VACE-Fun-A14B"&gt;PAI/Wan2.2-VACE-Fun-A14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-VACE-Fun-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-VACE-Fun-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-VACE-Fun-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-VACE-Fun-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-VACE-Fun-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.2-Fun-A14B-InP"&gt;PAI/Wan2.2-Fun-A14B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-Fun-A14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-Fun-A14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-Fun-A14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-Fun-A14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-Fun-A14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.2-Fun-A14B-Control"&gt;PAI/Wan2.2-Fun-A14B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;, &lt;code&gt;reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-Fun-A14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-Fun-A14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-Fun-A14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-Fun-A14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-Fun-A14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.2-Fun-A14B-Control-Camera"&gt;PAI/Wan2.2-Fun-A14B-Control-Camera&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_camera_video&lt;/code&gt;, &lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-Fun-A14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-Fun-A14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-Fun-A14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-Fun-A14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-Fun-A14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;Innovative Achievements&lt;/h2&gt; 
&lt;p&gt;DiffSynth-Studio is not just an engineered model framework, but also an incubator for innovative achievements.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;AttriCtrl: Attribute Intensity Control for Image Generation Models&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.02151"&gt;AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-AttriCtrl.py"&gt;/examples/flux/model_inference/FLUX.1-dev-AttriCtrl.py&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev"&gt;ModelScope&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;brightness scale = 0.1&lt;/th&gt; 
    &lt;th&gt;brightness scale = 0.3&lt;/th&gt; 
    &lt;th&gt;brightness scale = 0.5&lt;/th&gt; 
    &lt;th&gt;brightness scale = 0.7&lt;/th&gt; 
    &lt;th&gt;brightness scale = 0.9&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.1.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.3.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.5.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.7.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.9.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;AutoLoRA: Automated LoRA Retrieval and Fusion&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.02107"&gt;AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py"&gt;/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev"&gt;ModelScope&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://modelscope.cn/models/cancel13/cxsk"&gt;LoRA 1&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://modelscope.cn/models/wy413928499/xuancai2"&gt;LoRA 2&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;LoRA 3&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://modelscope.cn/models/hongyanbujian/JPL"&gt;LoRA 4&lt;/a&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/cancel13/cxsk"&gt;LoRA 1&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_0.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_1.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_3.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/wy413928499/xuancai2"&gt;LoRA 2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_1.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_1.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_3.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;LoRA 3&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_2_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_2_3.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/hongyanbujian/JPL"&gt;LoRA 4&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_3.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_3.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_2_3.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_3_3.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Nexus-Gen: Unified Architecture for Image Understanding, Generation, and Editing&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detailed Page: &lt;a href="https://github.com/modelscope/Nexus-Gen"&gt;https://github.com/modelscope/Nexus-Gen&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/pdf/2504.21356"&gt;Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/Nexus-GenV2"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset"&gt;ModelScope Dataset&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen"&gt;ModelScope Nexus-Gen Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;img src="https://github.com/modelscope/Nexus-Gen/raw/main/assets/illustrations/gen_edit.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ArtAug: Aesthetic Enhancement for Image Generation Models&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detailed Page: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ArtAug/"&gt;./examples/ArtAug/&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2412.12888"&gt;ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&amp;amp;versionId=7228&amp;amp;modelType=LoRA&amp;amp;sdVersion=FLUX_1&amp;amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0"&gt;ModelScope AIGC Tab&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;FLUX.1-dev&lt;/th&gt; 
    &lt;th&gt;FLUX.1-dev + ArtAug LoRA&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/e1d5c505-b423-45fe-be01-25c2758f5417" alt="image_1_base" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/335908e3-d0bd-41c2-9d99-d10528a2d719" alt="image_1_enhance" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;EliGen: Precise Image Partition Control&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.01097"&gt;EliGen: Entity-Level Controlled Image Generation with Regional Attention&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-EliGen.py"&gt;/examples/flux/model_inference/FLUX.1-dev-EliGen.py&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Eligen"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/EliGen"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen"&gt;ModelScope EliGen Studio&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet"&gt;EliGen Train Set&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Entity Control Region&lt;/th&gt; 
    &lt;th&gt;Generated Image&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/1c6d9445-5022-4d91-ad2e-dc05321883d1" alt="eligen_example_2_mask_0" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/86739945-cb07-4a49-b3b3-3bb65c90d14f" alt="eligen_example_2_0" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ExVideo: Extended Training for Video Generation Models&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Project Page: &lt;a href="https://ecnu-cilab.github.io/ExVideoProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2406.14130"&gt;ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: Please refer to the &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/ExVideo"&gt;older version&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc"&gt;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Diffutoon: High-Resolution Anime-Style Video Rendering&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Project Page: &lt;a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2401.16224"&gt;Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: Please refer to the &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/Diffutoon"&gt;older version&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd"&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DiffSynth: The Original Version of This Project&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Project Page: &lt;a href="https://ecnu-cilab.github.io/DiffSynth.github.io/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2308.03463"&gt;DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: Please refer to the &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/diffsynth"&gt;older version&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea"&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/hello-agents</title>
      <link>https://github.com/datawhalechina/hello-agents</link>
      <description>&lt;p&gt;ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹&lt;/p&gt;&lt;hr&gt;&lt;div align="right"&gt; 
 &lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/README_EN.md"&gt;English&lt;/a&gt; | ä¸­æ–‡ 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/hello-agents.png" alt="alt text" width="100%" /&gt; 
 &lt;h1&gt;Hello-Agents&lt;/h1&gt; 
 &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt; 
 &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt; 
 &lt;img src="https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub stars" /&gt; 
 &lt;img src="https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub forks" /&gt; 
 &lt;img src="https://img.shields.io/badge/language-Chinese-brightgreen?style=flat" alt="Language" /&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github" alt="GitHub Project" /&gt;&lt;/a&gt; 
 &lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;&lt;img src="https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;amp;logo=gitbook" alt="Online Reading" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¯ é¡¹ç›®ä»‹ç»&lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒå¦‚æœè¯´ 2024 å¹´æ˜¯"ç™¾æ¨¡å¤§æˆ˜"çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†"Agent å…ƒå¹´"ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒHello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„"ä½¿ç”¨è€…"ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„"æ„å»ºè€…"ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ“š å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;h3&gt;åœ¨çº¿é˜…è¯»&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»&lt;/a&gt;&lt;/strong&gt; - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://book.heterocat.com.cn/"&gt;ğŸ“– Cookbook(æµ‹è¯•ç‰ˆ)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;æœ¬åœ°é˜…è¯»&lt;/h3&gt; 
&lt;p&gt;å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚&lt;/p&gt; 
&lt;h3&gt;âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿&lt;/li&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨&lt;/li&gt; 
 &lt;li&gt;ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶&lt;a href="https://github.com/jjyaoao/helloagents"&gt;HelloAgents&lt;/a&gt;&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶&lt;/li&gt; 
 &lt;li&gt;âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯&lt;/li&gt; 
 &lt;li&gt;ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“– å†…å®¹å¯¼èˆª&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç« èŠ‚&lt;/th&gt; 
   &lt;th&gt;å…³é”®å†…å®¹&lt;/th&gt; 
   &lt;th&gt;çŠ¶æ€&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/%E5%89%8D%E8%A8%80.md"&gt;å‰è¨€&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%88%9D%E8%AF%86%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E5%8F%91%E5%B1%95%E5%8F%B2.md"&gt;ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.md"&gt;ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E7%BB%8F%E5%85%B8%E8%8C%83%E5%BC%8F%E6%9E%84%E5%BB%BA.md"&gt;ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%9F%BA%E4%BA%8E%E4%BD%8E%E4%BB%A3%E7%A0%81%E5%B9%B3%E5%8F%B0%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93%E6%90%AD%E5%BB%BA.md"&gt;ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E6%A1%86%E6%9E%B6%E5%BC%80%E5%8F%91%E5%AE%9E%E8%B7%B5.md"&gt;ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E4%BD%A0%E7%9A%84Agent%E6%A1%86%E6%9E%B6.md"&gt;ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter8/%E7%AC%AC%E5%85%AB%E7%AB%A0%20%E8%AE%B0%E5%BF%86%E4%B8%8E%E6%A3%80%E7%B4%A2.md"&gt;ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter9/%E7%AC%AC%E4%B9%9D%E7%AB%A0%20%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B.md"&gt;ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æŒç»­äº¤äº’çš„"æƒ…å¢ƒç†è§£"&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter10/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE.md"&gt;ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCPã€A2Aã€ANP ç­‰åè®®è§£æ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter11/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0%20Agentic-RL.md"&gt;ç¬¬åä¸€ç«  Agentic-RL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter12/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0.md"&gt;ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter13/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0%20%E6%99%BA%E8%83%BD%E6%97%85%E8%A1%8C%E5%8A%A9%E6%89%8B.md"&gt;ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter14/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0%20%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A0%94%E7%A9%B6%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DeepResearch Agent å¤ç°ä¸è§£æ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter15/%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E8%B5%9B%E5%8D%9A%E5%B0%8F%E9%95%87.md"&gt;ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter16/%E7%AC%AC%E5%8D%81%E5%85%AD%E7%AB%A0%20%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1.md"&gt;ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)&lt;/h3&gt; 
&lt;p&gt;â€ƒâ€ƒæ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç¤¾åŒºç²¾é€‰&lt;/th&gt; 
   &lt;th&gt;å†…å®¹æ€»ç»“&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.md"&gt;01-Agenté¢è¯•é¢˜æ€»ç»“&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E5%8F%82%E8%80%83%E7%AD%94%E6%A1%88.md"&gt;01-Agenté¢è¯•é¢˜ç­”æ¡ˆ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra02-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86.md"&gt;02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra03-Dify%E6%99%BA%E8%83%BD%E4%BD%93%E5%88%9B%E5%BB%BA%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B.md"&gt;03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra04-DatawhaleFAQ.md"&gt;04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;PDF ç‰ˆæœ¬ä¸‹è½½&lt;/h3&gt; 
&lt;p&gt;â€ƒâ€ƒ&lt;em&gt;&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Hello-Agents PDF : &lt;a href="https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0"&gt;https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0&lt;/a&gt;&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : &lt;a href="https://www.datawhale.cn/learn/summary/239"&gt;https://www.datawhale.cn/learn/summary/239&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ’¡ å¦‚ä½•å­¦ä¹ &lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒæ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒé¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†"æ™ºèƒ½ä½“"è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;â€ƒâ€ƒæ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„&lt;code&gt;code&lt;/code&gt;æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼&lt;/p&gt; 
&lt;h2&gt;ä¸‹ä¸€æ­¥è§„åˆ’&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[]è‹±æ–‡ç‰ˆæ•™ç¨‹&lt;/li&gt; 
 &lt;li&gt;[]åŒè¯­è§†é¢‘è¯¾ç¨‹[è‹±æ–‡+ä¸­æ–‡]ï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰&lt;/li&gt; 
 &lt;li&gt;[]å…±åˆ›ç¬¬16ç« ï¼ˆæ‰“é€ å„ç±»Agentåº”ç”¨,æ›´æ‰“é€ Agentç”Ÿæ€ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ å¦‚ä½•è´¡çŒ®&lt;/h2&gt; 
&lt;p&gt;æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue&lt;/li&gt; 
 &lt;li&gt;ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request&lt;/li&gt; 
 &lt;li&gt;âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨"ç¤¾åŒºè´¡çŒ®ç²¾é€‰"ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™ è‡´è°¢&lt;/h2&gt; 
&lt;h3&gt;æ ¸å¿ƒè´¡çŒ®è€…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jjyaoao"&gt;é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fengju0213"&gt;å­™éŸ¬-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; (Datawhale æˆå‘˜, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tsumugii24"&gt;å§œèˆ’å‡¡-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt;ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HeteroCat"&gt;é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜&lt;/a&gt; (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fancyboi999"&gt;æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ&lt;/a&gt; (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xinzhongzhu.github.io/"&gt;æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶&lt;/a&gt; (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extra-Chapter è´¡çŒ®è€…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/WHQAQ11"&gt;WH&lt;/a&gt; (å†…å®¹è´¡çŒ®è€…)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/thunderbolt-fire"&gt;å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ&lt;/a&gt; (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tasselszcx"&gt;å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…&lt;/a&gt;(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/XiaoMa-PM"&gt;é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ&lt;/a&gt; (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ç‰¹åˆ«æ„Ÿè°¢&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ„Ÿè°¢ &lt;a href="https://github.com/Sm1les"&gt;@Sm1les&lt;/a&gt; å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ&lt;/li&gt; 
 &lt;li&gt;æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center" style="margin-top: 30px;"&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=datawhalechina/Hello-Agents" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/star-history-20251212.png" alt="Datawhale" width="90%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;å…³äº Datawhale&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/datawhale.png" alt="Datawhale" width="30%" /&gt; 
 &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“œ å¼€æºåè®®&lt;/h2&gt; 
&lt;p&gt;æœ¬ä½œå“é‡‡ç”¨&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®&lt;/a&gt;è¿›è¡Œè®¸å¯ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mindsdb/mindsdb</title>
      <link>https://github.com/mindsdb/mindsdb</link>
      <description>&lt;p&gt;Federated query engine for AI - The only MCP Server you'll ever need&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.org/project/MindsDB/" target="_blank"&gt;&lt;img src="https://badge.fury.io/py/MindsDB.svg?sanitize=true" alt="MindsDB Release" /&gt;&lt;/a&gt; 
 &lt;a href="https://www.python.org/downloads/" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg?sanitize=true" alt="Python supported" /&gt;&lt;/a&gt; 
 &lt;a href="https://hub.docker.com/u/mindsdb" target="_blank"&gt;&lt;img src="https://img.shields.io/docker/pulls/mindsdb/mindsdb" alt="Docker pulls" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/3068" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3068" alt="mindsdb%2Fmindsdb | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;a href="https://github.com/mindsdb/mindsdb"&gt; &lt;img src="https://raw.githubusercontent.com/mindsdb/mindsdb/main/docs/assets/mindsdb_logo.png" alt="MindsDB" width="300" /&gt; &lt;/a&gt; 
 &lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://www.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Website&lt;/a&gt; Â· &lt;a href="https://docs.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Docs&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/contact"&gt;Contact us for a Demo&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Community Slack&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.&lt;/p&gt; 
&lt;a href="https://www.youtube.com/watch?v=MX3OKpnsoLM" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064" alt="MindsDB Demo" /&gt; &lt;/a&gt; 
&lt;h2&gt;Install MindsDB Server&lt;/h2&gt; 
&lt;p&gt;MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker-desktop"&gt;Using Docker Desktop&lt;/a&gt;. This is the fastest and recommended way to get started and have it all running.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker"&gt;Using Docker&lt;/a&gt;. This is also simple, but gives you more flexibility on how to further customize your server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;MindsDB has an MCP server built in&lt;/a&gt; that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Core Philosophy: Connect, Unify, Respond&lt;/h1&gt; 
&lt;p&gt;MindsDB's architecture is built around three fundamental capabilities:&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;Connect&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;You can connect to hundreds of enterprise &lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;data sources (learn more)&lt;/a&gt;. These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/overview"&gt;Unify&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;In many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/knowledge-bases"&gt;&lt;strong&gt;KNOWLEDGE BASES&lt;/strong&gt;&lt;/a&gt; â€“ Index and organize unstructured data for efficient Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/view"&gt;&lt;strong&gt;VIEWS&lt;/strong&gt;&lt;/a&gt; â€“ Simplify data access by creating unified views across different sources (no-ETL).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unification of data can be automated using JOBs&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs"&gt;&lt;strong&gt;JOBS&lt;/strong&gt;&lt;/a&gt; â€“ Schedule synchronization and transformation tasks for real-time processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;Respond&lt;/a&gt; From Your Data&lt;/h2&gt; 
&lt;p&gt;Chat with Your Data&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;&lt;strong&gt;AGENTS&lt;/strong&gt;&lt;/a&gt; â€“ Configure built-in agents specialized in answering questions over your connected and unified data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt; â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contribute&lt;/h2&gt; 
&lt;p&gt;Interested in contributing to MindsDB? Follow our &lt;a href="https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;installation guide for development&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find our &lt;a href="https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contribution guide here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.&lt;/p&gt; 
&lt;p&gt;This project adheres to a &lt;a href="https://github.com/mindsdb/mindsdb/raw/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to follow its terms.&lt;/p&gt; 
&lt;p&gt;Also, check out our &lt;a href="https://mindsdb.com/community?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;community rewards and programs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Support&lt;/h2&gt; 
&lt;p&gt;If you find a bug, please submit an &lt;a href="https://github.com/mindsdb/mindsdb/issues/new/choose"&gt;issue on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Hereâ€™s how you can get community support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ask a question in our &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Slack Community&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join our &lt;a href="https://github.com/mindsdb/mindsdb/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Post on &lt;a href="https://stackoverflow.com/questions/tagged/mindsdb"&gt;Stack Overflow&lt;/a&gt; with the MindsDB tag.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For commercial support, please &lt;a href="https://mindsdb.com/contact?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contact the MindsDB team&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ’š Current Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/mindsdb/mindsdb/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=mindsdb/mindsdb" /&gt; &lt;/a&gt; 
&lt;p&gt;Generated with &lt;a href="https://contributors-img.web.app"&gt;contributors-img&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ”” Subscribe for Updates&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://mindsdb.com/joincommunity"&gt;Slack community&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trustedsec/social-engineer-toolkit</title>
      <link>https://github.com/trustedsec/social-engineer-toolkit</link>
      <description>&lt;p&gt;The Social-Engineer Toolkit (SET) repository from TrustedSec - All new versions of SET will be deployed here.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Social-Engineer Toolkit (SET)&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copyright &lt;span&gt;Â©&lt;/span&gt; 2020&lt;/li&gt; 
 &lt;li&gt;Written by: David Kennedy (ReL1K) @HackingDave&lt;/li&gt; 
 &lt;li&gt;Company: &lt;a href="https://www.trustedsec.com"&gt;TrustedSec&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;The Social-Engineer Toolkit is an open-source penetration testing framework designed for social engineering. SET has a number of custom attack vectors that allow you to make a believable attack quickly. SET is a product of TrustedSec, LLC â€“ an information security consulting firm located in Cleveland, Ohio.&lt;/p&gt; 
&lt;p&gt;DISCLAIMER: This is &lt;em&gt;only&lt;/em&gt; for testing purposes and can only be used where strict consent has been given. Do not use this for illegal purposes, period. Please read the LICENSE under readme/LICENSE for the licensing of SET.&lt;/p&gt; 
&lt;h4&gt;Supported platforms:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Linux&lt;/li&gt; 
 &lt;li&gt;Mac OS X (experimental)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;h2&gt;Install via requirements.txt&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install -r requirements.txt
python3 setup.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Install SET&lt;/h2&gt; 
&lt;p&gt;=======&lt;/p&gt; 
&lt;h4&gt;Mac OS X&lt;/h4&gt; 
&lt;p&gt;You will need to use a virtual environment for the Python install if you are using an M2 Macbook with the following instructions in your CLI within the social-engineer-toolkit directory.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;    # to install dependencies, run the following:
    python3 -m venv path/to/venv
    source path/to/venv/bin/activate
    python3 -m pip install -r requirements.txt

    # to install SET
    sudo python3 setup.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h4&gt;Windows 10 WSL/WSL2 Kali Linux&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install set -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Kali Linux on Windows 10 is a minimal installation so it doesn't have any tools installed. You can easily install Social Engineer Toolkit on WSL/WSL2 without needing pip using the above command.&lt;/p&gt; 
&lt;h4&gt;Linux&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/trustedsec/social-engineer-toolkit/ setoolkit/
cd setoolkit
pip3 install -r requirements.txt
python setup.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h2&gt;SET Tutorial&lt;/h2&gt; 
&lt;p&gt;For a full document on how to use SET, &lt;a href="https://github.com/trustedsec/social-engineer-toolkit/raw/master/readme/User_Manual.pdf"&gt;visit the SET user manual&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Bugs and enhancements&lt;/h2&gt; 
&lt;p&gt;For bug reports or enhancements, please open an &lt;a href="https://github.com/trustedsec/social-engineer-toolkit/issues"&gt;issue&lt;/a&gt; here. &lt;br /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;ğŸ™ï¸ VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=microsoft" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader" alt="Technical Report" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;ğŸ“° News&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/Status-New-brightgreen?style=flat" alt="New" /&gt; 
 &lt;img src="https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;amp;logo=soundcharts" alt="Realtime TTS" /&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-03: ğŸ“£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoiceâ€‘Realtimeâ€‘0.5B&lt;/strong&gt;&lt;/a&gt;, a realâ€‘time textâ€‘toâ€‘speech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-09: ğŸ“£ Weâ€™ve added experimental speakers in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) for explorationâ€”welcome to try them out and share your feedback.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers. &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;(Launch your own realtime demo via the websocket example in &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo"&gt;Usage&lt;/a&gt;).&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoftâ€™s guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/p&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;VibeVoice is a novel framework designed for generating &lt;strong&gt;expressive&lt;/strong&gt;, &lt;strong&gt;long-form&lt;/strong&gt;, &lt;strong&gt;multi-speaker&lt;/strong&gt; conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.&lt;/p&gt; 
&lt;p&gt;VibeVoice currently includes two model variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Long-form multi-speaker model&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; with up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt;, surpassing the typical 1â€“2 speaker limits of many prior models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;Realtime streaming TTS model&lt;/a&gt;&lt;/strong&gt;: Produces initial audible speech in ~&lt;strong&gt;300 ms&lt;/strong&gt; and supports &lt;strong&gt;streaming text input&lt;/strong&gt; for single-speaker &lt;strong&gt;real-time&lt;/strong&gt; speech generation; designed for low-latency generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/MOS-preference.png" alt="MOS Preference Results" height="260px" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice.jpg" alt="VibeVoice Overview" height="250px" style="margin-right: 10px;" /&gt; &lt;/p&gt; 
&lt;h3&gt;ğŸµ Demo Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Video Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We produced this video with &lt;a href="https://github.com/Wan-Video/Wan2.2"&gt;Wan2.2&lt;/a&gt;. We sincerely appreciate the Wan-Video team for their great work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;For more examples, see the &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Risks and limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.&lt;/p&gt; 
&lt;p&gt;Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.&lt;/p&gt; 
&lt;p&gt;Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/cutile-python</title>
      <link>https://github.com/NVIDIA/cutile-python</link>
      <description>&lt;p&gt;cuTile is a programming model for writing parallel kernels for NVIDIA GPUs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;cuTile Python&lt;/h1&gt; 
&lt;p&gt;cuTile Python is a programming language for NVIDIA GPUs. The official documentation can be found on &lt;a href="https://docs.nvidia.com/cuda/cutile-python"&gt;docs.nvidia.com&lt;/a&gt;, or built from source located in the &lt;a href="https://raw.githubusercontent.com/NVIDIA/cutile-python/main/docs/"&gt;docs&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h2&gt;Example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# This examples uses CuPy which can be installed via `pip install cupy-cuda13x`
# Make sure cuda toolkit 13.1+ is installed: https://developer.nvidia.com/cuda-downloads

import cuda.tile as ct
import cupy

TILE_SIZE = 16

# cuTile kernel for adding two dense vectors. It runs in parallel on the GPU.
@ct.kernel
def vector_add_kernel(a, b, result):
    block_id = ct.bid(0)
    a_tile = ct.load(a, index=(block_id,), shape=(TILE_SIZE,))
    b_tile = ct.load(b, index=(block_id,), shape=(TILE_SIZE,))
    result_tile = a_tile + b_tile
    ct.store(result, index=(block_id,), tile=result_tile)

# Host-side function that launches the above kernel.
def vector_add(a: cupy.ndarray, b: cupy.ndarray, result: cupy.ndarray):
    assert a.shape == b.shape == result.shape
    grid = (ct.cdiv(a.shape[0], TILE_SIZE), 1, 1)
    ct.launch(cupy.cuda.get_current_stream(), grid, vector_add_kernel, (a, b, result))


import numpy as np

def test_vector_add():
    a = cupy.random.uniform(-5, 5, 128)
    b = cupy.random.uniform(-5, 5, 128)
    result = cupy.zeros_like(a)

    vector_add(a, b, result)

    a_np = cupy.asnumpy(a)
    b_np = cupy.asnumpy(b)
    result_np = cupy.asnumpy(result)

    expected = a_np + b_np
    np.testing.assert_array_almost_equal(result_np, expected)

test_vector_add()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;System Requirements&lt;/h2&gt; 
&lt;p&gt;cuTile Python generates kernels based on &lt;a href="https://docs.nvidia.com/cuda/tile-ir/"&gt;Tile IR&lt;/a&gt; which requries NVIDIA Driver r580 or later to run. Furthermore, the &lt;code&gt;tileiras&lt;/code&gt; compiler only supports Blackwell GPU with 13.1 release, but the restriction will be removed in the coming versions. Checkout the &lt;a href="https://docs.nvidia.com/cuda/cutile-python/quickstart.html#prerequisites"&gt;prerequisites&lt;/a&gt; for full list of requirements.&lt;/p&gt; 
&lt;h2&gt;Installing from PyPI&lt;/h2&gt; 
&lt;p&gt;cuTile Python is published on &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt; under the &lt;a href="https://pypi.org/project/cuda-tile/"&gt;cuda-tile&lt;/a&gt; package name and can be installed with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install cuda-tile
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Currently, the &lt;a href="https://developer.nvidia.com/cuda-downloads"&gt;CUDA Toolkit 13.1+&lt;/a&gt; is required and needs to be installed separately. On a Debian-based system, use &lt;code&gt;apt-get install cuda-tileiras-13.1 cuda-compiler-13.1&lt;/code&gt; instead of &lt;code&gt;apt-get install cuda-toolkit-13.1&lt;/code&gt; if you wish to avoid installing the full CUDA Toolkit.&lt;/p&gt; 
&lt;h2&gt;Building from Source&lt;/h2&gt; 
&lt;p&gt;cuTile is written mostly in Python, but includes a C++ extension which needs to be built. You will need:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A C++17-capable compiler, such as GNU C++ or MSVC;&lt;/li&gt; 
 &lt;li&gt;CMake 3.18+;&lt;/li&gt; 
 &lt;li&gt;GNU Make on Linux or msbuild on Windows;&lt;/li&gt; 
 &lt;li&gt;Python 3.10+ with development headers (&lt;code&gt;venv&lt;/code&gt; module is recommended but optional);&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/cuda-downloads"&gt;CUDA Toolkit 13.1+&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;On an Ubuntu system, the first four dependencies can be installed with APT:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install build-essential cmake python3-dev python3-venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The CMakeLists.txt script will also automatically download the &lt;a href="https://github.com/dmlc/dlpack"&gt;DLPack&lt;/a&gt; dependency from GitHub. If you wish to disable this behavior and provide your own copy of DLPack, set the &lt;code&gt;CUDA_TILE_CMAKE_DLPACK_PATH&lt;/code&gt; environment variable to a local path to the DLPack source tree.&lt;/p&gt; 
&lt;p&gt;Unless you are already using a Python virtual environment, it is recommended to create one in order to avoid installing cuTile globally:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m venv env
source env/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the build dependencies are in place, the simplest way to build cuTile is to install it in editable mode by running the following command in the source root directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create the &lt;code&gt;build&lt;/code&gt; directory and invoke the CMake-based build process. In editable mode, the compiled extension module will be placed in the build directory, and then a symbolic link to it will be created in the source directory. This makes sure that the &lt;code&gt;pip install -e .&lt;/code&gt; command above is needed only once, and recompiling the extension after making changes to the C++ code can be done with &lt;code&gt;make -C build&lt;/code&gt; which is much faster. This logic is defined in &lt;a href="https://raw.githubusercontent.com/NVIDIA/cutile-python/main/setup.py"&gt;setup.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Running Tests&lt;/h2&gt; 
&lt;p&gt;cuTile uses the &lt;a href="https://pytest.org"&gt;pytest&lt;/a&gt; framework for testing. Tests have extra dependencies, such as PyTorch, which can be installed with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -r test/requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The tests are located in the &lt;a href="https://raw.githubusercontent.com/NVIDIA/cutile-python/main/test/"&gt;test/&lt;/a&gt; directory. To run a specific test file, for example &lt;code&gt;test_copy.py&lt;/code&gt;, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pytest test/test_copy.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Copyright and License Information&lt;/h2&gt; 
&lt;p&gt;Copyright Â© 2025 NVIDIA CORPORATION &amp;amp; AFFILIATES. All rights reserved.&lt;/p&gt; 
&lt;p&gt;cuTile-Python is licensed under the Apache 2.0 license. See the &lt;a href="https://raw.githubusercontent.com/NVIDIA/cutile-python/main/LICENSES/"&gt;LICENSES&lt;/a&gt; folder for the full license text.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>strands-agents/sdk-python</title>
      <link>https://github.com/strands-agents/sdk-python</link>
      <description>&lt;p&gt;A model-driven approach to building AI agents in just a few lines of code.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div&gt; 
  &lt;a href="https://strandsagents.com"&gt; &lt;img src="https://strandsagents.com/latest/assets/logo-github.svg?sanitize=true" alt="Strands Agents" width="55px" height="105px" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h1&gt; Strands Agents &lt;/h1&gt; 
 &lt;h2&gt; A model-driven approach to building AI agents in just a few lines of code. &lt;/h2&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/graphs/commit-activity"&gt;&lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/issues"&gt;&lt;img alt="GitHub open issues" src="https://img.shields.io/github/issues/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/pulls"&gt;&lt;img alt="GitHub open pull requests" src="https://img.shields.io/github/issues-pr/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://pypi.org/project/strands-agents/"&gt;&lt;img alt="PyPI version" src="https://img.shields.io/pypi/v/strands-agents" /&gt;&lt;/a&gt; 
  &lt;a href="https://python.org"&gt;&lt;img alt="Python versions" src="https://img.shields.io/pypi/pyversions/strands-agents" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;p&gt; &lt;a href="https://strandsagents.com/"&gt;Documentation&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/samples"&gt;Samples&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/sdk-python"&gt;Python SDK&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/tools"&gt;Tools&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/agent-builder"&gt;Agent Builder&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/mcp-server"&gt;MCP Server&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs.&lt;/p&gt; 
&lt;h2&gt;Feature Overview&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Flexible&lt;/strong&gt;: Simple agent loop that just works and is fully customizable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Agnostic&lt;/strong&gt;: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Capabilities&lt;/strong&gt;: Multi-agent systems, autonomous agents, and streaming support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in MCP&lt;/strong&gt;: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Strands Agents
pip install strands-agents strands-agents-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764")
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For the default Amazon Bedrock model provider, you'll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the &lt;a href="https://strandsagents.com/"&gt;Quickstart Guide&lt;/a&gt; for details on configuring other model providers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Ensure you have Python 3.10+ installed, then:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate

# Install Strands and tools
pip install strands-agents strands-agents-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Features at a Glance&lt;/h2&gt; 
&lt;h3&gt;Python-Based Tools&lt;/h3&gt; 
&lt;p&gt;Easily build tools using Python decorators:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent, tool

@tool
def word_count(text: str) -&amp;gt; int:
    """Count words in text.

    This docstring is used by the LLM to understand the tool's purpose.
    """
    return len(text.split())

agent = Agent(tools=[word_count])
response = agent("How many words are in this sentence?")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Hot Reloading from Directory:&lt;/strong&gt; Enable automatic tool loading and reloading from the &lt;code&gt;./tools/&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent

# Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent("Use any tools you find in the tools directory")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP Support&lt;/h3&gt; 
&lt;p&gt;Seamlessly integrate Model Context Protocol (MCP) servers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters

aws_docs_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(command="uvx", args=["awslabs.aws-documentation-mcp-server@latest"]))
)

with aws_docs_client:
   agent = Agent(tools=aws_docs_client.list_tools_sync())
   response = agent("Tell me about Amazon Bedrock and how to use it with Python")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multiple Model Providers&lt;/h3&gt; 
&lt;p&gt;Support for various model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel
from strands.models.gemini import GeminiModel
from strands.models.llamacpp import LlamaCppModel

# Bedrock
bedrock_model = BedrockModel(
  model_id="us.amazon.nova-pro-v1:0",
  temperature=0.3,
  streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent("Tell me about Agentic AI")

# Google Gemini
gemini_model = GeminiModel(
  client_args={
    "api_key": "your_gemini_api_key",
  },
  model_id="gemini-2.5-flash",
  params={"temperature": 0.7}
)
agent = Agent(model=gemini_model)
agent("Tell me about Agentic AI")

# Ollama
ollama_model = OllamaModel(
  host="http://localhost:11434",
  model_id="llama3"
)
agent = Agent(model=ollama_model)
agent("Tell me about Agentic AI")

# Llama API
llama_model = LlamaAPIModel(
    model_id="Llama-4-Maverick-17B-128E-Instruct-FP8",
)
agent = Agent(model=llama_model)
response = agent("Tell me about Agentic AI")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Built-in providers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/amazon-bedrock/"&gt;Amazon Bedrock&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/anthropic/"&gt;Anthropic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/gemini/"&gt;Gemini&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/cohere/"&gt;Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/litellm/"&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/llamacpp/"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/llamaapi/"&gt;LlamaAPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/mistral/"&gt;MistralAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/ollama/"&gt;Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/openai/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/sagemaker/"&gt;SageMaker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/writer/"&gt;Writer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Custom providers can be implemented using &lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/custom_model_provider/"&gt;Custom Providers&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Example tools&lt;/h3&gt; 
&lt;p&gt;Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It's also available on GitHub via &lt;a href="https://github.com/strands-agents/tools"&gt;strands-agents/tools&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Bidirectional Streaming&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ Experimental Feature&lt;/strong&gt;: Bidirectional streaming is currently in experimental status. APIs may change in future releases as we refine the feature based on user feedback and evolving model capabilities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Build real-time voice and audio conversations with persistent streaming connections. Unlike traditional request-response patterns, bidirectional streaming maintains long-running conversations where users can interrupt, provide continuous input, and receive real-time audio responses. Get started with your first BidiAgent by following the &lt;a href="https://strandsagents.com/latest/documentation/docs/user-guide/concepts/experimental/bidirectional-streaming/quickstart"&gt;Quickstart&lt;/a&gt; guide.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Supported Model Providers:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Amazon Nova Sonic (&lt;code&gt;amazon.nova-sonic-v1:0&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Google Gemini Live (&lt;code&gt;gemini-2.5-flash-native-audio-preview-09-2025&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;OpenAI Realtime API (&lt;code&gt;gpt-realtime&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from strands.experimental.bidi import BidiAgent
from strands.experimental.bidi.models import BidiNovaSonicModel
from strands.experimental.bidi.io import BidiAudioIO, BidiTextIO
from strands.experimental.bidi.tools import stop_conversation
from strands_tools import calculator

async def main():
    # Create bidirectional agent with audio model
    model = BidiNovaSonicModel()
    agent = BidiAgent(model=model, tools=[calculator, stop_conversation])

    # Setup audio and text I/O
    audio_io = BidiAudioIO()
    text_io = BidiTextIO()

    # Run with real-time audio streaming
    # Say "stop conversation" to gracefully end the conversation
    await agent.run(
        inputs=[audio_io.input()],
        outputs=[audio_io.output(), text_io.output()]
    )

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Configuration Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Configure audio settings
model = BidiNovaSonicModel(
    provider_config={
        "audio": {
            "input_rate": 16000,
            "output_rate": 16000,
            "voice": "matthew"
        },
        "inference": {
            "max_tokens": 2048,
            "temperature": 0.7
        }
    }
)

# Configure I/O devices
audio_io = BidiAudioIO(
    input_device_index=0,  # Specific microphone
    output_device_index=1,  # Specific speaker
    input_buffer_size=10,
    output_buffer_size=10
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For detailed guidance &amp;amp; examples, explore our documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/"&gt;User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/quickstart/"&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/agents/agent-loop/"&gt;Agent Loop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/examples/"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/api-reference/agent/"&gt;API Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/deploy/operating-agents-in-production/"&gt;Production &amp;amp; Deployment Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing â¤ï¸&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! See our &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for details on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reporting bugs &amp;amp; features&lt;/li&gt; 
 &lt;li&gt;Development setup&lt;/li&gt; 
 &lt;li&gt;Contributing via Pull Requests&lt;/li&gt; 
 &lt;li&gt;Code of Conduct&lt;/li&gt; 
 &lt;li&gt;Reporting of security issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/CONTRIBUTING.md#security-issue-notifications"&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zhu-xlab/GlobalBuildingAtlas</title>
      <link>https://github.com/zhu-xlab/GlobalBuildingAtlas</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GlobalBuildingAtlas&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;In this project, we provide the level of detail 1 (LoD1) data of buildings across the globe.&lt;/p&gt; 
&lt;p&gt;A overview of the dataset is illustrated bellow:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/zhu-xlab/GlobalBuildingAtlas/main/figures/overview.png" width="800" /&gt; 
&lt;h2&gt;Access to the Data&lt;/h2&gt; 
&lt;h3&gt;Web Feature Service (WFS)&lt;/h3&gt; 
&lt;p&gt;A WFS is provided so that one can access the data using other websites or GIS softwares such as QGIS and ArcGIS.&lt;/p&gt; 
&lt;p&gt;Url: &lt;code&gt;https://tubvsig-so2sat-vm1.srv.mwn.de/geoserver/ows?&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Web Viewer&lt;/h3&gt; 
&lt;p&gt;A web interface for viewing the data is available at: &lt;a href="https://tubvsig-so2sat-vm1.srv.mwn.de"&gt;website&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Note: Over the past few days, our web viewer has received nearly 280,000 access requests. Due to this unusually high traffic, some data may not load completely, which may result in a significant portion of buildings not being displayed.&lt;/p&gt; 
&lt;h3&gt;Full Data Download&lt;/h3&gt; 
&lt;p&gt;The full data can be downloaded from &lt;a href="https://mediatum.ub.tum.de/1782307"&gt;mediaTUM&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development Code&lt;/h2&gt; 
&lt;h3&gt;Global Building Polygon Generation using Satellite Data (Sec. 4.3)&lt;/h3&gt; 
&lt;p&gt;For codes related to building map extraction, regularization, polygonization, and simplification, i.e., generating building polygons from satellite images (Sec. 4.3.2, Sec. 4.3.3, and Sec. 4.3.4), please refer to &lt;code&gt;./im2bf&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Global Building Height Estimation (Sec. 4.4)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;For codes related to monocular height estimation using HTC-DC Net (Sec. 4.4.2), please refer to &lt;code&gt;./im2bh&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;For codes related to the global inference and uncertainty quantification (Sec. 4.4.3), please refer to &lt;code&gt;./infer_height&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Global LoD1 Building Model Generation (Sec. 4.5)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;For codes related to quality-guided building polygon fusion (Sec. 4.5.1), please refer to &lt;code&gt;./fuse_bf&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;For codes related to LoD1 building model generation (Sec. 4.5.2), please refer to &lt;code&gt;./make_lod1&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Visualization Code&lt;/h2&gt; 
&lt;p&gt;For codes to reproduce the plots in the manuscript, please refer to &lt;code&gt;./make_plots&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Code License&lt;/h2&gt; 
&lt;p&gt;MIT with Commons Clause (no commercial use allowed). See &lt;a href="https://github.com/zhu-xlab/GlobalBuildingAtlas/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How to cite&lt;/h2&gt; 
&lt;p&gt;If you find this dataset helpful in your work, please cite the following paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@Article{essd-17-6647-2025,
AUTHOR = {Zhu, X. X. and Chen, S. and Zhang, F. and Shi, Y. and Wang, Y.},
TITLE = {GlobalBuildingAtlas: an open global and complete dataset of building polygons, heights and LoD1 3D models},
JOURNAL = {Earth System Science Data},
VOLUME = {17},
YEAR = {2025},
NUMBER = {12},
PAGES = {6647--6668},
URL = {https://essd.copernicus.org/articles/17/6647/2025/},
DOI = {10.5194/essd-17-6647-2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>RosettaCommons/foundry</title>
      <link>https://github.com/RosettaCommons/foundry</link>
      <description>&lt;p&gt;Central repository for biomolecular foundation models with shared trainers and pipeline components&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protein design with Foundry&lt;/h1&gt; 
&lt;p&gt;Foundry provides tooling and infrastructure for using and training all classes of models for protein design, including design (RFD3), inverse folding (ProteinMPNN) and protein folding (RF3).&lt;/p&gt; 
&lt;p&gt;All models within Foundry rely on &lt;a href="https://github.com/RosettaCommons/atomworks"&gt;AtomWorks&lt;/a&gt; - a unified framework for manipulating and processing biomolecular structures - for both training and inference.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] We have a slack now! Join for updates and to get your questions answered &lt;a href="https://join.slack.com/t/proteinmodelfoundry/shared_invite/zt-3kpwru8c6-nrmTW6LNHnSE7h16GNnfLA"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Quickstart guide&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "rc-foundry[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Downloading weights&lt;/strong&gt; Models can be downloaded to a target folder with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;foundry install base-models --checkpoint-dir &amp;lt;path/to/ckpt/dir&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;where &lt;code&gt;checkpoint-dir&lt;/code&gt; will be &lt;code&gt;~/.foundry/checkpoints&lt;/code&gt; by default. Foundry always searches &lt;code&gt;~/.foundry/checkpoints&lt;/code&gt; plus any colon-separated entries in &lt;code&gt;$FOUNDRY_CHECKPOINT_DIRS&lt;/code&gt; during inference or subsequent commands to find checkpoints. &lt;code&gt;base-models&lt;/code&gt; installs the latest RFD3, RF3 and MPNN variants - you can also download all of the models supported (including multiple checkpoints of RF3) with &lt;code&gt;all&lt;/code&gt;, or by listing the models sequentially (e.g. &lt;code&gt;foundry install rfd3 rf3 ...&lt;/code&gt;). To list the registry of available checkpoints:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;foundry list-available
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To check what you already have downloaded (searches &lt;code&gt;~/.foundry/checkpoints&lt;/code&gt; plus &lt;code&gt;$FOUNDRY_CHECKPOINT_DIRS&lt;/code&gt; if set):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;foundry list-installed
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;See &lt;code&gt;examples/all.ipynb&lt;/code&gt; for how to run each model and design proteins end-to-end in a notebook.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Google Colab&lt;/h3&gt; 
&lt;p&gt;For an interactive Google Colab notebook walking through a basic design pipeline with RFD3, MPNN, and RF3, please see the &lt;a href="https://colab.research.google.com/drive/1ZwIMV3n9h0ZOnIXX0GyKUuoiahgifBxh?usp=sharing"&gt;IPD Design Pipeline Tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;RFdiffusion3 (RFD3)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.biorxiv.org/content/10.1101/2025.09.18.676967v2"&gt;RFdiffusion3&lt;/a&gt; is an all-atom generative model capable of designing protein structures under complex constraints.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/RosettaCommons/foundry/production/docs/_static/cover.png" alt="RFdiffusion3 generation trajectory." width="700" /&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;See &lt;a href="https://raw.githubusercontent.com/RosettaCommons/foundry/production/models/rfd3/README.md"&gt;models/rfd3/README.md&lt;/a&gt; for complete documentation.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;RosettaFold3 (RF3)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://doi.org/10.1101/2025.08.14.670328"&gt;RF3&lt;/a&gt; is a structure prediction neural network that narrows the gap between closed-source AF-3 and open-source alternatives.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/RosettaCommons/foundry/production/docs/_static/prot_dna.png" alt="Protein-DNA complex prediction" width="400" /&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;See &lt;a href="https://raw.githubusercontent.com/RosettaCommons/foundry/production/models/rf3/README.md"&gt;models/rf3/README.md&lt;/a&gt; for complete documentation.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ProteinMPNN&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.science.org/doi/10.1126/science.add2187"&gt;ProteinMPNN&lt;/a&gt; and &lt;a href="https://www.nature.com/articles/s41592-025-02626-1"&gt;LigandMPNN&lt;/a&gt; are lightweight inverse-folding models which can be use to design diverse sequences for backbones under constrained conditions.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;See &lt;a href="https://raw.githubusercontent.com/RosettaCommons/foundry/production/models/mpnn/README.md"&gt;models/mpnn/README.md&lt;/a&gt; for complete documentation.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Code Organization&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Strict dependency flow:&lt;/strong&gt; &lt;code&gt;foundry&lt;/code&gt; â†’ &lt;code&gt;atomworks&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;atomworks&lt;/strong&gt;: Structure I/O, preprocessing, featurization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;foundry&lt;/strong&gt;: Model architectures, training, inference endpoints&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;models/&amp;lt;model&amp;gt;:&lt;/strong&gt; Released models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;For Core Developers (Multiple Packages)&lt;/h4&gt; 
&lt;p&gt;Install both &lt;code&gt;foundry&lt;/code&gt; and models in editable mode for development:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install -e '.[all,dev]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This approach allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modify &lt;code&gt;foundry&lt;/code&gt; shared utilities and see changes immediately&lt;/li&gt; 
 &lt;li&gt;Work on specific models without installing all models&lt;/li&gt; 
 &lt;li&gt;Add new models as independent packages in &lt;code&gt;models/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Running tests is not currently supported, test files may be missing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Adding New Models&lt;/h3&gt; 
&lt;p&gt;To add a new model:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create &lt;code&gt;models/&amp;lt;model_name&amp;gt;/&lt;/code&gt; directory with its own &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Add &lt;code&gt;foundry&lt;/code&gt; as a dependency&lt;/li&gt; 
 &lt;li&gt;Implement model-specific code in &lt;code&gt;models/&amp;lt;model_name&amp;gt;/src/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Users can install with: &lt;code&gt;uv pip install -e ./models/&amp;lt;model_name&amp;gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Pre-commit Formatting&lt;/h3&gt; 
&lt;p&gt;We ship a &lt;code&gt;.pre-commit-config.yaml&lt;/code&gt; that runs &lt;code&gt;make format&lt;/code&gt; (via &lt;code&gt;ruff format&lt;/code&gt;) before each commit. Enable it once per clone:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit  # if not already installed
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installation the hook automatically formats the repo whenever you &lt;code&gt;git commit&lt;/code&gt;. Use &lt;code&gt;pre-commit run --all-files&lt;/code&gt; to apply it manually.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use this repository code or data in your work, please cite the relavant work as below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{corley2025accelerating,
  title={Accelerating biomolecular modeling with atomworks and rf3},
  author={Corley, Nathaniel and Mathis, Simon and Krishna, Rohith and Bauer, Magnus S and Thompson, Tuscan R and Ahern, Woody and Kazman, Maxwell W and Brent, Rafael I and Didi, Kieran and Kubaney, Andrew and others},
  journal={bioRxiv},
  year={2025}
}

@article {butcher2025_rfdiffusion3,
    author = {Butcher, Jasper and Krishna, Rohith and Mitra, Raktim and Brent, Rafael Isaac and Li, Yanjing and Corley, Nathaniel and Kim, Paul T and Funk, Jonathan and Mathis, Simon Valentin and Salike, Saman and Muraishi, Aiko and Eisenach, Helen and Thompson, Tuscan Rock and Chen, Jie and Politanska, Yuliya and Sehgal, Enisha and Coventry, Brian and Zhang, Odin and Qiang, Bo and Didi, Kieran and Kazman, Maxwell and DiMaio, Frank and Baker, David},
    title = {De novo Design of All-atom Biomolecular Interactions with RFdiffusion3},
    elocation-id = {2025.09.18.676967},
    year = {2025},
    doi = {10.1101/2025.09.18.676967},
    publisher = {Cold Spring Harbor Laboratory},
    URL = {https://www.biorxiv.org/content/early/2025/11/19/2025.09.18.676967},
    eprint = {https://www.biorxiv.org/content/early/2025/11/19/2025.09.18.676967.full.pdf},
    journal = {bioRxiv}
}

@article{dauparas2022robust,
  title={Robust deep learning--based protein sequence design using ProteinMPNN},
  author={Dauparas, Justas and Anishchenko, Ivan and Bennett, Nathaniel and Bai, Hua and Ragotte, Robert J and Milles, Lukas F and Wicky, Basile IM and Courbet, Alexis and de Haas, Rob J and Bethel, Neville and others},
  journal={Science},
  volume={378},
  number={6615},
  pages={49--56},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{dauparas2025atomic,
  title={Atomic context-conditioned protein sequence design using LigandMPNN},
  author={Dauparas, Justas and Lee, Gyu Rie and Pecoraro, Robert and An, Linna and Anishchenko, Ivan and Glasscock, Cameron and Baker, David},
  journal={Nature Methods},
  pages={1--7},
  year={2025},
  publisher={Nature Publishing Group US New York}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We thank Rachel Clune and Hope Woods from the RosettaCommons for their collaboration on the codebase, documentation, tutorials and examples.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>infiniflow/ragflow</title>
      <link>https://github.com/infiniflow/ragflow</link>
      <description>&lt;p&gt;RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://demo.ragflow.io/"&gt; &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow/main/web/src/assets/logo-with-text.svg?sanitize=true" width="520" alt="ragflow logo" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README.md"&gt;&lt;img alt="README in English" src="https://img.shields.io/badge/English-DBEDFA" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_zh.md"&gt;&lt;img alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_tzh.md"&gt;&lt;img alt="ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ja.md"&gt;&lt;img alt="æ—¥æœ¬èªã®README" src="https://img.shields.io/badge/æ—¥æœ¬èª-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ko.md"&gt;&lt;img alt="í•œêµ­ì–´" src="https://img.shields.io/badge/í•œêµ­ì–´-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_id.md"&gt;&lt;img alt="Bahasa Indonesia" src="https://img.shields.io/badge/Bahasa Indonesia-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_pt_br.md"&gt;&lt;img alt="PortuguÃªs(Brasil)" src="https://img.shields.io/badge/PortuguÃªs(Brasil)-DFE0E5" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/intent/follow?screen_name=infiniflowai" target="_blank"&gt; &lt;img src="https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;amp;color=%20%23f5f5f5" alt="follow on X(Twitter)" /&gt; &lt;/a&gt; &lt;a href="https://demo.ragflow.io" target="_blank"&gt; &lt;img alt="Static Badge" src="https://img.shields.io/badge/Online-Demo-4e6b99" /&gt; &lt;/a&gt; &lt;a href="https://hub.docker.com/r/infiniflow/ragflow" target="_blank"&gt; &lt;img src="https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;amp;color=0db7ed&amp;amp;logo=docker&amp;amp;logoColor=white&amp;amp;style=flat-square" alt="docker pull infiniflow/ragflow:v0.22.1" /&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/releases/latest"&gt; &lt;img src="https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;amp;label=Latest%20Release" alt="Latest Release" /&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/raw/main/LICENSE"&gt; &lt;img height="21" src="https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;amp;color=2e6cc4" alt="license" /&gt; &lt;/a&gt; &lt;a href="https://deepwiki.com/infiniflow/ragflow"&gt; &lt;img alt="Ask DeepWiki" src="https://deepwiki.com/badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://ragflow.io/docs/dev/"&gt;Document&lt;/a&gt; | &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;Roadmap&lt;/a&gt; | &lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt; | &lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt; | &lt;a href="https://demo.ragflow.io"&gt;Demo&lt;/a&gt; &lt;/h4&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png" width="1200" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/9064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9064" alt="infiniflow%2Fragflow | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ“• Table of Contents&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ’¡ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-what-is-ragflow"&gt;What is RAGFlow?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ® &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-demo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ“Œ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-latest-updates"&gt;Latest Updates&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸŒŸ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ” &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-system-architecture"&gt;System Architecture&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ¬ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-get-started"&gt;Get Started&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ”§ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-configurations"&gt;Configurations&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ”§ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-build-a-docker-image"&gt;Build a Docker image&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ”¨ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-launch-service-from-source-for-development"&gt;Launch service from source for development&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ“š &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ“œ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ„ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-community"&gt;Community&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ™Œ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ’¡ What is RAGFlow?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://ragflow.io/"&gt;RAGFlow&lt;/a&gt; is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.&lt;/p&gt; 
&lt;h2&gt;ğŸ® Demo&lt;/h2&gt; 
&lt;p&gt;Try our demo at &lt;a href="https://demo.ragflow.io"&gt;https://demo.ragflow.io&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif" width="1200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif" width="1200" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ”¥ Latest Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-11-19 Supports Gemini 3 Pro.&lt;/li&gt; 
 &lt;li&gt;2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.&lt;/li&gt; 
 &lt;li&gt;2025-10-23 Supports MinerU &amp;amp; Docling as document parsing methods.&lt;/li&gt; 
 &lt;li&gt;2025-10-15 Supports orchestrable ingestion pipeline.&lt;/li&gt; 
 &lt;li&gt;2025-08-08 Supports OpenAI's latest GPT-5 series models.&lt;/li&gt; 
 &lt;li&gt;2025-08-01 Supports agentic workflow and MCP.&lt;/li&gt; 
 &lt;li&gt;2025-05-23 Adds a Python/JavaScript code executor component to Agent.&lt;/li&gt; 
 &lt;li&gt;2025-05-05 Supports cross-language query.&lt;/li&gt; 
 &lt;li&gt;2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‰ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;â­ï¸ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new releases! ğŸŒŸ&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba" width="1200" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ Key Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ­ &lt;strong&gt;"Quality in, quality out"&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/deepdoc/README.md"&gt;Deep document understanding&lt;/a&gt;-based knowledge extraction from unstructured data with complicated formats.&lt;/li&gt; 
 &lt;li&gt;Finds "needle in a data haystack" of literally unlimited tokens.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ± &lt;strong&gt;Template-based chunking&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Intelligent and explainable.&lt;/li&gt; 
 &lt;li&gt;Plenty of template options to choose from.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸŒ± &lt;strong&gt;Grounded citations with reduced hallucinations&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visualization of text chunking to allow human intervention.&lt;/li&gt; 
 &lt;li&gt;Quick view of the key references and traceable citations to support grounded answers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ” &lt;strong&gt;Compatibility with heterogeneous data sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ›€ &lt;strong&gt;Automated and effortless RAG workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Streamlined RAG orchestration catered to both personal and large businesses.&lt;/li&gt; 
 &lt;li&gt;Configurable LLMs as well as embedding models.&lt;/li&gt; 
 &lt;li&gt;Multiple recall paired with fused re-ranking.&lt;/li&gt; 
 &lt;li&gt;Intuitive APIs for seamless integration with business.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ” System Architecture&lt;/h2&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2" width="1000" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ¬ Get Started&lt;/h2&gt; 
&lt;h3&gt;ğŸ“ Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;CPU &amp;gt;= 4 cores&lt;/li&gt; 
 &lt;li&gt;RAM &amp;gt;= 16 GB&lt;/li&gt; 
 &lt;li&gt;Disk &amp;gt;= 50 GB&lt;/li&gt; 
 &lt;li&gt;Docker &amp;gt;= 24.0.0 &amp;amp; Docker Compose &amp;gt;= v2.26.1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gvisor.dev/docs/user_guide/install/"&gt;gVisor&lt;/a&gt;: Required only if you intend to use the code executor (sandbox) feature of RAGFlow.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you have not installed Docker on your local machine (Windows, Mac, or Linux), see &lt;a href="https://docs.docker.com/engine/install/"&gt;Install Docker Engine&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸš€ Start up the server&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;vm.max_map_count&lt;/code&gt; &amp;gt;= 262144:&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;To check the value of &lt;code&gt;vm.max_map_count&lt;/code&gt;:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;$ sysctl vm.max_map_count
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;Reset &lt;code&gt;vm.max_map_count&lt;/code&gt; to a value at least 262144 if it is not.&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# In this case, we set it to 262144:
$ sudo sysctl -w vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;This change will be reset after a system reboot. To ensure your change remains permanent, add or update the &lt;code&gt;vm.max_map_count&lt;/code&gt; value in &lt;strong&gt;/etc/sysctl.conf&lt;/strong&gt; accordingly:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ git clone https://github.com/infiniflow/ragflow.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start up the server using the pre-built Docker images:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] All Docker images are built for x86 platforms. We don't currently offer Docker images for ARM64. If you are on an ARM64 platform, follow &lt;a href="https://ragflow.io/docs/dev/build_docker_image"&gt;this guide&lt;/a&gt; to build a Docker image compatible with your system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The command below downloads the &lt;code&gt;v0.22.1&lt;/code&gt; edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from &lt;code&gt;v0.22.1&lt;/code&gt;, update the &lt;code&gt;RAGFLOW_IMAGE&lt;/code&gt; variable accordingly in &lt;strong&gt;docker/.env&lt;/strong&gt; before using &lt;code&gt;docker compose&lt;/code&gt; to start the server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;   $ cd ragflow/docker
  
   # git checkout v0.22.1
   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)
   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.
   
   # Use CPU for DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate DeepDoc tasks:
   # sed -i '1i DEVICE=gpu' .env
   # docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Prior to &lt;code&gt;v0.22.0&lt;/code&gt;, we provided both images with embedding models and slim images without embedding models. Details as follows:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;RAGFlow image tag&lt;/th&gt; 
   &lt;th&gt;Image size (GB)&lt;/th&gt; 
   &lt;th&gt;Has embedding models?&lt;/th&gt; 
   &lt;th&gt;Stable?&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.21.1&lt;/td&gt; 
   &lt;td&gt;â‰ˆ9&lt;/td&gt; 
   &lt;td&gt;âœ”ï¸&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.21.1-slim&lt;/td&gt; 
   &lt;td&gt;â‰ˆ2&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Starting with &lt;code&gt;v0.22.0&lt;/code&gt;, we ship only the slim edition and no longer append the &lt;strong&gt;-slim&lt;/strong&gt; suffix to the image tag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt; &lt;p&gt;Check the server status after having the server up and running:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker logs -f docker-ragflow-cpu-1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;
      ____   ___    ______ ______ __
     / __ \ /   |  / ____// ____// /____  _      __
    / /_/ // /| | / / __ / /_   / // __ \| | /| / /
   / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
  /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

 * Running on all addresses (0.0.0.0)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a &lt;code&gt;network anormal&lt;/code&gt; error because, at that moment, your RAGFlow may not be fully initialized.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In your web browser, enter the IP address of your server and log in to RAGFlow.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;With the default settings, you only need to enter &lt;code&gt;http://IP_OF_YOUR_MACHINE&lt;/code&gt; (&lt;strong&gt;sans&lt;/strong&gt; port number) as the default HTTP serving port &lt;code&gt;80&lt;/code&gt; can be omitted when using the default configurations.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;, select the desired LLM factory in &lt;code&gt;user_default_llm&lt;/code&gt; and update the &lt;code&gt;API_KEY&lt;/code&gt; field with the corresponding API key.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;See &lt;a href="https://ragflow.io/docs/dev/llm_api_key_setup"&gt;llm_api_key_setup&lt;/a&gt; for more information.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;The show is on!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ”§ Configurations&lt;/h2&gt; 
&lt;p&gt;When it comes to system configurations, you will need to manage the following files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env"&gt;.env&lt;/a&gt;: Keeps the fundamental setups for the system, such as &lt;code&gt;SVR_HTTP_PORT&lt;/code&gt;, &lt;code&gt;MYSQL_PASSWORD&lt;/code&gt;, and &lt;code&gt;MINIO_PASSWORD&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;: Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;: The system relies on &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; to start up.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md"&gt;./docker/README&lt;/a&gt; file provides a detailed description of the environment settings and service configurations which can be used as &lt;code&gt;${ENV_VARS}&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To update the default HTTP serving port (80), go to &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; and change &lt;code&gt;80:80&lt;/code&gt; to &lt;code&gt;&amp;lt;YOUR_SERVING_PORT&amp;gt;:80&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Updates to the above configurations require a reboot of all containers to take effect:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Switch doc engine from Elasticsearch to Infinity&lt;/h3&gt; 
&lt;p&gt;RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to &lt;a href="https://github.com/infiniflow/infinity/"&gt;Infinity&lt;/a&gt;, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Stop all running containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker/docker-compose.yml down -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;code&gt;-v&lt;/code&gt; will delete the docker container volumes, and the existing data will be cleared.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;Set &lt;code&gt;DOC_ENGINE&lt;/code&gt; in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;infinity&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start the containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Switching to Infinity on a Linux/arm64 machine is not yet officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ”§ Build a Docker image&lt;/h2&gt; 
&lt;p&gt;This image is approximately 2 GB in size and relies on external LLM and embedding services.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ”¨ Launch service from source for development&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;pre-commit&lt;/code&gt;, or skip this step if they are already installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pipx install uv pre-commit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source code and install Python dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
uv sync --python 3.12 # install RAGFlow dependent python modules
uv run download_deps.py
pre-commit install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose -f docker/docker-compose-base.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the following line to &lt;code&gt;/etc/hosts&lt;/code&gt; to resolve all hosts specified in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;127.0.0.1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you cannot access HuggingFace, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable to use a mirror site:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export HF_ENDPOINT=https://hf-mirror.com
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If your operating system does not have jemalloc, please install it as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Ubuntu
sudo apt-get install libjemalloc-dev
# CentOS
sudo yum install jemalloc
# OpenSUSE
sudo zypper install jemalloc
# macOS
sudo brew install jemalloc
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch backend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
export PYTHONPATH=$(pwd)
bash docker/launch_backend_service.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install frontend dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd web
npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch frontend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Stop RAGFlow front-end and back-end service after development is complete:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pkill -f "ragflow_server.py|task_executor.py"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/configurations"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/release_notes"&gt;Release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/guides"&gt;User guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/developers"&gt;Developer guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/references"&gt;References&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/faq"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“œ Roadmap&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;RAGFlow Roadmap 2025&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ„ Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/orgs/infiniflow/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™Œ Contributing&lt;/h2&gt; 
&lt;p&gt;RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our &lt;a href="https://ragflow.io/docs/dev/contributing"&gt;Contribution Guidelines&lt;/a&gt; first.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SkyworkAI/SkyReels-V2</title>
      <link>https://github.com/SkyworkAI/SkyReels-V2</link>
      <description>&lt;p&gt;SkyReels-V2: Infinite-length Film Generative model&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/logo2.png" alt="SkyReels Logo" width="50%" /&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;SkyReels V2: Infinite-Length Film Generative Model&lt;/h1&gt; 
&lt;p align="center"&gt; ğŸ“‘ &lt;a href="https://arxiv.org/pdf/2504.13074"&gt;Technical Report&lt;/a&gt; Â· ğŸ‘‹ &lt;a href="https://www.skyreels.ai/home?utm_campaign=github_SkyReels_V2" target="_blank"&gt;Playground&lt;/a&gt; Â· ğŸ’¬ &lt;a href="https://discord.gg/PwM6NYtccQ" target="_blank"&gt;Discord&lt;/a&gt; Â· ğŸ¤— &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9" target="_blank"&gt;Hugging Face&lt;/a&gt; Â· ğŸ¤– &lt;a href="https://www.modelscope.cn/collections/SkyReels-V2-f665650130b144" target="_blank"&gt;ModelScope&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Welcome to the &lt;strong&gt;SkyReels V2&lt;/strong&gt; repository! Here, you'll find the model weights and inference code for our infinite-length film generative models. To the best of our knowledge, it represents the first open-source video generative model employing &lt;strong&gt;AutoRegressive Diffusion-Forcing architecture&lt;/strong&gt; that achieves the &lt;strong&gt;SOTA performance&lt;/strong&gt; among publicly available models.&lt;/p&gt; 
&lt;h2&gt;ğŸ”¥ğŸ”¥ğŸ”¥ News!!&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jun 1, 2025: ğŸ‰ We published the technical report, &lt;a href="https://arxiv.org/pdf/2506.00830"&gt;SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;May 16, 2025: ğŸ”¥ We release the inference code for &lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#ve"&gt;video extension&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#se"&gt;start/end frame control&lt;/a&gt; in diffusion forcing model.&lt;/li&gt; 
 &lt;li&gt;Apr 24, 2025: ğŸ”¥ We release the 720P models, &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P"&gt;SkyReels-V2-DF-14B-720P&lt;/a&gt; and &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P"&gt;SkyReels-V2-I2V-14B-720P&lt;/a&gt;. The former facilitates infinite-length autoregressive video generation, and the latter focuses on Image2Video synthesis.&lt;/li&gt; 
 &lt;li&gt;Apr 21, 2025: ğŸ‘‹ We release the inference code and model weights of &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9"&gt;SkyReels-V2&lt;/a&gt; Series Models and the video captioning model &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; .&lt;/li&gt; 
 &lt;li&gt;Apr 3, 2025: ğŸ”¥ We also release &lt;a href="https://github.com/SkyworkAI/SkyReels-A2"&gt;SkyReels-A2&lt;/a&gt;. This is an open-sourced controllable video generation framework capable of assembling arbitrary visual elements.&lt;/li&gt; 
 &lt;li&gt;Feb 18, 2025: ğŸ”¥ we released &lt;a href="https://github.com/SkyworkAI/SkyReels-A1"&gt;SkyReels-A1&lt;/a&gt;. This is an open-sourced and effective framework for portrait image animation.&lt;/li&gt; 
 &lt;li&gt;Feb 18, 2025: ğŸ”¥ We released &lt;a href="https://github.com/SkyworkAI/SkyReels-V1"&gt;SkyReels-V1&lt;/a&gt;. This is the first and most advanced open-source human-centric video foundation model.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¥ Demos&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/f6f9f9a7-5d5f-433c-9d73-d8d593b7ad25" width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/0eb13415-f4d9-4aaf-bcd3-3031851109b9" width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/dcd16603-5bf4-4786-8e4d-1ed23889d07a" width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; The demos above showcase 30-second videos generated using our SkyReels-V2 Diffusion Forcing model. 
&lt;h2&gt;ğŸ“‘ TODO List&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://arxiv.org/pdf/2504.13074"&gt;Technical Report&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Checkpoints of the 14B and 1.3B Models Series&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Single-GPU &amp;amp; Multi-GPU Inference Code&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt;: A Video Captioning Model&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Prompt Enhancer&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Diffusers integration&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Checkpoints of the 5B Models Series&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Checkpoints of the Camera Director Models&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Checkpoints of the Step &amp;amp; Guidance Distill Model&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Quickstart&lt;/h2&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# clone the repository.
git clone https://github.com/SkyworkAI/SkyReels-V2
cd SkyReels-V2
# Install dependencies. Test environment uses Python 3.10.12.
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Download&lt;/h4&gt; 
&lt;p&gt;You can download our models from Hugging Face:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Model Variant&lt;/th&gt; 
   &lt;th&gt;Recommended Height/Width/Frame&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Diffusion Forcing&lt;/td&gt; 
   &lt;td&gt;1.3B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-1.3B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-1.3B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-720P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Text-to-Video&lt;/td&gt; 
   &lt;td&gt;1.3B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-720P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-720P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Image-to-Video&lt;/td&gt; 
   &lt;td&gt;1.3B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-1.3B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-720P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="3"&gt;Camera Director&lt;/td&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;After downloading, set the model path in your generation commands:&lt;/p&gt; 
&lt;h4&gt;Single GPU Inference&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusion Forcing for Long Video Generation&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;a href="https://arxiv.org/abs/2407.01392"&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/a&gt; version model allows us to generate Infinite-Length videos. This model supports both &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt; and &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; tasks, and it can perform inference in both synchronous and asynchronous modes. Here we demonstrate 2 running scripts as examples for long video generation. If you want to adjust the inference parameters, e.g., the duration of video, inference mode, read the Note below first.&lt;/p&gt; 
&lt;p&gt;synchronous generation for 10s video&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# synchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 257 \
  --overlap_history 17 \
  --prompt "A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed." \
  --addnoise_condition 20 \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;asynchronous generation for 30s video&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# asynchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 5 \
  --causal_block_size 5 \
  --base_num_frames 97 \
  --num_frames 737 \
  --overlap_history 17 \
  --prompt "A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed." \
  --addnoise_condition 20 \
  --offload
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Text-to-video with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from diffusers import AutoModel, SkyReelsV2DiffusionForcingPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video

vae = AutoModel.from_pretrained("Skywork/SkyReels-V2-DF-14B-540P-Diffusers", subfolder="vae", torch_dtype=torch.float32)

pipeline = SkyReelsV2DiffusionForcingPipeline.from_pretrained(
    "Skywork/SkyReels-V2-DF-14B-540P-Diffusers",
    vae=vae,
    torch_dtype=torch.bfloat16
)
flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline = pipeline.to("cuda")

prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."

output = pipeline(
    prompt=prompt,
    num_inference_steps=30,
    height=544,  # 720 for 720P
    width=960,   # 1280 for 720P
    num_frames=97,
    base_num_frames=97,  # 121 for 720P
    ar_step=5,  # Controls asynchronous inference (0 for synchronous mode)
    causal_block_size=5,  # Number of frames in each block for asynchronous processing
    overlap_history=None,  # Number of frames to overlap for smooth transitions in long videos; 17 for long video generations
    addnoise_condition=20,  # Improves consistency in long video generation
).frames[0]
export_to_video(output, "T2V.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Image-to-video with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingImageToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_image

model_id = "Skywork/SkyReels-V2-DF-14B-720P-Diffusers"
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingImageToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to("cuda")

first_frame = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_first_frame.png")
last_frame = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_last_frame.png")

def aspect_ratio_resize(image, pipeline, max_area=720 * 1280):
    aspect_ratio = image.height / image.width
    mod_value = pipeline.vae_scale_factor_spatial * pipeline.transformer.config.patch_size[1]
    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value
    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value
    image = image.resize((width, height))
    return image, height, width

def center_crop_resize(image, height, width):
    # Calculate resize ratio to match first frame dimensions
    resize_ratio = max(width / image.width, height / image.height)

    # Resize the image
    width = round(image.width * resize_ratio)
    height = round(image.height * resize_ratio)
    size = [width, height]
    image = TF.center_crop(image, size)

    return image, height, width

first_frame, height, width = aspect_ratio_resize(first_frame, pipeline)
if last_frame.size != first_frame.size:
    last_frame, _, _ = center_crop_resize(last_frame, height, width)

prompt = "CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird's feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective."

output = pipeline(
    image=first_frame, last_image=last_frame, prompt=prompt, height=height, width=width, guidance_scale=5.0
).frames[0]
export_to_video(output, "output.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;If you want to run the &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; task, add &lt;code&gt;--image ${image_path}&lt;/code&gt; to your command and it is also better to use &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt;-like prompt which includes some descriptions of the first-frame image.&lt;/li&gt; 
  &lt;li&gt;For long video generation, you can just switch the &lt;code&gt;--num_frames&lt;/code&gt;, e.g., &lt;code&gt;--num_frames 257&lt;/code&gt; for 10s video, &lt;code&gt;--num_frames 377&lt;/code&gt; for 15s video, &lt;code&gt;--num_frames 737&lt;/code&gt; for 30s video, &lt;code&gt;--num_frames 1457&lt;/code&gt; for 60s video. The number is not strictly aligned with the logical frame number for specified time duration, but it is aligned with some training parameters, which means it may perform better. When you use asynchronous inference with causal_block_size &amp;gt; 1, the &lt;code&gt;--num_frames&lt;/code&gt; should be carefully set.&lt;/li&gt; 
  &lt;li&gt;You can use &lt;code&gt;--ar_step 5&lt;/code&gt; to enable asynchronous inference. When asynchronous inference, &lt;code&gt;--causal_block_size 5&lt;/code&gt; is recommended while it is not supposed to be set for synchronous generation. REMEMBER that the frame latent number inputted into the model in every iteration, e.g., base frame latent number (e.g., (97-1)//4+1=25 for base_num_frames=97) and (e.g., (237-97-(97-17)x1+17-1)//4+1=20 for base_num_frames=97, num_frames=237, overlap_history=17) for the last iteration, MUST be divided by causal_block_size. If you find it too hard to calculate and set proper values, just use our recommended setting above :). Asynchronous inference will take more steps to diffuse the whole sequence which means it will be SLOWER than synchronous mode. In our experiments, asynchronous inference may improve the instruction following and visual consistent performance.&lt;/li&gt; 
  &lt;li&gt;To reduce peak VRAM, just lower the &lt;code&gt;--base_num_frames&lt;/code&gt;, e.g., to 77 or 57, while keeping the same generative length &lt;code&gt;--num_frames&lt;/code&gt; you want to generate. This may slightly reduce video quality, and it should not be set too small.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--addnoise_condition&lt;/code&gt; is used to help smooth the long video generation by adding some noise to the clean condition. Too large noise can cause the inconsistency as well. 20 is a recommended value, and you may try larger ones, but it is recommended to not exceed 50.&lt;/li&gt; 
  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 51.2GB peak VRAM.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span id="ve"&gt;Video Extention&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# video extention
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 120 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --video_path ${video_path}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When performing video extension, you need to pass the &lt;code&gt;--video_path ${video_path}&lt;/code&gt; parameter to specify the video to be extended.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span id="se"&gt;Start/End Frame Control&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# start/end frame control
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 97 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --image ${image} \
  --end_image ${end_image}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When controlling the start and end frames, you need to pass the &lt;code&gt;--image ${image}&lt;/code&gt; parameter to control the generation of the start frame and the &lt;code&gt;--end_image ${end_image}&lt;/code&gt; parameter to control the generation of the end frame.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Video extension with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingVideoToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_video

model_id = "Skywork/SkyReels-V2-DF-14B-540P-Diffusers"
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingVideoToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to("cuda")

video = load_video("input_video.mp4")

prompt = "CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird's feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective."

output = pipeline(
    video=video, prompt=prompt, height=544, width=960, guidance_scale=5.0,
    num_inference_steps=30, num_frames=257, base_num_frames=97#, ar_step=5, causal_block_size=5,
).frames[0]
export_to_video(output, "output.mp4", fps=24, quality=8)
# Total frames will be the number of frames of given video + 257
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# run Text-to-Video Generation
model_id=Skywork/SkyReels-V2-T2V-14B-540P
python3 generate_video.py \
  --model_id ${model_id} \
  --resolution 540P \
  --num_frames 97 \
  --guidance_scale 6.0 \
  --shift 8.0 \
  --fps 24 \
  --prompt "A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface." \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; 
  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 43.4GB peak VRAM.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;T2V models with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from diffusers import (
    SkyReelsV2Pipeline,
    UniPCMultistepScheduler,
    AutoencoderKLWan,
)
from diffusers.utils import export_to_video

# Load the pipeline
# Available models:
# - Skywork/SkyReels-V2-T2V-14B-540P-Diffusers
# - Skywork/SkyReels-V2-T2V-14B-720P-Diffusers
vae = AutoencoderKLWan.from_pretrained(
    "Skywork/SkyReels-V2-T2V-14B-720P-Diffusers",
    subfolder="vae",
    torch_dtype=torch.float32,
)
pipe = SkyReelsV2Pipeline.from_pretrained(
    "Skywork/SkyReels-V2-T2V-14B-720P-Diffusers",
    vae=vae,
    torch_dtype=torch.bfloat16,
)
flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
pipe = pipe.to("cuda")

prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."

output = pipe(
    prompt=prompt,
    num_inference_steps=50,
    height=544,
    width=960,
    guidance_scale=6.0,  # 6.0 for T2V, 5.0 for I2V
    num_frames=97,
).frames[0]
export_to_video(output, "video.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;I2V models with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from diffusers import (
    SkyReelsV2ImageToVideoPipeline,
    UniPCMultistepScheduler,
    AutoencoderKLWan,
)
from diffusers.utils import export_to_video
from PIL import Image

# Load the pipeline
# Available models:
# - Skywork/SkyReels-V2-I2V-1.3B-540P-Diffusers
# - Skywork/SkyReels-V2-I2V-14B-540P-Diffusers
# - Skywork/SkyReels-V2-I2V-14B-720P-Diffusers
vae = AutoencoderKLWan.from_pretrained(
    "Skywork/SkyReels-V2-I2V-14B-720P-Diffusers",
    subfolder="vae",
    torch_dtype=torch.float32,
)
pipe = SkyReelsV2ImageToVideoPipeline.from_pretrained(
    "Skywork/SkyReels-V2-I2V-14B-720P-Diffusers",
    vae=vae,
    torch_dtype=torch.bfloat16,
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
pipe = pipe.to("cuda")

prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."
image = Image.open("path/to/image.png")

output = pipe(
    image=image,
    prompt=prompt,
    num_inference_steps=50,
    height=544,
    width=960,
    guidance_scale=5.0,  # 6.0 for T2V, 5.0 for I2V
    num_frames=97,
).frames[0]
export_to_video(output, "video.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt Enhancer&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The prompt enhancer is implemented based on &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B-Instruct&lt;/a&gt; and is utilized via the &lt;code&gt;--prompt_enhancer&lt;/code&gt; parameter. It works ideally for short prompts, while for long prompts, it might generate an excessively lengthy prompt that could lead to over-saturation in the generative video. Note the peak memory of GPU is 64G+ if you use &lt;code&gt;--prompt_enhancer&lt;/code&gt;. If you want to obtain the enhanced prompt separately, you can also run the prompt_enhancer script separately for testing. The steps are as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd skyreels_v2_infer/pipelines
python3 prompt_enhancer.py --prompt "A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface."
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;--prompt_enhancer&lt;/code&gt; is not allowed if using &lt;code&gt;--use_usp&lt;/code&gt;. We recommend running the skyreels_v2_infer/pipelines/prompt_enhancer.py script first to generate enhanced prompt before enabling the &lt;code&gt;--use_usp&lt;/code&gt; parameter.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Advanced Configuration Options&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Below are the key parameters you can customize for video generation:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Parameter&lt;/th&gt; 
   &lt;th align="center"&gt;Recommended Value&lt;/th&gt; 
   &lt;th align="center"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--prompt&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Text description for generating your video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--image&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Path to input image for image-to-video generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--resolution&lt;/td&gt; 
   &lt;td align="center"&gt;540P or 720P&lt;/td&gt; 
   &lt;td align="center"&gt;Output video resolution (select based on model type)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--num_frames&lt;/td&gt; 
   &lt;td align="center"&gt;97 or 121&lt;/td&gt; 
   &lt;td align="center"&gt;Total frames to generate (&lt;strong&gt;97 for 540P models&lt;/strong&gt;, &lt;strong&gt;121 for 720P models&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--inference_steps&lt;/td&gt; 
   &lt;td align="center"&gt;50&lt;/td&gt; 
   &lt;td align="center"&gt;Number of denoising steps&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--fps&lt;/td&gt; 
   &lt;td align="center"&gt;24&lt;/td&gt; 
   &lt;td align="center"&gt;Frames per second in the output video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--shift&lt;/td&gt; 
   &lt;td align="center"&gt;8.0 or 5.0&lt;/td&gt; 
   &lt;td align="center"&gt;Flow matching scheduler parameter (&lt;strong&gt;8.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--guidance_scale&lt;/td&gt; 
   &lt;td align="center"&gt;6.0 or 5.0&lt;/td&gt; 
   &lt;td align="center"&gt;Controls text adherence strength (&lt;strong&gt;6.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--seed&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Fixed seed for reproducible results (omit for random generation)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--offload&lt;/td&gt; 
   &lt;td align="center"&gt;True&lt;/td&gt; 
   &lt;td align="center"&gt;Offloads model components to CPU to reduce VRAM usage (recommended)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--use_usp&lt;/td&gt; 
   &lt;td align="center"&gt;True&lt;/td&gt; 
   &lt;td align="center"&gt;Enables multi-GPU acceleration with xDiT USP&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--outdir&lt;/td&gt; 
   &lt;td align="center"&gt;./video_out&lt;/td&gt; 
   &lt;td align="center"&gt;Directory where generated videos will be saved&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--prompt_enhancer&lt;/td&gt; 
   &lt;td align="center"&gt;True&lt;/td&gt; 
   &lt;td align="center"&gt;Expand the prompt into a more detailed description&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--teacache&lt;/td&gt; 
   &lt;td align="center"&gt;False&lt;/td&gt; 
   &lt;td align="center"&gt;Enables teacache for faster inference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--teacache_thresh&lt;/td&gt; 
   &lt;td align="center"&gt;0.2&lt;/td&gt; 
   &lt;td align="center"&gt;Higher speedup will cause to worse quality&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--use_ret_steps&lt;/td&gt; 
   &lt;td align="center"&gt;False&lt;/td&gt; 
   &lt;td align="center"&gt;Retention Steps for teacache&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Diffusion Forcing Additional Parameters&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Parameter&lt;/th&gt; 
   &lt;th align="center"&gt;Recommended Value&lt;/th&gt; 
   &lt;th align="center"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--ar_step&lt;/td&gt; 
   &lt;td align="center"&gt;0&lt;/td&gt; 
   &lt;td align="center"&gt;Controls asynchronous inference (0 for synchronous mode)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--base_num_frames&lt;/td&gt; 
   &lt;td align="center"&gt;97 or 121&lt;/td&gt; 
   &lt;td align="center"&gt;Base frame count (&lt;strong&gt;97 for 540P&lt;/strong&gt;, &lt;strong&gt;121 for 720P&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--overlap_history&lt;/td&gt; 
   &lt;td align="center"&gt;17&lt;/td&gt; 
   &lt;td align="center"&gt;Number of frames to overlap for smooth transitions in long videos&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--addnoise_condition&lt;/td&gt; 
   &lt;td align="center"&gt;20&lt;/td&gt; 
   &lt;td align="center"&gt;Improves consistency in long video generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--causal_block_size&lt;/td&gt; 
   &lt;td align="center"&gt;5&lt;/td&gt; 
   &lt;td align="center"&gt;Recommended when using asynchronous inference (--ar_step &amp;gt; 0)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--video_path&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Path to input video for video extension&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--end_image&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Path to input image for end frame control&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Multi-GPU inference using xDiT USP&lt;/h4&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/xdit-project/xDiT"&gt;xDiT&lt;/a&gt; USP to accelerate inference. For example, to generate a video with 2 GPUs, you can use the following command:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# diffusion forcing synchronous inference
torchrun --nproc_per_node=2 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 257 \
  --overlap_history 17 \
  --prompt "A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed." \
  --addnoise_condition 20 \
  --use_usp \
  --offload \
  --seed 42
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# run Text-to-Video Generation
model_id=Skywork/SkyReels-V2-T2V-14B-540P
torchrun --nproc_per_node=2 generate_video.py \
  --model_id ${model_id} \
  --resolution 540P \
  --num_frames 97 \
  --guidance_scale 6.0 \
  --shift 8.0 \
  --fps 24 \
  --offload \
  --prompt "A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface." \
  --use_usp \
  --seed 42
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#abstract"&gt;Abstract&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#methodology-of-skyreels-v2"&gt;Methodology of SkyReels-V2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#key-contributions-of-skyreels-v2"&gt;Key Contributions of SkyReels-V2&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#video-captioner"&gt;Video Captioner&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#diffusion-forcing"&gt;Diffusion Forcing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#high-quality-supervised-fine-tuning-sft"&gt;High-Quality Supervised Fine-Tuning(SFT)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#performance"&gt;Performance&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Abstract&lt;/h2&gt; 
&lt;p&gt;Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation.&lt;/p&gt; 
&lt;p&gt;To address these limitations, we introduce SkyReels-V2, the world's first infinite-length film generative model using a Diffusion Forcing framework. Our approach synergizes Multi-modal Large Language Models (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing techniques to achieve comprehensive optimization. Beyond its technical innovations, SkyReels-V2 enables multiple practical applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and multi-subject consistent video generation through our &lt;a href="https://github.com/SkyworkAI/SkyReels-A2"&gt;Skyreels-A2&lt;/a&gt; system.&lt;/p&gt; 
&lt;h2&gt;Methodology of SkyReels-V2&lt;/h2&gt; 
&lt;p&gt;The SkyReels-V2 methodology consists of several interconnected components. It starts with a comprehensive data processing pipeline that prepares various quality training data. At its core is the Video Captioner architecture, which provides detailed annotations for video content. The system employs a multi-task pretraining strategy to build fundamental video generation capabilities. Post-training optimization includes Reinforcement Learning to enhance motion quality, Diffusion Forcing Training for generating extended videos, and High-quality Supervised Fine-Tuning (SFT) stages for visual refinement. The model runs on optimized computational infrastructure for efficient training and inference. SkyReels-V2 supports multiple applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and Elements-to-Video Generation.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/main_pipeline.jpg" alt="mainpipeline" width="100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;Key Contributions of SkyReels-V2&lt;/h2&gt; 
&lt;h4&gt;Video Captioner&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; serves as our video captioning model for data annotation. This model is trained on the captioning result from the base model &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Instruct&lt;/a&gt; and the sub-expert captioners on a balanced video data. The balanced video data is a carefully curated dataset of approximately 2 million videos to ensure conceptual balance and annotation quality. Built upon the &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Qwen2.5-VL-7B-Instruct&lt;/a&gt; foundation model, &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; is fine-tuned to enhance performance in domain-specific video captioning tasks. To compare the performance with the SOTA models, we conducted a manual assessment of accuracy across different captioning fields using a test set of 1,000 samples. The proposed &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; achieves the highest average accuracy among the baseline models, and show a dramatic result in the shot related fields&lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Qwen2.5-VL-7B-Ins.&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Ins.&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/omni-research/Tarsier2-Recap-7b"&gt;Tarsier2-Recap-7b&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Avg accuracy&lt;/td&gt; 
   &lt;td&gt;51.4%&lt;/td&gt; 
   &lt;td&gt;58.7%&lt;/td&gt; 
   &lt;td&gt;49.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;76.3%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;shot type&lt;/td&gt; 
   &lt;td&gt;76.8%&lt;/td&gt; 
   &lt;td&gt;82.5%&lt;/td&gt; 
   &lt;td&gt;60.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;93.7%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;shot angle&lt;/td&gt; 
   &lt;td&gt;60.0%&lt;/td&gt; 
   &lt;td&gt;73.7%&lt;/td&gt; 
   &lt;td&gt;52.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;89.8%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;shot position&lt;/td&gt; 
   &lt;td&gt;28.4%&lt;/td&gt; 
   &lt;td&gt;32.7%&lt;/td&gt; 
   &lt;td&gt;23.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.1%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;camera motion&lt;/td&gt; 
   &lt;td&gt;62.0%&lt;/td&gt; 
   &lt;td&gt;61.2%&lt;/td&gt; 
   &lt;td&gt;45.3%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;85.3%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;expression&lt;/td&gt; 
   &lt;td&gt;43.6%&lt;/td&gt; 
   &lt;td&gt;51.5%&lt;/td&gt; 
   &lt;td&gt;54.3%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;68.8%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan="5" style="text-align: center; border-bottom: 1px solid #ddd; padding: 8px;"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TYPES_type&lt;/td&gt; 
   &lt;td&gt;43.5%&lt;/td&gt; 
   &lt;td&gt;49.7%&lt;/td&gt; 
   &lt;td&gt;47.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;82.5%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TYPES_sub_type&lt;/td&gt; 
   &lt;td&gt;38.9%&lt;/td&gt; 
   &lt;td&gt;44.9%&lt;/td&gt; 
   &lt;td&gt;45.9%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;75.4%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;appearance&lt;/td&gt; 
   &lt;td&gt;40.9%&lt;/td&gt; 
   &lt;td&gt;52.0%&lt;/td&gt; 
   &lt;td&gt;45.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.3%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;action&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;52.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;69.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;68.8%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;position&lt;/td&gt; 
   &lt;td&gt;35.4%&lt;/td&gt; 
   &lt;td&gt;48.6%&lt;/td&gt; 
   &lt;td&gt;45.5%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.5%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;is_main_subject&lt;/td&gt; 
   &lt;td&gt;58.5%&lt;/td&gt; 
   &lt;td&gt;68.7%&lt;/td&gt; 
   &lt;td&gt;69.7%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;80.9%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;environment&lt;/td&gt; 
   &lt;td&gt;70.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;72.7%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;61.4%&lt;/td&gt; 
   &lt;td&gt;70.5%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lighting&lt;/td&gt; 
   &lt;td&gt;77.1%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;80.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;21.2%&lt;/td&gt; 
   &lt;td&gt;76.5%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Reinforcement Learning&lt;/h4&gt; 
&lt;p&gt;Inspired by the previous success in LLM, we propose to enhance the performance of the generative model by Reinforcement Learning. Specifically, we focus on the motion quality because we find that the main drawback of our generative model is:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the generative model does not handle well with large, deformable motions.&lt;/li&gt; 
 &lt;li&gt;the generated videos may violate the physical law.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To avoid the degradation in other metrics, such as text alignment and video quality, we ensure the preference data pairs have comparable text alignment and video quality, while only the motion quality varies. This requirement poses greater challenges in obtaining preference annotations due to the inherently higher costs of human annotation. To address this challenge, we propose a semi-automatic pipeline that strategically combines automatically generated motion pairs and human annotation results. This hybrid approach not only enhances the data scale but also improves alignment with human preferences through curated quality control. Leveraging this enhanced dataset, we first train a specialized reward model to capture the generic motion quality differences between paired samples. This learned reward function subsequently guides the sample selection process for Direct Preference Optimization (DPO), enhancing the motion quality of the generative model.&lt;/p&gt; 
&lt;h4&gt;Diffusion Forcing&lt;/h4&gt; 
&lt;p&gt;We introduce the Diffusion Forcing Transformer to unlock our modelâ€™s ability to generate long videos. Diffusion Forcing is a training and sampling strategy where each token is assigned an independent noise level. This allows tokens to be denoised according to arbitrary, per-token schedules. Conceptually, this approach functions as a form of partial masking: a token with zero noise is fully unmasked, while complete noise fully masks it. Diffusion Forcing trains the model to "unmask" any combination of variably noised tokens, using the cleaner tokens as conditional information to guide the recovery of noisy ones. Building on this, our Diffusion Forcing Transformer can extend video generation indefinitely based on the last frames of the previous segment. Note that the synchronous full sequence diffusion is a special case of Diffusion Forcing, where all tokens share the same noise level. This relationship allows us to fine-tune the Diffusion Forcing Transformer from a full-sequence diffusion model.&lt;/p&gt; 
&lt;h4&gt;High-Quality Supervised Fine-Tuning (SFT)&lt;/h4&gt; 
&lt;p&gt;We implement two sequential high-quality supervised fine-tuning (SFT) stages at 540p and 720p resolutions respectively, with the initial SFT phase conducted immediately after pretraining but prior to reinforcement learning (RL) stage.This first-stage SFT serves as a conceptual equilibrium trainer, building upon the foundation modelâ€™s pretraining outcomes that utilized only fps24 video data, while strategically removing FPS embedding components to streamline thearchitecture. Trained with the high-quality concept-balanced samples, this phase establishes optimized initialization parameters for subsequent training processes. Following this, we execute a secondary high-resolution SFT at 720p after completing the diffusion forcing stage, incorporating identical loss formulations and the higher-quality concept-balanced datasets by the manually filter. This final refinement phase focuses on resolution increase such that the overall video quality will be further enhanced.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;To comprehensively evaluate our proposed method, we construct the SkyReels-Bench for human assessment and leveraged the open-source &lt;a href="https://github.com/Vchitect/VBench"&gt;V-Bench&lt;/a&gt; for automated evaluation. This allows us to compare our model with the state-of-the-art (SOTA) baselines, including both open-source and proprietary models.&lt;/p&gt; 
&lt;h4&gt;Human Evaluation&lt;/h4&gt; 
&lt;p&gt;For human evaluation, we design SkyReels-Bench with 1,020 text prompts, systematically assessing three dimensions: Instruction Adherence, Motion Quality, Consistency and Visual Quality. This benchmark is designed to evaluate both text-to-video (T2V) and image-to-video (I2V) generation models, providing comprehensive assessment across different generation paradigms. To ensure fairness, all models were evaluated under default settings with consistent resolutions, and no post-generation filtering was applied.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Text To Video Models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;Average&lt;/th&gt; 
   &lt;th&gt;Instruction Adherence&lt;/th&gt; 
   &lt;th&gt;Consistency&lt;/th&gt; 
   &lt;th&gt;Visual Quality&lt;/th&gt; 
   &lt;th&gt;Motion Quality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://runwayml.com/research/introducing-gen-3-alpha"&gt;Runway-Gen3 Alpha&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.53&lt;/td&gt; 
   &lt;td&gt;2.19&lt;/td&gt; 
   &lt;td&gt;2.57&lt;/td&gt; 
   &lt;td&gt;3.23&lt;/td&gt; 
   &lt;td&gt;2.11&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo"&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.82&lt;/td&gt; 
   &lt;td&gt;2.64&lt;/td&gt; 
   &lt;td&gt;2.81&lt;/td&gt; 
   &lt;td&gt;3.20&lt;/td&gt; 
   &lt;td&gt;2.61&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://klingai.com"&gt;Kling-1.6 STD Mode&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.99&lt;/td&gt; 
   &lt;td&gt;2.77&lt;/td&gt; 
   &lt;td&gt;3.05&lt;/td&gt; 
   &lt;td&gt;3.39&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;2.76&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hailuoai.video"&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.0&lt;/td&gt; 
   &lt;td&gt;2.8&lt;/td&gt; 
   &lt;td&gt;3.08&lt;/td&gt; 
   &lt;td&gt;3.29&lt;/td&gt; 
   &lt;td&gt;2.74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.12&lt;/td&gt; 
   &lt;td&gt;2.91&lt;/td&gt; 
   &lt;td&gt;3.31&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.54&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;2.71&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.14&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.15&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.35&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3.34&lt;/td&gt; 
   &lt;td&gt;2.74&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;The evaluation demonstrates that our model achieves significant advancements in &lt;strong&gt;instruction adherence (3.15)&lt;/strong&gt; compared to baseline methods, while maintaining competitive performance in &lt;strong&gt;motion quality (2.74)&lt;/strong&gt; without sacrificing the &lt;strong&gt;consistency (3.35)&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image To Video Models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Average&lt;/th&gt; 
   &lt;th&gt;Instruction Adherence&lt;/th&gt; 
   &lt;th&gt;Consistency&lt;/th&gt; 
   &lt;th&gt;Visual Quality&lt;/th&gt; 
   &lt;th&gt;Motion Quality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo"&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.84&lt;/td&gt; 
   &lt;td&gt;2.97&lt;/td&gt; 
   &lt;td&gt;2.95&lt;/td&gt; 
   &lt;td&gt;2.87&lt;/td&gt; 
   &lt;td&gt;2.56&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.85&lt;/td&gt; 
   &lt;td&gt;3.10&lt;/td&gt; 
   &lt;td&gt;2.81&lt;/td&gt; 
   &lt;td&gt;3.00&lt;/td&gt; 
   &lt;td&gt;2.48&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hailuoai.video"&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.05&lt;/td&gt; 
   &lt;td&gt;3.31&lt;/td&gt; 
   &lt;td&gt;2.58&lt;/td&gt; 
   &lt;td&gt;3.55&lt;/td&gt; 
   &lt;td&gt;2.74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://klingai.com"&gt;Kling-1.6 Pro Mode&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.4&lt;/td&gt; 
   &lt;td&gt;3.56&lt;/td&gt; 
   &lt;td&gt;3.03&lt;/td&gt; 
   &lt;td&gt;3.58&lt;/td&gt; 
   &lt;td&gt;3.41&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://runwayml.com/research/introducing-runway-gen-4"&gt;Runway-Gen4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.39&lt;/td&gt; 
   &lt;td&gt;3.75&lt;/td&gt; 
   &lt;td&gt;3.2&lt;/td&gt; 
   &lt;td&gt;3.4&lt;/td&gt; 
   &lt;td&gt;3.37&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2-DF&lt;/td&gt; 
   &lt;td&gt;3.24&lt;/td&gt; 
   &lt;td&gt;3.64&lt;/td&gt; 
   &lt;td&gt;3.21&lt;/td&gt; 
   &lt;td&gt;3.18&lt;/td&gt; 
   &lt;td&gt;2.93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2-I2V&lt;/td&gt; 
   &lt;td&gt;3.29&lt;/td&gt; 
   &lt;td&gt;3.42&lt;/td&gt; 
   &lt;td&gt;3.18&lt;/td&gt; 
   &lt;td&gt;3.56&lt;/td&gt; 
   &lt;td&gt;3.01&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;Our results demonstrate that both &lt;strong&gt;SkyReels-V2-I2V (3.29)&lt;/strong&gt; and &lt;strong&gt;SkyReels-V2-DF (3.24)&lt;/strong&gt; achieve state-of-the-art performance among open-source models, significantly outperforming HunyuanVideo-13B (2.84) and Wan2.1-14B (2.85) across all quality dimensions. With an average score of 3.29, SkyReels-V2-I2V demonstrates comparable performance to proprietary models Kling-1.6 (3.4) and Runway-Gen4 (3.39).&lt;/p&gt; 
&lt;h4&gt;VBench&lt;/h4&gt; 
&lt;p&gt;To objectively compare SkyReels-V2 Model against other leading open-source Text-To-Video models, we conduct comprehensive evaluations using the public benchmark &lt;a href="https://github.com/Vchitect/VBench"&gt;V-Bench&lt;/a&gt;. Our evaluation specifically leverages the benchmarkâ€™s longer version prompt. For fair comparison with baseline models, we strictly follow their recommended setting for inference.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Total Score&lt;/th&gt; 
   &lt;th&gt;Quality Score&lt;/th&gt; 
   &lt;th&gt;Semantic Score&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora"&gt;OpenSora 2.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;81.5 %&lt;/td&gt; 
   &lt;td&gt;82.1 %&lt;/td&gt; 
   &lt;td&gt;78.2 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/THUDM/CogVideo"&gt;CogVideoX1.5-5B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;80.3 %&lt;/td&gt; 
   &lt;td&gt;80.9 %&lt;/td&gt; 
   &lt;td&gt;77.9 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo"&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;82.7 %&lt;/td&gt; 
   &lt;td&gt;84.4 %&lt;/td&gt; 
   &lt;td&gt;76.2 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;83.7 %&lt;/td&gt; 
   &lt;td&gt;84.2 %&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;81.4 %&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.9 %&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.7 %&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;80.8 %&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;The VBench results demonstrate that SkyReels-V2 outperforms all compared models including HunyuanVideo-13B and Wan2.1-14B, With the highest &lt;strong&gt;total score (83.9%)&lt;/strong&gt; and &lt;strong&gt;quality score (84.7%)&lt;/strong&gt;. In this evaluation, the semantic score is slightly lower than Wan2.1-14B, while we outperform Wan2.1-14B in human evaluations, with the primary gap attributed to V-Benchâ€™s insufficient evaluation of shot-scenario semantic adherence.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to thank the contributors of &lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan 2.1&lt;/a&gt;, &lt;a href="https://github.com/xdit-project/xDiT"&gt;XDit&lt;/a&gt; and &lt;a href="https://qwenlm.github.io/blog/qwen2.5/"&gt;Qwen 2.5&lt;/a&gt; repositories, for their open research and contributions.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{chen2025skyreelsv2infinitelengthfilmgenerative,
      title={SkyReels-V2: Infinite-length Film Generative Model}, 
      author={Guibin Chen and Dixuan Lin and Jiangping Yang and Chunze Lin and Junchen Zhu and Mingyuan Fan and Hao Zhang and Sheng Chen and Zheng Chen and Chengcheng Ma and Weiming Xiong and Wei Wang and Nuo Pang and Kang Kang and Zhiheng Xu and Yuzhe Jin and Yupeng Liang and Yubing Song and Peng Zhao and Boyuan Xu and Di Qiu and Debang Li and Zhengcong Fei and Yang Li and Yahui Zhou},
      year={2025},
      eprint={2504.13074},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.13074}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>