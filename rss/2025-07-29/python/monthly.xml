<rss version="2.0">
  <channel>
    <title>GitHub Python Monthly Trending</title>
    <description>Monthly Trending of Python in GitHub</description>
    <pubDate>Mon, 28 Jul 2025 02:01:18 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads"&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions, provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-Coder</title>
      <link>https://github.com/QwenLM/Qwen3-Coder</link>
      <description>&lt;p&gt;Qwen3-Coder is the code version of Qwen3, the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3_coder.png" width="400"&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-main.jpg" width="800"&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; ğŸ’œ &lt;a href="https://chat.qwenlm.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤— &lt;a href="https://huggingface.co/collections/Qwen/qwen3-coder-687fc861e53c939e52d52d10"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤– &lt;a href="https://modelscope.cn/organization/qwen"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ“‘ &lt;a href="https://qwenlm.github.io/blog/qwen3-coder"&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ï½œ &amp;nbsp;&amp;nbsp;ğŸ“– &lt;a href="https://qwen.readthedocs.io/"&gt;Documentation&lt;/a&gt; &lt;br&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸŒ &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Coder-WebDev"&gt;WebDev&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ’¬ &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (å¾®ä¿¡)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ«¨ &lt;a href="https://discord.gg/CV4E9rpNSD"&gt; Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ“„ &lt;a href="https://arxiv.org/abs/2505.09388"&gt;Arxiv&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ‘½ &lt;a href="https://github.com/QwenLM/qwen-code"&gt;Qwen Code&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-Coder-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; 
&lt;h1&gt;Qwen3-Coder: Agentic Coding in the World.&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Today, we're announcing Qwen3-Coder, our most agentic code model to date. &lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is available in multiple sizes, but we're excited to introduce its most powerful variant first: &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; â€” a 480B-parameter Mixture-of-Experts model with 35B active parameters, offering exceptional performance in both coding and agentic tasks. &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; sets new state-of-the-art results among open models on Agentic Coding, Agentic Browser-Use, and Agentic Tool-Use, comparable to Claude Sonnet.&lt;/p&gt; 
&lt;p&gt;ğŸ’» &lt;strong&gt;Significant Performance&lt;/strong&gt;: among open models on &lt;strong&gt;Agentic Coding&lt;/strong&gt;, &lt;strong&gt;Agentic Browser-Use&lt;/strong&gt;, and other foundational coding tasks, achieving results comparable to Claude Sonnet;&lt;/p&gt; 
&lt;p&gt;ğŸ“š &lt;strong&gt;Long-context Capabilities&lt;/strong&gt;: with native support for &lt;strong&gt;256K&lt;/strong&gt; tokens, extendable up to &lt;strong&gt;1M&lt;/strong&gt; tokens using Yarn, optimized for repository-scale understanding;&lt;/p&gt; 
&lt;p&gt;ğŸ›  &lt;strong&gt;Agentic Coding&lt;/strong&gt;: supporting for most platform such as &lt;strong&gt;Qwen Code&lt;/strong&gt;, &lt;strong&gt;CLINE&lt;/strong&gt;, featuring a specially designed function call format;&lt;/p&gt; 
&lt;h2&gt;Basic information&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;âœ¨ Supporting long context understanding and generation with the context length of 256K tokens;&lt;/li&gt; 
 &lt;li&gt;âœ¨ Supporting 358 coding languages;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;['ABAP', 'ActionScript', 'Ada', 'Agda', 'Alloy', 'ApacheConf', 'AppleScript', 'Arc', 'Arduino', 'AsciiDoc', 'AspectJ', 'Assembly', 'Augeas', 'AutoHotkey', 'AutoIt', 'Awk', 'Batchfile', 'Befunge', 'Bison', 'BitBake', 'BlitzBasic', 'BlitzMax', 'Bluespec', 'Boo', 'Brainfuck', 'Brightscript', 'Bro', 'C', 'C#', 'C++', 'C2hs Haskell', 'CLIPS', 'CMake', 'COBOL', 'CSS', 'CSV', "Cap'n Proto", 'CartoCSS', 'Ceylon', 'Chapel', 'ChucK', 'Cirru', 'Clarion', 'Clean', 'Click', 'Clojure', 'CoffeeScript', 'ColdFusion', 'ColdFusion CFC', 'Common Lisp', 'Component Pascal', 'Coq', 'Creole', 'Crystal', 'Csound', 'Cucumber', 'Cuda', 'Cycript', 'Cython', 'D', 'DIGITAL Command Language', 'DM', 'DNS Zone', 'Darcs Patch', 'Dart', 'Diff', 'Dockerfile', 'Dogescript', 'Dylan', 'E', 'ECL', 'Eagle', 'Ecere Projects', 'Eiffel', 'Elixir', 'Elm', 'Emacs Lisp', 'EmberScript', 'Erlang', 'F#', 'FLUX', 'FORTRAN', 'Factor', 'Fancy', 'Fantom', 'Forth', 'FreeMarker', 'G-code', 'GAMS', 'GAP', 'GAS', 'GDScript', 'GLSL', 'Genshi', 'Gentoo Ebuild', 'Gentoo Eclass', 'Gettext Catalog', 'Glyph', 'Gnuplot', 'Go', 'Golo', 'Gosu', 'Grace', 'Gradle', 'Grammatical Framework', 'GraphQL', 'Graphviz (DOT)', 'Groff', 'Groovy', 'Groovy Server Pages', 'HCL', 'HLSL', 'HTML', 'HTML+Django', 'HTML+EEX', 'HTML+ERB', 'HTML+PHP', 'HTTP', 'Haml', 'Handlebars', 'Harbour', 'Haskell', 'Haxe', 'Hy', 'IDL', 'IGOR Pro', 'INI', 'IRC log', 'Idris', 'Inform 7', 'Inno Setup', 'Io', 'Ioke', 'Isabelle', 'J', 'JFlex', 'JSON', 'JSON5', 'JSONLD', 'JSONiq', 'JSX', 'Jade', 'Jasmin', 'Java', 'Java Server Pages', 'JavaScript', 'Julia', 'Jupyter Notebook', 'KRL', 'KiCad', 'Kit', 'Kotlin', 'LFE', 'LLVM', 'LOLCODE', 'LSL', 'LabVIEW', 'Lasso', 'Latte', 'Lean', 'Less', 'Lex', 'LilyPond', 'Linker Script', 'Liquid', 'Literate Agda', 'Literate CoffeeScript', 'Literate Haskell', 'LiveScript', 'Logos', 'Logtalk', 'LookML', 'Lua', 'M', 'M4', 'MAXScript', 'MTML', 'MUF', 'Makefile', 'Mako', 'Maple', 'Markdown', 'Mask', 'Mathematica', 'Matlab', 'Max', 'MediaWiki', 'Metal', 'MiniD', 'Mirah', 'Modelica', 'Module Management System', 'Monkey', 'MoonScript', 'Myghty', 'NSIS', 'NetLinx', 'NetLogo', 'Nginx', 'Nimrod', 'Ninja', 'Nit', 'Nix', 'Nu', 'NumPy', 'OCaml', 'ObjDump', 'Objective-C++', 'Objective-J', 'Octave', 'Omgrofl', 'Opa', 'Opal', 'OpenCL', 'OpenEdge ABL', 'OpenSCAD', 'Org', 'Ox', 'Oxygene', 'Oz', 'PAWN', 'PHP', 'POV-Ray SDL', 'Pan', 'Papyrus', 'Parrot', 'Parrot Assembly', 'Parrot Internal Representation', 'Pascal', 'Perl', 'Perl6', 'Pickle', 'PigLatin', 'Pike', 'Pod', 'PogoScript', 'Pony', 'PostScript', 'PowerShell', 'Processing', 'Prolog', 'Propeller Spin', 'Protocol Buffer', 'Public Key', 'Pure Data', 'PureBasic', 'PureScript', 'Python', 'Python traceback', 'QML', 'QMake', 'R', 'RAML', 'RDoc', 'REALbasic', 'RHTML', 'RMarkdown', 'Racket', 'Ragel in Ruby Host', 'Raw token data', 'Rebol', 'Red', 'Redcode', "Ren'Py", 'RenderScript', 'RobotFramework', 'Rouge', 'Ruby', 'Rust', 'SAS', 'SCSS', 'SMT', 'SPARQL', 'SQF', 'SQL', 'STON', 'SVG', 'Sage', 'SaltStack', 'Sass', 'Scala', 'Scaml', 'Scheme', 'Scilab', 'Self', 'Shell', 'ShellSession', 'Shen', 'Slash', 'Slim', 'Smali', 'Smalltalk', 'Smarty', 'Solidity', 'SourcePawn', 'Squirrel', 'Stan', 'Standard ML', 'Stata', 'Stylus', 'SuperCollider', 'Swift', 'SystemVerilog', 'TOML', 'TXL', 'Tcl', 'Tcsh', 'TeX', 'Tea', 'Text', 'Textile', 'Thrift', 'Turing', 'Turtle', 'Twig', 'TypeScript', 'Unified Parallel C', 'Unity3D Asset', 'Uno', 'UnrealScript', 'UrWeb', 'VCL', 'VHDL', 'Vala', 'Verilog', 'VimL', 'Visual Basic', 'Volt', 'Vue', 'Web Ontology Language', 'WebAssembly', 'WebIDL', 'X10', 'XC', 'XML', 'XPages', 'XProc', 'XQuery', 'XS', 'XSLT', 'Xojo', 'Xtend', 'YAML', 'YANG', 'Yacc', 'Zephir', 'Zig', 'Zimpl', 'desktop', 'eC', 'edn', 'fish', 'mupad', 'nesC', 'ooc', 'reStructuredText', 'wisp', 'xBase']
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;âœ¨ Retain strengths in math and general capabilities from base model.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important]&lt;/p&gt; 
 &lt;p&gt;Qwen3-coder function calling relies on our new tool parser &lt;code&gt;qwen3coder_tool_parser.py&lt;/code&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct/blob/main/qwen3coder_tool_parser.py"&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;We updated both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen3. Please make sure to use the new tokenizer.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model name&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;length&lt;/th&gt; 
   &lt;th&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;Hugging Face&lt;/a&gt; â€¢ ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct-FP8&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;Hugging Face&lt;/a&gt; â€¢ ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Detailed performance and introduction are shown in this &lt;a href="https://qwenlm.github.io/blog/qwen3-coder"&gt; ğŸ“‘ blog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; are instruction models for chatting;&lt;/p&gt; 
 &lt;p&gt;This model supports only non-thinking mode and does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks in its output. Meanwhile, specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required.**&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ‘‰ğŸ» Chat with Qwen3-Coder-480B-A35B-Instruct&lt;/h3&gt; 
&lt;p&gt;You can just write several lines of code with &lt;code&gt;transformers&lt;/code&gt; to chat with Qwen3-Coder-480B-A35B-Instruct. Essentially, we build the tokenizer and the model with &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use generate method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-Coder-480B-A35B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "write a quick sort algorithm."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=65536
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;apply_chat_template()&lt;/code&gt; function is used to convert the messages into a format that the model can understand. The &lt;code&gt;add_generation_prompt&lt;/code&gt; argument is used to add a generation prompt, which refers to &lt;code&gt;&amp;lt;|im_start|&amp;gt;assistant\n&lt;/code&gt; to the input. Notably, we apply ChatML template for chat models following our previous practice. The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;tokenizer.batch_decode()&lt;/code&gt; function is used to decode the response. In terms of the input, the above messages is an example to show how to format your dialog history and system prompt. You can use the other size of instruct model in the same way.&lt;/p&gt; 
&lt;h4&gt;Fill in the middle with Qwen3-Coder-480B-A35B-Instruct&lt;/h4&gt; 
&lt;p&gt;The code insertion task, also referred to as the "fill-in-the-middle" challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context. For an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper "Efficient Training of Language Models to Fill in the Middle"[&lt;a href="https://arxiv.org/abs/2207.14255"&gt;arxiv&lt;/a&gt;].&lt;/p&gt; 
&lt;p&gt;The prompt should be structured as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;prompt = '&amp;lt;|fim_prefix|&amp;gt;' + prefix_code + '&amp;lt;|fim_suffix|&amp;gt;' + suffix_code + '&amp;lt;|fim_middle|&amp;gt;'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Following the approach mentioned, an example would be structured in this manner:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoTokenizer, AutoModelForCausalLM
# load model
device = "cuda" # the device to load the model onto

TOKENIZER = AutoTokenizer.from_pretrained("Qwen/Qwen3-Coder-480B-A35B-Instruct")
MODEL = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-Coder-480B-A35B-Instruct", device_map="auto").eval()


input_text = """&amp;lt;|fim_prefix|&amp;gt;def quicksort(arr):
    if len(arr) &amp;lt;= 1:
        return arr
    pivot = arr[len(arr) // 2]
    &amp;lt;|fim_suffix|&amp;gt;
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x &amp;gt; pivot]
    return quicksort(left) + middle + quicksort(right)&amp;lt;|fim_middle|&amp;gt;"""
            
messages = [
    {"role": "system", "content": "You are a code completion assistant."},
    {"role": "user", "content": input_text}
]


text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = TOKENIZER([text], return_tensors="pt").to(model.device)

# Use `max_new_tokens` to control the maximum output length.
generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]
# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.
output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)

print(f"Prompt: {input_text}\n\nGenerated text: {output_text}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Cases&lt;/h2&gt; 
&lt;h3&gt;Example: Physics-Based Chimney Demolition Simulation with Controlled Explosion&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;ä½¿ç”¨ three.js, cannon-es.js ç”Ÿæˆä¸€ä¸ªéœ‡æ’¼çš„3Då»ºç­‘æ‹†é™¤æ¼”ç¤ºã€‚

## åœºæ™¯è®¾ç½®ï¼š
- åœ°é¢æ˜¯ä¸€ä¸ªæ·±ç°è‰²æ··å‡åœŸå¹³é¢ï¼Œå°ºå¯¸80*80ï¼Œ
- æ‰€æœ‰ç‰©ä½“ä¸¥æ ¼éµå¾ªç°å®ç‰©ç†è§„åˆ™ï¼ŒåŒ…æ‹¬é‡åŠ›ã€æ‘©æ“¦åŠ›ã€ç¢°æ’æ£€æµ‹å’ŒåŠ¨é‡å®ˆæ’

## å»ºç­‘ç»“æ„ï¼š
- ä¸€åº§åœ†å½¢é«˜å±‚å»ºç­‘ï¼Œå‘¨é•¿å¯¹åº”20ä¸ªæ–¹å—
- å»ºç­‘æ€»é«˜åº¦60ä¸ªæ–¹å—
- æ¯å±‚é‡‡ç”¨ç –ç Œç»“æ„ï¼Œæ–¹å—ä¸ç –ç»“æ„å»ºç­‘ä¸€è‡´, é”™å¼€50%æ’åˆ—ï¼Œå¢å¼ºç»“æ„ç¨³å®šæ€§
- å»ºç­‘å¤–å¢™ä½¿ç”¨ç±³è‰²æ–¹å—
- **é‡è¦ï¼šæ–¹å—åˆå§‹æ’åˆ—æ—¶å¿…é¡»ç¡®ä¿ç´§å¯†è´´åˆï¼Œæ— é—´éš™ï¼Œå¯ä»¥é€šè¿‡è½»å¾®é‡å æˆ–è°ƒæ•´åŠå¾„æ¥å®ç°**
- **é‡è¦ï¼šå»ºç­‘åˆå§‹åŒ–å®Œæˆåï¼Œæ‰€æœ‰æ–¹å—åº”è¯¥å¤„äºç‰©ç†"ç¡çœ "çŠ¶æ€ï¼Œç¡®ä¿å»ºç­‘åœ¨çˆ†ç‚¸å‰ä¿æŒå®Œç¾çš„é™æ­¢çŠ¶æ€ï¼Œä¸ä¼šå› é‡åŠ›è€Œä¸‹æ²‰æˆ–æ¾æ•£**
- å»ºç­‘ç –å—ä¹‹é—´ä½¿ç”¨ç²˜æ€§ææ–™å¡«å……ï¼ˆä¸å¯è§ï¼‰ï¼Œé€šè¿‡é«˜æ‘©æ“¦åŠ›ï¼ˆ0.8+ï¼‰å’Œä½å¼¹æ€§ï¼ˆ0.05ä»¥ä¸‹ï¼‰æ¥æ¨¡æ‹Ÿç²˜åˆæ•ˆæœ
- ç –å—åœ¨å»ºç­‘å€’å¡Œç¬é—´ä¸ä¼šæ•£æ‰ï¼Œè€Œæ˜¯å»ºç­‘ä½œä¸ºä¸€ä¸ªæ•´ä½“å€’åœ¨åœ°é¢çš„æ—¶å€™æ‰å› å—åŠ›è¿‡å¤§è€Œæ•£æ‰

## å®šå‘çˆ†ç ´ç³»ç»Ÿï¼š
- åœ¨å»ºç­‘çš„ç¬¬1å±‚çš„æœ€å³ä¾§æ–¹å—é™„è¿‘å®‰è£…çˆ†ç‚¸è£…ç½®ï¼ˆä¸å¯è§ï¼‰
- æä¾›æ“ä½œæŒ‰é’®ç‚¹å‡»çˆ†ç‚¸
- **çˆ†ç‚¸æ—¶å”¤é†’æ‰€æœ‰ç›¸å…³æ–¹å—çš„ç‰©ç†çŠ¶æ€**
- çˆ†ç‚¸ç‚¹äº§ç”ŸåŠå¾„2çš„å¼ºåŠ›å†²å‡»æ³¢ï¼Œå†²å‡»æ³¢å½±å“åˆ°çš„æ–¹å—, å—åˆ°2-5å•ä½çš„å†²å‡»åŠ›

## å»ºç­‘ç¨³å®šæ€§è¦æ±‚ï¼š
- **ç¡®ä¿å»ºç­‘åœ¨æœªçˆ†ç‚¸æ—¶å®Œå…¨é™æ­¢ï¼Œæ— ä»»ä½•æ™ƒåŠ¨æˆ–ä¸‹æ²‰**
- **ç‰©ç†ä¸–ç•Œåˆå§‹åŒ–åç»™å»ºç­‘å‡ ä¸ªç‰©ç†æ­¥éª¤æ¥è‡ªç„¶ç¨³å®šï¼Œæˆ–ä½¿ç”¨ç¡çœ æœºåˆ¶**
- **æ–¹å—é—´çš„æ¥è§¦ææ–™åº”å…·æœ‰é«˜æ‘©æ“¦åŠ›å’Œæä½å¼¹æ€§ï¼Œæ¨¡æ‹Ÿç –å—é—´çš„ç ‚æµ†ç²˜åˆ**

## éœ‡æ’¼çš„å€’å¡Œæ•ˆæœï¼š
- æ–¹å—åœ¨çˆ†ç‚¸å†²å‡»ä¸‹ä¸ä»…é£æ•£ï¼Œè¿˜ä¼šåœ¨ç©ºä¸­ç¿»æ»šå’Œç¢°æ’
- çƒŸå°˜ä¼šéšç€å»ºç­‘å€’å¡Œé€æ¸æ‰©æ•£ï¼Œè¥é€ çœŸå®çš„æ‹†é™¤ç°åœºæ°›å›´

## å¢å¼ºçš„è§†è§‰æ•ˆæœï¼š
- æ·»åŠ ç¯å¢ƒå…‰ç…§å˜åŒ–ï¼šçˆ†ç‚¸ç¬é—´äº®åº¦æ¿€å¢ï¼Œç„¶åè¢«çƒŸå°˜é®æŒ¡å˜æš—
- ç²’å­ç³»ç»ŸåŒ…æ‹¬ï¼šçƒŸé›¾ã€ç°å°˜

## æŠ€æœ¯è¦æ±‚ï¼š
- ç²’å­ç³»ç»Ÿç”¨äºçƒŸé›¾å’Œç°å°˜æ•ˆæœ
- æ‰€æœ‰ä»£ç é›†æˆåœ¨å•ä¸ªHTMLæ–‡ä»¶ä¸­ï¼ŒåŒ…å«å¿…è¦çš„CSSæ ·å¼
- æ·»åŠ ç®€å•çš„UIæ§åˆ¶ï¼šé‡ç½®æŒ‰é’®ã€ç›¸æœºè§’åº¦åˆ‡æ¢, çˆ†ç‚¸æŒ‰é’®, é¼ æ ‡å·¦é”®æ§åˆ¶æ‘„åƒæœºè§’åº¦ï¼Œå³é”®æ§åˆ¶æ‘„åƒæœºä½ç½®ï¼Œæ»šè½®æ§åˆ¶æ‘„åƒæœºç„¦è·
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo1.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example1.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Multicolor and Interactive Animation&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create an amazing animation multicolor and interactive using p5js

use this cdn:
https://cdn.jsdelivr.net/npm/p5@1.7.0/lib/p5.min.js
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo2.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example2.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: 3D Google Earth&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;To create a 3D Google Earth, you need to load the terrain map correctly. You can use any online resource. The code is written into an HTML file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo3.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example3.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Testing Your WPM with a Famous Quote&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Qwen-Code CLI &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create an interesting typing game with a keyboard in the lower middle of the screen and some famous articles in the upper middle. When the user types a word correctly, a cool reaction should be given to encourage him. Design a modern soft color scheme inspired by macarons. Come up with a very creative solution first, and then start writing code.
The game should be able to support typing, and you need to neglect upcase and lowercase.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo4.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example4.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Bouncing Ball in Rotation Hypercube&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Make a page in HTML that shows an animation of a ball bouncing in a rotating hypercube
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo5.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example5.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Solar System Simulation&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;write a web page to show the solar system simulation
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo6.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example6.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: DUET Game&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create a complete, single-file HTML game with CSS and JavaScript. The game is inspired by "Duet".

Gameplay:

There are two balls, one red and one blue, rotating around a central point.
The player uses the 'A' and 'D' keys to rotate them counter-clockwise and clockwise.
White rectangular obstacles move down from the top of the screen.
The player must rotate the balls to avoid hitting the obstacles.
If a ball hits an obstacle, the game is over.
Visuals:

Make the visual effects amazing.
Use a dark background with neon glowing effects for the balls and obstacles.
Animations should be very smooth.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo7.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example7.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#QwenLM/Qwen3-Coder&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=QwenLM/Qwen3-Coder&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={Qwen Team},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388},
}
@article{hui2024qwen2,
  title={Qwen2. 5-Coder Technical Report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt; 
&lt;p align="right" style="font-size: 14px; color: #555; margin-top: 20px;"&gt; &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#readme-top" style="text-decoration: none; color: #007bff; font-weight: bold;"&gt; â†‘ Back to Top â†‘ &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Alibaba-NLP/WebAgent</title>
      <link>https://github.com/Alibaba-NLP/WebAgent</link>
      <description>&lt;p&gt;ğŸŒ WebAgent for Information Seeking built by Tongyi Lab: WebWalker &amp; WebDancer &amp; WebSailor &amp; WebShaper https://arxiv.org/abs/2507.15061 https://arxiv.org/pdf/2507.02592&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;WebAgent for Information Seeking built by Tongyi Lab, Alibaba Group &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png" width="30px" style="display:inline;"&gt;&lt;/h2&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14217" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14217" alt="Alibaba-NLP%2FWebAgent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; ğŸ¤— &lt;a href="https://huggingface.co/datasets/Alibaba-NLP/WebShaper" target="_blank"&gt;WebShaperQA&lt;/a&gt; ï½œ &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png" width="14px" style="display:inline;"&gt; &lt;a href="https://modelscope.cn/datasets/iic/WebShaper" target="_blank"&gt;WebShaperQA&lt;/a&gt; ï½œ ğŸ¤— &lt;a href="https://huggingface.co/Alibaba-NLP/WebSailor-3B" target="_blank"&gt;WebSailor-3B&lt;/a&gt; ï½œ &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png" width="14px" style="display:inline;"&gt; &lt;a href="https://modelscope.cn/models/iic/WebSailor-3B" target="_blank"&gt;ModelScope WebSailor-3B&lt;/a&gt; | &lt;/p&gt; 
&lt;p align="center"&gt; ğŸ¤— &lt;a href="https://huggingface.co/Alibaba-NLP/WebDancer-32B" target="_blank"&gt;WebDancer-QwQ-32B&lt;/a&gt; | &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/tongyi.png" width="14px" style="display:inline;"&gt; &lt;a href="https://modelscope.cn/models/iic/WebDancer-32B" target="_blank"&gt;ModelScope WebDancer-QwQ-32B&lt;/a&gt; | ğŸ¤— &lt;a href="https://huggingface.co/datasets/callanwu/WebWalkerQA" target="_blank"&gt;WebWalkerQA&lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/roadmap.png" width="100%" height="400%"&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can check the paper of &lt;a href="https://arxiv.org/pdf/2505.22648"&gt;WebDancer&lt;/a&gt; and &lt;a href="https://arxiv.org/pdf/2501.07572"&gt;WebWalker&lt;/a&gt; and &lt;a href="https://arxiv.org/pdf/2507.02592"&gt;WebSailor&lt;/a&gt; and &lt;a href="https://arxiv.org/pdf/2507.15061"&gt;WebShaper&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ’¥ ğŸ’¥ ğŸ’¥ Stay tuned for more updates! We are working on building native agentic model based on the Browser and more open-domain environments!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebShaper"&gt;&lt;strong&gt;WebShaper&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebSailor"&gt;&lt;strong&gt;WebSailor&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer"&gt;&lt;strong&gt;WebDancer&lt;/strong&gt;&lt;/a&gt; (Preprint 2025) - WebDancer: Towards Autonomous Information Seeking Agency&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebWalker"&gt;&lt;strong&gt;WebWalker&lt;/strong&gt;&lt;/a&gt; (ACL 2025) - WebWalker: Benchmarking LLMs in Web Traversal&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“° News and Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;2025.07.22&lt;/code&gt; ğŸ”¥ğŸ”¥ğŸ”¥We release &lt;strong&gt;WebShaper&lt;/strong&gt;: Agentically Data Synthesizing via Information-Seeking Formalization.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025.07.11&lt;/code&gt; ğŸ”¥ğŸ”¥ğŸ”¥&lt;strong&gt;WebSailor-3B&lt;/strong&gt; is &lt;a href="https://huggingface.co/Alibaba-NLP/WebSailor-3B"&gt;released&lt;/a&gt;. You can deploy it with one click using &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/aliyun.png" width="14px" style="display:inline;"&gt; &lt;a href="https://functionai.console.aliyun.com/template-detail?template=Alibaba-NLP-WebSailor-3B"&gt;Alibaba Cloud's FunctionAI&lt;/a&gt; in ten minutes!&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025.07.03&lt;/code&gt; ğŸ”¥ğŸ”¥ğŸ”¥We release &lt;strong&gt;WebSailor&lt;/strong&gt;, an agentic search model specialized in performing extremely complex information seeking tasks, achieving open-source SOTA on some of the most difficult browsing benchmarks. &lt;strong&gt;WebSailor&lt;/strong&gt; topped the HuggingFace &lt;a href="https://huggingface.co/papers/2507.02592"&gt;daily papers&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025.06.23&lt;/code&gt; ğŸ”¥ğŸ”¥ğŸ”¥The model, interactive demo, and some of the data of &lt;strong&gt;WebDancer&lt;/strong&gt; have been open-sourced. You're welcome to try them out!&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025.05.29&lt;/code&gt; ğŸ”¥ğŸ”¥ğŸ”¥We release &lt;strong&gt;WebDancer&lt;/strong&gt;, a native agentic search model towards autonomous information seeking agency and &lt;em&gt;Deep Research&lt;/em&gt;-like model.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025.05.15&lt;/code&gt; &lt;strong&gt;WebWalker&lt;/strong&gt; is accepted by ACL 2025 main conference.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025.01.14&lt;/code&gt; We release &lt;strong&gt;WebWalker&lt;/strong&gt;, a benchmark for LLMs in web traversal and a multi-agent framework for information seeking.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ’ Results Showcase&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/webagent-gaia.png" width="800%" height="400%"&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/assets/webagent-bc.png" width="800%" height="400%"&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ’¡ Features for WebShaper&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A &lt;strong&gt;&lt;code&gt;formalization-driven&lt;/code&gt;&lt;/strong&gt; data synthesis method for information-seeking agents, grounded in our proposed task formalization. Leveraging this method, we construct the &lt;strong&gt;WebShaper&lt;/strong&gt; dataset, which enables systematic generation of IS instances.&lt;/li&gt; 
 &lt;li&gt;We propose an agentic Expander that iteratively generates and validates questions in alignment with the formalization.&lt;/li&gt; 
 &lt;li&gt;We conduct extensive experiments across multiple benchmarks to evaluate the effectiveness of WebShaper. We achieve new state-of-the-art results on &lt;strong&gt;GAIA&lt;/strong&gt; (&lt;strong&gt;60.19&lt;/strong&gt;) and &lt;strong&gt;WebWalkerQA&lt;/strong&gt; (&lt;strong&gt;52.50&lt;/strong&gt;) benchmarks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â›µï¸ Features for WebSailor&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A complete post-training methodology enabling models to engage in extended thinking and information seeking, ultimately allowing them to successfully complete extremely complex tasks previously considered unsolvable.&lt;/li&gt; 
 &lt;li&gt;Introduces &lt;strong&gt;SailorFog-QA&lt;/strong&gt;, a scalable QA benchmark with high uncertainty and difficulty, curated with a novel data synthesis method through graph sampling and information obfuscation. Example SailorFog-QA data samples can be found at: &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebSailor/dataset/sailorfog-QA.jsonl"&gt;&lt;code&gt;WebSailor/dataset/sailorfog-QA.jsonl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Effective post-training pipeline consisting of (1) high-quality reconstruction of concise reasoning from expert trajectories for clean supervision, (2) a two-stage training process involving an RFT cold start stage, followed by &lt;strong&gt;Duplicating Sampling Policy Optimization (DUPO)&lt;/strong&gt;, an efficient agentic RL algorithm excelling in effectiveness and efficiency.&lt;/li&gt; 
 &lt;li&gt;WebSailor-72B significantly outperforms all open-source agents and frameworks while closing the performance gap with leading proprietary systems, achieving a score of &lt;strong&gt;12.0%&lt;/strong&gt; on BrowseComp-en, &lt;strong&gt;30.1%&lt;/strong&gt; on BrowseComp-zh, and &lt;strong&gt;55.4%&lt;/strong&gt; on GAIA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;The checkpoint is coming soon.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸŒ Features for WebDancer&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Native agentic search reasoning model using ReAct framework towards autonomous information seeking agency and &lt;em&gt;Deep Research&lt;/em&gt;-like model.&lt;/li&gt; 
 &lt;li&gt;We introduce a four-stage training paradigm comprising &lt;strong&gt;browsing data construction, trajectory sampling, supervised fine-tuning for effective cold start, and reinforcement learning for improved generalization&lt;/strong&gt;, enabling the agent to autonomously acquire autonomous search and reasoning skills.&lt;/li&gt; 
 &lt;li&gt;Our data-centric approach integrates trajectory-level supervision fine-tuning and reinforcement learning (DAPO) to develop a scalable pipeline for &lt;strong&gt;training agentic systems&lt;/strong&gt; via SFT or RL.&lt;/li&gt; 
 &lt;li&gt;WebDancer achieves a Pass@3 score of 64.1% on GAIA and 62.0% on WebWalkerQA.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;p&gt;You need to enter the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer"&gt;&lt;code&gt;WebDancer&lt;/code&gt;&lt;/a&gt; folder for the following commands.&lt;/p&gt; 
&lt;h3&gt;Step 0: Set Up the Environment&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n webdancer python=3.12
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 1: Deploy the Model&lt;/h3&gt; 
&lt;p&gt;Download the WebDancer model from &lt;a href="https://huggingface.co/Alibaba-NLP/WebDancer-32B"&gt;ğŸ¤— HuggingFace&lt;/a&gt; and deploy it using the provided scripts with &lt;a href="https://github.com/sgl-project/sglang"&gt;sglang&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd scripts
bash deploy_model.sh WebDancer_PATH
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Replace &lt;code&gt;WebDancer_PATH&lt;/code&gt; with the actual path to the downloaded model.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Step 2: Run the Demo&lt;/h3&gt; 
&lt;p&gt;Edit the following keys in &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/WebDancer/scripts/run_demo.sh"&gt;&lt;code&gt;WebDancer/scripts/run_demo.sh&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GOOGLE_SEARCH_KEY&lt;/code&gt;, you can get it from &lt;a href="https://serper.dev/"&gt;serper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;JINA_API_KEY&lt;/code&gt;, you can get it from &lt;a href="https://jina.ai/api-dashboard/"&gt;jina&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DASHSCOPE_API_KEY&lt;/code&gt;, you can get it from &lt;a href="https://dashscope.aliyun.com/"&gt;dashscope&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Then, launch the demo with Gradio to interact with the WebDancer model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd scripts
bash run_demo.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¥ WebSailor Demos&lt;/h2&gt; 
&lt;p&gt;We provide demos for BrowseComp-en, BrowseComp-zh and Daily Use. Our model can complete highly difficult and uncertain tasks requiring massive information acquisition and complex reasoning.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;BrowseComp-en&lt;/h3&gt; 
 &lt;video src="https://github.com/user-attachments/assets/2dc0b03a-c241-4f70-bf11-92fda28020fa"&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;BrowseComp-zh&lt;/h3&gt; 
 &lt;video src="https://github.com/user-attachments/assets/f9aed746-ffc8-4b76-b135-715ec0eab544"&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;Daily Use&lt;/h3&gt; 
 &lt;video src="https://github.com/user-attachments/assets/1299c5a8-cee3-4a70-b68b-c5d227cf8055"&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ¥ WebDancer Demos&lt;/h2&gt; 
&lt;p&gt;We provide demos for WebWalkerQA, GAIA and Daily Use. Our model can execute the long-horizon tasks with &lt;strong&gt;multiple steps&lt;/strong&gt; and &lt;strong&gt;complex reasoning&lt;/strong&gt;, such as web traversal, information seeking and question answering.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;WebWalkerQA&lt;/h3&gt; 
 &lt;video src="https://github.com/user-attachments/assets/0bbaf55b-897e-4c57-967d-a6e8bbd2167e"&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;GAIA&lt;/h3&gt; 
 &lt;video src="https://github.com/user-attachments/assets/935c668e-6169-4712-9c04-ac80f0531872"&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;Daily Use&lt;/h3&gt; 
 &lt;video src="https://github.com/user-attachments/assets/d1d5b533-4009-478b-bd87-96b86389327d"&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“ƒ License&lt;/h2&gt; 
&lt;p&gt;The content of this project itself is licensed under &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/WebAgent/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸš© Citation&lt;/h2&gt; 
&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bigquery"&gt;@misc{tao2025webshaper,
      title={WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization},
      author={Zhengwei Tao and Jialong Wu and Wenbiao Yin and Junkai Zhang and Baixuan Li and Haiyang Shen and Kuan Li and Liwen Zhang and Xinyu Wang and Yong Jiang and Pengjun Xie and Fei Huang and Jingren Zhou},
      year={2025},
      eprint={2507.15061},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.15061},
}
@misc{li2025websailor,
      title={WebSailor: Navigating Super-human Reasoning for Web Agent},
      author={Kuan Li and Zhongwang Zhang and Huifeng Yin and Liwen Zhang and Litu Ou and Jialong Wu and Wenbiao Yin and Baixuan Li and Zhengwei Tao and Xinyu Wang and Weizhou Shen and Junkai Zhang and Dingchu Zhang and Xixi Wu and Yong Jiang and Ming Yan and Pengjun Xie and Fei Huang and Jingren Zhou},
      year={2025},
      eprint={2507.02592},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.02592},
}
@misc{wu2025webdancer,
      title={WebDancer: Towards Autonomous Information Seeking Agency},
      author={Jialong Wu and Baixuan Li and Runnan Fang and Wenbiao Yin and Liwen Zhang and Zhengwei Tao and Dingchu Zhang and Zekun Xi and Yong Jiang and Pengjun Xie and Fei Huang and Jingren Zhou},
      year={2025},
      eprint={2505.22648},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.22648},
}
@misc{wu2025webwalker,
      title={WebWalker: Benchmarking LLMs in Web Traversal},
      author={Jialong Wu and Wenbiao Yin and Yong Jiang and Zhenglin Wang and Zekun Xi and Runnan Fang and Deyu Zhou and Pengjun Xie and Fei Huang},
      year={2025},
      eprint={2501.07572},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.07572},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸŒŸ Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#Alibaba-NLP/WebAgent&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Alibaba-NLP/WebAgent&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸš© Talent Recruitment&lt;/h2&gt; 
&lt;p&gt;ğŸ”¥ğŸ”¥ğŸ”¥ We are hiring! Research intern positions are open (based in Hangzhouã€Beijingã€Shanghai)&lt;/p&gt; 
&lt;p&gt;ğŸ“š &lt;strong&gt;Research Area&lt;/strong&gt;ï¼šWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; 
&lt;p&gt;â˜ï¸ &lt;strong&gt;Contact&lt;/strong&gt;ï¼š&lt;a href=""&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href="mailto:yongjiang.jy@alibaba-inc.com"&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NanmiCoder/MediaCrawler</title>
      <link>https://github.com/NanmiCoder/MediaCrawler</link>
      <description>&lt;p&gt;å°çº¢ä¹¦ç¬”è®° | è¯„è®ºçˆ¬è™«ã€æŠ–éŸ³è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€å¿«æ‰‹è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€B ç«™è§†é¢‘ ï½œ è¯„è®ºçˆ¬è™«ã€å¾®åšå¸–å­ ï½œ è¯„è®ºçˆ¬è™«ã€ç™¾åº¦è´´å§å¸–å­ ï½œ ç™¾åº¦è´´å§è¯„è®ºå›å¤çˆ¬è™« | çŸ¥ä¹é—®ç­”æ–‡ç« ï½œè¯„è®ºçˆ¬è™«&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ”¥ MediaCrawler - è‡ªåª’ä½“å¹³å°çˆ¬è™« ğŸ•·ï¸&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/8291" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/8291" alt="NanmiCoder%2FMediaCrawler | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;a href="https://github.com/NanmiCoder/MediaCrawler/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/NanmiCoder/MediaCrawler?style=social" alt="GitHub Stars"&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/NanmiCoder/MediaCrawler?style=social" alt="GitHub Forks"&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/issues"&gt;&lt;img src="https://img.shields.io/github/issues/NanmiCoder/MediaCrawler" alt="GitHub Issues"&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/NanmiCoder/MediaCrawler" alt="GitHub Pull Requests"&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/NanmiCoder/MediaCrawler" alt="License"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%87%A8%F0%9F%87%B3_%E4%B8%AD%E6%96%87-%E5%BD%93%E5%89%8D-blue" alt="ä¸­æ–‡"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README_en.md"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%87%BA%F0%9F%87%B8_English-Available-green" alt="English"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README_es.md"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%87%AA%F0%9F%87%B8_Espa%C3%B1ol-Available-green" alt="EspaÃ±ol"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;å…è´£å£°æ˜ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;å¤§å®¶è¯·ä»¥å­¦ä¹ ä¸ºç›®çš„ä½¿ç”¨æœ¬ä»“åº“âš ï¸âš ï¸âš ï¸âš ï¸ï¼Œ&lt;a href="https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China"&gt;çˆ¬è™«è¿æ³•è¿è§„çš„æ¡ˆä»¶&lt;/a&gt; &lt;br&gt;&lt;/p&gt; 
 &lt;p&gt;æœ¬ä»“åº“çš„æ‰€æœ‰å†…å®¹ä»…ä¾›å­¦ä¹ å’Œå‚è€ƒä¹‹ç”¨ï¼Œç¦æ­¢ç”¨äºå•†ä¸šç”¨é€”ã€‚ä»»ä½•äººæˆ–ç»„ç»‡ä¸å¾—å°†æœ¬ä»“åº“çš„å†…å®¹ç”¨äºéæ³•ç”¨é€”æˆ–ä¾µçŠ¯ä»–äººåˆæ³•æƒç›Šã€‚æœ¬ä»“åº“æ‰€æ¶‰åŠçš„çˆ¬è™«æŠ€æœ¯ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ï¼Œä¸å¾—ç”¨äºå¯¹å…¶ä»–å¹³å°è¿›è¡Œå¤§è§„æ¨¡çˆ¬è™«æˆ–å…¶ä»–éæ³•è¡Œä¸ºã€‚å¯¹äºå› ä½¿ç”¨æœ¬ä»“åº“å†…å®¹è€Œå¼•èµ·çš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œæœ¬ä»“åº“ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚ä½¿ç”¨æœ¬ä»“åº“çš„å†…å®¹å³è¡¨ç¤ºæ‚¨åŒæ„æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰æ¡æ¬¾å’Œæ¡ä»¶ã€‚&lt;/p&gt; 
 &lt;p&gt;ç‚¹å‡»æŸ¥çœ‹æ›´ä¸ºè¯¦ç»†çš„å…è´£å£°æ˜ã€‚&lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/#disclaimer"&gt;ç‚¹å‡»è·³è½¬&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ“– é¡¹ç›®ç®€ä»‹&lt;/h2&gt; 
&lt;p&gt;ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„&lt;strong&gt;å¤šå¹³å°è‡ªåª’ä½“æ•°æ®é‡‡é›†å·¥å…·&lt;/strong&gt;ï¼Œæ”¯æŒå°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ã€Bç«™ã€å¾®åšã€è´´å§ã€çŸ¥ä¹ç­‰ä¸»æµå¹³å°çš„å…¬å¼€ä¿¡æ¯æŠ“å–ã€‚&lt;/p&gt; 
&lt;h3&gt;ğŸ”§ æŠ€æœ¯åŸç†&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ ¸å¿ƒæŠ€æœ¯&lt;/strong&gt;ï¼šåŸºäº &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt; æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¡†æ¶ç™»å½•ä¿å­˜ç™»å½•æ€&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ— éœ€JSé€†å‘&lt;/strong&gt;ï¼šåˆ©ç”¨ä¿ç•™ç™»å½•æ€çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œé€šè¿‡ JS è¡¨è¾¾å¼è·å–ç­¾åå‚æ•°&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿ç‰¹ç‚¹&lt;/strong&gt;ï¼šæ— éœ€é€†å‘å¤æ‚çš„åŠ å¯†ç®—æ³•ï¼Œå¤§å¹…é™ä½æŠ€æœ¯é—¨æ§›&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âœ¨ åŠŸèƒ½ç‰¹æ€§&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;å¹³å°&lt;/th&gt; 
   &lt;th&gt;å…³é”®è¯æœç´¢&lt;/th&gt; 
   &lt;th&gt;æŒ‡å®šå¸–å­IDçˆ¬å–&lt;/th&gt; 
   &lt;th&gt;äºŒçº§è¯„è®º&lt;/th&gt; 
   &lt;th&gt;æŒ‡å®šåˆ›ä½œè€…ä¸»é¡µ&lt;/th&gt; 
   &lt;th&gt;ç™»å½•æ€ç¼“å­˜&lt;/th&gt; 
   &lt;th&gt;IPä»£ç†æ± &lt;/th&gt; 
   &lt;th&gt;ç”Ÿæˆè¯„è®ºè¯äº‘å›¾&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å°çº¢ä¹¦&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;æŠ–éŸ³&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¿«æ‰‹&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;B ç«™&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¾®åš&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è´´å§&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;çŸ¥ä¹&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details id="pro-version"&gt; 
 &lt;summary&gt;ğŸ”— &lt;strong&gt;ğŸš€ MediaCrawlerPro é‡ç£…å‘å¸ƒï¼æ›´å¤šçš„åŠŸèƒ½ï¼Œæ›´å¥½çš„æ¶æ„è®¾è®¡ï¼&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;ğŸš€ MediaCrawlerPro é‡ç£…å‘å¸ƒï¼&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ä¸“æ³¨äºå­¦ä¹ æˆç†Ÿé¡¹ç›®çš„æ¶æ„è®¾è®¡ï¼Œä¸ä»…ä»…æ˜¯çˆ¬è™«æŠ€æœ¯ï¼ŒPro ç‰ˆæœ¬çš„ä»£ç è®¾è®¡æ€è·¯åŒæ ·å€¼å¾—æ·±å…¥å­¦ä¹ ï¼&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;a href="https://github.com/MediaCrawlerPro"&gt;MediaCrawlerPro&lt;/a&gt; ç›¸è¾ƒäºå¼€æºç‰ˆæœ¬çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š&lt;/p&gt; 
 &lt;h4&gt;ğŸ¯ æ ¸å¿ƒåŠŸèƒ½å‡çº§&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½&lt;/strong&gt;ï¼ˆé‡ç‚¹ç‰¹æ€§ï¼‰&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å¤šè´¦å· + IPä»£ç†æ± æ”¯æŒ&lt;/strong&gt;ï¼ˆé‡ç‚¹ç‰¹æ€§ï¼‰&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å»é™¤ Playwright ä¾èµ–&lt;/strong&gt;ï¼Œä½¿ç”¨æ›´ç®€å•&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å®Œæ•´ Linux ç¯å¢ƒæ”¯æŒ&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;ğŸ—ï¸ æ¶æ„è®¾è®¡ä¼˜åŒ–&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;ä»£ç é‡æ„ä¼˜åŒ–&lt;/strong&gt;ï¼Œæ›´æ˜“è¯»æ˜“ç»´æŠ¤ï¼ˆè§£è€¦ JS ç­¾åé€»è¾‘ï¼‰&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;ä¼ä¸šçº§ä»£ç è´¨é‡&lt;/strong&gt;ï¼Œé€‚åˆæ„å»ºå¤§å‹çˆ¬è™«é¡¹ç›®&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å®Œç¾æ¶æ„è®¾è®¡&lt;/strong&gt;ï¼Œé«˜æ‰©å±•æ€§ï¼Œæºç å­¦ä¹ ä»·å€¼æ›´å¤§&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;ğŸ é¢å¤–åŠŸèƒ½&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;è‡ªåª’ä½“è§†é¢‘ä¸‹è½½å™¨æ¡Œé¢ç«¯&lt;/strong&gt;ï¼ˆé€‚åˆå­¦ä¹ å…¨æ ˆå¼€å‘ï¼‰&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å¤šå¹³å°é¦–é¡µä¿¡æ¯æµæ¨è&lt;/strong&gt;ï¼ˆHomeFeedï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled&gt; &lt;strong&gt;åŸºäºè‡ªåª’ä½“å¹³å°çš„AI Agentæ­£åœ¨å¼€å‘ä¸­ ğŸš€ğŸš€&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ç‚¹å‡»æŸ¥çœ‹ï¼š&lt;a href="https://github.com/MediaCrawlerPro"&gt;MediaCrawlerPro é¡¹ç›®ä¸»é¡µ&lt;/a&gt; æ›´å¤šä»‹ç»&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ’¡ &lt;strong&gt;å¼€æºä¸æ˜“ï¼Œå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ“‹ å‰ç½®ä¾èµ–&lt;/h2&gt; 
&lt;h3&gt;ğŸš€ uv å®‰è£…ï¼ˆæ¨èï¼‰&lt;/h3&gt; 
&lt;p&gt;åœ¨è¿›è¡Œä¸‹ä¸€æ­¥æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®ä¿ç”µè„‘ä¸Šå·²ç»å®‰è£…äº† uvï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å®‰è£…åœ°å€&lt;/strong&gt;ï¼š&lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;uv å®˜æ–¹å®‰è£…æŒ‡å—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;éªŒè¯å®‰è£…&lt;/strong&gt;ï¼šç»ˆç«¯è¾“å…¥å‘½ä»¤ &lt;code&gt;uv --version&lt;/code&gt;ï¼Œå¦‚æœæ­£å¸¸æ˜¾ç¤ºç‰ˆæœ¬å·ï¼Œè¯æ˜å·²ç»å®‰è£…æˆåŠŸ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ¨èç†ç”±&lt;/strong&gt;ï¼šuv æ˜¯ç›®å‰æœ€å¼ºçš„ Python åŒ…ç®¡ç†å·¥å…·ï¼Œé€Ÿåº¦å¿«ã€ä¾èµ–è§£æå‡†ç¡®&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸŸ¢ Node.js å®‰è£…&lt;/h3&gt; 
&lt;p&gt;é¡¹ç›®ä¾èµ– Node.jsï¼Œè¯·å‰å¾€å®˜ç½‘ä¸‹è½½å®‰è£…ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ä¸‹è½½åœ°å€&lt;/strong&gt;ï¼š&lt;a href="https://nodejs.org/en/download/"&gt;https://nodejs.org/en/download/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ç‰ˆæœ¬è¦æ±‚&lt;/strong&gt;ï¼š&amp;gt;= 16.0.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“¦ Python åŒ…å®‰è£…&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# è¿›å…¥é¡¹ç›®ç›®å½•
cd MediaCrawler

# ä½¿ç”¨ uv sync å‘½ä»¤æ¥ä¿è¯ python ç‰ˆæœ¬å’Œç›¸å…³ä¾èµ–åŒ…çš„ä¸€è‡´æ€§
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸŒ æµè§ˆå™¨é©±åŠ¨å®‰è£…&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# å®‰è£…æµè§ˆå™¨é©±åŠ¨
uv run playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ æç¤º&lt;/strong&gt;ï¼šMediaCrawler ç›®å‰å·²ç»æ”¯æŒä½¿ç”¨ playwright è¿æ¥ä½ æœ¬åœ°çš„ Chrome æµè§ˆå™¨äº†ï¼Œä¸€äº›å› ä¸º Webdriver å¯¼è‡´çš„é—®é¢˜è¿åˆƒè€Œè§£äº†ã€‚&lt;/p&gt; 
 &lt;p&gt;ç›®å‰å¼€æ”¾äº† &lt;code&gt;xhs&lt;/code&gt; å’Œ &lt;code&gt;dy&lt;/code&gt; è¿™ä¸¤ä¸ªä½¿ç”¨ CDP çš„æ–¹å¼è¿æ¥æœ¬åœ°æµè§ˆå™¨ï¼Œå¦‚æœ‰éœ€è¦ï¼ŒæŸ¥çœ‹ &lt;code&gt;config/base_config.py&lt;/code&gt; ä¸­çš„é…ç½®é¡¹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸš€ è¿è¡Œçˆ¬è™«ç¨‹åº&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# é¡¹ç›®é»˜è®¤æ˜¯æ²¡æœ‰å¼€å¯è¯„è®ºçˆ¬å–æ¨¡å¼ï¼Œå¦‚éœ€è¯„è®ºè¯·åœ¨ config/base_config.py ä¸­çš„ ENABLE_GET_COMMENTS å˜é‡ä¿®æ”¹
# ä¸€äº›å…¶ä»–æ”¯æŒé¡¹ï¼Œä¹Ÿå¯ä»¥åœ¨ config/base_config.py æŸ¥çœ‹åŠŸèƒ½ï¼Œå†™çš„æœ‰ä¸­æ–‡æ³¨é‡Š

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯æœç´¢ç›¸å…³çš„å¸–å­å¹¶çˆ¬å–å¸–å­ä¿¡æ¯ä¸è¯„è®º
uv run main.py --platform xhs --lt qrcode --type search

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨è·å–æŒ‡å®šå¸–å­çš„ä¿¡æ¯ä¸è¯„è®ºä¿¡æ¯
uv run main.py --platform xhs --lt qrcode --type detail

# æ‰“å¼€å¯¹åº”APPæ‰«äºŒç»´ç ç™»å½•

# å…¶ä»–å¹³å°çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹
uv run main.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ”— &lt;strong&gt;ä½¿ç”¨ Python åŸç”Ÿ venv ç®¡ç†ç¯å¢ƒï¼ˆä¸æ¨èï¼‰&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;åˆ›å»ºå¹¶æ¿€æ´» Python è™šæ‹Ÿç¯å¢ƒ&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;å¦‚æœæ˜¯çˆ¬å–æŠ–éŸ³å’ŒçŸ¥ä¹ï¼Œéœ€è¦æå‰å®‰è£… nodejs ç¯å¢ƒï¼Œç‰ˆæœ¬å¤§äºç­‰äºï¼š&lt;code&gt;16&lt;/code&gt; å³å¯&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd MediaCrawler

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
# æˆ‘çš„ python ç‰ˆæœ¬æ˜¯ï¼š3.9.6ï¼Œrequirements.txt ä¸­çš„åº“æ˜¯åŸºäºè¿™ä¸ªç‰ˆæœ¬çš„
# å¦‚æœæ˜¯å…¶ä»– python ç‰ˆæœ¬ï¼Œå¯èƒ½ requirements.txt ä¸­çš„åº“ä¸å…¼å®¹ï¼Œéœ€è‡ªè¡Œè§£å†³
python -m venv venv

# macOS &amp;amp; Linux æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# Windows æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;å®‰è£…ä¾èµ–åº“&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;å®‰è£… playwright æµè§ˆå™¨é©±åŠ¨&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;playwright install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;è¿è¡Œçˆ¬è™«ç¨‹åºï¼ˆåŸç”Ÿç¯å¢ƒï¼‰&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;# é¡¹ç›®é»˜è®¤æ˜¯æ²¡æœ‰å¼€å¯è¯„è®ºçˆ¬å–æ¨¡å¼ï¼Œå¦‚éœ€è¯„è®ºè¯·åœ¨ config/base_config.py ä¸­çš„ ENABLE_GET_COMMENTS å˜é‡ä¿®æ”¹
# ä¸€äº›å…¶ä»–æ”¯æŒé¡¹ï¼Œä¹Ÿå¯ä»¥åœ¨ config/base_config.py æŸ¥çœ‹åŠŸèƒ½ï¼Œå†™çš„æœ‰ä¸­æ–‡æ³¨é‡Š

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯æœç´¢ç›¸å…³çš„å¸–å­å¹¶çˆ¬å–å¸–å­ä¿¡æ¯ä¸è¯„è®º
python main.py --platform xhs --lt qrcode --type search

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨è·å–æŒ‡å®šå¸–å­çš„ä¿¡æ¯ä¸è¯„è®ºä¿¡æ¯
python main.py --platform xhs --lt qrcode --type detail

# æ‰“å¼€å¯¹åº”APPæ‰«äºŒç»´ç ç™»å½•

# å…¶ä»–å¹³å°çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹
python main.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ’¾ æ•°æ®ä¿å­˜&lt;/h2&gt; 
&lt;p&gt;æ”¯æŒå¤šç§æ•°æ®å­˜å‚¨æ–¹å¼ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SQLite æ•°æ®åº“&lt;/strong&gt;ï¼šè½»é‡çº§æ•°æ®åº“ï¼Œæ— éœ€æœåŠ¡å™¨ï¼Œé€‚åˆä¸ªäººä½¿ç”¨ï¼ˆæ¨èï¼‰ 
  &lt;ul&gt; 
   &lt;li&gt;å‚æ•°ï¼š&lt;code&gt;--save_data_option sqlite&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;è‡ªåŠ¨åˆ›å»ºæ•°æ®åº“æ–‡ä»¶&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MySQL æ•°æ®åº“&lt;/strong&gt;ï¼šæ”¯æŒå…³ç³»å‹æ•°æ®åº“ MySQL ä¸­ä¿å­˜ï¼ˆéœ€è¦æå‰åˆ›å»ºæ•°æ®åº“ï¼‰ 
  &lt;ul&gt; 
   &lt;li&gt;æ‰§è¡Œ &lt;code&gt;python db.py&lt;/code&gt; åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„ï¼ˆåªåœ¨é¦–æ¬¡æ‰§è¡Œï¼‰&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CSV æ–‡ä»¶&lt;/strong&gt;ï¼šæ”¯æŒä¿å­˜åˆ° CSV ä¸­ï¼ˆ&lt;code&gt;data/&lt;/code&gt; ç›®å½•ä¸‹ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JSON æ–‡ä»¶&lt;/strong&gt;ï¼šæ”¯æŒä¿å­˜åˆ° JSON ä¸­ï¼ˆ&lt;code&gt;data/&lt;/code&gt; ç›®å½•ä¸‹ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ä½¿ç”¨ç¤ºä¾‹ï¼š&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# ä½¿ç”¨ SQLiteï¼ˆæ¨èä¸ªäººç”¨æˆ·ä½¿ç”¨ï¼‰
uv run main.py --platform xhs --lt qrcode --type search --save_data_option sqlite

# ä½¿ç”¨ MySQL
uv run main.py --platform xhs --lt qrcode --type search --save_data_option db
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a href="https://github.com/MediaCrawlerPro"&gt;ğŸš€ MediaCrawlerPro é‡ç£…å‘å¸ƒ ğŸš€ï¼æ›´å¤šçš„åŠŸèƒ½ï¼Œæ›´å¥½çš„æ¶æ„è®¾è®¡ï¼&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ ç¤¾åŒºä¸æ”¯æŒ&lt;/h2&gt; 
&lt;h3&gt;ğŸ’¬ äº¤æµç¾¤ç»„&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å¾®ä¿¡äº¤æµç¾¤&lt;/strong&gt;ï¼š&lt;a href="https://nanmicoder.github.io/MediaCrawler/%E5%BE%AE%E4%BF%A1%E4%BA%A4%E6%B5%81%E7%BE%A4.html"&gt;ç‚¹å‡»åŠ å…¥&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“š æ–‡æ¡£ä¸æ•™ç¨‹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;åœ¨çº¿æ–‡æ¡£&lt;/strong&gt;ï¼š&lt;a href="https://nanmicoder.github.io/MediaCrawler/"&gt;MediaCrawler å®Œæ•´æ–‡æ¡£&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;çˆ¬è™«æ•™ç¨‹&lt;/strong&gt;ï¼š&lt;a href="https://github.com/NanmiCoder/CrawlerTutorial"&gt;CrawlerTutorial å…è´¹æ•™ç¨‹&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;å…¶ä»–å¸¸è§é—®é¢˜å¯ä»¥æŸ¥çœ‹åœ¨çº¿æ–‡æ¡£&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;åœ¨çº¿æ–‡æ¡£åŒ…å«ä½¿ç”¨æ–¹æ³•ã€å¸¸è§é—®é¢˜ã€åŠ å…¥é¡¹ç›®äº¤æµç¾¤ç­‰ã€‚ &lt;a href="https://nanmicoder.github.io/MediaCrawler/"&gt;MediaCrawleråœ¨çº¿æ–‡æ¡£&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;ä½œè€…æä¾›çš„çŸ¥è¯†æœåŠ¡&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å¦‚æœæƒ³å¿«é€Ÿå…¥é—¨å’Œå­¦ä¹ è¯¥é¡¹ç›®çš„ä½¿ç”¨ã€æºç æ¶æ„è®¾è®¡ç­‰ã€å­¦ä¹ ç¼–ç¨‹æŠ€æœ¯ã€äº¦æˆ–è€…æƒ³äº†è§£MediaCrawlerProçš„æºä»£ç è®¾è®¡å¯ä»¥çœ‹ä¸‹æˆ‘çš„çŸ¥è¯†ä»˜è´¹æ ç›®ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://nanmicoder.github.io/MediaCrawler/%E7%9F%A5%E8%AF%86%E4%BB%98%E8%B4%B9%E4%BB%8B%E7%BB%8D.html"&gt;ä½œè€…çš„çŸ¥è¯†ä»˜è´¹æ ç›®ä»‹ç»&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;â­ Star è¶‹åŠ¿å›¾&lt;/h2&gt; 
&lt;p&gt;å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼Œè®©æ›´å¤šçš„äººçœ‹åˆ° MediaCrawlerï¼&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#NanmiCoder/MediaCrawler&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ’° èµåŠ©å•†å±•ç¤º&lt;/h3&gt; 
&lt;a href="https://www.swiftproxy.net/?ref=nanmi"&gt; &lt;img src="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/img_5.png"&gt; &lt;br&gt; Swiftproxy - 90M+ å…¨çƒé«˜è´¨é‡çº¯å‡€ä½å®…IPï¼Œæ³¨å†Œå¯é¢†å…è´¹ 500MB æµ‹è¯•æµé‡ï¼ŒåŠ¨æ€æµé‡ä¸è¿‡æœŸï¼ &amp;gt; ä¸“å±æŠ˜æ‰£ç ï¼š**GHB5** ç«‹äº«ä¹æŠ˜ä¼˜æƒ ï¼ &lt;/a&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;a href="https://www.tkyds.com/?=MediaCrawler"&gt; &lt;img src="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/img_6.png"&gt; &lt;br&gt; TKäº‘å¤§å¸ˆ,ä¸“ä¸šçš„TikTokçŸ©é˜µç³»ç»Ÿ,AIèµ‹èƒ½è‡ªåŠ¨åŒ–,å•äººè½»æ¾ç®¡ç†ä¸Šä¸‡è´¦å·ï¼ &lt;/a&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;a href="https://www.thordata.com/?ls=github&amp;amp;lk=Crawler"&gt; &lt;img src="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/img_7.png"&gt; &lt;br&gt; Thordataæ˜¯å…¨çƒä»£ç†IPè§£å†³æ–¹æ¡ˆæä¾›å•†ï¼Œæ”¯æŒå¤§è§„æ¨¡é‡‡é›†å…¬å…±ç½‘ç»œæ•°æ®ï¼Œæä¾› 195+ å›½å®¶åŸå¸‚ã€6000 ä¸‡ä½å®…IPï¼Œä»·æ ¼ä½è‡³ $0.65/GBï¼Œæ”¯æŒä¸é™æµé‡ã€ä¸é™IPã€ä¸é™å¹¶å‘ï¼›è¿˜åŒ…æ‹¬æœ¬åœŸç‹¬äº«ISPé™æ€ä»£ç†å’Œé«˜æ€§èƒ½æ•°æ®ä¸­å¿ƒä»£ç†ï¼ˆå‡ä¸º $0.75/IPï¼Œå¼¹æ€§å®šä»·ï¼‰ã€‚ç‚¹å‡»å›¾ç‰‡æ³¨å†Œåè”ç³»ä¸­æ–‡å®¢æœå³å¯å…è´¹è¯•ç”¨ï¼Œç°åœ¨é¦–å……è¿˜æœ‰èµ é€åŒé¢é‡‘é¢æ´»åŠ¨ã€‚å¯ä¸EasySpiderå·¥å…·é…åˆä½¿ç”¨ï¼Œé«˜æ•ˆé‡‡é›†ç½‘ç»œæ•°æ®ã€‚ &lt;/a&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href="https://sider.ai/ad-land-redirect?source=github&amp;amp;p1=mi&amp;amp;p2=kk"&gt;&lt;strong&gt;Sider&lt;/strong&gt; - å…¨ç½‘æœ€ç«çš„ ChatGPT æ’ä»¶ï¼Œä½“éªŒæ‹‰æ»¡ï¼&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ¤ æˆä¸ºèµåŠ©è€…&lt;/h3&gt; 
&lt;p&gt;æˆä¸ºèµåŠ©è€…ï¼Œå¯ä»¥å°†æ‚¨çš„äº§å“å±•ç¤ºåœ¨è¿™é‡Œï¼Œæ¯å¤©è·å¾—å¤§é‡æ›å…‰ï¼&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;è”ç³»æ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å¾®ä¿¡ï¼š&lt;code&gt;yzglan&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;é‚®ç®±ï¼š&lt;code&gt;relakkes@gmail.com&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“š å‚è€ƒ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å°çº¢ä¹¦å®¢æˆ·ç«¯&lt;/strong&gt;ï¼š&lt;a href="https://github.com/ReaJason/xhs"&gt;ReaJason çš„ xhs ä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;çŸ­ä¿¡è½¬å‘&lt;/strong&gt;ï¼š&lt;a href="https://github.com/pppscn/SmsForwarder"&gt;SmsForwarder å‚è€ƒä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å†…ç½‘ç©¿é€å·¥å…·&lt;/strong&gt;ï¼š&lt;a href="https://ngrok.com/docs/"&gt;ngrok å®˜æ–¹æ–‡æ¡£&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;å…è´£å£°æ˜&lt;/h1&gt; 
&lt;div id="disclaimer"&gt; 
 &lt;h2&gt;1. é¡¹ç›®ç›®çš„ä¸æ€§è´¨&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®ï¼ˆä»¥ä¸‹ç®€ç§°â€œæœ¬é¡¹ç›®â€ï¼‰æ˜¯ä½œä¸ºä¸€ä¸ªæŠ€æœ¯ç ”ç©¶ä¸å­¦ä¹ å·¥å…·è€Œåˆ›å»ºçš„ï¼Œæ—¨åœ¨æ¢ç´¢å’Œå­¦ä¹ ç½‘ç»œæ•°æ®é‡‡é›†æŠ€æœ¯ã€‚æœ¬é¡¹ç›®ä¸“æ³¨äºè‡ªåª’ä½“å¹³å°çš„æ•°æ®çˆ¬å–æŠ€æœ¯ç ”ç©¶ï¼Œæ—¨åœ¨æä¾›ç»™å­¦ä¹ è€…å’Œç ”ç©¶è€…ä½œä¸ºæŠ€æœ¯äº¤æµä¹‹ç”¨ã€‚&lt;/p&gt; 
 &lt;h2&gt;2. æ³•å¾‹åˆè§„æ€§å£°æ˜&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®å¼€å‘è€…ï¼ˆä»¥ä¸‹ç®€ç§°â€œå¼€å‘è€…â€ï¼‰éƒ‘é‡æé†’ç”¨æˆ·åœ¨ä¸‹è½½ã€å®‰è£…å’Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶ï¼Œä¸¥æ ¼éµå®ˆä¸­åäººæ°‘å…±å’Œå›½ç›¸å…³æ³•å¾‹æ³•è§„ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºã€Šä¸­åäººæ°‘å…±å’Œå›½ç½‘ç»œå®‰å…¨æ³•ã€‹ã€ã€Šä¸­åäººæ°‘å…±å’Œå›½åé—´è°æ³•ã€‹ç­‰æ‰€æœ‰é€‚ç”¨çš„å›½å®¶æ³•å¾‹å’Œæ”¿ç­–ã€‚ç”¨æˆ·åº”è‡ªè¡Œæ‰¿æ‹…ä¸€åˆ‡å› ä½¿ç”¨æœ¬é¡¹ç›®è€Œå¯èƒ½å¼•èµ·çš„æ³•å¾‹è´£ä»»ã€‚&lt;/p&gt; 
 &lt;h2&gt;3. ä½¿ç”¨ç›®çš„é™åˆ¶&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®ä¸¥ç¦ç”¨äºä»»ä½•éæ³•ç›®çš„æˆ–éå­¦ä¹ ã€éç ”ç©¶çš„å•†ä¸šè¡Œä¸ºã€‚æœ¬é¡¹ç›®ä¸å¾—ç”¨äºä»»ä½•å½¢å¼çš„éæ³•ä¾µå…¥ä»–äººè®¡ç®—æœºç³»ç»Ÿï¼Œä¸å¾—ç”¨äºä»»ä½•ä¾µçŠ¯ä»–äººçŸ¥è¯†äº§æƒæˆ–å…¶ä»–åˆæ³•æƒç›Šçš„è¡Œä¸ºã€‚ç”¨æˆ·åº”ä¿è¯å…¶ä½¿ç”¨æœ¬é¡¹ç›®çš„ç›®çš„çº¯å±ä¸ªäººå­¦ä¹ å’ŒæŠ€æœ¯ç ”ç©¶ï¼Œä¸å¾—ç”¨äºä»»ä½•å½¢å¼çš„éæ³•æ´»åŠ¨ã€‚&lt;/p&gt; 
 &lt;h2&gt;4. å…è´£å£°æ˜&lt;/h2&gt; 
 &lt;p&gt;å¼€å‘è€…å·²å°½æœ€å¤§åŠªåŠ›ç¡®ä¿æœ¬é¡¹ç›®çš„æ­£å½“æ€§åŠå®‰å…¨æ€§ï¼Œä½†ä¸å¯¹ç”¨æˆ·ä½¿ç”¨æœ¬é¡¹ç›®å¯èƒ½å¼•èµ·çš„ä»»ä½•å½¢å¼çš„ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚åŒ…æ‹¬ä½†ä¸é™äºç”±äºä½¿ç”¨æœ¬é¡¹ç›®è€Œå¯¼è‡´çš„ä»»ä½•æ•°æ®ä¸¢å¤±ã€è®¾å¤‡æŸåã€æ³•å¾‹è¯‰è®¼ç­‰ã€‚&lt;/p&gt; 
 &lt;h2&gt;5. çŸ¥è¯†äº§æƒå£°æ˜&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®çš„çŸ¥è¯†äº§æƒå½’å¼€å‘è€…æ‰€æœ‰ã€‚æœ¬é¡¹ç›®å—åˆ°è‘—ä½œæƒæ³•å’Œå›½é™…è‘—ä½œæƒæ¡çº¦ä»¥åŠå…¶ä»–çŸ¥è¯†äº§æƒæ³•å¾‹å’Œæ¡çº¦çš„ä¿æŠ¤ã€‚ç”¨æˆ·åœ¨éµå®ˆæœ¬å£°æ˜åŠç›¸å…³æ³•å¾‹æ³•è§„çš„å‰æä¸‹ï¼Œå¯ä»¥ä¸‹è½½å’Œä½¿ç”¨æœ¬é¡¹ç›®ã€‚&lt;/p&gt; 
 &lt;h2&gt;6. æœ€ç»ˆè§£é‡Šæƒ&lt;/h2&gt; 
 &lt;p&gt;å…³äºæœ¬é¡¹ç›®çš„æœ€ç»ˆè§£é‡Šæƒå½’å¼€å‘è€…æ‰€æœ‰ã€‚å¼€å‘è€…ä¿ç•™éšæ—¶æ›´æ”¹æˆ–æ›´æ–°æœ¬å…è´£å£°æ˜çš„æƒåˆ©ï¼Œæ•ä¸å¦è¡Œé€šçŸ¥ã€‚&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ™ è‡´è°¢&lt;/h2&gt; 
&lt;h3&gt;JetBrains å¼€æºè®¸å¯è¯æ”¯æŒ&lt;/h3&gt; 
&lt;p&gt;æ„Ÿè°¢ JetBrains ä¸ºæœ¬é¡¹ç›®æä¾›å…è´¹çš„å¼€æºè®¸å¯è¯æ”¯æŒï¼&lt;/p&gt; 
&lt;a href="https://www.jetbrains.com/?from=MediaCrawler"&gt; &lt;img src="https://www.jetbrains.com/company/brand/img/jetbrains_logo.png" width="100" alt="JetBrains"&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Netflix/metaflow</title>
      <link>https://github.com/Netflix/metaflow</link>
      <description>&lt;p&gt;Build, Manage and Deploy AI/ML Systems&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/763451/89453116-96a57e00-d713-11ea-9fa6-82b29d4d6eff.png" alt="Metaflow_Logo_Horizontal_FullColor_Ribbon_Dark_RGB"&gt;&lt;/p&gt; 
&lt;h1&gt;Metaflow&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://metaflow.org"&gt;Metaflow&lt;/a&gt; is a human-centric framework designed to help scientists and engineers &lt;strong&gt;build and manage real-life AI and ML systems&lt;/strong&gt;. Serving teams of all sizes and scale, Metaflow streamlines the entire development lifecycleâ€”from rapid prototyping in notebooks to reliable, maintainable production deploymentsâ€”enabling teams to iterate quickly and deliver robust systems efficiently.&lt;/p&gt; 
&lt;p&gt;Originally developed at &lt;a href="https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9"&gt;Netflix&lt;/a&gt; and now supported by &lt;a href="https://outerbounds.com"&gt;Outerbounds&lt;/a&gt;, Metaflow is designed to boost the productivity for research and engineering teams working on &lt;a href="https://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d"&gt;a wide variety of projects&lt;/a&gt;, from classical statistics to state-of-the-art deep learning and foundation models. By unifying code, data, and compute at every stage, Metaflow ensures seamless, end-to-end management of real-world AI and ML systems.&lt;/p&gt; 
&lt;p&gt;Today, Metaflow powers thousands of AI and ML experiences across a diverse array of companies, large and small, including Amazon, Doordash, Dyson, Goldman Sachs, Ramp, and &lt;a href="https://raw.githubusercontent.com/Netflix/metaflow/master/ADOPTERS.md"&gt;many others&lt;/a&gt;. At Netflix alone, Metaflow supports over 3000 AI and ML projects, executes hundreds of millions of data-intensive high-performance compute jobs processing petabytes of data and manages tens of petabytes of models and artifacts for hundreds of users across its AI, ML, data science, and engineering teams.&lt;/p&gt; 
&lt;h2&gt;From prototype to production (and back)&lt;/h2&gt; 
&lt;p&gt;Metaflow provides a simple and friendly pythonic &lt;a href="https://docs.metaflow.org"&gt;API&lt;/a&gt; that covers foundational needs of AI and ML systems: &lt;img src="https://raw.githubusercontent.com/Netflix/metaflow/master/docs/prototype-to-prod.png" width="800px"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.metaflow.org/metaflow/basics"&gt;Rapid local prototyping&lt;/a&gt;, &lt;a href="https://docs.metaflow.org/metaflow/managing-flows/notebook-runs"&gt;support for notebooks&lt;/a&gt;, and built-in support for &lt;a href="https://docs.metaflow.org/metaflow/client"&gt;experiment tracking, versioning&lt;/a&gt; and &lt;a href="https://docs.metaflow.org/metaflow/visualizing-results"&gt;visualization&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.metaflow.org/scaling/remote-tasks/introduction"&gt;Effortlessly scale horizontally and vertically in your cloud&lt;/a&gt;, utilizing both CPUs and GPUs, with &lt;a href="https://docs.metaflow.org/scaling/data"&gt;fast data access&lt;/a&gt; for running &lt;a href="https://docs.metaflow.org/metaflow/basics#foreach"&gt;massive embarrassingly parallel&lt;/a&gt; as well as &lt;a href="https://docs.metaflow.org/scaling/remote-tasks/distributed-computing"&gt;gang-scheduled&lt;/a&gt; compute workloads &lt;a href="https://docs.metaflow.org/scaling/failures"&gt;reliably&lt;/a&gt; and &lt;a href="https://docs.metaflow.org/scaling/checkpoint/introduction"&gt;efficiently&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.metaflow.org/scaling/dependencies"&gt;Easily manage dependencies&lt;/a&gt; and &lt;a href="https://docs.metaflow.org/production/introduction"&gt;deploy with one-click&lt;/a&gt; to highly available production orchestrators with built in support for &lt;a href="https://docs.metaflow.org/production/event-triggering"&gt;reactive orchestration&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For full documentation, check out our &lt;a href="https://docs.metaflow.org/api"&gt;API Reference&lt;/a&gt; or see our &lt;a href="https://github.com/Netflix/metaflow/releases"&gt;Release Notes&lt;/a&gt; for the latest features and improvements.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Getting up and running is easy. If you don't know where to start, &lt;a href="https://outerbounds.com/sandbox"&gt;Metaflow sandbox&lt;/a&gt; will have you running and exploring in seconds.&lt;/p&gt; 
&lt;h3&gt;Installing Metaflow&lt;/h3&gt; 
&lt;p&gt;To install Metaflow in your Python environment from &lt;a href="https://pypi.org/project/metaflow/"&gt;PyPI&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install metaflow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, using &lt;a href="https://anaconda.org/conda-forge/metaflow"&gt;conda-forge&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge metaflow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once installed, a great way to get started is by following our &lt;a href="https://docs.metaflow.org/getting-started/tutorials"&gt;tutorial&lt;/a&gt;. It walks you through creating and running your first Metaflow flow step by step.&lt;/p&gt; 
&lt;p&gt;For more details on Metaflowâ€™s features and best practices, check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.metaflow.org/metaflow/basics"&gt;How Metaflow works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.metaflow.org/introduction/metaflow-resources"&gt;Additional resources&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you need help, donâ€™t hesitate to reach out on our &lt;a href="http://slack.outerbounds.co/"&gt;Slack community&lt;/a&gt;!&lt;/p&gt; 
&lt;h3&gt;Deploying infrastructure for Metaflow in your cloud&lt;/h3&gt; 
&lt;img src="https://raw.githubusercontent.com/Netflix/metaflow/master/docs/multicloud.png" width="800px"&gt; 
&lt;p&gt;While you can get started with Metaflow easily on your laptop, the main benefits of Metaflow lie in its ability to &lt;a href="https://docs.metaflow.org/scaling/remote-tasks/introduction"&gt;scale out to external compute clusters&lt;/a&gt; and to &lt;a href="https://docs.metaflow.org/production/introduction"&gt;deploy to production-grade workflow orchestrators&lt;/a&gt;. To benefit from these features, follow this &lt;a href="https://outerbounds.com/engineering/welcome/"&gt;guide&lt;/a&gt; to configure Metaflow and the infrastructure behind it appropriately.&lt;/p&gt; 
&lt;h2&gt;Get in touch&lt;/h2&gt; 
&lt;p&gt;We'd love to hear from you. Join our community &lt;a href="http://slack.outerbounds.co/"&gt;Slack workspace&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to Metaflow. Please see our &lt;a href="https://docs.metaflow.org/introduction/contributing-to-metaflow"&gt;contribution guide&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Genesis-Embodied-AI/Genesis</title>
      <link>https://github.com/Genesis-Embodied-AI/Genesis</link>
      <description>&lt;p&gt;A generative world for general-purpose robotics &amp; embodied AI learning.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/big_text.png" alt="Genesis"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/teaser.png" alt="Teaser"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/genesis-world/"&gt;&lt;img src="https://img.shields.io/pypi/v/genesis-world" alt="PyPI - Version"&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/genesis-world"&gt;&lt;img src="https://static.pepy.tech/badge/genesis-world" alt="PyPI Downloads"&gt;&lt;/a&gt; &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/issues"&gt;&lt;img src="https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis" alt="GitHub Issues"&gt;&lt;/a&gt; &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/discussions"&gt;&lt;img src="https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis" alt="GitHub Discussions"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/nukCuhB47p"&gt;&lt;img src="https://img.shields.io/discord/1322086972302430269?logo=discord" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://drive.google.com/uc?export=view&amp;amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ"&gt;&lt;img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white" height="20" style="display:inline"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/English-d9d9d9" alt="README in English"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_FR.md"&gt;&lt;img src="https://img.shields.io/badge/Francais-d9d9d9" alt="README en FranÃ§ais"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_KR.md"&gt;&lt;img src="https://img.shields.io/badge/%ED%95%9C%EA%B5%AD%EC%96%B4-d9d9d9" alt="í•œêµ­ì–´ README"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_CN.md"&gt;&lt;img src="https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-d9d9d9" alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_JA.md"&gt;&lt;img src="https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-d9d9d9" alt="æ—¥æœ¬èªç‰ˆ README"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Genesis&lt;/h1&gt; 
&lt;h2&gt;ğŸ”¥ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025-07-02] The development of Genesis is now officially supported by &lt;a href="https://genesis-ai.company/"&gt;Genesis AI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025-01-09] We released a &lt;a href="https://github.com/zhouxian/genesis-speed-benchmark"&gt;detailed performance benchmarking and comparison report&lt;/a&gt; on Genesis, together with all the test scripts.&lt;/li&gt; 
 &lt;li&gt;[2025-01-08] Released v0.2.1 ğŸŠ ğŸ‰&lt;/li&gt; 
 &lt;li&gt;[2025-01-08] Created &lt;a href="https://discord.gg/nukCuhB47p"&gt;Discord&lt;/a&gt; and &lt;a href="https://drive.google.com/uc?export=view&amp;amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ"&gt;Wechat&lt;/a&gt; group.&lt;/li&gt; 
 &lt;li&gt;[2024-12-25] Added a &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#docker"&gt;docker&lt;/a&gt; including support for the ray-tracing renderer&lt;/li&gt; 
 &lt;li&gt;[2024-12-24] Added guidelines for &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/raw/main/.github/CONTRIBUTING.md"&gt;contributing to Genesis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#what-is-genesis"&gt;What is Genesis?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#quick-installation"&gt;Quick Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#contributing-to-genesis"&gt;Contributing to Genesis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#support"&gt;Support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#license-and-acknowledgments"&gt;License and Acknowledgments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#associated-papers"&gt;Associated Papers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;What is Genesis?&lt;/h2&gt; 
&lt;p&gt;Genesis is a physics platform designed for general-purpose &lt;em&gt;Robotics/Embodied AI/Physical AI&lt;/em&gt; applications. It is simultaneously multiple things:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A &lt;strong&gt;universal physics engine&lt;/strong&gt; re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;lightweight&lt;/strong&gt;, &lt;strong&gt;ultra-fast&lt;/strong&gt;, &lt;strong&gt;pythonic&lt;/strong&gt;, and &lt;strong&gt;user-friendly&lt;/strong&gt; robotics simulation platform.&lt;/li&gt; 
 &lt;li&gt;A powerful and fast &lt;strong&gt;photo-realistic rendering system&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;generative data engine&lt;/strong&gt; that transforms user-prompted natural language description into various modalities of data.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Powered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully automated data generation for robotics and beyond.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Currently, we are open-sourcing the &lt;em&gt;underlying physics engine&lt;/em&gt; and the &lt;em&gt;simulation platform&lt;/em&gt;. Our &lt;em&gt;generative framework&lt;/em&gt; is a modular system that incorporates many different generative modules, each handling a certain range of data modalities, routed by a high level agent. Some of the modules integrated existing papers and some are still under submission. Access to our generative feature will be gradually rolled out in the near future. If you are interested, feel free to explore more in the &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#associated-papers"&gt;paper list&lt;/a&gt; below.&lt;/p&gt; 
&lt;p&gt;Genesis aims to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lower the barrier&lt;/strong&gt; to using physics simulations, making robotics research accessible to everyone. See our &lt;a href="https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html"&gt;mission statement&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unify diverse physics solvers&lt;/strong&gt; into a single framework to recreate the physical world with the highest fidelity.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automate data generation&lt;/strong&gt;, reducing human effort and letting the data flywheel spin on its own.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Project Page: &lt;a href="https://genesis-embodied-ai.github.io/"&gt;https://genesis-embodied-ai.github.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Over 43 million FPS when simulating a Franka robotic arm with a single RTX 4090 (430,000 times faster than real-time).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt;: Runs on Linux, macOS, Windows, and supports multiple compute backends (CPU, Nvidia/AMD GPUs, Apple Metal).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integration of diverse physics solvers&lt;/strong&gt;: Rigid body, MPM, SPH, FEM, PBD, Stable Fluid.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wide range of material models&lt;/strong&gt;: Simulation and coupling of rigid bodies, liquids, gases, deformable objects, thin-shell objects, and granular materials.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compatibility with various robots&lt;/strong&gt;: Robotic arms, legged robots, drones, &lt;em&gt;soft robots&lt;/em&gt;, and support for loading &lt;code&gt;MJCF (.xml)&lt;/code&gt;, &lt;code&gt;URDF&lt;/code&gt;, &lt;code&gt;.obj&lt;/code&gt;, &lt;code&gt;.glb&lt;/code&gt;, &lt;code&gt;.ply&lt;/code&gt;, &lt;code&gt;.stl&lt;/code&gt;, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Photo-realistic rendering&lt;/strong&gt;: Native ray-tracing-based rendering.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Differentiability&lt;/strong&gt;: Genesis is designed to be fully differentiable. Currently, our MPM solver and Tool Solver support differentiability, with other solvers planned for future versions (starting with rigid &amp;amp; articulated body solver).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Physics-based tactile simulation&lt;/strong&gt;: Differentiable &lt;a href="https://github.com/Genesis-Embodied-AI/DiffTactile"&gt;tactile sensor simulation&lt;/a&gt; coming soon (expected in version 0.3.0).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User-friendliness&lt;/strong&gt;: Designed for simplicity, with intuitive installation and APIs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Installation&lt;/h2&gt; 
&lt;p&gt;Install &lt;strong&gt;PyTorch&lt;/strong&gt; first following the &lt;a href="https://pytorch.org/get-started/locally/"&gt;official instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Then, install Genesis via PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install genesis-world  # Requires Python&amp;gt;=3.10,&amp;lt;3.13;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the latest version to date, make sure that &lt;code&gt;pip&lt;/code&gt; is up-to-date via &lt;code&gt;pip install --upgrade pip&lt;/code&gt;, then run command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/Genesis-Embodied-AI/Genesis.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the package must still be updated manually to sync with main branch.&lt;/p&gt; 
&lt;p&gt;Users seeking to edit the source code of Genesis are encourage to install Genesis in editable mode. First, make sure that &lt;code&gt;genesis-world&lt;/code&gt; has been uninstalled, then clone the repository and install locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Genesis-Embodied-AI/Genesis.git
cd Genesis
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;If you want to use Genesis from Docker, you can first build the Docker image as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t genesis -f docker/Dockerfile docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can run the examples inside the docker image (mounted to &lt;code&gt;/workspace/examples&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;xhost +local:root # Allow the container to access the display

docker run --gpus all --rm -it \
-e DISPLAY=$DISPLAY \
-v /dev/dri:/dev/dri \
-v /tmp/.X11-unix/:/tmp/.X11-unix \
-v $PWD:/workspace \
genesis
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;AMD users&lt;/h3&gt; 
&lt;p&gt;AMD users can use Genesis using the &lt;code&gt;docker/Dockerfile.amdgpu&lt;/code&gt; file, which is built by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker build -t genesis-amd -f docker/Dockerfile.amdgpu docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and can then be used by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-xhost"&gt;docker run -it --network=host \
 --device=/dev/kfd \
 --device=/dev/dri \
 --group-add=video \
 --ipc=host \
 --cap-add=SYS_PTRACE \
 --security-opt seccomp=unconfined \
 --shm-size 8G \
 -v $PWD:/workspace \
 -e DISPLAY=$DISPLAY \
 genesis-amd
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The examples will be accessible from &lt;code&gt;/workspace/examples&lt;/code&gt;. Note: AMD users should use the vulkan backend. This means you will need to call &lt;code&gt;gs.init(vulkan)&lt;/code&gt; to initialise Genesis.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation is available in &lt;a href="https://genesis-world.readthedocs.io/en/latest/user_guide/index.html"&gt;English&lt;/a&gt;, &lt;a href="https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html"&gt;Chinese&lt;/a&gt;, and &lt;a href="https://genesis-world.readthedocs.io/ja/latest/user_guide/index.html"&gt;Japanese&lt;/a&gt;. This includes detailed installation steps, tutorials, and API references.&lt;/p&gt; 
&lt;h2&gt;Contributing to Genesis&lt;/h2&gt; 
&lt;p&gt;The Genesis project is an open and collaborative effort. We welcome all forms of contributions from the community, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pull requests&lt;/strong&gt; for new features or bug fixes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bug reports&lt;/strong&gt; through GitHub Issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Suggestions&lt;/strong&gt; to improve Genesis's usability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Refer to our &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/raw/main/.github/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report bugs or request features via GitHub &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/issues"&gt;Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join discussions or ask questions on GitHub &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/discussions"&gt;Discussions&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License and Acknowledgments&lt;/h2&gt; 
&lt;p&gt;The Genesis source code is licensed under Apache 2.0.&lt;/p&gt; 
&lt;p&gt;Genesis's development has been made possible thanks to these open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/taichi-dev/taichi"&gt;Taichi&lt;/a&gt;: High-performance cross-platform compute backend. Kudos to the Taichi team for their technical support!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zhouxian/FluidLab"&gt;FluidLab&lt;/a&gt;: Reference MPM solver implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/erizmr/SPH_Taichi"&gt;SPH_Taichi&lt;/a&gt;: Reference SPH solver implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://matthias-research.github.io/pages/tenMinutePhysics/index.html"&gt;Ten Minute Physics&lt;/a&gt; and &lt;a href="https://github.com/WASD4959/PBF3D"&gt;PBF3D&lt;/a&gt;: Reference PBD solver implementations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google-deepmind/mujoco"&gt;MuJoCo&lt;/a&gt;: Reference for rigid body dynamics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/danfis/libccd"&gt;libccd&lt;/a&gt;: Reference for collision detection.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mmatl/pyrender"&gt;PyRender&lt;/a&gt;: Rasterization-based renderer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuisaGroup/LuisaCompute"&gt;LuisaCompute&lt;/a&gt; and &lt;a href="https://github.com/LuisaGroup/LuisaRender"&gt;LuisaRender&lt;/a&gt;: Ray-tracing DSL.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Associated Papers&lt;/h2&gt; 
&lt;p&gt;Genesis is a large scale effort that integrates state-of-the-art technologies of various existing and on-going research work into a single system. Here we include a non-exhaustive list of all the papers that contributed to the Genesis project in one way or another:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Xian, Zhou, et al. "Fluidlab: A differentiable environment for benchmarking complex fluid manipulation." arXiv preprint arXiv:2303.02346 (2023).&lt;/li&gt; 
 &lt;li&gt;Xu, Zhenjia, et al. "Roboninja: Learning an adaptive cutting policy for multi-material objects." arXiv preprint arXiv:2302.11553 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Yufei, et al. "Robogen: Towards unleashing infinite data for automated robot learning via generative simulation." arXiv preprint arXiv:2311.01455 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Tsun-Hsuan, et al. "Softzoo: A soft robot co-design benchmark for locomotion in diverse environments." arXiv preprint arXiv:2303.09555 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Tsun-Hsuan Johnson, et al. "Diffusebot: Breeding soft robots with physics-augmented generative diffusion models." Advances in Neural Information Processing Systems 36 (2023): 44398-44423.&lt;/li&gt; 
 &lt;li&gt;Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. "Gen2sim: Scaling up robot learning in simulation with generative models." 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.&lt;/li&gt; 
 &lt;li&gt;Si, Zilin, et al. "DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation." arXiv preprint arXiv:2403.08716 (2024).&lt;/li&gt; 
 &lt;li&gt;Wang, Yian, et al. "Thin-Shell Object Manipulations With Differentiable Physics Simulations." arXiv preprint arXiv:2404.00451 (2024).&lt;/li&gt; 
 &lt;li&gt;Lin, Chunru, et al. "UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments." arXiv preprint arXiv:2411.12711 (2024).&lt;/li&gt; 
 &lt;li&gt;Zhou, Wenyang, et al. "EMDM: Efficient motion diffusion model for fast and high-quality motion generation." European Conference on Computer Vision. Springer, Cham, 2025.&lt;/li&gt; 
 &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. "Scalable differentiable physics for learning and control." International Conference on Machine Learning. PMLR, 2020.&lt;/li&gt; 
 &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. "Efficient differentiable simulation of articulated bodies." In International Conference on Machine Learning, PMLR, 2021.&lt;/li&gt; 
 &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. "Differentiable simulation of soft multi-body systems." Advances in Neural Information Processing Systems 34 (2021).&lt;/li&gt; 
 &lt;li&gt;Wan, Weilin, et al. "Tlcontrol: Trajectory and language control for human motion synthesis." arXiv preprint arXiv:2311.17135 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Yian, et al. "Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting." arXiv preprint arXiv:2411.09823 (2024).&lt;/li&gt; 
 &lt;li&gt;Zheng, Shaokun, et al. "LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures." ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.&lt;/li&gt; 
 &lt;li&gt;Fan, Yingruo, et al. "Faceformer: Speech-driven 3d facial animation with transformers." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.&lt;/li&gt; 
 &lt;li&gt;Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE." Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.&lt;/li&gt; 
 &lt;li&gt;Dou, Zhiyang, et al. "CÂ· ase: Learning conditional adversarial skill embeddings for physics-based characters." SIGGRAPH Asia 2023 Conference Papers. 2023.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;... and many more on-going work.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use Genesis in your research, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{Genesis,
  author = {Genesis Authors},
  title = {Genesis: A Generative and Universal Physics Engine for Robotics and Beyond},
  month = {December},
  year = {2024},
  url = {https://github.com/Genesis-Embodied-AI/Genesis}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>LMCache/LMCache</title>
      <link>https://github.com/LMCache/LMCache</link>
      <description>&lt;p&gt;Supercharge Your LLM with the Fastest KV Cache Layer&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/LMCache/LMCache/dev/asset/logo.png" width="720" alt="lmcache logo"&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.lmcache.ai/"&gt;&lt;img src="https://img.shields.io/badge/docs-live-brightgreen" alt="Docs"&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/v/lmcache" alt="PyPI"&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/lmcache" alt="PyPI - Python Version"&gt;&lt;/a&gt; &lt;a href="https://buildkite.com/lmcache/lmcache-unittests"&gt;&lt;img src="https://badge.buildkite.com/ce25f1819a274b7966273bfa54f0e02f092c3de0d7563c5c9d.svg?sanitize=true" alt="Unit Tests"&gt;&lt;/a&gt; &lt;a href="https://github.com/LMCache/LMCache/actions/workflows/code_quality_checks.yml"&gt;&lt;img src="https://github.com/lmcache/lmcache/actions/workflows/code_quality_checks.yml/badge.svg?branch=dev&amp;amp;label=tests" alt="Code Quality"&gt;&lt;/a&gt; &lt;a href="https://buildkite.com/lmcache/lmcache-vllm-integration-tests"&gt;&lt;img src="https://badge.buildkite.com/108ddd4ab482a2480999dec8c62a640a3315ed4e6c4e86798e.svg?sanitize=true" alt="Integration Tests"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;a href="https://www.bestpractices.dev/projects/10841"&gt;&lt;img src="https://www.bestpractices.dev/projects/10841/badge" alt="OpenSSF Best Practices"&gt;&lt;/a&gt; &lt;a href="https://scorecard.dev/viewer/?uri=github.com/LMCache/LMCache"&gt;&lt;img src="https://api.scorecard.dev/projects/github.com/LMCache/LMCache/badge" alt="OpenSSF Scorecard"&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/LMCache/LMCache/"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki"&gt;&lt;/a&gt; &lt;a href="https://github.com/LMCache/LMCache/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/w/LMCache/LMCache" alt="GitHub commit activity"&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/dm/lmcache" alt="PyPI - Downloads"&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA"&gt;&lt;img src="https://img.shields.io/youtube/channel/views/UC58zMz55n70rtf1Ak2PULJA" alt="YouTube Channel Views"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;p&gt;| &lt;a href="https://blog.lmcache.ai/"&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://docs.lmcache.ai/"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-36x1m765z-8FgDA_73vcXtlZ_4XvpE6Q"&gt;&lt;strong&gt;Join Slack&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://forms.gle/MHwLiYDU6kcW3dLj7"&gt;&lt;strong&gt;Interest Form&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/LMCache/LMCache/issues/574"&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ğŸ”¥ &lt;strong&gt;NEW: For enterprise-scale deployment of LMCache and vLLM, please check out vLLM &lt;a href="https://github.com/vllm-project/production-stack"&gt;Production Stack&lt;/a&gt;. LMCache is also officially supported in &lt;a href="https://github.com/llm-d/llm-d/"&gt;llm-d&lt;/a&gt; and &lt;a href="https://github.com/kserve/kserve"&gt;KServe&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;LMCache is an &lt;strong&gt;LLM&lt;/strong&gt; serving engine extension to &lt;strong&gt;reduce TTFT&lt;/strong&gt; and &lt;strong&gt;increase throughput&lt;/strong&gt;, especially under long-context scenarios. By storing the KV caches of reusable texts across various locations, including (GPU, CPU DRAM, Local Disk), LMCache reuses the KV caches of &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; reused text (not necessarily prefix) in &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; serving engine instance. Thus, LMCache saves precious GPU cycles and reduces user response delay.&lt;/p&gt; 
&lt;p&gt;By combining LMCache with vLLM, developers achieve 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/86137f17-f216-41a0-96a7-e537764f7a4c" alt="performance"&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; ğŸ”¥ Integration with vLLM v1 with the following features: 
  &lt;ul&gt; 
   &lt;li&gt;High performance CPU KVCache offloading&lt;/li&gt; 
   &lt;li&gt;Disaggregated prefill&lt;/li&gt; 
   &lt;li&gt;P2P KVCache sharing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; LMCache is supported in the &lt;a href="https://github.com/vllm-project/production-stack/"&gt;vLLM production stack&lt;/a&gt;, &lt;a href="https://github.com/llm-d/llm-d/"&gt;llm-d&lt;/a&gt;, and &lt;a href="https://github.com/kserve/kserve"&gt;KServe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Stable support for non-prefix KV caches&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Storage support as follows: 
  &lt;ul&gt; 
   &lt;li&gt;CPU&lt;/li&gt; 
   &lt;li&gt;Disk&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/ai-dynamo/nixl"&gt;NIXL&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Installation support through pip and latest vLLM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To use LMCache, simply install &lt;code&gt;lmcache&lt;/code&gt; from your package manager, e.g. pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install lmcache
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Works on Linux NVIDIA GPU platform.&lt;/p&gt; 
&lt;p&gt;More &lt;a href="https://docs.lmcache.ai/getting_started/installation"&gt;detailed installation instructions&lt;/a&gt; are available in the docs.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The best way to get started is to checkout the &lt;a href="https://docs.lmcache.ai/getting_started/quickstart/"&gt;Quickstart Examples&lt;/a&gt; in the docs.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Check out the LMCache &lt;a href="https://docs.lmcache.ai/"&gt;documentation&lt;/a&gt; which is available online.&lt;/p&gt; 
&lt;p&gt;We also post regularly in &lt;a href="https://blog.lmcache.ai/"&gt;LMCache blogs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Go hands-on with our &lt;a href="https://github.com/LMCache/LMCache/tree/dev/examples"&gt;examples&lt;/a&gt;, demonstrating how to address different use cases with LMCache.&lt;/p&gt; 
&lt;h2&gt;Interested in Connecting?&lt;/h2&gt; 
&lt;p&gt;Fill out the &lt;a href="https://forms.gle/mQfQDUXbKfp2St1z7"&gt;interest form&lt;/a&gt;, &lt;a href="https://mailchi.mp/tensormesh/lmcache-sign-up-newsletter"&gt;sign up for our newsletter&lt;/a&gt;, &lt;a href="https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ"&gt;join LMCache slack&lt;/a&gt;, &lt;a href="https://lmcache.ai/"&gt;check out LMCache website&lt;/a&gt;, or &lt;a href="https://raw.githubusercontent.com/LMCache/LMCache/dev/contact@lmcache.ai"&gt;drop an email&lt;/a&gt;, and our team will reach out to you!&lt;/p&gt; 
&lt;h2&gt;Community meeting&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://uchicago.zoom.us/j/6603596916?pwd=Z1E5MDRWUSt2am5XbEt4dTFkNGx6QT09"&gt;community meeting&lt;/a&gt; for LMCache is hosted bi-weekly. All are welcome to join!&lt;/p&gt; 
&lt;p&gt;Meetings are held bi-weekly on: Tuesdays at 9:00 AM PT â€“ &lt;a href="https://drive.usercontent.google.com/u/0/uc?id=1f5EXbooGcwNwzIpTgn5u4PHqXgfypMtu&amp;amp;export=download"&gt;Add to Calendar&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We keep notes from each meeting on this &lt;a href="https://docs.google.com/document/d/1_Fl3vLtERFa3vTH00cezri78NihNBtSClK-_1tSrcow"&gt;document&lt;/a&gt; for summaries of standups, discussion, and action items.&lt;/p&gt; 
&lt;p&gt;Recordings of meetings are available on the &lt;a href="https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA"&gt;YouTube LMCache channel&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value all contributions and collaborations. Please check out &lt;a href="https://raw.githubusercontent.com/LMCache/LMCache/dev/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; on how to contribute.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use LMCache for your research, please cite our papers:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{liu2024cachegen,
  title={Cachegen: Kv cache compression and streaming for fast large language model serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}

@article{cheng2024large,
  title={Do Large Language Models Need a Content Delivery Network?},
  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},
  journal={arXiv preprint arXiv:2409.13761},
  year={2024}
}

@inproceedings{10.1145/3689031.3696098,
  author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  title = {CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion},
  year = {2025},
  url = {https://doi.org/10.1145/3689031.3696098},
  doi = {10.1145/3689031.3696098},
  booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
  pages = {94â€“109},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Socials&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.linkedin.com/company/lmcache-lab/?viewAsMember=true"&gt;Linkedin&lt;/a&gt; | &lt;a href="https://x.com/lmcache"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.youtube.com/@LMCacheTeam"&gt;Youtube&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The LMCache codebase is licensed under Apache License 2.0. See the &lt;a href="https://raw.githubusercontent.com/LMCache/LMCache/dev/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>nunchaku-tech/nunchaku</title>
      <link>https://github.com/nunchaku-tech/nunchaku</link>
      <description>&lt;p&gt;[ICLR2025 Spotlight] SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;div align="center" id="nunchaku_logo"&gt; 
 &lt;img src="https://huggingface.co/datasets/nunchaku-tech/cdn/resolve/main/nunchaku/assets/nunchaku.svg?sanitize=true" alt="logo" width="220"&gt; 
&lt;/div&gt; 
&lt;h3 align="center"&gt; &lt;a href="http://arxiv.org/abs/2411.05007"&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://nunchaku.tech/docs/nunchaku/"&gt;&lt;b&gt;Docs&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://hanlab.mit.edu/projects/svdquant"&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://hanlab.mit.edu/blog/svdquant"&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://svdquant.mit.edu"&gt;&lt;b&gt;Demo&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://huggingface.co/nunchaku-tech"&gt;&lt;b&gt;Hugging Face&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://modelscope.cn/organization/nunchaku-tech"&gt;&lt;b&gt;ModelScope&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://github.com/nunchaku-tech/ComfyUI-nunchaku"&gt;&lt;b&gt;ComfyUI&lt;/b&gt;&lt;/a&gt; &lt;/h3&gt; 
&lt;h3 align="center"&gt; &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/README.md"&gt;&lt;b&gt;English&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/README_ZH.md"&gt;&lt;b&gt;ä¸­æ–‡&lt;/b&gt;&lt;/a&gt; &lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Nunchaku&lt;/strong&gt; is a high-performance inference engine optimized for 4-bit neural networks, as introduced in our paper &lt;a href="http://arxiv.org/abs/2411.05007"&gt;SVDQuant&lt;/a&gt;. For the underlying quantization library, check out &lt;a href="https://github.com/nunchaku-tech/deepcompressor"&gt;DeepCompressor&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Join our user groups on &lt;a href="https://join.slack.com/t/nunchaku/shared_invite/zt-3170agzoz-NgZzWaTrEj~n2KEV3Hpl5Q"&gt;&lt;strong&gt;Slack&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://discord.gg/Wk6PnwX9Sm"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/nunchaku-tech/cdn/resolve/main/nunchaku/assets/wechat.jpg"&gt;&lt;strong&gt;WeChat&lt;/strong&gt;&lt;/a&gt; to engage in discussions with the community! More details can be found &lt;a href="https://github.com/nunchaku-tech/nunchaku/issues/149"&gt;here&lt;/a&gt;. If you have any questions, run into issues, or are interested in contributing, donâ€™t hesitate to reach out!&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-07-13]&lt;/strong&gt; ğŸš€ The official &lt;a href="https://nunchaku.tech/docs/nunchaku/"&gt;&lt;strong&gt;Nunchaku documentation&lt;/strong&gt;&lt;/a&gt; is now live! Explore comprehensive guides and resources to help you get started.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-06-29]&lt;/strong&gt; ğŸ”¥ Support &lt;strong&gt;FLUX.1-Kontext&lt;/strong&gt;! Try out our &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/examples/flux.1-kontext-dev.py"&gt;example script&lt;/a&gt; to see it in action! Our demo is available at this &lt;a href="https://svdquant.mit.edu/kontext/"&gt;link&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-06-01]&lt;/strong&gt; ğŸš€ &lt;strong&gt;Release v0.3.0!&lt;/strong&gt; This update adds support for multiple-batch inference, &lt;a href="https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0"&gt;&lt;strong&gt;ControlNet-Union-Pro 2.0&lt;/strong&gt;&lt;/a&gt;, initial integration of &lt;a href="https://github.com/ToTheBeginning/PuLID"&gt;&lt;strong&gt;PuLID&lt;/strong&gt;&lt;/a&gt;, and introduces &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/examples/flux.1-dev-double_cache.py"&gt;&lt;strong&gt;Double FB Cache&lt;/strong&gt;&lt;/a&gt;. You can now load Nunchaku FLUX models as a single file, and our upgraded &lt;a href="https://huggingface.co/nunchaku-tech/nunchaku-t5"&gt;&lt;strong&gt;4-bit T5 encoder&lt;/strong&gt;&lt;/a&gt; now matches &lt;strong&gt;FP8 T5&lt;/strong&gt; in quality!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-04-16]&lt;/strong&gt; ğŸ¥ Released tutorial videos in both &lt;a href="https://youtu.be/YHAVe-oM7U8?si=cM9zaby_aEHiFXk0"&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://www.bilibili.com/video/BV1BTocYjEk5/?share_source=copy_web&amp;amp;vd_source=8926212fef622f25cc95380515ac74ee"&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/a&gt; to assist installation and usage.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-04-09]&lt;/strong&gt; ğŸ“¢ Published the &lt;a href="https://github.com/nunchaku-tech/nunchaku/issues/266"&gt;April roadmap&lt;/a&gt; and an &lt;a href="https://github.com/nunchaku-tech/nunchaku/discussions/262"&gt;FAQ&lt;/a&gt; to help the community get started and stay up to date with Nunchakuâ€™s development.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-04-05]&lt;/strong&gt; ğŸš€ &lt;strong&gt;Nunchaku v0.2.0 released!&lt;/strong&gt; This release brings &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/examples/flux.1-dev-multiple-lora.py"&gt;&lt;strong&gt;multi-LoRA&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/examples/flux.1-dev-controlnet-union-pro.py"&gt;&lt;strong&gt;ControlNet&lt;/strong&gt;&lt;/a&gt; support with even faster performance powered by &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/#fp16-attention"&gt;&lt;strong&gt;FP16 attention&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/#first-block-cache"&gt;&lt;strong&gt;First-Block Cache&lt;/strong&gt;&lt;/a&gt;. We've also added compatibility for &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/examples/flux.1-dev-turing.py"&gt;&lt;strong&gt;20-series GPUs&lt;/strong&gt;&lt;/a&gt; â€” Nunchaku is now more accessible than ever!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-03-07]&lt;/strong&gt; ğŸš€ &lt;strong&gt;Nunchaku v0.1.4 Released!&lt;/strong&gt; We've supported &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/#Low-Memory-Inference"&gt;4-bit text encoder and per-layer CPU offloading&lt;/a&gt;, reducing FLUX's minimum memory requirement to just &lt;strong&gt;4 GiB&lt;/strong&gt; while maintaining a &lt;strong&gt;2â€“3Ã— speedup&lt;/strong&gt;. This update also fixes various issues related to resolution, LoRA, pin memory, and runtime stability. Check out the release notes for full details!&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-02-20]&lt;/strong&gt; ğŸš€ &lt;strong&gt;Support NVFP4 precision on NVIDIA RTX 5090!&lt;/strong&gt; NVFP4 delivers superior image quality compared to INT4, offering &lt;strong&gt;~3Ã— speedup&lt;/strong&gt; on the RTX 5090 over BF16. Learn more in our &lt;a href="https://hanlab.mit.edu/blog/svdquant-nvfp4"&gt;blog&lt;/a&gt;, checkout &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/examples"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; for usage and try &lt;a href="https://svdquant.mit.edu/flux1-schnell/"&gt;our demo&lt;/a&gt; online!&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-02-18]&lt;/strong&gt; ğŸ”¥ &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/#Customized-LoRA"&gt;&lt;strong&gt;Customized LoRA conversion&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/#Customized-Model-Quantization"&gt;&lt;strong&gt;model quantization&lt;/strong&gt;&lt;/a&gt; instructions are now available! &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/comfyui"&gt;ComfyUI&lt;/a&gt;&lt;/strong&gt; workflows now support &lt;strong&gt;customized LoRA&lt;/strong&gt;, along with &lt;strong&gt;FLUX.1-Tools&lt;/strong&gt;!&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-02-11]&lt;/strong&gt; ğŸ‰ &lt;strong&gt;&lt;a href="http://arxiv.org/abs/2411.05007"&gt;SVDQuant&lt;/a&gt; has been selected as a ICLR 2025 Spotlight! FLUX.1-tools Gradio demos are now available!&lt;/strong&gt; Check &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/#gradio-demos"&gt;here&lt;/a&gt; for the usage details! Our new &lt;a href="https://svdquant.mit.edu/flux1-depth-dev/"&gt;depth-to-image demo&lt;/a&gt; is also onlineâ€”try it out!&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-02-04]&lt;/strong&gt; &lt;strong&gt;ğŸš€ 4-bit &lt;a href="https://blackforestlabs.ai/flux-1-tools/"&gt;FLUX.1-tools&lt;/a&gt; is here!&lt;/strong&gt; Enjoy a &lt;strong&gt;2-3Ã— speedup&lt;/strong&gt; over the original models. Check out the &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/examples"&gt;examples&lt;/a&gt; for usage. &lt;strong&gt;ComfyUI integration is coming soon!&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-01-23]&lt;/strong&gt; ğŸš€ &lt;strong&gt;4-bit &lt;a href="https://nvlabs.github.io/Sana/"&gt;SANA&lt;/a&gt; support is here!&lt;/strong&gt; Experience a 2-3Ã— speedup compared to the 16-bit model. Check out the &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/examples/sana1.6b_pag.py"&gt;usage example&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/app/sana/t2i"&gt;deployment guide&lt;/a&gt; for more details. Explore our live demo at &lt;a href="https://svdquant.mit.edu"&gt;svdquant.mit.edu&lt;/a&gt;!&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-01-22]&lt;/strong&gt; ğŸ‰ &lt;a href="http://arxiv.org/abs/2411.05007"&gt;&lt;strong&gt;SVDQuant&lt;/strong&gt;&lt;/a&gt; has been accepted to &lt;strong&gt;ICLR 2025&lt;/strong&gt;!&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2024-12-08]&lt;/strong&gt; Support &lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI&lt;/a&gt;. Please check &lt;a href="https://github.com/nunchaku-tech/ComfyUI-nunchaku"&gt;ComfyUI-nunchaku&lt;/a&gt; for the usage.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2024-11-07]&lt;/strong&gt; ğŸ”¥ Our latest &lt;strong&gt;W4A4&lt;/strong&gt; Diffusion model quantization work &lt;a href="https://hanlab.mit.edu/projects/svdquant"&gt;&lt;strong&gt;SVDQuant&lt;/strong&gt;&lt;/a&gt; is publicly released! Check &lt;a href="https://github.com/nunchaku-tech/deepcompressor"&gt;&lt;strong&gt;DeepCompressor&lt;/strong&gt;&lt;/a&gt; for the quantization library.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/nunchaku-tech/cdn/resolve/main/nunchaku/assets/teaser.jpg" alt="teaser"&gt; &lt;strong&gt;Nunchaku&lt;/strong&gt; is a high-performance inference engine for low-bit neural networks. It implements &lt;strong&gt;SVDQuant&lt;/strong&gt;, a post-training quantization technique for 4-bit weights and activations that well maintains visual fidelity. On 12B FLUX.1-dev, it achieves 3.6Ã— memory reduction compared to the BF16 model. By eliminating CPU offloading, it offers 8.7Ã— speedup over the 16-bit model when on a 16GB laptop 4090 GPU, 3Ã— faster than the NF4 W4A16 baseline. On PixArt-âˆ‘, it demonstrates significantly superior visual quality over other W4A4 or even W4A8 baselines. "E2E" means the end-to-end latency including the text encoder and VAE decoder.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;a href="https://lmxyy.me"&gt;Muyang Li&lt;/a&gt;*, &lt;a href="https://yujunlin.com"&gt;Yujun Lin&lt;/a&gt;*, &lt;a href="https://hanlab.mit.edu/team/zhekai-zhang"&gt;Zhekai Zhang&lt;/a&gt;*, &lt;a href="https://www.tianle.website/#/"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="https://xiuyuli.com"&gt;Xiuyu Li&lt;/a&gt;, &lt;a href="https://github.com/JerryGJX"&gt;Junxian Guo&lt;/a&gt;, &lt;a href="https://xieenze.github.io"&gt;Enze Xie&lt;/a&gt;, &lt;a href="https://cs.stanford.edu/~chenlin/"&gt;Chenlin Meng&lt;/a&gt;, &lt;a href="https://www.cs.cmu.edu/~junyanz/"&gt;Jun-Yan Zhu&lt;/a&gt;, and &lt;a href="https://hanlab.mit.edu/songhan"&gt;Song Han&lt;/a&gt; &lt;br&gt; &lt;em&gt;MIT, NVIDIA, CMU, Princeton, UC Berkeley, SJTU, and Pika Labs&lt;/em&gt; &lt;br&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fdd4ab68-6489-4c65-8768-259bd866e8f8"&gt;https://github.com/user-attachments/assets/fdd4ab68-6489-4c65-8768-259bd866e8f8&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Method&lt;/h2&gt; 
&lt;h4&gt;Quantization Method -- SVDQuant&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/nunchaku-tech/cdn/resolve/main/nunchaku/assets/intuition.gif" alt="intuition"&gt;Overview of SVDQuant. Stage1: Originally, both the activation $\boldsymbol{X}$ and weights $\boldsymbol{W}$ contain outliers, making 4-bit quantization challenging. Stage 2: We migrate the outliers from activations to weights, resulting in the updated activation $\hat{\boldsymbol{X}}$ and weights $\hat{\boldsymbol{W}}$. While $\hat{\boldsymbol{X}}$ becomes easier to quantize, $\hat{\boldsymbol{W}}$ now becomes more difficult. Stage 3: SVDQuant further decomposes $\hat{\boldsymbol{W}}$ into a low-rank component $\boldsymbol{L}_1\boldsymbol{L}_2$ and a residual $\hat{\boldsymbol{W}}-\boldsymbol{L}_1\boldsymbol{L}_2$ with SVD. Thus, the quantization difficulty is alleviated by the low-rank branch, which runs at 16-bit precision.&lt;/p&gt; 
&lt;h4&gt;Nunchaku Engine Design&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/nunchaku-tech/cdn/resolve/main/nunchaku/assets/engine.jpg" alt="engine"&gt; (a) NaÃ¯vely running low-rank branch with rank 32 will introduce 57% latency overhead due to extra read of 16-bit inputs in &lt;em&gt;Down Projection&lt;/em&gt; and extra write of 16-bit outputs in &lt;em&gt;Up Projection&lt;/em&gt;. Nunchaku optimizes this overhead with kernel fusion. (b) &lt;em&gt;Down Projection&lt;/em&gt; and &lt;em&gt;Quantize&lt;/em&gt; kernels use the same input, while &lt;em&gt;Up Projection&lt;/em&gt; and &lt;em&gt;4-Bit Compute&lt;/em&gt; kernels share the same output. To reduce data movement overhead, we fuse the first two and the latter two kernels together.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/nunchaku-tech/cdn/resolve/main/nunchaku/assets/efficiency.jpg" alt="efficiency"&gt;SVDQuant reduces the 12B FLUX.1 model size by 3.6Ã— and cuts the 16-bit model's memory usage by 3.5Ã—. With Nunchaku, our INT4 model runs 3.0Ã— faster than the NF4 W4A16 baseline on both desktop and laptop NVIDIA RTX 4090 GPUs. Notably, on the laptop 4090, it achieves a total 10.1Ã— speedup by eliminating CPU offloading. Our NVFP4 model is also 3.1Ã— faster than both BF16 and NF4 on the RTX 5090 GPU.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nunchaku.tech/docs/nunchaku/installation/installation.html"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nunchaku.tech/docs/nunchaku/usage/basic_usage.html"&gt;Usage Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nunchaku-tech/ComfyUI-nunchaku"&gt;ComfyUI Plugin: ComfyUI-nunchaku&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nunchaku-tech/deepcompressor"&gt;Custom Model Quantization: DeepCompressor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nunchaku-tech/nunchaku/tree/main/app"&gt;Gradio Demo Apps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nunchaku-tech/nunchaku/main/app/flux.1/t2i"&gt;Reproduce SVDQuant Paper Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nunchaku.tech/docs/nunchaku/python_api/nunchaku.html"&gt;API Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nunchaku.tech/docs/nunchaku/developer/contribution_guide.html"&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nunchaku.tech/docs/nunchaku/faq/faq.html"&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;Please check &lt;a href="https://github.com/nunchaku-tech/nunchaku/issues/431"&gt;here&lt;/a&gt; for the roadmap for the Summer.&lt;/p&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;For enterprises interested in adopting SVDQuant or Nunchaku, including technical consulting, sponsorship opportunities, or partnership inquiries, please contact us at &lt;a href="mailto:muyangli@mit.edu"&gt;muyangli@mit.edu&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2211.02048"&gt;Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models&lt;/a&gt;, NeurIPS 2022 &amp;amp; T-PAMI 2023&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2211.10438"&gt;SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models&lt;/a&gt;, ICML 2023&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2302.04304"&gt;Q-Diffusion: Quantizing Diffusion Models&lt;/a&gt;, ICCV 2023&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2306.00978"&gt;AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration&lt;/a&gt;, MLSys 2024&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2402.19481"&gt;DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models&lt;/a&gt;, CVPR 2024&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.04532"&gt;QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving&lt;/a&gt;, MLSys 2025&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2410.10629"&gt;SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers&lt;/a&gt;, ICLR 2025&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mit-han-lab/radial-attention"&gt;Radial Attention: $O(n \log n)$ Sparse Attention with Energy Decay for Long Video Generation&lt;/a&gt;, ArXiv 2025&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find &lt;code&gt;nunchaku&lt;/code&gt; useful or relevant to your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{
  li2024svdquant,
  title={SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},
  author={Li*, Muyang and Lin*, Yujun and Zhang*, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We thank MIT-IBM Watson AI Lab, MIT and Amazon Science Hub, MIT AI Hardware Program, National Science Foundation, Packard Foundation, Dell, LG, Hyundai, and Samsung for supporting this research. We thank NVIDIA for donating the DGX server. We thank &lt;a href="https://www.first-intelligence.com/"&gt;First Intelligence&lt;/a&gt; and &lt;a href="https://www.yottalabs.ai/"&gt;Yotta Labs&lt;/a&gt; for generously sponsoring our computing resources.&lt;/p&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/GaParmar/img2img-turbo"&gt;img2img-turbo&lt;/a&gt; to train the sketch-to-image LoRA. Our text-to-image and image-to-image UI is built upon &lt;a href="https://huggingface.co/spaces/playgroundai/playground-v2.5/blob/main/app.py"&gt;playground-v.25&lt;/a&gt; and &lt;a href="https://github.com/GaParmar/img2img-turbo/raw/main/gradio_sketch2image.py"&gt;img2img-turbo&lt;/a&gt;, respectively. Our safety checker is borrowed from &lt;a href="https://github.com/mit-han-lab/hart"&gt;hart&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Nunchaku is also inspired by many open-source libraries, including (but not limited to) &lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;, &lt;a href="https://github.com/mit-han-lab/qserve"&gt;QServe&lt;/a&gt;, &lt;a href="https://github.com/mit-han-lab/llm-awq"&gt;AWQ&lt;/a&gt;, &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;FlashAttention-2&lt;/a&gt;, and &lt;a href="https://github.com/efeslab/Atom"&gt;Atom&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#nunchaku-tech/nunchaku&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=nunchaku-tech/nunchaku&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>swisskyrepo/PayloadsAllTheThings</title>
      <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
      <description>&lt;p&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Payloads All The Things&lt;/h1&gt; 
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security. Feel free to improve with your payloads and techniques !&lt;/p&gt; 
&lt;p&gt;You can also contribute with a &lt;span&gt;ğŸ»&lt;/span&gt; IRL, or using the sponsor button.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/swisskyrepo"&gt;&lt;img src="https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;link=https://github.com/sponsors/swisskyrepo" alt="Sponsor"&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/"&gt;&lt;img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" alt="Tweet"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An alternative display version is available at &lt;a href="https://swisskyrepo.github.io/PayloadsAllTheThings/"&gt;PayloadsAllTheThingsWeb&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png" alt="banner"&gt; &lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ“–&lt;/span&gt; Documentation&lt;/h2&gt; 
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;README.md - vulnerability description and how to exploit it, including several payloads&lt;/li&gt; 
 &lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt; 
 &lt;li&gt;Images - pictures for the README.md&lt;/li&gt; 
 &lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might also like the other projects from the AllTheThings family :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/InternalAllTheThings/"&gt;InternalAllTheThings&lt;/a&gt; - Active Directory and Internal Pentest Cheatsheets&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/HardwareAllTheThings/"&gt;HardwareAllTheThings&lt;/a&gt; - Hardware/IOT Pentesting Wiki&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You want more ? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/YOUTUBE.md"&gt;Youtube channel&lt;/a&gt; selections.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ§‘ğŸ’»&lt;/span&gt; Contributions&lt;/h2&gt; 
&lt;p&gt;Be sure to read &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;amp;max=36" alt="sponsors-list"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Thanks again for your contribution! &lt;span&gt;â¤ï¸&lt;/span&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ»&lt;/span&gt; Sponsors&lt;/h2&gt; 
&lt;p&gt;This project is proudly sponsored by these companies.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Logo&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://serpapi.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/34724717?s=40&amp;amp;v=4" alt="sponsor-serpapi"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SerpApi&lt;/strong&gt; is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://projectdiscovery.io/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50994705?s=40&amp;amp;v=4" alt="sponsor-projectdiscovery"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ProjectDiscovery&lt;/strong&gt; - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.vaadata.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/48131541?s=40&amp;amp;v=4" alt="sponsor-vaadata"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;VAADATA&lt;/strong&gt; - Ethical Hacking Services&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>hesreallyhim/awesome-claude-code</title>
      <link>https://github.com/hesreallyhim/awesome-claude-code</link>
      <description>&lt;p&gt;A curated list of awesome commands, files, and workflows for Claude Code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;/h1&gt; 
&lt;!-- [![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re) --&gt; 
&lt;pre style="display: inline-block; text-align: left;"&gt;
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ”    â–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ”   â–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”‚    â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”Œâ”€â”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚ â–ˆâ” â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜  â””â”€â”€â”€â”€â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜
â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ–ˆâ”Œâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ”‚ â””â”€â”˜ â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â””â”€â”˜  â””â”€â”˜ â””â”€â”€â”˜â””â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”˜     â””â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜

 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ”   â–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜    â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”Œâ”€â”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜
â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”      â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜      â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜
â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”    â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
 â””â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”˜  â””â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
&lt;/pre&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;p&gt;&lt;a href="https://awesome.re"&gt;&lt;img src="https://awesome.re/badge-flat2.svg?sanitize=true" alt="Awesome"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;Awesome Claude Code&lt;/a&gt; ğŸ¤ &lt;a href="https://github.com/hesreallyhim/awesome-claude-code-agents"&gt;Awesome Claude Code Agents&lt;/a&gt;&lt;/h1&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;p&gt;This is a curated list of slash-commands, &lt;code&gt;CLAUDE.md&lt;/code&gt; files, CLI tools, and other resources and guides for enhancing your &lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;Claude Code&lt;/a&gt; workflow, productivity, and vibes.&lt;/p&gt; 
&lt;!--lint enable double-link--&gt; 
&lt;p&gt;Claude Code is a cutting-edge CLI-based coding assistant and agent that you can access in your terminal or IDE. It is a rapidly evolving tool that offers a number of powerful capabilities, and allows for a lot of configuration, in a lot of different ways. Users are actively working out best practices and workflows. It is the hope that this repo will help the community share knowledge and understand how to get the most out of Claude Code.&lt;/p&gt; 
&lt;h3&gt;Announcements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-07-26 - Anthropic done done it again, and Claude Code now has another super-power in its arsenal - custom user Sub Agents! I was thinking I might add a little section at the bottom about this "agent" thing, but then I tried it, and I think they're amazing, and probably deserve their own repo, so come and check out &lt;a href="https://github.com/hesreallyhim/awesome-claude-code-agents"&gt;awesome-claude-code-agents&lt;/a&gt; and get those submissions rolling in. Can't wait to see what people are getting up to already with this new tech.&lt;/li&gt; 
 &lt;li&gt;2025-07-25 - The new-new submission workflow is up now, I've managed to make it about 4-5 times more complicated than it should be ğŸ˜œ so check out &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; if you'd like to submit a new resource.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;p&gt;â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#workflows--knowledge-guides-"&gt;Workflows &amp;amp; Knowledge Guides&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#tooling-"&gt;Tooling&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ide-integrations"&gt;IDE Integrations&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#hooks-"&gt;Hooks&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#slash-commands-"&gt;Slash-Commands&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#version-control--git"&gt;Version Control &amp;amp; Git&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#code-analysis--testing"&gt;Code Analysis &amp;amp; Testing&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#context-loading--priming"&gt;Context Loading &amp;amp; Priming&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#documentation--changelogs"&gt;Documentation &amp;amp; Changelogs&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ci--deployment"&gt;CI / Deployment&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project--task-management"&gt;Project &amp;amp; Task Management&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#miscellaneous"&gt;Miscellaneous&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#claudemd-files-"&gt;CLAUDE.md Files&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#language-specific"&gt;Language-Specific&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#domain-specific"&gt;Domain-Specific&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project-scaffolding--mcp"&gt;Project Scaffolding &amp;amp; MCP&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#official-documentation-"&gt;Official Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Workflows &amp;amp; Knowledge Guides ğŸ§ &lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A &lt;strong&gt;workflow&lt;/strong&gt; is a tightly coupled set of Claude Code-native resources that facilitate specific projects&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/cloudartisan/cloudartisan.github.io/tree/main/.claude/commands"&gt;&lt;code&gt;Blogging Platform Instructions&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/cloudartisan"&gt;cloudartisan&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;CC-BY-SA-4.0&lt;br&gt; Provides a well-structured set of commands for publishing and maintaining a blogging platform, including commands for creating posts, managing categories, and handling media files.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://claudelog.com"&gt;&lt;code&gt;ClaudeLog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://www.reddit.com/user/inventor_black/"&gt;InventorBlack&lt;/a&gt;&lt;br&gt; A comprehensive knowledge base with detailed breakdowns of advanced &lt;a href="https://claudelog.com/mechanics/you-are-the-main-thread/"&gt;mechanics&lt;/a&gt; including &lt;a href="https://claudelog.com/mechanics/claude-md-supremacy"&gt;CLAUDE.md best practices&lt;/a&gt;, practical technique guides like &lt;a href="https://claudelog.com/mechanics/plan-mode"&gt;plan mode&lt;/a&gt;, &lt;a href="https://claudelog.com/faqs/what-is-ultrathink/"&gt;ultrathink&lt;/a&gt;, &lt;a href="https://claudelog.com/mechanics/task-agent-tools/"&gt;sub-agents&lt;/a&gt;, &lt;a href="https://claudelog.com/mechanics/agent-first-design/"&gt;agent-first design&lt;/a&gt; and &lt;a href="https://claudelog.com/configuration"&gt;configuration guides&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/tree/main/.claude/commands"&gt;&lt;code&gt;Context Priming&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br&gt; Provides a systematic approach to priming Claude Code with comprehensive project context through specialized commands for different project scenarios and development contexts.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/tree/main/.claude/commands"&gt;&lt;code&gt;n8n_agent&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br&gt; Amazing comprehensive set of comments for code analysis, QA, design, documentation, project structure, project management, optimization, and many more.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/tree/main/.claude/commands"&gt;&lt;code&gt;Project Bootstrapping and Task Management&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br&gt; Provides a structured set of commands for bootstrapping and managing a new project, including meta-commands for creating and editing custom slash-commands.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/scopecraft/command/tree/main/.claude/commands"&gt;&lt;code&gt;Project Management, Implementation, Planning, and Release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br&gt; Really comprehensive set of commands for all aspects of SDLC.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/harperreed/dotfiles/tree/master/.claude/commands"&gt;&lt;code&gt;Project Workflow System&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/harperreed"&gt;harperreed&lt;/a&gt;&lt;br&gt; A set of commands that provide a comprehensive workflow system for managing projects, including task management, code review, and deployment processes.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://diwank.space/field-notes-from-shipping-real-code-with-claude"&gt;&lt;code&gt;Shipping Real Code w/ Claude&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/creatorrr"&gt;Diwank&lt;/a&gt;&lt;br&gt; A detailed blog post explaining the author's process for shipping a product with Claude Code, including CLAUDE.md files and other interesting resources.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Helmi/claude-simone"&gt;&lt;code&gt;Simone&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Helmi"&gt;Helmi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A broader project management workflow for Claude Code that encompasses not just a set of commands, but a system of documents, guidelines, and processes to facilitate project planning and execution.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/wcygan/dotfiles/tree/d8ab6b9f5a7a81007b7f5fa3025d4f83ce12cc02/claude/commands"&gt;&lt;code&gt;Slash-commands megalist&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/wcygan"&gt;wcygan&lt;/a&gt;&lt;br&gt; A pretty stunning list (88 at the time of this post!) of slash-commands ranging from agent orchestration, code review, project management, security, documentation, self-assessment, almost anything you can dream of.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Tooling ğŸ§°&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tooling&lt;/strong&gt; denotes applications that are built on top of Claude Code and consist of more components than slash-commands and &lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/ryoppippi/ccusage"&gt;&lt;code&gt;CC Usage&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ryoppippi"&gt;ryoppippi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Handy CLI tool for managing and analyzing Claude Code usage, based on analyzing local Claude Code logs. Presents a nice dashboard regarding cost information, token consumption, etc.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/nyatinte/ccexp"&gt;&lt;code&gt;ccexp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nyatinte"&gt;nyatinte&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Interactive CLI tool for discovering and managing Claude Code configuration files and slash commands with a beautiful terminal UI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ruvnet/claude-code-flow"&gt;&lt;code&gt;Claude Code Flow&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ruvnet"&gt;ruvnet&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; This mode serves as a code-first orchestration layer, enabling Claude to write, edit, test, and optimize code autonomously across recursive agent cycles.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor"&gt;&lt;code&gt;Claude Code Usage Monitor&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Maciek-roboblog"&gt;Maciek-roboblog&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A real-time terminal-based tool for monitoring Claude Code token usage. It shows live token consumption, burn rate, and predictions for token depletion. Features include visual progress bars, session-aware analytics, and support for multiple subscription plans.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/possibilities/claude-composer"&gt;&lt;code&gt;Claude Composer&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/possibilities"&gt;Mike Bannister&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Unlicense&lt;br&gt; A tool that adds small enhancements to Claude Code.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/claude-did-this/claude-hub"&gt;&lt;code&gt;Claude Hub&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/claude-did-this"&gt;Claude Did This&lt;/a&gt;&lt;br&gt; A webhook service that connects Claude Code to GitHub repositories, enabling AI-powered code assistance directly through pull requests and issues. This integration allows Claude to analyze repositories, answer technical questions, and help developers understand and improve their codebase through simple @mentions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/smtg-ai/claude-squad"&gt;&lt;code&gt;Claude Squad&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/smtg-ai"&gt;smtg-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Claude Squad is a terminal app that manages multiple Claude Code, Codex (and other local agents including Aider) in separate workspaces, allowing you to work on multiple tasks simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/parruda/claude-swarm"&gt;&lt;code&gt;Claude Swarm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/parruda"&gt;parruda&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Launch Claude Code session that is connected to a swarm of Claude Code Agents.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/eyaltoledano/claude-task-master"&gt;&lt;code&gt;Claude Task Master&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eyaltoledano"&gt;eyaltoledano&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; A task management system for AI-driven development with Claude, designed to work seamlessly with Cursor AI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-task-runner"&gt;&lt;code&gt;Claude Task Runner&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt;&lt;br&gt; A specialized tool to manage context isolation and focused task execution with Claude Code, solving the critical challenge of context length limitations and task focus when working with Claude on complex, multi-step projects.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/dagger/container-use"&gt;&lt;code&gt;Container Use&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/dagger"&gt;dagger&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Development environments for coding agents. Enable multiple agents to work safely and independently with your preferred stack.&lt;/p&gt; 
&lt;h3&gt;IDE Integrations&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=AndrePimenta.claude-code-chat"&gt;&lt;code&gt;Claude Code Chat&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/andrepimenta"&gt;andrepimenta&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Â©&lt;br&gt; An elegant and user-friendly Claude Code chat interface for VS Code.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/stevemolitor/claude-code.el"&gt;&lt;code&gt;claude-code.el&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stevemolitor"&gt;stevemolitor&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; An Emacs interface for Claude Code CLI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/greggh/claude-code.nvim"&gt;&lt;code&gt;claude-code.nvim&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/greggh"&gt;greggh&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A seamless integration between Claude Code AI assistant and Neovim.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/stravu/crystal"&gt;&lt;code&gt;crystal&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stravu"&gt;stravu&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A full-fledged desktop application for orchestrating, monitoring, and interacting with Claude Code agents.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Hooks ğŸª&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Hooks&lt;/strong&gt; are a brand new API for Claude Code that allows users to activate commands and run scripts at different points in Claude's agentic lifecycle.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;[Experimental]&lt;/strong&gt; - The resources listed in this section have not been fully vetted and may not work as expected, given the bleeding-edge nature of Claude Code hooks. Nevertheless, I wished to include them at least as a source of inspiration and to explore this unknown terrain. YMMV!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/beyondcode/claude-hooks-sdk"&gt;&lt;code&gt;claude-code-hooks-sdk&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/beyondcode"&gt;beyondcode&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A Laravel-inspired PHP SDK for building Claude Code hook responses with a clean, fluent API. This SDK makes it easy to create structured JSON responses for Claude Code hooks using an expressive, chainable interface.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/johnlindquist/claude-hooks"&gt;&lt;code&gt;claude-hooks&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/johnlindquist"&gt;John Lindquist&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A TypeScript-based system for configuring and customizing Claude Code hooks with a powerful and flexible interface.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Veraticus/nix-config/tree/main/home-manager/claude-code/hooks"&gt;&lt;code&gt;Linting, testing, and notifications (in go)&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Veraticus"&gt;Josh Symonds&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Nice set of hooks for enforcing code quality (linting, testing, notifications), with a nice configuration setup as well.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/nizos/tdd-guard"&gt;&lt;code&gt;TDD Guard&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nizos"&gt;Nizar Selander&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A hooks-driven system that monitors file operations in real-time and blocks changes that violate TDD principles.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Slash-Commands ğŸ”ª&lt;/h2&gt; 
&lt;h3&gt;Version Control &amp;amp; Git&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/danielscholl/mvn-mcp-server/raw/main/.claude/commands/bug-fix.md"&gt;&lt;code&gt;/bug-fix&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/danielscholl"&gt;danielscholl&lt;/a&gt;&lt;br&gt; Streamlines bug fixing by creating a GitHub issue first, then a feature branch for implementing and thoroughly testing the solution before merging.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/commit.md"&gt;&lt;code&gt;/commit&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates git commits using conventional commit format with appropriate emojis, following project standards and creating descriptive messages that explain the purpose of changes.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/.claude/commands/2-commit-fast.md"&gt;&lt;code&gt;/commit-fast&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br&gt; Automates git commit process by selecting the first suggested message, generating structured commits with consistent formatting while skipping manual confirmation and removing Claude co-Contributorship footer&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/toyamarinyon/giselle/raw/main/.claude/commands/create-pr.md"&gt;&lt;code&gt;/create-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/toyamarinyon"&gt;toyamarinyon&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Streamlines pull request creation by handling the entire workflow: creating a new branch, committing changes, formatting modified files with Biome, and submitting the PR.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/liam-hq/liam/raw/main/.claude/commands/create-pull-request.md"&gt;&lt;code&gt;/create-pull-request&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/liam-hq"&gt;liam-hq&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Provides comprehensive PR creation guidance with GitHub CLI, enforcing title conventions, following template structure, and offering concrete command examples with best practices.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/create-worktrees.md"&gt;&lt;code&gt;/create-worktrees&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates git worktrees for all open PRs or specific branches, handling branches with slashes, cleaning up stale worktrees, and supporting custom branch creation for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/jeremymailen/kotlinter-gradle/raw/master/.claude/commands/fix-github-issue.md"&gt;&lt;code&gt;/fix-github-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jeremymailen"&gt;jeremymailen&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Analyzes and fixes GitHub issues using a structured approach with GitHub CLI for issue details, implementing necessary code changes, running tests, and creating proper commit messages.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-issue.md"&gt;&lt;code&gt;/fix-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Addresses GitHub issues by taking issue number as parameter, analyzing context, implementing solution, and testing/validating the fix for proper integration.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-pr.md"&gt;&lt;code&gt;/fix-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Fetches and fixes unresolved PR comments by automatically retrieving feedback, addressing reviewer concerns, making targeted code improvements, and streamlining the review process.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/husky.md"&gt;&lt;code&gt;/husky&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Sets up and manages Husky Git hooks by configuring pre-commit hooks, establishing commit message standards, integrating with linting tools, and ensuring code quality on commits.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/arkavo-org/opentdf-rs/raw/main/.claude/commands/pr-review.md"&gt;&lt;code&gt;/pr-review&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/arkavo-org"&gt;arkavo-org&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Reviews pull request changes to provide feedback, check for issues, and suggest improvements before merging into the main codebase.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/.claude/commands/update-branch-name.md"&gt;&lt;code&gt;/update-branch-name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Updates branch names with proper prefixes and formats, enforcing naming conventions, supporting semantic prefixes, and managing remote branch updates.&lt;/p&gt; 
&lt;h3&gt;Code Analysis &amp;amp; Testing&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/rygwdn/slack-tools/raw/main/.claude/commands/check.md"&gt;&lt;code&gt;/check&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rygwdn"&gt;rygwdn&lt;/a&gt;&lt;br&gt; Performs comprehensive code quality and security checks, featuring static analysis integration, security vulnerability scanning, code style enforcement, and detailed reporting.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Graphlet-AI/eridu/raw/main/.claude/commands/clean.md"&gt;&lt;code&gt;/clean&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Graphlet-AI"&gt;Graphlet-AI&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Addresses code formatting and quality issues by fixing black formatting problems, organizing imports with isort, resolving flake8 linting issues, and correcting mypy type errors.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/raw/main/.claude/commands/code_analysis.md"&gt;&lt;code&gt;/code_analysis&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br&gt; Provides a menu of advanced code analysis commands for deep inspection, including knowledge graph generation, optimization suggestions, and quality evaluation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/to4iki/ai-project-rules/raw/main/.claude/commands/optimize.md"&gt;&lt;code&gt;/optimize&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/to4iki"&gt;to4iki&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Analyzes code performance to identify bottlenecks, proposing concrete optimizations with implementation guidance for improved application performance.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rzykov/metabase/raw/master/.claude/commands/repro-issue.md"&gt;&lt;code&gt;/repro-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rzykov"&gt;rzykov&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Creates reproducible test cases for GitHub issues, ensuring tests fail reliably and documenting clear reproduction steps for developers.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/zscott/pane/raw/main/.claude/commands/tdd.md"&gt;&lt;code&gt;/tdd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zscott"&gt;zscott&lt;/a&gt;&lt;br&gt; Guides development using Test-Driven Development principles, enforcing Red-Green-Refactor discipline, integrating with git workflow, and managing PR creation.&lt;/p&gt; 
&lt;h3&gt;Context Loading &amp;amp; Priming&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/elizaOS/elizaos.github.io/raw/main/.claude/commands/context-prime.md"&gt;&lt;code&gt;/context-prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/elizaOS"&gt;elizaOS&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Primes Claude with comprehensive project understanding by loading repository structure, setting development context, establishing project goals, and defining collaboration parameters.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/okuvshynov/cubestat/raw/main/.claude/commands/initref.md"&gt;&lt;code&gt;/initref&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/okuvshynov"&gt;okuvshynov&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Initializes reference documentation structure with standard doc templates, API reference setup, documentation conventions, and placeholder content generation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ethpandaops/xatu-data/raw/master/.claude/commands/load-llms-txt.md"&gt;&lt;code&gt;/load-llms-txt&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ethpandaops"&gt;ethpandaops&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Loads LLM configuration files to context, importing specific terminology, model configurations, and establishing baseline terminology for AI discussions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_coo_context.md"&gt;&lt;code&gt;/load_coo_context&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; References specific files for sparse matrix operations, explains transform usage, compares with previous approaches, and sets data formatting context for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_dango_pipeline.md"&gt;&lt;code&gt;/load_dango_pipeline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; Sets context for model training by referencing pipeline files, establishing working context, and preparing for pipeline work with relevant documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/yzyydev/AI-Engineering-Structure/raw/main/.claude/commands/prime.md"&gt;&lt;code&gt;/prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/yzyydev"&gt;yzyydev&lt;/a&gt;&lt;br&gt; Sets up initial project context by viewing directory structure and reading key files, creating standardized context with directory visualization and key documentation focus.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ddisisto/si/raw/main/.claude/commands/rsi.md"&gt;&lt;code&gt;/rsi&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ddisisto"&gt;ddisisto&lt;/a&gt;&lt;br&gt; Reads all commands and key project files to optimize AI-assisted development by streamlining the process, loading command context, and setting up for better development workflow.&lt;/p&gt; 
&lt;h3&gt;Documentation &amp;amp; Changelogs&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/berrydev-ai/blockdoc-python/raw/main/.claude/commands/add-to-changelog.md"&gt;&lt;code&gt;/add-to-changelog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/berrydev-ai"&gt;berrydev-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Adds new entries to changelog files while maintaining format consistency, properly documenting changes, and following established project standards for version tracking.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/tree/feature/issue-227-ai-suggestions/.claude/commands/analyze-issue.md"&gt;&lt;code&gt;/create-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Analyzes code structure and purpose to create comprehensive documentation detailing inputs/outputs, behavior, user interaction flows, and edge cases with error handling.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/slunsford/coffee-analytics/raw/main/.claude/commands/docs.md"&gt;&lt;code&gt;/docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/slunsford"&gt;slunsford&lt;/a&gt;&lt;br&gt; Generates comprehensive documentation that follows project structure, documenting APIs and usage patterns with consistent formatting for better user understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/explain-issue-fix.md"&gt;&lt;code&gt;/explain-issue-fix&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br&gt; Documents solution approaches for GitHub issues, explaining technical decisions, detailing challenges overcome, and providing implementation context for better understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Consiliency/Flutter-Structurizr/raw/main/.claude/commands/update-docs.md"&gt;&lt;code&gt;/update-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Consiliency"&gt;Consiliency&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Reviews current documentation status, updates implementation progress, reviews phase documents, and maintains documentation consistency across the project.&lt;/p&gt; 
&lt;h3&gt;CI / Deployment&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/kelp/webdown/raw/main/.claude/commands/release.md"&gt;&lt;code&gt;/release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kelp"&gt;kelp&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Manages software releases by updating changelogs, reviewing README changes, evaluating version increments, and documenting release changes for better version tracking.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/run-ci.md"&gt;&lt;code&gt;/run-ci&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br&gt; Activates virtual environments, runs CI-compatible check scripts, iteratively fixes errors, and ensures all tests pass before completion.&lt;/p&gt; 
&lt;h3&gt;Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/scopecraft/command/raw/main/.claude/commands/create-command.md"&gt;&lt;code&gt;/create-command&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br&gt; Guides Claude through creating new custom commands with proper structure by analyzing requirements, templating commands by category, enforcing command standards, and creating supporting documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-jtbd.md"&gt;&lt;code&gt;/create-jtbd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Creates Jobs-to-be-Done frameworks that outline user needs with structured format, focusing on specific user problems and organizing by job categories for product development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-prd.md"&gt;&lt;code&gt;/create-prd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Generates comprehensive product requirement documents outlining detailed specifications, requirements, and features following standardized document structure and format.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Wirasm/claudecode-utils/raw/main/.claude/commands/create-prp.md"&gt;&lt;code&gt;/create-prp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Wirasm"&gt;Wirasm&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates product requirement plans by reading PRP methodology, following template structure, creating comprehensive requirements, and structuring product definitions for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/raw/main/.claude/commands/project_hello_w_name.md"&gt;&lt;code&gt;/project_hello_w_name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br&gt; Creates customizable greeting components with name input, demonstrating argument passing, component reusability, state management, and user input handling.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/chrisleyva/todo-slash-command/raw/main/todo.md"&gt;&lt;code&gt;/todo&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/chrisleyva"&gt;chrisleyva&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A convenient command to quickly manage project todo items without leaving the Claude Code interface, featuring due dates, sorting, task prioritization, and comprehensive todo list management.&lt;/p&gt; 
&lt;h3&gt;Miscellaneous&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/TuckerTucker/tkr-portfolio/raw/main/.claude/commands/five.md"&gt;&lt;code&gt;/five&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/TuckerTucker"&gt;TuckerTucker&lt;/a&gt;&lt;br&gt; Applies the "five whys" methodology to perform root cause analysis, identify underlying issues, and create solution approaches for complex problems.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/fixing_go_in_graph.md"&gt;&lt;code&gt;/fixing_go_in_graph&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; Focuses on Gene Ontology annotation integration in graph databases, handling multiple data sources, addressing graph representation issues, and ensuring correct data incorporation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/GaloyMoney/lana-bank/raw/main/.claude/commands/mermaid.md"&gt;&lt;code&gt;/mermaid&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/GaloyMoney"&gt;GaloyMoney&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Generates Mermaid diagrams from SQL schema files, creating entity relationship diagrams with table properties, validating diagram compilation, and ensuring complete entity coverage.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/review_dcell_model.md"&gt;&lt;code&gt;/review_dcell_model&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; Reviews old Dcell implementation files, comparing with newer Dango model, noting changes over time, and analyzing refactoring approaches for better code organization.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/zuplo/docs/raw/main/.claude/commands/use-stepper.md"&gt;&lt;code&gt;/use-stepper&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zuplo"&gt;zuplo&lt;/a&gt;&lt;br&gt; Reformats documentation to use React Stepper component, transforming heading formats, applying proper indentation, and maintaining markdown compatibility with admonition formatting.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;CLAUDE.md Files ğŸ“‚&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/strong&gt; are files that contain important guidelines and context-specfic information or instructions that help Claude Code to better understand your project and your coding standards&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Language-Specific&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/didalgolab/ai-intellij-plugin/raw/main/CLAUDE.md"&gt;&lt;code&gt;AI IntelliJ Plugin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/didalgolab"&gt;didalgolab&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Provides comprehensive Gradle commands for IntelliJ plugin development with platform-specific coding patterns, detailed package structure guidelines, and clear internationalization standards.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/alexei-led/aws-mcp-server/raw/main/CLAUDE.md"&gt;&lt;code&gt;AWS MCP Server&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/alexei-led"&gt;alexei-led&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Features multiple Python environment setup options with detailed code style guidelines, comprehensive error handling recommendations, and security considerations for AWS CLI interactions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/touchlab/DroidconKotlin/raw/main/CLAUDE.md"&gt;&lt;code&gt;DroidconKotlin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/touchlab"&gt;touchlab&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Delivers comprehensive Gradle commands for cross-platform Kotlin Multiplatform development with clear module structure and practical guidance for dependency injection.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/expectedparrot/edsl/raw/main/CLAUDE.md"&gt;&lt;code&gt;EDSL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/expectedparrot"&gt;expectedparrot&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Offers detailed build and test commands with strict code style enforcement, comprehensive testing requirements, and standardized development workflow using Black and mypy.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/CLAUDE.md"&gt;&lt;code&gt;Giselle&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Provides detailed build and test commands using pnpm and Vitest with strict code formatting requirements and comprehensive naming conventions for code consistency.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hashintel/hash/raw/main/CLAUDE.md"&gt;&lt;code&gt;HASH&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hashintel"&gt;hashintel&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Features comprehensive repository structure breakdown with strong emphasis on coding standards, detailed Rust documentation guidelines, and systematic PR review process.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/inkline/inkline/raw/main/CLAUDE.md"&gt;&lt;code&gt;Inkline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/inkline"&gt;inkline&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Structures development workflow using pnpm with emphasis on TypeScript and Vue 3 Composition API, detailed component creation process, and comprehensive testing recommendations.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/mattgodbolt/jsbeeb/raw/main/CLAUDE.md"&gt;&lt;code&gt;JSBeeb&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/mattgodbolt"&gt;mattgodbolt&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br&gt; Provides development guide for JavaScript BBC Micro emulator with build and testing instructions, architecture documentation, and debugging workflows.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/LamoomAI/lamoom-python/raw/main/CLAUDE.md"&gt;&lt;code&gt;Lamoom Python&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/LamoomAI"&gt;LamoomAI&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Serves as reference for production prompt engineering library with load balancing of AI Models, API documentation, and usage patterns with examples.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/langchain-ai/langgraphjs/raw/main/CLAUDE.md"&gt;&lt;code&gt;LangGraphJS&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/langchain-ai"&gt;langchain-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Offers comprehensive build and test commands with detailed TypeScript style guidelines, layered library architecture, and monorepo structure using yarn workspaces.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/CLAUDE.md"&gt;&lt;code&gt;Metabase&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Details workflow for REPL-driven development in Clojure/ClojureScript with emphasis on incremental development, testing, and step-by-step approach for feature implementation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgcarstrends/backend/raw/main/CLAUDE.md"&gt;&lt;code&gt;SG Cars Trends Backend&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/sgcarstrends"&gt;sgcarstrends&lt;/a&gt;&lt;br&gt; Provides comprehensive structure for TypeScript monorepo projects with detailed commands for development, testing, deployment, and AWS/Cloudflare integration.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/spylang/spy/raw/main/CLAUDE.md"&gt;&lt;code&gt;SPy&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/spylang"&gt;spylang&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Enforces strict coding conventions with comprehensive testing guidelines, multiple code compilation options, and backend-specific test decorators for targeted filtering.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KarpelesLab/tpl/raw/master/CLAUDE.md"&gt;&lt;code&gt;TPL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/KarpelesLab"&gt;KarpelesLab&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Details Go project conventions with comprehensive error handling recommendations, table-driven testing approach guidelines, and modernization suggestions for latest Go features.&lt;/p&gt; 
&lt;h3&gt;Domain-Specific&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/Layr-Labs/avs-vibe-developer-guide/raw/master/CLAUDE.md"&gt;&lt;code&gt;AVS Vibe Developer Guide&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Layr-Labs"&gt;Layr-Labs&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Structures AI-assisted EigenLayer AVS development workflow with consistent naming conventions for prompt files and established terminology standards for blockchain concepts.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/CommE2E/comm/raw/master/CLAUDE.md"&gt;&lt;code&gt;Comm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/CommE2E"&gt;CommE2E&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;BSD-3-Clause&lt;br&gt; Serves as a development reference for E2E-encrypted messaging applications with code organization architecture, security implementation details, and testing procedures.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/badass-courses/course-builder/raw/main/CLAUDE.md"&gt;&lt;code&gt;Course Builder&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/badass-courses"&gt;badass-courses&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Enables real-time multiplayer capabilities for collaborative course creation with diverse tech stack integration and monorepo architecture using Turborepo.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/eastlondoner/cursor-tools/raw/main/CLAUDE.md"&gt;&lt;code&gt;Cursor Tools&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eastlondoner"&gt;eastlondoner&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates a versatile AI command interface supporting multiple providers and models with flexible command options and browser automation through "Stagehand" feature.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/soramimi/Guitar/raw/master/CLAUDE.md"&gt;&lt;code&gt;Guitar&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/soramimi"&gt;soramimi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-2.0&lt;br&gt; Serves as development guide for Guitar Git GUI Client with build commands for various platforms, code style guidelines for contributing, and project structure explanation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Fimeg/NetworkChronicles/raw/legacy-v1/CLAUDE.md"&gt;&lt;code&gt;Network Chronicles&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Fimeg"&gt;Fimeg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Presents detailed implementation plan for AI-driven game characters with technical specifications for LLM integration, character guidelines, and service discovery mechanics.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/different-ai/note-companion/raw/master/CLAUDE.md"&gt;&lt;code&gt;Note Companion&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/different-ai"&gt;different-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Provides detailed styling isolation techniques for Obsidian plugins using Tailwind with custom prefix to prevent style conflicts and practical troubleshooting steps.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ParetoSecurity/pareto-mac/raw/main/CLAUDE.md"&gt;&lt;code&gt;Pareto Mac&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ParetoSecurity"&gt;ParetoSecurity&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br&gt; Serves as development guide for Mac security audit tool with build instructions, contribution guidelines, testing procedures, and workflow documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/CLAUDE.md"&gt;&lt;code&gt;SteadyStart&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br&gt; Clear and direct instructives about style, permissions, Claude's "role", communications, and documentation of Claude Code sessions for other team members to stay abreast.&lt;/p&gt; 
&lt;h3&gt;Project Scaffolding &amp;amp; MCP&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/basicmachines-co/basic-memory/raw/main/CLAUDE.md"&gt;&lt;code&gt;Basic Memory&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/basicmachines-co"&gt;basicmachines-co&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Presents an innovative AI-human collaboration framework with Model Context Protocol for bidirectional LLM-markdown communication and flexible knowledge structure for complex projects.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-code-mcp-enhanced/raw/main/CLAUDE.md"&gt;&lt;code&gt;claude-code-mcp-enhanced&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Provides detailed and emphatic instructions for Claude to follow as a coding agent, with testing guidance, code examples, and compliance checks.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Family-IT-Guy/perplexity-mcp/raw/main/CLAUDE.md"&gt;&lt;code&gt;Perplexity MCP&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Family-IT-Guy"&gt;Family-IT-Guy&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;ISC&lt;br&gt; Offers clear step-by-step installation instructions with multiple configuration options, detailed troubleshooting guidance, and concise architecture overview of the MCP protocol.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Official Documentation ğŸ›ï¸&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Links to some of Anthropic's terrific documentation and resources regarding Claude Code&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;p&gt;&lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;&lt;code&gt;Anthropic Documentation&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Â©&lt;br&gt; The official documentation for Claude Code, including installation instructions, usage guidelines, API references, tutorials, examples, loads of information that I won't list individually. Like Claude Code, the documentation is frequently updated.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/anthropics/anthropic-quickstarts/raw/main/CLAUDE.md"&gt;&lt;code&gt;Anthropic Quickstarts&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Offers comprehensive development guides for three distinct AI-powered demo projects with standardized workflows, strict code style guidelines, and containerization instructions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/anthropics/claude-code-action/tree/main/examples"&gt;&lt;code&gt;Claude Code GitHub Actions&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Official GitHub Actions integration for Claude Code with examples and documentation for automating AI-powered workflows in CI/CD pipelines.&lt;/p&gt; 
&lt;h2&gt;Contributing ğŸŒ»&lt;/h2&gt; 
&lt;p&gt;Please note that this project is released with a &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/code-of-conduct.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating in this project you agree to abide by its terms.&lt;/p&gt; 
&lt;p&gt;Regarding content, we especially welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Proven, effective resources that follow best practices and may even be in use in production.&lt;/li&gt; 
 &lt;li&gt;Innovative, creative, or experimental workflows that perhaps are still being iterated upon, but have high potential value, and push the boundaries of Claude Code's documented capabilities and use cases.&lt;/li&gt; 
 &lt;li&gt;Additional libraries and tooling that are built on top of Claude Code and offer enhanced functionality.&lt;/li&gt; 
 &lt;li&gt;Applications of Claude Code outside of the traditional "coding assistant" context, e.g., CI/CD integration, testing, documentation, dev-ops, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information on how to contribute to this project. Or, fire up Claude Code and invoke the &lt;code&gt;/project:add-new-resource&lt;/code&gt; command and let Claude walk you through it!&lt;/p&gt; 
&lt;p&gt;If you have any suggestions or thoughts on how to improve the repo, or how to best organize the list, feel free to start a Discussion topic. This is meant to be for the Claude Code community, and in general I prefer not to act on sole authority.&lt;/p&gt; 
&lt;h3&gt;A note about licenses&lt;/h3&gt; 
&lt;p&gt;Because simply listing a hyperlink does not qualify as redistribution, the license of the original source is not relevant to its inclusion. However, for posterity and convenience, we do host copies of all resources whose license permits it. Therefore, please include information about the resource's license. Additionally, take note: &lt;em&gt;if you do not include a LICENSE in your GitHub repo, then by default it is fully copyrighted and redistribution is not allowed&lt;/em&gt;. So, if you are intending to make an open source project, it's critical to pick from one of the many available open source licenses. This is just a reminder that without a LICENSE, your project is not open source (it's merely source-code-available) - it may of course still be included on this list, but this notice is to inform readers about the default rules regarding GitHub and LICENSE files. See &lt;a href="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository"&gt;here&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>roboflow/supervision</title>
      <link>https://github.com/roboflow/supervision</link>
      <description>&lt;p&gt;We write your reusable computer vision tools. ğŸ’œ&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://supervision.roboflow.com"&gt; &lt;img width="100%" src="https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529"&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;a href="https://github.com/roboflow/notebooks"&gt;notebooks&lt;/a&gt; | &lt;a href="https://github.com/roboflow/inference"&gt;inference&lt;/a&gt; | &lt;a href="https://github.com/autodistill/autodistill"&gt;autodistill&lt;/a&gt; | &lt;a href="https://github.com/roboflow/multimodal-maestro"&gt;maestro&lt;/a&gt;&lt;/p&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/supervision"&gt;&lt;img src="https://badge.fury.io/py/supervision.svg?sanitize=true" alt="version"&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/supervision"&gt;&lt;img src="https://img.shields.io/pypi/dm/supervision" alt="downloads"&gt;&lt;/a&gt; &lt;a href="https://snyk.io/advisor/python/supervision"&gt;&lt;img src="https://snyk.io/advisor/python/supervision/badge.svg?sanitize=true" alt="snyk"&gt;&lt;/a&gt; &lt;a href="https://github.com/roboflow/supervision/raw/main/LICENSE.md"&gt;&lt;img src="https://img.shields.io/pypi/l/supervision" alt="license"&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/supervision"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/supervision" alt="python-version"&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="colab"&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/Roboflow/Annotators"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="gradio"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/GbfgXGJ8Bk"&gt;&lt;img src="https://img.shields.io/discord/1159501506232451173?logo=discord&amp;amp;label=discord&amp;amp;labelColor=fff&amp;amp;color=5865f2&amp;amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk" alt="discord"&gt;&lt;/a&gt; &lt;a href="https://squidfunk.github.io/mkdocs-material/"&gt;&lt;img src="https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&amp;amp;logoColor=white" alt="built-with-material-for-mkdocs"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/124" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/124" alt="roboflow%2Fsupervision | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ‘‹ hello&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We write your reusable computer vision tools.&lt;/strong&gt; Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! ğŸ¤&lt;/p&gt; 
&lt;h2&gt;ğŸ’» install&lt;/h2&gt; 
&lt;p&gt;Pip install the supervision package in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.9&lt;/strong&gt;&lt;/a&gt; environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install supervision
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more about conda, mamba, and installing from source in our &lt;a href="https://roboflow.github.io/supervision/"&gt;guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ”¥ quickstart&lt;/h2&gt; 
&lt;h3&gt;models&lt;/h3&gt; 
&lt;p&gt;Supervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created &lt;a href="https://supervision.roboflow.com/latest/detection/core/#detections"&gt;connectors&lt;/a&gt; for the most popular libraries like Ultralytics, Transformers, or MMDetection.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import cv2
import supervision as sv
from ultralytics import YOLO

image = cv2.imread(...)
model = YOLO("yolov8s.pt")
result = model(image)[0]
detections = sv.Detections.from_ultralytics(result)

len(detections)
# 5
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ‘‰ more model connectors&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;inference&lt;/p&gt; &lt;p&gt;Running with &lt;a href="https://github.com/roboflow/inference"&gt;Inference&lt;/a&gt; requires a &lt;a href="https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key"&gt;Roboflow API KEY&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;import cv2
import supervision as sv
from inference import get_model

image = cv2.imread(...)
model = get_model(model_id="yolov8s-640", api_key=&amp;lt;ROBOFLOW API KEY&amp;gt;)
result = model.infer(image)[0]
detections = sv.Detections.from_inference(result)

len(detections)
# 5
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;annotators&lt;/h3&gt; 
&lt;p&gt;Supervision offers a wide range of highly customizable &lt;a href="https://supervision.roboflow.com/latest/detection/annotators/"&gt;annotators&lt;/a&gt;, allowing you to compose the perfect visualization for your use case.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import cv2
import supervision as sv

image = cv2.imread(...)
detections = sv.Detections(...)

box_annotator = sv.BoxAnnotator()
annotated_frame = box_annotator.annotate(
  scene=image.copy(),
  detections=detections)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce"&gt;https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;datasets&lt;/h3&gt; 
&lt;p&gt;Supervision provides a set of &lt;a href="https://supervision.roboflow.com/latest/datasets/core/"&gt;utils&lt;/a&gt; that allow you to load, split, merge, and save datasets in one of the supported formats.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import supervision as sv
from roboflow import Roboflow

project = Roboflow().workspace(&amp;lt;WORKSPACE_ID&amp;gt;).project(&amp;lt;PROJECT_ID&amp;gt;)
dataset = project.version(&amp;lt;PROJECT_VERSION&amp;gt;).download("coco")

ds = sv.DetectionDataset.from_coco(
    images_directory_path=f"{dataset.location}/train",
    annotations_path=f"{dataset.location}/train/_annotations.coco.json",
)

path, image, annotation = ds[0]
    # loads image on demand

for path, image, annotation in ds:
    # loads image on demand
&lt;/code&gt;&lt;/pre&gt; 
&lt;details close&gt; 
 &lt;summary&gt;ğŸ‘‰ more dataset utils&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;load&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;dataset = sv.DetectionDataset.from_yolo(
    images_directory_path=...,
    annotations_directory_path=...,
    data_yaml_path=...
)

dataset = sv.DetectionDataset.from_pascal_voc(
    images_directory_path=...,
    annotations_directory_path=...
)

dataset = sv.DetectionDataset.from_coco(
    images_directory_path=...,
    annotations_path=...
)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;split&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;train_dataset, test_dataset = dataset.split(split_ratio=0.7)
test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)

len(train_dataset), len(test_dataset), len(valid_dataset)
# (700, 150, 150)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;merge&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;ds_1 = sv.DetectionDataset(...)
len(ds_1)
# 100
ds_1.classes
# ['dog', 'person']

ds_2 = sv.DetectionDataset(...)
len(ds_2)
# 200
ds_2.classes
# ['cat']

ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])
len(ds_merged)
# 300
ds_merged.classes
# ['cat', 'dog', 'person']
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;save&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;dataset.as_yolo(
    images_directory_path=...,
    annotations_directory_path=...,
    data_yaml_path=...
)

dataset.as_pascal_voc(
    images_directory_path=...,
    annotations_directory_path=...
)

dataset.as_coco(
    images_directory_path=...,
    annotations_path=...
)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;convert&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;sv.DetectionDataset.from_yolo(
    images_directory_path=...,
    annotations_directory_path=...,
    data_yaml_path=...
).as_pascal_voc(
    images_directory_path=...,
    annotations_directory_path=...
)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ¬ tutorials&lt;/h2&gt; 
&lt;p&gt;Want to learn how to use Supervision? Explore our &lt;a href="https://supervision.roboflow.com/develop/how_to/detect_and_annotate/"&gt;how-to guides&lt;/a&gt;, &lt;a href="https://github.com/roboflow/supervision/tree/develop/examples"&gt;end-to-end examples&lt;/a&gt;, &lt;a href="https://roboflow.github.io/cheatsheet-supervision/"&gt;cheatsheet&lt;/a&gt;, and &lt;a href="https://supervision.roboflow.com/develop/cookbooks/"&gt;cookbooks&lt;/a&gt;!&lt;/p&gt; 
&lt;br&gt; 
&lt;p align="left"&gt; &lt;a href="https://youtu.be/hAWpsIuem10" title="Dwell Time Analysis with Computer Vision | Real-Time Stream Processing"&gt;&lt;img src="https://github.com/SkalskiP/SkalskiP/assets/26109316/a742823d-c158-407d-b30f-063a5d11b4e1" alt="Dwell Time Analysis with Computer Vision | Real-Time Stream Processing" width="300px" align="left"&gt;&lt;/a&gt; &lt;a href="https://youtu.be/hAWpsIuem10" title="Dwell Time Analysis with Computer Vision | Real-Time Stream Processing"&gt;&lt;strong&gt;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 5 Apr 2024&lt;/strong&gt;
&lt;/div&gt; 
&lt;br&gt;Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios.
&lt;p&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;p align="left"&gt; &lt;a href="https://youtu.be/uWP6UjDeZvY" title="Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source"&gt;&lt;img src="https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91" alt="Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source" width="300px" align="left"&gt;&lt;/a&gt; &lt;a href="https://youtu.be/uWP6UjDeZvY" title="Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source"&gt;&lt;strong&gt;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 11 Jan 2024&lt;/strong&gt;
&lt;/div&gt; 
&lt;br&gt;Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ’œ built with supervision&lt;/h2&gt; 
&lt;p&gt;Did you build something cool using supervision? &lt;a href="https://github.com/roboflow/supervision/discussions/categories/built-with-supervision"&gt;Let us know!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4"&gt;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900"&gt;https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f"&gt;https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“š documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://roboflow.github.io/supervision"&gt;documentation&lt;/a&gt; page to learn how supervision can help you build computer vision applications faster and more reliably.&lt;/p&gt; 
&lt;h2&gt;ğŸ† contribution&lt;/h2&gt; 
&lt;p&gt;We love your input! Please see our &lt;a href="https://github.com/roboflow/supervision/raw/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; to get started. Thank you ğŸ™ to all our contributors!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/roboflow/supervision/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=roboflow/supervision"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;br&gt; 
&lt;div align="center"&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://youtube.com/roboflow"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652" width="3%"&gt; &lt;/a&gt; 
  &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; 
  &lt;a href="https://roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649" width="3%"&gt; &lt;/a&gt; 
  &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; 
  &lt;a href="https://www.linkedin.com/company/roboflow-ai/"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691" width="3%"&gt; &lt;/a&gt; 
  &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; 
  &lt;a href="https://docs.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511" width="3%"&gt; &lt;/a&gt; 
  &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; 
  &lt;a href="https://discuss.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584" width="3%"&gt; &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"&gt; &lt;/a&gt;
  &lt;a href="https://blog.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605" width="3%"&gt; &lt;/a&gt;  
 &lt;/div&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>getzep/graphiti</title>
      <link>https://github.com/getzep/graphiti</link>
      <description>&lt;p&gt;Build Real-Time Knowledge Graphs for AI Agents&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://www.getzep.com/"&gt; &lt;img src="https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73" width="150" alt="Zep Logo"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; Graphiti &lt;/h1&gt; 
&lt;h2 align="center"&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/getzep/Graphiti/actions/workflows/lint.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat" alt="Lint"&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg?sanitize=true" alt="Unit Tests"&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg?sanitize=true" alt="MyPy Check"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/github/stars/getzep/graphiti" alt="GitHub Repo stars"&gt; &lt;a href="https://discord.com/invite/W8Kw6bsgXQ"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.13956"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat" alt="arXiv"&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/graphiti/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;amp;label=Release&amp;amp;color=limegreen" alt="Release"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/12986" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/12986" alt="getzep%2Fgraphiti | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;â­&lt;/span&gt; &lt;em&gt;Help us reach more developers and grow the Graphiti community. Star this repo!&lt;/em&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Check out the new &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md"&gt;MCP server for Graphiti&lt;/a&gt;! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.&lt;/p&gt; 
&lt;p&gt;Use Graphiti to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integrate and maintain dynamic user interactions and business data.&lt;/li&gt; 
 &lt;li&gt;Facilitate state-based reasoning and task automation for agents.&lt;/li&gt; 
 &lt;li&gt;Query complex, evolving data with semantic, keyword, and graph-based search methods.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-graph-intro.gif" alt="Graphiti temporal walkthrough" width="700px"&gt; &lt;/p&gt; 
&lt;br&gt; 
&lt;p&gt;A knowledge graph is a network of interconnected facts, such as &lt;em&gt;"Kendra loves Adidas shoes."&lt;/em&gt; Each fact is a "triplet" represented by two entities, or nodes ("Kendra", "Adidas shoes"), and their relationship, or edge ("loves"). Knowledge Graphs have been explored extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph while handling changing relationships and maintaining historical context.&lt;/p&gt; 
&lt;h2&gt;Graphiti and Zep's Context Engineering Platform.&lt;/h2&gt; 
&lt;p&gt;Graphiti powers the core of &lt;a href="https://www.getzep.com"&gt;Zep&lt;/a&gt;, a turn-key context engineering platform for AI Agents. Zep offers agent memory, Graph RAG for dynamic data, and context retrieval and assembly.&lt;/p&gt; 
&lt;p&gt;Using Graphiti, we've demonstrated Zep is the &lt;a href="https://blog.getzep.com/state-of-the-art-agent-memory/"&gt;State of the Art in Agent Memory&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Read our paper: &lt;a href="https://arxiv.org/abs/2501.13956"&gt;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2501.13956"&gt;&lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/arxiv-screenshot.png" alt="Zep: A Temporal Knowledge Graph Architecture for Agent Memory" width="700px"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Why Graphiti?&lt;/h2&gt; 
&lt;p&gt;Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Incremental Updates:&lt;/strong&gt; Immediate integration of new data episodes without batch recomputation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bi-Temporal Data Model:&lt;/strong&gt; Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Hybrid Retrieval:&lt;/strong&gt; Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Entity Definitions:&lt;/strong&gt; Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Efficiently manages large datasets with parallel processing, suitable for enterprise environments.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-intro-slides-stock-2.gif" alt="Graphiti structured + unstructured demo" width="700px"&gt; &lt;/p&gt; 
&lt;h2&gt;Graphiti vs. GraphRAG&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;GraphRAG&lt;/th&gt; 
   &lt;th&gt;Graphiti&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Primary Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Static document summarization&lt;/td&gt; 
   &lt;td&gt;Dynamic data management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Data Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Batch-oriented processing&lt;/td&gt; 
   &lt;td&gt;Continuous, incremental updates&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Knowledge Structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Entity clusters &amp;amp; community summaries&lt;/td&gt; 
   &lt;td&gt;Episodic data, semantic entities, communities&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Retrieval Method&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential LLM summarization&lt;/td&gt; 
   &lt;td&gt;Hybrid semantic, keyword, and graph-based search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Adaptability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Low&lt;/td&gt; 
   &lt;td&gt;High&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Basic timestamp tracking&lt;/td&gt; 
   &lt;td&gt;Explicit bi-temporal tracking&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Contradiction Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;LLM-driven summarization judgments&lt;/td&gt; 
   &lt;td&gt;Temporal edge invalidation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Query Latency&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Seconds to tens of seconds&lt;/td&gt; 
   &lt;td&gt;Typically sub-second latency&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Custom Entity Types&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Yes, customizable&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Moderate&lt;/td&gt; 
   &lt;td&gt;High, optimized for large datasets&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or higher&lt;/li&gt; 
 &lt;li&gt;Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)&lt;/li&gt; 
 &lt;li&gt;OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini). Using other services may result in incorrect output schemas and ingestion failures. This is particularly problematic when using smaller models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Optional:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The simplest way to install Neo4j is via &lt;a href="https://neo4j.com/download/"&gt;Neo4j Desktop&lt;/a&gt;. It provides a user-friendly interface to manage Neo4j instances and databases. Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add graphiti-core
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing with FalkorDB Support&lt;/h3&gt; 
&lt;p&gt;If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core[falkordb]

# or with uv
uv add graphiti-core[falkordb]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;You can also install optional LLM providers as extras:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]

# Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Default to Low Concurrency; LLM Provider 429 Rate Limit Errors&lt;/h2&gt; 
&lt;p&gt;Graphiti's ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM Provider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below.&lt;/p&gt; 
&lt;p&gt;Concurrency controlled by the &lt;code&gt;SEMAPHORE_LIMIT&lt;/code&gt; environment variable. By default, &lt;code&gt;SEMAPHORE_LIMIT&lt;/code&gt; is set to &lt;code&gt;10&lt;/code&gt; concurrent operations to help prevent &lt;code&gt;429&lt;/code&gt; rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.&lt;/p&gt; 
&lt;p&gt;If your LLM provider allows higher throughput, you can increase &lt;code&gt;SEMAPHORE_LIMIT&lt;/code&gt; to boost episode ingestion performance.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is set in your environment. Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI compatible APIs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For a complete working example, see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/examples/quickstart/README.md"&gt;Quickstart Example&lt;/a&gt; in the examples directory. The quickstart demonstrates:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Connecting to a Neo4j or FalkorDB database&lt;/li&gt; 
 &lt;li&gt;Initializing Graphiti indices and constraints&lt;/li&gt; 
 &lt;li&gt;Adding episodes to the graph (both text and structured JSON)&lt;/li&gt; 
 &lt;li&gt;Searching for relationships (edges) using hybrid search&lt;/li&gt; 
 &lt;li&gt;Reranking search results using graph distance&lt;/li&gt; 
 &lt;li&gt;Searching for nodes using predefined search recipes&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.&lt;/p&gt; 
&lt;h2&gt;MCP Server&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;mcp_server&lt;/code&gt; directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.&lt;/p&gt; 
&lt;p&gt;Key features of the MCP server include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Episode management (add, retrieve, delete)&lt;/li&gt; 
 &lt;li&gt;Entity management and relationship handling&lt;/li&gt; 
 &lt;li&gt;Semantic and hybrid search capabilities&lt;/li&gt; 
 &lt;li&gt;Group management for organizing related data&lt;/li&gt; 
 &lt;li&gt;Graph maintenance operations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.&lt;/p&gt; 
&lt;p&gt;For detailed setup instructions and usage examples, see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md"&gt;MCP server README&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;REST Service&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;server&lt;/code&gt; directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.&lt;/p&gt; 
&lt;p&gt;Please see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/server/README.md"&gt;server README&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Optional Environment Variables&lt;/h2&gt; 
&lt;p&gt;In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables. If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables must be set.&lt;/p&gt; 
&lt;h3&gt;Database Configuration&lt;/h3&gt; 
&lt;p&gt;Database names are configured directly in the driver constructors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Neo4j&lt;/strong&gt;: Database name defaults to &lt;code&gt;neo4j&lt;/code&gt; (hardcoded in Neo4jDriver)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FalkorDB&lt;/strong&gt;: Database name defaults to &lt;code&gt;default_db&lt;/code&gt; (hardcoded in FalkorDriver)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the &lt;code&gt;graph_driver&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;h4&gt;Neo4j with Custom Database Name&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver

# Create a Neo4j driver with custom database name
driver = Neo4jDriver(
    uri="bolt://localhost:7687",
    user="neo4j",
    password="password",
    database="my_custom_database"  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FalkorDB with Custom Database Name&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver

# Create a FalkorDB driver with custom database name
driver = FalkorDriver(
    host="localhost",
    port=6379,
    username="falkor_user",  # Optional
    password="falkor_password",  # Optional
    database="my_custom_graph"  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Performance Configuration&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;USE_PARALLEL_RUNTIME&lt;/code&gt; is an optional boolean variable that can be set to true if you wish to enable Neo4j's parallel runtime feature for several of our search queries. Note that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances, as such this feature is off by default.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import AsyncAzureOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client import LLMConfig, OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Azure OpenAI configuration - use separate endpoints for different services
api_key = "&amp;lt;your-api-key&amp;gt;"
api_version = "&amp;lt;your-api-version&amp;gt;"
llm_endpoint = "&amp;lt;your-llm-endpoint&amp;gt;"  # e.g., "https://your-llm-resource.openai.azure.com/"
embedding_endpoint = "&amp;lt;your-embedding-endpoint&amp;gt;"  # e.g., "https://your-embedding-resource.openai.azure.com/"

# Create separate Azure OpenAI clients for different services
llm_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=llm_endpoint
)

embedding_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=embedding_endpoint
)

# Create LLM Config with your Azure deployment names
azure_llm_config = LLMConfig(
    small_model="gpt-4.1-nano",
    model="gpt-4.1-mini",
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=OpenAIClient(
        llm_config=azure_llm_config,
        client=llm_client_azure
    ),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            embedding_model="text-embedding-3-small-deployment"  # Your Azure embedding deployment name
        ),
        client=embedding_client_azure
    ),
    cross_encoder=OpenAIRerankerClient(
        llm_config=LLMConfig(
            model=azure_llm_config.small_model  # Use small model for reranking
        ),
        client=llm_client_azure
    )
)

# Now you can use Graphiti with Azure OpenAI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Google Gemini&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Google's Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you'll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.&lt;/p&gt; 
&lt;p&gt;Install Graphiti:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add "graphiti-core[google-genai]"

# or

pip install "graphiti-core[google-genai]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig
from graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient

# Google API key configuration
api_key = "&amp;lt;your-google-api-key&amp;gt;"

# Initialize Graphiti with Gemini clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=api_key,
            model="gemini-2.0-flash"
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=api_key,
            embedding_model="embedding-001"
        )
    ),
    cross_encoder=GeminiRerankerClient(
        config=LLMConfig(
            api_key=api_key,
            model="gemini-2.5-flash-lite-preview-06-17"
        )
    )
)

# Now you can use Graphiti with Google Gemini for all components
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Gemini reranker uses the &lt;code&gt;gemini-2.5-flash-lite-preview-06-17&lt;/code&gt; model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini's log probabilities feature to rank passage relevance.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Ollama (Local LLM)&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Ollama for running local LLMs and embedding models via Ollama's OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.&lt;/p&gt; 
&lt;p&gt;Install the models: ollama pull deepseek-r1:7b # LLM ollama pull nomic-embed-text # embeddings&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_client import OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Configure Ollama LLM client
llm_config = LLMConfig(
    api_key="abc",  # Ollama doesn't require a real API key
    model="deepseek-r1:7b",
    small_model="deepseek-r1:7b",
    base_url="http://localhost:11434/v1", # Ollama provides this port
)

llm_client = OpenAIClient(config=llm_config)

# Initialize Graphiti with Ollama clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=llm_client,
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            api_key="abc",
            embedding_model="nomic-embed-text",
            embedding_dim=768,
            base_url="http://localhost:11434/v1",
        )
    ),
    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),
)

# Now you can use Graphiti with local Ollama models
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ensure Ollama is running (&lt;code&gt;ollama serve&lt;/code&gt;) and that you have pulled the models you want to use.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti"&gt;Guides and API documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti/graphiti/quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti/graphiti/lang-graph-agent"&gt;Building an agent with LangChain's LangGraph and Graphiti&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;Graphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.&lt;/p&gt; 
&lt;h3&gt;What We Collect&lt;/h3&gt; 
&lt;p&gt;When you initialize a Graphiti instance, we collect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Anonymous identifier&lt;/strong&gt;: A randomly generated UUID stored locally in &lt;code&gt;~/.cache/graphiti/telemetry_anon_id&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System information&lt;/strong&gt;: Operating system, Python version, and system architecture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Graphiti version&lt;/strong&gt;: The version you're using&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configuration choices&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;LLM provider type (OpenAI, Azure, Anthropic, etc.)&lt;/li&gt; 
   &lt;li&gt;Database backend (Neo4j, FalkorDB)&lt;/li&gt; 
   &lt;li&gt;Embedder provider (OpenAI, Azure, Voyage, etc.)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;What We Don't Collect&lt;/h3&gt; 
&lt;p&gt;We are committed to protecting your privacy. We &lt;strong&gt;never&lt;/strong&gt; collect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Personal information or identifiers&lt;/li&gt; 
 &lt;li&gt;API keys or credentials&lt;/li&gt; 
 &lt;li&gt;Your actual data, queries, or graph content&lt;/li&gt; 
 &lt;li&gt;IP addresses or hostnames&lt;/li&gt; 
 &lt;li&gt;File paths or system-specific information&lt;/li&gt; 
 &lt;li&gt;Any content from your episodes, nodes, or edges&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Why We Collect This Data&lt;/h3&gt; 
&lt;p&gt;This information helps us:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Understand which configurations are most popular to prioritize support and testing&lt;/li&gt; 
 &lt;li&gt;Identify which LLM and database providers to focus development efforts on&lt;/li&gt; 
 &lt;li&gt;Track adoption patterns to guide our roadmap&lt;/li&gt; 
 &lt;li&gt;Ensure compatibility across different Python versions and operating systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By sharing this anonymous information, you help us make Graphiti better for everyone in the community.&lt;/p&gt; 
&lt;h3&gt;View the Telemetry Code&lt;/h3&gt; 
&lt;p&gt;The Telemetry code &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/graphiti_core/telemetry/telemetry.py"&gt;may be found here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;How to Disable Telemetry&lt;/h3&gt; 
&lt;p&gt;Telemetry is &lt;strong&gt;opt-out&lt;/strong&gt; and can be disabled at any time. To disable telemetry collection:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GRAPHITI_TELEMETRY_ENABLED=false
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: Set in your shell profile&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For bash users (~/.bashrc or ~/.bash_profile)
echo 'export GRAPHITI_TELEMETRY_ENABLED=false' &amp;gt;&amp;gt; ~/.bashrc

# For zsh users (~/.zshrc)
echo 'export GRAPHITI_TELEMETRY_ENABLED=false' &amp;gt;&amp;gt; ~/.zshrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Set for a specific Python session&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'

# Then initialize Graphiti as usual
from graphiti_core import Graphiti
graphiti = Graphiti(...)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Telemetry is automatically disabled during test runs (when &lt;code&gt;pytest&lt;/code&gt; is detected).&lt;/p&gt; 
&lt;h3&gt;Technical Details&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Telemetry uses PostHog for anonymous analytics collection&lt;/li&gt; 
 &lt;li&gt;All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality&lt;/li&gt; 
 &lt;li&gt;The anonymous ID is stored locally and is not tied to any personal information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Status and Roadmap&lt;/h2&gt; 
&lt;p&gt;Graphiti is under active development. We aim to maintain API stability while working on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Supporting custom graph schemas: 
  &lt;ul&gt; 
   &lt;li&gt;Allow developers to provide their own defined node and edge classes when ingesting episodes&lt;/li&gt; 
   &lt;li&gt;Enable more flexible knowledge representation tailored to specific use cases&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Enhancing retrieval capabilities with more robust and configurable options&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Graphiti MCP Server&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Expanding test coverage to ensure reliability and catch edge cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or answering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer to &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.com/invite/W8Kw6bsgXQ"&gt;Zep Discord server&lt;/a&gt; and make your way to the &lt;strong&gt;#Graphiti&lt;/strong&gt; channel!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Investment Research for Everyone, Everywhere.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-light.svg?raw=true#gh-light-mode-only" alt="OpenBB Platform logo" width="600"&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only" alt="OpenBB Platform logo" width="600"&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter"&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield"&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers"&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20"&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab"&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The first financial Platform that is open source.&lt;/p&gt; 
&lt;p&gt;The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.&lt;/p&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can sign up to the &lt;a href="https://my.openbb.co/login"&gt;OpenBB Hub&lt;/a&gt; to get the most out of the OpenBB ecosystem.&lt;/p&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/platform/reference"&gt;https://docs.openbb.co/platform/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the OpenBB Platform is all about an integration to dozens of different data vendors, the interface is either Python or a CLI.&lt;/p&gt; 
&lt;p&gt;If you want an enterprise UI to visualize this datasets and use AI agents on top, you can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000"&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating OpenBB Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run OpenBB Platform backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate OpenBB Platform backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: OpenBB Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The OpenBB Platform can be installed as a &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/platform/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;OpenBB Platform CLI installation&lt;/h3&gt; 
&lt;p&gt;The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now â­ï¸)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/platform/developer_guide/misc/contributing"&gt;Contributing Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the OpenBB Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800"&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/open_deep_research</title>
      <link>https://github.com/langchain-ai/open_deep_research</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Deep Research&lt;/h1&gt; 
&lt;img width="1388" height="298" alt="full_diagram" src="https://github.com/user-attachments/assets/12a2371b-8be2-4219-9b48-90503eb43c69"&gt; 
&lt;p&gt;Deep research has broken out as one of the most popular agent applications. This is a simple, configurable, fully open source deep research agent that works across many model providers, search tools, and MCP servers.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read more in our &lt;a href="https://blog.langchain.com/open-deep-research/"&gt;blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See our &lt;a href="https://www.youtube.com/watch?v=agGiWUpxkhg"&gt;video&lt;/a&gt; for a quick overview&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ Quickstart&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository and activate a virtual environment:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/langchain-ai/open_deep_research.git
cd open_deep_research
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install -r pyproject.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Set up your &lt;code&gt;.env&lt;/code&gt; file to customize the environment variables (for model selection, search tools, and other configuration settings):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Launch the assistant with the LangGraph server locally to open LangGraph Studio in your browser:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies and start the LangGraph server
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev --allow-blocking
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use this to open the Studio UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- ğŸš€ API: http://127.0.0.1:2024
- ğŸ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- ğŸ“š API Docs: http://127.0.0.1:2024/docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;img width="817" height="666" alt="Screenshot 2025-07-13 at 11 21 12â€¯PM" src="https://github.com/user-attachments/assets/052f2ed3-c664-4a4f-8ec2-074349dcaa3f"&gt; 
&lt;p&gt;Ask a question in the &lt;code&gt;messages&lt;/code&gt; input field and click &lt;code&gt;Submit&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Configurations&lt;/h3&gt; 
&lt;p&gt;Open Deep Research offers extensive configuration options to customize the research process and model behavior. All configurations can be set via the web UI, environment variables, or by modifying the configuration directly.&lt;/p&gt; 
&lt;h4&gt;General Settings&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Max Structured Output Retries&lt;/strong&gt; (default: 3): Maximum number of retries for structured output calls from models when parsing fails&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Allow Clarification&lt;/strong&gt; (default: true): Whether to allow the researcher to ask clarifying questions before starting research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Max Concurrent Research Units&lt;/strong&gt; (default: 5): Maximum number of research units to run concurrently using sub-agents. Higher values enable faster research but may hit rate limits&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Research Configuration&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Search API&lt;/strong&gt; (default: Tavily): Choose from Tavily (works with all models), OpenAI Native Web Search, Anthropic Native Web Search, or None&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Max Researcher Iterations&lt;/strong&gt; (default: 3): Number of times the Research Supervisor will reflect on research and ask follow-up questions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Max React Tool Calls&lt;/strong&gt; (default: 5): Maximum number of tool calling iterations in a single researcher step&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Models&lt;/h4&gt; 
&lt;p&gt;Open Deep Research uses multiple specialized models for different research tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Summarization Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1-nano&lt;/code&gt;): Summarizes research results from search APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Research Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1&lt;/code&gt;): Conducts research and analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compression Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1-mini&lt;/code&gt;): Compresses research findings from sub-agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Final Report Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1&lt;/code&gt;): Writes the final comprehensive report&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All models are configured using &lt;a href="https://python.langchain.com/docs/how_to/chat_models_universal_init/"&gt;init_chat_model() API&lt;/a&gt; which supports providers like OpenAI, Anthropic, Google Vertex AI, and others.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important Model Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Structured Outputs&lt;/strong&gt;: All models must support structured outputs. Check support &lt;a href="https://python.langchain.com/docs/integrations/chat/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Search API Compatibility&lt;/strong&gt;: Research and Compression models must support your selected search API:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Anthropic search requires Anthropic models with web search capability&lt;/li&gt; 
   &lt;li&gt;OpenAI search requires OpenAI models with web search capability&lt;/li&gt; 
   &lt;li&gt;Tavily works with all models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Calling&lt;/strong&gt;: All models must support tool calling functionality&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Special Configurations&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For OpenRouter: Follow &lt;a href="https://github.com/langchain-ai/open_deep_research/issues/75#issuecomment-2811472408"&gt;this guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;For local models via Ollama: See &lt;a href="https://github.com/langchain-ai/open_deep_research/issues/65#issuecomment-2743586318"&gt;setup instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Example MCP (Model Context Protocol) Servers&lt;/h4&gt; 
&lt;p&gt;Open Deep Research supports MCP servers to extend research capabilities.&lt;/p&gt; 
&lt;h4&gt;Local MCP Servers&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Filesystem MCP Server&lt;/strong&gt; provides secure file system operations with robust access control:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read, write, and manage files and directories&lt;/li&gt; 
 &lt;li&gt;Perform operations like reading file contents, creating directories, moving files, and searching&lt;/li&gt; 
 &lt;li&gt;Restrict operations to predefined directories for security&lt;/li&gt; 
 &lt;li&gt;Support for both command-line configuration and dynamic MCP roots&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mcp-server-filesystem /path/to/allowed/dir1 /path/to/allowed/dir2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Remote MCP Servers&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Remote MCP servers&lt;/strong&gt; enable distributed agent coordination and support streamable HTTP requests. Unlike local servers, they can be multi-tenant and require more complex authentication.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Arcade MCP Server Example&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "url": "https://api.arcade.dev/v1/mcps/ms_0ujssxh0cECutqzMgbtXSGnjorm",
  "tools": ["Search_SearchHotels", "Search_SearchOneWayFlights", "Search_SearchRoundtripFlights"]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Remote servers can be configured as authenticated or unauthenticated and support JWT-based authentication through OAuth endpoints.&lt;/p&gt; 
&lt;h3&gt;Evaluation&lt;/h3&gt; 
&lt;p&gt;A comprehensive batch evaluation system designed for detailed analysis and comparative studies.&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-dimensional Scoring&lt;/strong&gt;: Specialized evaluators with 0-1 scale ratings&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dataset-driven Evaluation&lt;/strong&gt;: Batch processing across multiple test cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run comprehensive evaluation on LangSmith datasets
python tests/run_evaluate.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Key Files:&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;tests/run_evaluate.py&lt;/code&gt;: Main evaluation script&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tests/evaluators.py&lt;/code&gt;: Specialized evaluator functions&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tests/prompts.py&lt;/code&gt;: Evaluation prompts for each dimension&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deployments and Usages&lt;/h3&gt; 
&lt;h4&gt;LangGraph Studio&lt;/h4&gt; 
&lt;p&gt;Follow the &lt;a href="https://raw.githubusercontent.com/langchain-ai/open_deep_research/main/#-quickstart"&gt;quickstart&lt;/a&gt; to start LangGraph server locally and test the agent out on LangGraph Studio.&lt;/p&gt; 
&lt;h4&gt;Hosted deployment&lt;/h4&gt; 
&lt;p&gt;You can easily deploy to &lt;a href="https://langchain-ai.github.io/langgraph/concepts/#deployment-options"&gt;LangGraph Platform&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Open Agent Platform&lt;/h4&gt; 
&lt;p&gt;Open Agent Platform (OAP) is a UI from which non-technical users can build and configure their own agents. OAP is great for allowing users to configure the Deep Researcher with different MCP tools and search APIs that are best suited to their needs and the problems that they want to solve.&lt;/p&gt; 
&lt;p&gt;We've deployed Open Deep Research to our public demo instance of OAP. All you need to do is add your API Keys, and you can test out the Deep Researcher for yourself! Try it out &lt;a href="https://oap.langchain.com"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also deploy your own instance of OAP, and make your own custom agents (like Deep Researcher) available on it to your users.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.oap.langchain.com/quickstart"&gt;Deploy Open Agent Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.oap.langchain.com/setup/agents"&gt;Add Deep Researcher to OAP&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Updates ğŸ”¥&lt;/h3&gt; 
&lt;h3&gt;Legacy Implementations ğŸ›ï¸&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;src/legacy/&lt;/code&gt; folder contains two earlier implementations that provide alternative approaches to automated research:&lt;/p&gt; 
&lt;h4&gt;1. Workflow Implementation (&lt;code&gt;legacy/graph.py&lt;/code&gt;)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Plan-and-Execute&lt;/strong&gt;: Structured workflow with human-in-the-loop planning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sequential Processing&lt;/strong&gt;: Creates sections one by one with reflection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Control&lt;/strong&gt;: Allows feedback and approval of report plans&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Focused&lt;/strong&gt;: Emphasizes accuracy through iterative refinement&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2. Multi-Agent Implementation (&lt;code&gt;legacy/multi_agent.py&lt;/code&gt;)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Supervisor-Researcher Architecture&lt;/strong&gt;: Coordinated multi-agent system&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Processing&lt;/strong&gt;: Multiple researchers work simultaneously&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speed Optimized&lt;/strong&gt;: Faster report generation through concurrency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Support&lt;/strong&gt;: Extensive Model Context Protocol integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;code&gt;src/legacy/legacy.md&lt;/code&gt; for detailed documentation, configuration options, and usage examples for both legacy implementations.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>stanford-oval/storm</title>
      <link>https://github.com/stanford-oval/storm</link>
      <description>&lt;p&gt;An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/stanford-oval/storm/main/assets/logo.svg?sanitize=true" style="width: 25%; height: auto;"&gt; &lt;/p&gt; 
&lt;h1&gt;STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking&lt;/h1&gt; 
&lt;p align="center"&gt; | &lt;a href="http://storm.genie.stanford.edu"&gt;&lt;b&gt;Research preview&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2402.14207"&gt;&lt;b&gt;STORM Paper&lt;/b&gt;&lt;/a&gt;| &lt;a href="https://www.arxiv.org/abs/2408.15232"&gt;&lt;b&gt;Co-STORM Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://storm-project.stanford.edu/"&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; **Latest News** ğŸ”¥ 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;[2025/01] We add &lt;a href="https://github.com/BerriAI/litellm"&gt;litellm&lt;/a&gt; integration for language models and embedding models in &lt;code&gt;knowledge-storm&lt;/code&gt; v1.1.0.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024/09] Co-STORM codebase is now released and integrated into &lt;code&gt;knowledge-storm&lt;/code&gt; python package v1.0.0. Run &lt;code&gt;pip install knowledge-storm --upgrade&lt;/code&gt; to check it out.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024/09] We introduce collaborative STORM (Co-STORM) to support human-AI collaborative knowledge curation! &lt;a href="https://www.arxiv.org/abs/2408.15232"&gt;Co-STORM Paper&lt;/a&gt; has been accepted to EMNLP 2024 main conference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024/07] You can now install our package with &lt;code&gt;pip install knowledge-storm&lt;/code&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024/07] We add &lt;code&gt;VectorRM&lt;/code&gt; to support grounding on user-provided documents, complementing existing support of search engines (&lt;code&gt;YouRM&lt;/code&gt;, &lt;code&gt;BingSearch&lt;/code&gt;). (check out &lt;a href="https://github.com/stanford-oval/storm/pull/58"&gt;#58&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024/07] We release demo light for developers a minimal user interface built with streamlit framework in Python, handy for local development and demo hosting (checkout &lt;a href="https://github.com/stanford-oval/storm/pull/54"&gt;#54&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024/06] We will present STORM at NAACL 2024! Find us at Poster Session 2 on June 17 or check our &lt;a href="https://raw.githubusercontent.com/stanford-oval/storm/main/assets/storm_naacl2024_slides.pdf"&gt;presentation material&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024/05] We add Bing Search support in &lt;a href="https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/rm.py"&gt;rm.py&lt;/a&gt;. Test STORM with &lt;code&gt;GPT-4o&lt;/code&gt; - we now configure the article generation part in our demo using &lt;code&gt;GPT-4o&lt;/code&gt; model.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024/04] We release refactored version of STORM codebase! We define &lt;a href="https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/interface.py"&gt;interface&lt;/a&gt; for STORM pipeline and reimplement STORM-wiki (check out &lt;a href="https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/storm_wiki"&gt;&lt;code&gt;src/storm_wiki&lt;/code&gt;&lt;/a&gt;) to demonstrate how to instantiate the pipeline. We provide API to support customization of different language models and retrieval/search integration.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview &lt;a href="https://storm.genie.stanford.edu/"&gt;(Try STORM now!)&lt;/a&gt;&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/stanford-oval/storm/main/assets/overview.svg?sanitize=true" style="width: 90%; height: auto;"&gt; &lt;/p&gt; STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. Co-STORM further enhanced its feature by enabling human to collaborative LLM system to support more aligned and preferred information seeking and knowledge curation. 
&lt;p&gt;While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;More than 70,000 people have tried our &lt;a href="https://storm.genie.stanford.edu/"&gt;live research preview&lt;/a&gt;. Try it out to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system ğŸ™!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;How STORM &amp;amp; Co-STORM works&lt;/h2&gt; 
&lt;h3&gt;STORM&lt;/h3&gt; 
&lt;p&gt;STORM breaks down generating long articles with citations into two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-writing stage&lt;/strong&gt;: The system conducts Internet-based research to collect references and generates an outline.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Writing stage&lt;/strong&gt;: The system uses the outline and references to generate the full-length article with citations.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/stanford-oval/storm/main/assets/two_stages.jpg" style="width: 60%; height: auto;"&gt; &lt;/p&gt; 
&lt;p&gt;STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Perspective-Guided Question Asking&lt;/strong&gt;: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simulated Conversation&lt;/strong&gt;: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;CO-STORM&lt;/h3&gt; 
&lt;p&gt;Co-STORM proposes &lt;strong&gt;a collaborative discourse protocol&lt;/strong&gt; which implements a turn management policy to support smooth collaboration among&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Co-STORM LLM experts&lt;/strong&gt;: This type of agent generates answers grounded on external knowledge sources and/or raises follow-up questions based on the discourse history.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Moderator&lt;/strong&gt;: This agent generates thought-provoking questions inspired by information discovered by the retriever but not directly used in previous turns. Question generation can also be grounded!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human user&lt;/strong&gt;: The human user will take the initiative to either (1) observe the discourse to gain deeper understanding of the topic, or (2) actively engage in the conversation by injecting utterances to steer the discussion focus.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/stanford-oval/storm/main/assets/co-storm-workflow.jpg" style="width: 60%; height: auto;"&gt; &lt;/p&gt; 
&lt;p&gt;Co-STORM also maintains a dynamic updated &lt;strong&gt;mind map&lt;/strong&gt;, which organize collected information into a hierarchical concept structure, aiming to &lt;strong&gt;build a shared conceptual space between the human user and the system&lt;/strong&gt;. The mind map has been proven to help reduce the mental load when the discourse goes long and in-depth.&lt;/p&gt; 
&lt;p&gt;Both STORM and Co-STORM are implemented in a highly modular way using &lt;a href="https://github.com/stanfordnlp/dspy"&gt;dspy&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install the knowledge storm library, use &lt;code&gt;pip install knowledge-storm&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You could also install the source code which allows you to modify the behavior of STORM engine directly.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the git repository.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/stanford-oval/storm.git
cd storm
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the required packages.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;conda create -n storm python=3.11
conda activate storm
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;API&lt;/h2&gt; 
&lt;p&gt;Currently, our package support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Language model components: All language models supported by litellm as listed &lt;a href="https://docs.litellm.ai/docs/providers"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Embedding model components: All embedding models supported by litellm as listed &lt;a href="https://docs.litellm.ai/docs/embedding/supported_embedding"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;retrieval module components: &lt;code&gt;YouRM&lt;/code&gt;, &lt;code&gt;BingSearch&lt;/code&gt;, &lt;code&gt;VectorRM&lt;/code&gt;, &lt;code&gt;SerperRM&lt;/code&gt;, &lt;code&gt;BraveRM&lt;/code&gt;, &lt;code&gt;SearXNG&lt;/code&gt;, &lt;code&gt;DuckDuckGoSearchRM&lt;/code&gt;, &lt;code&gt;TavilySearchRM&lt;/code&gt;, &lt;code&gt;GoogleSearch&lt;/code&gt;, and &lt;code&gt;AzureAISearch&lt;/code&gt; as&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span&gt;ğŸŒŸ&lt;/span&gt; &lt;strong&gt;PRs for integrating more search engines/retrievers into &lt;a href="https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/rm.py"&gt;knowledge_storm/rm.py&lt;/a&gt; are highly appreciated!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Both STORM and Co-STORM are working in the information curation layer, you need to set up the information retrieval module and language model module to create their &lt;code&gt;Runner&lt;/code&gt; classes respectively.&lt;/p&gt; 
&lt;h3&gt;STORM&lt;/h3&gt; 
&lt;p&gt;The STORM knowledge curation engine is defined as a simple Python &lt;code&gt;STORMWikiRunner&lt;/code&gt; class. Here is an example of using You.com search engine and OpenAI models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs
from knowledge_storm.lm import LitellmModel
from knowledge_storm.rm import YouRM

lm_configs = STORMWikiLMConfigs()
openai_kwargs = {
    'api_key': os.getenv("OPENAI_API_KEY"),
    'temperature': 1.0,
    'top_p': 0.9,
}
# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.
# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.
# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.
gpt_35 = LitellmModel(model='gpt-3.5-turbo', max_tokens=500, **openai_kwargs)
gpt_4 = LitellmModel(model='gpt-4o', max_tokens=3000, **openai_kwargs)
lm_configs.set_conv_simulator_lm(gpt_35)
lm_configs.set_question_asker_lm(gpt_35)
lm_configs.set_outline_gen_lm(gpt_4)
lm_configs.set_article_gen_lm(gpt_4)
lm_configs.set_article_polish_lm(gpt_4)
# Check out the STORMWikiRunnerArguments class for more configurations.
engine_args = STORMWikiRunnerArguments(...)
rm = YouRM(ydc_api_key=os.getenv('YDC_API_KEY'), k=engine_args.search_top_k)
runner = STORMWikiRunner(engine_args, lm_configs, rm)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;STORMWikiRunner&lt;/code&gt; instance can be evoked with the simple &lt;code&gt;run&lt;/code&gt; method:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;topic = input('Topic: ')
runner.run(
    topic=topic,
    do_research=True,
    do_generate_outline=True,
    do_generate_article=True,
    do_polish_article=True,
)
runner.post_run()
runner.summary()
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;do_research&lt;/code&gt;: if True, simulate conversations with difference perspectives to collect information about the topic; otherwise, load the results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;do_generate_outline&lt;/code&gt;: if True, generate an outline for the topic; otherwise, load the results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;do_generate_article&lt;/code&gt;: if True, generate an article for the topic based on the outline and the collected information; otherwise, load the results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;do_polish_article&lt;/code&gt;: if True, polish the article by adding a summarization section and (optionally) removing duplicate content; otherwise, load the results.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Co-STORM&lt;/h3&gt; 
&lt;p&gt;The Co-STORM knowledge curation engine is defined as a simple Python &lt;code&gt;CoStormRunner&lt;/code&gt; class. Here is an example of using Bing search engine and OpenAI models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from knowledge_storm.collaborative_storm.engine import CollaborativeStormLMConfigs, RunnerArgument, CoStormRunner
from knowledge_storm.lm import LitellmModel
from knowledge_storm.logging_wrapper import LoggingWrapper
from knowledge_storm.rm import BingSearch

# Co-STORM adopts the same multi LM system paradigm as STORM 
lm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()
openai_kwargs = {
    "api_key": os.getenv("OPENAI_API_KEY"),
    "api_provider": "openai",
    "temperature": 1.0,
    "top_p": 0.9,
    "api_base": None,
} 
question_answering_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)
discourse_manage_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)
utterance_polishing_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=2000, **openai_kwargs)
warmstart_outline_gen_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)
question_asking_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=300, **openai_kwargs)
knowledge_base_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)

lm_config.set_question_answering_lm(question_answering_lm)
lm_config.set_discourse_manage_lm(discourse_manage_lm)
lm_config.set_utterance_polishing_lm(utterance_polishing_lm)
lm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)
lm_config.set_question_asking_lm(question_asking_lm)
lm_config.set_knowledge_base_lm(knowledge_base_lm)

# Check out the Co-STORM's RunnerArguments class for more configurations.
topic = input('Topic: ')
runner_argument = RunnerArgument(topic=topic, ...)
logging_wrapper = LoggingWrapper(lm_config)
bing_rm = BingSearch(bing_search_api_key=os.environ.get("BING_SEARCH_API_KEY"),
                     k=runner_argument.retrieve_top_k)
costorm_runner = CoStormRunner(lm_config=lm_config,
                               runner_argument=runner_argument,
                               logging_wrapper=logging_wrapper,
                               rm=bing_rm)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;CoStormRunner&lt;/code&gt; instance can be evoked with the &lt;code&gt;warmstart()&lt;/code&gt; and &lt;code&gt;step(...)&lt;/code&gt; methods.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Warm start the system to build shared conceptual space between Co-STORM and users
costorm_runner.warm_start()

# Step through the collaborative discourse 
# Run either of the code snippets below in any order, as many times as you'd like
# To observe the conversation:
conv_turn = costorm_runner.step()
# To inject your utterance to actively steer the conversation:
costorm_runner.step(user_utterance="YOUR UTTERANCE HERE")

# Generate report based on the collaborative discourse
costorm_runner.knowledge_base.reorganize()
article = costorm_runner.generate_report()
print(article)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start with Example Scripts&lt;/h2&gt; 
&lt;p&gt;We provide scripts in our &lt;a href="https://raw.githubusercontent.com/stanford-oval/storm/main/examples"&gt;examples folder&lt;/a&gt; as a quick start to run STORM and Co-STORM with different configurations.&lt;/p&gt; 
&lt;p&gt;We suggest using &lt;code&gt;secrets.toml&lt;/code&gt; to set up the API keys. Create a file &lt;code&gt;secrets.toml&lt;/code&gt; under the root directory and add the following content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# ============ language model configurations ============ 
# Set up OpenAI API key.
OPENAI_API_KEY="your_openai_api_key"
# If you are using the API service provided by OpenAI, include the following line:
OPENAI_API_TYPE="openai"
# If you are using the API service provided by Microsoft Azure, include the following lines:
OPENAI_API_TYPE="azure"
AZURE_API_BASE="your_azure_api_base_url"
AZURE_API_VERSION="your_azure_api_version"
# ============ retriever configurations ============ 
BING_SEARCH_API_KEY="your_bing_search_api_key" # if using bing search
# ============ encoder configurations ============ 
ENCODER_API_TYPE="openai" # if using openai encoder
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;STORM examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;To run STORM with &lt;code&gt;gpt&lt;/code&gt; family models with default configurations:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Run the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/storm_examples/run_storm_wiki_gpt.py \
    --output-dir $OUTPUT_DIR \
    --retriever bing \
    --do-research \
    --do-generate-outline \
    --do-generate-article \
    --do-polish-article
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To run STORM using your favorite language models or grounding on your own corpus:&lt;/strong&gt; Check out &lt;a href="https://raw.githubusercontent.com/stanford-oval/storm/main/examples/storm_examples/README.md"&gt;examples/storm_examples/README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Co-STORM examples&lt;/h3&gt; 
&lt;p&gt;To run Co-STORM with &lt;code&gt;gpt&lt;/code&gt; family models with default configurations,&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Add &lt;code&gt;BING_SEARCH_API_KEY="xxx"&lt;/code&gt; and &lt;code&gt;ENCODER_API_TYPE="xxx"&lt;/code&gt; to &lt;code&gt;secrets.toml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run the following command&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/costorm_examples/run_costorm_gpt.py \
    --output-dir $OUTPUT_DIR \
    --retriever bing
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Customization of the Pipeline&lt;/h2&gt; 
&lt;h3&gt;STORM&lt;/h3&gt; 
&lt;p&gt;If you have installed the source code, you can customize STORM based on your own use case. STORM engine consists of 4 modules:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Knowledge Curation Module: Collects a broad coverage of information about the given topic.&lt;/li&gt; 
 &lt;li&gt;Outline Generation Module: Organizes the collected information by generating a hierarchical outline for the curated knowledge.&lt;/li&gt; 
 &lt;li&gt;Article Generation Module: Populates the generated outline with the collected information.&lt;/li&gt; 
 &lt;li&gt;Article Polishing Module: Refines and enhances the written article for better presentation.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The interface for each module is defined in &lt;code&gt;knowledge_storm/interface.py&lt;/code&gt;, while their implementations are instantiated in &lt;code&gt;knowledge_storm/storm_wiki/modules/*&lt;/code&gt;. These modules can be customized according to your specific requirements (e.g., generating sections in bullet point format instead of full paragraphs).&lt;/p&gt; 
&lt;h3&gt;Co-STORM&lt;/h3&gt; 
&lt;p&gt;If you have installed the source code, you can customize Co-STORM based on your own use case&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Co-STORM introduces multiple LLM agent types (i.e. Co-STORM experts and Moderator). LLM agent interface is defined in &lt;code&gt;knowledge_storm/interface.py&lt;/code&gt; , while its implementation is instantiated in &lt;code&gt;knowledge_storm/collaborative_storm/modules/co_storm_agents.py&lt;/code&gt;. Different LLM agent policies can be customized.&lt;/li&gt; 
 &lt;li&gt;Co-STORM introduces a collaborative discourse protocol, with its core function centered on turn policy management. We provide an example implementation of turn policy management through &lt;code&gt;DiscourseManager&lt;/code&gt; in &lt;code&gt;knowledge_storm/collaborative_storm/engine.py&lt;/code&gt;. It can be customized and further improved.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Datasets&lt;/h2&gt; 
&lt;p&gt;To facilitate the study of automatic knowledge curation and complex information seeking, our project releases the following datasets:&lt;/p&gt; 
&lt;h3&gt;FreshWiki&lt;/h3&gt; 
&lt;p&gt;The FreshWiki Dataset is a collection of 100 high-quality Wikipedia articles focusing on the most-edited pages from February 2022 to September 2023. See Section 2.1 in &lt;a href="https://arxiv.org/abs/2402.14207"&gt;STORM paper&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;You can download the dataset from &lt;a href="https://huggingface.co/datasets/EchoShao8899/FreshWiki"&gt;huggingface&lt;/a&gt; directly. To ease the data contamination issue, we archive the &lt;a href="https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup/FreshWiki"&gt;source code&lt;/a&gt; for the data construction pipeline that can be repeated at future dates.&lt;/p&gt; 
&lt;h3&gt;WildSeek&lt;/h3&gt; 
&lt;p&gt;To study usersâ€™ interests in complex information seeking tasks in the wild, we utilized data collected from the web research preview to create the WildSeek dataset. We downsampled the data to ensure the diversity of the topics and the quality of the data. Each data point is a pair comprising a topic and the userâ€™s goal for conducting deep search on the topic. For more details, please refer to Section 2.2 and Appendix A of &lt;a href="https://www.arxiv.org/abs/2408.15232"&gt;Co-STORM paper&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The WildSeek dataset is available &lt;a href="https://huggingface.co/datasets/YuchengJiang/WildSeek"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Replicate STORM &amp;amp; Co-STORM paper result&lt;/h2&gt; 
&lt;p&gt;For STORM paper experiments, please switch to the branch &lt;code&gt;NAACL-2024-code-backup&lt;/code&gt; &lt;a href="https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For Co-STORM paper experiments, please switch to the branch &lt;code&gt;EMNLP-2024-code-backup&lt;/code&gt; (placeholder for now, will be updated soon).&lt;/p&gt; 
&lt;h2&gt;Roadmap &amp;amp; Contributions&lt;/h2&gt; 
&lt;p&gt;Our team is actively working on:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Human-in-the-Loop Functionalities: Supporting user participation in the knowledge curation process.&lt;/li&gt; 
 &lt;li&gt;Information Abstraction: Developing abstractions for curated information to support presentation formats beyond the Wikipedia-style report.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!&lt;/p&gt; 
&lt;p&gt;Contact person: &lt;a href="mailto:shaoyj@stanford.edu"&gt;Yijia Shao&lt;/a&gt; and &lt;a href="mailto:yuchengj@stanford.edu"&gt;Yucheng Jiang&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We would like to thank Wikipedia for its excellent open-source content. The FreshWiki dataset is sourced from Wikipedia, licensed under the Creative Commons Attribution-ShareAlike (CC BY-SA) license.&lt;/p&gt; 
&lt;p&gt;We are very grateful to &lt;a href="https://michelle123lam.github.io/"&gt;Michelle Lam&lt;/a&gt; for designing the logo for this project and &lt;a href="https://dekun.me"&gt;Dekun Ma&lt;/a&gt; for leading the UI development.&lt;/p&gt; 
&lt;p&gt;Thanks to Vercel for their support of &lt;a href="https://storm.genie.stanford.edu"&gt;open-source software&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please cite our paper if you use this code or part of it in your work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{jiang-etal-2024-unknown,
    title = "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations",
    author = "Jiang, Yucheng  and
      Shao, Yijia  and
      Ma, Dekun  and
      Semnani, Sina  and
      Lam, Monica",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.554/",
    doi = "10.18653/v1/2024.emnlp-main.554",
    pages = "9917--9955",
}

@inproceedings{shao-etal-2024-assisting,
    title = "Assisting in Writing {W}ikipedia-like Articles From Scratch with Large Language Models",
    author = "Shao, Yijia  and
      Jiang, Yucheng  and
      Kanell, Theodore  and
      Xu, Peter  and
      Khattab, Omar  and
      Lam, Monica",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.347/",
    doi = "10.18653/v1/2024.naacl-long.347",
    pages = "6252--6278",
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>opendatalab/MinerU</title>
      <link>https://github.com/opendatalab/MinerU</link>
      <description>&lt;p&gt;A high-quality tool for convert PDF to Markdown and JSON.ä¸€ç«™å¼å¼€æºé«˜è´¨é‡æ•°æ®æå–å·¥å…·ï¼Œå°†PDFè½¬æ¢æˆMarkdownå’ŒJSONæ ¼å¼ã€‚&lt;/p&gt;&lt;hr&gt;&lt;div align="center" xmlns="http://www.w3.org/1999/html"&gt; 
 &lt;!-- logo --&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/images/MinerU-logo.png" width="300px" style="vertical-align:middle;"&gt; &lt;/p&gt; 
 &lt;!-- icon --&gt; 
 &lt;p&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU.svg?sanitize=true" alt="stars"&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/forks/opendatalab/MinerU.svg?sanitize=true" alt="forks"&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-raw/opendatalab/MinerU" alt="open issues"&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU" alt="issue resolution"&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/v/mineru" alt="PyPI version"&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/mineru" alt="PyPI - Python Version"&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru" alt="Downloads"&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru/month" alt="Downloads"&gt;&lt;/a&gt; &lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab"&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace"&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope"&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/gist/myhloli/3b3a00a4a0a61577b6c30f989092d20d/mineru_demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab"&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2409.18839"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2409.18839-b31b1b.svg?logo=arXiv" alt="arXiv"&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11174" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11174" alt="opendatalab%2FMinerU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- language --&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README_zh-CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- hot link --&gt; 
 &lt;p align="center"&gt; ğŸš€&lt;a href="https://mineru.net/?source=github"&gt;Access MinerU Nowâ†’âœ… Zero-Install Web Version âœ… Full-Featured Desktop Client âœ… Instant API Access; Skip deployment headaches â€“ get all product formats in one click. Developers, dive in!&lt;/a&gt; &lt;/p&gt; 
 &lt;!-- join us --&gt; 
 &lt;p align="center"&gt; ğŸ‘‹ join us on &lt;a href="https://discord.gg/Tdedn9GTXq" target="_blank"&gt;Discord&lt;/a&gt; and &lt;a href="http://mineru.space/s/V85Yl" target="_blank"&gt;WeChat&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;Changelog&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025/07/27 version 2.1.7 Released 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.0 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2025/07/26 2.1.6 Released 
  &lt;ul&gt; 
   &lt;li&gt;Fixed table parsing issues in handwritten documents when using &lt;code&gt;vlm&lt;/code&gt; backend&lt;/li&gt; 
   &lt;li&gt;Fixed visualization box position drift issue when document is rotated #3175&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2025/07/24 2.1.5 Released 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9 version adaptation, synchronously upgrading the dockerfile base image to sglang 0.4.9.post3&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2025/07/23 2.1.4 Released 
  &lt;ul&gt; 
   &lt;li&gt;Bug Fixes 
    &lt;ul&gt; 
     &lt;li&gt;Fixed the issue of excessive memory consumption during the &lt;code&gt;MFR&lt;/code&gt; step in the &lt;code&gt;pipeline&lt;/code&gt; backend under certain scenarios #2771&lt;/li&gt; 
     &lt;li&gt;Fixed the inaccurate matching between &lt;code&gt;image&lt;/code&gt;/&lt;code&gt;table&lt;/code&gt; and &lt;code&gt;caption&lt;/code&gt;/&lt;code&gt;footnote&lt;/code&gt; under certain conditions #3129&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2025/07/16 2.1.1 Released 
  &lt;ul&gt; 
   &lt;li&gt;Bug fixes 
    &lt;ul&gt; 
     &lt;li&gt;Fixed text block content loss issue that could occur in certain &lt;code&gt;pipeline&lt;/code&gt; scenarios #3005&lt;/li&gt; 
     &lt;li&gt;Fixed issue where &lt;code&gt;sglang-client&lt;/code&gt; required unnecessary packages like &lt;code&gt;torch&lt;/code&gt; #2968&lt;/li&gt; 
     &lt;li&gt;Updated &lt;code&gt;dockerfile&lt;/code&gt; to fix incomplete text content parsing due to missing fonts in Linux #2915&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Usability improvements 
    &lt;ul&gt; 
     &lt;li&gt;Updated &lt;code&gt;compose.yaml&lt;/code&gt; to facilitate direct startup of &lt;code&gt;sglang-server&lt;/code&gt;, &lt;code&gt;mineru-api&lt;/code&gt;, and &lt;code&gt;mineru-gradio&lt;/code&gt; services&lt;/li&gt; 
     &lt;li&gt;Launched brand new &lt;a href="https://opendatalab.github.io/MinerU/"&gt;online documentation site&lt;/a&gt;, simplified readme, providing better documentation experience&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;2025/07/05 Version 2.1.0 Released 
  &lt;ul&gt; 
   &lt;li&gt;This is the first major update of MinerU 2, which includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes. The detailed update contents are as follows:&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimizations:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Significantly improved preprocessing speed for documents with specific resolutions (around 2000 pixels on the long side).&lt;/li&gt; 
     &lt;li&gt;Greatly enhanced post-processing speed when the &lt;code&gt;pipeline&lt;/code&gt; backend handles batch processing of documents with fewer pages (&amp;lt;10 pages).&lt;/li&gt; 
     &lt;li&gt;Layout analysis speed of the &lt;code&gt;pipeline&lt;/code&gt; backend has been increased by approximately 20%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Experience Enhancements:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Built-in ready-to-use &lt;code&gt;fastapi service&lt;/code&gt; and &lt;code&gt;gradio webui&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#advanced-usage-via-api-webui-sglang-clientserver"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;Adapted to &lt;code&gt;sglang&lt;/code&gt; version &lt;code&gt;0.4.8&lt;/code&gt;, significantly reducing the GPU memory requirements for the &lt;code&gt;vlm-sglang&lt;/code&gt; backend. It can now run on graphics cards with as little as &lt;code&gt;8GB GPU memory&lt;/code&gt; (Turing architecture or newer).&lt;/li&gt; 
     &lt;li&gt;Added transparent parameter passing for all commands related to &lt;code&gt;sglang&lt;/code&gt;, allowing the &lt;code&gt;sglang-engine&lt;/code&gt; backend to receive all &lt;code&gt;sglang&lt;/code&gt; parameters consistently with the &lt;code&gt;sglang-server&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Supports feature extensions based on configuration files, including &lt;code&gt;custom formula delimiters&lt;/code&gt;, &lt;code&gt;enabling heading classification&lt;/code&gt;, and &lt;code&gt;customizing local model directories&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#extending-mineru-functionality-with-configuration-files"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Features:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Updated the &lt;code&gt;pipeline&lt;/code&gt; backend with the PP-OCRv5 multilingual text recognition model, supporting text recognition in 37 languages such as French, Spanish, Portuguese, Russian, and Korean, with an average accuracy improvement of over 30%. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Introduced limited support for vertical text layout in the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;History Log&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/20 2.0.6 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed occasional parsing interruptions caused by invalid block content in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed parsing interruptions caused by incomplete table structures in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/17 2.0.5 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where models were still required to be downloaded in the &lt;code&gt;sglang-client&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the &lt;code&gt;sglang-client&lt;/code&gt; mode unnecessarily depended on packages like &lt;code&gt;torch&lt;/code&gt; during runtime.&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where only the first instance would take effect when attempting to launch multiple &lt;code&gt;sglang-client&lt;/code&gt; instances via multiple URLs within the same process&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/15 2.0.3 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed a configuration file key-value update error that occurred when downloading model type was set to &lt;code&gt;all&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the formula and table feature toggle switches were not working in &lt;code&gt;command line mode&lt;/code&gt;, causing the features to remain enabled.&lt;/li&gt; 
   &lt;li&gt;Fixed compatibility issues with sglang version 0.4.7 in the &lt;code&gt;sglang-engine&lt;/code&gt; mode.&lt;/li&gt; 
   &lt;li&gt;Updated Dockerfile and installation documentation for deploying the full version of MinerU in sglang environment&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/13 2.0.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New Architecture&lt;/strong&gt;: MinerU 2.0 has been deeply restructured in code organization and interaction methods, significantly improving system usability, maintainability, and extensibility. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Removal of Third-party Dependency Limitations&lt;/strong&gt;: Completely eliminated the dependency on &lt;code&gt;pymupdf&lt;/code&gt;, moving the project toward a more open and compliant open-source direction.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ready-to-use, Easy Configuration&lt;/strong&gt;: No need to manually edit JSON configuration files; most parameters can now be set directly via command line or API.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Automatic Model Management&lt;/strong&gt;: Added automatic model download and update mechanisms, allowing users to complete model deployment without manual intervention.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Offline Deployment Friendly&lt;/strong&gt;: Provides built-in model download commands, supporting deployment requirements in completely offline environments.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Streamlined Code Structure&lt;/strong&gt;: Removed thousands of lines of redundant code, simplified class inheritance logic, significantly improving code readability and development efficiency.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Unified Intermediate Format Output&lt;/strong&gt;: Adopted standardized &lt;code&gt;middle_json&lt;/code&gt; format, compatible with most secondary development scenarios based on this format, ensuring seamless ecosystem business migration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Model&lt;/strong&gt;: MinerU 2.0 integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-speed, high-precision document understanding. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Small Model, Big Capabilities&lt;/strong&gt;: With parameters under 1B, yet surpassing traditional 72B-level vision-language models (VLMs) in parsing accuracy.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multiple Functions in One&lt;/strong&gt;: A single model covers multilingual recognition, handwriting recognition, layout analysis, table parsing, formula recognition, reading order sorting, and other core tasks.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ultimate Inference Speed&lt;/strong&gt;: Achieves peak throughput exceeding 10,000 tokens/s through &lt;code&gt;sglang&lt;/code&gt; acceleration on a single NVIDIA 4090 card, easily handling large-scale document processing requirements.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Online Experience&lt;/strong&gt;: You can experience our brand-new VLM model on &lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;MinerU.net&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;Hugging Face&lt;/a&gt;, and &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Incompatible Changes Notice&lt;/strong&gt;: To improve overall architectural rationality and long-term maintainability, this version contains some incompatible changes: 
    &lt;ul&gt; 
     &lt;li&gt;Python package name changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;, and the command-line tool changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;. Please update your scripts and command calls accordingly.&lt;/li&gt; 
     &lt;li&gt;For modular system design and ecosystem consistency considerations, MinerU 2.0 no longer includes the LibreOffice document conversion module. If you need to process Office documents, we recommend converting them to PDF format through an independently deployed LibreOffice service before proceeding with subsequent parsing operations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/05/24 Release 1.3.12&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for PPOCRv5 models, updated &lt;code&gt;ch_server&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt;, and &lt;code&gt;ch_lite&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;In testing, we found that PPOCRv5(server) has some improvement for handwritten documents, but has slightly lower accuracy than v4_server_doc for other document types, so the default ch model remains unchanged as &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Since PPOCRv5 has enhanced recognition capabilities for handwriting and special characters, you can manually choose the PPOCRv5 model for Japanese-Traditional Chinese mixed scenarios and handwritten documents&lt;/li&gt; 
     &lt;li&gt;You can select the appropriate model through the lang parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line): 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;ch&lt;/code&gt;: &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (default) (Chinese/English/Japanese/Traditional Chinese mixed/15K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_server&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_mobile&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Added support for handwritten documents through optimized layout recognition of handwritten text areas 
    &lt;ul&gt; 
     &lt;li&gt;This feature is supported by default, no additional configuration required&lt;/li&gt; 
     &lt;li&gt;You can refer to the instructions above to manually select the PPOCRv5 model for better handwritten document parsing results&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;modelscope&lt;/code&gt; demos have been updated to versions that support handwriting recognition and PPOCRv5 models, which you can experience online&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/29 Release 1.3.10&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for custom formula delimiters, which can be configured by modifying the &lt;code&gt;latex-delimiter-config&lt;/code&gt; section in the &lt;code&gt;magic-pdf.json&lt;/code&gt; file in your user directory.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/27 Release 1.3.9&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Optimized formula parsing functionality, improved formula rendering success rate&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/23 Release 1.3.8&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The default &lt;code&gt;ocr&lt;/code&gt; model (&lt;code&gt;ch&lt;/code&gt;) has been updated to &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; is trained on a mixture of more Chinese document data and PP-OCR training data based on &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, adding recognition capabilities for some traditional Chinese characters, Japanese, and special characters. It can recognize over 15,000 characters and improves both document-specific and general text recognition abilities.&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_recognition.html#_3"&gt;Performance comparison of PP-OCRv4_server_rec_doc/PP-OCRv4_server_rec/PP-OCRv4_mobile_rec&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;After verification, the &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; model shows significant accuracy improvements in Chinese/English/Japanese/Traditional Chinese in both single language and mixed language scenarios, with comparable speed to &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, making it suitable for most use cases.&lt;/li&gt; 
     &lt;li&gt;In some pure English scenarios, &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; may have word adhesion issues, while &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; performs better in these cases. Therefore, we've kept the &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; model, which users can access by adding the parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/22 Release 1.3.7&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where the lang parameter was ineffective during table parsing model initialization&lt;/li&gt; 
   &lt;li&gt;Fixed the significant speed reduction of OCR and table parsing in &lt;code&gt;cpu&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/16 Release 1.3.4&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Slightly improved OCR-det speed by removing some unnecessary blocks&lt;/li&gt; 
   &lt;li&gt;Fixed page-internal sorting errors caused by footnotes in certain cases&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/12 Release 1.3.2&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed dependency version incompatibility issues when installing on Windows with Python 3.13&lt;/li&gt; 
   &lt;li&gt;Optimized memory usage during batch inference&lt;/li&gt; 
   &lt;li&gt;Improved parsing of tables rotated 90 degrees&lt;/li&gt; 
   &lt;li&gt;Enhanced parsing of oversized tables in financial report samples&lt;/li&gt; 
   &lt;li&gt;Fixed the occasional word adhesion issue in English text areas when OCR language is not specified (model update required)&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/08 Release 1.3.1&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed several compatibility issues 
    &lt;ul&gt; 
     &lt;li&gt;Added support for Python 3.13&lt;/li&gt; 
     &lt;li&gt;Made final adaptations for outdated Linux systems (such as CentOS 7) with no guarantee of continued support in future versions, &lt;a href="https://github.com/opendatalab/MinerU/issues/1004"&gt;installation instructions&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/03 Release 1.3.0&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Installation and compatibility optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Resolved compatibility issues caused by &lt;code&gt;detectron2&lt;/code&gt; by removing &lt;code&gt;layoutlmv3&lt;/code&gt; usage in layout&lt;/li&gt; 
     &lt;li&gt;Extended torch version compatibility to 2.2~2.6 (excluding 2.5)&lt;/li&gt; 
     &lt;li&gt;Added CUDA compatibility for versions 11.8/12.4/12.6/12.8 (CUDA version determined by torch), solving compatibility issues for users with 50-series and H-series GPUs&lt;/li&gt; 
     &lt;li&gt;Extended Python compatibility to versions 3.10~3.12, fixing the issue of automatic downgrade to version 0.6.1 when installing in non-3.10 environments&lt;/li&gt; 
     &lt;li&gt;Optimized offline deployment process, eliminating the need to download any model files after successful deployment&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Enhanced parsing speed for batches of small files by supporting batch processing of multiple PDF files (&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/demo/batch_demo.py"&gt;script example&lt;/a&gt;), with formula parsing speed improved by up to 1400% and overall parsing speed improved by up to 500% compared to version 1.0.1&lt;/li&gt; 
     &lt;li&gt;Reduced memory usage and improved parsing speed by optimizing MFR model loading and usage (requires re-running the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/how_to_download_models_zh_cn.md"&gt;model download process&lt;/a&gt; to get incremental updates to model files)&lt;/li&gt; 
     &lt;li&gt;Optimized GPU memory usage, requiring only 6GB minimum to run this project&lt;/li&gt; 
     &lt;li&gt;Improved running speed on MPS devices&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Parsing effect optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Updated MFR model to &lt;code&gt;unimernet(2503)&lt;/code&gt;, fixing line break loss issues in multi-line formulas&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Usability optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Completely replaced the &lt;code&gt;paddle&lt;/code&gt; framework and &lt;code&gt;paddleocr&lt;/code&gt; in the project by using &lt;code&gt;paddleocr2torch&lt;/code&gt;, resolving conflicts between &lt;code&gt;paddle&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt;, as well as thread safety issues caused by the &lt;code&gt;paddle&lt;/code&gt; framework&lt;/li&gt; 
     &lt;li&gt;Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/03/03 1.2.1 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the impact on punctuation marks during full-width to half-width conversion of letters and numbers&lt;/li&gt; 
   &lt;li&gt;Fixed caption matching inaccuracies in certain scenarios&lt;/li&gt; 
   &lt;li&gt;Fixed formula span loss issues in certain scenarios&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/02/24 1.2.0 released&lt;/summary&gt; 
  &lt;p&gt;This version includes several fixes and improvements to enhance parsing efficiency and accuracy:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Increased classification speed for PDF documents in auto mode.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Improved parsing logic for documents containing watermarks, significantly enhancing the parsing results for such documents.&lt;/li&gt; 
     &lt;li&gt;Enhanced the matching logic for multiple images/tables and captions within a single page, improving the accuracy of image-text matching in complex layouts.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed an issue where image/table spans were incorrectly filled into text blocks under certain conditions.&lt;/li&gt; 
     &lt;li&gt;Resolved an issue where title blocks were empty in some cases.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/22 1.1.0 released&lt;/summary&gt; 
  &lt;p&gt;In this version we have focused on improving parsing accuracy and efficiency:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model capability upgrade&lt;/strong&gt; (requires re-executing the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;model download process&lt;/a&gt; to obtain incremental updates of model files) 
    &lt;ul&gt; 
     &lt;li&gt;The layout recognition model has been upgraded to the latest &lt;code&gt;doclayout_yolo(2501)&lt;/code&gt; model, improving layout recognition accuracy.&lt;/li&gt; 
     &lt;li&gt;The formula parsing model has been upgraded to the latest &lt;code&gt;unimernet(2501)&lt;/code&gt; model, improving formula recognition accuracy.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;On devices that meet certain configuration requirements (16GB+ VRAM), by optimizing resource usage and restructuring the processing pipeline, overall parsing speed has been increased by more than 50%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing effect optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Added a new heading classification feature (testing version, enabled by default) to the online demo (&lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;mineru.net&lt;/a&gt;/&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;huggingface&lt;/a&gt;/&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;modelscope&lt;/a&gt;), which supports hierarchical classification of headings, thereby enhancing document structuring.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/10 1.0.1 released&lt;/summary&gt; 
  &lt;p&gt;This is our first official release, where we have introduced a completely new API interface and enhanced compatibility through extensive refactoring, as well as a brand new automatic language identification feature:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New API Interface&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.&lt;/li&gt; 
     &lt;li&gt;For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enhanced Compatibility&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By optimizing the dependency environment and configuration items, we ensure stable and efficient operation on ARM architecture Linux systems.&lt;/li&gt; 
     &lt;li&gt;We have deeply integrated with Huawei Ascend NPU acceleration, providing autonomous and controllable high-performance computing capabilities. This supports the localization and development of AI application platforms in China. &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/README_Ascend_NPU_Acceleration_zh_CN.md"&gt;Ascend NPU Acceleration&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Automatic Language Identification&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By introducing a new language recognition model, setting the &lt;code&gt;lang&lt;/code&gt; configuration to &lt;code&gt;auto&lt;/code&gt; during document parsing will automatically select the appropriate OCR language model, improving the accuracy of scanned document parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/22 0.10.0 released&lt;/summary&gt; 
  &lt;p&gt;Introducing hybrid OCR text extraction capabilities:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Significantly improved parsing performance in complex text distribution scenarios such as dense formulas, irregular span regions, and text represented by images.&lt;/li&gt; 
   &lt;li&gt;Combines the dual advantages of accurate content extraction and faster speed in text mode, and more precise span/line region recognition in OCR mode.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/15 0.9.3 released&lt;/summary&gt; 
  &lt;p&gt;Integrated &lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt; for table recognition, improving single-table parsing speed by more than 10 times, with higher accuracy and lower GPU memory usage.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/06 0.9.2 released&lt;/summary&gt; 
  &lt;p&gt;Integrated the &lt;a href="https://huggingface.co/U4R/StructTable-InternVL2-1B"&gt;StructTable-InternVL2-1B&lt;/a&gt; model for table recognition functionality.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/10/31 0.9.0 released&lt;/summary&gt; 
  &lt;p&gt;This is a major new version with extensive code refactoring, addressing numerous issues, improving performance, reducing hardware requirements, and enhancing usability:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Refactored the sorting module code to use &lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt; for reading order sorting, ensuring high accuracy in various layouts.&lt;/li&gt; 
   &lt;li&gt;Refactored the paragraph concatenation module to achieve good results in cross-column, cross-page, cross-figure, and cross-table scenarios.&lt;/li&gt; 
   &lt;li&gt;Refactored the list and table of contents recognition functions, significantly improving the accuracy of list blocks and table of contents blocks, as well as the parsing of corresponding text paragraphs.&lt;/li&gt; 
   &lt;li&gt;Refactored the matching logic for figures, tables, and descriptive text, greatly enhancing the accuracy of matching captions and footnotes to figures and tables, and reducing the loss rate of descriptive text to near zero.&lt;/li&gt; 
   &lt;li&gt;Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations"&gt;OCR Language Support List&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Added memory recycling logic and other memory optimization measures, significantly reducing memory usage. The memory requirement for enabling all acceleration features except table acceleration (layout/formula/OCR) has been reduced from 16GB to 8GB, and the memory requirement for enabling all acceleration features has been reduced from 24GB to 10GB.&lt;/li&gt; 
   &lt;li&gt;Optimized configuration file feature switches, adding an independent formula detection switch to significantly improve speed and parsing results when formula detection is not needed.&lt;/li&gt; 
   &lt;li&gt;Integrated &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit 1.0&lt;/a&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;Added the self-developed &lt;code&gt;doclayout_yolo&lt;/code&gt; model, which speeds up processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with &lt;code&gt;layoutlmv3&lt;/code&gt; via the configuration file.&lt;/li&gt; 
     &lt;li&gt;Upgraded formula parsing to &lt;code&gt;unimernet 0.2.1&lt;/code&gt;, improving formula parsing accuracy while significantly reducing memory usage.&lt;/li&gt; 
     &lt;li&gt;Due to the repository change for &lt;code&gt;PDF-Extract-Kit 1.0&lt;/code&gt;, you need to re-download the model. Please refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;How to Download Models&lt;/a&gt; for detailed steps.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/27 Version 0.8.1 released&lt;/summary&gt; 
  &lt;p&gt;Fixed some bugs, and providing a &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web_demo/README.md"&gt;localized deployment version&lt;/a&gt; of the &lt;a href="https://opendatalab.com/OpenSourceTools/Extractor/PDF/"&gt;online demo&lt;/a&gt; and the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web/README.md"&gt;front-end interface&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/09 Version 0.8.0 released&lt;/summary&gt; 
  &lt;p&gt;Supporting fast deployment with Dockerfile, and launching demos on Huggingface and Modelscope.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/30 Version 0.7.1 released&lt;/summary&gt; 
  &lt;p&gt;Add paddle tablemaster table recognition option&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/09 Version 0.7.0b1 released&lt;/summary&gt; 
  &lt;p&gt;Simplified installation process, added table recognition functionality&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/01 Version 0.6.2b1 released&lt;/summary&gt; 
  &lt;p&gt;Optimized dependency conflict issues and installation documentation&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/07/05 Initial open-source release&lt;/summary&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;h1&gt;MinerU&lt;/h1&gt; 
&lt;h2&gt;Project Introduction&lt;/h2&gt; 
&lt;p&gt;MinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format. MinerU was born during the pre-training process of &lt;a href="https://github.com/InternLM/InternLM"&gt;InternLM&lt;/a&gt;. We focus on solving symbol conversion issues in scientific literature and hope to contribute to technological development in the era of large models. Compared to well-known commercial products, MinerU is still young. If you encounter any issues or if the results are not as expected, please submit an issue on &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;issue&lt;/a&gt; and &lt;strong&gt;attach the relevant PDF&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c"&gt;https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.&lt;/li&gt; 
 &lt;li&gt;Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.&lt;/li&gt; 
 &lt;li&gt;Preserve the structure of the original document, including headings, paragraphs, lists, etc.&lt;/li&gt; 
 &lt;li&gt;Extract images, image descriptions, tables, table titles, and footnotes.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert formulas in the document to LaTeX format.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert tables in the document to HTML format.&lt;/li&gt; 
 &lt;li&gt;Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality.&lt;/li&gt; 
 &lt;li&gt;OCR supports detection and recognition of 84 languages.&lt;/li&gt; 
 &lt;li&gt;Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats.&lt;/li&gt; 
 &lt;li&gt;Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality.&lt;/li&gt; 
 &lt;li&gt;Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration&lt;/li&gt; 
 &lt;li&gt;Compatible with Windows, Linux, and Mac platforms.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;If you encounter any installation issues, please first consult the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#faq"&gt;FAQ&lt;/a&gt;. &lt;br&gt; If the parsing results are not as expected, refer to the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#known-issues"&gt;Known Issues&lt;/a&gt;. &lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;Online Experience&lt;/h2&gt; 
&lt;h3&gt;Official online web application&lt;/h3&gt; 
&lt;p&gt;The official online version has the same functionality as the client, with a beautiful interface and rich features, requires login to use&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab"&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Gradio-based online demo&lt;/h3&gt; 
&lt;p&gt;A WebUI developed based on Gradio, with a simple interface and only core parsing functionality, no login required&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope"&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace"&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Local Deployment&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Pre-installation Noticeâ€”Hardware and Software Environment Support&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To ensure the stability and reliability of the project, we only optimize and test for specific hardware and software environments during development. This ensures that users deploying and running the project on recommended system configurations will get the best performance with the fewest compatibility issues.&lt;/p&gt; 
 &lt;p&gt;By focusing resources on the mainline environment, our team can more efficiently resolve potential bugs and develop new features.&lt;/p&gt; 
 &lt;p&gt;In non-mainline environments, due to the diversity of hardware and software configurations, as well as third-party dependency compatibility issues, we cannot guarantee 100% project availability. Therefore, for users who wish to use this project in non-recommended environments, we suggest carefully reading the documentation and FAQ first. Most issues already have corresponding solutions in the FAQ. We also encourage community feedback to help us gradually expand support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;Parsing Backend&lt;/td&gt; 
   &lt;td&gt;pipeline&lt;/td&gt; 
   &lt;td&gt;vlm-transformers&lt;/td&gt; 
   &lt;td&gt;vlm-sglang&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operating System&lt;/td&gt; 
   &lt;td&gt;Linux / Windows / macOS&lt;/td&gt; 
   &lt;td&gt;Linux / Windows&lt;/td&gt; 
   &lt;td&gt;Linux / Windows (via WSL2)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Inference Support&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td colspan="2"&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPU Requirements&lt;/td&gt; 
   &lt;td&gt;Turing architecture and later, 6GB+ VRAM or Apple Silicon&lt;/td&gt; 
   &lt;td colspan="2"&gt;Turing architecture and later, 8GB+ VRAM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Memory Requirements&lt;/td&gt; 
   &lt;td colspan="3"&gt;Minimum 16GB+, recommended 32GB+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Disk Space Requirements&lt;/td&gt; 
   &lt;td colspan="3"&gt;20GB+, SSD recommended&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python Version&lt;/td&gt; 
   &lt;td colspan="3"&gt;3.10-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Install MinerU&lt;/h3&gt; 
&lt;h4&gt;Install MinerU using pip or uv&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install uv
uv pip install -U "mineru[core]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install MinerU from source code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/opendatalab/MinerU.git
cd MinerU
uv pip install -e .[core]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;code&gt;mineru[core]&lt;/code&gt; includes all core features except &lt;code&gt;sglang&lt;/code&gt; acceleration, compatible with Windows / Linux / macOS systems, suitable for most users. If you need to use &lt;code&gt;sglang&lt;/code&gt; acceleration for VLM model inference or install a lightweight client on edge devices, please refer to the documentation &lt;a href="https://opendatalab.github.io/MinerU/quick_start/extension_modules/"&gt;Extension Modules Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;h4&gt;Deploy MinerU using Docker&lt;/h4&gt; 
&lt;p&gt;MinerU provides a convenient Docker deployment method, which helps quickly set up the environment and solve some tricky environment compatibility issues. You can get the &lt;a href="https://opendatalab.github.io/MinerU/quick_start/docker_deployment/"&gt;Docker Deployment Instructions&lt;/a&gt; in the documentation.&lt;/p&gt; 
&lt;hr&gt; 
&lt;h3&gt;Using MinerU&lt;/h3&gt; 
&lt;p&gt;The simplest command line invocation is:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mineru -p &amp;lt;input_path&amp;gt; -o &amp;lt;output_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use MinerU for PDF parsing through various methods such as command line, API, and WebUI. For detailed instructions, please refer to the &lt;a href="https://opendatalab.github.io/MinerU/usage/"&gt;Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;TODO&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Reading order based on the model&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Recognition of &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt; in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Table recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Heading Classification&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Handwritten Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Vertical Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Latin Accent Mark Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Code block recognition in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/chemical_knowledge_introduction/introduction.pdf"&gt;Chemical formula recognition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Geometric shape recognition&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Known Issues&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reading order is determined by the model based on the spatial distribution of readable content, and may be out of order in some areas under extremely complex layouts.&lt;/li&gt; 
 &lt;li&gt;Limited support for vertical text.&lt;/li&gt; 
 &lt;li&gt;Tables of contents and lists are recognized through rules, and some uncommon list formats may not be recognized.&lt;/li&gt; 
 &lt;li&gt;Code blocks are not yet supported in the layout model.&lt;/li&gt; 
 &lt;li&gt;Comic books, art albums, primary school textbooks, and exercises cannot be parsed well.&lt;/li&gt; 
 &lt;li&gt;Table recognition may result in row/column recognition errors in complex tables.&lt;/li&gt; 
 &lt;li&gt;OCR recognition may produce inaccurate characters in PDFs of lesser-known languages (e.g., diacritical marks in Latin script, easily confused characters in Arabic script).&lt;/li&gt; 
 &lt;li&gt;Some formulas may not render correctly in Markdown.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you encounter any issues during usage, you can first check the &lt;a href="https://opendatalab.github.io/MinerU/faq/"&gt;FAQ&lt;/a&gt; for solutions.&lt;/li&gt; 
 &lt;li&gt;If your issue remains unresolved, you may also use &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;DeepWiki&lt;/a&gt; to interact with an AI assistant, which can address most common problems.&lt;/li&gt; 
 &lt;li&gt;If you still cannot resolve the issue, you are welcome to join our community via &lt;a href="https://discord.gg/Tdedn9GTXq"&gt;Discord&lt;/a&gt; or &lt;a href="http://mineru.space/s/V85Yl"&gt;WeChat&lt;/a&gt; to discuss with other users and developers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;All Thanks To Our Contributors&lt;/h1&gt; 
&lt;a href="https://github.com/opendatalab/MinerU/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=opendatalab/MinerU"&gt; &lt;/a&gt; 
&lt;h1&gt;License Information&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/LICENSE.md"&gt;LICENSE.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Currently, some models in this project are trained based on YOLO. However, since YOLO follows the AGPL license, it may impose restrictions on certain use cases. In future iterations, we plan to explore and replace these with models under more permissive licenses to enhance user-friendliness and flexibility.&lt;/p&gt; 
&lt;h1&gt;Acknowledgments&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/DocLayout-YOLO"&gt;DocLayout-YOLO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/UniMERNet"&gt;UniMERNet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;PaddleOCR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frotms/PaddleOCR2Pytorch"&gt;PaddleOCR2Pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Sanster/xy-cut"&gt;xy-cut&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LlmKira/fast-langdetect"&gt;fast-langdetect&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pypdfium2-team/pypdfium2"&gt;pypdfium2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/datalab-to/pdftext"&gt;pdftext&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pdfminer/pdfminer.six"&gt;pdfminer.six&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/py-pdf/pypdf"&gt;pypdf&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2024mineruopensourcesolutionprecise,
      title={MinerU: An Open-Source Solution for Precise Document Content Extraction}, 
      author={Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
      year={2024},
      eprint={2409.18839},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.18839}, 
}

@article{he2024opendatalab,
  title={Opendatalab: Empowering general artificial intelligence with open datasets},
  author={He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},
  journal={arXiv preprint arXiv:2407.13773},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;a&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date&amp;amp;theme=dark"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date"&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date"&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h1&gt;Links&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenDCAI/DataFlow"&gt;Easy Data Preparation with latest LLMs-based Operators and Pipelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/Vis3"&gt;Vis3 (OSS browser based on s3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/labelU"&gt;LabelU (A Lightweight Multi-modal Data Annotation Tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/LabelLLM"&gt;LabelLLM (An Open-source LLM Dialogue Annotation Platform)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit (A Comprehensive Toolkit for High-Quality PDF Content Extraction)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/OmniDocBench"&gt;OmniDocBench (A Comprehensive Benchmark for Document Parsing and Evaluation)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/magic-html"&gt;Magic-HTML (Mixed web page extraction tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InternLM/magic-doc"&gt;Magic-Doc (Fast speed ppt/pptx/doc/docx/pdf extraction tool)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>