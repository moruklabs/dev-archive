<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Mon, 05 Jan 2026 01:37:18 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Financial data platform for analysts, quants and AI agents.&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-light.svg?raw=true#gh-light-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers" /&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20" /&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.&lt;/p&gt; 
&lt;p&gt;ODP operates as the "connect once, consume everywhere" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/python/reference"&gt;https://docs.openbb.co/python/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the Open Data Platform provides the open-source data integration foundation, &lt;strong&gt;OpenBB Workspace&lt;/strong&gt; offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's "connect once, consume everywhere" architecture enables seamless integration between the two.&lt;/p&gt; 
&lt;p&gt;You can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;. &lt;a href="https://pro.openbb.co"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating Open Data Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run an ODP backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate the ODP Backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: Open Data Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The ODP Python Package can be installed from &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/python/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ODP CLI installation&lt;/h3&gt; 
&lt;p&gt;The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ‚≠êÔ∏è)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/python/developer"&gt;Developer Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;among the existing issues&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the Open Data Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800" /&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>sherlock-project/sherlock</title>
      <link>https://github.com/sherlock-project/sherlock</link>
      <description>&lt;p&gt;Hunt down social media accounts by username across social networks&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://sherlock-project.github.io/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/sherlock-logo.png" alt="sherlock" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;span&gt;Hunt down social media accounts by username across &lt;a href="https://sherlockproject.xyz/sites"&gt;400+ social networks&lt;/a&gt;&lt;/span&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://sherlockproject.xyz/installation"&gt;Installation&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/usage"&gt;Usage&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/contribute"&gt;Contributing&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="70%" height="70%" src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/demo.png" alt="demo" /&gt; &lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;br /&gt; Packages for ParrotOS and Ubuntu 24.04, maintained by a third party, appear to be &lt;strong&gt;broken&lt;/strong&gt;.&lt;br /&gt; Users of these systems should defer to pipx/pip or Docker.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pipx install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip&lt;/code&gt; may be used in place of &lt;code&gt;pipx&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;docker run -it --rm sherlock/sherlock&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dnf install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Community-maintained packages are available for Debian (&amp;gt;= 13), Ubuntu (&amp;gt;= 22.10), Homebrew, Kali, and BlackArch. These packages are not directly supported or maintained by the Sherlock Project.&lt;/p&gt; 
&lt;p&gt;See all alternative installation methods &lt;a href="https://sherlockproject.xyz/installation"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General usage&lt;/h2&gt; 
&lt;p&gt;To search for only one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To search for more than one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user1 user2 user3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Accounts found will be stored in an individual text file with the corresponding username (e.g &lt;code&gt;user123.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ sherlock --help
usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT]
                [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--xlsx]
                [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE]
                [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color]
                [--browse] [--local] [--nsfw]
                USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.14.3)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.
                        Check similar usernames using {?} (replace to '_', '-', '.').

optional arguments:
  -h, --help            show this help message and exit
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results will be
                        saved to this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result will be saved
                        to this file.
  --tor, -t             Make requests over Tor; increases runtime; requires Tor to be
                        installed and in system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each request;
                        increases runtime; requires Tor to be installed and in system
                        path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --xlsx                Create the standard file for the modern Microsoft Excel
                        spreadsheet (xlsx).
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple options to
                        specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g. socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON file.
  --timeout TIMEOUT     Time (in seconds) to wait for response to requests (Default: 60)
  --print-all           Output sites where the username was not found.
  --print-found         Output sites where the username was found.
  --no-color            Don't color terminal output
  --browse, -b          Browse to all results on default browser.
  --local, -l           Force the use of the local data.json file.
  --nsfw                Include checking of NSFW sites from default list.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Apify Actor Usage &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/actor-badge?actor=netmilk/sherlock" alt="Sherlock Actor" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/ext/run-on-apify.png" alt="Run Sherlock Actor on Apify" width="176" height="39" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can run Sherlock in the cloud without installation using the &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;Sherlock Actor&lt;/a&gt; on &lt;a href="https://apify.com?fpr=sherlock"&gt;Apify&lt;/a&gt; free of charge.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ echo '{"usernames":["user123"]}' | apify call -so netmilk/sherlock
[{
  "username": "user123",
  "links": [
    "https://www.1337x.to/user/user123/",
    ...
  ]
}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more about the &lt;a href="https://raw.githubusercontent.com/sherlock-project/sherlock/.actor/README.md"&gt;Sherlock Actor&lt;/a&gt;, including how to use it programmatically via the Apify &lt;a href="https://apify.com/netmilk/sherlock/api?fpr=sherlock"&gt;API&lt;/a&gt;, &lt;a href="https://docs.apify.com/cli/?fpr=sherlock"&gt;CLI&lt;/a&gt; and &lt;a href="https://docs.apify.com/sdk?fpr=sherlock"&gt;JS/TS and Python SDKs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Thank you to everyone who has contributed to Sherlock! ‚ù§Ô∏è&lt;/p&gt; 
&lt;a href="https://github.com/sherlock-project/sherlock/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?&amp;amp;columns=25&amp;amp;max=10000&amp;amp;&amp;amp;repo=sherlock-project/sherlock" alt="contributors" /&gt; &lt;/a&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date" /&gt; 
 &lt;img alt="Sherlock Project Star History Chart" src="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT ¬© Sherlock Project&lt;br /&gt; Original Creator - &lt;a href="https://github.com/sdushantha"&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Reference Links --&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;‚å®Ô∏è Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;üñ•Ô∏è Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;‚å®Ô∏è Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03‚ÄØPM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PrimeIntellect-ai/verifiers</title>
      <link>https://github.com/PrimeIntellect-ai/verifiers</link>
      <description>&lt;p&gt;Our library for RL environments + evals&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/40c36e38-c5bd-4c5a-9cb3-f7b902cd155d" /&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/6414bc9b-126b-41ca-9307-9e982430cde8" /&gt; 
  &lt;img alt="Prime Intellect" src="https://github.com/user-attachments/assets/6414bc9b-126b-41ca-9307-9e982430cde8" width="312" style="max-width: 100%;" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3 align="center"&gt; Verifiers: Environments for LLM Reinforcement Learning &lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://docs.primeintellect.ai/verifiers"&gt;Documentation&lt;/a&gt; ‚Ä¢ &lt;a href="https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars"&gt;Environments Hub&lt;/a&gt; ‚Ä¢ &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;PRIME-RL&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/style.yml"&gt; &lt;img src="https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/style.yml/badge.svg?sanitize=true" alt="Style" /&gt; &lt;/a&gt; &lt;a href="https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/test.yml"&gt; &lt;img src="https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/test.yml/badge.svg?sanitize=true" alt="Test" /&gt; &lt;/a&gt; &lt;a href="https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/publish-envs.yml"&gt; &lt;img src="https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/publish-envs.yml/badge.svg?sanitize=true" alt="Envs" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: This repository hosts community-contributed, lightly reviewed environment implementations. Environments maintained by the Prime Intellect research team are available at &lt;a href="https://github.com/PrimeIntellect-ai/research-environments/tree/main"&gt;research-environments&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;News &amp;amp; Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[11/19/25] v0.1.8 is released, featuring a major refactor of the rollout system to use trajectory-based tracking for token-in token-out training across turns, as well as support for truncated or branching rollouts.&lt;/li&gt; 
 &lt;li&gt;[11/07/25] Verifiers v0.1.7 is released! This includes an improved quickstart configuration for training with [prime-rl], a new included "nano" trainer (&lt;code&gt;vf.RLTrainer&lt;/code&gt;, replacing &lt;code&gt;vf.GRPOTrainer&lt;/code&gt;), and a number of bug fixes and improvements to the documentation.&lt;/li&gt; 
 &lt;li&gt;[10/27/25] A new iteration of the Prime Intellect &lt;a href="https://docs.google.com/spreadsheets/d/13UDfRDjgIZXsMI2s9-Lmn8KSMMsgk2_zsfju6cx_pNU/edit?gid=0#gid=0"&gt;Environments Program&lt;/a&gt; is live!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Verifiers is a library of modular components for creating RL environments and training LLM agents. Environments built with Verifiers can be used directly as LLM evaluations, synthetic data pipelines, or agent harnesses for any OpenAI-compatible model endpoint, in addition to RL training. Verifiers is supported by &lt;code&gt;prime-rl&lt;/code&gt; for large-scale performance-optimized async RL training, includes a minimal &lt;code&gt;transformers&lt;/code&gt;-based trainer (&lt;code&gt;vf.RLTrainer&lt;/code&gt;) for simple algorithmic experiments, and can easily be integrated into any RL training stack which exposes an OpenAI-compatible inference client.&lt;/p&gt; 
&lt;p&gt;Full documentation is available &lt;a href="https://docs.primeintellect.ai/verifiers"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Verifiers is the native library used by Prime Intellect's &lt;a href="https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars"&gt;Environments Hub&lt;/a&gt;; see &lt;a href="https://docs.primeintellect.ai/tutorials-environments/environments"&gt;here&lt;/a&gt; for information about publishing your Environments to the Hub, and &lt;a href="https://github.com/PrimeIntellect-ai/prime-environments"&gt;here&lt;/a&gt; for a collection of Environments built with Verifiers.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Verifiers supports CPU-based environment development and evaluation with API models, as well as large-scale GPU-based RL training with &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;&lt;code&gt;prime-rl&lt;/code&gt;&lt;/a&gt; and several other trainers. Environments built with Verifiers are standalone Python packages that can be installed and used in your own projects, or shared with the community through the &lt;a href="https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars"&gt;Environments Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To get started, install &lt;code&gt;uv&lt;/code&gt; and the &lt;code&gt;prime&lt;/code&gt; &lt;a href="https://github.com/PrimeIntellect-ai/prime-cli"&gt;CLI&lt;/a&gt;, and add &lt;code&gt;verifiers&lt;/code&gt; to your project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
uv init &amp;amp;&amp;amp; uv venv --python 3.12    # to create a new project if needed
uv tool install prime
uv add verifiers
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Select an environment from the &lt;a href="https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars"&gt;Environments Hub&lt;/a&gt; to install:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;prime env install will/wiki-search
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or install an environment from this repo:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run vf-install wordle --from-repo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run a quick evaluation with OpenAI models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run vf-eval wordle -m gpt-5-nano
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For advanced evaluation configurations with the &lt;code&gt;prime&lt;/code&gt; &lt;a href="https://github.com/PrimeIntellect-ai/prime-cli"&gt;CLI&lt;/a&gt;, see &lt;a href="https://docs.primeintellect.ai/tutorials-environments/evaluating"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;RL Training&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;prime-rl&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;We recommend using the &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;&lt;code&gt;prime-rl&lt;/code&gt;&lt;/a&gt; trainer, and provide a basic setup guide below. See the &lt;a href="https://docs.primeintellect.ai/prime-rl"&gt;prime-rl documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;To get started, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run vf-setup
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will clone and install the &lt;code&gt;prime-rl&lt;/code&gt; trainer and its dependencies, and set up a default configuration for training with the included &lt;code&gt;wiki-search&lt;/code&gt; Environment.&lt;/p&gt; 
&lt;p&gt;Then, you can start training with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run prime-rl @ configs/prime-rl/wiki-search.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a tmux session with separate panes for the trainer, orchestrator, and inference server.&lt;/p&gt; 
&lt;h3&gt;vf.RLTrainer&lt;/h3&gt; 
&lt;p&gt;The included &lt;code&gt;RLTrainer&lt;/code&gt; is a minimal, hackable training loop based on &lt;code&gt;transformers.Trainer&lt;/code&gt; that supports both full-parameter finetuning and LoRA training. &lt;code&gt;RLTrainer&lt;/code&gt; can be viewed as a "baby" &lt;code&gt;prime-rl&lt;/code&gt; that adopts a similar default training recipe (async CISPO with one-step off-policy overlap), intended for single-node test runs with dense models. The primary files (&lt;code&gt;trainer.py&lt;/code&gt; and &lt;code&gt;orchestrator.py&lt;/code&gt;, located in &lt;code&gt;verifiers/rl/trainer/&lt;/code&gt;) are under 1000 lines of code, and are designed to be a convenient starting point for writing your own training loop.&lt;/p&gt; 
&lt;p&gt;The feature set is intentionally kept minimal and focused. Users seeking maximum performance, MoE support, multi-node training, multidimensional parallelism, and other advanced features should use the &lt;code&gt;prime-rl&lt;/code&gt; trainer.&lt;/p&gt; 
&lt;p&gt;To use &lt;code&gt;vf.RLTrainer&lt;/code&gt; in your own project, install with RL extras:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add 'verifiers[rl]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, create a training configuration file, e.g. &lt;code&gt;configs/vf-rl/wiki-search.toml&lt;/code&gt;, and do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run vf-rl @ configs/vf-rl/wiki-search.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example configuration files can be created in your project by running &lt;code&gt;uv run vf-setup&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Other Trainers&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;verifiers&lt;/code&gt; is intended to be largely trainer-agnostic. It is supported by [SkyRL] and [Tinker], and is straightforward to support for any trainer which can expose an OpenAI-compatible inference client for rollouts. See the &lt;a href="https://github.com/PrimeIntellect-ai/verifiers/tree/main/integrations"&gt;integrations&lt;/a&gt; directory for more information.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To install &lt;code&gt;verifiers&lt;/code&gt; from source for core library development, or to use the latest &lt;code&gt;main&lt;/code&gt; branch, install with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://raw.githubusercontent.com/PrimeIntellect-ai/verifiers/main/scripts/install.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to develop with RL extras enabled in this repo, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --extra rl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please use the &lt;a href="https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars"&gt;Environments Hub&lt;/a&gt; to share your Environments with the community, rather than PRs to this repo. If you find yourself needing to clone and modify the core library in order to implement key functionality for your project, please open an issue or PR so that we can help you.&lt;/p&gt; 
&lt;h2&gt;Environments&lt;/h2&gt; 
&lt;p&gt;Environments in Verifiers are installable Python modules which can specify dependencies in a &lt;code&gt;pyproject.toml&lt;/code&gt;, and which expose a &lt;code&gt;load_environment&lt;/code&gt; function for instantiation by downstream applications (e.g. trainers). See &lt;code&gt;environments/&lt;/code&gt; for examples.&lt;/p&gt; 
&lt;p&gt;To initialize a blank Environment module template, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run vf-init environment-name # -p /path/to/environments (defaults to "./environments")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install an Environment module into your project, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run vf-install environment-name # -p /path/to/environments (defaults to "./environments") 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install an Environment module from the &lt;a href="https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars"&gt;Environments Hub&lt;/a&gt;, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;prime env install user/environment-name
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install an Environment module from this repo's &lt;code&gt;environments&lt;/code&gt; folder, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run vf-install math-python --from-repo # -b branch_or_commit (defaults to "main")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once an Environment module is installed, you can create an instance of the Environment using &lt;code&gt;load_environment&lt;/code&gt;, passing any necessary args:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.load_environment("environment-name", **env_args)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run a quick evaluation of your Environment with an API-based model, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run vf-eval environment-name -s # run and save eval results locally
# vf-eval -h for config options; defaults to gpt-4.1-mini, 5 prompts, 3 rollouts for each
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you're using Prime Intellect infrastructure, the &lt;a href="https://github.com/PrimeIntellect-ai/prime-cli"&gt;&lt;code&gt;prime&lt;/code&gt; CLI&lt;/a&gt; provides first-class commands for working with Verifiers environments through the &lt;a href="https://docs.primeintellect.ai/tutorials-environments/environments"&gt;Environments Hub&lt;/a&gt;. Install it with &lt;code&gt;uv tool install prime&lt;/code&gt;, authenticate via &lt;code&gt;prime login&lt;/code&gt;, then use &lt;code&gt;prime env push&lt;/code&gt; to publish your package and &lt;code&gt;prime env install owner/name&lt;/code&gt; (optionally pinning a version) to consume it from pods or local machines.&lt;/p&gt; 
&lt;p&gt;The core elements of Environments are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Datasets: a Hugging Face &lt;code&gt;Dataset&lt;/code&gt; with a &lt;code&gt;prompt&lt;/code&gt; column for inputs, and optionally &lt;code&gt;answer (str)&lt;/code&gt; or &lt;code&gt;info (dict)&lt;/code&gt; columns for evaluation (both can be omitted for environments that evaluate based solely on completion quality)&lt;/li&gt; 
 &lt;li&gt;Rollout logic: interactions between models and the environment (e.g. &lt;code&gt;env_response&lt;/code&gt; + &lt;code&gt;is_completed&lt;/code&gt; for any &lt;code&gt;MultiTurnEnv&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Rubrics: an encapsulation for one or more reward functions&lt;/li&gt; 
 &lt;li&gt;Parsers: optional; an encapsulation for reusable parsing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We support both &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style and &lt;code&gt;/v1/completions&lt;/code&gt;-style inference via OpenAI clients, though we generally recommend &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style inference for the vast majority of applications. Both &lt;code&gt;prime-rl&lt;/code&gt; as well as the included &lt;code&gt;vf.RLTrainer&lt;/code&gt; support the full set of &lt;a href="https://docs.vllm.ai/en/stable/api/vllm/sampling_params.html#vllm.sampling_params.SamplingParams"&gt;SamplingParams&lt;/a&gt; exposed by vLLM (via their OpenAI-compatible &lt;a href="https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html"&gt;server&lt;/a&gt; interface), and leveraging this will often be the appropriate way to implement rollout strategies requiring finer-grained control, such as interrupting and resuming generations for interleaved tool use, or enforcing reasoning budgets.&lt;/p&gt; 
&lt;h3&gt;SingleTurnEnv&lt;/h3&gt; 
&lt;p&gt;For tasks requiring only a single response from a model for each prompt, you can use &lt;code&gt;SingleTurnEnv&lt;/code&gt; directly by specifying a Dataset and a Rubric. Rubrics are sets of reward functions, which can be either sync or async.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from datasets import load_dataset
import verifiers as vf

dataset = load_dataset("my-account/my-dataset", split="train")

def reward_A(prompt, completion, info) -&amp;gt; float:
	# reward fn, e.g. correctness
	...

def reward_B(parser, completion) -&amp;gt; float:
	# auxiliary reward fn, e.g. format
	...

async def metric(completion) -&amp;gt; float:
	# non-reward metric, e.g. proper noun count
	...

rubric = vf.Rubric(funcs=[reward_A, reward_B, metric], weights=[1.0, 0.5, 0.0])

vf_env = vf.SingleTurnEnv(
	dataset=dataset,
	rubric=rubric
)

# Async evaluation (recommended)
from openai import AsyncOpenAI
results = await vf_env.evaluate(client=AsyncOpenAI(), model="gpt-4.1-mini", num_examples=100, rollouts_per_example=1)

# Sync evaluation
from openai import OpenAI
results = vf_env.evaluate_sync(client=OpenAI(), model="gpt-4.1-mini", num_examples=100, rollouts_per_example=1)

vf_env.make_dataset(results) # HF dataset format
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Datasets should be formatted with columns for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;'prompt' (List[ChatMessage])&lt;/code&gt; OR &lt;code&gt;'question' (str)&lt;/code&gt; fields 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ChatMessage&lt;/code&gt; = e.g. &lt;code&gt;{'role': 'user', 'content': '...'}&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;if &lt;code&gt;question&lt;/code&gt; is set instead of &lt;code&gt;prompt&lt;/code&gt;, you can also pass &lt;code&gt;system_prompt (str)&lt;/code&gt; and/or &lt;code&gt;few_shot (List[ChatMessage])&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer (str)&lt;/code&gt; AND/OR &lt;code&gt;info (dict)&lt;/code&gt; (both optional, can be omitted entirely)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task (str)&lt;/code&gt;: optional, used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt; for orchestrating composition of Environments and Rubrics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following named attributes available for use by reward functions in your Rubric:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: sequence of input messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;completion&lt;/code&gt;: sequence of messages generated during rollout by model and Environment&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer&lt;/code&gt;: primary answer column, optional (defaults to empty string if omitted)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;state&lt;/code&gt;: can be modified during rollout to accumulate any metadata (&lt;code&gt;state['trajectory']&lt;/code&gt; includes the full list of &lt;code&gt;TrajectoryStep&lt;/code&gt; objects by default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;info&lt;/code&gt;: auxiliary info needed for reward computation (e.g. test cases), optional (defaults to empty dict if omitted)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task&lt;/code&gt;: tag for task type (used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;parser&lt;/code&gt;: the parser object declared. Note: &lt;code&gt;vf.Parser().get_format_reward_func()&lt;/code&gt; is a no-op (always 1.0); use &lt;code&gt;vf.ThinkParser&lt;/code&gt; or a custom parser if you want a real format adherence reward.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Some environments can fully evaluate using only &lt;code&gt;prompt&lt;/code&gt;, &lt;code&gt;completion&lt;/code&gt;, and &lt;code&gt;state&lt;/code&gt; without requiring ground truth &lt;code&gt;answer&lt;/code&gt; or &lt;code&gt;info&lt;/code&gt; data. Examples include format compliance checking, completion quality assessment, or length-based rewards.&lt;/p&gt; 
&lt;p&gt;For tasks involving LLM judges, you may wish to use &lt;code&gt;vf.JudgeRubric()&lt;/code&gt; for managing requests to auxiliary models.&lt;/p&gt; 
&lt;h3&gt;ToolEnv&lt;/h3&gt; 
&lt;p&gt;For many applications involving tool use, you can use &lt;code&gt;ToolEnv&lt;/code&gt; to leverage models' native tool/function-calling capabilities in an agentic loop. Tools must be stateless and idempotent‚Äîeach call should be fully determined by the provided arguments‚Äîbecause the environment will automatically terminate once the assistant responds without tool calls. Tools can be specified as generic Python functions (with type hints and docstrings), which will then be passed in JSON schema form to each inference request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.ToolEnv(
	dataset= ... # HF Dataset with 'prompt'/'question' and optionally 'answer'/'info' columns
	rubric= ... # Rubric object; vf.ToolRubric() can be optionally used for counting tool invocations in each rollout
	tools=[search_tool, read_article_tool, python_tool], # python functions with type hints + docstrings
	max_turns=10
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In cases where your tools require heavy computational resources, we recommend hosting your tools as standalone servers (e.g. MCP servers) and creating lightweight wrapper functions to pass to &lt;code&gt;ToolEnv&lt;/code&gt;. Parallel tool call support is enabled by default. If you need to inject per-rollout or cross-call state (IDs, credentials, cached resources), promote the environment to &lt;code&gt;StatefulToolEnv&lt;/code&gt; and populate that state through &lt;code&gt;setup_state&lt;/code&gt;/&lt;code&gt;update_tool_args&lt;/code&gt; instead of hiding globals.&lt;/p&gt; 
&lt;h4&gt;StatefulToolEnv&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;StatefulToolEnv&lt;/code&gt; extends &lt;code&gt;ToolEnv&lt;/code&gt; for workflows where tool calls must incorporate dynamic state (for example, sandbox handles or per-user secrets). Implement &lt;code&gt;setup_state&lt;/code&gt; to seed the state dict and override &lt;code&gt;update_tool_args&lt;/code&gt; to merge state into each tool invocation. Any arguments you strip from the OpenAI schema via &lt;code&gt;args_to_skip&lt;/code&gt; should be tracked in &lt;code&gt;skipped_args&lt;/code&gt; so the model never sees sensitive parameters. Avoid storing global state; keep everything in the provided &lt;code&gt;state&lt;/code&gt; dict.&lt;/p&gt; 
&lt;h4&gt;SandboxEnv &amp;amp; PythonEnv&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;SandboxEnv&lt;/code&gt; builds on &lt;code&gt;StatefulToolEnv&lt;/code&gt; to coordinate long-running sandboxes. Queue heavyweight provisioning inside &lt;code&gt;setup_state&lt;/code&gt; (without awaiting) and gate tool execution on readiness inside &lt;code&gt;update_tool_args&lt;/code&gt; or the tools themselves. &lt;code&gt;PythonEnv&lt;/code&gt; is a concrete sandboxed executor that demonstrates the pattern: it spins up a Prime sandbox, injects the sandbox ID into each tool call, and tears down resources when the rollout finishes. Treat both environments as references when building similar stateful tool workflows.&lt;/p&gt; 
&lt;p&gt;For training, or self-hosted endpoints, you'll want to enable auto tool choice in &lt;a href="https://docs.vllm.ai/en/stable/features/tool_calling.html#automatic-function-calling"&gt;vLLM&lt;/a&gt; with the appropriate parser. If your model does not support native tool calling, you may find the &lt;code&gt;XMLParser&lt;/code&gt; abstraction useful for rolling your own tool call parsing on top of &lt;code&gt;MultiTurnEnv&lt;/code&gt;; see &lt;code&gt;environments/xml_tool_env&lt;/code&gt; for an example.&lt;/p&gt; 
&lt;h3&gt;MultiTurnEnv&lt;/h3&gt; 
&lt;p&gt;Both &lt;code&gt;SingleTurnEnv&lt;/code&gt; and &lt;code&gt;ToolEnv&lt;/code&gt; are instances of &lt;code&gt;MultiTurnEnv&lt;/code&gt;, which exposes an interface for writing custom Environment interaction protocols. To implement a custom protocol, define an &lt;code&gt;env_response&lt;/code&gt; method and use &lt;code&gt;@vf.stop&lt;/code&gt; decorators for termination conditions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
from verifiers.types import Messages, State

class YourMultiTurnEnv(vf.MultiTurnEnv):
    def __init__(self,
                 dataset: Dataset,
                 rubric: Rubric,
				 max_turns: int,
                 **kwargs):

    async def env_response(self, messages: Messages, state: State, **kwargs) -&amp;gt; Messages:
        # return new environment message(s); state can be updated in-place
        return [{"role": "user", "content": "feedback"}]

    @vf.stop
    async def task_complete(self, state: State) -&amp;gt; bool:
        # return whether or not a rollout is completed
        return state.get("task_complete", False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your application requires more fine-grained control than is allowed by &lt;code&gt;MultiTurnEnv&lt;/code&gt;, you may want to inherit from the base &lt;code&gt;Environment&lt;/code&gt; functionality directly and override the &lt;code&gt;rollout&lt;/code&gt; method.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ensure your &lt;code&gt;wandb&lt;/code&gt; and &lt;code&gt;huggingface-cli&lt;/code&gt; logins are set up (or set &lt;code&gt;report_to=None&lt;/code&gt; in &lt;code&gt;training_args&lt;/code&gt;). You should also have something set as your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in your environment (can be a dummy key for vLLM).&lt;/li&gt; 
 &lt;li&gt;If using high max concurrency, increase the number of allowed open sockets (e.g. &lt;code&gt;ulimit -n 4096&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;On some setups, inter-GPU communication can &lt;a href="https://github.com/huggingface/trl/issues/2923"&gt;hang&lt;/a&gt; or crash during vLLM weight syncing. This can usually be alleviated by setting (or unsetting) &lt;code&gt;NCCL_P2P_DISABLE=1&lt;/code&gt; in your environment (or potentially &lt;code&gt;NCCL_CUMEM_ENABLE=1&lt;/code&gt;). Try this as your first step if you experience NCCL-related issues.&lt;/li&gt; 
 &lt;li&gt;If problems persist, please open an &lt;a href="https://github.com/PrimeIntellect-ai/verifiers/issues"&gt;issue&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resource Requirements&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;prime-rl&lt;/code&gt; can be run on a single GPU by allocating only a fraction of the available memory to the inference server (see &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl/tree/main/examples/alphabet_sort"&gt;here&lt;/a&gt; for an example configuration), and can also be scaled to hundreds of GPUs for large-scale training. A wide range of competitively-priced cluster configurations are available on &lt;a href="https://app.primeintellect.ai/dashboard/create-cluster?image=ubuntu_22_cuda_12"&gt;Prime Intellect&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Originally created by Will Brown (&lt;a href="https://github.com/willccbb"&gt;@willccbb&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;If you use this code in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{brown_verifiers_2025,
  author       = {William&amp;nbsp;Brown},
  title        = {{Verifiers}: Environments for LLM Reinforcement Learning},
  howpublished = {\url{https://github.com/PrimeIntellect-ai/verifiers}},
  note         = {Commit abcdefg ‚Ä¢ accessed DD‚ÄØMon‚ÄØYYYY},
  year         = {2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>google-research/timesfm</title>
      <link>https://github.com/google-research/timesfm</link>
      <description>&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; 
&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2310.10688"&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, ICML 2024.&lt;/li&gt; 
 &lt;li&gt;All checkpoints: &lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;TimesFM Hugging Face Collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/"&gt;Google Research blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/docs/timesfm-model"&gt;TimesFM in BigQuery&lt;/a&gt;: an official Google product.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This open version is not an officially supported Google product.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Latest Model Version:&lt;/strong&gt; TimesFM 2.5&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Archived Model Versions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1.0 and 2.0: relevant code archived in the sub directory &lt;code&gt;v1&lt;/code&gt;. You can &lt;code&gt;pip install timesfm==1.3.0&lt;/code&gt; to install an older version of this package to load them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Update - Oct. 29, 2025&lt;/h2&gt; 
&lt;p&gt;Added back the covariate support through XReg for TimesFM 2.5.&lt;/p&gt; 
&lt;h2&gt;Update - Sept. 15, 2025&lt;/h2&gt; 
&lt;p&gt;TimesFM 2.5 is out!&lt;/p&gt; 
&lt;p&gt;Comparing to TimesFM 2.0, this new 2.5 model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;uses 200M parameters, down from 500M.&lt;/li&gt; 
 &lt;li&gt;supports up to 16k context length, up from 2048.&lt;/li&gt; 
 &lt;li&gt;supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.&lt;/li&gt; 
 &lt;li&gt;gets rid of the &lt;code&gt;frequency&lt;/code&gt; indicator.&lt;/li&gt; 
 &lt;li&gt;has a couple of new forecasting flags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;add support for an upcoming Flax version of the model (faster inference).&lt;/li&gt; 
 &lt;li&gt;add back covariate support.&lt;/li&gt; 
 &lt;li&gt;populate more docstrings, docs and notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/google-research/timesfm.git
cd timesfm
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create a virtual environment and install dependencies using &lt;code&gt;uv&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;# Create a virtual environment
uv venv

# Activate the environment
source .venv/bin/activate

# Install the package in editable mode with torch
uv pip install -e .[torch]
# Or with flax
uv pip install -e .[flax]
# Or XReg is needed
uv pip install -e .[xreg]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[Optional] Install your preferred &lt;code&gt;torch&lt;/code&gt; / &lt;code&gt;jax&lt;/code&gt; backend based on your OS and accelerators (CPU, GPU, TPU or Apple Silicon).:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org/get-started/locally/"&gt;Install PyTorch&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.jax.dev/en/latest/installation.html#installation"&gt;Install Jax&lt;/a&gt; for Flax.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Code Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
import numpy as np
import timesfm

torch.set_float32_matmul_precision("high")

model = timesfm.TimesFM_2p5_200M_torch.from_pretrained("google/timesfm-2.5-200m-pytorch")

model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>python/cpython</title>
      <link>https://github.com/python/cpython</link>
      <description>&lt;p&gt;The Python programming language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;This is Python version 3.15.0 alpha 3&lt;/h1&gt; 
&lt;p&gt;.. image:: &lt;a href="https://github.com/python/cpython/actions/workflows/build.yml/badge.svg?branch=main&amp;amp;event=push"&gt;https://github.com/python/cpython/actions/workflows/build.yml/badge.svg?branch=main&amp;amp;event=push&lt;/a&gt; :alt: CPython build status on GitHub Actions :target: &lt;a href="https://github.com/python/cpython/actions"&gt;https://github.com/python/cpython/actions&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=main"&gt;https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=main&lt;/a&gt; :alt: CPython build status on Azure DevOps :target: &lt;a href="https://dev.azure.com/python/cpython/_build/latest?definitionId=4&amp;amp;branchName=main"&gt;https://dev.azure.com/python/cpython/_build/latest?definitionId=4&amp;amp;branchName=main&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://img.shields.io/badge/discourse-join_chat-brightgreen.svg"&gt;https://img.shields.io/badge/discourse-join_chat-brightgreen.svg&lt;/a&gt; :alt: Python Discourse chat :target: &lt;a href="https://discuss.python.org/"&gt;https://discuss.python.org/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 2001 Python Software Foundation. All rights reserved.&lt;/p&gt; 
&lt;p&gt;See the end of this file for further copyright and license information.&lt;/p&gt; 
&lt;p&gt;.. contents::&lt;/p&gt; 
&lt;h2&gt;General Information&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Website: &lt;a href="https://www.python.org"&gt;https://www.python.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Source code: &lt;a href="https://github.com/python/cpython"&gt;https://github.com/python/cpython&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Issue tracker: &lt;a href="https://github.com/python/cpython/issues"&gt;https://github.com/python/cpython/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Documentation: &lt;a href="https://docs.python.org"&gt;https://docs.python.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Developer's Guide: &lt;a href="https://devguide.python.org/"&gt;https://devguide.python.org/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing to CPython&lt;/h2&gt; 
&lt;p&gt;For more complete instructions on contributing to CPython development, see the &lt;code&gt;Developer Guide&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;.. _Developer Guide: &lt;a href="https://devguide.python.org/"&gt;https://devguide.python.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Using Python&lt;/h2&gt; 
&lt;p&gt;Installable Python kits, and information about using Python, are available at &lt;code&gt;python.org&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;.. _python.org: &lt;a href="https://www.python.org/"&gt;https://www.python.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Build Instructions&lt;/h2&gt; 
&lt;p&gt;On Unix, Linux, BSD, macOS, and Cygwin::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./configure
make
make test
sudo make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will install Python as &lt;code&gt;python3&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can pass many options to the configure script; run &lt;code&gt;./configure --help&lt;/code&gt; to find out more. On macOS case-insensitive file systems and on Cygwin, the executable is called &lt;code&gt;python.exe&lt;/code&gt;; elsewhere it's just &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Building a complete Python installation requires the use of various additional third-party libraries, depending on your build platform and configure options. Not all standard library modules are buildable or usable on all platforms. Refer to the &lt;code&gt;Install dependencies &amp;lt;https://devguide.python.org/getting-started/setup-building.html#build-dependencies&amp;gt;&lt;/code&gt;_ section of the &lt;code&gt;Developer Guide&lt;/code&gt;_ for current detailed information on dependencies for various Linux distributions and macOS.&lt;/p&gt; 
&lt;p&gt;On macOS, there are additional configure and build options related to macOS framework and universal builds. Refer to &lt;code&gt;Mac/README.rst &amp;lt;https://github.com/python/cpython/blob/main/Mac/README.rst&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;On Windows, see &lt;code&gt;PCbuild/readme.txt &amp;lt;https://github.com/python/cpython/blob/main/PCbuild/readme.txt&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;To build Windows installer, see &lt;code&gt;Tools/msi/README.txt &amp;lt;https://github.com/python/cpython/blob/main/Tools/msi/README.txt&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;If you wish, you can create a subdirectory and invoke configure from there. For example::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mkdir debug
cd debug
../configure --with-pydebug
make
make test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(This will fail if you &lt;em&gt;also&lt;/em&gt; built at the top-level directory. You should do a &lt;code&gt;make clean&lt;/code&gt; at the top-level first.)&lt;/p&gt; 
&lt;p&gt;To get an optimized build of Python, &lt;code&gt;configure --enable-optimizations&lt;/code&gt; before you run &lt;code&gt;make&lt;/code&gt;. This sets the default make targets up to enable Profile Guided Optimization (PGO) and may be used to auto-enable Link Time Optimization (LTO) on some platforms. For more details, see the sections below.&lt;/p&gt; 
&lt;p&gt;Profile Guided Optimization ^^^^^^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;PGO takes advantage of recent versions of the GCC or Clang compilers. If used, either via &lt;code&gt;configure --enable-optimizations&lt;/code&gt; or by manually running &lt;code&gt;make profile-opt&lt;/code&gt; regardless of configure flags, the optimized build process will perform the following steps:&lt;/p&gt; 
&lt;p&gt;The entire Python directory is cleaned of temporary files that may have resulted from a previous compilation.&lt;/p&gt; 
&lt;p&gt;An instrumented version of the interpreter is built, using suitable compiler flags for each flavor. Note that this is just an intermediary step. The binary resulting from this step is not good for real-life workloads as it has profiling instructions embedded inside.&lt;/p&gt; 
&lt;p&gt;After the instrumented interpreter is built, the Makefile will run a training workload. This is necessary in order to profile the interpreter's execution. Note also that any output, both stdout and stderr, that may appear at this step is suppressed.&lt;/p&gt; 
&lt;p&gt;The final step is to build the actual interpreter, using the information collected from the instrumented one. The end result will be a Python binary that is optimized; suitable for distribution or production installation.&lt;/p&gt; 
&lt;p&gt;Link Time Optimization ^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Enabled via configure's &lt;code&gt;--with-lto&lt;/code&gt; flag. LTO takes advantage of the ability of recent compiler toolchains to optimize across the otherwise arbitrary &lt;code&gt;.o&lt;/code&gt; file boundary when building final executables or shared libraries for additional performance gains.&lt;/p&gt; 
&lt;h2&gt;What's New&lt;/h2&gt; 
&lt;p&gt;We have a comprehensive overview of the changes in the &lt;code&gt;What's new in Python 3.15 &amp;lt;https://docs.python.org/3.15/whatsnew/3.15.html&amp;gt;&lt;/code&gt;_ document. For a more detailed change log, read &lt;code&gt;Misc/NEWS &amp;lt;https://github.com/python/cpython/tree/main/Misc/NEWS.d&amp;gt;&lt;/code&gt;&lt;em&gt;, but a full accounting of changes can only be gleaned from the &lt;code&gt;commit history &amp;lt;https://github.com/python/cpython/commits/main&amp;gt;&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to install multiple versions of Python, see the section below entitled "Installing multiple versions".&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;Documentation for Python 3.15 &amp;lt;https://docs.python.org/3.15/&amp;gt;&lt;/code&gt;_ is online, updated daily.&lt;/p&gt; 
&lt;p&gt;It can also be downloaded in many formats for faster access. The documentation is downloadable in HTML, EPUB, and reStructuredText formats; the latter version is primarily for documentation authors, translators, and people with special formatting requirements.&lt;/p&gt; 
&lt;p&gt;For information about building Python's documentation, refer to &lt;code&gt;Doc/README.rst &amp;lt;https://github.com/python/cpython/blob/main/Doc/README.rst&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;p&gt;To test the interpreter, type &lt;code&gt;make test&lt;/code&gt; in the top-level directory. The test set produces some output. You can generally ignore the messages about skipped tests due to optional features which can't be imported. If a message is printed about a failed test or a traceback or core dump is produced, something is wrong.&lt;/p&gt; 
&lt;p&gt;By default, tests are prevented from overusing resources like disk space and memory. To enable these tests, run &lt;code&gt;make buildbottest&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If any tests fail, you can re-run the failing test(s) in verbose mode. For example, if &lt;code&gt;test_os&lt;/code&gt; and &lt;code&gt;test_gdb&lt;/code&gt; failed, you can run::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;make test TESTOPTS="-v test_os test_gdb"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the failure persists and appears to be a problem with Python rather than your environment, you can &lt;code&gt;file a bug report &amp;lt;https://github.com/python/cpython/issues&amp;gt;&lt;/code&gt;_ and include relevant output from that command to show the issue.&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;Running &amp;amp; Writing Tests &amp;lt;https://devguide.python.org/testing/run-write-tests.html&amp;gt;&lt;/code&gt;_ for more on running tests.&lt;/p&gt; 
&lt;h2&gt;Installing multiple versions&lt;/h2&gt; 
&lt;p&gt;On Unix and Mac systems if you intend to install multiple versions of Python using the same installation prefix (&lt;code&gt;--prefix&lt;/code&gt; argument to the configure script) you must take care that your primary python executable is not overwritten by the installation of a different version. All files and directories installed using &lt;code&gt;make altinstall&lt;/code&gt; contain the major and minor version and can thus live side-by-side. &lt;code&gt;make install&lt;/code&gt; also creates &lt;code&gt;${prefix}/bin/python3&lt;/code&gt; which refers to &lt;code&gt;${prefix}/bin/python3.X&lt;/code&gt;. If you intend to install multiple versions using the same prefix you must decide which version (if any) is your "primary" version. Install that version using &lt;code&gt;make install&lt;/code&gt;. Install all other versions using &lt;code&gt;make altinstall&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you want to install Python 2.7, 3.6, and 3.15 with 3.15 being the primary version, you would execute &lt;code&gt;make install&lt;/code&gt; in your 3.15 build directory and &lt;code&gt;make altinstall&lt;/code&gt; in the others.&lt;/p&gt; 
&lt;h2&gt;Release Schedule&lt;/h2&gt; 
&lt;p&gt;See &lt;code&gt;PEP 790 &amp;lt;https://peps.python.org/pep-0790/&amp;gt;&lt;/code&gt;__ for Python 3.15 release details.&lt;/p&gt; 
&lt;h2&gt;Copyright and License Information&lt;/h2&gt; 
&lt;p&gt;Copyright ¬© 2001 Python Software Foundation. All rights reserved.&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 2000 BeOpen.com. All rights reserved.&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 1995-2001 Corporation for National Research Initiatives. All rights reserved.&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 1991-1995 Stichting Mathematisch Centrum. All rights reserved.&lt;/p&gt; 
&lt;p&gt;See the &lt;code&gt;LICENSE &amp;lt;https://github.com/python/cpython/blob/main/LICENSE&amp;gt;&lt;/code&gt;_ for information on the history of this software, terms &amp;amp; conditions for usage, and a DISCLAIMER OF ALL WARRANTIES.&lt;/p&gt; 
&lt;p&gt;This Python distribution contains &lt;em&gt;no&lt;/em&gt; GNU General Public License (GPL) code, so it may be used in proprietary projects. There are interfaces to some GNU code but these are entirely optional.&lt;/p&gt; 
&lt;p&gt;All trademarks referenced herein are property of their respective holders.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenMind/OM1</title>
      <link>https://github.com/OpenMind/OM1</link>
      <description>&lt;p&gt;Modular AI runtime for robots&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c" alt="OM_Banner_X2 (1)" /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2412.18588"&gt;Technical Paper&lt;/a&gt; | &lt;a href="https://docs.openmind.org/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://x.com/openmind_agi"&gt;X&lt;/a&gt; | &lt;a href="https://discord.gg/openmind"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OpenMind's OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots&lt;/strong&gt;, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.&lt;/p&gt; 
&lt;h2&gt;Capabilities of OM1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Modular Architecture&lt;/strong&gt;: Designed with Python for simplicity and seamless integration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Input&lt;/strong&gt;: Easily handles new data and sensors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware Support via Plugins&lt;/strong&gt;: Supports new hardware through plugins for API endpoints and specific robot hardware connections to &lt;code&gt;ROS2&lt;/code&gt;, &lt;code&gt;Zenoh&lt;/code&gt;, and &lt;code&gt;CycloneDDS&lt;/code&gt;. (We recommend &lt;code&gt;Zenoh&lt;/code&gt; for all new development).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web-Based Debugging Display&lt;/strong&gt;: Monitor the system in action with WebSim (available at &lt;a href="http://localhost:8000/"&gt;http://localhost:8000/&lt;/a&gt;) for easy visual debugging.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-configured Endpoints&lt;/strong&gt;: Supports Text-to-Speech, multiple LLMs from OpenAI, xAI, DeepSeek, Anthropic, Meta, Gemini, NearAI and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture Overview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/dd91457d-010f-43d8-960e-d1165834aa58" alt="Artboard 1@4x 1 (1)" /&gt;&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To get started with OM1, let's run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to the LLM, which returns &lt;code&gt;movement&lt;/code&gt;, &lt;code&gt;speech&lt;/code&gt; and &lt;code&gt;face&lt;/code&gt; action commands. These commands are displayed on WebSim along with basic timing and other debugging information.&lt;/p&gt; 
&lt;h3&gt;Package Management and VENV&lt;/h3&gt; 
&lt;p&gt;You will need the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt; package manager&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Clone the Repo&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/OpenMind/OM1.git
cd OM1
git submodule update --init
uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Dependencies&lt;/h3&gt; 
&lt;p&gt;For MacOS&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install portaudio ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Linux&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Obtain an OpenMind API Key&lt;/h3&gt; 
&lt;p&gt;Obtain your API Key at &lt;a href="https://portal.openmind.org/"&gt;OpenMind Portal&lt;/a&gt;. Copy it to &lt;code&gt;config/spot.json5&lt;/code&gt;, replacing the &lt;code&gt;openmind_free&lt;/code&gt; placeholder. Or, &lt;code&gt;cp env.example .env&lt;/code&gt; and add your key to the &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Launching OM1&lt;/h3&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run src/run.py spot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see &lt;a href="https://docs.openmind.org/developing/1_get-started"&gt;getting started&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: This is just an example agent configuration. If you want to interact with the agent and see how it works, make sure ASR and TTS are configured in spot.json5.&lt;/p&gt; 
&lt;h2&gt;What's Next?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try out some &lt;a href="https://docs.openmind.org/examples"&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add new &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;actions&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Design custom agents and robots by creating your own &lt;code&gt;json5&lt;/code&gt; config files with custom combinations of inputs and actions.&lt;/li&gt; 
 &lt;li&gt;Change the system prompts in the configuration files (located in &lt;code&gt;/config/&lt;/code&gt;) to create new behaviors.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Interfacing with New Robot Hardware&lt;/h2&gt; 
&lt;p&gt;OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as &lt;code&gt;backflip&lt;/code&gt;, &lt;code&gt;run&lt;/code&gt;, &lt;code&gt;gently pick up the red apple&lt;/code&gt;, &lt;code&gt;move(0.37, 0, 0)&lt;/code&gt;, and &lt;code&gt;smile&lt;/code&gt;. An example is provided in &lt;code&gt;src/actions/move/connector/ros2.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
elif output_interface.action == "shake paw":
    if self.sport_client:
        self.sport_client.Hello()
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers.&lt;/p&gt; 
&lt;p&gt;OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see &lt;a href="https://github.com/unitreerobotics/unitree_sdk2/raw/adee312b081c656ecd0bb4e936eed96325546296/example/g1/high_level/g1_loco_client_example.cpp#L159"&gt;Unitree's C++ SDK&lt;/a&gt;. Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.&lt;/p&gt; 
&lt;h2&gt;Recommended Development Platforms&lt;/h2&gt; 
&lt;p&gt;OM1 is developed on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nvidia Thor (running JetPak 7.0) - full support&lt;/li&gt; 
 &lt;li&gt;Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1) - limited support&lt;/li&gt; 
 &lt;li&gt;Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Generic Linux machines (running Ubuntu 22.04)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;OM1 &lt;em&gt;should&lt;/em&gt; run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.&lt;/p&gt; 
&lt;h2&gt;Full Autonomy Guidance&lt;/h2&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;full autonomy&lt;/strong&gt; for Unitree Go2 and G1. Full autonomy has four services that work together in a loop without manual intervention:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;om1&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;unitree_sdk&lt;/strong&gt; ‚Äì A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;om1-avatar&lt;/strong&gt; ‚Äì A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;om1-video-processor&lt;/strong&gt; - The OM1 Video Processor is a Docker-based solution that enables real-time video streaming, face recognition, and audio capture for OM1 robots.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to BrainPack?&lt;/h2&gt; 
&lt;p&gt;From research to real-world autonomy, a platform that learns, moves, and builds with you. We'll shortly be releasing the &lt;strong&gt;BOM&lt;/strong&gt; and details on &lt;strong&gt;DIY&lt;/strong&gt; for it. Stay tuned!&lt;/p&gt; 
&lt;p&gt;Clone the following repos -&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1.git"&gt;https://github.com/OpenMind/OM1.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/unitree-sdk.git"&gt;https://github.com/OpenMind/unitree-sdk.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1-avatar.git"&gt;https://github.com/OpenMind/OM1-avatar.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1-video-processor.git"&gt;https://github.com/OpenMind/OM1-video-processor.git&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Starting the system&lt;/h2&gt; 
&lt;p&gt;To start all services, run the following commands:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Setup the API key&lt;/p&gt; 
&lt;p&gt;For Bash: vim ~/.bashrc or ~/.bash_profile.&lt;/p&gt; 
&lt;p&gt;For Zsh: vim ~/.zshrc.&lt;/p&gt; 
&lt;p&gt;Add&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OM_API_KEY="your_api_key"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update the docker-compose file. Replace "unitree_go2_autonomy_advance" with the agent you want to run.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;command: ["unitree_go2_autonomy_advance"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1
docker compose up om1 -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For unitree_sdk&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd unitree_sdk
docker compose up orchestrator -d --no-build
docker compose up om1_sensor -d --no-build
docker compose up watchdog -d --no-build
docker compose up zenoh_bridge -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1-avatar&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1-avatar
docker compose up om1_avatar -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1-video-processor&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1-video-processor
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Detailed Documentation&lt;/h2&gt; 
&lt;p&gt;More detailed documentation can be accessed at &lt;a href="https://docs.openmind.org/"&gt;docs.openmind.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please make sure to read the &lt;a href="https://raw.githubusercontent.com/OpenMind/OM1/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; before making a pull request.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>chidiwilliams/buzz</title>
      <link>https://github.com/chidiwilliams/buzz</link>
      <description>&lt;p&gt;Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;[&lt;a href="https://raw.githubusercontent.com/chidiwilliams/buzz/main/readme/README.zh_CN.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;] &amp;lt;- ÁÇπÂáªÊü•Áúã‰∏≠ÊñáÈ°µÈù¢„ÄÇ&lt;/p&gt; 
&lt;h1&gt;Buzz&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://chidiwilliams.github.io/buzz/"&gt;Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Transcribe and translate audio offline on your personal computer. Powered by OpenAI's &lt;a href="https://github.com/openai/whisper"&gt;Whisper&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/license-MIT-green" alt="MIT License" /&gt; &lt;a href="https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/chidiwilliams/buzz"&gt;&lt;img src="https://codecov.io/github/chidiwilliams/buzz/branch/main/graph/badge.svg?token=YJSB8S2VEP" alt="codecov" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/v/release/chidiwilliams/buzz" alt="GitHub release (latest by date)" /&gt; &lt;a href="https://GitHub.com/chidiwilliams/buzz/releases/"&gt;&lt;img src="https://img.shields.io/github/downloads/chidiwilliams/buzz/total.svg?sanitize=true" alt="Github all releases" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;p&gt;Download the &lt;code&gt;.dmg&lt;/code&gt; from the &lt;a href="https://sourceforge.net/projects/buzz-captions/files/"&gt;SourceForge&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Get the installation files from the &lt;a href="https://sourceforge.net/projects/buzz-captions/files/"&gt;SourceForge&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;App is not signed, you will get a warning when you install it. Select &lt;code&gt;More info&lt;/code&gt; -&amp;gt; &lt;code&gt;Run anyway&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Alternatively, install with &lt;a href="https://learn.microsoft.com/en-us/windows/package-manager/winget/"&gt;winget&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;winget install ChidiWilliams.Buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;p&gt;Buzz is available as a &lt;a href="https://flathub.org/apps/io.github.chidiwilliams.Buzz"&gt;Flatpak&lt;/a&gt; or a &lt;a href="https://snapcraft.io/buzz"&gt;Snap&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To install flatpak, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;flatpak install flathub io.github.chidiwilliams.Buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install snap, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo apt-get install libportaudio2 libcanberra-gtk-module libcanberra-gtk3-module
sudo snap install buzz
sudo snap connect buzz:password-manager-service
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;PyPI&lt;/h3&gt; 
&lt;p&gt;Install &lt;a href="https://www.ffmpeg.org/download.html"&gt;ffmpeg&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Ensure you use Python 3.12 environment.&lt;/p&gt; 
&lt;p&gt;Install Buzz&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install buzz-captions
python -m buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;GPU support for PyPI&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To have GPU support for Nvidia GPUS on Windows, for PyPI installed version ensure, CUDA support for &lt;a href="https://pytorch.org/get-started/locally/"&gt;torch&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip3 install -U torch==2.8.0+cu129 torchaudio==2.8.0+cu129 --index-url https://download.pytorch.org/whl/cu129
pip3 install nvidia-cublas-cu12==12.9.1.4 nvidia-cuda-cupti-cu12==12.9.79 nvidia-cuda-runtime-cu12==12.9.79 --extra-index-url https://pypi.ngc.nvidia.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Latest development version&lt;/h3&gt; 
&lt;p&gt;For info on how to get latest development version with latest features and bug fixes see &lt;a href="https://chidiwilliams.github.io/buzz/docs/faq#9-where-can-i-get-latest-development-version"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Screenshots&lt;/h3&gt; 
&lt;div style="display: flex; flex-wrap: wrap;"&gt; 
 &lt;img alt="File import" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-1-import.png" style="max-width: 18%; margin-right: 1%;" /&gt; 
 &lt;img alt="Main screen" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-2-main_screen.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Preferences" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-3-preferences.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Model preferences" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-3.2-model-preferences.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Transcript" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-4-transcript.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Live recording" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-5-live_recording.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Resize" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-6-resize.png" style="max-width: 18%;" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ladaapp/lada</title>
      <link>https://github.com/ladaapp/lada</link>
      <description>&lt;p&gt;Restore videos with pixelated/mosaic regions&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/ladaapp/lada/main/packaging/flatpak/share/io.github.ladaapp.lada.png" alt="Lada Icon" style="display: block; width: 64px; height: 64px;" /&gt; &lt;br /&gt; Lada &lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Lada&lt;/em&gt; is a tool designed to recover pixelated adult videos (JAV). It helps restore the visual quality of such content, making it more enjoyable to watch.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Recover Pixelated Videos&lt;/strong&gt;: Restore pixelated or mosaic scenes in adult videos.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Watch/Export Videos&lt;/strong&gt;: Use either the CLI or GUI to watch or export your restored videos.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;GUI&lt;/h3&gt; 
&lt;p&gt;After opening a file, you can either watch the restored via in realtime or export it to a new file to watch it later:&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="assets/screenshot_gui_1_dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="assets/screenshot_gui_1_light.png" /&gt; 
 &lt;img alt="Screenshot showing video preview" src="https://raw.githubusercontent.com/ladaapp/lada/main/assets/screenshot_gui_1_dark.png" width="36%" /&gt; 
&lt;/picture&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="assets/screenshot_gui_2_dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="assets/screenshot_gui_2_light.png" /&gt; 
 &lt;img alt="Screenshot showing video export" src="https://raw.githubusercontent.com/ladaapp/lada/main/assets/screenshot_gui_2_dark.png" width="45%" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;Additional settings can be found in the left sidebar.&lt;/p&gt; 
&lt;h3&gt;CLI&lt;/h3&gt; 
&lt;p&gt;You can also use the command-line interface (CLI) to restore video(s):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;lada-cli --input &amp;lt;input video path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/ladaapp/lada/main/assets/screenshot_cli_1.png" alt="screenshot showing video export" width="60%" /&gt; 
&lt;p&gt;For more information about additional options, use the &lt;code&gt;--help&lt;/code&gt; argument.&lt;/p&gt; 
&lt;h2&gt;Performance and hardware requirements&lt;/h2&gt; 
&lt;p&gt;Don't expect this to work perfectly, some scenes can be pretty good and close to the real thing. Other scenes can be rather meh and show worse artifacts than the original mosaics.&lt;/p&gt; 
&lt;p&gt;You'll need a GPU and some patience to run the app. If your card has at least 4-6GB of VRAM then it should work out of the box.&lt;/p&gt; 
&lt;p&gt;The CPU is used for encoding the restored video so shouldn't be too slow either. But you can also use GPU encoding and run both the restoration and encoding tasks on the GPU.&lt;/p&gt; 
&lt;p&gt;The app also needs quite a bit of RAM for buffering to increase throughput. For 1080p content you should be fine with 6-8GB RAM, 4K will need a lot more.&lt;/p&gt; 
&lt;p&gt;To watch the restored video in realtime you'll need a pretty beefy machine or you'll see the player pausing and buffering until next restored frames are computed. When viewing the video no encoding is done but it will use more additional RAM for buffering.&lt;/p&gt; 
&lt;p&gt;If your GPU is not fast enough to watch the video in real-time you'll have to export it first and watch it later with your favorite media player (available in GUI and CLI).&lt;/p&gt; 
&lt;p&gt;Technically running the app on your CPU is also supported but it will be so slow that it's not really practical.&lt;/p&gt; 
&lt;p&gt;Here are some speed performance numbers using Lada v0.7.0 on my available hardware to give you an idea what to expect (used libx264/CPU codec with default settings; RTX 3090 results are limited by CPU encoding and could be a lot faster by switching to NVENC/GPU encoder):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Video name&lt;/th&gt; 
   &lt;th&gt;Video description&lt;/th&gt; 
   &lt;th&gt;Video&lt;br /&gt;duration / resolution / FPS&lt;/th&gt; 
   &lt;th&gt;Lada&lt;br /&gt;runtime / FPS&lt;br /&gt;Nvidia RTX 3050&lt;br /&gt;(&lt;em&gt;Laptop GPU&lt;/em&gt;)&lt;/th&gt; 
   &lt;th&gt;Lada&lt;br /&gt;runtime / FPS&lt;br /&gt;Nvidia RTX 3090&lt;br /&gt;(Desktop GPU)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vid1&lt;/td&gt; 
   &lt;td&gt;multiple mosaic regions present on all frames&lt;/td&gt; 
   &lt;td&gt;1m30s / 10920x1080 / 30 FPS&lt;/td&gt; 
   &lt;td&gt;3m36s / 12 FPS&lt;/td&gt; 
   &lt;td&gt;1m33s / 30 FPS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vid2&lt;/td&gt; 
   &lt;td&gt;single mosaic region present on all frames&lt;/td&gt; 
   &lt;td&gt;3m0s / 1920x1080 / 30 FPS&lt;/td&gt; 
   &lt;td&gt;4m11s / 21 FPS&lt;/td&gt; 
   &lt;td&gt;2m16s / 39 FPS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vid3&lt;/td&gt; 
   &lt;td&gt;half of the video doesn't have any mosaics present,&lt;br /&gt;the other half mostly single mosaic per frame&lt;/td&gt; 
   &lt;td&gt;41m16s / 852x480 / 30 FPS&lt;/td&gt; 
   &lt;td&gt;26m30s / 46 FPS&lt;/td&gt; 
   &lt;td&gt;10m20s / 119 FPS&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Using Flatpak&lt;/h3&gt; 
&lt;p&gt;The easiest way to install the app (CLI and GUI) on Linux is via Flathub:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://flathub.org/apps/details/io.github.ladaapp.lada"&gt;&lt;img width="200" alt="Download from Flathub" src="https://flathub.org/api/badge?svg&amp;amp;locale=en" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The Flatpak only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date. It can also be used without a GPU but it will be very slow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] After installation you should find Lada in your application launcher to start the GUI. You can also run it via &lt;code&gt;flatpak run io.github.ladaapp.lada&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] When using the CLI via Flatpak we need to make the file/directory available by giving it permission to the file system so it can access the input/output files&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;flatpak run --filesystem=host --command=lada-cli io.github.ladaapp.lada --input &amp;lt;input video path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You may want to set an alias to make it easier to use&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;alias lada-cli="flatpak run --filesystem=host --command=lada-cli io.github.ladaapp.lada"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You could also give the filesystem permission permanently via &lt;a href="https://flathub.org/apps/com.github.tchx84.Flatseal"&gt;Flatseal&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you want to use the Post-export action feature to run a command/script after export has finished you'll need to give the Flatpak additional permissions. Add the &lt;code&gt;--talk-name=org.freedesktop.Flatpak&lt;/code&gt; permission and then run your command via &lt;code&gt;flatpak-spawn&lt;/code&gt;. For example: If the script you want to run is /home/user/myscript.sh then set custom command as &lt;code&gt;flatpak-spawn --host /home/user/myscript.sh&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you installed Lada from Flathub and drag-and-drop doesn't work, your file browser might not support &lt;a href="https://flatpak.github.io/xdg-desktop-portal/docs/doc-org.freedesktop.portal.FileTransfer.html"&gt;File Transfer Portal&lt;/a&gt;. You can fix this by:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Switching or updating your file browser to one that supports it.&lt;/li&gt; 
  &lt;li&gt;Granting the app filesystem permissions (e.g., via &lt;a href="https://flathub.org/apps/com.github.tchx84.Flatseal"&gt;Flatseal&lt;/a&gt; so it can read files directly).&lt;/li&gt; 
  &lt;li&gt;Using the 'Open' button to select the file instead of drag-and-drop.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Using Docker&lt;/h3&gt; 
&lt;p&gt;The app is also available via Docker (CLI only). You can get the image &lt;code&gt;ladaapp/lada&lt;/code&gt; from &lt;a href="https://hub.docker.com/r/ladaapp/lada"&gt;Docker Hub&lt;/a&gt; with this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker pull ladaapp/lada:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The Docker image only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date. It can also be used without a GPU but it will be very slow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Make sure that you have installed the &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"&gt;NVIDIA Container Toolkit&lt;/a&gt; on your system so Docker can pass through the GPU&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] When using Docker you'll need to make the file/directory available to the container as well as the GPU:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;docker run --rm --gpus all --mount type=bind,src=&amp;lt;input video path&amp;gt;,dst=/mnt ladaapp/lada:latest --input "/mnt/&amp;lt;input video file&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you want to use hardware encoders like &lt;code&gt;hevc_nvenc&lt;/code&gt; you have to provide the container with &lt;code&gt;video&lt;/code&gt; capability.&lt;/p&gt; 
 &lt;p&gt;With docker run you can use &lt;code&gt;--gpus 'all,"capabilities=compute,video"'&lt;/code&gt;. Learn more &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/docker-specialized.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Using Windows&lt;/h3&gt; 
&lt;p&gt;For Windows users, the app (CLI and GUI) is packaged as a standalone .7z archive file. You'll need &lt;a href="https://7-zip.org/"&gt;7-zip&lt;/a&gt; to unpack the files. It is recommended to validate the file after downloading. See the Tip below.&lt;/p&gt; 
&lt;p&gt;Get the latest release from the &lt;a href="https://codeberg.org/ladaapp/lada/releases"&gt;Releases Page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You'll find &lt;code&gt;lada.exe&lt;/code&gt; and &lt;code&gt;lada-cli.exe&lt;/code&gt; after extracting the archive.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The Windows release only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date. It can also be used without a GPU but it will be very slow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Be aware that the first start of lada.exe or lada-cli.exe could take a while before Windows Defender or your AV has scanned it. The next time you open the program it should start fast.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] It is recommended to compare the checksum of the downloaded file against the value you'll find in the release announcement. This makes sure that you got the correct and unaltered file, especially important if you got the file from an unofficial source.&lt;/p&gt; 
 &lt;p&gt;Calculate the checksum of the downloaded file on your computer and compare it against the &lt;code&gt;SHA256&lt;/code&gt; value you'll find in the release announcement. They must be the same!&lt;/p&gt; 
 &lt;p&gt;You can do this with Powershell &lt;code&gt;Get-FileHash /path/to/file.7z&lt;/code&gt; or &lt;a href="https://www.quickhash-gui.org/"&gt;QuickHash-GUI&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Alternative Installation Methods&lt;/h3&gt; 
&lt;p&gt;If the packages above don't work for you then you'll have to follow the &lt;a href="https://raw.githubusercontent.com/ladaapp/lada/main/#build"&gt;Build&lt;/a&gt; steps to set up the project.&lt;/p&gt; 
&lt;p&gt;Note that these instructions are mostly intended for developers to set up their environment to start working on the source code. But you should hopefully be able to follow the instructions even if you aren't a developer.&lt;/p&gt; 
&lt;p&gt;All packages currently only work with Nvidia cards (or CPU) but there have been reports that, following the Build instructions, newer Intel Xe GPUs and AMD ROCm-compatible cards work as well.&lt;/p&gt; 
&lt;p&gt;Reach out if you can support packaging the app for other operating systems or hardware.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;You can find the Lada project &lt;a href="https://github.com/ladaapp/lada"&gt;on GitHub&lt;/a&gt; and &lt;a href="https://codeberg.org/ladaapp/lada"&gt;on Codeberg&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The home of the project is on Codeberg. GitHub is set up only as a mirror so it's code will stay in sync with the main branch on Codeberg.&lt;/p&gt; 
&lt;p&gt;For contributing code, ideas or bug reports use &lt;a href="https://codeberg.org/ladaapp/lada/pulls"&gt;Pull requests&lt;/a&gt; and the &lt;a href="https://codeberg.org/ladaapp/lada/issues"&gt;Issue tracker&lt;/a&gt; on Codeberg.&lt;/p&gt; 
&lt;p&gt;If you want to help translating the app you can contribute to existing translations or set up a new language over at &lt;a href="https://translate.codeberg.org/projects/lada/lada/"&gt;Codeberg Translate&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://translate.codeberg.org/engage/lada/"&gt;&lt;img src="https://translate.codeberg.org/widget/lada/lada/multi-auto.svg?sanitize=true" alt="Translation status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;p&gt;New releases will be published on both &lt;a href="https://github.com/ladaapp/lada/releases"&gt;GitHub Releases&lt;/a&gt; and &lt;a href="https://codeberg.org/ladaapp/lada/releases"&gt;Codeberg Releases&lt;/a&gt;. You should get a notification about new releases if you star the project on either platform.&lt;/p&gt; 
&lt;h2&gt;Build&lt;/h2&gt; 
&lt;p&gt;If you want to start hacking on this project you'll need to install the app from source. Check out the detailed installation guides for &lt;a href="https://raw.githubusercontent.com/ladaapp/lada/main/docs/linux_install.md"&gt;Linux&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/ladaapp/lada/main/docs/windows_install.md"&gt;Windows&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Training and dataset creation&lt;/h2&gt; 
&lt;p&gt;For instructions on training your own models and datasets, refer to &lt;a href="https://raw.githubusercontent.com/ladaapp/lada/main/docs/training_and_dataset_creation.md"&gt;Training and dataset creation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Source code and models are licensed under AGPL-3.0. See the &lt;a href="https://raw.githubusercontent.com/ladaapp/lada/main/LICENSE.md"&gt;LICENSE.md&lt;/a&gt; file for full details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;This project builds upon work done by these fantastic individuals and projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HypoX64/DeepMosaics"&gt;DeepMosaics&lt;/a&gt;: Provided code for mosaic dataset creation. Also inspired me to start this project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ckkelvinchan.github.io/projects/BasicVSR++"&gt;BasicVSR++&lt;/a&gt; / &lt;a href="https://github.com/open-mmlab/mmagic"&gt;MMagic&lt;/a&gt;: Used as the base model for mosaic removal.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ultralytics/ultralytics"&gt;YOLO/Ultralytics&lt;/a&gt;: Used for training mosaic and NSFW detection models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/VQAssessment/DOVER"&gt;DOVER&lt;/a&gt;: Used to assess video quality of created clips during the dataset creation process to filter out low-quality clips.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tgenlis83/dnn-watermark"&gt;DNN Watermark / PITA Dataset&lt;/a&gt;: Used most of its code for creating a watermark detection dataset used to filter out scenes obstructed by text/watermarks/logos.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/notAI-tech/NudeNet/"&gt;NudeNet&lt;/a&gt;: Used as an additional NSFW classifier to filter out false positives by our own NSFW segmentation model&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/twitter/twemoji"&gt;Twitter Emoji&lt;/a&gt;: Provided eggplant emoji as base for the app icon.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xinntao/Real-ESRGAN"&gt;Real-ESRGAN&lt;/a&gt;: Used their image degradation model design for our mosaic detection model degradation pipeline.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hnuzhy/BPJDet"&gt;BPJDet&lt;/a&gt;: Model for detecting human body and head. Used for creating SFW mosaics so that mosaic detection model can be trained so skip such material.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Star-Clouds/CenterFace"&gt;CenterFace&lt;/a&gt;: Model for detecting human faces. Used for creating SFW mosaics so that mosaic detection model can be trained so skip such material.&lt;/li&gt; 
 &lt;li&gt;PyTorch, FFmpeg, GStreamer, GTK and &lt;a href="https://xkcd.com/2347/"&gt;all other folks building our ecosystem&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>GodsScion/Auto_job_applier_linkedIn</title>
      <link>https://github.com/GodsScion/Auto_job_applier_linkedIn</link>
      <description>&lt;p&gt;Make your job hunt easy by automating your application process with this Auto Applier&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LinkedIn AI Auto Job Applier ü§ñ&lt;/h1&gt; 
&lt;p&gt;This is an web scraping bot that automates the process of job applications on LinkedIn. It searches for jobs relevant to you, answers all questions in application form, customizes your resume based on the collected job information, such as skills required, description, about company, etc. and applies to the job. Can apply 100+ jobs in less than 1 hour.&lt;/p&gt; 
&lt;h2&gt;üìΩÔ∏è See it in Action&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/gMbB1fWZDHw"&gt;&lt;img src="https://github.com/GodsScion/Auto_job_applier_linkedIn/assets/100998531/429f7753-ebb0-499b-bc5e-5b4ee28c4f69" alt="Auto Job Applier demo video" /&gt;&lt;/a&gt; Click on above image to watch the demo or use this link &lt;a href="https://youtu.be/gMbB1fWZDHw"&gt;https://youtu.be/gMbB1fWZDHw&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® Content&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#linkedin-ai-auto-job-applier-"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#%EF%B8%8F-see-it-in-action"&gt;Demo Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-content"&gt;Index&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#%EF%B8%8F-how-to-install"&gt;Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-how-to-configure"&gt;Configure&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#%E2%80%8D-contributor-guidelines"&gt;Contributor Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/%EF%B8%8F-major-updates-history"&gt;Updates&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#%EF%B8%8F-terms-and-conditions"&gt;Terms and Conditions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#%EF%B8%8F-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-socials"&gt;Socials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-community-support-and-discussions"&gt;Support and Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;‚öôÔ∏è How to install&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/f9rdz74e1lM?si=4fRBcte0nuvr6tEH"&gt;&lt;img src="https://github.com/user-attachments/assets/9e876187-ed3e-4fbf-bd87-4acc145880a2" alt="Auto Job Applier setup tutorial video" /&gt;&lt;/a&gt; Click on above image to watch the tutorial for installation and configuration or use this link &lt;a href="https://youtu.be/f9rdz74e1lM"&gt;https://youtu.be/f9rdz74e1lM&lt;/a&gt; (Recommended to watch it in 2x speed)&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/"&gt;Python 3.10&lt;/a&gt; or above. Visit &lt;a href="https://www.python.org/downloads/"&gt;https://www.python.org/downloads/&lt;/a&gt; to download and install Python, or for windows you could visit Microsoft Store and search for "Python". &lt;strong&gt;Please make sure Python is added to Path in System Environment Variables&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Install necessary &lt;a href="https://pypi.org/project/undetected-chromedriver/"&gt;Undetected Chromedriver&lt;/a&gt;, &lt;a href="https://pypi.org/project/PyAutoGUI/"&gt;PyAutoGUI&lt;/a&gt; and &lt;a href="https://pypi.org/project/setuptools/"&gt;Setuptools&lt;/a&gt; packages. After Python is installed, OPEN a console/terminal or shell, Use below command that uses the &lt;a href="https://pip.pypa.io/en/stable"&gt;pip&lt;/a&gt; command-line tool to install these 3 package.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;pip install undetected-chromedriver pyautogui setuptools openai flask-cors flask
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Download and install latest version of &lt;a href="https://www.google.com/chrome"&gt;Google Chrome&lt;/a&gt; in it's default location, visit &lt;a href="https://www.google.com/chrome"&gt;https://www.google.com/chrome&lt;/a&gt; to download it's installer.&lt;/li&gt; 
 &lt;li&gt;Clone the current git repo or download it as a zip file, url to the latest update &lt;a href="https://github.com/GodsScion/Auto_job_applier_linkedIn"&gt;https://github.com/GodsScion/Auto_job_applier_linkedIn&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;(Not needed if you set &lt;code&gt;stealth_mode = True&lt;/code&gt; in &lt;code&gt;config/settings.py&lt;/code&gt; ) Download and install the appropriate &lt;a href="https://googlechromelabs.github.io/chrome-for-testing/"&gt;Chrome Driver&lt;/a&gt; for Google Chrome and paste it in the location Chrome was installed, visit &lt;a href="https://googlechromelabs.github.io/chrome-for-testing/"&gt;https://googlechromelabs.github.io/chrome-for-testing/&lt;/a&gt; to download. &lt;br /&gt; &lt;br /&gt; &lt;em&gt;&lt;strong&gt;OR&lt;/strong&gt;&lt;/em&gt; &lt;br /&gt; &lt;br /&gt; If you are using Windows, click on &lt;code&gt;windows-setup.bat&lt;/code&gt; available in the &lt;code&gt;/setup&lt;/code&gt; folder, this will install the latest chromedriver automatically.&lt;/li&gt; 
 &lt;li&gt;If you have questions or need help setting it up or to talk in general, join the github server: &lt;a href="https://discord.gg/fFp7uUzWCY"&gt;https://discord.gg/fFp7uUzWCY&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-content"&gt;back to index&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üîß How to configure&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open &lt;code&gt;personals.py&lt;/code&gt; file in &lt;code&gt;/config&lt;/code&gt; folder and enter your details like name, phone number, address, etc. Whatever you want to fill in your applications.&lt;/li&gt; 
 &lt;li&gt;Open &lt;code&gt;questions.py&lt;/code&gt; file in &lt;code&gt;/config&lt;/code&gt; folder and enter your answers for application questions, configure wether you want the bot to pause before submission or pause if it can't answer unknown questions.&lt;/li&gt; 
 &lt;li&gt;Open &lt;code&gt;search.py&lt;/code&gt; file in &lt;code&gt;/config&lt;/code&gt; folder and enter your search preferences, job filters, configure the bot as per your needs (these settings decide which jobs to apply for or skip).&lt;/li&gt; 
 &lt;li&gt;Open &lt;code&gt;secrets.py&lt;/code&gt; file in &lt;code&gt;/config&lt;/code&gt; folder and enter your LinkedIn username, password to login and OpenAI API Key for generation of job tailored resumes and cover letters (This entire step is optional). If you do not provide username or password or leave them as default, it will login with saved profile in browser, if failed will ask you to login manually.&lt;/li&gt; 
 &lt;li&gt;Open &lt;code&gt;settings.py&lt;/code&gt; file in &lt;code&gt;/config&lt;/code&gt; folder to configure the bot settings like, keep screen awake, click intervals (click intervals are randomized to seem like human behavior), run in background, stealth mode (to avoid bot detection), etc. as per your needs.&lt;/li&gt; 
 &lt;li&gt;(Optional) Don't forget to add you default resume in the location you mentioned in &lt;code&gt;default_resume_path = "all resumes/default/resume.pdf"&lt;/code&gt; given in &lt;code&gt;/config/questions.py&lt;/code&gt;. If one is not provided, it will use your previous resume submitted in LinkedIn or (In Development) generate custom resume if OpenAI APT key is provided!&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;runAiBot.py&lt;/code&gt; and see the magic happen.&lt;/li&gt; 
 &lt;li&gt;To run the Applied Jobs history UI, run &lt;code&gt;app.py&lt;/code&gt; and open web browser on &lt;code&gt;http://localhost:5000&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;If you have questions or need help setting it up or to talk in general, join the github server: &lt;a href="https://discord.gg/fFp7uUzWCY"&gt;https://discord.gg/fFp7uUzWCY&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-content"&gt;back to index&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üßë‚Äçüíª Contributor Guidelines&lt;/h2&gt; 
&lt;p&gt;Thank you for your efforts and being a part of the community. All contributions are appreciated no matter how small or big. Once you contribute to the code base, your work will be remembered forever.&lt;/p&gt; 
&lt;p&gt;NOTE: Only Pull request to &lt;code&gt;community-version&lt;/code&gt; branch will be accepted. Any other requests will be declined by default, especially to main branch. Once your code is tested, your changes will be merged to the &lt;code&gt;main&lt;/code&gt; branch in next cycle.&lt;/p&gt; 
&lt;h3&gt;Code Guidelines&lt;/h3&gt; 
&lt;h4&gt;Functions:&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;All functions or methods are named lower case and snake case&lt;/li&gt; 
 &lt;li&gt;Must have explanation of their purpose. Write explanation surrounded in &lt;code&gt;''' Explanation '''&lt;/code&gt; under the definition &lt;code&gt;def function() -&amp;gt; None:&lt;/code&gt;. Example: &lt;pre&gt;&lt;code class="language-python"&gt;def function() -&amp;gt; None:
  '''
  This function does nothing, it's just an example for explanation placement!
  '''
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;The Types &lt;code&gt;(str, list, int, list[str], int | float)&lt;/code&gt; for the parameters and returns must be given. Example: &lt;pre&gt;&lt;code class="language-python"&gt;def function(param1: str, param2: list[str], param3: int) -&amp;gt; str:
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Putting all that together some valid examples for function or method declarations would be as follows. &lt;pre&gt;&lt;code class="language-python"&gt;def function_name_in_camel_case(parameter1: driver, parameter2: str) -&amp;gt; list[str] | ValueError:
  '''
  This function is an example for code guidelines
  '''
  return [parameter2, parameter2.lower()]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;The hashtag comments on top of functions are optional, which are intended for developers &lt;code&gt;# Comments for developers&lt;/code&gt;. &lt;pre&gt;&lt;code class="language-python"&gt;# Enter input text function
def text_input_by_ID(driver: WebDriver, id: str, value: str, time: float=5.0) -&amp;gt; None | Exception:
    '''
    Enters `value` into the input field with the given `id` if found, else throws NotFoundException.
    - `time` is the max time to wait for the element to be found.
    '''
    username_field = WebDriverWait(driver, time).until(EC.presence_of_element_located((By.ID, id)))
    username_field.send_keys(Keys.CONTROL + "a")
    username_field.send_keys(value)

&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Variables&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;All variables must start with lower case, must be in explainable full words. If someone reads the variable name, it should be easy to understand what the variable stores.&lt;/li&gt; 
 &lt;li&gt;All local variables are camel case. Examples: &lt;pre&gt;&lt;code class="language-python"&gt;jobListingsElement = None
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-python"&gt;localBufferTime = 5.5
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;All global variables are snake case. Example: &lt;pre&gt;&lt;code&gt;total_runs = 1
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Mentioning types are optional. &lt;pre&gt;&lt;code class="language-python"&gt;localBufferTime: float | int = 5.5
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Configuration variables&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;All config variables are treated as global variables. They have some extra guidelines.&lt;/li&gt; 
 &lt;li&gt;Must have variable setting explanation, and examples of valid values. Examples: &lt;pre&gt;&lt;code class="language-python"&gt;# Explanation of what this setting will do, and instructions to enter it correctly
config_variable = "value1"    #  &amp;lt;Valid values examples, and NOTES&amp;gt; "value1", "value2", etc. Don't forget quotes ("")
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-python"&gt;# Do you want to randomize the search order for search_terms?
randomize_search_order = False     # True of False, Note: True or False are case-sensitive
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-python"&gt;# Avoid applying to jobs if their required experience is above your current_experience. (Set value as -1 if you want to apply to all ignoring their required experience...)
current_experience = 5             # Integers &amp;gt; -2 (Ex: -1, 0, 1, 2, 3, 4...)
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-python"&gt;# Search location, this will be filled in "City, state, or zip code" search box. If left empty as "", tool will not fill it.
search_location = "United States"               # Some valid examples: "", "United States", "India", "Chicago, Illinois, United States", "90001, Los Angeles, California, United States", "Bengaluru, Karnataka, India", etc.

&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Add the config variable in appropriate &lt;code&gt;/config/file&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Every config variable must be validated. Go to &lt;code&gt;/modules/validator.py&lt;/code&gt; and add it over there. Example: For config variable &lt;code&gt;search_location = ""&lt;/code&gt; found in &lt;code&gt;/config/search.py&lt;/code&gt;, string validation is added in file &lt;code&gt;/modules/validator.py&lt;/code&gt; under the method &lt;code&gt;def validate_search()&lt;/code&gt;. &lt;pre&gt;&lt;code class="language-python"&gt;def validate_search() -&amp;gt; None | ValueError | TypeError:
    '''
    Validates all variables in the `/config/search.py` file.
    '''
    check_string(search_location, "search_location")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-content"&gt;back to index&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Attestation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;All contributions require proper attestion. Format for attestation:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;##&amp;gt; ------ &amp;lt;Your full name&amp;gt; : &amp;lt;github id&amp;gt; OR &amp;lt;email&amp;gt; - &amp;lt;Type of change&amp;gt; ------
    print("My contributions üòç") # Your code
##&amp;lt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Examples for proper attestation: New feature example&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;##&amp;gt; ------ Sai Vignesh Golla : godsscion - Feature ------
def alert_box(title: str, message: str) -&amp;gt; None:
  '''
  Shows an alert box with the given `title` and `message`.
  '''
  from pyautogui import alert
  return alert(title, message)

##&amp;lt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Bug fix example&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def alert_box(title: str, message: str) -&amp;gt; None:
  '''
  Shows an alert box with the given `title` and `message`.
  '''
  from pyautogui import alert

##&amp;gt; ------ Sai Vignesh Golla : saivigneshgolla@outlook.com - Bug fix ------
  return alert(message, title)
##&amp;lt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-content"&gt;back to index&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üóìÔ∏è Major Updates History:&lt;/h2&gt; 
&lt;h3&gt;Jul 20, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Contributions from community have been added&lt;/li&gt; 
 &lt;li&gt;Better AI support, minor bug fixes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Nov 28, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Patched to work for latest changes in Linkedin.&lt;/li&gt; 
 &lt;li&gt;Users can now select to follow or not follow companies when submitting application.&lt;/li&gt; 
 &lt;li&gt;Frameworks for future AI Developments have been added.&lt;/li&gt; 
 &lt;li&gt;AI can now be used to extract skills from job description.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Oct 16, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Framework for OpenAI API and Local LLMs&lt;/li&gt; 
 &lt;li&gt;Framework for RAG&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Sep 09, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Smarter Auto-fill for salaries and notice periods&lt;/li&gt; 
 &lt;li&gt;Robust Search location filter, will work in window mode (No need for full screen)&lt;/li&gt; 
 &lt;li&gt;Better logic for Select and Radio type questions&lt;/li&gt; 
 &lt;li&gt;Proper functioning of Decline to answer questions in Equal Employment opportunity questions&lt;/li&gt; 
 &lt;li&gt;Checkbox questions select fail bug fixed&lt;/li&gt; 
 &lt;li&gt;Annotations are clearer in instructions for setup&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Sep 07, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Annotations for developers&lt;/li&gt; 
 &lt;li&gt;Robust input validations&lt;/li&gt; 
 &lt;li&gt;Restructured config file&lt;/li&gt; 
 &lt;li&gt;Fixed pagination bug&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Aug 21, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Performance improvements (skip clicking on applied jobs and blacklisted companies)&lt;/li&gt; 
 &lt;li&gt;Stop when easy apply application limit is reached&lt;/li&gt; 
 &lt;li&gt;Added ability to discard from pause at submission dialogue box&lt;/li&gt; 
 &lt;li&gt;Added support for address input&lt;/li&gt; 
 &lt;li&gt;Bug fixed radio questions, added support for physical disability questions&lt;/li&gt; 
 &lt;li&gt;Added framework for future config file updates&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 19, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Major Bug fixes (Text Area type questions)&lt;/li&gt; 
 &lt;li&gt;Made uploading default resume as not required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;May 15, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added functionality for textarea type questions &lt;code&gt;summary&lt;/code&gt;, &lt;code&gt;cover_letter&lt;/code&gt;(Summary, Cover letter); checkbox type questions (acknowledgements)&lt;/li&gt; 
 &lt;li&gt;Added feature to skip irrelevant jobs based on &lt;code&gt;bad_words&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Improved performance for answering questions&lt;/li&gt; 
 &lt;li&gt;Logic change for masters students skipping&lt;/li&gt; 
 &lt;li&gt;Change variable names &lt;code&gt;blacklist_exceptions&lt;/code&gt; -&amp;gt; &lt;code&gt;about_company_good_words&lt;/code&gt; and &lt;code&gt;blacklist_words&lt;/code&gt; -&amp;gt; &lt;code&gt;about_company_bad_words&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Added session summary for logs&lt;/li&gt; 
 &lt;li&gt;Added option to turn off "Pause before Submit" until next run&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;May 05, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For questions similar to "What is your current location?", City posted in Job description will be posted as the answer if &lt;code&gt;current_city&lt;/code&gt; is left empty in the configuration&lt;/li&gt; 
 &lt;li&gt;Added option to over write previously saved answers for a question &lt;code&gt;overwrite_previous_answers&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Tool will now save previous answer of a question&lt;/li&gt; 
 &lt;li&gt;Tool will now collect all available options for a Radio type or Select type question&lt;/li&gt; 
 &lt;li&gt;Major update in answering logic for Easy Apply Application questions&lt;/li&gt; 
 &lt;li&gt;Added Safe mode option for quick stable launches &lt;code&gt;safe_mode&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;May 04, 2024&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added option to fill in "City, state, or zip code" search box &lt;code&gt;search_location&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Bug fixes in answering City or location question&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-content"&gt;back to index&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üìú Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;This program is for educational purposes only. By downloading, using, copying, replicating, or interacting with this program or its code, you acknowledge and agree to abide by all the Terms, Conditions, Policies, and Licenses mentioned, which are subject to modification without prior notice. The responsibility of staying informed of any changes or updates bears upon yourself. For the latest Terms &amp;amp; Conditions, Licenses, or Policies, please refer to &lt;a href="https://github.com/GodsScion/Auto_job_applier_linkedIn"&gt;Auto Job Applier&lt;/a&gt;. Additionally, kindly adhere to and comply with LinkedIn's terms of service and policies pertaining to web scraping. Usage is at your own risk. The creators and contributors of this program emphasize that they bear no responsibility or liability for any misuse, damages, or legal consequences resulting from its usage.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üèõÔ∏è Terms and Conditions&lt;/h2&gt; 
&lt;p&gt;Please consider the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LinkedIn Policies&lt;/strong&gt;: LinkedIn has specific policies regarding web scraping and data collection. The responsibility to review and comply with these policies before engaging, interacting, or undertaking any actions with this program bears upon yourself. Be aware of the limitations and restrictions imposed by LinkedIn to avoid any potential violation(s).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;No Warranties or Guarantees&lt;/strong&gt;: This program is provided as-is, without any warranties or guarantees of any kind. The accuracy, reliability, and effectiveness of the program cannot be guaranteed. Use it at your own risk.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Disclaimer of Liability&lt;/strong&gt;: The creators and contributors of this program shall not be held responsible or liable for any damages or consequences arising from the direct or indirect use, interaction, or actions performed with this program. This includes but is not limited to any legal issues, loss of data, or other damages incurred.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Use at Your Own Risk&lt;/strong&gt;: It is important to exercise caution and ensure that your usage, interactions, and actions with this program comply with the applicable laws and regulations. Understand the potential risks and consequences associated with web scraping and data collection activities.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chrome Driver&lt;/strong&gt;: This program utilizes the Chrome Driver for web scraping. Please review and comply with the terms and conditions specified for &lt;a href="https://chromedriver.chromium.org/home"&gt;Chrome Driver&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚öñÔ∏è License&lt;/h2&gt; 
&lt;p&gt;Copyright (C) 2024 Sai Vignesh Golla &lt;a href="mailto:saivigneshgolla@outlook.com"&gt;saivigneshgolla@outlook.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.&lt;/p&gt; 
&lt;p&gt;This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.&lt;/p&gt; 
&lt;p&gt;You should have received a copy of the GNU Affero General Public License along with this program. If not, see &lt;a href="https://www.gnu.org/licenses/"&gt;https://www.gnu.org/licenses/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/LICENSE"&gt;AGPLv3 LICENSE&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#-content"&gt;back to index&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üêß Socials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LinkedIn&lt;/strong&gt; : &lt;a href="https://www.linkedin.com/in/saivigneshgolla/"&gt;https://www.linkedin.com/in/saivigneshgolla/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt; : &lt;a href="mailto:saivigneshgolla@outlook.com"&gt;saivigneshgolla@outlook.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;X/Twitter&lt;/strong&gt;: &lt;a href="https://x.com/saivigneshgolla"&gt;https://x.com/saivigneshgolla&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt; : godsscion&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôå Community Support and Discussions&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Discord Server&lt;/strong&gt; : &lt;a href="https://discord.gg/fFp7uUzWCY"&gt;https://discord.gg/fFp7uUzWCY&lt;/a&gt; alternate link: &lt;a href="https://discord.gg/ykfDjRFB"&gt;https://discord.gg/ykfDjRFB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions"&gt;All Discussions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/announcements"&gt;Announcements&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/general"&gt;General&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/feature-requests-or-ideas"&gt;Feature requests or Ideas&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/polls"&gt;Polls&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/community-flex"&gt;Community Flex&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/support-q-a"&gt;Support Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ÑπÔ∏è Version: 25.07.20.9.30 Community Alpha&lt;/h4&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GodsScion/Auto_job_applier_linkedIn/main/#linkedin-ai-auto-job-applier-"&gt;back to the top&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>wasmerio/Python-Scripts</title>
      <link>https://github.com/wasmerio/Python-Scripts</link>
      <description>&lt;p&gt;A curated list of python scripts for automating your tasks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Table of Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/wasmerio/Python-Scripts/main/#contributing"&gt;Contributing&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/wasmerio/Python-Scripts/main/#adding-a-new-script"&gt;Adding a New Script&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/wasmerio/Python-Scripts/main/#list-of-scripts-in-repo"&gt;List of Scripts in Repo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/wasmerio/Python-Scripts/main/#gitpod"&gt;Gitpod&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/wasmerio/Python-Scripts/main/#wall-of-contributors"&gt;Wall of Contributors&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://discord.com/invite/Yn9g6KuWyA"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Server-blue?logo=discord&amp;amp;style=for-the-badge" alt="Join Our Discord" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/@dhanushnehru?sub_confirmation=1"&gt;&lt;img src="https://img.shields.io/badge/YouTube-Subscribe-red?logo=youtube&amp;amp;style=for-the-badge" alt="Subscribe on YouTube" /&gt;&lt;/a&gt; &lt;a href="https://dhanushn.substack.com/"&gt;&lt;img src="https://img.shields.io/badge/Newsletter-Subscribe-orange?style=for-the-badge" alt="Subscribe to Newsletter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Python Scripts&lt;/h1&gt; 
&lt;p&gt;This repository consists of a list of more than 60 Python scripts, primarily those which automate a specific task. Each folder contains one or more .py files and a README to explain what that specific Python script needs to run. These scripts are free to use as long as long as the original contributor is credited.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We encourage contributions from the community to make this repository even more valuable. Whether you want to add a new Python script or enhance an existing one, we welcome your input. Here's how you can contribute:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note: Please follow the maintainer of the repository for quick approval of the PR's via &lt;a href="https://twitter.com/Dhanush_Nehru"&gt;Twitter&lt;/a&gt;, &lt;a href="https://www.instagram.com/dhanush_nehru/"&gt;Instagram&lt;/a&gt;, &lt;a href="https://www.youtube.com/@dhanushnehru?sub_confirmation=1"&gt;Youtube&lt;/a&gt;, &lt;a href="https://github.com/DhanushNehru"&gt;Github&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Adding a New Script&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;1. Create an issue:&lt;/strong&gt; Start by creating an issue in this repository. Describe the new script you'd like to contribute, its purpose, and any specific features it will provide.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. Fork the repository:&lt;/strong&gt; Fork this repository to your own GitHub account to create a copy you can work on.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3. Create a Pull Request (PR):&lt;/strong&gt; In your forked repository, develop the Python script and include a &lt;code&gt;README&lt;/code&gt; file explaining how to use it. Ensure you credit the original contributor if you're building upon an existing script.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;4. Update the List:&lt;/strong&gt; If you're adding a new script, make sure to include it in the "List of Python Scripts" section below, providing a title, a link to the script's folder, and a brief description.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Remember to close your issues once your PR has been merged! They can be reopened if needed.&lt;/p&gt; 
&lt;p&gt;More information on contributing and the general code of conduct for discussion and feedback in this repo can be found here: &lt;a href="https://github.com/DhanushNehru/Python-Scripts/raw/main/CONTRIBUTIONS.md"&gt;CONTRIBUTIONS.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;List of Scripts in Repo&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Script&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Arrange It&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Arrange%20It"&gt;Arrange It&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script that can automatically move files into corresponding folders based on their extensions.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Auto WiFi Check&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Auto%20WiFi%20Check"&gt;Auto WiFi Check&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to monitor if the WiFi connection is active or not&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AutoCert&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/AutoCert"&gt;AutoCert&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to auto-generate e-certificates in bulk.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Autocomplete Notes App&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Autocomplete%20Notes%20App"&gt;Autocomplete Notes App&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script that provides a notes application with autocomplete features.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Automated Emails&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Automate%20Emails%20Daily"&gt;Automated Emails&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to send out personalized emails by reading a CSV file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Black Hat Python&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Black%20Hat%20Python"&gt;Black Hat Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Source code from the book Black Hat Python&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Blackjack&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Blackjack"&gt;Blackjack&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A game of Blackjack - let's get a 21.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chessboard&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Chess%20Board"&gt;Chessboard&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Creates a chessboard using matplotlib.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Compound Interest Calculator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Calculate%20Compound%20Interest"&gt;Compound Interest Calculator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to calculate compound interest.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Convert Temperature&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Convert%20Temperature"&gt;Convert Temperature&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A python script to convert temperature between Fahreheit, Celsius and Kelvin&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Connect Four&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Connect%20Four"&gt;Connect Four&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script for the board game Connect Four, which can be played by 1-2 players&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Countdown Timer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Countdown%20Timer"&gt;Countdown Timer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Displays a message when the Input time elapses.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Crop Images&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Crop%20Images"&gt;Crop Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to crop a given image.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CSV to Excel&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/CSV%20to%20Excel"&gt;CSV to Excel&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to convert a CSV to an Excel file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CSV_TO_NDJSON&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/CSV_TO_NDJSON"&gt;CSV to Excel&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to convert a CSV to an NDJSON files file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Currency Script&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Currency%20Script"&gt;Currency Script&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to convert the currency of one country to that of another.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Digital Clock&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Digital%20Clock"&gt;Digital Clock&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to preview a digital clock in the terminal.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Disk Usage Visualizer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Disk%20Usage%20Visualizer"&gt;Disk Usage Visualizer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to display the top N directories taking up space in your disk.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Display Popup Window&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Display%20Popup%20Window"&gt;Display Popup Window&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to preview a GUI interface to the user.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Distance Calculator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Mathdallas-code/Python-Scripts/tree/main/Distance%20Calculator"&gt;Distance Calculator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to calculate the distance between two points.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Duplicate Finder&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Duplicate%Fnder"&gt;Duplicate Finder&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The script identifies duplicate files by MD5 hash and allows deletion or relocation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Emoji&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Emoji"&gt;Emoji&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The script generates a PDF with an emoji using a custom TrueType font.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Emoji to PDF&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Emoji%20To%20Pdf"&gt;Emoji to PDF&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python Script to view Emoji in PDF.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Expense Tracker&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Expense%20Tracker"&gt;Expense Tracker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script which can track expenses.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Face Reaction&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Face%20Reaction"&gt;Face Reaction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A script which attempts to detect facial expressions.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fake Profiles&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Fake%20Profile"&gt;Fake Profiles&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Creates fake profiles.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;File Encryption Decryption&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/File%20Encryption%20Decryption"&gt;File Encryption Decryption&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Encrypts and Decrypts files using AES Algorithms for Security purposes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;File Search&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/debojit11/Python-Scripts/tree/main/File_Search"&gt;File_search&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A python script that searches a specified folder for all files, regardless of file type, within its directory and subdirectories.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Font Art&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Font%20Art"&gt;Font Art&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Displays a font art using Python.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;File Organizer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/FileOrganizer"&gt;FileOrganizer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Organizes files into different folders according to their file type&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;File Renamer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/FileRenamer"&gt;FileRenamer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Bulk renames files with the same start/end&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;File Text Search&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/FileTextSearch"&gt;FileTextSearch&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Searches for a keyword/phrase accross different files&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Freelance Helper Program&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/freelance-help-program"&gt;freelance-helper&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Takes an Excel file with working hours and calculates the payment.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Get Hexcodes From Websites&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Get%20Hexcodes%20From%20Websites"&gt;Get Hexcodes From Websites&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generates a Python list containing Hexcodes from a website.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hand_Volume&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Hand%20Volume"&gt;Hand_Volume&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Detects and tracks hand movements to control volume.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hangman Game&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Hangman%20Game"&gt;Hangman&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A classic word-guessing game where players guess letters to find the hidden word&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hardware Detector&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Hardware%20Detector"&gt;Hardware Detector&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script that detects and displays hardware specifications and resource usage, including CPU, RAM, disk space, and GPU information.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Harvest Predictor&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Harvest%20Predictor"&gt;Harvest Predictor&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Takes some necessary input parameters and predicts harvest according to it.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Html-to-images&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/HTML%20to%20Images"&gt;html-to-images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts HTML documents to image files.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image Capture&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Capture"&gt;Image Capture&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Captures image from your webcam and saves it on your local device.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image Compress&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Compress"&gt;Image Compress&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Takes an image and compresses it.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image Manipulation without libraries&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Manipulation%20without%20libraries"&gt;Image Manipulation without libraries&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Manipulates images without using any external libraries.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image Text&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Text"&gt;Image Text&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extracts text from the image.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image Text to PDF&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Text%20to%20PDF"&gt;Image Text to PDF&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Adds an image and text to a PDF.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image Uploader&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Uploader"&gt;Image Uploader&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Uploads images to Imgur using a keyboard shortcut.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image Watermarker&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Watermarker"&gt;Image Watermarker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Adds a watermark to an image.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image to ASCII&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20to%20ASCII"&gt;Image to ASCII&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts an image into ASCII art.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image to Gif&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20to%20GIF"&gt;Image to Gif&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate gif from images.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Images to WebP Converter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Images%20to%20WebP%20Converter"&gt;Images to WebP Converter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts images to WebP via cmd or GUI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Interactive Dictionary&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20InteractiveDictionary"&gt;Interactive Dictionary&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;finding out meanings of words&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;IP Geolocator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/IP%20Geolocator"&gt;IP Geolocator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Uses an IP address to geolocate a location on Earth.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Jokes Generator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Jokes%20Generator"&gt;Jokes generator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generates jokes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JSON to CSV 1&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/JSON%20to%20CSV"&gt;JSON to CSV 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts JSON to CSV files.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JSON to CSV 2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/JSON%20to%20CSV%202"&gt;JSON to CSV 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts a JSON file to a CSV file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JSON to CSV converter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Json%20to%20CSV%20Convertor"&gt;JSON to CSV converter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts JSON file to CSV files. It can convert nested JSON files as well. A sample JSON is included for testing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JSON to YAML converter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/JSON%20to%20YAML"&gt;JSON to YAML converter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts JSON file to YAML files. A sample JSON is included for testing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Keylogger&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Keylogger"&gt;Keylogger&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Keylogger that can track your keystrokes, clipboard text, take screenshots at regular intervals, and records audio.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Keyword - Retweeting&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Keyword%20Retweet%20Twitter%20Bot"&gt;Keyword - Retweeting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Find the latest tweets containing given keywords and then retweet them.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LinkedIn Bot&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/LinkedIn%20Bot"&gt;LinkedIn Bot&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Automates the process of searching for public profiles on LinkedIn and exporting the data to an Excel sheet.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Longitude &amp;amp; Latitude to conical coverter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/wasmerio/Python-Scripts/main/master/Longitude%20Latitude%20conical%20converter"&gt;Longitude Latitude conical converter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts Longitude and Latitude to Lambert conformal conic projection.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mail Sender&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Mail%20Sender"&gt;Mail Sender&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sends an email.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Merge Two Images&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Merge%20Two%20Images"&gt;Merge Two Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Merges two images horizontally or vertically.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mood based youtube song generator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Mood%20based%20youtube%20song%20generator"&gt;Mood based youtube song generator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This Python script fetches a random song from YouTube based on your mood input and opens it in your default web browser.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mouse mover&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Mouse%20Mover"&gt;Mouse mover&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Moves your mouse every 15 seconds.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi-Platform Icon Generator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Multi-Platform%20Icon%20Generator"&gt;Multi-Platform Icon Generator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Automates creation of 35+ platform-specific icon sizes from one source image for app stores and web deployment.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Morse Code&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Morse%20Code"&gt;Morse Code&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Encodes and decodes Morse code.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;No Screensaver&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/No%20Screensaver"&gt;No Screensaver&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Prevents screensaver from turning on.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OTP Verification&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/OTP%20%20Verify"&gt;OTP Verification&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;An OTP Verification Checker.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Password Generator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Password%20Generator"&gt;Password Generator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generates a random password.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Password Manager&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/nem5345/Python-Scripts/tree/main/Password%20Manager"&gt;Password Manager&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate and interact with a password manager.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Password Strength Checker&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/nem5345/Python-Scripts/tree/main/Password%20Strength%20Checker"&gt;Password Strength Checker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Evaluates how strong a given password is.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PDF Merger&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/PDF%20Merger"&gt;PDF Merger&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Merges multiple PDF files into a single PDF, with options for output location and custom order.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PDF to Audio&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/PDF%20to%20Audio"&gt;PDF to Audio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts PDF to audio.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PDF to Text&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/PDF%20to%20text"&gt;PDF to text&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Converts PDF to text.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PDF merger and splitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/AbhijitMotekar99/Python-Scripts/raw/main/PDF%20Merger%20and%20Splitter/PDF%20Merger%20and%20Splitter.py"&gt;PDF Merger and Splitter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Create a tool that can merge multiple PDF files into one or split a single PDF into separate pages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pizza Order&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Pizza%20Order"&gt;Pizza Order&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;An algorithm designed to handle pizza orders from customers with accurate receipts and calculations.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planet Simulation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Planet%20Simulation"&gt;Planet Simulation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A simulation of several planets rotating around the sun.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Playlist Exchange&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Playlist%20Exchange"&gt;Playlist Exchange&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to exchange songs and playlists between Spotify and Python.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pigeonhole Sort&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/PigeonHole"&gt;Algorithm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The pigeonhole sort algorithm to sort your arrays efficiently!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PNG TO JPG CONVERTOR&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/PNG%20To%20JPG"&gt;PNG-To-JPG&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A PNG TO JPG IMAGE CONVERTOR.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pomodoro Timer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Pomodoro%20Timer"&gt;Pomodoro Timer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Pomodoro timer&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python GUI Notepad&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Python%20GUI%20Notepad"&gt;Python GUI Notepad&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python-based GUI Notepad with essential features like saving, opening, editing text files, basic formatting, and a simple user interface for quick note-taking.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Profanity Checker&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Profanity%20Checker"&gt;Profanity Checker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to detect and optionally censor profanity in text.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QR Code Generator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/QR%20Code%20Generator"&gt;QR Code Generator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This is generate a QR code from the provided link&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QR Code Scanner&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/QR%20Code%20Scanner"&gt;QR Code Scanner&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Helps in Sacanning the QR code in form of PNG or JPG just by running the python script.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QR Code with logo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/QR%20with%20Logo"&gt;QR code with Logo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;QR Code Customization Feature&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Random Color Generator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Random%20Color%20Generator"&gt;Random Color Generator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A random color generator that will show you the color and values!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Remove Background&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Remove%20Background"&gt;Remove Background&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Removes the background of images.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Road-Lane-Detection&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NotIncorecc/Python-Scripts/tree/main/Road-Lane-Detection"&gt;Road-Lane-Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Detects the lanes of the road&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rock Paper Scissor 1&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Rock%20Paper%20Scissor%201"&gt;Rock Paper Scissor 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A game of Rock Paper Scissors.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rock Paper Scissor 2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Rock%20Paper%20Scissor%202"&gt;Rock Paper Scissor 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A new version game of Rock Paper Scissors.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Run Then Notify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Run%20Then%20Notify"&gt;Run Then Notify&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Runs a slow command and emails you when it completes execution.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Save File To Drive&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/master/Save%20file%20to%20Drive"&gt;Save File To Drive&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Saves all files and folder with proper structure from a folder to drive easily through a python script .&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Selfie with Python&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Selfie%20with%20Python"&gt;Selfie with Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Take your selfie with python .&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Simple DDOS&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Simple%20DDOS"&gt;Simple DDOS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The code allows you to send multiple HTTP requests concurrently for a specified duration.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Simple TCP Chat Server&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/TCP%20Chat%20Server"&gt;Simple TCP Chat Server&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Creates a local server on your LAN for receiving and sending messages!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Smart Attendance System&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Smart%20Attendance%20System"&gt;Smart Attendance System&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This OpenCV framework is for Smart Attendance by actively decoding a student's QR Code.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Snake Game&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Snake%20Game"&gt;Snake Game&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Classic snake game using python.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Snake Water Gun&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Snake%20Water%20Gun"&gt;Snake Water Gun&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A game similar to Rock Paper Scissors.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Sorting&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Sorting"&gt;Sorting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Algorithm for bubble sorting.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Star Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Star%20Pattern"&gt;Star Pattern&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Creates a star pattern pyramid.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Subnetting Calculator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Subnetting%20Calculator"&gt;Subnetting Calculator&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Calculates network information based on a given IP address and subnet mask.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Take a break&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Take%20A%20Break"&gt;Take a break&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python code to take a break while working long hours.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text Recognition&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/Text-Recognition/Text%20Recognition"&gt;Text Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Image Text Recognition ML Model to extract text from Images&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text to Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Text%20to%20Image"&gt;Text to Image&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script that will take your text and convert it to a JPEG.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Thread Progress&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Thread%20Progress"&gt;Thread Progress&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script demonstrating safe multithreading by using a lock to update a shared progress variable concurrently.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tic Tac Toe 1&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Tic-Tac-Toe%201"&gt;Tic Tac Toe 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A game of Tic Tac Toe.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tik Tac Toe 2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Tic-Tac-Toe%202"&gt;Tik Tac Toe 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A game of Tik Tac Toe.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Turtle Art &amp;amp; Patterns&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Turtle%20Art"&gt;Turtle Art&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Scripts to view turtle art also have prompt-based ones.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Turtle Graphics&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Turtle%20Graphics"&gt;Turtle Graphics&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Code using turtle graphics.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter Selenium Bot&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Twitter%20Selenium%20Bot"&gt;Twitter Selenium Bot&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A bot that can interact with Twitter in a variety of ways.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Umbrella Reminder&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Umbrella%20Reminder"&gt;Umbrella Reminder&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A reminder for umbrellas.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;University Rankings&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/University%20Rankings"&gt;University Rankings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ranks Universities across the globes using the Country, Continent, and Captial&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;URL Shortener&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/URL%20Shortener"&gt;URL Shortener&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A URL shortener code compresses long URLs into shorter, more manageable links&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Video Downloader&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Video%20Downloader"&gt;Video Downloader&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Download Videos from youtube to your local system.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Video Watermarker&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Video%20Watermarker"&gt;Video Watermarker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Adds watermark to any video of your choice.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Virtual Painter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Virtual%20Painter"&gt;Virtual Painter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Virtual painting application.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wallpaper Changer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Wallpaper%20Changer"&gt;Wallpaper Changer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Automatically changes home wallpaper, adding a random quote and stock tickers on it.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Weather GUI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Weather%20GUI"&gt;Weather GUI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Displays information on the weather.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Website Blocker&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Website%20Blocker"&gt;Website Blocker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Downloads the website and loads it on your homepage in your local IP.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Website Cloner&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Website%20Cloner"&gt;Website Cloner&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Clones any website and opens the site in your local IP.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Web Scraper&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Web%20Scraper"&gt;Web Scraper&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script that scrapes blog titles from &lt;a href="https://www.python.org/"&gt;Python.org&lt;/a&gt; and saves them to a file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Weight Converter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Weight%20Converter"&gt;Weight Converter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Simple GUI script to convert weight in different measurement units.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wikipedia Data Extractor&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Wikipedia%20Data%20Extractor"&gt;Wikipedia Data Extractor&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A simple Wikipedia data extractor script to get output in your IDE.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Word to PDF&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Word%20to%20PDF%20converter"&gt;Word to PDF&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Python script to convert an MS Word file to a PDF file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Youtube Downloader&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Youtube%20Downloader"&gt;Youtube Downloader&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Downloads any video from &lt;a href="https://youtube.com"&gt;YouTube&lt;/a&gt; in video or audio format!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Youtube Playlist Info Scraper&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/DhanushNehru/Python-Scripts/tree/main/Youtube%20Playlist%20Info%20Scraper"&gt;Youtube Playlist Info Scraper&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This python module retrieve information about a YouTube playlist in json format using playlist link.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Wall of Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/DhanushNehru/Python-Scripts/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=DhanushNehru/Python-Scripts" /&gt; &lt;/a&gt; 
&lt;p&gt;If you liked this repository, support it by starring ‚≠ê&lt;/p&gt; 
&lt;p&gt;Thank You for being here :)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=microsoft" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader" alt="Technical Report" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/Status-New-brightgreen?style=flat" alt="New" /&gt; 
 &lt;img src="https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;amp;logo=soundcharts" alt="Realtime TTS" /&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-16: üì£ We added more experimental speakers for exploration, including multilingual voices and 11 distinct English style voices. &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices"&gt;Try it&lt;/a&gt;. More speaker types will be added over time.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;2025-12-09: üì£ We added experimental speakers in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) for exploration‚Äîwelcome to try them out and share your feedback.&lt;/p&gt; 
 &lt;p&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers. &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;(Launch your own realtime demo via the websocket example in &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo"&gt;Usage&lt;/a&gt;).&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/p&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;VibeVoice is a novel framework designed for generating &lt;strong&gt;expressive&lt;/strong&gt;, &lt;strong&gt;long-form&lt;/strong&gt;, &lt;strong&gt;multi-speaker&lt;/strong&gt; conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.&lt;/p&gt; 
&lt;p&gt;VibeVoice currently includes two model variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Long-form multi-speaker model&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; with up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt;, surpassing the typical 1‚Äì2 speaker limits of many prior models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;Realtime streaming TTS model&lt;/a&gt;&lt;/strong&gt;: Produces initial audible speech in ~&lt;strong&gt;300 ms&lt;/strong&gt; and supports &lt;strong&gt;streaming text input&lt;/strong&gt; for single-speaker &lt;strong&gt;real-time&lt;/strong&gt; speech generation; designed for low-latency generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/MOS-preference.png" alt="MOS Preference Results" height="260px" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice.jpg" alt="VibeVoice Overview" height="250px" style="margin-right: 10px;" /&gt; &lt;/p&gt; 
&lt;h3&gt;üéµ Demo Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Video Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We produced this video with &lt;a href="https://github.com/Wan-Video/Wan2.2"&gt;Wan2.2&lt;/a&gt;. We sincerely appreciate the Wan-Video team for their great work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;For more examples, see the &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Risks and limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.&lt;/p&gt; 
&lt;p&gt;Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.&lt;/p&gt; 
&lt;p&gt;Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PennyLaneAI/pennylane</title>
      <link>https://github.com/PennyLaneAI/pennylane</link>
      <description>&lt;p&gt;PennyLane is a cross-platform Python library for quantum computing, quantum machine learning, and quantum chemistry. Built by researchers, for research.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;!-- Tests (GitHub actions) --&gt; &lt;a href="https://github.com/PennyLaneAI/pennylane/actions?query=workflow%3ATests"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/PennyLaneAI/PennyLane/tests.yml?branch=master&amp;amp;style=flat-square" /&gt; &lt;/a&gt; 
 &lt;!-- CodeCov --&gt; &lt;a href="https://codecov.io/gh/PennyLaneAI/pennylane"&gt; &lt;img src="https://img.shields.io/codecov/c/github/PennyLaneAI/pennylane/master.svg?logo=codecov&amp;amp;style=flat-square" /&gt; &lt;/a&gt; 
 &lt;!-- ReadTheDocs --&gt; &lt;a href="https://docs.pennylane.ai/en/latest"&gt; &lt;img src="https://readthedocs.com/projects/xanaduai-pennylane/badge/?version=latest&amp;amp;style=flat-square" /&gt; &lt;/a&gt; 
 &lt;!-- PyPI --&gt; &lt;a href="https://pypi.org/project/PennyLane"&gt; &lt;img src="https://img.shields.io/pypi/v/PennyLane.svg?style=flat-square" /&gt; &lt;/a&gt; 
 &lt;!-- Forum --&gt; &lt;a href="https://discuss.pennylane.ai"&gt; &lt;img src="https://img.shields.io/discourse/https/discuss.pennylane.ai/posts.svg?logo=discourse&amp;amp;style=flat-square" /&gt; &lt;/a&gt; 
 &lt;!-- License --&gt; &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt; &lt;img src="https://img.shields.io/pypi/l/PennyLane.svg?logo=apache&amp;amp;style=flat-square" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pennylane.ai"&gt;PennyLane&lt;/a&gt; is a cross-platform Python library for &lt;a href="https://pennylane.ai/qml/quantum-computing/"&gt;quantum computing&lt;/a&gt;, &lt;a href="https://pennylane.ai/qml/quantum-machine-learning/"&gt;quantum machine learning&lt;/a&gt;, and &lt;a href="https://pennylane.ai/qml/quantum-chemistry/"&gt;quantum chemistry&lt;/a&gt;. &lt;/p&gt; 
&lt;p align="center"&gt; The definitive open-source framework for quantum programming. Built by researchers, for research. &lt;img src="https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/readme/pl-logo-lightmode.png#gh-light-mode-only" width="700px" /&gt; 
 &lt;!--
    Use a relative import for the dark mode image. When loading on PyPI, this
    will fail automatically and show nothing.
    --&gt; &lt;img src="https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/readme/pl-logo-darkmode.png#gh-dark-mode-only" width="700px" onerror="this.style.display='none'" alt="" /&gt; &lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;img src="https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/code.png" width="400px" align="right" /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Program quantum computers&lt;/em&gt;&lt;/strong&gt;. Build quantum circuits with a wide range of state preparations, gates, and measurements. Run on &lt;a href="https://pennylane.ai/performance/"&gt;high-performance simulators&lt;/a&gt; or &lt;a href="https://pennylane.ai/plugins/"&gt;various hardware devices&lt;/a&gt;, with advanced features like mid-circuit measurements and error mitigation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Master quantum algorithms&lt;/em&gt;&lt;/strong&gt;. From NISQ to fault-tolerant quantum computing, unlock algorithms for research and application. Analyze performance, visualize circuits, and access tools for &lt;a href="https://docs.pennylane.ai/en/stable/introduction/chemistry.html"&gt;quantum chemistry&lt;/a&gt; and &lt;a href="https://pennylane.ai/search/?contentType=DEMO&amp;amp;categories=algorithms&amp;amp;sort=publication_date"&gt;algorithm development&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Machine learning with quantum hardware and simulators&lt;/em&gt;&lt;/strong&gt;. Integrate with &lt;strong&gt;PyTorch&lt;/strong&gt;, &lt;strong&gt;TensorFlow&lt;/strong&gt;, &lt;strong&gt;JAX&lt;/strong&gt;, &lt;strong&gt;Keras&lt;/strong&gt;, or &lt;strong&gt;NumPy&lt;/strong&gt; to define and train hybrid models using quantum-aware optimizers and hardware-compatible gradients for advanced research tasks. &lt;a href="https://docs.pennylane.ai/en/stable/introduction/interfaces.html"&gt;Quantum machine learning quickstart&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Quantum datasets&lt;/em&gt;&lt;/strong&gt;. Access high-quality, pre-simulated datasets to decrease time-to-research and accelerate algorithm development. &lt;a href="https://pennylane.ai/datasets/"&gt;Browse the datasets&lt;/a&gt; or contribute your own data.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Compilation and performance&lt;/em&gt;&lt;/strong&gt;. Experimental support for just-in-time compilation. Compile your entire hybrid workflow, with support for advanced features such as adaptive circuits, real-time measurement feedback, and unbounded loops. See &lt;a href="https://github.com/pennylaneai/catalyst"&gt;Catalyst&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more details and additional features, please see the &lt;a href="https://pennylane.ai/features/"&gt;PennyLane website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;PennyLane requires Python version 3.11 and above. Installation of PennyLane, as well as all dependencies, can be done using pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;python -m pip install pennylane
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker support&lt;/h2&gt; 
&lt;p&gt;Docker images are found on the &lt;a href="https://hub.docker.com/u/pennylaneai"&gt;PennyLane Docker Hub page&lt;/a&gt;, where there is also a detailed description about PennyLane Docker support. &lt;a href="https://docs.pennylane.ai/projects/lightning/en/stable/dev/docker.html"&gt;See description here&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Get up and running quickly with PennyLane by following our &lt;a href="https://docs.pennylane.ai/en/stable/introduction/pennylane.html"&gt;quickstart guide&lt;/a&gt;, designed to introduce key features and help you start building quantum circuits right away.&lt;/p&gt; 
&lt;p&gt;Whether you're exploring quantum machine learning (QML), quantum computing, or quantum chemistry, PennyLane offers a wide range of tools and resources to support your research:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/readme/research.png" align="right" width="350px" /&gt; 
&lt;h3&gt;Key Resources:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pennylane.ai/qml/demonstrations"&gt;Research-oriented Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pennylane.ai/qml/"&gt;Learn Quantum Programming&lt;/a&gt; with the &lt;a href="https://pennylane.ai/codebook/"&gt;Codebook&lt;/a&gt; and &lt;a href="https://pennylane.ai/challenges/"&gt;Coding Challenges&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pennylane.ai/faq"&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pennylane.ai/qml/glossary"&gt;Glossary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pennylane.ai/qml/videos"&gt;Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also check out our &lt;a href="https://pennylane.readthedocs.io"&gt;documentation&lt;/a&gt; for &lt;a href="https://pennylane.readthedocs.io/en/stable/introduction/pennylane.html"&gt;quickstart guides&lt;/a&gt; to using PennyLane, and detailed developer guides on &lt;a href="https://pennylane.readthedocs.io/en/stable/development/plugins.html"&gt;how to write your own&lt;/a&gt; PennyLane-compatible quantum device.&lt;/p&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;Take a deeper dive into quantum computing by exploring cutting-edge algorithms using PennyLane and quantum hardware. &lt;a href="https://pennylane.ai/qml/demonstrations"&gt;Explore PennyLane demos&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://pennylane.ai/qml/demonstrations"&gt; &lt;img src="https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/readme/demos.png" width="900px" /&gt; &lt;/a&gt; 
&lt;p&gt;If you would like to contribute your own demo, see our &lt;a href="https://pennylane.ai/qml/demos_submission"&gt;demo submission guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Research Applications&lt;/h2&gt; 
&lt;p&gt;PennyLane is at the forefront of research in quantum computing, quantum machine learning, and quantum chemistry. Explore how PennyLane is used for research in the following publications:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Quantum Computing&lt;/strong&gt;: &lt;a href="https://quantum-journal.org/papers/q-2023-03-02-934/"&gt;Fast quantum circuit cutting with randomized measurements&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Quantum Machine Learning&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2403.07059"&gt;Better than classical? The subtle art of benchmarking quantum machine learning models&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Quantum Chemistry&lt;/strong&gt;: &lt;a href="https://quantum-journal.org/papers/q-2024-06-13-1371/"&gt;Accelerating Quantum Computations of Chemistry Through Regularized Compressed Double Factorization&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Impactful research drives PennyLane. Let us know what features you need for your research on &lt;a href="https://github.com/PennyLaneAI/pennylane/issues/new?assignees=&amp;amp;labels=enhancement+%3Asparkles%3A&amp;amp;projects=&amp;amp;template=feature_request.yml"&gt;GitHub&lt;/a&gt; or on our &lt;a href="https://pennylane.ai/research"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing to PennyLane&lt;/h2&gt; 
&lt;p&gt;We welcome contributions‚Äîsimply fork the PennyLane repository, and then make a &lt;a href="https://help.github.com/articles/about-pull-requests/"&gt;pull request&lt;/a&gt; containing your contribution. All contributors to PennyLane will be listed as authors on the releases. All users who contribute significantly to the code (new plugins, new functionality, etc.) will be listed on the PennyLane arXiv paper.&lt;/p&gt; 
&lt;p&gt;We also encourage bug reports, suggestions for new features and enhancements, and even links to cool projects or applications built on PennyLane.&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://github.com/PennyLaneAI/pennylane/raw/master/.github/CONTRIBUTING.md"&gt;contributions page&lt;/a&gt; and our &lt;a href="https://pennylane.readthedocs.io/en/stable/development/guide.html"&gt;Development guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Source Code:&lt;/strong&gt; &lt;a href="https://github.com/PennyLaneAI/pennylane"&gt;https://github.com/PennyLaneAI/pennylane&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Issue Tracker:&lt;/strong&gt; &lt;a href="https://github.com/PennyLaneAI/pennylane/issues"&gt;https://github.com/PennyLaneAI/pennylane/issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are having issues, please let us know by posting the issue on our GitHub issue tracker.&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://discuss.pennylane.ai/"&gt;PennyLane Discussion Forum&lt;/a&gt; to connect with the quantum community, get support, and engage directly with our team. It‚Äôs the perfect place to share ideas, ask questions, and collaborate with fellow researchers and developers!&lt;/p&gt; 
&lt;p&gt;Note that we are committed to providing a friendly, safe, and welcoming environment for all. Please read and respect the &lt;a href="https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/.github/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;PennyLane is the work of &lt;a href="https://github.com/PennyLaneAI/pennylane/graphs/contributors"&gt;many contributors&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are doing research using PennyLane, please cite &lt;a href="https://arxiv.org/abs/1811.04968"&gt;our paper&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ville Bergholm et al. &lt;em&gt;PennyLane: Automatic differentiation of hybrid quantum-classical computations.&lt;/em&gt; 2018. arXiv:1811.04968&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;PennyLane is &lt;strong&gt;free&lt;/strong&gt; and &lt;strong&gt;open source&lt;/strong&gt;, released under the Apache License, Version 2.0.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>beancount/beancount</title>
      <link>https://github.com/beancount/beancount</link>
      <description>&lt;p&gt;Beancount: Double-Entry Accounting from Text Files.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;======================================================== beancount: Double-Entry Accounting from Text Files&lt;/h1&gt; 
&lt;p&gt;.. contents:: .. 1 Description 2 Documentation 3 Download &amp;amp; Installation 4 Versions 5 Filing Bugs 6 Copyright and License 7 Donations 8 Sincerely, thank you. 9 Author&lt;/p&gt; 
&lt;h1&gt;Description&lt;/h1&gt; 
&lt;p&gt;A double-entry bookkeeping computer language that lets you define financial transaction records in a text file, read them in memory, generate a variety of reports from them, and provides a web interface.&lt;/p&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;Documentation can be found at:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://beancount.github.io/docs/"&gt;https://beancount.github.io/docs/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Documentation authoring happens on Google Docs, where you can contribute by requesting access or commenting on individual documents. An index of all source documents is available here:&lt;/p&gt; 
&lt;p&gt;&lt;a href="http://furius.ca/beancount/doc/index"&gt;http://furius.ca/beancount/doc/index&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;There's a &lt;code&gt;mailing-list dedicated to Beancount &amp;lt;https://groups.google.com/forum/#!forum/beancount&amp;gt;&lt;/code&gt;&lt;em&gt;. Please post questions there, so others can share in the responses. More general discussions about command-line accounting also occur on the &lt;code&gt;Ledger mailing-list &amp;lt;https://groups.google.com/forum/#!forum/ledger-cli&amp;gt;&lt;/code&gt;&lt;/em&gt; so you might be interested in that group as well.&lt;/p&gt; 
&lt;h1&gt;Download &amp;amp; Installation&lt;/h1&gt; 
&lt;p&gt;You can obtain the source code from the official Git repository on Github:&lt;/p&gt; 
&lt;p&gt;| &lt;a href="https://github.com/beancount/beancount/"&gt;https://github.com/beancount/beancount/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;Installing Beancount&lt;/code&gt;__ for more details.&lt;/p&gt; 
&lt;p&gt;__ &lt;a href="http://furius.ca/beancount/doc/install"&gt;http://furius.ca/beancount/doc/install&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Versions&lt;/h1&gt; 
&lt;p&gt;There are three versions&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Version 3&lt;/strong&gt; (&lt;code&gt;branch v3 &amp;lt;http://github.com/beancount/beancount/tree/v3&amp;gt;&lt;/code&gt;_): The current stable version of Beancount since June 2024. Use this. Note that this version is trimmed down from v2 and most of the tools the v2 branch included have been moved to their own independent projects on Github.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Version 2&lt;/strong&gt; (&lt;code&gt;branch v2 &amp;lt;http://github.com/beancount/beancount/tree/v2&amp;gt;&lt;/code&gt;_): The previous stable version of Beancount, in maintenance mode from 2020 until 2024 and now frozen and obsolete. This was a complete rewrite of the first version, which introduced a number of constraints and a new grammar and much more.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Version 1&lt;/strong&gt; (&lt;code&gt;branch v1 &amp;lt;http://github.com/beancount/beancount/tree/v1&amp;gt;&lt;/code&gt;_): The original version of Beancount. Development on this version halted in 2013. This initial version was intended to be similar to and partially compatible with Ledger. Do not use this.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Filing Bugs&lt;/h1&gt; 
&lt;p&gt;Tickets can be filed at on the Github project page:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/beancount/beancount/issues"&gt;https://github.com/beancount/beancount/issues&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Copyright and License&lt;/h1&gt; 
&lt;p&gt;Copyright (C) 2007-2025 Martin Blais. All Rights Reserved.&lt;/p&gt; 
&lt;p&gt;This code is distributed under the terms of the "GNU GPLv2 only". See COPYING file for details.&lt;/p&gt; 
&lt;h1&gt;Donations&lt;/h1&gt; 
&lt;p&gt;Beancount has found itself being useful to many users, companies, and foundations since I started it around 2007. I never ask for money, as my intent with this project is to build something that is useful to me first, as well as for others, in the simplest, most durable manner, and I believe in the genuinely free and open stance of Open Source software. Though its ends are utilitarian - it is about doing my own accounting in the first order - it is also a labor of love and I take great pride in it, pride which has pushed me to add the polish so that it would be usable and understandable by others. This is one of the rare areas of my software practice where I can let my desire for perfection and minimalism run untamed from the demands of time and external constraints.&lt;/p&gt; 
&lt;p&gt;Many people have asked where they can donate for the project. If you would like to give back, you can send a donation via Wise (preferably):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://wise.com/share/martinb4019
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or PayPal at:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;https://www.paypal.com/paypalme/misislavski
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Your donation is always appreciated in any amount, and while the countless hours spent on building this project are impossible to match, the impact of each donation is much larger than its financial import. I truly appreciate every person who offers one; software can be a lonely endeavour, and those donations as well as words of appreciation keep reminding me of the positive impact my side projects can have on others. I feel gratitude for all users of Beancount.&lt;/p&gt; 
&lt;p&gt;Thank you!&lt;/p&gt; 
&lt;h1&gt;Author&lt;/h1&gt; 
&lt;p&gt;Martin Blais &lt;a href="mailto:blais@furius.ca"&gt;blais@furius.ca&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>