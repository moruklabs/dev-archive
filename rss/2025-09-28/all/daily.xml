<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:29:54 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>bytedance/Dolphin</title>
      <link>https://github.com/bytedance/Dolphin</link>
      <description>&lt;p&gt;The official repo for ‚ÄúDolphin: Document Image Parsing via Heterogeneous Anchor Prompting‚Äù, ACL, 2025.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/dolphin.png" width="300" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://arxiv.org/abs/2505.14059"&gt; &lt;img src="https://img.shields.io/badge/Paper-arXiv-red" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/HuggingFace-Dolphin-yellow" /&gt; &lt;/a&gt; 
 &lt;a href="https://modelscope.cn/models/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/ModelScope-Dolphin-purple" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/spaces/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/Demo-Dolphin-blue" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/bytedance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/Code-Github-green" /&gt; &lt;/a&gt; 
 &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-lightgray" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/demo.gif" width="800" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting&lt;/h1&gt; 
&lt;p&gt;Dolphin (&lt;strong&gt;Do&lt;/strong&gt;cument Image &lt;strong&gt;P&lt;/strong&gt;arsing via &lt;strong&gt;H&lt;/strong&gt;eterogeneous Anchor Prompt&lt;strong&gt;in&lt;/strong&gt;g) is a novel multimodal document image parsing model following an analyze-then-parse paradigm. This repository contains the demo code and pre-trained models for Dolphin.&lt;/p&gt; 
&lt;h2&gt;üìë Overview&lt;/h2&gt; 
&lt;p&gt;Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Dolphin addresses these challenges through a two-stage approach:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Stage 1&lt;/strong&gt;: Comprehensive page-level layout analysis by generating element sequence in natural reading order&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üß© Stage 2&lt;/strong&gt;: Efficient parallel parsing of document elements using heterogeneous anchors and task-specific prompts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/framework.png" width="680" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Dolphin achieves promising performance across diverse page-level and element-level parsing tasks while ensuring superior efficiency through its lightweight architecture and parallel parsing mechanism.&lt;/p&gt; 
&lt;h2&gt;üöÄ Demo&lt;/h2&gt; 
&lt;p&gt;Try our demo on &lt;a href="https://huggingface.co/spaces/ByteDance/Dolphin"&gt;Demo-Dolphin&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìÖ Changelog&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üî• &lt;strong&gt;2025.07.10&lt;/strong&gt; Released the &lt;em&gt;Fox-Page Benchmark&lt;/em&gt;, a manually refined subset of the original &lt;a href="https://github.com/ucaslcl/Fox"&gt;Fox dataset&lt;/a&gt;. Download via: &lt;a href="https://pan.baidu.com/share/init?surl=t746ULp6iU5bUraVrPlMSw&amp;amp;pwd=fox1"&gt;Baidu Yun&lt;/a&gt; | &lt;a href="https://drive.google.com/file/d/1yZQZqI34QCqvhB4Tmdl3X_XEvYvQyP0q/view?usp=sharing"&gt;Google Drive&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;2025.06.30&lt;/strong&gt; Added &lt;a href="https://github.com/bytedance/Dolphin/raw/master/deployment/tensorrt_llm/ReadMe.md"&gt;TensorRT-LLM support&lt;/a&gt; for accelerated inferenceÔºÅ&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;2025.06.27&lt;/strong&gt; Added &lt;a href="https://github.com/bytedance/Dolphin/raw/master/deployment/vllm/ReadMe.md"&gt;vLLM support&lt;/a&gt; for accelerated inferenceÔºÅ&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;2025.06.13&lt;/strong&gt; Added multi-page PDF document parsing capability.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;2025.05.21&lt;/strong&gt; Our demo is released at &lt;a href="http://115.190.42.15:8888/dolphin/"&gt;link&lt;/a&gt;. Check it out!&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;2025.05.20&lt;/strong&gt; The pretrained model and inference code of Dolphin are released.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;2025.05.16&lt;/strong&gt; Our paper has been accepted by ACL 2025. Paper link: &lt;a href="https://arxiv.org/abs/2505.14059"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üõ†Ô∏è Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ByteDance/Dolphin.git
cd Dolphin
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download the pre-trained models using one of the following options:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A: Original Model Format (config-based)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download from &lt;a href="https://pan.baidu.com/s/15zcARoX0CTOHKbW8bFZovQ?pwd=9rpx"&gt;Baidu Yun&lt;/a&gt; or &lt;a href="https://drive.google.com/drive/folders/1PQJ3UutepXvunizZEw-uGaQ0BCzf-mie?usp=sharing"&gt;Google Drive&lt;/a&gt; and put them in the &lt;code&gt;./checkpoints&lt;/code&gt; folder.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option B: Hugging Face Model Format&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Visit our Huggingface &lt;a href="https://huggingface.co/ByteDance/Dolphin"&gt;model card&lt;/a&gt;, or download model by:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Download the model from Hugging Face Hub
git lfs install
git clone https://huggingface.co/ByteDance/Dolphin ./hf_model
# Or use the Hugging Face CLI
pip install huggingface_hub
huggingface-cli download ByteDance/Dolphin --local-dir ./hf_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;‚ö° Inference&lt;/h2&gt; 
&lt;p&gt;Dolphin provides two inference frameworks with support for two parsing granularities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Page-level Parsing&lt;/strong&gt;: Parse the entire document page into a structured JSON and Markdown format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Element-level Parsing&lt;/strong&gt;: Parse individual document elements (text, table, formula)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÑ Page-level Parsing&lt;/h3&gt; 
&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single document image
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results

# Process a single document pdf
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results

# Process all documents in a directory
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results

# Process with custom batch size for parallel element decoding
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single document image
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results

# Process a single document pdf
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results

# Process all documents in a directory
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results

# Process with custom batch size for parallel element decoding
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üß© Element-level Parsing&lt;/h3&gt; 
&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single table image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/table_1.jpeg --element_type table

# Process a single formula image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula

# Process a single text paragraph image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/para_1.jpg --element_type text
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single table image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/table_1.jpeg --element_type table

# Process a single formula image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula

# Process a single text paragraph image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/para_1.jpg --element_type text
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üåü Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîÑ Two-stage analyze-then-parse approach based on a single VLM&lt;/li&gt; 
 &lt;li&gt;üìä Promising performance on document parsing tasks&lt;/li&gt; 
 &lt;li&gt;üîç Natural reading order element sequence generation&lt;/li&gt; 
 &lt;li&gt;üß© Heterogeneous anchor prompting for different document elements&lt;/li&gt; 
 &lt;li&gt;‚è±Ô∏è Efficient parallel parsing mechanism&lt;/li&gt; 
 &lt;li&gt;ü§ó Support for Hugging Face Transformers for easier integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÆ Notice&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Call for Bad Cases:&lt;/strong&gt; If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model.&lt;/p&gt; 
&lt;h2&gt;üíñ Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We would like to acknowledge the following open-source projects that provided inspiration and reference for this work:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/clovaai/donut/"&gt;Donut&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/nougat"&gt;Nougat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Ucas-HaoranWei/GOT-OCR2.0"&gt;GOT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/MinerU/tree/master"&gt;MinerU&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Swin-Transformer"&gt;Swin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;Hugging Face Transformers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìù Citation&lt;/h2&gt; 
&lt;p&gt;If you find this code useful for your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{feng2025dolphin,
  title={Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting},
  author={Feng, Hao and Wei, Shu and Fei, Xiang and Shi, Wei and Han, Yingdong and Liao, Lei and Lu, Jinghui and Wu, Binghong and Liu, Qi and Lin, Chunhui and others},
  journal={arXiv preprint arXiv:2505.14059},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#bytedance/Dolphin&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=bytedance/Dolphin&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ericciarla/trendFinder</title>
      <link>https://github.com/ericciarla/trendFinder</link>
      <description>&lt;p&gt;Stay on top of trending topics on social media and the web with AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Trend Finder üî¶&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Stay on top of trending topics on social media ‚Äî all in one place.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Trend Finder collects and analyzes posts from key influencers, then sends a Slack or Discord notification when it detects new trends or product launches. This has been a complete game-changer for the Firecrawl marketing team by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Saving time&lt;/strong&gt; normally spent manually searching social channels&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Keeping you informed&lt;/strong&gt; of relevant, real-time conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enabling rapid response&lt;/strong&gt; to new opportunities or emerging industry shifts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Spend less time hunting for trends and more time creating impactful campaigns.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Watch the Demo &amp;amp; Tutorial video&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=puimQSun92g"&gt;&lt;img src="https://i.ytimg.com/vi/puimQSun92g/hqdefault.jpg" alt="Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Learn how to set up Trend Finder and start monitoring trends in this video!&lt;/p&gt; 
&lt;h2&gt;How it Works&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Collection&lt;/strong&gt; üì•&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Monitors selected influencers' posts on Twitter/X using the X API (Warning: the X API free plan is rate limited to only monitor 1 X account every 15 min)&lt;/li&gt; 
   &lt;li&gt;Monitors websites for new releases and news with Firecrawl's /extract&lt;/li&gt; 
   &lt;li&gt;Runs on a scheduled basis using cron jobs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AI Analysis&lt;/strong&gt; üß†&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Processes collected content through Together AI&lt;/li&gt; 
   &lt;li&gt;Identifies emerging trends, releases, and news.&lt;/li&gt; 
   &lt;li&gt;Analyzes sentiment and relevance&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Notification System&lt;/strong&gt; üì¢&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;When significant trends are detected, sends Slack or Discord notifications based on cron job setup&lt;/li&gt; 
   &lt;li&gt;Provides context about the trend and its sources&lt;/li&gt; 
   &lt;li&gt;Enables quick response to emerging opportunities&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü§ñ AI-powered trend analysis using Together AI&lt;/li&gt; 
 &lt;li&gt;üì± Social media monitoring (Twitter/X integration)&lt;/li&gt; 
 &lt;li&gt;üîç Website monitoring with Firecrawl&lt;/li&gt; 
 &lt;li&gt;üí¨ Instant Slack or Discord notifications&lt;/li&gt; 
 &lt;li&gt;‚è±Ô∏è Scheduled monitoring using cron jobs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js (v14 or higher)&lt;/li&gt; 
 &lt;li&gt;npm or yarn&lt;/li&gt; 
 &lt;li&gt;Docker&lt;/li&gt; 
 &lt;li&gt;Docker Compose&lt;/li&gt; 
 &lt;li&gt;Slack workspace with webhook permissions&lt;/li&gt; 
 &lt;li&gt;API keys for required services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Environment Variables&lt;/h2&gt; 
&lt;p&gt;Copy &lt;code&gt;.env.example&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; and configure the following variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Optional: API key from Together AI for trend analysis (https://www.together.ai/)
TOGETHER_API_KEY=your_together_api_key_here

# Optional: API key from DeepSeek for trend analysis (https://deepseek.com/)
DEEPSEEK_API_KEY=

# Optional: API key from OpenAI for trend analysis (https://openai.com/)
OPENAI_API_KEY=

# Required if monitoring web pages (https://www.firecrawl.dev/)
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# Required if monitoring Twitter/X trends (https://developer.x.com/)
X_API_BEARER_TOKEN=your_twitter_api_bearer_token_here

# Notification driver. Supported drivers: "slack", "discord"
NOTIFICATION_DRIVER=discord

# Required (if NOTIFICATION_DRIVER is "slack"): Incoming Webhook URL from Slack for notifications
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Required (if NOTIFICATION_DRIVER is "discord"): Incoming Webhook URL from Discord for notifications
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/WEBHOOK/URL
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone [repository-url]
cd trend-finder
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure environment variables:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
# Edit .env with your configuration
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the application:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Development mode with hot reloading
npm run start

# Build for production
npm run build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using Docker&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Build the Docker image:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t trend-finder .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the Docker container:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:3000 --env-file .env trend-finder
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using Docker Compose&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the application with Docker Compose:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up --build -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stop the application with Docker Compose:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose down
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Project Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;trend-finder/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ controllers/    # Request handlers
‚îÇ   ‚îú‚îÄ‚îÄ services/       # Business logic
‚îÇ   ‚îî‚îÄ‚îÄ index.ts        # Application entry point
‚îú‚îÄ‚îÄ .env.example        # Environment variables template
‚îú‚îÄ‚îÄ package.json        # Dependencies and scripts
‚îî‚îÄ‚îÄ tsconfig.json       # TypeScript configuration
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create your feature branch (&lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Commit your changes (&lt;code&gt;git commit -m 'Add some amazing feature'&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Push to the branch (&lt;code&gt;git push origin feature/amazing-feature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Open a Pull Request&lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>jellyfin/jellyfin</title>
      <link>https://github.com/jellyfin/jellyfin</link>
      <description>&lt;p&gt;The Free Software Media System - Server Backend &amp; API&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Jellyfin&lt;/h1&gt; 
&lt;h3 align="center"&gt;The Free Software Media System&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img alt="Logo Banner" src="https://raw.githubusercontent.com/jellyfin/jellyfin-ux/master/branding/SVG/banner-logo-solid.svg?sanitize=true" /&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://github.com/jellyfin/jellyfin"&gt; &lt;img alt="GPL 2.0 License" src="https://img.shields.io/github/license/jellyfin/jellyfin.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://github.com/jellyfin/jellyfin/releases"&gt; &lt;img alt="Current Release" src="https://img.shields.io/github/release/jellyfin/jellyfin.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://translate.jellyfin.org/projects/jellyfin/jellyfin-core/?utm_source=widget"&gt; &lt;img alt="Translation Status" src="https://translate.jellyfin.org/widgets/jellyfin/-/jellyfin-core/svg-badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://hub.docker.com/r/jellyfin/jellyfin"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/jellyfin/jellyfin.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://opencollective.com/jellyfin"&gt; &lt;img alt="Donate" src="https://img.shields.io/opencollective/all/jellyfin.svg?label=backers" /&gt; &lt;/a&gt; &lt;a href="https://features.jellyfin.org"&gt; &lt;img alt="Submit Feature Requests" src="https://img.shields.io/badge/fider-vote%20on%20features-success.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://matrix.to/#/#jellyfinorg:matrix.org"&gt; &lt;img alt="Chat on Matrix" src="https://img.shields.io/matrix/jellyfinorg:matrix.org.svg?logo=matrix" /&gt; &lt;/a&gt; &lt;a href="https://github.com/jellyfin/jellyfin/releases.atom"&gt; &lt;img alt="Release RSS Feed" src="https://img.shields.io/badge/rss-releases-ffa500?logo=rss" /&gt; &lt;/a&gt; &lt;a href="https://github.com/jellyfin/jellyfin/commits/master.atom"&gt; &lt;img alt="Master Commits RSS Feed" src="https://img.shields.io/badge/rss-commits-ffa500?logo=rss" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET platform to enable full cross-platform support.&lt;/p&gt; 
&lt;p&gt;There are no strings attached, no premium licenses or features, and no hidden agendas: just a team that wants to build something better and work together to achieve it. We welcome anyone who is interested in joining us in our quest!&lt;/p&gt; 
&lt;p&gt;For further details, please see &lt;a href="https://jellyfin.org/docs/"&gt;our documentation page&lt;/a&gt;. To receive the latest updates, get help with Jellyfin, and join the community, please visit &lt;a href="https://jellyfin.org/docs/general/getting-help"&gt;one of our communication channels&lt;/a&gt;. For more information about the project, please see our &lt;a href="https://jellyfin.org/docs/general/about"&gt;about page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;br /&gt; Check out our &lt;a href="https://jellyfin.org/downloads"&gt;downloads page&lt;/a&gt; or our &lt;a href="https://jellyfin.org/docs/general/installation/"&gt;installation guide&lt;/a&gt;, then see our &lt;a href="https://jellyfin.org/docs/general/quick-start"&gt;quick start guide&lt;/a&gt;. You can also &lt;a href="https://jellyfin.org/docs/general/installation/source"&gt;build from source&lt;/a&gt;.&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Something not working right?&lt;/strong&gt;&lt;br /&gt; Open an &lt;a href="https://jellyfin.org/docs/general/contributing/issues"&gt;Issue&lt;/a&gt; on GitHub.&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Want to contribute?&lt;/strong&gt;&lt;br /&gt; Check out our &lt;a href="https://jellyfin.org/contribute"&gt;contributing choose-your-own-adventure&lt;/a&gt; to see where you can help, then see our &lt;a href="https://jellyfin.org/docs/general/contributing/"&gt;contributing guide&lt;/a&gt; and our &lt;a href="https://jellyfin.org/docs/general/community-standards"&gt;community standards&lt;/a&gt;.&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;New idea or improvement?&lt;/strong&gt;&lt;br /&gt; Check out our &lt;a href="https://features.jellyfin.org/?view=most-wanted"&gt;feature request hub&lt;/a&gt;.&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Don't see Jellyfin in your language?&lt;/strong&gt;&lt;br /&gt; Check out our &lt;a href="https://translate.jellyfin.org"&gt;Weblate instance&lt;/a&gt; to help translate Jellyfin and its subprojects.&lt;br /&gt;&lt;/p&gt; 
&lt;a href="https://translate.jellyfin.org/engage/jellyfin/?utm_source=widget"&gt; &lt;img src="https://translate.jellyfin.org/widgets/jellyfin/-/jellyfin-web/multi-auto.svg?sanitize=true" alt="Detailed Translation Status" /&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Jellyfin Server&lt;/h2&gt; 
&lt;p&gt;This repository contains the code for Jellyfin's backend server. Note that this is only one of many projects under the Jellyfin GitHub &lt;a href="https://github.com/jellyfin/"&gt;organization&lt;/a&gt; on GitHub. If you want to contribute, you can start by checking out our &lt;a href="https://jellyfin.org/docs/general/contributing/index.html"&gt;documentation&lt;/a&gt; to see what to work on.&lt;/p&gt; 
&lt;h2&gt;Server Development&lt;/h2&gt; 
&lt;p&gt;These instructions will help you get set up with a local development environment in order to contribute to this repository. Before you start, please be sure to completely read our &lt;a href="https://jellyfin.org/docs/general/contributing/development.html"&gt;guidelines on development contributions&lt;/a&gt;. Note that this project is supported on all major operating systems except FreeBSD, which is still incompatible.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;Before the project can be built, you must first install the &lt;a href="https://dotnet.microsoft.com/download/dotnet"&gt;.NET 9.0 SDK&lt;/a&gt; on your system.&lt;/p&gt; 
&lt;p&gt;Instructions to run this project from the command line are included here, but you will also need to install an IDE if you want to debug the server while it is running. Any IDE that supports .NET 6 development will work, but two options are recent versions of &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio&lt;/a&gt; (at least 2022) and &lt;a href="https://code.visualstudio.com/Download"&gt;Visual Studio Code&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/jellyfin/jellyfin-ffmpeg"&gt;ffmpeg&lt;/a&gt; will also need to be installed.&lt;/p&gt; 
&lt;h3&gt;Cloning the Repository&lt;/h3&gt; 
&lt;p&gt;After dependencies have been installed you will need to clone a local copy of this repository. If you just want to run the server from source you can clone this repository directly, but if you are intending to contribute code changes to the project, you should &lt;a href="https://jellyfin.org/docs/general/contributing/development.html#set-up-your-copy-of-the-repo"&gt;set up your own fork&lt;/a&gt; of the repository. The following example shows how you can clone the repository directly over HTTPS.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/jellyfin/jellyfin.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing the Web Client&lt;/h3&gt; 
&lt;p&gt;The server is configured to host the static files required for the &lt;a href="https://github.com/jellyfin/jellyfin-web"&gt;web client&lt;/a&gt; in addition to serving the backend by default. Before you can run the server, you will need to get a copy of the web client since they are not included in this repository directly.&lt;/p&gt; 
&lt;p&gt;Note that it is also possible to &lt;a href="https://raw.githubusercontent.com/jellyfin/jellyfin/master/#hosting-the-web-client-separately"&gt;host the web client separately&lt;/a&gt; from the web server with some additional configuration, in which case you can skip this step.&lt;/p&gt; 
&lt;p&gt;There are three options to get the files for the web client.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download one of the finished builds from the &lt;a href="https://dev.azure.com/jellyfin-project/jellyfin/_build?definitionId=27"&gt;Azure DevOps pipeline&lt;/a&gt;. You can download the build for a specific release by looking at the &lt;a href="https://dev.azure.com/jellyfin-project/jellyfin/_build?definitionId=27&amp;amp;_a=summary&amp;amp;repositoryFilter=6&amp;amp;view=branches"&gt;branches tab&lt;/a&gt; of the pipelines page.&lt;/li&gt; 
 &lt;li&gt;Build them from source following the instructions on the &lt;a href="https://github.com/jellyfin/jellyfin-web"&gt;jellyfin-web repository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Get the pre-built files from an existing installation of the server. For example, with a Windows server installation the client files are located at &lt;code&gt;C:\Program Files\Jellyfin\Server\jellyfin-web&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Running The Server&lt;/h3&gt; 
&lt;p&gt;The following instructions will help you get the project up and running via the command line, or your preferred IDE.&lt;/p&gt; 
&lt;h4&gt;Running With Visual Studio&lt;/h4&gt; 
&lt;p&gt;To run the project with Visual Studio you can open the Solution (&lt;code&gt;.sln&lt;/code&gt;) file and then press &lt;code&gt;F5&lt;/code&gt; to run the server.&lt;/p&gt; 
&lt;h4&gt;Running With Visual Studio Code&lt;/h4&gt; 
&lt;p&gt;To run the project with Visual Studio Code you will first need to open the repository directory with Visual Studio Code using the &lt;code&gt;Open Folder...&lt;/code&gt; option.&lt;/p&gt; 
&lt;p&gt;Second, you need to &lt;a href="https://code.visualstudio.com/docs/editor/extension-gallery#_recommended-extensions"&gt;install the recommended extensions for the workspace&lt;/a&gt;. Note that extension recommendations are classified as either "Workspace Recommendations" or "Other Recommendations", but only the "Workspace Recommendations" are required.&lt;/p&gt; 
&lt;p&gt;After the required extensions are installed, you can run the server by pressing &lt;code&gt;F5&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Running From the Command Line&lt;/h4&gt; 
&lt;p&gt;To run the server from the command line you can use the &lt;code&gt;dotnet run&lt;/code&gt; command. The example below shows how to do this if you have cloned the repository into a directory named &lt;code&gt;jellyfin&lt;/code&gt; (the default directory name) and should work on all operating systems.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd jellyfin                          # Move into the repository directory
dotnet run --project Jellyfin.Server --webdir /absolute/path/to/jellyfin-web/dist # Run the server startup project
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A second option is to build the project and then run the resulting executable file directly. When running the executable directly you can easily add command line options. Add the &lt;code&gt;--help&lt;/code&gt; flag to list details on all the supported command line options.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet build                       # Build the project
cd Jellyfin.Server/bin/Debug/net9.0 # Change into the build output directory
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Execute the build output. On Linux, Mac, etc. use &lt;code&gt;./jellyfin&lt;/code&gt; and on Windows use &lt;code&gt;jellyfin.exe&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Accessing the Hosted Web Client&lt;/h4&gt; 
&lt;p&gt;If the Server is configured to host the Web Client, and the Server is running, the Web Client can be accessed at &lt;code&gt;http://localhost:8096&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;API documentation can be viewed at &lt;code&gt;http://localhost:8096/api-docs/swagger/index.html&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Running from GitHub Codespaces&lt;/h3&gt; 
&lt;p&gt;As Jellyfin will run on a container on a GitHub hosted server, JF needs to handle some things differently.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Depending on the selected configuration (if you just click 'create codespace' it will create a default configuration one) it might take 20-30 seconds to load all extensions and prepare the environment while VS Code is already open. Just give it some time and wait until you see &lt;code&gt;Downloading .NET version(s) 7.0.15~x64 ...... Done!&lt;/code&gt; in the output tab.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you want to access the JF instance from outside, like with a WebClient on another PC, remember to set the "ports" in the lower VS Code window to public.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; When first opening the server instance with any WebUI, you will be sent to the login instead of the setup page. Refresh the login page once and you should be redirected to the Setup.&lt;/p&gt; 
&lt;p&gt;There are two configurations for you to choose from.&lt;/p&gt; 
&lt;h4&gt;Default - Development Jellyfin Server&lt;/h4&gt; 
&lt;p&gt;This creates a container that has everything to run and debug the Jellyfin Media server but does not setup anything else. Each time you create a new container you have to run through the whole setup again. There is also no ffmpeg, webclient or media preloaded. Use the &lt;code&gt;.NET Launch (nowebclient)&lt;/code&gt; launch config to start the server.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Keep in mind that as this has no web client you have to connect to it via an external client. This can be just another codespace container running the WebUI. vuejs does not work from the get-go as it does not support the setup steps.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Development Jellyfin Server ffmpeg&lt;/h4&gt; 
&lt;p&gt;this extends the default server with a default installation of ffmpeg6 though the means described here: &lt;a href="https://jellyfin.org/docs/general/installation/linux#repository-manual"&gt;https://jellyfin.org/docs/general/installation/linux#repository-manual&lt;/a&gt; If you want to install a specific ffmpeg version, follow the comments embedded in the &lt;code&gt;.devcontainer/Dev - Server Ffmpeg/install.ffmpeg.sh&lt;/code&gt; file.&lt;/p&gt; 
&lt;p&gt;Use the &lt;code&gt;ghcs .NET Launch (nowebclient, ffmpeg)&lt;/code&gt; launch config to run with the jellyfin-ffmpeg enabled.&lt;/p&gt; 
&lt;h3&gt;Running The Tests&lt;/h3&gt; 
&lt;p&gt;This repository also includes unit tests that are used to validate functionality as part of a CI pipeline on Azure. There are several ways to run these tests.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run tests from the command line using &lt;code&gt;dotnet test&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run tests in Visual Studio using the &lt;a href="https://docs.microsoft.com/en-us/visualstudio/test/run-unit-tests-with-test-explorer"&gt;Test Explorer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Run individual tests in Visual Studio Code using the associated &lt;a href="https://github.com/OmniSharp/omnisharp-vscode/wiki/How-to-run-and-debug-unit-tests"&gt;CodeLens annotation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Advanced Configuration&lt;/h3&gt; 
&lt;p&gt;The following sections describe some more advanced scenarios for running the server from source that build upon the standard instructions above.&lt;/p&gt; 
&lt;h4&gt;Hosting The Web Client Separately&lt;/h4&gt; 
&lt;p&gt;It is not necessary to host the frontend web client as part of the backend server. Hosting these two components separately may be useful for frontend developers who would prefer to host the client in a separate webpack development server for a tighter development loop. See the &lt;a href="https://github.com/jellyfin/jellyfin-web#getting-started"&gt;jellyfin-web&lt;/a&gt; repo for instructions on how to do this.&lt;/p&gt; 
&lt;p&gt;To instruct the server not to host the web content, there is a &lt;code&gt;nowebclient&lt;/code&gt; configuration flag that must be set. This can be specified using the command line switch &lt;code&gt;--nowebclient&lt;/code&gt; or the environment variable &lt;code&gt;JELLYFIN_NOWEBCONTENT=true&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Since this is a common scenario, there is also a separate launch profile defined for Visual Studio called &lt;code&gt;Jellyfin.Server (nowebcontent)&lt;/code&gt; that can be selected from the 'Start Debugging' dropdown in the main toolbar.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; The setup wizard cannot be run if the web client is hosted separately.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; This project is supported by: &lt;br /&gt; &lt;br /&gt; &lt;a href="https://www.digitalocean.com"&gt;&lt;img src="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg?sanitize=true" height="50px" alt="DigitalOcean" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://www.jetbrains.com"&gt;&lt;img src="https://gist.githubusercontent.com/anthonylavado/e8b2403deee9581e0b4cb8cd675af7db/raw/fa104b7d73f759d7262794b94569f1b89df41c0b/jetbrains.svg?sanitize=true" height="50px" alt="JetBrains logo" /&gt;&lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ZuodaoTech/everyone-can-use-english</title>
      <link>https://github.com/ZuodaoTech/everyone-can-use-english</link>
      <description>&lt;p&gt;‰∫∫‰∫∫ÈÉΩËÉΩÁî®Ëã±ËØ≠&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/assets/icon.png" alt="Clash" width="128" /&gt; 
&lt;/div&gt; 
&lt;h3 align="center"&gt; AI ÊòØÂΩì‰ªä‰∏ñÁïå‰∏äÊúÄÂ•ΩÁöÑÂ§ñËØ≠ËÄÅÂ∏àÔºåEnjoy ÂÅö AI ÊúÄÂ•ΩÁöÑÂä©Êïô„ÄÇ &lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/deploy-1000h.yml"&gt;&lt;img src="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/deploy-1000h.yml/badge.svg?sanitize=true" alt="Deploy 1000h website" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/test-enjoy-app.yml"&gt;&lt;img src="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/test-enjoy-app.yml/badge.svg?sanitize=true" alt="Test Enjoy App" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/release-enjoy-app.yml"&gt;&lt;img src="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/release-enjoy-app.yml/badge.svg?sanitize=true" alt="Release Enjoy App" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fenjoy.bot%2Fapi%2Fconfig%2Fapp_version&amp;amp;query=%24.version&amp;amp;label=Latest&amp;amp;link=https%3A%2F%2F1000h.org%2Fenjoy-app%2Finstall.html" alt="Latest Version" /&gt; &lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fenjoy.bot%2Fapi%2Fbadges%2Frecordings" alt="Recording Duration" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ÁΩëÈ°µÁâà&lt;/h2&gt; 
&lt;p&gt;Enjoy ÁΩëÈ°µÁâàÂ∑≤Áªè‰∏äÁ∫øÔºåÂèØËÆøÈóÆ &lt;a href="https://enjoy.bot"&gt;https://enjoy.bot&lt;/a&gt; Áõ¥Êé•‰ΩøÁî®„ÄÇ&lt;/p&gt; 
&lt;div align="center" style="display:flex;overflow:auto;gap:10px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-audios.jpg" alt="Audios" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-add-audio.jpg" alt="Add Audio" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-audio-shadow.jpg" alt="Shadow" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-audio-assessment.jpg" alt="Assessment" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-new-chat.jpg" alt="New Chat" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-chat.jpg" alt="Chat" width="300" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ê°åÈù¢ÁâàÂÆâË£ÖÂèä‰ΩøÁî®&lt;/h2&gt; 
&lt;p&gt;‰∏ãËΩΩÂèä‰ΩøÁî®Áõ∏ÂÖ≥ËØ¥ÊòéÔºåËØ∑ÂèÇÈòÖ &lt;a href="https://1000h.org/enjoy-app/"&gt;ÊñáÊ°£&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h2&gt;È¢ÑËßà&lt;/h2&gt; 
&lt;div align="center" style="display:flex;overflow:auto;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/home.png" alt="Home" width="800" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/shadow.png" alt="Home" width="800" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/assessment.png" alt="Home" width="800" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/document.png" alt="Home" width="800" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/chat.png" alt="Home" width="800" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Ê°åÈù¢ÁâàÂºÄÂèë&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;yarn install
yarn enjoy:start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Áõ∏ÂÖ≥ÈòÖËØª&lt;/h2&gt; 
&lt;h3&gt;‰∏ÄÂçÉÂ∞èÊó∂Ôºà2024Ôºâ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/intro.html"&gt;ÁÆÄË¶ÅËØ¥Êòé&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/training-tasks/kick-off.html"&gt;ËÆ≠ÁªÉ‰ªªÂä°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/sounds-of-american-english/0-intro.html"&gt;ËØ≠Èü≥Â°ëÈÄ†&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/in-the-brain/01-inifinite.html"&gt;Â§ßËÑëÂÜÖÈÉ®&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/self-training/00-intro.html"&gt;Ëá™ÊàëËÆ≠ÁªÉ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‰∫∫‰∫∫ÈÉΩËÉΩÁî®Ëã±ËØ≠Ôºà2010Ôºâ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/README.md"&gt;ÁÆÄ‰ªã&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter1.md"&gt;Á¨¨‰∏ÄÁ´†ÔºöËµ∑ÁÇπ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter2.md"&gt;Á¨¨‰∫åÁ´†ÔºöÂè£ËØ≠&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter3.md"&gt;Á¨¨‰∏âÁ´†ÔºöËØ≠Èü≥&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter4.md"&gt;Á¨¨ÂõõÁ´†ÔºöÊúóËØª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter5.md"&gt;Á¨¨‰∫îÁ´†ÔºöËØçÂÖ∏&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter6.md"&gt;Á¨¨ÂÖ≠Á´†ÔºöËØ≠Ê≥ï&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter7.md"&gt;Á¨¨‰∏ÉÁ´†ÔºöÁ≤æËØª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter8.md"&gt;Á¨¨ÂÖ´Á´†ÔºöÂèÆÂò±&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/end.md"&gt;ÂêéËÆ∞&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Â∏∏ËßÅÈóÆÈ¢ò&lt;/h2&gt; 
&lt;p&gt;ËØ∑Êü•ËØ¢ &lt;a href="https://1000h.org/enjoy-app/faq.html"&gt;ÊñáÊ°£ FAQ&lt;/a&gt;„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://discord.gg/EUnjGpsmWw"&gt;Discord&lt;/a&gt; | &lt;a href="https://t.me/+Kh-KqHTzFYg3MGNk"&gt;Telegram&lt;/a&gt; | &lt;a href="https://x.com/exolabs"&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main"&gt;&lt;img src="https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License: GPL v3" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Update: exo is hiring. See &lt;a href="https://exolabs.net"&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; 
 &lt;h2&gt;Interested in running exo in your business? &lt;a href="mailto:hello@exolabs.net"&gt;Contact us&lt;/a&gt; to discuss.&lt;/h2&gt; 
&lt;/div&gt; 
&lt;h2&gt;Get Involved&lt;/h2&gt; 
&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; 
&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href="https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing"&gt;this sheet&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Wide Model Support&lt;/h3&gt; 
&lt;p&gt;exo supports different models including LLaMA (&lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py"&gt;MLX&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py"&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen, and Deepseek.&lt;/p&gt; 
&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; 
&lt;p&gt;exo &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; 
&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; 
&lt;p&gt;exo will &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154"&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; 
&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; 
&lt;p&gt;exo provides a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py"&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It's a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh"&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; 
&lt;h3&gt;Device Equality&lt;/h3&gt; 
&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161"&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; 
&lt;p&gt;Exo supports different &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py"&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-screenshot.jpg" alt="&amp;quot;A screenshot of exo running 5 nodes" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href="https://github.com/exo-explore/exo/issues/5"&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; 
 &lt;li&gt;For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA): 
  &lt;ul&gt; 
   &lt;li&gt;NVIDIA driver - verify with &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;CUDA toolkit - install from &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation"&gt;NVIDIA CUDA guide&lt;/a&gt;, verify with &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cuDNN library - download from &lt;a href="https://developer.nvidia.com/cudnn-downloads"&gt;NVIDIA cuDNN page&lt;/a&gt;, verify installation by following &lt;a href="https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older"&gt;these steps&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: 
  &lt;ul&gt; 
   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; 
   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; 
   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/exo-explore/exo.git
cd exo
pip install -e .
# alternatively, with venv
source install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If running on Mac, MLX has an &lt;a href="https://ml-explore.github.io/mlx/build/html/install.html"&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upgrade to the latest version of macOS Sequoia.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Example Usage on Multiple macOS Devices&lt;/h3&gt; 
&lt;h4&gt;Device 1:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Device 2:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; 
&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href="https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat"&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href="http://localhost:52415"&gt;http://localhost:52415&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href="http://localhost:52415/v1/chat/completions"&gt;http://localhost:52415/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; 
&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.2-3b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.1-405b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;DeepSeek R1 (full 671B):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "deepseek-r1",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llava-1.5-7b-hf",
     "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What are these?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "http://images.cocodataset.org/val2017/000000039769.jpg"
            }
          }
        ]
      }
    ],
     "temperature": 0.0
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (macOS + Linux)&lt;/h3&gt; 
&lt;h4&gt;Device 1 (macOS):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: We don't need to explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine. &lt;strong&gt;MLX&lt;/strong&gt; and &lt;strong&gt;tinygrad&lt;/strong&gt; are interoperable!&lt;/p&gt; 
&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; 
&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href="https://docs.tinygrad.org/env_vars/"&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage on a single device with "exo run" command&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With a custom prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b --prompt "What is the meaning of exo?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Storage&lt;/h3&gt; 
&lt;p&gt;Models by default are stored in &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can set a different model storage location by setting the &lt;code&gt;EXO_HOME&lt;/code&gt; env var.&lt;/p&gt; 
&lt;h2&gt;Model Downloading&lt;/h2&gt; 
&lt;p&gt;Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;To download models from a proxy endpoint, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable. For example, to run exo with the huggingface mirror endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;HF_ENDPOINT=https://hf-mirror.com exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;DEBUG=9 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;TINYGRAD_DEBUG=2 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Formatting&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; to format the code. To format the code, first install the formatting requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install -e '.[formatting]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the formatting script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python3 format.py ./exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the &lt;code&gt;Install Certificates&lt;/code&gt; command, typicall as follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;/Applications/Python 3.x/Install Certificates.command
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöß As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email &lt;a href="mailto:alex@exolabs.net"&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Inference Engines&lt;/h2&gt; 
&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py"&gt;MLX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py"&gt;tinygrad&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß &lt;a href="https://github.com/exo-explore/exo/pull/139"&gt;PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß &lt;a href="https://github.com/exo-explore/exo/issues/167"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discovery Modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/udp"&gt;UDP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/manual"&gt;Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/tailscale"&gt;Tailscale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß Radio&lt;/li&gt; 
 &lt;li&gt;üöß Bluetooth&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Peer Networking Modules&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc"&gt;GRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß NCCL&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>humanlayer/humanlayer</title>
      <link>https://github.com/humanlayer/humanlayer</link>
      <description>&lt;p&gt;The best way to get AI to solve hard problems in complex codebases.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/humanlayer/humanlayer/main/docs/images/wordmark-light.svg?sanitize=true" alt="Wordmark Logo of HumanLayer" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;Close your editor forever.&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;CodeLayer is an open source IDE that lets you orchestrate AI coding agents.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;It comes with battle-tested workflows that enable AI to solve hard problems in large, complex codebases.&lt;/p&gt; 
 &lt;p&gt;Built on Claude Code. Open source. Scale from your laptop to your entire team.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/humanlayer/humanlayer"&gt;&lt;img src="https://img.shields.io/github/stars/humanlayer/humanlayer" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2"&gt;&lt;img src="https://img.shields.io/badge/License-Apache-green.svg?sanitize=true" alt="License: Apache-2" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://humanlayer.dev/code"&gt;Join Waitlist&lt;/a&gt; | &lt;a href="https://humanlayer.dev/discord"&gt;Discord&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;img referrerpolicy="no-referrer-when-downgrade" src="https://static.scarf.sh/a.png?x-pxid=fcfc0926-d841-47fb-b8a6-6aba3a6c3228" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"Our entire company is using CodeLayer now. We're shipping one banger PR after the other. It is so f-ing good. Unbelievable dude."&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;‚Äì Ren√© Brandel, Founder @ Casco (YC X25)&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Superhuman for Claude Code&lt;/h2&gt; 
&lt;p&gt;Keyboard-first workflows designed for builders who value speed and control.&lt;/p&gt; 
&lt;h2&gt;Advanced Context Engineering&lt;/h2&gt; 
&lt;p&gt;Scale AI-first dev to your entire team, without devolving into a chaotic slop-fest.&lt;/p&gt; 
&lt;h2&gt;M U L T I C L A U D E&lt;/h2&gt; 
&lt;p&gt;Run Claude Code sessions in parallel. Worktrees? Done. Remote cloud workers? You got it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"This has improved my productivity (and token consumption) by at least 50%. Taking a superhuman style approach just makes soo much sense. Also, its so freaking cool to look back at all the work you've done in a day."&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;‚Äì Tyler Brown, Founder @ Revlo.ai&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;From the team that brought you "Context Engineering"&lt;/h2&gt; 
&lt;p&gt;Leading experts on getting the most out of today's models.&lt;/p&gt; 
&lt;h3&gt;üìö Resources&lt;/h3&gt; 
&lt;h4&gt;&lt;a href="https://github.com/humanlayer/humanlayer"&gt;Advanced Context Engineering for Coding Agents&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;This talk, given at YC on August 20th, 2025 lays out the groundwork for using AI to solve hard problems in complex codebases.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/humanlayer/humanlayer"&gt;GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://humanlayer.dev/youtube"&gt;YouTube&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://github.com/humanlayer/humanlayer"&gt;12 Factor Agents&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;A set of principles for building reliable and scalable LLM applications, inspired by the original 12-Factor App methodology.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/humanlayer/humanlayer"&gt;GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://humanlayer.dev/youtube"&gt;YouTube&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The original repo that coined the term "context engineering" back in April 2025.&lt;/p&gt; 
&lt;h4&gt;&lt;a href="https://humanlayer.dev/podcast"&gt;ü¶Ñ AI That Works&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;A weekly conversation about how we can all get the most juice out of todays models with @hellovai &amp;amp; @dexhorthy&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/humanlayer/humanlayer"&gt;GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://humanlayer.dev/podcast"&gt;Podcast&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;For Teams&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Invest in outcomes, not tools.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Want to scale AI-first development to your entire org? Get tailored workflows, custom integrations, and cutting-edge advice.&lt;/p&gt; 
&lt;p&gt;HumanLayer's expert engineers will ship in the trenches with you and your team until everyone is a 100x engineer.&lt;/p&gt; 
&lt;p&gt;üìß Shoot us an email at &lt;strong&gt;&lt;a href="mailto:contact@humanlayer.dev"&gt;contact@humanlayer.dev&lt;/a&gt;&lt;/strong&gt;, mention your team size and current AI development stack.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Coming soon - join the waitlist for early access
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Legacy Documentation&lt;/h2&gt; 
&lt;p&gt;Looking for the HumanLayer SDK documentation? See &lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/humanlayer.md"&gt;humanlayer.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;CodeLayer and the HumanLayer SDK are open-source and we welcome contributions in the form of issues, documentation, pull requests, and more. See &lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The HumanLayer SDK and CodeLayer sources in this repo are licensed under the Apache 2 License.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#humanlayer/humanlayer&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=humanlayer/humanlayer&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>onyx-dot-app/onyx</title>
      <link>https://github.com/onyx-dot-app/onyx</link>
      <description>&lt;p&gt;Open Source AI Platform - AI Chat with advanced features that works with every LLM&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2 align="center"&gt; &lt;a href="https://www.onyx.app/"&gt; &lt;img width="50%" src="https://github.com/onyx-dot-app/onyx/raw/logo/OnyxLogoCropped.jpg?raw=true)" /&gt;&lt;/a&gt; &lt;/h2&gt; 
&lt;p align="center"&gt;Open Source AI Platform&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://discord.gg/TDJ59cGV2X" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/discord-join-blue.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt; &lt;/a&gt; &lt;a href="https://docs.onyx.app/" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/docs-view-blue" alt="Documentation" /&gt; &lt;/a&gt; &lt;a href="https://docs.onyx.app/" target="_blank"&gt; &lt;img src="https://img.shields.io/website?url=https://www.onyx.app&amp;amp;up_message=visit&amp;amp;up_color=blue" alt="Documentation" /&gt; &lt;/a&gt; &lt;a href="https://github.com/onyx-dot-app/onyx/raw/main/LICENSE" target="_blank"&gt; &lt;img src="https://img.shields.io/static/v1?label=license&amp;amp;message=MIT&amp;amp;color=blue" alt="License" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.onyx.app/"&gt;Onyx&lt;/a&gt;&lt;/strong&gt; is a feature-rich, self-hostable Chat UI that works with any LLM. It is easy to deploy and can run in a completely airgapped environment.&lt;/p&gt; 
&lt;p&gt;Onyx comes loaded with advanced features like Agents, Web Search, RAG, MCP, Deep Research, Connectors to 40+ knowledge sources, and more.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Run Onyx with one command (or see deployment section below):&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;curl -fsSL https://raw.githubusercontent.com/onyx-dot-app/onyx/main/deployment/docker_compose/install.sh &amp;gt; install.sh &amp;amp;&amp;amp; chmod +x install.sh &amp;amp;&amp;amp; ./install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;img src="https://github.com/onyx-dot-app/onyx/releases/download/v0.21.1/OnyxChatSilentDemo.gif" alt="Onyx Chat Silent Demo" /&gt;&lt;/p&gt; 
&lt;h2&gt;‚≠ê Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Custom Agents:&lt;/strong&gt; Build AI Agents with unique instructions, knowledge and actions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üåç Web Search:&lt;/strong&gt; Browse the web with Google PSE, Exa, and Serper as well as an in-house scraper or Firecrawl.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç RAG:&lt;/strong&gt; Best in class hybrid-search + knowledge graph for uploaded files and ingested documents from connectors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ Connectors:&lt;/strong&gt; Pull knowledge, metadata, and access information from over 40 applications.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üî¨ Deep Research:&lt;/strong&gt; Get in depth answers with an agentic multi-step search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ñ∂Ô∏è Actions &amp;amp; MCP:&lt;/strong&gt; Give AI Agents the ability to interact with external systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíª Code Interpreter:&lt;/strong&gt; Execute code to analyze data, render graphs and create files.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üé® Image Generation:&lt;/strong&gt; Generate images based on user prompts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üë• Collaboration:&lt;/strong&gt; Chat sharing, feedback gathering, user management, usage analytics, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Onyx works with all LLMs (like OpenAI, Anthropic, Gemini, etc.) and self-hosted LLMs (like Ollama, vLLM, etc.)&lt;/p&gt; 
&lt;p&gt;To learn more about the features, check out our &lt;a href="https://docs.onyx.app/welcome"&gt;documentation&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;üöÄ Deployment&lt;/h2&gt; 
&lt;p&gt;Onyx supports deployments in Docker, Kubernetes, Terraform, along with guides for major cloud providers.&lt;/p&gt; 
&lt;p&gt;See guides below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.onyx.app/deployment/local/docker"&gt;Docker&lt;/a&gt; or &lt;a href="https://docs.onyx.app/deployment/getting_started/quickstart"&gt;Quickstart&lt;/a&gt; (best for most users)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.onyx.app/deployment/local/kubernetes"&gt;Kubernetes&lt;/a&gt; (best for large teams)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.onyx.app/deployment/local/terraform"&gt;Terraform&lt;/a&gt; (best for teams already using Terraform)&lt;/li&gt; 
 &lt;li&gt;Cloud specific guides (best if specifically using &lt;a href="https://docs.onyx.app/deployment/cloud/aws/eks"&gt;AWS EKS&lt;/a&gt;, &lt;a href="https://docs.onyx.app/deployment/cloud/azure"&gt;Azure VMs&lt;/a&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;br /&gt; &lt;strong&gt;To try Onyx for free without deploying, check out &lt;a href="https://cloud.onyx.app/signup"&gt;Onyx Cloud&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üîç Other Notable Benefits&lt;/h2&gt; 
&lt;p&gt;Onyx is built for teams of all sizes, from individual users to the largest global enterprises.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise Search&lt;/strong&gt;: far more than simple RAG, Onyx has custom indexing and retrieval that remains performant and accurate for scales of up to tens of millions of documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: SSO (OIDC/SAML/OAuth2), RBAC, encryption of credentials, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Management UI&lt;/strong&gt;: different user roles such as basic, curator, and admin.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Permissioning&lt;/strong&gt;: mirrors user access from external apps for RAG use cases.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöß Roadmap&lt;/h2&gt; 
&lt;p&gt;To see ongoing and upcoming projects, check out our &lt;a href="https://github.com/orgs/onyx-dot-app/projects/2"&gt;roadmap&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;üìö Licensing&lt;/h2&gt; 
&lt;p&gt;There are two editions of Onyx:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Onyx Community Edition (CE) is available freely under the MIT license.&lt;/li&gt; 
 &lt;li&gt;Onyx Enterprise Edition (EE) includes extra features that are primarily useful for larger organizations. For feature details, check out &lt;a href="https://www.onyx.app/pricing"&gt;our website&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üë™ Community&lt;/h2&gt; 
&lt;p&gt;Join our open source community on &lt;strong&gt;&lt;a href="https://discord.gg/TDJ59cGV2X"&gt;Discord&lt;/a&gt;&lt;/strong&gt;!&lt;/p&gt; 
&lt;h2&gt;üí° Contributing&lt;/h2&gt; 
&lt;p&gt;Looking to contribute? Please check out the &lt;a href="https://raw.githubusercontent.com/onyx-dot-app/onyx/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-gemini/gemini-cli</title>
      <link>https://github.com/google-gemini/gemini-cli</link>
      <description>&lt;p&gt;An open-source AI agent that brings the power of Gemini directly into your terminal.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gemini CLI&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="Gemini CLI CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google-gemini/gemini-cli/actions/workflows/e2e.yml"&gt;&lt;img src="https://github.com/google-gemini/gemini-cli/actions/workflows/e2e.yml/badge.svg?sanitize=true" alt="Gemini CLI E2E" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/@google/gemini-cli"&gt;&lt;img src="https://img.shields.io/npm/v/@google/gemini-cli" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google-gemini/gemini-cli/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/google-gemini/gemini-cli" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/assets/gemini-screenshot.png" alt="Gemini CLI Screenshot" /&gt;&lt;/p&gt; 
&lt;p&gt;Gemini CLI is an open-source AI agent that brings the power of Gemini directly into your terminal. It provides lightweight access to Gemini, giving you the most direct path from your prompt to our model.&lt;/p&gt; 
&lt;h2&gt;üöÄ Why Gemini CLI?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ Free tier&lt;/strong&gt;: 60 requests/min and 1,000 requests/day with personal Google account&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üß† Powerful Gemini 2.5 Pro&lt;/strong&gt;: Access to 1M token context window&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Built-in tools&lt;/strong&gt;: Google Search grounding, file operations, shell commands, web fetching&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîå Extensible&lt;/strong&gt;: MCP (Model Context Protocol) support for custom integrations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíª Terminal-first&lt;/strong&gt;: Designed for developers who live in the command line&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üõ°Ô∏è Open source&lt;/strong&gt;: Apache 2.0 licensed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì¶ Installation&lt;/h2&gt; 
&lt;h3&gt;Quick Install&lt;/h3&gt; 
&lt;h4&gt;Run instantly with npx&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using npx (no installation required)
npx https://github.com/google-gemini/gemini-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install globally with npm&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @google/gemini-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install globally with Homebrew (macOS/Linux)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install gemini-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;System Requirements&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js version 20 or higher&lt;/li&gt; 
 &lt;li&gt;macOS, Linux, or Windows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Release Cadence and Tags&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/releases.md"&gt;Releases&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Preview&lt;/h3&gt; 
&lt;p&gt;New preview releases will be published each week at UTC 2359 on Tuesdays. These releases will not have been fully vetted and may contain regressions or other outstanding issues. Please help us test and install with &lt;code&gt;preview&lt;/code&gt; tag.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @google/gemini-cli@preview
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Stable&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;New stable releases will be published each week at UTC 2000 on Tuesdays, this will be the full promotion of last week's &lt;code&gt;preview&lt;/code&gt; release + any bug fixes and validations. Use &lt;code&gt;latest&lt;/code&gt; tag.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @google/gemini-cli@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Nightly&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;New releases will be published each week at UTC 0000 each day, This will be all changes from the main branch as represented at time of release. It should be assumed there are pending validations and issues. Use &lt;code&gt;nightly&lt;/code&gt; tag.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @google/gemini-cli@nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìã Key Features&lt;/h2&gt; 
&lt;h3&gt;Code Understanding &amp;amp; Generation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Query and edit large codebases&lt;/li&gt; 
 &lt;li&gt;Generate new apps from PDFs, images, or sketches using multimodal capabilities&lt;/li&gt; 
 &lt;li&gt;Debug issues and troubleshoot with natural language&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Automation &amp;amp; Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automate operational tasks like querying pull requests or handling complex rebases&lt;/li&gt; 
 &lt;li&gt;Use MCP servers to connect new capabilities, including &lt;a href="https://github.com/GoogleCloudPlatform/vertex-ai-creative-studio/tree/main/experiments/mcp-genmedia"&gt;media generation with Imagen, Veo or Lyria&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Run non-interactively in scripts for workflow automation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ground your queries with built-in &lt;a href="https://ai.google.dev/gemini-api/docs/grounding"&gt;Google Search&lt;/a&gt; for real-time information&lt;/li&gt; 
 &lt;li&gt;Conversation checkpointing to save and resume complex sessions&lt;/li&gt; 
 &lt;li&gt;Custom context files (GEMINI.md) to tailor behavior for your projects&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;GitHub Integration&lt;/h3&gt; 
&lt;p&gt;Integrate Gemini CLI directly into your GitHub workflows with &lt;a href="https://github.com/google-github-actions/run-gemini-cli"&gt;&lt;strong&gt;Gemini CLI GitHub Action&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pull Request Reviews&lt;/strong&gt;: Automated code review with contextual feedback and suggestions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Issue Triage&lt;/strong&gt;: Automated labeling and prioritization of GitHub issues based on content analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-demand Assistance&lt;/strong&gt;: Mention &lt;code&gt;@gemini-cli&lt;/code&gt; in issues and pull requests for help with debugging, explanations, or task delegation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Workflows&lt;/strong&gt;: Build automated, scheduled and on-demand workflows tailored to your team's needs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîê Authentication Options&lt;/h2&gt; 
&lt;p&gt;Choose the authentication method that best fits your needs:&lt;/p&gt; 
&lt;h3&gt;Option 1: Login with Google (OAuth login using your Google Account)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;‚ú® Best for:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Individual developers.&lt;/li&gt; 
 &lt;li&gt;Google AI Pro and AI Ultra subscribers.&lt;/li&gt; 
 &lt;li&gt;Anyone who has a Gemini Code Assist license.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;See &lt;a href="https://cloud.google.com/gemini/docs/quotas"&gt;quota limits and terms of service&lt;/a&gt; for details.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Free tier&lt;/strong&gt; with 60 requests/min and 1,000 requests/day&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini 2.5 Pro and Flash&lt;/strong&gt; with 1M token context window&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No API key management&lt;/strong&gt; - just sign in with your Google account&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic updates&lt;/strong&gt; to our latest models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Start Gemini CLI, then choose &lt;em&gt;Login with Google&lt;/em&gt; and follow the browser authentication flow when prompted&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;gemini
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;If you are using a paid Code Assist License from your organization, remember to set the Google Cloud Project&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Set your Google Cloud Project
export GOOGLE_CLOUD_PROJECT="YOUR_PROJECT_NAME"
gemini
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Gemini API Key&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;‚ú® Best for:&lt;/strong&gt; Developers who need specific model control or paid tier access&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Free tier&lt;/strong&gt;: 100 requests/day with Gemini 2.5 Pro&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model selection&lt;/strong&gt;: Choose specific Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Usage-based billing&lt;/strong&gt;: Upgrade for higher limits when needed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get your key from https://aistudio.google.com/apikey
export GEMINI_API_KEY="YOUR_API_KEY"
gemini
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 3: Vertex AI&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;‚ú® Best for:&lt;/strong&gt; Enterprise teams and production workloads&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise features&lt;/strong&gt;: Advanced security and compliance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt;: Higher rate limits with billing account&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integration&lt;/strong&gt;: Works with existing Google Cloud infrastructure&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get your key from Google Cloud Console
export GOOGLE_API_KEY="YOUR_API_KEY"
export GOOGLE_GENAI_USE_VERTEXAI=true
gemini
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Google Workspace accounts and other authentication methods, see the &lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/cli/authentication.md"&gt;authentication guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;h4&gt;Start in current directory&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;gemini
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Include multiple directories&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;gemini --include-directories ../lib,../docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Use specific model&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;gemini -m gemini-2.5-flash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Non-interactive mode for scripts&lt;/h4&gt; 
&lt;p&gt;Get a simple text response:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;gemini -p "Explain the architecture of this codebase"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more advanced scripting, including how to parse JSON and handle errors, use the &lt;code&gt;--output-format json&lt;/code&gt; flag to get structured output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;gemini -p "Explain the architecture of this codebase" --output-format json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quick Examples&lt;/h3&gt; 
&lt;h4&gt;Start a new project&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd new-project/
gemini
&amp;gt; Write me a Discord bot that answers questions using a FAQ.md file I will provide
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Analyze existing code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google-gemini/gemini-cli
cd gemini-cli
gemini
&amp;gt; Give me a summary of all of the changes that went in yesterday
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/cli/index.md"&gt;&lt;strong&gt;Quickstart Guide&lt;/strong&gt;&lt;/a&gt; - Get up and running quickly&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/cli/authentication.md"&gt;&lt;strong&gt;Authentication Setup&lt;/strong&gt;&lt;/a&gt; - Detailed auth configuration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/cli/configuration.md"&gt;&lt;strong&gt;Configuration Guide&lt;/strong&gt;&lt;/a&gt; - Settings and customization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/keyboard-shortcuts.md"&gt;&lt;strong&gt;Keyboard Shortcuts&lt;/strong&gt;&lt;/a&gt; - Productivity tips&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Core Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/cli/commands.md"&gt;&lt;strong&gt;Commands Reference&lt;/strong&gt;&lt;/a&gt; - All slash commands (&lt;code&gt;/help&lt;/code&gt;, &lt;code&gt;/chat&lt;/code&gt;, &lt;code&gt;/mcp&lt;/code&gt;, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/checkpointing.md"&gt;&lt;strong&gt;Checkpointing&lt;/strong&gt;&lt;/a&gt; - Save and resume conversations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/tools/memory.md"&gt;&lt;strong&gt;Memory Management&lt;/strong&gt;&lt;/a&gt; - Using GEMINI.md context files&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/cli/token-caching.md"&gt;&lt;strong&gt;Token Caching&lt;/strong&gt;&lt;/a&gt; - Optimize token usage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tools &amp;amp; Extensions&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/tools/index.md"&gt;&lt;strong&gt;Built-in Tools Overview&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/tools/file-system.md"&gt;File System Operations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/tools/shell.md"&gt;Shell Commands&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/tools/web-fetch.md"&gt;Web Fetch &amp;amp; Search&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/tools/multi-file.md"&gt;Multi-file Operations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/tools/mcp-server.md"&gt;&lt;strong&gt;MCP Server Integration&lt;/strong&gt;&lt;/a&gt; - Extend with custom tools&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/extension.md"&gt;&lt;strong&gt;Custom Extensions&lt;/strong&gt;&lt;/a&gt; - Build your own commands&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/architecture.md"&gt;&lt;strong&gt;Architecture Overview&lt;/strong&gt;&lt;/a&gt; - How Gemini CLI works&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/ide-integration.md"&gt;&lt;strong&gt;IDE Integration&lt;/strong&gt;&lt;/a&gt; - VS Code companion&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/sandbox.md"&gt;&lt;strong&gt;Sandboxing &amp;amp; Security&lt;/strong&gt;&lt;/a&gt; - Safe execution environments&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/deployment.md"&gt;&lt;strong&gt;Enterprise Deployment&lt;/strong&gt;&lt;/a&gt; - Docker, system-wide config&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/telemetry.md"&gt;&lt;strong&gt;Telemetry &amp;amp; Monitoring&lt;/strong&gt;&lt;/a&gt; - Usage tracking&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/core/tools-api.md"&gt;&lt;strong&gt;Tools API Development&lt;/strong&gt;&lt;/a&gt; - Create custom tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Configuration &amp;amp; Customization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/cli/configuration.md"&gt;&lt;strong&gt;Settings Reference&lt;/strong&gt;&lt;/a&gt; - All configuration options&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/cli/themes.md"&gt;&lt;strong&gt;Theme Customization&lt;/strong&gt;&lt;/a&gt; - Visual customization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/gemini-ignore.md"&gt;&lt;strong&gt;.gemini Directory&lt;/strong&gt;&lt;/a&gt; - Project-specific settings&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/cli/configuration.md#environment-variables"&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Troubleshooting &amp;amp; Support&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/troubleshooting.md"&gt;&lt;strong&gt;Troubleshooting Guide&lt;/strong&gt;&lt;/a&gt; - Common issues and solutions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/troubleshooting.md#frequently-asked-questions"&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt; - Quick answers&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;/bug&lt;/code&gt; command to report issues directly from the CLI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Using MCP Servers&lt;/h3&gt; 
&lt;p&gt;Configure MCP servers in &lt;code&gt;~/.gemini/settings.json&lt;/code&gt; to extend Gemini CLI with custom tools:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;gt; @github List my open pull requests
&amp;gt; @slack Send a summary of today's commits to #dev channel
&amp;gt; @database Run a query to find inactive users
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/tools/mcp-server.md"&gt;MCP Server Integration guide&lt;/a&gt; for setup instructions.&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! Gemini CLI is fully open source (Apache 2.0), and we encourage the community to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report bugs and suggest features&lt;/li&gt; 
 &lt;li&gt;Improve documentation&lt;/li&gt; 
 &lt;li&gt;Submit code improvements&lt;/li&gt; 
 &lt;li&gt;Share your MCP servers and extensions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for development setup, coding standards, and how to submit pull requests.&lt;/p&gt; 
&lt;p&gt;Check our &lt;a href="https://github.com/orgs/google-gemini/projects/11/"&gt;Official Roadmap&lt;/a&gt; for planned features and priorities.&lt;/p&gt; 
&lt;h2&gt;üìñ Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/ROADMAP.md"&gt;Official Roadmap&lt;/a&gt;&lt;/strong&gt; - See what's coming next&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.npmjs.com/package/@google/gemini-cli"&gt;NPM Package&lt;/a&gt;&lt;/strong&gt; - Package registry&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-gemini/gemini-cli/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs or request features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-gemini/gemini-cli/security/advisories"&gt;Security Advisories&lt;/a&gt;&lt;/strong&gt; - Security updates&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Uninstall&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/Uninstall.md"&gt;Uninstall Guide&lt;/a&gt; for removal instructions.&lt;/p&gt; 
&lt;h2&gt;üìÑ Legal&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Terms of Service&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/docs/tos-privacy.md"&gt;Terms &amp;amp; Privacy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/google-gemini/gemini-cli/main/SECURITY.md"&gt;Security Policy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; Built with ‚ù§Ô∏è by Google and the open source community &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>gin-gonic/gin</title>
      <link>https://github.com/gin-gonic/gin</link>
      <description>&lt;p&gt;Gin is a high-performance HTTP web framework written in Go. It provides a Martini-like API but with significantly better performance‚Äîup to 40 times faster‚Äîthanks to httprouter. Gin is designed for building REST APIs, web applications, and microservices.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gin Web Framework&lt;/h1&gt; 
&lt;img align="right" width="159px" src="https://raw.githubusercontent.com/gin-gonic/logo/master/color.png" /&gt; 
&lt;p&gt;&lt;a href="https://github.com/gin-gonic/gin/actions/workflows/gin.yml"&gt;&lt;img src="https://github.com/gin-gonic/gin/actions/workflows/gin.yml/badge.svg?branch=master" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/gin-gonic/gin"&gt;&lt;img src="https://codecov.io/gh/gin-gonic/gin/branch/master/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/gin-gonic/gin"&gt;&lt;img src="https://goreportcard.com/badge/github.com/gin-gonic/gin" alt="Go Report Card" /&gt;&lt;/a&gt; &lt;a href="https://pkg.go.dev/github.com/gin-gonic/gin?tab=doc"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/gin-gonic/gin?status.svg?sanitize=true" alt="Go Reference" /&gt;&lt;/a&gt; &lt;a href="https://sourcegraph.com/github.com/gin-gonic/gin?badge"&gt;&lt;img src="https://sourcegraph.com/github.com/gin-gonic/gin/-/badge.svg?sanitize=true" alt="Sourcegraph" /&gt;&lt;/a&gt; &lt;a href="https://www.codetriage.com/gin-gonic/gin"&gt;&lt;img src="https://www.codetriage.com/gin-gonic/gin/badges/users.svg?sanitize=true" alt="Open Source Helpers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/gin-gonic/gin/releases"&gt;&lt;img src="https://img.shields.io/github/release/gin-gonic/gin.svg?style=flat-square" alt="Release" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üì∞ &lt;a href="https://gin-gonic.com/en/blog/news/gin-1-11-0-release-announcement/"&gt;Announcing Gin 1.11.0!&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Read about the latest features and improvements in Gin 1.11.0 on our official blog.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Gin is a high-performance HTTP web framework written in &lt;a href="https://go.dev/"&gt;Go&lt;/a&gt;. It provides a Martini-like API but with significantly better performance‚Äîup to 40 times faster‚Äîthanks to &lt;a href="https://github.com/julienschmidt/httprouter"&gt;httprouter&lt;/a&gt;. Gin is designed for building REST APIs, web applications, and microservices where speed and developer productivity are essential.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Why choose Gin?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Gin combines the simplicity of Express.js-style routing with Go's performance characteristics, making it ideal for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Building high-throughput REST APIs&lt;/li&gt; 
 &lt;li&gt;Developing microservices that need to handle many concurrent requests&lt;/li&gt; 
 &lt;li&gt;Creating web applications that require fast response times&lt;/li&gt; 
 &lt;li&gt;Prototyping web services quickly with minimal boilerplate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Gin's key features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Zero allocation router&lt;/strong&gt; - Extremely memory-efficient routing with no heap allocations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt; - Benchmarks show superior speed compared to other Go web frameworks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Middleware support&lt;/strong&gt; - Extensible middleware system for authentication, logging, CORS, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Crash-free&lt;/strong&gt; - Built-in recovery middleware prevents panics from crashing your server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JSON validation&lt;/strong&gt; - Automatic request/response JSON binding and validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Route grouping&lt;/strong&gt; - Organize related routes and apply common middleware&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Error management&lt;/strong&gt; - Centralized error handling and logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in rendering&lt;/strong&gt; - Support for JSON, XML, HTML templates, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt; - Large ecosystem of community middleware and plugins&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Go version&lt;/strong&gt;: Gin requires &lt;a href="https://go.dev/"&gt;Go&lt;/a&gt; version &lt;a href="https://go.dev/doc/devel/release#go1.23.0"&gt;1.23&lt;/a&gt; or above&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Basic Go knowledge&lt;/strong&gt;: Familiarity with Go syntax and package management is helpful&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;With &lt;a href="https://go.dev/wiki/Modules#how-to-use-modules"&gt;Go's module support&lt;/a&gt;, simply import Gin in your code and Go will automatically fetch it during build:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import "github.com/gin-gonic/gin"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Your First Gin Application&lt;/h3&gt; 
&lt;p&gt;Here's a complete example that demonstrates Gin's simplicity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;package main

import (
  "net/http"

  "github.com/gin-gonic/gin"
)

func main() {
  // Create a Gin router with default middleware (logger and recovery)
  r := gin.Default()
  
  // Define a simple GET endpoint
  r.GET("/ping", func(c *gin.Context) {
    // Return JSON response
    c.JSON(http.StatusOK, gin.H{
      "message": "pong",
    })
  })
  
  // Start server on port 8080 (default)
  // Server will listen on 0.0.0.0:8080 (localhost:8080 on Windows)
  r.Run()
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Running the application:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Save the code above as &lt;code&gt;main.go&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the application:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;go run main.go
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Open your browser and visit &lt;a href="http://localhost:8080/ping"&gt;&lt;code&gt;http://localhost:8080/ping&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You should see: &lt;code&gt;{"message":"pong"}&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;What this example demonstrates:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creating a Gin router with default middleware&lt;/li&gt; 
 &lt;li&gt;Defining HTTP endpoints with simple handler functions&lt;/li&gt; 
 &lt;li&gt;Returning JSON responses&lt;/li&gt; 
 &lt;li&gt;Starting an HTTP server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Next Steps&lt;/h3&gt; 
&lt;p&gt;After running your first Gin application, explore these resources to learn more:&lt;/p&gt; 
&lt;h4&gt;üìö Learning Resources&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/gin-gonic/gin/master/docs/doc.md"&gt;Gin Quick Start Guide&lt;/a&gt;&lt;/strong&gt; - Comprehensive tutorial with API examples and build configurations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/gin-gonic/examples"&gt;Example Repository&lt;/a&gt;&lt;/strong&gt; - Ready-to-run examples demonstrating various Gin use cases: 
  &lt;ul&gt; 
   &lt;li&gt;REST API development&lt;/li&gt; 
   &lt;li&gt;Authentication &amp;amp; middleware&lt;/li&gt; 
   &lt;li&gt;File uploads and downloads&lt;/li&gt; 
   &lt;li&gt;WebSocket connections&lt;/li&gt; 
   &lt;li&gt;Template rendering&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;h3&gt;API Reference&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://pkg.go.dev/github.com/gin-gonic/gin"&gt;Go.dev API Documentation&lt;/a&gt;&lt;/strong&gt; - Complete API reference with examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guides&lt;/h3&gt; 
&lt;p&gt;The comprehensive documentation is available on &lt;a href="https://gin-gonic.com"&gt;gin-gonic.com&lt;/a&gt; in multiple languages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://gin-gonic.com/en/docs/"&gt;English&lt;/a&gt; | &lt;a href="https://gin-gonic.com/zh-cn/docs/"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://gin-gonic.com/zh-tw/docs/"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gin-gonic.com/ja/docs/"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://gin-gonic.com/ko-kr/docs/"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://gin-gonic.com/es/docs/"&gt;Espa√±ol&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gin-gonic.com/tr/docs/"&gt;Turkish&lt;/a&gt; | &lt;a href="https://gin-gonic.com/fa/docs/"&gt;Persian&lt;/a&gt; | &lt;a href="https://gin-gonic.com/pt/docs/"&gt;Portugu√™s&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gin-gonic.com/ru/docs/"&gt;Russian&lt;/a&gt; | &lt;a href="https://gin-gonic.com/id/docs/"&gt;Indonesian&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Official Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://go.dev/doc/tutorial/web-service-gin"&gt;Go.dev Tutorial: Developing a RESTful API with Go and Gin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Performance Benchmarks&lt;/h2&gt; 
&lt;p&gt;Gin demonstrates exceptional performance compared to other Go web frameworks. It uses a custom version of &lt;a href="https://github.com/julienschmidt/httprouter"&gt;HttpRouter&lt;/a&gt; for maximum efficiency. &lt;a href="https://raw.githubusercontent.com/gin-gonic/gin/master/BENCHMARKS.md"&gt;View detailed benchmarks ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Gin vs. Other Go Frameworks&lt;/strong&gt; (GitHub API routing benchmark):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark name&lt;/th&gt; 
   &lt;th align="right"&gt;(1)&lt;/th&gt; 
   &lt;th align="right"&gt;(2)&lt;/th&gt; 
   &lt;th align="right"&gt;(3)&lt;/th&gt; 
   &lt;th align="right"&gt;(4)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGin_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;43550&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;27364 ns/op&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;0 B/op&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;0 allocs/op&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkAce_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;40543&lt;/td&gt; 
   &lt;td align="right"&gt;29670 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkAero_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;57632&lt;/td&gt; 
   &lt;td align="right"&gt;20648 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkBear_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;9234&lt;/td&gt; 
   &lt;td align="right"&gt;216179 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;86448 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;943 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkBeego_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;7407&lt;/td&gt; 
   &lt;td align="right"&gt;243496 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;71456 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;609 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkBone_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;420&lt;/td&gt; 
   &lt;td align="right"&gt;2922835 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;720160 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;8620 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkChi_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;7620&lt;/td&gt; 
   &lt;td align="right"&gt;238331 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;87696 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;609 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkDenco_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;18355&lt;/td&gt; 
   &lt;td align="right"&gt;64494 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;20224 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;167 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkEcho_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;31251&lt;/td&gt; 
   &lt;td align="right"&gt;38479 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGocraftWeb_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;4117&lt;/td&gt; 
   &lt;td align="right"&gt;300062 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;131656 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;1686 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGoji_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;3274&lt;/td&gt; 
   &lt;td align="right"&gt;416158 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;56112 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;334 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGojiv2_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;1402&lt;/td&gt; 
   &lt;td align="right"&gt;870518 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;352720 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;4321 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGoJsonRest_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;2976&lt;/td&gt; 
   &lt;td align="right"&gt;401507 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;134371 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;2737 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGoRestful_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;410&lt;/td&gt; 
   &lt;td align="right"&gt;2913158 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;910144 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;2938 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGorillaMux_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;346&lt;/td&gt; 
   &lt;td align="right"&gt;3384987 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;251650 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;1994 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGowwwRouter_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;10000&lt;/td&gt; 
   &lt;td align="right"&gt;143025 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;72144 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;501 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkHttpRouter_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;55938&lt;/td&gt; 
   &lt;td align="right"&gt;21360 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkHttpTreeMux_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;10000&lt;/td&gt; 
   &lt;td align="right"&gt;153944 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;65856 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;671 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkKocha_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;10000&lt;/td&gt; 
   &lt;td align="right"&gt;106315 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;23304 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;843 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkLARS_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;47779&lt;/td&gt; 
   &lt;td align="right"&gt;25084 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkMacaron_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;3266&lt;/td&gt; 
   &lt;td align="right"&gt;371907 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;149409 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;1624 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkMartini_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;331&lt;/td&gt; 
   &lt;td align="right"&gt;3444706 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;226551 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;2325 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkPat_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;273&lt;/td&gt; 
   &lt;td align="right"&gt;4381818 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;1483152 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;26963 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkPossum_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;10000&lt;/td&gt; 
   &lt;td align="right"&gt;164367 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;84448 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;609 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkR2router_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;10000&lt;/td&gt; 
   &lt;td align="right"&gt;160220 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;77328 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;979 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkRivet_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;14625&lt;/td&gt; 
   &lt;td align="right"&gt;82453 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;16272 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;167 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkTango_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;6255&lt;/td&gt; 
   &lt;td align="right"&gt;279611 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;63826 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;1618 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkTigerTonic_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;2008&lt;/td&gt; 
   &lt;td align="right"&gt;687874 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;193856 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;4474 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkTraffic_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;355&lt;/td&gt; 
   &lt;td align="right"&gt;3478508 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;820744 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;14114 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkVulcan_GithubAll&lt;/td&gt; 
   &lt;td align="right"&gt;6885&lt;/td&gt; 
   &lt;td align="right"&gt;193333 ns/op&lt;/td&gt; 
   &lt;td align="right"&gt;19894 B/op&lt;/td&gt; 
   &lt;td align="right"&gt;609 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;(1): Total Repetitions achieved in constant time, higher means more confident result&lt;/li&gt; 
 &lt;li&gt;(2): Single Repetition Duration (ns/op), lower is better&lt;/li&gt; 
 &lt;li&gt;(3): Heap Memory (B/op), lower is better&lt;/li&gt; 
 &lt;li&gt;(4): Average Allocations per Repetition (allocs/op), lower is better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîå Middleware Ecosystem&lt;/h2&gt; 
&lt;p&gt;Gin has a rich ecosystem of middleware for common web development needs. Explore community-contributed middleware:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/gin-contrib"&gt;gin-contrib&lt;/a&gt;&lt;/strong&gt; - Official middleware collection including:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Authentication (JWT, Basic Auth, Sessions)&lt;/li&gt; 
   &lt;li&gt;CORS, Rate limiting, Compression&lt;/li&gt; 
   &lt;li&gt;Logging, Metrics, Tracing&lt;/li&gt; 
   &lt;li&gt;Static file serving, Template engines&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/gin-gonic/contrib"&gt;gin-gonic/contrib&lt;/a&gt;&lt;/strong&gt; - Additional community middleware&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üè¢ Production Usage&lt;/h2&gt; 
&lt;p&gt;Gin powers many high-traffic applications and services in production:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/appleboy/gorush"&gt;gorush&lt;/a&gt;&lt;/strong&gt; - High-performance push notification server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/fnproject/fn"&gt;fnproject&lt;/a&gt;&lt;/strong&gt; - Container-native, serverless platform&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/photoprism/photoprism"&gt;photoprism&lt;/a&gt;&lt;/strong&gt; - AI-powered personal photo management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/luraproject/lura"&gt;lura&lt;/a&gt;&lt;/strong&gt; - Ultra-performant API Gateway framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/thoas/picfit"&gt;picfit&lt;/a&gt;&lt;/strong&gt; - Real-time image processing server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/distribworks/dkron"&gt;dkron&lt;/a&gt;&lt;/strong&gt; - Distributed job scheduling system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;Gin is the work of hundreds of contributors from around the world. We welcome and appreciate your contributions!&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Report bugs&lt;/strong&gt; - Help us identify and fix issues&lt;/li&gt; 
 &lt;li&gt;üí° &lt;strong&gt;Suggest features&lt;/strong&gt; - Share your ideas for improvements&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;Improve documentation&lt;/strong&gt; - Help make our docs clearer&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Submit code&lt;/strong&gt; - Fix bugs or implement new features&lt;/li&gt; 
 &lt;li&gt;üß™ &lt;strong&gt;Write tests&lt;/strong&gt; - Improve our test coverage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting Started with Contributing&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check out our &lt;a href="https://raw.githubusercontent.com/gin-gonic/gin/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for detailed guidelines&lt;/li&gt; 
 &lt;li&gt;Join our community discussions and ask questions&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;All contributions are valued and help make Gin better for everyone!&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/RAG-Anything</title>
      <link>https://github.com/HKUDS/RAG-Anything</link>
      <description>&lt;p&gt;"RAG-Anything: All-in-One RAG Framework"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div style="margin: 20px 0;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/logo.png" width="120" height="120" alt="RAG-Anything Logo" style="border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;üöÄ RAG-Anything: All-in-One RAG Framework&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14959" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14959" alt="HKUDS%2FRAG-Anything | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&amp;amp;size=24&amp;amp;duration=3000&amp;amp;pause=1000&amp;amp;color=00D9FF&amp;amp;center=true&amp;amp;vCenter=true&amp;amp;width=600&amp;amp;lines=Welcome+to+RAG-Anything;Next-Gen+Multimodal+RAG+System;Powered+by+Advanced+AI+Technology" alt="Typing Animation" /&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;"&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;&lt;img src="https://img.shields.io/badge/üî•Project-Page-00d9ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2410.05779"&gt;&lt;img src="https://img.shields.io/badge/üìÑarXiv-2410.05779-ff6b6b?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt;&lt;img src="https://img.shields.io/badge/‚ö°Based%20on-LightRAG-4ecdc4?style=for-the-badge&amp;amp;logo=lightning&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/RAG-Anything?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.10-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/raganything/"&gt;&lt;img src="https://img.shields.io/pypi/v/raganything.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;img src="https://img.shields.io/badge/‚ö°uv-Ready-ff6b6b?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/issues/7"&gt;&lt;img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README_zh.md"&gt;&lt;img src="https://img.shields.io/badge/üá®üá≥‰∏≠ÊñáÁâà-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/üá∫üá∏English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéâ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.08.12]üéØüì¢ üîç RAG-Anything now features &lt;strong&gt;VLM-Enhanced Query&lt;/strong&gt; mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07.05]üéØüì¢ RAG-Anything now features a &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/docs/context_aware_processing.md"&gt;context configuration module&lt;/a&gt;, enabling intelligent integration of relevant contextual information to enhance multimodal content processing.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07.04]üéØüì¢ üöÄ RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07.03]üéØüì¢ üéâ RAG-Anything has reached 1küåü stars on GitHub! Thank you for your incredible support and valuable contributions to the project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåü System Overview&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Next-Generation Multimodal Intelligence&lt;/em&gt;&lt;/p&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border: 2px solid #00d9ff; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);"&gt; 
 &lt;p&gt;Modern documents increasingly contain diverse multimodal content‚Äîtext, images, tables, equations, charts, and multimedia‚Äîthat traditional text-focused RAG systems cannot effectively process. &lt;strong&gt;RAG-Anything&lt;/strong&gt; addresses this challenge as a comprehensive &lt;strong&gt;All-in-One Multimodal Document Processing RAG system&lt;/strong&gt; built on &lt;a href="https://github.com/HKUDS/LightRAG"&gt;LightRAG&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;As a unified solution, RAG-Anything &lt;strong&gt;eliminates the need for multiple specialized tools&lt;/strong&gt;. It provides &lt;strong&gt;seamless processing and querying across all content modalities&lt;/strong&gt; within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers &lt;strong&gt;comprehensive multimodal retrieval capabilities&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;Users can query documents containing &lt;strong&gt;interleaved text&lt;/strong&gt;, &lt;strong&gt;visual diagrams&lt;/strong&gt;, &lt;strong&gt;structured tables&lt;/strong&gt;, and &lt;strong&gt;mathematical formulations&lt;/strong&gt; through &lt;strong&gt;one cohesive interface&lt;/strong&gt;. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a &lt;strong&gt;unified processing framework&lt;/strong&gt;.&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/rag_anything_framework.png" alt="RAG-Anything" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Key Features&lt;/h3&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); border-radius: 15px; padding: 25px; margin: 20px 0;"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;üîÑ End-to-End Multimodal Pipeline&lt;/strong&gt; - Complete workflow from document ingestion and parsing to intelligent multimodal query answering&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üìÑ Universal Document Support&lt;/strong&gt; - Seamless processing of PDFs, Office documents, images, and diverse file formats&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üß† Specialized Content Analysis&lt;/strong&gt; - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üîó Multimodal Knowledge Graph&lt;/strong&gt; - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;‚ö° Adaptive Processing Modes&lt;/strong&gt; - Flexible MinerU-based parsing or direct multimodal content injection workflows&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üìã Direct Content List Insertion&lt;/strong&gt; - Bypass document parsing by directly inserting pre-parsed content lists from external sources&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üéØ Hybrid Intelligent Retrieval&lt;/strong&gt; - Advanced search capabilities spanning textual and multimodal content with contextual understanding&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Algorithm &amp;amp; Architecture&lt;/h2&gt; 
&lt;div style="background: linear-gradient(135deg, #0f0f23 0%, #1a1a2e 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border-left: 5px solid #00d9ff;"&gt; 
 &lt;h3&gt;Core Algorithm&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;RAG-Anything&lt;/strong&gt; implements an effective &lt;strong&gt;multi-stage multimodal pipeline&lt;/strong&gt; that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 20px;"&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     üìÑ
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Document Parsing
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    ‚Üí
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     üß†
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Content Analysis
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    ‚Üí
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     üîç
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Knowledge Graph
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    ‚Üí
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     üéØ
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Intelligent Retrieval
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;1. Document Parsing Stage&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚öôÔ∏è MinerU Integration&lt;/strong&gt;: Leverages &lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; for high-fidelity document structure extraction and semantic preservation across complex layouts.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üß© Adaptive Content Decomposition&lt;/strong&gt;: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìÅ Universal Format Support&lt;/strong&gt;: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Multi-Modal Content Understanding &amp;amp; Processing&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üéØ Autonomous Content Categorization and Routing&lt;/strong&gt;: Automatically identify, categorize, and route different content types through optimized execution channels.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚ö° Concurrent Multi-Pipeline Architecture&lt;/strong&gt;: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Document Hierarchy Extraction&lt;/strong&gt;: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;3. Multimodal Analysis Engine&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #0f3460 0%, #1a1a2e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #00d9ff;"&gt; 
 &lt;p&gt;The system deploys modality-aware processing units for heterogeneous data modalities:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Specialized Analyzers:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Visual Content Analyzer&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Integrate vision model for image analysis.&lt;/li&gt; 
    &lt;li&gt;Generates context-aware descriptive captions based on visual semantics.&lt;/li&gt; 
    &lt;li&gt;Extracts spatial relationships and hierarchical structures between visual elements.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìä Structured Data Interpreter&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Performs systematic interpretation of tabular and structured data formats.&lt;/li&gt; 
    &lt;li&gt;Implements statistical pattern recognition algorithms for data trend analysis.&lt;/li&gt; 
    &lt;li&gt;Identifies semantic relationships and dependencies across multiple tabular datasets.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìê Mathematical Expression Parser&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Parses complex mathematical expressions and formulas with high accuracy.&lt;/li&gt; 
    &lt;li&gt;Provides native LaTeX format support for seamless integration with academic workflows.&lt;/li&gt; 
    &lt;li&gt;Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîß Extensible Modality Handler&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Provides configurable processing framework for custom and emerging content types.&lt;/li&gt; 
    &lt;li&gt;Enables dynamic integration of new modality processors through plugin architecture.&lt;/li&gt; 
    &lt;li&gt;Supports runtime configuration of processing pipelines for specialized use cases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;4. Multimodal Knowledge Graph Index&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Core Functions:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Multi-Modal Entity Extraction&lt;/strong&gt;: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîó Cross-Modal Relationship Mapping&lt;/strong&gt;: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Hierarchical Structure Preservation&lt;/strong&gt;: Maintains original document organization through "belongs_to" relationship chains. These chains preserve logical content hierarchy and sectional dependencies.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚öñÔ∏è Weighted Relationship Scoring&lt;/strong&gt;: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;5. Modality-Aware Retrieval&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Retrieval Mechanisms:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîÄ Vector-Graph Fusion&lt;/strong&gt;: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìä Modality-Aware Ranking&lt;/strong&gt;: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîó Relational Coherence Maintenance&lt;/strong&gt;: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Initialize Your AI Journey&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif" width="400" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;Option 1: Install from PyPI (Recommended)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic installation
pip install raganything

# With optional dependencies for extended format support:
pip install 'raganything[all]'              # All optional features
pip install 'raganything[image]'            # Image format conversion (BMP, TIFF, GIF, WebP)
pip install 'raganything[text]'             # Text file processing (TXT, MD)
pip install 'raganything[image,text]'       # Multiple features
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Install from Source&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and setup the project with uv
git clone https://github.com/HKUDS/RAG-Anything.git
cd RAG-Anything

# Install the package and dependencies in a virtual environment
uv sync

# If you encounter network timeouts (especially for opencv packages):
# UV_HTTP_TIMEOUT=120 uv sync

# Run commands directly with uv (recommended approach)
uv run python examples/raganything_example.py --help

# Install with optional dependencies
uv sync --extra image --extra text  # Specific extras
uv sync --all-extras                 # All optional features
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[image]&lt;/code&gt;&lt;/strong&gt; - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[text]&lt;/code&gt;&lt;/strong&gt; - Enables processing of TXT and MD files (requires ReportLab)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[all]&lt;/code&gt;&lt;/strong&gt; - Includes all Python optional dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Office Document Processing Requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require &lt;strong&gt;LibreOffice&lt;/strong&gt; installation&lt;/li&gt; 
  &lt;li&gt;Download from &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice official website&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Download installer from official website&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;brew install --cask libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Ubuntu/Debian&lt;/strong&gt;: &lt;code&gt;sudo apt-get install libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;CentOS/RHEL&lt;/strong&gt;: &lt;code&gt;sudo yum install libreoffice&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Check MinerU installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Verify installation
mineru --version

# Check if properly configured
python -c "from raganything import RAGAnything; rag = RAGAnything(); print('‚úÖ MinerU installed properly' if rag.check_parser_installation() else '‚ùå MinerU installation issue')"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models are downloaded automatically on first use. For manual download, refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/README.md#22-model-source-configuration"&gt;MinerU Model Source Configuration&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;h4&gt;1. End-to-End Document Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        parser="mineru",  # Parser selection: mineru or docling
        parse_method="auto",  # Parse method: auto, ocr, or txt
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define LLM model function
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Define embedding function
    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Process a document
    await rag.process_document_complete(
        file_path="path/to/your/document.pdf",
        output_dir="./output",
        parse_method="auto"
    )

    # Query the processed content
    # Pure text query - for basic knowledge base search
    text_result = await rag.aquery(
        "What are the main findings shown in the figures and tables?",
        mode="hybrid"
    )
    print("Text query result:", text_result)

    # Multimodal query with specific multimodal content
    multimodal_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
    print("Multimodal query result:", multimodal_result)

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Direct Multimodal Content Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from raganything.modalprocessors import ImageModalProcessor, TableModalProcessor

async def process_multimodal_content():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Initialize LightRAG
    rag = LightRAG(
        working_dir="./rag_storage",
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )
    await rag.initialize_storages()

    # Process an image
    image_processor = ImageModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
            "gpt-4o",
            "",
            system_prompt=None,
            history_messages=[],
            messages=[
                {"role": "system", "content": system_prompt} if system_prompt else None,
                {"role": "user", "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                ]} if image_data else {"role": "user", "content": prompt}
            ],
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ) if image_data else openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    image_content = {
        "img_path": "path/to/image.jpg",
        "image_caption": ["Figure 1: Experimental results"],
        "image_footnote": ["Data collected in 2024"]
    }

    description, entity_info = await image_processor.process_multimodal_content(
        modal_content=image_content,
        content_type="image",
        file_path="research_paper.pdf",
        entity_name="Experimental Results Figure"
    )

    # Process a table
    table_processor = TableModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    table_content = {
        "table_body": """
        | Method | Accuracy | F1-Score |
        |--------|----------|----------|
        | RAGAnything | 95.2% | 0.94 |
        | Baseline | 87.3% | 0.85 |
        """,
        "table_caption": ["Performance Comparison"],
        "table_footnote": ["Results on test dataset"]
    }

    description, entity_info = await table_processor.process_multimodal_content(
        modal_content=table_content,
        content_type="table",
        file_path="research_paper.pdf",
        entity_name="Performance Results Table"
    )

if __name__ == "__main__":
    asyncio.run(process_multimodal_content())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Batch Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process multiple documents
await rag.process_folder_complete(
    folder_path="./documents",
    output_dir="./output",
    file_extensions=[".pdf", ".docx", ".pptx"],
    recursive=True,
    max_workers=4
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Custom Modal Processors&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from raganything.modalprocessors import GenericModalProcessor

class CustomModalProcessor(GenericModalProcessor):
    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):
        # Your custom processing logic
        enhanced_description = await self.analyze_custom_content(modal_content)
        entity_info = self.create_custom_entity(enhanced_description, entity_name)
        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Query Options&lt;/h4&gt; 
&lt;p&gt;RAG-Anything provides three types of query methods:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Pure Text Queries&lt;/strong&gt; - Direct knowledge base search using LightRAG:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Different query modes for text queries
text_result_hybrid = await rag.aquery("Your question", mode="hybrid")
text_result_local = await rag.aquery("Your question", mode="local")
text_result_global = await rag.aquery("Your question", mode="global")
text_result_naive = await rag.aquery("Your question", mode="naive")

# Synchronous version
sync_text_result = rag.query("Your question", mode="hybrid")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;VLM Enhanced Queries&lt;/strong&gt; - Automatically analyze images in retrieved context using VLM:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# VLM enhanced query (automatically enabled when vision_model_func is provided)
vlm_result = await rag.aquery(
    "Analyze the charts and figures in the document",
    mode="hybrid"
    # vlm_enhanced=True is automatically set when vision_model_func is available
)

# Manually control VLM enhancement
vlm_enabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=True  # Force enable VLM enhancement
)

vlm_disabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=False  # Force disable VLM enhancement
)

# When documents contain images, VLM can see and analyze them directly
# The system will automatically:
# 1. Retrieve relevant context containing image paths
# 2. Load and encode images as base64
# 3. Send both text context and images to VLM for comprehensive analysis
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Multimodal Queries&lt;/strong&gt; - Enhanced queries with specific multimodal content analysis:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Query with table data
table_result = await rag.aquery_with_multimodal(
    "Compare these performance metrics with the document content",
    multimodal_content=[{
        "type": "table",
        "table_data": """Method,Accuracy,Speed
                        RAGAnything,95.2%,120ms
                        Traditional,87.3%,180ms""",
        "table_caption": "Performance comparison"
    }],
    mode="hybrid"
)

# Query with equation content
equation_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;6. Loading Existing LightRAG Instance&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import EmbeddingFunc
import os

async def load_existing_lightrag():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # First, create or load existing LightRAG instance
    lightrag_working_dir = "./existing_lightrag_storage"

    # Check if previous LightRAG instance exists
    if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
        print("‚úÖ Found existing LightRAG instance, loading...")
    else:
        print("‚ùå No existing LightRAG instance found, will create new one")

    # Create/load LightRAG instance with your configuration
    lightrag_instance = LightRAG(
        working_dir=lightrag_working_dir,
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )

    # Initialize storage (this will load existing data if available)
    await lightrag_instance.initialize_storages()
    await initialize_pipeline_status()

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Now use existing LightRAG instance to initialize RAGAnything
    rag = RAGAnything(
        lightrag=lightrag_instance,  # Pass existing LightRAG instance
        vision_model_func=vision_model_func,
        # Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance
    )

    # Query existing knowledge base
    result = await rag.aquery(
        "What data has been processed in this LightRAG instance?",
        mode="hybrid"
    )
    print("Query result:", result)

    # Add new multimodal document to existing LightRAG instance
    await rag.process_document_complete(
        file_path="path/to/new/multimodal_document.pdf",
        output_dir="./output"
    )

if __name__ == "__main__":
    asyncio.run(load_existing_lightrag())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;7. Direct Content List Insertion&lt;/h4&gt; 
&lt;p&gt;For scenarios where you already have a pre-parsed content list (e.g., from external parsers or previous processing), you can directly insert it into RAGAnything without document parsing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def insert_content_list_example():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define model functions
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                        ],
                    } if image_data else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Example: Pre-parsed content list from external source
    content_list = [
        {
            "type": "text",
            "text": "This is the introduction section of our research paper.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "image",
            "img_path": "/absolute/path/to/figure1.jpg",  # IMPORTANT: Use absolute path
            "image_caption": ["Figure 1: System Architecture"],
            "image_footnote": ["Source: Authors' original design"],
            "page_idx": 1  # Page number where this image appears
        },
        {
            "type": "table",
            "table_body": "| Method | Accuracy | F1-Score |\n|--------|----------|----------|\n| Ours | 95.2% | 0.94 |\n| Baseline | 87.3% | 0.85 |",
            "table_caption": ["Table 1: Performance Comparison"],
            "table_footnote": ["Results on test dataset"],
            "page_idx": 2  # Page number where this table appears
        },
        {
            "type": "equation",
            "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
            "text": "Document relevance probability formula",
            "page_idx": 3  # Page number where this equation appears
        },
        {
            "type": "text",
            "text": "In conclusion, our method demonstrates superior performance across all metrics.",
            "page_idx": 4  # Page number where this content appears
        }
    ]

    # Insert the content list directly
    await rag.insert_content_list(
        content_list=content_list,
        file_path="research_paper.pdf",  # Reference file name for citation
        split_by_character=None,         # Optional text splitting
        split_by_character_only=False,   # Optional text splitting mode
        doc_id=None,                     # Optional custom document ID (will be auto-generated if not provided)
        display_stats=True               # Show content statistics
    )

    # Query the inserted content
    result = await rag.aquery(
        "What are the key findings and performance metrics mentioned in the research?",
        mode="hybrid"
    )
    print("Query result:", result)

    # You can also insert multiple content lists with different document IDs
    another_content_list = [
        {
            "type": "text",
            "text": "This is content from another document.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "table",
            "table_body": "| Feature | Value |\n|---------|-------|\n| Speed | Fast |\n| Accuracy | High |",
            "table_caption": ["Feature Comparison"],
            "page_idx": 1  # Page number where this table appears
        }
    ]

    await rag.insert_content_list(
        content_list=another_content_list,
        file_path="another_document.pdf",
        doc_id="custom-doc-id-123"  # Custom document ID
    )

if __name__ == "__main__":
    asyncio.run(insert_content_list_example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Content List Format:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;content_list&lt;/code&gt; should follow the standard format with each item being a dictionary containing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text content&lt;/strong&gt;: &lt;code&gt;{"type": "text", "text": "content text", "page_idx": 0}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Image content&lt;/strong&gt;: &lt;code&gt;{"type": "image", "img_path": "/absolute/path/to/image.jpg", "image_caption": ["caption"], "image_footnote": ["note"], "page_idx": 1}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Table content&lt;/strong&gt;: &lt;code&gt;{"type": "table", "table_body": "markdown table", "table_caption": ["caption"], "table_footnote": ["note"], "page_idx": 2}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equation content&lt;/strong&gt;: &lt;code&gt;{"type": "equation", "latex": "LaTeX formula", "text": "description", "page_idx": 3}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic content&lt;/strong&gt;: &lt;code&gt;{"type": "custom_type", "content": "any content", "page_idx": 4}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;img_path&lt;/code&gt;&lt;/strong&gt;: Must be an absolute path to the image file (e.g., &lt;code&gt;/home/user/images/chart.jpg&lt;/code&gt; or &lt;code&gt;C:\Users\user\images\chart.jpg&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;page_idx&lt;/code&gt;&lt;/strong&gt;: Represents the page number where the content appears in the original document (0-based indexing)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content ordering&lt;/strong&gt;: Items are processed in the order they appear in the list&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This method is particularly useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You have content from external parsers (non-MinerU/Docling)&lt;/li&gt; 
 &lt;li&gt;You want to process programmatically generated content&lt;/li&gt; 
 &lt;li&gt;You need to insert content from multiple sources into a single knowledge base&lt;/li&gt; 
 &lt;li&gt;You have cached parsing results that you want to reuse&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üõ†Ô∏è Examples&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Practical Implementation Demos&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif" width="300" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;code&gt;examples/&lt;/code&gt; directory contains comprehensive usage examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;raganything_example.py&lt;/code&gt;&lt;/strong&gt;: End-to-end document processing with MinerU&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;modalprocessors_example.py&lt;/code&gt;&lt;/strong&gt;: Direct multimodal content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;office_document_test.py&lt;/code&gt;&lt;/strong&gt;: Office document parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;image_format_test.py&lt;/code&gt;&lt;/strong&gt;: Image format parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;text_format_test.py&lt;/code&gt;&lt;/strong&gt;: Text format parsing test with MinerU (no API key required)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Run examples:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# End-to-end processing with parser selection
python examples/raganything_example.py path/to/document.pdf --api-key YOUR_API_KEY --parser mineru

# Direct modal processing
python examples/modalprocessors_example.py --api-key YOUR_API_KEY

# Office document parsing test (MinerU only)
python examples/office_document_test.py --file path/to/document.docx

# Image format parsing test (MinerU only)
python examples/image_format_test.py --file path/to/image.bmp

# Text format parsing test (MinerU only)
python examples/text_format_test.py --file path/to/document.md

# Check LibreOffice installation
python examples/office_document_test.py --check-libreoffice --file dummy

# Check PIL/Pillow installation
python examples/image_format_test.py --check-pillow --file dummy

# Check ReportLab installation
python examples/text_format_test.py --check-reportlab --file dummy
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîß Configuration&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;System Optimization Parameters&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file (refer to &lt;code&gt;.env.example&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=your_base_url  # Optional
OUTPUT_DIR=./output             # Default output directory for parsed documents
PARSER=mineru                   # Parser selection: mineru or docling
PARSE_METHOD=auto              # Parse method: auto, ocr, or txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For backward compatibility, legacy environment variable names are still supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MINERU_PARSE_METHOD&lt;/code&gt; is deprecated, please use &lt;code&gt;PARSE_METHOD&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: API keys are only required for full RAG processing with LLM integration. The parsing test files (&lt;code&gt;office_document_test.py&lt;/code&gt; and &lt;code&gt;image_format_test.py&lt;/code&gt;) only test parser functionality and do not require API keys.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Parser Configuration&lt;/h3&gt; 
&lt;p&gt;RAGAnything now supports multiple parsers, each with specific advantages:&lt;/p&gt; 
&lt;h4&gt;MinerU Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports PDF, images, Office documents, and more formats&lt;/li&gt; 
 &lt;li&gt;Powerful OCR and table extraction capabilities&lt;/li&gt; 
 &lt;li&gt;GPU acceleration support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Docling Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optimized for Office documents and HTML files&lt;/li&gt; 
 &lt;li&gt;Better document structure preservation&lt;/li&gt; 
 &lt;li&gt;Native support for multiple Office formats&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MinerU Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MinerU 2.0 uses command-line parameters instead of config files
# Check available options:
mineru --help

# Common configurations:
mineru -p input.pdf -o output_dir -m auto    # Automatic parsing mode
mineru -p input.pdf -o output_dir -m ocr     # OCR-focused parsing
mineru -p input.pdf -o output_dir -b pipeline --device cuda  # GPU acceleration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also configure parsing through RAGAnything parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Basic parsing configuration with parser selection
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # or "ocr", "txt"
    parser="mineru"               # Optional: "mineru" or "docling"
)

# Advanced parsing configuration with special parameters
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # Parsing method: "auto", "ocr", "txt"
    parser="mineru",              # Parser selection: "mineru" or "docling"

    # MinerU special parameters - all supported kwargs:
    lang="ch",                   # Document language for OCR optimization (e.g., "ch", "en", "ja")
    device="cuda:0",             # Inference device: "cpu", "cuda", "cuda:0", "npu", "mps"
    start_page=0,                # Starting page number (0-based, for PDF)
    end_page=10,                 # Ending page number (0-based, for PDF)
    formula=True,                # Enable formula parsing
    table=True,                  # Enable table parsing
    backend="pipeline",          # Parsing backend: pipeline|vlm-transformers|vlm-sglang-engine|vlm-sglang-client.
    source="huggingface",        # Model source: "huggingface", "modelscope", "local"
    # vlm_url="http://127.0.0.1:3000" # Service address when using backend=vlm-sglang-client

    # Standard RAGAnything parameters
    display_stats=True,          # Display content statistics
    split_by_character=None,     # Optional character to split text by
    doc_id=None                  # Optional document ID
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MinerU 2.0 no longer uses the &lt;code&gt;magic-pdf.json&lt;/code&gt; configuration file. All settings are now passed as command-line parameters or function arguments. RAG-Anything now supports multiple document parsers - you can choose between MinerU and Docling based on your needs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Processing Requirements&lt;/h3&gt; 
&lt;p&gt;Different content types require specific optional dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; (.doc, .docx, .ppt, .pptx, .xls, .xlsx): Install &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extended Image Formats&lt;/strong&gt; (.bmp, .tiff, .gif, .webp): Install with &lt;code&gt;pip install raganything[image]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; (.txt, .md): Install with &lt;code&gt;pip install raganything[text]&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìã Quick Install&lt;/strong&gt;: Use &lt;code&gt;pip install raganything[all]&lt;/code&gt; to enable all format support (Python dependencies only - LibreOffice still needs separate installation)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üß™ Supported Content Types&lt;/h2&gt; 
&lt;h3&gt;Document Formats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PDFs&lt;/strong&gt; - Research papers, reports, presentations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; - DOC, DOCX, PPT, PPTX, XLS, XLSX&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - JPG, PNG, BMP, TIFF, GIF, WebP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; - TXT, MD&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Elements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - Photographs, diagrams, charts, screenshots&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tables&lt;/strong&gt; - Data tables, comparison charts, statistical summaries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equations&lt;/strong&gt; - Mathematical formulas in LaTeX format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic Content&lt;/strong&gt; - Custom content types via extensible processors&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;For installation of format-specific dependencies, see the &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-configuration"&gt;Configuration&lt;/a&gt; section.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Academic Reference&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 60px; height: 60px; margin: 20px auto; position: relative;"&gt; 
  &lt;div style="width: 100%; height: 100%; border: 2px solid #00d9ff; border-radius: 50%; position: relative;"&gt; 
   &lt;div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); font-size: 24px; color: #00d9ff;"&gt;
    üìñ
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div style="position: absolute; bottom: -5px; left: 50%; transform: translateX(-50%); width: 20px; height: 20px; background: white; border-right: 2px solid #00d9ff; border-bottom: 2px solid #00d9ff; transform: rotate(45deg);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;If you find RAG-Anything useful in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{guo2024lightrag,
  title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
  author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
  year={2024},
  eprint={2410.05779},
  archivePrefix={arXiv},
  primaryClass={cs.IR}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîó Related Projects&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Ecosystem &amp;amp; Extensions&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;‚ö°&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;LightRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Simple and Fast RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/VideoRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;üé•&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;VideoRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extreme Long-Context Video RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/MiniRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;‚ú®&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;MiniRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extremely Simple RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://star-history.com/#HKUDS/RAG-Anything&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contribution&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Join the Innovation&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt;
  We thank all our contributors for their valuable contributions. 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/HKUDS/RAG-Anything/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=HKUDS/RAG-Anything" style="border-radius: 15px; box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 30px 0;"&gt; 
 &lt;div&gt; 
  &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="500" /&gt; 
 &lt;/div&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/‚≠ê%20Star%20us%20on%20GitHub-1a1a2e?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/issues" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/üêõ%20Report%20Issues-ff6b6b?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/discussions" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/üí¨%20Discussions-4ecdc4?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: center; align-items: center; gap: 15px;"&gt; 
   &lt;span style="font-size: 24px;"&gt;‚≠ê&lt;/span&gt; 
   &lt;span style="color: #00d9ff; font-size: 18px;"&gt;Thank you for visiting RAG-Anything!&lt;/span&gt; 
   &lt;span style="font-size: 24px;"&gt;‚≠ê&lt;/span&gt; 
  &lt;/div&gt; 
  &lt;div style="margin-top: 10px; color: #00d9ff; font-size: 16px;"&gt;
   Building the Future of Multimodal AI
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>basecamp/omarchy</title>
      <link>https://github.com/basecamp/omarchy</link>
      <description>&lt;p&gt;Opinionated Arch/Hyprland Setup&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Omarchy&lt;/h1&gt; 
&lt;p&gt;Turn a fresh Arch installation into a fully-configured, beautiful, and modern web development system based on Hyprland by running a single command. That's the one-line pitch for Omarchy (like it was for Omakub). No need to write bespoke configs for every essential tool just to get started or to be up on all the latest command-line tools. Omarchy is an opinionated take on what Linux can be at its best.&lt;/p&gt; 
&lt;p&gt;Read more at &lt;a href="https://omarchy.org"&gt;omarchy.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Omarchy is released under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>is-a-dev/register</title>
      <link>https://github.com/is-a-dev/register</link>
      <description>&lt;p&gt;Grab your own sweet-looking '.is-a.dev' subdomain.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="is-a.dev Banner" src="https://raw.githubusercontent.com/is-a-dev/register/main/media/banner.png" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img alt="Domains" src="https://img.shields.io/github/directory-file-count/is-a-dev/register/domains?color=5c46eb&amp;amp;label=domains&amp;amp;style=for-the-badge" /&gt; &lt;img alt="Open Pull Requests" src="https://img.shields.io/github/issues-raw/is-a-dev/register?color=5c46eb&amp;amp;label=issues&amp;amp;style=for-the-badge" /&gt; &lt;img alt="Open Issues" src="https://img.shields.io/github/issues-pr-raw/is-a-dev/register?color=5c46eb&amp;amp;label=pull%20requests&amp;amp;style=for-the-badge" /&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;is-a.dev&lt;/h1&gt; 
&lt;p align="center"&gt;&lt;strong&gt;is-a.dev&lt;/strong&gt; is a service that allows developers to get a sweet-looking &lt;code&gt;.is-a.dev&lt;/code&gt; subdomain for their personal websites.&lt;/p&gt; 
&lt;h2&gt;Announcements &amp;amp; Status Updates&lt;/h2&gt; 
&lt;p&gt;Please join our &lt;a href="https://discord.gg/is-a-dev-830872854677422150"&gt;Discord server&lt;/a&gt; for announcements, updates &amp;amp; upgrades, and downtime notifications regarding the service. Not all of these will be posted on GitHub[^1], however they will always be posted in our Discord server.&lt;/p&gt; 
&lt;p&gt;[^1]: We usually only post announcements on GitHub in the case of a serious incident. In that case, you'll likely see it at the top of this README file.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/is-a-dev-830872854677422150"&gt;&lt;img alt="Discord Server" src="https://invidget.api.hrsn.dev/is-a-dev-830872854677422150" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Register&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;If you want a visual guide, check out &lt;a href="https://wdh.gg/tX3ghge"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/is-a-dev/register/fork"&gt;Fork&lt;/a&gt; this repository.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.is-a.dev"&gt;Read the documentation&lt;/a&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;If you are applying for NS records please read &lt;a href="https://raw.githubusercontent.com/is-a-dev/register/main/#ns-records"&gt;this&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Your pull request will be reviewed and merged. &lt;em&gt;Keep an eye on it in case changes are needed!&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;After the pull request is merged, your DNS records should be published with-in a few minutes.&lt;/li&gt; 
 &lt;li&gt;Enjoy your new &lt;code&gt;.is-a.dev&lt;/code&gt; subdomain! Please consider leaving us a star ‚≠êÔ∏è to help support us!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;NS Records&lt;/h3&gt; 
&lt;p&gt;When applying for NS records, please be aware we already support a &lt;a href="https://docs.is-a.dev/faq/#which-dns-record-types-are-supported"&gt;wide range of DNS records&lt;/a&gt;, so you likely do not need them.&lt;/p&gt; 
&lt;p&gt;In your PR, please explain why you need NS records, including examples, to help mitigate potential abuse. Refer to the &lt;a href="https://docs.is-a.dev/faq/#who-can-use-ns-records"&gt;FAQ&lt;/a&gt; for guidelines on allowed usage.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Pull requests adding NS records without sufficient reasoning will be closed.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Also see: &lt;a href="https://docs.is-a.dev/faq/#why-are-ns-records-restricted"&gt;Why are NS records restricted?&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Report Abuse&lt;/h2&gt; 
&lt;p&gt;If you find any subdomains being used for abusive purposes, please report them by &lt;a href="https://github.com/is-a-dev/register/issues/new?assignees=&amp;amp;labels=report-abuse&amp;amp;projects=&amp;amp;template=report-abuse.md&amp;amp;title=Report+abuse"&gt;creating an issue&lt;/a&gt; with the relevant evidence.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;We are proud to announce that we are supported by Cloudflare's &lt;a href="https://www.cloudflare.com/lp/project-alexandria"&gt;Project Alexandria&lt;/a&gt; sponsorship program. We would not be able to operate without their help! üíñ&lt;/p&gt; 
&lt;a href="https://www.cloudflare.com"&gt; &lt;img alt="Cloudflare Logo" src="https://raw.githubusercontent.com/is-a-dev/register/main/media/cloudflare.png" height="96" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>ultralytics/ultralytics</title>
      <link>https://github.com/ultralytics/ultralytics</link>
      <description>&lt;p&gt;Ultralytics YOLO üöÄ&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://www.ultralytics.com/events/yolovision?utm_source=github&amp;amp;utm_medium=org&amp;amp;utm_campaign=yv25_event" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.ultralytics.com/zh/"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ko/"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ja/"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ru/"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/de/"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/fr/"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/pt/"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/tr/"&gt;T√ºrk√ße&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/vi/"&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ar/"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;a href="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="Ultralytics CI" /&gt;&lt;/a&gt; 
  &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; 
  &lt;a href="https://zenodo.org/badge/latestdoi/264818686"&gt;&lt;img src="https://zenodo.org/badge/264818686.svg?sanitize=true" alt="Ultralytics YOLO Citation" /&gt;&lt;/a&gt; 
  &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img alt="Ultralytics Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://community.ultralytics.com/"&gt;&lt;img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;amp;logo=discourse&amp;amp;label=Forums&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;&lt;img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;amp;logo=reddit&amp;amp;logoColor=white&amp;amp;label=Reddit&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;br /&gt; 
  &lt;a href="https://console.paperspace.com/github/ultralytics/ultralytics"&gt;&lt;img src="https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true" alt="Run Ultralytics on Gradient" /&gt;&lt;/a&gt; 
  &lt;a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Ultralytics In Colab" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.kaggle.com/models/ultralytics/yolo11"&gt;&lt;img src="https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true" alt="Open Ultralytics In Kaggle" /&gt;&lt;/a&gt; 
  &lt;a href="https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb"&gt;&lt;img src="https://mybinder.org/badge_logo.svg?sanitize=true" alt="Open Ultralytics In Binder" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://www.ultralytics.com/"&gt;Ultralytics&lt;/a&gt; creates cutting-edge, state-of-the-art (SOTA) &lt;a href="https://www.ultralytics.com/yolo"&gt;YOLO models&lt;/a&gt; built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are &lt;strong&gt;fast&lt;/strong&gt;, &lt;strong&gt;accurate&lt;/strong&gt;, and &lt;strong&gt;easy to use&lt;/strong&gt;. They excel at &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;object detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;tracking&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;instance segmentation&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;image classification&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;pose estimation&lt;/a&gt; tasks.&lt;/p&gt; 
&lt;p&gt;Find detailed documentation in the &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;. Get support via &lt;a href="https://github.com/ultralytics/ultralytics/issues/new/choose"&gt;GitHub Issues&lt;/a&gt;. Join discussions on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Request an Enterprise License for commercial use at &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/models/yolo11/" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png" alt="YOLO11 performance plots" /&gt; &lt;/a&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="2%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="2%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="2%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="2%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="2%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="2%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="2%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìÑ Documentation&lt;/h2&gt; 
&lt;p&gt;See below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Install&lt;/summary&gt; 
 &lt;p&gt;Install the &lt;code&gt;ultralytics&lt;/code&gt; package, including all &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/pyproject.toml"&gt;requirements&lt;/a&gt;, in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment with &lt;a href="https://pytorch.org/get-started/locally/"&gt;&lt;strong&gt;PyTorch&amp;gt;=1.8&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/v/ultralytics?logo=pypi&amp;amp;logoColor=white" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ultralytics?logo=python&amp;amp;logoColor=gold" alt="PyPI - Python Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install ultralytics
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For alternative installation methods, including &lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;Conda&lt;/a&gt;, &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;Docker&lt;/a&gt;, and building from source via Git, please consult the &lt;a href="https://docs.ultralytics.com/quickstart/"&gt;Quickstart Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge" alt="Conda Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&amp;amp;logo=docker" alt="Docker Image Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker" alt="Ultralytics Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Usage&lt;/summary&gt; 
 &lt;h3&gt;CLI&lt;/h3&gt; 
 &lt;p&gt;You can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the &lt;code&gt;yolo&lt;/code&gt; command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Predict using a pretrained YOLO model (e.g., YOLO11n) on an image
yolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;yolo&lt;/code&gt; command supports various tasks and modes, accepting additional arguments like &lt;code&gt;imgsz=640&lt;/code&gt;. Explore the YOLO &lt;a href="https://docs.ultralytics.com/usage/cli/"&gt;CLI Docs&lt;/a&gt; for more examples.&lt;/p&gt; 
 &lt;h3&gt;Python&lt;/h3&gt; 
 &lt;p&gt;Ultralytics YOLO can also be integrated directly into your Python projects. It accepts the same &lt;a href="https://docs.ultralytics.com/usage/cfg/"&gt;configuration arguments&lt;/a&gt; as the CLI:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from ultralytics import YOLO

# Load a pretrained YOLO11n model
model = YOLO("yolo11n.pt")

# Train the model on the COCO8 dataset for 100 epochs
train_results = model.train(
    data="coco8.yaml",  # Path to dataset configuration file
    epochs=100,  # Number of training epochs
    imgsz=640,  # Image size for training
    device="cpu",  # Device to run on (e.g., 'cpu', 0, [0,1,2,3])
)

# Evaluate the model's performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model("path/to/image.jpg")  # Predict on an image
results[0].show()  # Display results

# Export the model to ONNX format for deployment
path = model.export(format="onnx")  # Returns the path to the exported model
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Discover more examples in the YOLO &lt;a href="https://docs.ultralytics.com/usage/python/"&gt;Python Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Models&lt;/h2&gt; 
&lt;p&gt;Ultralytics supports a wide range of YOLO models, from early versions like &lt;a href="https://docs.ultralytics.com/models/yolov3/"&gt;YOLOv3&lt;/a&gt; to the latest &lt;a href="https://docs.ultralytics.com/models/yolo11/"&gt;YOLO11&lt;/a&gt;. The tables below showcase YOLO11 models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/detect/coco/"&gt;COCO&lt;/a&gt; dataset for &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation&lt;/a&gt;. Additionally, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification&lt;/a&gt; models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt; dataset are available. &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;Tracking&lt;/a&gt; mode is compatible with all Detection, Segmentation, and Pose models. All &lt;a href="https://docs.ultralytics.com/models/"&gt;Models&lt;/a&gt; are automatically downloaded from the latest Ultralytics &lt;a href="https://github.com/ultralytics/assets/releases"&gt;release&lt;/a&gt; upon first use.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/tasks/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif" alt="Ultralytics YOLO supported tasks" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;details open&gt;
 &lt;summary&gt;Detection (COCO)&lt;/summary&gt; 
 &lt;p&gt;Explore the &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection Docs&lt;/a&gt; for usage examples. These models are trained on the &lt;a href="https://cocodataset.org/"&gt;COCO dataset&lt;/a&gt;, featuring 80 object classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt"&gt;YOLO11n&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;39.5&lt;/td&gt; 
    &lt;td&gt;56.1 ¬± 0.8&lt;/td&gt; 
    &lt;td&gt;1.5 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.6&lt;/td&gt; 
    &lt;td&gt;6.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt"&gt;YOLO11s&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;47.0&lt;/td&gt; 
    &lt;td&gt;90.0 ¬± 1.2&lt;/td&gt; 
    &lt;td&gt;2.5 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;9.4&lt;/td&gt; 
    &lt;td&gt;21.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt"&gt;YOLO11m&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;51.5&lt;/td&gt; 
    &lt;td&gt;183.2 ¬± 2.0&lt;/td&gt; 
    &lt;td&gt;4.7 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;20.1&lt;/td&gt; 
    &lt;td&gt;68.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt"&gt;YOLO11l&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;53.4&lt;/td&gt; 
    &lt;td&gt;238.6 ¬± 1.4&lt;/td&gt; 
    &lt;td&gt;6.2 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;25.3&lt;/td&gt; 
    &lt;td&gt;86.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt"&gt;YOLO11x&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;54.7&lt;/td&gt; 
    &lt;td&gt;462.8 ¬± 6.7&lt;/td&gt; 
    &lt;td&gt;11.3 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;56.9&lt;/td&gt; 
    &lt;td&gt;194.9&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values refer to single-model single-scale performance on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Segmentation (COCO)&lt;/summary&gt; 
 &lt;p&gt;Refer to the &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/segment/coco/"&gt;COCO-Seg&lt;/a&gt;, including 80 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;box&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;mask&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt"&gt;YOLO11n-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;38.9&lt;/td&gt; 
    &lt;td&gt;32.0&lt;/td&gt; 
    &lt;td&gt;65.9 ¬± 1.1&lt;/td&gt; 
    &lt;td&gt;1.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.9&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt"&gt;YOLO11s-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;46.6&lt;/td&gt; 
    &lt;td&gt;37.8&lt;/td&gt; 
    &lt;td&gt;117.6 ¬± 4.9&lt;/td&gt; 
    &lt;td&gt;2.9 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;10.1&lt;/td&gt; 
    &lt;td&gt;35.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt"&gt;YOLO11m-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;51.5&lt;/td&gt; 
    &lt;td&gt;41.5&lt;/td&gt; 
    &lt;td&gt;281.6 ¬± 1.2&lt;/td&gt; 
    &lt;td&gt;6.3 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;22.4&lt;/td&gt; 
    &lt;td&gt;123.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt"&gt;YOLO11l-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;53.4&lt;/td&gt; 
    &lt;td&gt;42.9&lt;/td&gt; 
    &lt;td&gt;344.2 ¬± 3.2&lt;/td&gt; 
    &lt;td&gt;7.8 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;27.6&lt;/td&gt; 
    &lt;td&gt;142.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt"&gt;YOLO11x-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;54.7&lt;/td&gt; 
    &lt;td&gt;43.8&lt;/td&gt; 
    &lt;td&gt;664.5 ¬± 3.2&lt;/td&gt; 
    &lt;td&gt;15.8 ¬± 0.7&lt;/td&gt; 
    &lt;td&gt;62.1&lt;/td&gt; 
    &lt;td&gt;319.0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Classification (ImageNet)&lt;/summary&gt; 
 &lt;p&gt;Consult the &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt;, covering 1000 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top1&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B) at 224&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt"&gt;YOLO11n-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;70.0&lt;/td&gt; 
    &lt;td&gt;89.4&lt;/td&gt; 
    &lt;td&gt;5.0 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;1.1 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;1.6&lt;/td&gt; 
    &lt;td&gt;0.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-cls.pt"&gt;YOLO11s-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;75.4&lt;/td&gt; 
    &lt;td&gt;92.7&lt;/td&gt; 
    &lt;td&gt;7.9 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;1.3 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;5.5&lt;/td&gt; 
    &lt;td&gt;1.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt"&gt;YOLO11m-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;93.9&lt;/td&gt; 
    &lt;td&gt;17.2 ¬± 0.4&lt;/td&gt; 
    &lt;td&gt;2.0 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
    &lt;td&gt;5.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt"&gt;YOLO11l-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;78.3&lt;/td&gt; 
    &lt;td&gt;94.3&lt;/td&gt; 
    &lt;td&gt;23.2 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;2.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;12.9&lt;/td&gt; 
    &lt;td&gt;6.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt"&gt;YOLO11x-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;94.9&lt;/td&gt; 
    &lt;td&gt;41.4 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;3.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;13.7&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;acc&lt;/strong&gt; values represent model accuracy on the &lt;a href="https://www.image-net.org/"&gt;ImageNet&lt;/a&gt; dataset validation set. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over ImageNet val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Pose (COCO)&lt;/summary&gt; 
 &lt;p&gt;See the &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO-Pose&lt;/a&gt;, focusing on the 'person' class.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt"&gt;YOLO11n-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;50.0&lt;/td&gt; 
    &lt;td&gt;81.0&lt;/td&gt; 
    &lt;td&gt;52.4 ¬± 0.5&lt;/td&gt; 
    &lt;td&gt;1.7 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.9&lt;/td&gt; 
    &lt;td&gt;7.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-pose.pt"&gt;YOLO11s-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;58.9&lt;/td&gt; 
    &lt;td&gt;86.3&lt;/td&gt; 
    &lt;td&gt;90.5 ¬± 0.6&lt;/td&gt; 
    &lt;td&gt;2.6 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;9.9&lt;/td&gt; 
    &lt;td&gt;23.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt"&gt;YOLO11m-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;64.9&lt;/td&gt; 
    &lt;td&gt;89.4&lt;/td&gt; 
    &lt;td&gt;187.3 ¬± 0.8&lt;/td&gt; 
    &lt;td&gt;4.9 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;20.9&lt;/td&gt; 
    &lt;td&gt;71.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-pose.pt"&gt;YOLO11l-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;66.1&lt;/td&gt; 
    &lt;td&gt;89.9&lt;/td&gt; 
    &lt;td&gt;247.7 ¬± 1.1&lt;/td&gt; 
    &lt;td&gt;6.4 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;26.2&lt;/td&gt; 
    &lt;td&gt;90.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt"&gt;YOLO11x-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;69.5&lt;/td&gt; 
    &lt;td&gt;91.1&lt;/td&gt; 
    &lt;td&gt;488.0 ¬± 13.9&lt;/td&gt; 
    &lt;td&gt;12.1 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;58.8&lt;/td&gt; 
    &lt;td&gt;203.3&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO Keypoints val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Oriented Bounding Boxes (DOTAv1)&lt;/summary&gt; 
 &lt;p&gt;Check the &lt;a href="https://docs.ultralytics.com/tasks/obb/"&gt;OBB Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/"&gt;DOTAv1&lt;/a&gt;, including 15 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;test&lt;br /&gt;50&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-obb.pt"&gt;YOLO11n-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;78.4&lt;/td&gt; 
    &lt;td&gt;117.6 ¬± 0.8&lt;/td&gt; 
    &lt;td&gt;4.4 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.7&lt;/td&gt; 
    &lt;td&gt;17.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-obb.pt"&gt;YOLO11s-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;219.4 ¬± 4.0&lt;/td&gt; 
    &lt;td&gt;5.1 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;9.7&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-obb.pt"&gt;YOLO11m-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;80.9&lt;/td&gt; 
    &lt;td&gt;562.8 ¬± 2.9&lt;/td&gt; 
    &lt;td&gt;10.1 ¬± 0.4&lt;/td&gt; 
    &lt;td&gt;20.9&lt;/td&gt; 
    &lt;td&gt;183.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-obb.pt"&gt;YOLO11l-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;81.0&lt;/td&gt; 
    &lt;td&gt;712.5 ¬± 5.0&lt;/td&gt; 
    &lt;td&gt;13.5 ¬± 0.6&lt;/td&gt; 
    &lt;td&gt;26.2&lt;/td&gt; 
    &lt;td&gt;232.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-obb.pt"&gt;YOLO11x-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;81.3&lt;/td&gt; 
    &lt;td&gt;1408.6 ¬± 7.7&lt;/td&gt; 
    &lt;td&gt;28.6 ¬± 1.0&lt;/td&gt; 
    &lt;td&gt;58.8&lt;/td&gt; 
    &lt;td&gt;520.2&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;test&lt;/sup&gt;&lt;/strong&gt; values are for single-model multiscale performance on the &lt;a href="https://captain-whu.github.io/DOTA/dataset.html"&gt;DOTAv1 test set&lt;/a&gt;. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml device=0 split=test&lt;/code&gt; and submit merged results to the &lt;a href="https://captain-whu.github.io/DOTA/evaluation.html"&gt;DOTA evaluation server&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10"&gt;DOTAv1 val images&lt;/a&gt; using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üß© Integrations&lt;/h2&gt; 
&lt;p&gt;Our key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/roboflow/"&gt;Roboflow&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/integrations/openvino/"&gt;Intel OpenVINO&lt;/a&gt;, can optimize your AI workflow. Explore more at &lt;a href="https://docs.ultralytics.com/integrations/"&gt;Ultralytics Integrations&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/integrations/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png" alt="Ultralytics active learning integrations" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.ultralytics.com/hub"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png" width="10%" alt="Ultralytics HUB logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png" width="10%" alt="Weights &amp;amp; Biases logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png" width="10%" alt="Comet ML logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png" width="10%" alt="Neural Magic logo" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Ultralytics HUB üåü&lt;/th&gt; 
   &lt;th align="center"&gt;Weights &amp;amp; Biases&lt;/th&gt; 
   &lt;th align="center"&gt;Comet&lt;/th&gt; 
   &lt;th align="center"&gt;Neural Magic&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Streamline YOLO workflows: Label, train, and deploy effortlessly with &lt;a href="https://hub.ultralytics.com/"&gt;Ultralytics HUB&lt;/a&gt;. Try now!&lt;/td&gt; 
   &lt;td align="center"&gt;Track experiments, hyperparameters, and results with &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;.&lt;/td&gt; 
   &lt;td align="center"&gt;Free forever, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt; lets you save YOLO models, resume training, and interactively visualize predictions.&lt;/td&gt; 
   &lt;td align="center"&gt;Run YOLO inference up to 6x faster with &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt;Neural Magic DeepSparse&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üåü Ultralytics HUB&lt;/h2&gt; 
&lt;p&gt;Experience seamless AI with &lt;a href="https://hub.ultralytics.com/"&gt;Ultralytics HUB&lt;/a&gt;, the all-in-one platform for data visualization, training YOLO models, and deployment‚Äîno coding required. Transform images into actionable insights and bring your AI visions to life effortlessly using our cutting-edge platform and user-friendly &lt;a href="https://www.ultralytics.com/app-install"&gt;Ultralytics App&lt;/a&gt;. Start your journey for &lt;strong&gt;Free&lt;/strong&gt; today!&lt;/p&gt; 
&lt;a href="https://www.ultralytics.com/hub" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png" alt="Ultralytics HUB preview image" /&gt;&lt;/a&gt; 
&lt;h2&gt;ü§ù Contribute&lt;/h2&gt; 
&lt;p&gt;We thrive on community collaboration! Ultralytics YOLO wouldn't be the SOTA framework it is without contributions from developers like you. Please see our &lt;a href="https://docs.ultralytics.com/help/contributing/"&gt;Contributing Guide&lt;/a&gt; to get started. We also welcome your feedback‚Äîshare your experience by completing our &lt;a href="https://www.ultralytics.com/survey?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=Survey"&gt;Survey&lt;/a&gt;. A huge &lt;strong&gt;Thank You&lt;/strong&gt; üôè to everyone who contributes!&lt;/p&gt; 
&lt;!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=1280 --&gt; 
&lt;p&gt;&lt;a href="https://github.com/ultralytics/ultralytics/graphs/contributors"&gt;&lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png" alt="Ultralytics open-source contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We look forward to your contributions to help make the Ultralytics ecosystem even better!&lt;/p&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;Ultralytics offers two licensing options to suit different needs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AGPL-3.0 License&lt;/strong&gt;: This &lt;a href="https://opensource.org/license"&gt;OSI-approved&lt;/a&gt; open-source license is perfect for students, researchers, and enthusiasts. It encourages open collaboration and knowledge sharing. See the &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for full details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ultralytics Enterprise License&lt;/strong&gt;: Designed for commercial use, this license allows for the seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. If your use case involves commercial deployment, please contact us via &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìû Contact&lt;/h2&gt; 
&lt;p&gt;For bug reports and feature requests related to Ultralytics software, please visit &lt;a href="https://github.com/ultralytics/ultralytics/issues"&gt;GitHub Issues&lt;/a&gt;. For questions, discussions, and community support, join our active communities on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;. We're here to help with all things Ultralytics!&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="3%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="3%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="3%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="3%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="3%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="3%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="3%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>netdata/netdata</title>
      <link>https://github.com/netdata/netdata</link>
      <description>&lt;p&gt;The fastest path to AI-powered full stack observability, even for lean teams.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://www.netdata.cloud#gh-light-mode-only"&gt; &lt;img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_light.png" alt="Netdata" width="300" /&gt; &lt;/a&gt; &lt;a href="https://www.netdata.cloud#gh-dark-mode-only"&gt; &lt;img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_dark.png" alt="Netdata" width="300" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt;X-Ray Vision for your infrastructure!&lt;/h3&gt; 
&lt;h4 align="center"&gt;Every Metric, Every Second. No BS.&lt;/h4&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/netdata/netdata/"&gt;&lt;img src="https://img.shields.io/github/stars/netdata/netdata?style=social" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://app.netdata.cloud/spaces/netdata-demo?utm_campaign=github_readme_demo_badge"&gt;&lt;img src="https://img.shields.io/badge/Live%20Demo-green" alt="Live Demo" /&gt;&lt;/a&gt; &lt;a href="https://github.com/netdata/netdata/releases/latest"&gt;&lt;img src="https://img.shields.io/github/release/netdata/netdata.svg?sanitize=true" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/netdata/netdata-nightlies/releases/latest"&gt;&lt;img src="https://img.shields.io/github/release/netdata/netdata-nightlies.svg?sanitize=true" alt="Latest nightly build" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://community.netdata.cloud"&gt;&lt;img alt="Discourse topics" src="https://img.shields.io/discourse/topics?server=https%3A%2F%2Fcommunity.netdata.cloud%2F&amp;amp;logo=discourse&amp;amp;label=discourse%20forum" /&gt;&lt;/a&gt; &lt;a href="https://github.com/netdata/netdata/discussions"&gt;&lt;img alt="GitHub Discussions" src="https://img.shields.io/github/discussions/netdata/netdata?logo=github&amp;amp;label=github%20discussions" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://bestpractices.coreinfrastructure.org/projects/2231"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/2231/badge" alt="CII Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://scan.coverity.com/projects/netdata-netdata?tab=overview"&gt;&lt;img alt="Coverity Scan" src="https://img.shields.io/coverity/scan/netdata" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://registry.my-netdata.io/#menu_netdata_submenu_registry"&gt;&lt;img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&amp;amp;dimensions=persons&amp;amp;label=user%20base&amp;amp;units=M&amp;amp;value_color=blue&amp;amp;precision=2&amp;amp;divide=1000000&amp;amp;options=unaligned&amp;amp;tier=1&amp;amp;v44" alt="User base" /&gt;&lt;/a&gt; &lt;a href="https://registry.my-netdata.io/#menu_netdata_submenu_registry"&gt;&lt;img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&amp;amp;dimensions=machines&amp;amp;label=servers%20monitored&amp;amp;units=M&amp;amp;divide=1000000&amp;amp;value_color=orange&amp;amp;precision=2&amp;amp;options=unaligned&amp;amp;tier=1&amp;amp;v44" alt="Servers monitored" /&gt;&lt;/a&gt; &lt;a href="https://registry.my-netdata.io/#menu_netdata_submenu_registry"&gt;&lt;img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_sessions&amp;amp;label=sessions%20served&amp;amp;units=M&amp;amp;value_color=yellowgreen&amp;amp;precision=2&amp;amp;divide=1000000&amp;amp;options=unaligned&amp;amp;tier=1&amp;amp;v44" alt="Sessions served" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/netdata/netdata"&gt;&lt;img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=dockerhub.pulls_sum&amp;amp;divide=1000000&amp;amp;precision=1&amp;amp;units=M&amp;amp;label=docker+hub+pulls&amp;amp;options=unaligned&amp;amp;tier=1&amp;amp;v44" alt="Docker Hub pulls" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&lt;b&gt;Visit our &lt;a href="https://www.netdata.cloud"&gt;Home Page&lt;/a&gt;&lt;/b&gt;&lt;/p&gt; 
&lt;hr class="solid" /&gt; 
&lt;p&gt;MENU: &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/#who-we-are"&gt;WHO WE ARE&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/#key-features"&gt;KEY FEATURES&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/#getting-started"&gt;GETTING STARTED&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/#how-it-works"&gt;HOW IT WORKS&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/#faq"&gt;FAQ&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/#book-documentation"&gt;DOCS&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/#tada-community"&gt;COMMUNITY&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/#pray-contribute"&gt;CONTRIBUTE&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/#scroll-license"&gt;LICENSE&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] People &lt;strong&gt;get addicted to Netdata.&lt;/strong&gt; Once you use it on your systems, &lt;em&gt;there's no going back.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/Platforms-Linux%20%7C%20macOS%20%7C%20FreeBSD%20%7C%20Windows-blue" alt="Platforms" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;WHO WE ARE&lt;/h2&gt; 
&lt;p&gt;Netdata is an open-source, real-time infrastructure monitoring platform. Monitor, detect, and act across your entire infrastructure.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Core Advantages:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Instant Insights&lt;/strong&gt; ‚Äì With Netdata you can access per-second metrics and visualizations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero Configuration&lt;/strong&gt; ‚Äì You can deploy immediately without complex setup.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ML-Powered&lt;/strong&gt; ‚Äì You can detect anomalies, predict issues, and automate analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient&lt;/strong&gt; ‚Äì You can monitor with minimal resource usage and maximum scalability.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure &amp;amp; Distributed&lt;/strong&gt; ‚Äì You can keep your data local with no central collection needed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With Netdata, you get real-time, per-second updates. Clear &lt;strong&gt;insights at a glance&lt;/strong&gt;, no complexity.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;All heroes have a great origin story. Click to discover ours.&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;In 2013, at the company where Costa Tsaousis was COO, a significant percentage of their cloud-based transactions failed silently, severely impacting business performance.&lt;/p&gt; 
 &lt;p&gt;Costa and his team tried every troubleshooting tool available at the time. None could identify the root cause. As Costa later wrote:&lt;/p&gt; 
 &lt;p&gt;‚Äú&lt;em&gt;I couldn‚Äôt believe that monitoring systems provide so few metrics and with such low resolution, scale so badly, and cost so much to run.&lt;/em&gt;‚Äù&lt;/p&gt; 
 &lt;p&gt;Frustrated, he decided to build his own monitoring tool, starting from scratch.&lt;/p&gt; 
 &lt;p&gt;That decision led to countless late nights and weekends. It also sparked a fundamental shift in how infrastructure monitoring and troubleshooting are approached, both in method and in cost.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Most Energy-Efficient Monitoring Tool&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-dark-mode-only"&gt; &lt;img src="https://github.com/netdata/netdata/assets/139226121/7118757a-38fb-48d7-b12a-53e709a8e8c0" alt="Energy Efficiency" width="800" /&gt; &lt;/a&gt; &lt;a href="https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-light-mode-only"&gt; &lt;img src="https://github.com/netdata/netdata/assets/139226121/4f64cbb6-05e4-48e3-b7c0-d1b79e37e219" alt="Energy efficiency" width="800" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;According to the &lt;a href="https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf"&gt;University of Amsterdam study&lt;/a&gt;, Netdata is the most energy-efficient tool for monitoring Docker-based systems. The study also shows Netdata excels in CPU usage, RAM usage, and execution time compared to other monitoring solutions.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;What Makes It Unique&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Real-Time&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Per-second data collection and processing&lt;/td&gt; 
   &lt;td&gt;Works in a beat ‚Äì click and see results instantly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Zero-Configuration&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automatic detection and discovery&lt;/td&gt; 
   &lt;td&gt;Auto-discovers everything on the nodes it runs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ML-Powered&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Unsupervised anomaly detection&lt;/td&gt; 
   &lt;td&gt;Trains multiple ML models per metric at the edge&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Long-Term Retention&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;High-performance storage&lt;/td&gt; 
   &lt;td&gt;~0.5 bytes per sample with tiered storage for archiving&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Advanced Visualization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Rich, interactive dashboards&lt;/td&gt; 
   &lt;td&gt;Slice and dice data without query language&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Extreme Scalability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Native horizontal scaling&lt;/td&gt; 
   &lt;td&gt;Parent-Child centralization with multi-million samples/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Complete Visibility&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;From infrastructure to applications&lt;/td&gt; 
   &lt;td&gt;Simplifies operations and eliminates silos&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Edge-Based&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Processing at your premises&lt;/td&gt; 
   &lt;td&gt;Distributes code instead of centralizing data&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; Want to put Netdata to the test against Prometheus? Explore the &lt;a href="https://www.netdata.cloud/blog/netdata-vs-prometheus-2025/"&gt;full comparison&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Netdata Ecosystem&lt;/h2&gt; 
&lt;p&gt;This three-part architecture enables you to scale from single nodes to complex multi-cloud environments:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;License&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Netdata Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Core monitoring engine&lt;br /&gt;‚Ä¢ Handles collection, storage, ML, alerts, exports&lt;br /&gt;‚Ä¢ Runs on servers, cloud, K8s, IoT&lt;br /&gt;‚Ä¢ Zero production impact&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.gnu.org/licenses/gpl-3.0"&gt;GPL v3+&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Netdata Cloud&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Enterprise features&lt;br /&gt;‚Ä¢ User management, RBAC, horizontal scaling&lt;br /&gt;‚Ä¢ Centralized alerts&lt;br /&gt;‚Ä¢ Free community tier&lt;br /&gt;‚Ä¢ No metric storage centralization&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Netdata UI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Dashboards and visualizations&lt;br /&gt;‚Ä¢ Free to use&lt;br /&gt;‚Ä¢ Included in standard packages&lt;br /&gt;‚Ä¢ Latest version via CDN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://app.netdata.cloud/LICENSE.txt"&gt;NCUL1&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;What You Can Monitor&lt;/h2&gt; 
&lt;p&gt;With Netdata you can monitor all these components across platforms:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="right"&gt;Component&lt;/th&gt; 
   &lt;th align="center"&gt;Linux&lt;/th&gt; 
   &lt;th align="center"&gt;FreeBSD&lt;/th&gt; 
   &lt;th align="center"&gt;macOS&lt;/th&gt; 
   &lt;th align="center"&gt;Windows&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;System Resources&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;CPU, Memory and system shared resources&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Full&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;Disks, Mount points, Filesystems, RAID arrays&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Full&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Network&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;Network Interfaces, Protocols, Firewall, etc&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Full&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Hardware &amp;amp; Sensors&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;Fans, Temperatures, Controllers, GPUs, etc&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Full&lt;/td&gt; 
   &lt;td align="center"&gt;Some&lt;/td&gt; 
   &lt;td align="center"&gt;Some&lt;/td&gt; 
   &lt;td align="center"&gt;Some&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;O/S Services&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;Resources, Performance and Status&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;small&gt;&lt;br /&gt;&lt;code&gt;systemd&lt;/code&gt;&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Processes&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;Resources, Performance, OOM, and more&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;System and Application &lt;strong&gt;Logs&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;small&gt;&lt;br /&gt;&lt;code&gt;systemd&lt;/code&gt;-journal&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;small&gt;&lt;br /&gt;&lt;code&gt;Windows Event Log&lt;/code&gt;, &lt;code&gt;ETW&lt;/code&gt;&lt;/small&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Network Connections&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;Live TCP and UDP sockets per PID&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Containers&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;Docker/containerd, LXC/LXD, Kubernetes, etc&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;VMs&lt;/strong&gt; (from the host)&lt;small&gt;&lt;br /&gt;KVM, qemu, libvirt, Proxmox, etc&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;small&gt;&lt;br /&gt;&lt;code&gt;cgroups&lt;/code&gt;&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;small&gt;&lt;br /&gt;&lt;code&gt;Hyper-V&lt;/code&gt;&lt;/small&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Synthetic Checks&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;Test APIs, TCP ports, Ping, Certificates, etc&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Packaged Applications&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;nginx, apache, postgres, redis, mongodb,&lt;br /&gt;and hundreds more&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Cloud Provider Infrastructure&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;AWS, GCP, Azure, and more&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;&lt;strong&gt;Custom Applications&lt;/strong&gt;&lt;small&gt;&lt;br /&gt;OpenMetrics, StatsD and soon OpenTelemetry&lt;/small&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
   &lt;td align="center"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;On Linux, you can continuously monitor all kernel features and hardware sensors for errors, including Intel/AMD/Nvidia GPUs, PCI AER, RAM EDAC, IPMI, S.M.A.R.T, Intel RAPL, NVMe, fans, power supplies, and voltage readings.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;You can install Netdata on all major operating systems. To begin:&lt;/p&gt; 
&lt;h3&gt;1. Install Netdata&lt;/h3&gt; 
&lt;p&gt;Choose your platform and follow the installation guide:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://learn.netdata.cloud/docs/installing/one-line-installer-for-all-linux-systems"&gt;Linux Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.netdata.cloud/docs/installing/macos"&gt;macOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.netdata.cloud/docs/installing/freebsd"&gt;FreeBSD&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.netdata.cloud/docs/netdata-agent/installation/windows"&gt;Windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netdata/netdata/master/packaging/docker/README.md"&gt;Docker Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.netdata.cloud/docs/installation/install-on-specific-environments/kubernetes"&gt;Kubernetes Setup&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You can access the Netdata UI at &lt;code&gt;http://localhost:19999&lt;/code&gt; (or &lt;code&gt;http://NODE:19999&lt;/code&gt; if remote).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;2. Configure Collectors&lt;/h3&gt; 
&lt;p&gt;Netdata auto-discovers most metrics, but you can manually configure some collectors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://learn.netdata.cloud/docs/data-collection/"&gt;All collectors&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.netdata.cloud/docs/data-collection/monitor-anything/networking/snmp"&gt;SNMP monitoring&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3. Configure Alerts&lt;/h3&gt; 
&lt;p&gt;You can use hundreds of built-in alerts and integrate with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;email&lt;/code&gt;, &lt;code&gt;Slack&lt;/code&gt;, &lt;code&gt;Telegram&lt;/code&gt;, &lt;code&gt;PagerDuty&lt;/code&gt;, &lt;code&gt;Discord&lt;/code&gt;, &lt;code&gt;Microsoft Teams&lt;/code&gt;, and more.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; Email alerts work by default if there's a configured MTA.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;4. Configure Parents&lt;/h3&gt; 
&lt;p&gt;You can centralize dashboards, alerts, and storage with Netdata Parents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://learn.netdata.cloud/docs/streaming/streaming-configuration-reference"&gt;Streaming Reference&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; You can use Netdata Parents for central dashboards, longer retention, and alert configuration.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;5. Connect to Netdata Cloud&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://app.netdata.cloud/sign-in"&gt;Sign in to Netdata Cloud&lt;/a&gt; and connect your nodes for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Access from anywhere&lt;/li&gt; 
 &lt;li&gt;Horizontal scalability and multi-node dashboards&lt;/li&gt; 
 &lt;li&gt;UI configuration for alerts and data collection&lt;/li&gt; 
 &lt;li&gt;Role-based access control&lt;/li&gt; 
 &lt;li&gt;Free tier available&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; Netdata Cloud is optional. Your data stays in your infrastructure.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Live Demo Sites&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;b&gt;See Netdata in action&lt;/b&gt;&lt;br /&gt; &lt;a href="https://frankfurt.netdata.rocks"&gt;&lt;b&gt;FRANKFURT&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://newyork.netdata.rocks"&gt;&lt;b&gt;NEWYORK&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://atlanta.netdata.rocks"&gt;&lt;b&gt;ATLANTA&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://sanfrancisco.netdata.rocks"&gt;&lt;b&gt;SANFRANCISCO&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://toronto.netdata.rocks"&gt;&lt;b&gt;TORONTO&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://singapore.netdata.rocks"&gt;&lt;b&gt;SINGAPORE&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://bangalore.netdata.rocks"&gt;&lt;b&gt;BANGALORE&lt;/b&gt;&lt;/a&gt; &lt;br /&gt; &lt;i&gt;These demo clusters run with default configuration and show real monitoring data.&lt;/i&gt; &lt;br /&gt; &lt;i&gt;Choose the instance closest to you for the best performance.&lt;/i&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;p&gt;With Netdata you can run a modular pipeline for metrics collection, processing, and visualization.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TB
  A[Netdata Agent]:::mainNode
  A1(Collect):::green --&amp;gt; A
  A2(Store):::green --&amp;gt; A
  A3(Learn):::green --&amp;gt; A
  A4(Detect):::green --&amp;gt; A
  A5(Check):::green --&amp;gt; A
  A6(Stream):::green --&amp;gt; A
  A7(Archive):::green --&amp;gt; A
  A8(Query):::green --&amp;gt; A
  A9(Score):::green --&amp;gt; A

  classDef green fill:#bbf3bb,stroke:#333,stroke-width:1px,color:#000
  classDef mainNode fill:#f0f0f0,stroke:#333,stroke-width:1px,color:#333
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With each Agent you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Collect&lt;/strong&gt; ‚Äì Gather metrics from systems, containers, apps, logs, APIs, and synthetic checks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Store&lt;/strong&gt; ‚Äì Save metrics to a high-efficiency, tiered time-series database.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Learn&lt;/strong&gt; ‚Äì Train ML models per metric using recent behavior.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Detect&lt;/strong&gt; ‚Äì Identify anomalies using trained ML models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check&lt;/strong&gt; ‚Äì Evaluate metrics against pre-set or custom alert rules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stream&lt;/strong&gt; ‚Äì Send metrics to Netdata Parents in real time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archive&lt;/strong&gt; ‚Äì Export metrics to Prometheus, InfluxDB, OpenTSDB, Graphite, and others.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Query&lt;/strong&gt; ‚Äì Access metrics via an API for dashboards or third-party tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Score&lt;/strong&gt; ‚Äì Use a scoring engine to find patterns and correlations across metrics.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; Learn more: &lt;a href="https://learn.netdata.cloud/docs/netdata-agent/#distributed-observability-pipeline"&gt;Netdata's architecture&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Agent Capabilities&lt;/h2&gt; 
&lt;p&gt;With the Netdata Agent, you can use these core capabilities out-of-the-box:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Capability&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensive Collection&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ 800+ integrations&lt;br /&gt;‚Ä¢ Systems, containers, VMs, hardware sensors&lt;br /&gt;‚Ä¢ OpenMetrics, StatsD, and logs&lt;br /&gt;‚Ä¢ OpenTelemetry support coming soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Performance &amp;amp; Precision&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Per-second collection&lt;br /&gt;‚Ä¢ Real-time visualization with 1-second latency&lt;br /&gt;‚Ä¢ High-resolution metrics&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Edge-Based ML&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ ML models trained at the edge&lt;br /&gt;‚Ä¢ Automatic anomaly detection per metric&lt;br /&gt;‚Ä¢ Pattern recognition based on historical behavior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Advanced Log Management&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Direct systemd-journald and Windows Event Log integration&lt;br /&gt;‚Ä¢ Process logs at the edge&lt;br /&gt;‚Ä¢ Rich log visualization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Observability Pipeline&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Parent-Child relationships&lt;br /&gt;‚Ä¢ Flexible centralization&lt;br /&gt;‚Ä¢ Multi-level replication and retention&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Automated Visualization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ NIDL data model&lt;br /&gt;‚Ä¢ Auto-generated dashboards&lt;br /&gt;‚Ä¢ No query language needed&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Smart Alerting&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Pre-configured alerts&lt;br /&gt;‚Ä¢ Multiple notification methods&lt;br /&gt;‚Ä¢ Proactive detection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Low Maintenance&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Auto-detection&lt;br /&gt;‚Ä¢ Zero-touch ML&lt;br /&gt;‚Ä¢ Easy scalability&lt;br /&gt;‚Ä¢ CI/CD friendly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open &amp;amp; Extensible&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Modular architecture&lt;br /&gt;‚Ä¢ Easy to customize&lt;br /&gt;‚Ä¢ Integrates with existing tools&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;CNCF Membership&lt;/h2&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/white/cncf-white.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg" /&gt; 
  &lt;img alt="CNCF Logo" src="https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg?sanitize=true" width="300" /&gt; 
 &lt;/picture&gt; &lt;br /&gt; Netdata actively supports and is a member of the Cloud Native Computing Foundation (CNCF).&lt;br /&gt; It is one of the most starred projects in the &lt;a href="https://landscape.cncf.io/?item=observability-and-analysis--observability--netdata"&gt;CNCF landscape&lt;/a&gt;. &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Is Netdata secure?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;Yes. Netdata follows &lt;a href="https://bestpractices.coreinfrastructure.org/en/projects/2231"&gt;OpenSSF best practices&lt;/a&gt;, has a security-first design, and is regularly audited by the community.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://learn.netdata.cloud/docs/security-and-privacy-design"&gt;Security design&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/netdata/netdata/security"&gt;Security policies and advisories&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Does Netdata use a lot of resources?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;No. Even with ML and per-second metrics, Netdata uses minimal resources.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;~5% CPU and 150MiB RAM by default on production systems&lt;/li&gt; 
  &lt;li&gt;&amp;lt;1% CPU and ~100MiB RAM when ML and alerts are disabled and using ephemeral storage&lt;/li&gt; 
  &lt;li&gt;Parents scale to millions of metrics per second with appropriate hardware&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;You can use the &lt;strong&gt;Netdata Monitoring&lt;/strong&gt; section in the dashboard to inspect its resource usage.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;How much data retention is possible?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;As much as your disk allows.&lt;/p&gt; 
 &lt;p&gt;With Netdata you can use tiered retention:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Tier 0: per-second resolution&lt;/li&gt; 
  &lt;li&gt;Tier 1: per-minute resolution&lt;/li&gt; 
  &lt;li&gt;Tier 2: per-hour resolution&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These are queried automatically based on the zoom level.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Can Netdata scale to many servers?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;Yes. With Netdata you can:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Scale horizontally with many Agents&lt;/li&gt; 
  &lt;li&gt;Scale vertically with powerful Parents&lt;/li&gt; 
  &lt;li&gt;Scale infinitely via Netdata Cloud&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;You can use Netdata Cloud to merge many independent infrastructures into one logical view.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Is disk I/O a concern?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;No. Netdata minimizes disk usage:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Metrics are flushed to disk every 17 minutes, spread out evenly&lt;/li&gt; 
  &lt;li&gt;Uses direct I/O and compression (ZSTD)&lt;/li&gt; 
  &lt;li&gt;Can run entirely in RAM or stream to a Parent&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;You can use &lt;code&gt;alloc&lt;/code&gt; or &lt;code&gt;ram&lt;/code&gt; mode for no disk writes.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;How is Netdata different from Prometheus + Grafana?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;With Netdata you get a complete monitoring solution‚Äînot just tools.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;No manual setup or dashboards needed&lt;/li&gt; 
  &lt;li&gt;Built-in ML, alerts, dashboards, and correlations&lt;/li&gt; 
  &lt;li&gt;More efficient and easier to deploy&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;a href="https://blog.netdata.cloud/netdata-vs-prometheus-performance-analysis/"&gt;Performance comparison&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;How is Netdata different from commercial SaaS tools?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;With Netdata you can store all metrics on your infrastructure‚Äîno sampling, no aggregation, no loss.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;High-resolution metrics by default&lt;/li&gt; 
  &lt;li&gt;ML per metric, not shared models&lt;/li&gt; 
  &lt;li&gt;Unlimited scalability without skyrocketing cost&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Can Netdata run alongside Nagios, Zabbix, etc.?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;Yes. You can use Netdata together with traditional tools.&lt;/p&gt; 
 &lt;p&gt;With Netdata you get:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Real-time, high-resolution monitoring&lt;/li&gt; 
  &lt;li&gt;Zero configuration and auto-generated dashboards&lt;/li&gt; 
  &lt;li&gt;Anomaly detection and advanced visualization&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;What if I feel overwhelmed?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;You can start small:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Use the dashboard's table of contents and search&lt;/li&gt; 
  &lt;li&gt;Explore anomaly scoring ("AR" toggle)&lt;/li&gt; 
  &lt;li&gt;Create custom dashboards in Netdata Cloud&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;a href="https://learn.netdata.cloud/guides"&gt;Docs and guides&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Do I have to use Netdata Cloud?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;No. Netdata Cloud is optional.&lt;/p&gt; 
 &lt;p&gt;Netdata works without it, but with Cloud you can:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Access remotely with SSO&lt;/li&gt; 
  &lt;li&gt;Save dashboard customizations&lt;/li&gt; 
  &lt;li&gt;Configure alerts centrally&lt;/li&gt; 
  &lt;li&gt;Collaborate with role-based access&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;What telemetry does Netdata collect?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;Anonymous telemetry helps improve the product. You can disable it:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Add &lt;code&gt;--disable-telemetry&lt;/code&gt; to the installer, or&lt;/li&gt; 
  &lt;li&gt;Create &lt;code&gt;/etc/netdata/.opt-out-from-anonymous-statistics&lt;/code&gt; and restart Netdata&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Telemetry helps us understand usage, not track users. No private data is collected.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Who uses Netdata?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;You'll join users including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Major companies (Amazon, ABN AMRO Bank, Facebook, Google, IBM, Intel, Netflix, Samsung)&lt;/li&gt; 
  &lt;li&gt;Universities (NYU, Columbia, Seoul National, UCL)&lt;/li&gt; 
  &lt;li&gt;Government organizations worldwide&lt;/li&gt; 
  &lt;li&gt;Infrastructure-intensive organizations&lt;/li&gt; 
  &lt;li&gt;Technology operators&lt;/li&gt; 
  &lt;li&gt;Startups and freelancers&lt;/li&gt; 
  &lt;li&gt;SysAdmins and DevOps professionals&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; Documentation&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://learn.netdata.cloud"&gt;Netdata Learn&lt;/a&gt; for full documentation and guides.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; Includes deployment, configuration, alerting, exporting, troubleshooting, and more.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üéâ&lt;/span&gt; Community&lt;/h2&gt; 
&lt;p&gt;Join the Netdata community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.com/invite/2mEmfW735j"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://community.netdata.cloud"&gt;Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/netdata/netdata/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; &lt;a href="https://github.com/netdata/.github/raw/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Follow us on: &lt;a href="https://twitter.com/netdatahq"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.reddit.com/r/netdata/"&gt;Reddit&lt;/a&gt; | &lt;a href="https://www.youtube.com/c/Netdata"&gt;YouTube&lt;/a&gt; | &lt;a href="https://www.linkedin.com/company/netdata-cloud/"&gt;LinkedIn&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üôè&lt;/span&gt; Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome your contributions.&lt;/p&gt; 
&lt;p&gt;Ways you help us stay sharp:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Share best practices and monitoring insights&lt;/li&gt; 
 &lt;li&gt;Report issues or missing features&lt;/li&gt; 
 &lt;li&gt;Improve documentation&lt;/li&gt; 
 &lt;li&gt;Develop new integrations or collectors&lt;/li&gt; 
 &lt;li&gt;Help users in forums and chats&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; &lt;a href="https://github.com/netdata/.github/raw/main/CONTRIBUTING.md"&gt;Contribution guide&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;span&gt;üìú&lt;/span&gt; License&lt;/h2&gt; 
&lt;p&gt;The Netdata ecosystem includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Netdata Agent&lt;/strong&gt; ‚Äì Open-source core (GPLv3+). &lt;strong&gt;Includes&lt;/strong&gt; data collection, storage, ML, alerting, APIs and &lt;strong&gt;redistributes&lt;/strong&gt; several other open-source tools and libraries. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/netdata/netdata/raw/master/LICENSE"&gt;Netdata Agent License&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/netdata/netdata/raw/master/REDISTRIBUTED.md"&gt;Netdata Agent Redistributed&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Netdata UI&lt;/strong&gt; ‚Äì Closed-source but free to use with Netdata Agent and Cloud. Delivered via CDN. It integrates third-party open-source components. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://app.netdata.cloud/LICENSE.txt"&gt;Netdata Cloud UI License&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://app.netdata.cloud/3D_PARTY_LICENSES.txt"&gt;Netdata UI third-party licenses&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Netdata Cloud&lt;/strong&gt; ‚Äì Closed-source, with free and paid tiers. Adds remote access, SSO, scalability.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google-gemini/cookbook</title>
      <link>https://github.com/google-gemini/cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the Gemini API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the Gemini API Cookbook&lt;/h1&gt; 
&lt;p&gt;This cookbook provides a structured learning path for using the Gemini API, focusing on hands-on tutorials and practical examples.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For comprehensive API documentation, visit &lt;a href="https://ai.google.dev/gemini-api/docs"&gt;ai.google.dev&lt;/a&gt;.&lt;/strong&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Navigating the Cookbook&lt;/h2&gt; 
&lt;p&gt;This cookbook is organized into two main categories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-gemini/cookbook/tree/main/quickstarts/"&gt;Quick Starts&lt;/a&gt;:&lt;/strong&gt; Step-by-step guides covering both introductory topics ("&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb"&gt;Get Started&lt;/a&gt;") and specific API features.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/google-gemini/cookbook/tree/main/examples/"&gt;Examples&lt;/a&gt;:&lt;/strong&gt; Practical use cases demonstrating how to combine multiple features.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We also showcase &lt;strong&gt;Demos&lt;/strong&gt; in separate repositories, illustrating end-to-end applications of the Gemini API. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;What's New?&lt;/h2&gt; 
&lt;p&gt;Here are the recent additions and updates to the Gemini API and the Cookbook:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini 2.5 models:&lt;/strong&gt; Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb"&gt;Get Started Guide&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_thinking.ipynb"&gt;thinking guide&lt;/a&gt; as they'll all be thinking ones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Imagen and Veo&lt;/strong&gt;: Get started with our media generation model with this brand new &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_Veo.ipynb"&gt;Veo guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_imagen.ipynb"&gt;Imagen guide&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini Robotics-ER 1.5&lt;/strong&gt;: Learn about this new Gemini model specifically for spatial understanding and reasoning for &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/gemini-robotics-er.ipynb"&gt;robotics applications&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lyria and TTS&lt;/strong&gt;: Get started with podcast and music generation with the &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_TTS.ipynb"&gt;TTS&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LyriaRealTime.ipynb"&gt;Lyria RealTime&lt;/a&gt; models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LiveAPI&lt;/strong&gt;: Get started with the &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LiveAPI.ipynb"&gt;multimodal Live API&lt;/a&gt; and unlock new interactivity with Gemini.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Recently Added Guides:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Image_out.ipynb"&gt;Image-out&lt;/a&gt;: Use Gemini's native image generation capabilities to edit images with high consistency or generate visual stories.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Grounding.ipynb"&gt;Grounding&lt;/a&gt;: Discover different ways to ground Gemini's answer using different tools, from Google Search to Youtube and the new &lt;strong&gt;url context&lt;/strong&gt; tool.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Batch_mode.ipynb"&gt;Batch-mode&lt;/a&gt;: Use Batch-mode to send large volume of non-time-sensitive requests to the model and get a 50% discount.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;1. Quick Starts&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://github.com/google-gemini/cookbook/tree/main/quickstarts/"&gt;quickstarts section&lt;/a&gt; contains step-by-step tutorials to get you started with Gemini and learn about its specific features.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To begin, you'll need:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A Google account.&lt;/li&gt; 
 &lt;li&gt;An API key (create one in &lt;a href="https://aistudio.google.com/app/apikey"&gt;Google AI Studio&lt;/a&gt;). &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We recommend starting with the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Authentication.ipynb"&gt;Authentication&lt;/a&gt;: Set up your API key for access.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb"&gt;&lt;strong&gt;Get started&lt;/strong&gt;&lt;/a&gt;: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input. &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Then, explore the other quickstarts tutorials to learn about individual features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LiveAPI.ipynb"&gt;Get started with Live API&lt;/a&gt;: Get started with the live API with this comprehensive overview of its capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_Veo.ipynb"&gt;Get started with Veo&lt;/a&gt;: Get started with our video generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_imagen.ipynb"&gt;Get started with Imagen&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Image_out.ipynb"&gt;Image-out&lt;/a&gt;: Get started with our image generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Search_Grounding.ipynb"&gt;Grounding&lt;/a&gt;: use Google Search for grounded responses&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Code_Execution.ipynb"&gt;Code execution&lt;/a&gt;: Generating and running Python code to solve complex tasks and even ouput graphs&lt;/li&gt; 
 &lt;li&gt;And &lt;a href="https://github.com/google-gemini/cookbook/tree/main/quickstarts/"&gt;many more&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;2. Examples (Practical Use Cases)&lt;/h2&gt; 
&lt;p&gt;These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Browser_as_a_tool.ipynb"&gt;Browser as a tool&lt;/a&gt;: Use a web browser for live and internal (intranet) web interactions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Book_illustration.ipynb"&gt;Illustrate a book&lt;/a&gt;: Use Gemini and Imagen to create illustration for an open-source book&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Animated_Story_Video_Generation_gemini.ipynb"&gt;Animated Story Generation&lt;/a&gt;: Create animated videos by combining Gemini's story generation, Imagen, and audio synthesis&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/LiveAPI_plotting_and_mapping.ipynb"&gt;Plotting and mapping Live&lt;/a&gt;: Mix &lt;em&gt;Live API&lt;/em&gt; and &lt;em&gt;Code execution&lt;/em&gt; to solve complex tasks live&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Spatial_understanding_3d.ipynb"&gt;3D Spatial understanding&lt;/a&gt;: Use Gemini &lt;em&gt;3D spatial&lt;/em&gt; abilities to understand 3D scenes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/gradio_audio.py"&gt;Gradio and live API&lt;/a&gt;: Use gradio to deploy your own instance of the &lt;em&gt;Live API&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;And &lt;a href="https://github.com/google-gemini/cookbook/tree/main/examples/"&gt;many many more&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;3. Demos (End-to-End Applications)&lt;/h2&gt; 
&lt;p&gt;These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google-gemini/gemini-api-quickstart"&gt;Gemini API quickstart&lt;/a&gt;: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini's multi-modal capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google-gemini/multimodal-live-api-web-console"&gt;Multimodal Live API Web Console&lt;/a&gt;: React-based starter app for using the Multimodal Live API over a websocket&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google-gemini/starter-applets"&gt;Google AI Studio Starter Applets&lt;/a&gt;: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Official SDKs&lt;/h2&gt; 
&lt;p&gt;The Gemini API is a REST API. You can call it directly using tools like &lt;code&gt;curl&lt;/code&gt; (see &lt;a href="https://github.com/google-gemini/cookbook/tree/main/quickstarts/rest/"&gt;REST examples&lt;/a&gt; or the great &lt;a href="https://www.postman.com/ai-on-postman/google-gemini-apis/overview"&gt;Postman workspace&lt;/a&gt;), or use one of our official SDKs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/googleapis/python-genai"&gt;Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/generative-ai-go"&gt;Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/generative-ai-js"&gt;Node.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/generative-ai-dart"&gt;Dart (Flutter)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/generative-ai-android"&gt;Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/generative-ai-swift"&gt;Swift&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Important: Migration&lt;/h2&gt; 
&lt;p&gt;With Gemini 2 we are offering a &lt;a href="https://github.com/googleapis/python-genai"&gt;new SDK&lt;/a&gt; (&lt;code&gt;&lt;a href="https://pypi.org/project/google-genai/"&gt;google-genai&lt;/a&gt;&lt;/code&gt;, &lt;code&gt;v1.0&lt;/code&gt;). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the &lt;a href="https://aistudio.google.com/live"&gt;live API&lt;/a&gt; (audio + video streaming), improved tool usage ( &lt;a href="https://ai.google.dev/gemini-api/docs/code-execution?lang=python"&gt;code execution&lt;/a&gt;, &lt;a href="https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python"&gt;function calling&lt;/a&gt; and integrated &lt;a href="https://ai.google.dev/gemini-api/docs/grounding?lang=python"&gt;Google search grounding&lt;/a&gt;), and media generation (&lt;a href="https://ai.google.dev/gemini-api/docs/imagen"&gt;Imagen&lt;/a&gt; and &lt;a href="https://ai.google.dev/gemini-api/docs/video"&gt;Veo&lt;/a&gt;). This SDK allows you to connect to the Gemini API through either &lt;a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash"&gt;Google AI Studio&lt;/a&gt; or &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2"&gt;Vertex AI&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;&lt;a href="https://pypi.org/project/google-generativeai"&gt;google-generativeai&lt;/a&gt;&lt;/code&gt; package will continue to support the original Gemini models. It &lt;em&gt;can&lt;/em&gt; also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://ai.google.dev/gemini-api/docs/migrate"&gt;migration guide&lt;/a&gt; for details. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Get Help&lt;/h2&gt; 
&lt;p&gt;Ask a question on the &lt;a href="https://discuss.ai.google.dev/"&gt;Google AI Developer Forum&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;The Gemini API on Google Cloud Vertex AI&lt;/h2&gt; 
&lt;p&gt;For enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai"&gt;this repo&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See &lt;a href="https://raw.githubusercontent.com/google-gemini/cookbook/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Thank you for developing with the Gemini API! We're excited to see what you create.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>