<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 12 Oct 2025 01:37:50 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>sapientinc/HRM</title>
      <link>https://github.com/sapientinc/HRM</link>
      <description>&lt;p&gt;Hierarchical Reasoning Model Official Release&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Hierarchical Reasoning Model&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sapientinc/HRM/main/assets/hrm.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRMâ€™s potential as a transformative advancement toward universal computation and general-purpose reasoning systems.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Join our Discord Community: &lt;a href="https://discord.gg/sapient"&gt;https://discord.gg/sapient&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start Guide ğŸš€&lt;/h2&gt; 
&lt;h3&gt;Prerequisites âš™ï¸&lt;/h3&gt; 
&lt;p&gt;Ensure PyTorch and CUDA are installed. The repo needs CUDA extensions to be built. If not present, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install CUDA 12.6
CUDA_URL=https://developer.download.nvidia.com/compute/cuda/12.6.3/local_installers/cuda_12.6.3_560.35.05_linux.run

wget -q --show-progress --progress=bar:force:noscroll -O cuda_installer.run $CUDA_URL
sudo sh cuda_installer.run --silent --toolkit --override

export CUDA_HOME=/usr/local/cuda-12.6

# Install PyTorch with CUDA 12.6
PYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu126

pip3 install torch torchvision torchaudio --index-url $PYTORCH_INDEX_URL

# Additional packages for building extensions
pip3 install packaging ninja wheel setuptools setuptools-scm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install FlashAttention. For Hopper GPUs, install FlashAttention 3&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:Dao-AILab/flash-attention.git
cd flash-attention/hopper
python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Ampere or earlier GPUs, install FlashAttention 2&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install flash-attn
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Install Python Dependencies ğŸ&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;W&amp;amp;B Integration ğŸ“ˆ&lt;/h2&gt; 
&lt;p&gt;This project uses &lt;a href="https://wandb.ai/"&gt;Weights &amp;amp; Biases&lt;/a&gt; for experiment tracking and metric visualization. Ensure you're logged in:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wandb login
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Run Experiments&lt;/h2&gt; 
&lt;h3&gt;Quick Demo: Sudoku Solver ğŸ’»ğŸ—²&lt;/h3&gt; 
&lt;p&gt;Train a master-level Sudoku AI capable of solving extremely difficult puzzles on a modern laptop GPU. ğŸ§©&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download and build Sudoku dataset
python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000

# Start training (single GPU, smaller batch size)
OMP_NUM_THREADS=8 python pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=20000 eval_interval=2000 global_batch_size=384 lr=7e-5 puzzle_emb_lr=7e-5 weight_decay=1.0 puzzle_emb_weight_decay=1.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runtime: ~10 hours on a RTX 4070 laptop GPU&lt;/p&gt; 
&lt;h2&gt;Trained Checkpoints ğŸš§&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/sapientinc/HRM-checkpoint-ARC-2"&gt;ARC-AGI-2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/sapientinc/HRM-checkpoint-sudoku-extreme"&gt;Sudoku 9x9 Extreme (1000 examples)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/sapientinc/HRM-checkpoint-maze-30x30-hard"&gt;Maze 30x30 Hard (1000 examples)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use the checkpoints, see Evaluation section below.&lt;/p&gt; 
&lt;h2&gt;Full-scale Experiments ğŸ”µ&lt;/h2&gt; 
&lt;p&gt;Experiments below assume an 8-GPU setup.&lt;/p&gt; 
&lt;h3&gt;Dataset Preparation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Initialize submodules
git submodule update --init --recursive

# ARC-1
python dataset/build_arc_dataset.py  # ARC offical + ConceptARC, 960 examples
# ARC-2
python dataset/build_arc_dataset.py --dataset-dirs dataset/raw-data/ARC-AGI-2/data --output-dir data/arc-2-aug-1000  # ARC-2 official, 1120 examples

# Sudoku-Extreme
python dataset/build_sudoku_dataset.py  # Full version
python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples

# Maze
python dataset/build_maze_dataset.py  # 1000 examples
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Dataset Visualization&lt;/h3&gt; 
&lt;p&gt;Explore the puzzles visually:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open &lt;code&gt;puzzle_visualizer.html&lt;/code&gt; in your browser.&lt;/li&gt; 
 &lt;li&gt;Upload the generated dataset folder located in &lt;code&gt;data/...&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Launch experiments&lt;/h2&gt; 
&lt;h3&gt;Small-sample (1K)&lt;/h3&gt; 
&lt;p&gt;ARC-1:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Runtime:&lt;/em&gt; ~24 hours&lt;/p&gt; 
&lt;p&gt;ARC-2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/arc-2-aug-1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Runtime:&lt;/em&gt; ~24 hours (checkpoint after 8 hours is often sufficient)&lt;/p&gt; 
&lt;p&gt;Sudoku Extreme (1k):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=20000 eval_interval=2000 lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Runtime:&lt;/em&gt; ~10 minutes&lt;/p&gt; 
&lt;p&gt;Maze 30x30 Hard (1k):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/maze-30x30-hard-1k epochs=20000 eval_interval=2000 lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Runtime:&lt;/em&gt; ~1 hour&lt;/p&gt; 
&lt;h3&gt;Full Sudoku-Hard&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/sudoku-hard-full epochs=100 eval_interval=10 lr_min_ratio=0.1 global_batch_size=2304 lr=3e-4 puzzle_emb_lr=3e-4 weight_decay=0.1 puzzle_emb_weight_decay=0.1 arch.loss.loss_type=softmax_cross_entropy arch.L_cycles=8 arch.halt_max_steps=8 arch.pos_encodings=learned
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Runtime:&lt;/em&gt; ~2 hours&lt;/p&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;Evaluate your trained models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check &lt;code&gt;eval/exact_accuracy&lt;/code&gt; in W&amp;amp;B.&lt;/li&gt; 
 &lt;li&gt;For ARC-AGI, follow these additional steps:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 evaluate.py checkpoint=&amp;lt;CHECKPOINT_PATH&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Then use the provided &lt;code&gt;arc_eval.ipynb&lt;/code&gt; notebook to finalize and inspect your results.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Notes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Small-sample learning typically exhibits accuracy variance of around Â±2 points.&lt;/li&gt; 
 &lt;li&gt;For Sudoku-Extreme (1,000-example dataset), late-stage overfitting may cause numerical instability during training and Q-learning. It is advisable to use early stopping once the training accuracy approaches 100%.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation ğŸ“œ&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2025hierarchicalreasoningmodel,
      title={Hierarchical Reasoning Model}, 
      author={Guan Wang and Jin Li and Yuhao Sun and Xing Chen and Changling Liu and Yue Wu and Meng Lu and Sen Song and Yasin Abbasi Yadkori},
      year={2025},
      eprint={2506.21734},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.21734}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;ğŸ”¥ åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚Text-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿ&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot æ˜¯ä¸€æ¬¾åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚SQLBot çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å¼€ç®±å³ç”¨&lt;/strong&gt;: åªéœ€é…ç½®å¤§æ¨¡å‹å’Œæ•°æ®æºå³å¯å¼€å¯é—®æ•°ä¹‹æ—…ï¼Œé€šè¿‡å¤§æ¨¡å‹å’Œ RAG çš„ç»“åˆæ¥å®ç°é«˜è´¨é‡çš„ text2sqlï¼›&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ˜“äºé›†æˆ&lt;/strong&gt;: æ”¯æŒå¿«é€ŸåµŒå…¥åˆ°ç¬¬ä¸‰æ–¹ä¸šåŠ¡ç³»ç»Ÿï¼Œä¹Ÿæ”¯æŒè¢« n8nã€MaxKBã€Difyã€Coze ç­‰ AI åº”ç”¨å¼€å‘å¹³å°é›†æˆè°ƒç”¨ï¼Œè®©å„ç±»åº”ç”¨å¿«é€Ÿæ‹¥æœ‰æ™ºèƒ½é—®æ•°èƒ½åŠ›ï¼›&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å®‰å…¨å¯æ§&lt;/strong&gt;: æä¾›åŸºäºå·¥ä½œç©ºé—´çš„èµ„æºéš”ç¦»æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„æ•°æ®æƒé™æ§åˆ¶ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;å·¥ä½œåŸç†&lt;/h2&gt; 
&lt;img width="1105" height="577" alt="system-arch" src="https://github.com/user-attachments/assets/462603fc-980b-4b8b-a6d4-a821c070a048" /&gt; 
&lt;h2&gt;å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;h3&gt;å®‰è£…éƒ¨ç½²&lt;/h3&gt; 
&lt;p&gt;å‡†å¤‡ä¸€å° Linux æœåŠ¡å™¨ï¼Œå®‰è£…å¥½ &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;ï¼Œæ‰§è¡Œä»¥ä¸‹ä¸€é”®å®‰è£…è„šæœ¬ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/file:/opt/sqlbot/data/file \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ä½ ä¹Ÿå¯ä»¥é€šè¿‡ &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel åº”ç”¨å•†åº—&lt;/a&gt; å¿«é€Ÿéƒ¨ç½² SQLBotã€‚&lt;/p&gt; 
&lt;p&gt;å¦‚æœæ˜¯å†…ç½‘ç¯å¢ƒï¼Œä½ å¯ä»¥é€šè¿‡ &lt;a href="https://community.fit2cloud.com/#/products/sqlbot/downloads"&gt;ç¦»çº¿å®‰è£…åŒ…æ–¹å¼&lt;/a&gt; éƒ¨ç½² SQLBotã€‚&lt;/p&gt; 
&lt;h3&gt;è®¿é—®æ–¹å¼&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://&amp;lt;ä½ çš„æœåŠ¡å™¨IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;ç”¨æˆ·å: admin&lt;/li&gt; 
 &lt;li&gt;å¯†ç : SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;è”ç³»æˆ‘ä»¬&lt;/h3&gt; 
&lt;p&gt;å¦‚ä½ æœ‰æ›´å¤šé—®é¢˜ï¼Œå¯ä»¥åŠ å…¥æˆ‘ä»¬çš„æŠ€æœ¯äº¤æµç¾¤ä¸æˆ‘ä»¬äº¤æµã€‚&lt;/p&gt; 
&lt;img width="180" height="180" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI å±•ç¤º&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;é£è‡´äº‘æ——ä¸‹çš„å…¶ä»–æ˜æ˜Ÿé¡¹ç›®&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - äººäººå¯ç”¨çš„å¼€æº BI å·¥å…·&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - ç°ä»£åŒ–ã€å¼€æºçš„ Linux æœåŠ¡å™¨è¿ç»´ç®¡ç†é¢æ¿&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - å¼ºå¤§æ˜“ç”¨çš„ä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - å¹¿å—æ¬¢è¿çš„å¼€æºå ¡å’æœº&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1Panel-dev/CordysCRM"&gt;Cordys CRM&lt;/a&gt; - æ–°ä¸€ä»£çš„å¼€æº AI CRM ç³»ç»Ÿ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - å¼ºå¤§æ˜“ç”¨çš„å¼€æºå»ºç«™å·¥å…·&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - æ–°ä¸€ä»£çš„å¼€æºæŒç»­æµ‹è¯•å·¥å…·&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;æœ¬ä»“åº“éµå¾ª &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; å¼€æºåè®®ï¼Œè¯¥è®¸å¯è¯æœ¬è´¨ä¸Šæ˜¯ GPLv3ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„é™åˆ¶ã€‚&lt;/p&gt; 
&lt;p&gt;ä½ å¯ä»¥åŸºäº SQLBot çš„æºä»£ç è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œä½†æ˜¯éœ€è¦éµå®ˆä»¥ä¸‹è§„å®šï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä¸èƒ½æ›¿æ¢å’Œä¿®æ”¹ SQLBot çš„ Logo å’Œç‰ˆæƒä¿¡æ¯ï¼›&lt;/li&gt; 
 &lt;li&gt;äºŒæ¬¡å¼€å‘åçš„è¡ç”Ÿä½œå“å¿…é¡»éµå®ˆ GPL V3 çš„å¼€æºä¹‰åŠ¡ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¦‚éœ€å•†ä¸šæˆæƒï¼Œè¯·è”ç³» &lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt; ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenMind/OM1</title>
      <link>https://github.com/OpenMind/OM1</link>
      <description>&lt;p&gt;Modular AI runtime for robots&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c" alt="OM_Banner_X2 (1)" /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2412.18588"&gt;Technical Paper&lt;/a&gt; | &lt;a href="https://docs.openmind.org/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://x.com/openmind_agi"&gt;X&lt;/a&gt; | &lt;a href="https://discord.gg/VUjpg4ef5n"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OpenMind's OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots&lt;/strong&gt;, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.&lt;/p&gt; 
&lt;h2&gt;Capabilities of OM1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Modular Architecture&lt;/strong&gt;: Designed with Python for simplicity and seamless integration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Input&lt;/strong&gt;: Easily handles new data and sensors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware Support via Plugins&lt;/strong&gt;: Supports new hardware through plugins for API endpoints and specific robot hardware connections to &lt;code&gt;ROS2&lt;/code&gt;, &lt;code&gt;Zenoh&lt;/code&gt;, and &lt;code&gt;CycloneDDS&lt;/code&gt;. (We recommend &lt;code&gt;Zenoh&lt;/code&gt; for all new development).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web-Based Debugging Display&lt;/strong&gt;: Monitor the system in action with WebSim (available at &lt;a href="http://localhost:8000/"&gt;http://localhost:8000/&lt;/a&gt;) for easy visual debugging.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-configured Endpoints&lt;/strong&gt;: Supports Voice-to-Speech, OpenAIâ€™s &lt;code&gt;gpt-4o&lt;/code&gt;, DeepSeek, and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture Overview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/14e9b916-4df7-4700-9336-2983c85be311" alt="Artboard 1@4x 1 (1)" /&gt;&lt;/p&gt; 
&lt;h2&gt;Getting Started - Hello World&lt;/h2&gt; 
&lt;p&gt;To get started with OM1, let's run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to &lt;code&gt;OpenAI 4o&lt;/code&gt;, which returns &lt;code&gt;movement&lt;/code&gt;, &lt;code&gt;speech&lt;/code&gt; and &lt;code&gt;face&lt;/code&gt; action commands. These commands are displayed on WebSim along with basic timing and other debugging information.&lt;/p&gt; 
&lt;h3&gt;Package Management and VENV&lt;/h3&gt; 
&lt;p&gt;You will need the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt; package manager&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Clone the Repo&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Dependencies&lt;/h3&gt; 
&lt;p&gt;For MacOS&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install portaudio ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Linux&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Obtain an OpenMind API Key&lt;/h3&gt; 
&lt;p&gt;Obtain your API Key at &lt;a href="https://portal.openmind.org/"&gt;OpenMind Portal&lt;/a&gt;. Copy it to &lt;code&gt;config/spot.json5&lt;/code&gt;, replacing the &lt;code&gt;openmind_free&lt;/code&gt; placeholder. Or, &lt;code&gt;cp env.example .env&lt;/code&gt; and add your key to the &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Launching OM1&lt;/h3&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run src/run.py spot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see &lt;a href="https://docs.openmind.org/getting-started"&gt;getting started&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;What's Next?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try out some &lt;a href="https://docs.openmind.org/examples"&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add new &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;actions&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Design custom agents and robots by creating your own &lt;code&gt;json5&lt;/code&gt; config files with custom combinations of inputs and actions.&lt;/li&gt; 
 &lt;li&gt;Change the system prompts in the configuration files (located in &lt;code&gt;/config/&lt;/code&gt;) to create new behaviors.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Interfacing with New Robot Hardware&lt;/h2&gt; 
&lt;p&gt;OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as &lt;code&gt;backflip&lt;/code&gt;, &lt;code&gt;run&lt;/code&gt;, &lt;code&gt;gently pick up the red apple&lt;/code&gt;, &lt;code&gt;move(0.37, 0, 0)&lt;/code&gt;, and &lt;code&gt;smile&lt;/code&gt;. An example is provided in &lt;code&gt;actions/move_safe/connector/ros2.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
elif output_interface.action == "shake paw":
    if self.sport_client:
        self.sport_client.Hello()
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers.&lt;/p&gt; 
&lt;p&gt;OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see &lt;a href="https://github.com/unitreerobotics/unitree_sdk2/raw/adee312b081c656ecd0bb4e936eed96325546296/example/g1/high_level/g1_loco_client_example.cpp#L159"&gt;Unitree's C++ SDK&lt;/a&gt;. Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.&lt;/p&gt; 
&lt;h2&gt;Recommended Development Platforms&lt;/h2&gt; 
&lt;p&gt;OM1 is developed on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1)&lt;/li&gt; 
 &lt;li&gt;Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Generic Linux machines (running Ubuntu 22.04)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;OM1 &lt;em&gt;should&lt;/em&gt; run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.&lt;/p&gt; 
&lt;h2&gt;Full Autonomy Guidance&lt;/h2&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;full autonomy mode&lt;/strong&gt;, where three services work together in a loop without manual intervention:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;om1&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;unitree_go2_ros2_sdk&lt;/strong&gt; â€“ A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;om1-avatar&lt;/strong&gt; â€“ A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to Backpack?&lt;/h2&gt; 
&lt;p&gt;From research to real-world autonomy, a platform that learns, moves, and builds with you. We'll shortly be releasing the &lt;strong&gt;BOM&lt;/strong&gt; and details on &lt;strong&gt;DIY&lt;/strong&gt; for the it. Stay tuned!&lt;/p&gt; 
&lt;p&gt;Clone the following repos -&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1.git"&gt;https://github.com/OpenMind/OM1.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/unitree_go2_ros2_sdk.git"&gt;https://github.com/OpenMind/unitree_go2_ros2_sdk.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1-avatar.git"&gt;https://github.com/OpenMind/OM1-avatar.git&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Starting the system&lt;/h2&gt; 
&lt;p&gt;To start all services, run the following commands:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Setup the API key&lt;/p&gt; 
&lt;p&gt;For Bash: vim ~/.bashrc or ~/.bash_profile.&lt;/p&gt; 
&lt;p&gt;For Zsh: vim ~/.zshrc.&lt;/p&gt; 
&lt;p&gt;Add&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OM_API_KEY="your_api_key"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update the docker-compose file. Replace "unitree_go2_autonomy_advance" with the agent you want to run.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;command: ["unitree_go2_autonomy_advance"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1
docker-compose up om1 -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For unitree_go2_ros2_sdk&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1-avatar&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1-avatar
docker-compose up om1_avatar -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Detailed Documentation&lt;/h2&gt; 
&lt;p&gt;More detailed documentation can be accessed at &lt;a href="https://docs.openmind.org/"&gt;docs.openmind.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please make sure to read the &lt;a href="https://raw.githubusercontent.com/OpenMind/OM1/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; before making a pull request.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Genesis-Embodied-AI/Genesis</title>
      <link>https://github.com/Genesis-Embodied-AI/Genesis</link>
      <description>&lt;p&gt;A generative world for general-purpose robotics &amp; embodied AI learning.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/big_text.png" alt="Genesis" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/imgs/teaser.png" alt="Teaser" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/genesis-world/"&gt;&lt;img src="https://img.shields.io/pypi/v/genesis-world" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/genesis-world"&gt;&lt;img src="https://static.pepy.tech/badge/genesis-world" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/issues"&gt;&lt;img src="https://img.shields.io/github/issues/Genesis-Embodied-AI/Genesis" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/discussions"&gt;&lt;img src="https://img.shields.io/github/discussions/Genesis-Embodied-AI/Genesis" alt="GitHub Discussions" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/nukCuhB47p"&gt;&lt;img src="https://img.shields.io/discord/1322086972302430269?logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://drive.google.com/uc?export=view&amp;amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ"&gt;&lt;img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white" height="20" style="display:inline" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/English-d9d9d9" alt="README in English" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_FR.md"&gt;&lt;img src="https://img.shields.io/badge/Francais-d9d9d9" alt="README en FranÃ§ais" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_KR.md"&gt;&lt;img src="https://img.shields.io/badge/%ED%95%9C%EA%B5%AD%EC%96%B4-d9d9d9" alt="í•œêµ­ì–´ README" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_CN.md"&gt;&lt;img src="https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-d9d9d9" alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/README_JA.md"&gt;&lt;img src="https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-d9d9d9" alt="æ—¥æœ¬èªç‰ˆ README" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Genesis&lt;/h1&gt; 
&lt;h2&gt;ğŸ”¥ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025-08-05] Released v0.3.0 ğŸŠ ğŸ‰&lt;/li&gt; 
 &lt;li&gt;[2025-07-02] The development of Genesis is now officially supported by &lt;a href="https://genesis-ai.company/"&gt;Genesis AI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025-01-09] We released a &lt;a href="https://github.com/zhouxian/genesis-speed-benchmark"&gt;detailed performance benchmarking and comparison report&lt;/a&gt; on Genesis, together with all the test scripts.&lt;/li&gt; 
 &lt;li&gt;[2025-01-08] Released v0.2.1 ğŸŠ ğŸ‰&lt;/li&gt; 
 &lt;li&gt;[2025-01-08] Created &lt;a href="https://discord.gg/nukCuhB47p"&gt;Discord&lt;/a&gt; and &lt;a href="https://drive.google.com/uc?export=view&amp;amp;id=1ZS9nnbQ-t1IwkzJlENBYqYIIOOZhXuBZ"&gt;Wechat&lt;/a&gt; group.&lt;/li&gt; 
 &lt;li&gt;[2024-12-25] Added a &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#docker"&gt;docker&lt;/a&gt; including support for the ray-tracing renderer&lt;/li&gt; 
 &lt;li&gt;[2024-12-24] Added guidelines for &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/raw/main/.github/CONTRIBUTING.md"&gt;contributing to Genesis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#what-is-genesis"&gt;What is Genesis?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#quick-installation"&gt;Quick Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#contributing-to-genesis"&gt;Contributing to Genesis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#support"&gt;Support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#license-and-acknowledgments"&gt;License and Acknowledgments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#associated-papers"&gt;Associated Papers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;What is Genesis?&lt;/h2&gt; 
&lt;p&gt;Genesis is a physics platform designed for general-purpose &lt;em&gt;Robotics/Embodied AI/Physical AI&lt;/em&gt; applications. It is simultaneously multiple things:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A &lt;strong&gt;universal physics engine&lt;/strong&gt; re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;lightweight&lt;/strong&gt;, &lt;strong&gt;ultra-fast&lt;/strong&gt;, &lt;strong&gt;pythonic&lt;/strong&gt;, and &lt;strong&gt;user-friendly&lt;/strong&gt; robotics simulation platform.&lt;/li&gt; 
 &lt;li&gt;A powerful and fast &lt;strong&gt;photo-realistic rendering system&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;A &lt;strong&gt;generative data engine&lt;/strong&gt; that transforms user-prompted natural language description into various modalities of data.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Powered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully automated data generation for robotics and beyond.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Currently, we are open-sourcing the &lt;em&gt;underlying physics engine&lt;/em&gt; and the &lt;em&gt;simulation platform&lt;/em&gt;. Our &lt;em&gt;generative framework&lt;/em&gt; is a modular system that incorporates many different generative modules, each handling a certain range of data modalities, routed by a high level agent. Some of the modules integrated existing papers and some are still under submission. Access to our generative feature will be gradually rolled out in the near future. If you are interested, feel free to explore more in the &lt;a href="https://raw.githubusercontent.com/Genesis-Embodied-AI/Genesis/main/#associated-papers"&gt;paper list&lt;/a&gt; below.&lt;/p&gt; 
&lt;p&gt;Genesis aims to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lower the barrier&lt;/strong&gt; to using physics simulations, making robotics research accessible to everyone. See our &lt;a href="https://genesis-world.readthedocs.io/en/latest/user_guide/overview/mission.html"&gt;mission statement&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unify diverse physics solvers&lt;/strong&gt; into a single framework to recreate the physical world with the highest fidelity.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automate data generation&lt;/strong&gt;, reducing human effort and letting the data flywheel spin on its own.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Project Page: &lt;a href="https://genesis-embodied-ai.github.io/"&gt;https://genesis-embodied-ai.github.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Over 43 million FPS when simulating a Franka robotic arm with a single RTX 4090 (430,000 times faster than real-time).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt;: Runs on Linux, macOS, Windows, and supports multiple compute backends (CPU, Nvidia/AMD GPUs, Apple Metal).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integration of diverse physics solvers&lt;/strong&gt;: Rigid body, MPM, SPH, FEM, PBD, Stable Fluid.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wide range of material models&lt;/strong&gt;: Simulation and coupling of rigid bodies, liquids, gases, deformable objects, thin-shell objects, and granular materials.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compatibility with various robots&lt;/strong&gt;: Robotic arms, legged robots, drones, &lt;em&gt;soft robots&lt;/em&gt;, and support for loading &lt;code&gt;MJCF (.xml)&lt;/code&gt;, &lt;code&gt;URDF&lt;/code&gt;, &lt;code&gt;.obj&lt;/code&gt;, &lt;code&gt;.glb&lt;/code&gt;, &lt;code&gt;.ply&lt;/code&gt;, &lt;code&gt;.stl&lt;/code&gt;, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Photo-realistic rendering&lt;/strong&gt;: Native ray-tracing-based rendering.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Differentiability&lt;/strong&gt;: Genesis is designed to be fully differentiable. Currently, our MPM solver and Tool Solver support differentiability, with other solvers planned for future versions (starting with rigid &amp;amp; articulated body solver).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User-friendliness&lt;/strong&gt;: Designed for simplicity, with intuitive installation and APIs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Installation&lt;/h2&gt; 
&lt;p&gt;Install &lt;strong&gt;PyTorch&lt;/strong&gt; first following the &lt;a href="https://pytorch.org/get-started/locally/"&gt;official instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Then, install Genesis via PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install genesis-world  # Requires Python&amp;gt;=3.10,&amp;lt;3.14;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the latest version to date, make sure that &lt;code&gt;pip&lt;/code&gt; is up-to-date via &lt;code&gt;pip install --upgrade pip&lt;/code&gt;, then run command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/Genesis-Embodied-AI/Genesis.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the package must still be updated manually to sync with main branch.&lt;/p&gt; 
&lt;p&gt;Users seeking to contribute are encouraged to install Genesis in editable mode. First, make sure that &lt;code&gt;genesis-world&lt;/code&gt; has been uninstalled, then clone the repository and install locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Genesis-Embodied-AI/Genesis.git
cd Genesis
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is recommended to systematically execute &lt;code&gt;pip install -e ".[dev]"&lt;/code&gt; after moving HEAD to make sure that all dependencies and entrypoints are up-to-date.&lt;/p&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;If you want to use Genesis from Docker, you can first build the Docker image as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t genesis -f docker/Dockerfile docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can run the examples inside the docker image (mounted to &lt;code&gt;/workspace/examples&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;xhost +local:root # Allow the container to access the display

docker run --gpus all --rm -it \
-e DISPLAY=$DISPLAY \
-e LOCAL_USER_ID="$(id -u)" \
-v /dev/dri:/dev/dri \
-v /tmp/.X11-unix/:/tmp/.X11-unix \
-v $(pwd):/workspace \
--name genesis genesis:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;AMD users&lt;/h3&gt; 
&lt;p&gt;AMD users can use Genesis using the &lt;code&gt;docker/Dockerfile.amdgpu&lt;/code&gt; file, which is built by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker build -t genesis-amd -f docker/Dockerfile.amdgpu docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and can then be used by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-xhost"&gt;docker run -it --network=host \
 --device=/dev/kfd \
 --device=/dev/dri \
 --group-add=video \
 --ipc=host \
 --cap-add=SYS_PTRACE \
 --security-opt seccomp=unconfined \
 --shm-size 8G \
 -v $PWD:/workspace \
 -e DISPLAY=$DISPLAY \
 genesis-amd
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The examples will be accessible from &lt;code&gt;/workspace/examples&lt;/code&gt;. Note: AMD users should use the vulkan backend. This means you will need to call &lt;code&gt;gs.init(vulkan)&lt;/code&gt; to initialise Genesis.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation is available in &lt;a href="https://genesis-world.readthedocs.io/en/latest/user_guide/index.html"&gt;English&lt;/a&gt;, &lt;a href="https://genesis-world.readthedocs.io/zh-cn/latest/user_guide/index.html"&gt;Chinese&lt;/a&gt;, and &lt;a href="https://genesis-world.readthedocs.io/ja/latest/user_guide/index.html"&gt;Japanese&lt;/a&gt;. This includes detailed installation steps, tutorials, and API references.&lt;/p&gt; 
&lt;h2&gt;Contributing to Genesis&lt;/h2&gt; 
&lt;p&gt;The Genesis project is an open and collaborative effort. We welcome all forms of contributions from the community, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pull requests&lt;/strong&gt; for new features or bug fixes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bug reports&lt;/strong&gt; through GitHub Issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Suggestions&lt;/strong&gt; to improve Genesis's usability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Refer to our &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/raw/main/.github/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report bugs or request features via GitHub &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/issues"&gt;Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join discussions or ask questions on GitHub &lt;a href="https://github.com/Genesis-Embodied-AI/Genesis/discussions"&gt;Discussions&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License and Acknowledgments&lt;/h2&gt; 
&lt;p&gt;The Genesis source code is licensed under Apache 2.0.&lt;/p&gt; 
&lt;p&gt;Genesis's development has been made possible thanks to these open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/taichi-dev/taichi"&gt;Taichi&lt;/a&gt;: High-performance cross-platform compute backend. Kudos to the Taichi team for their technical support!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zhouxian/FluidLab"&gt;FluidLab&lt;/a&gt;: Reference MPM solver implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/erizmr/SPH_Taichi"&gt;SPH_Taichi&lt;/a&gt;: Reference SPH solver implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://matthias-research.github.io/pages/tenMinutePhysics/index.html"&gt;Ten Minute Physics&lt;/a&gt; and &lt;a href="https://github.com/WASD4959/PBF3D"&gt;PBF3D&lt;/a&gt;: Reference PBD solver implementations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google-deepmind/mujoco"&gt;MuJoCo&lt;/a&gt;: Reference for rigid body dynamics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/danfis/libccd"&gt;libccd&lt;/a&gt;: Reference for collision detection.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mmatl/pyrender"&gt;PyRender&lt;/a&gt;: Rasterization-based renderer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LuisaGroup/LuisaCompute"&gt;LuisaCompute&lt;/a&gt; and &lt;a href="https://github.com/LuisaGroup/LuisaRender"&gt;LuisaRender&lt;/a&gt;: Ray-tracing DSL.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/shacklettbp/madrona"&gt;Madrona&lt;/a&gt; and &lt;a href="https://github.com/shacklettbp/madrona_mjx"&gt;Madrona-mjx&lt;/a&gt;: Batch renderer backend&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Associated Papers&lt;/h2&gt; 
&lt;p&gt;Genesis is a large scale effort that integrates state-of-the-art technologies of various existing and on-going research work into a single system. Here we include a non-exhaustive list of all the papers that contributed to the Genesis project in one way or another:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Xian, Zhou, et al. "Fluidlab: A differentiable environment for benchmarking complex fluid manipulation." arXiv preprint arXiv:2303.02346 (2023).&lt;/li&gt; 
 &lt;li&gt;Xu, Zhenjia, et al. "Roboninja: Learning an adaptive cutting policy for multi-material objects." arXiv preprint arXiv:2302.11553 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Yufei, et al. "Robogen: Towards unleashing infinite data for automated robot learning via generative simulation." arXiv preprint arXiv:2311.01455 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Tsun-Hsuan, et al. "Softzoo: A soft robot co-design benchmark for locomotion in diverse environments." arXiv preprint arXiv:2303.09555 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Tsun-Hsuan Johnson, et al. "Diffusebot: Breeding soft robots with physics-augmented generative diffusion models." Advances in Neural Information Processing Systems 36 (2023): 44398-44423.&lt;/li&gt; 
 &lt;li&gt;Katara, Pushkal, Zhou Xian, and Katerina Fragkiadaki. "Gen2sim: Scaling up robot learning in simulation with generative models." 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.&lt;/li&gt; 
 &lt;li&gt;Si, Zilin, et al. "DiffTactile: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation." arXiv preprint arXiv:2403.08716 (2024).&lt;/li&gt; 
 &lt;li&gt;Wang, Yian, et al. "Thin-Shell Object Manipulations With Differentiable Physics Simulations." arXiv preprint arXiv:2404.00451 (2024).&lt;/li&gt; 
 &lt;li&gt;Lin, Chunru, et al. "UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments." arXiv preprint arXiv:2411.12711 (2024).&lt;/li&gt; 
 &lt;li&gt;Zhou, Wenyang, et al. "EMDM: Efficient motion diffusion model for fast and high-quality motion generation." European Conference on Computer Vision. Springer, Cham, 2025.&lt;/li&gt; 
 &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. "Scalable differentiable physics for learning and control." International Conference on Machine Learning. PMLR, 2020.&lt;/li&gt; 
 &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming C. Lin. "Efficient differentiable simulation of articulated bodies." In International Conference on Machine Learning, PMLR, 2021.&lt;/li&gt; 
 &lt;li&gt;Qiao, Yi-Ling, Junbang Liang, Vladlen Koltun, and Ming Lin. "Differentiable simulation of soft multi-body systems." Advances in Neural Information Processing Systems 34 (2021).&lt;/li&gt; 
 &lt;li&gt;Wan, Weilin, et al. "Tlcontrol: Trajectory and language control for human motion synthesis." arXiv preprint arXiv:2311.17135 (2023).&lt;/li&gt; 
 &lt;li&gt;Wang, Yian, et al. "Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting." arXiv preprint arXiv:2411.09823 (2024).&lt;/li&gt; 
 &lt;li&gt;Zheng, Shaokun, et al. "LuisaRender: A high-performance rendering framework with layered and unified interfaces on stream architectures." ACM Transactions on Graphics (TOG) 41.6 (2022): 1-19.&lt;/li&gt; 
 &lt;li&gt;Fan, Yingruo, et al. "Faceformer: Speech-driven 3d facial animation with transformers." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.&lt;/li&gt; 
 &lt;li&gt;Wu, Sichun, Kazi Injamamul Haque, and Zerrin Yumak. "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE." Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games. 2024.&lt;/li&gt; 
 &lt;li&gt;Dou, Zhiyang, et al. "CÂ· ase: Learning conditional adversarial skill embeddings for physics-based characters." SIGGRAPH Asia 2023 Conference Papers. 2023.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;... and many more on-going work.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use Genesis in your research, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{Genesis,
  author = {Genesis Authors},
  title = {Genesis: A Generative and Universal Physics Engine for Robotics and Beyond},
  month = {December},
  year = {2024},
  url = {https://github.com/Genesis-Embodied-AI/Genesis}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>MODSetter/SurfSense</title>
      <link>https://github.com/MODSetter/SurfSense</link>
      <description>&lt;p&gt;Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65" alt="new_header" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://discord.gg/ejRNvftDp9"&gt; &lt;img src="https://img.shields.io/discord/1359368468260192417" alt="Discord" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;SurfSense&lt;/h1&gt; 
&lt;p&gt;While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13606" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13606" alt="MODSetter%2FSurfSense | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da"&gt;https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Podcast Sample&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7"&gt;https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ’¡ &lt;strong&gt;Idea&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.&lt;/p&gt; 
&lt;h3&gt;ğŸ“ &lt;strong&gt;Multiple File Format Uploading Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Save content from your own personal files &lt;em&gt;(Documents, images, videos and supports &lt;strong&gt;50+ file extensions&lt;/strong&gt;)&lt;/em&gt; to your own personal knowledge base .&lt;/p&gt; 
&lt;h3&gt;ğŸ” &lt;strong&gt;Powerful Search&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Quickly research or find anything in your saved content .&lt;/p&gt; 
&lt;h3&gt;ğŸ’¬ &lt;strong&gt;Chat with your Saved Content&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Interact in Natural Language and get cited answers.&lt;/p&gt; 
&lt;h3&gt;ğŸ“„ &lt;strong&gt;Cited Answers&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Get Cited answers just like Perplexity.&lt;/p&gt; 
&lt;h3&gt;ğŸ”” &lt;strong&gt;Privacy &amp;amp; Local LLM Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Works Flawlessly with Ollama local LLMs.&lt;/p&gt; 
&lt;h3&gt;ğŸ  &lt;strong&gt;Self Hostable&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Open source and easy to deploy locally.&lt;/p&gt; 
&lt;h3&gt;ğŸ™ï¸ Podcasts&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; 
 &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; 
 &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; 
 &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“Š &lt;strong&gt;Advanced RAG Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports 100+ LLM's&lt;/li&gt; 
 &lt;li&gt;Supports 6000+ Embedding Models.&lt;/li&gt; 
 &lt;li&gt;Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)&lt;/li&gt; 
 &lt;li&gt;Uses Hierarchical Indices (2 tiered RAG setup).&lt;/li&gt; 
 &lt;li&gt;Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).&lt;/li&gt; 
 &lt;li&gt;RAG as a Service API Backend.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;â„¹ï¸ &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; 
 &lt;li&gt;Slack&lt;/li&gt; 
 &lt;li&gt;Linear&lt;/li&gt; 
 &lt;li&gt;Jira&lt;/li&gt; 
 &lt;li&gt;ClickUp&lt;/li&gt; 
 &lt;li&gt;Confluence&lt;/li&gt; 
 &lt;li&gt;Notion&lt;/li&gt; 
 &lt;li&gt;Gmail&lt;/li&gt; 
 &lt;li&gt;Youtube Videos&lt;/li&gt; 
 &lt;li&gt;GitHub&lt;/li&gt; 
 &lt;li&gt;Discord&lt;/li&gt; 
 &lt;li&gt;Airtable&lt;/li&gt; 
 &lt;li&gt;Google Calendar&lt;/li&gt; 
 &lt;li&gt;Luma&lt;/li&gt; 
 &lt;li&gt;and more to come.....&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ &lt;strong&gt;Supported File Extensions&lt;/strong&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Documents &amp;amp; Text&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.docm&lt;/code&gt;, &lt;code&gt;.dot&lt;/code&gt;, &lt;code&gt;.dotm&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.wpd&lt;/code&gt;, &lt;code&gt;.pages&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.602&lt;/code&gt;, &lt;code&gt;.abw&lt;/code&gt;, &lt;code&gt;.cgm&lt;/code&gt;, &lt;code&gt;.cwk&lt;/code&gt;, &lt;code&gt;.hwp&lt;/code&gt;, &lt;code&gt;.lwp&lt;/code&gt;, &lt;code&gt;.mw&lt;/code&gt;, &lt;code&gt;.mcw&lt;/code&gt;, &lt;code&gt;.pbd&lt;/code&gt;, &lt;code&gt;.sda&lt;/code&gt;, &lt;code&gt;.sdd&lt;/code&gt;, &lt;code&gt;.sdp&lt;/code&gt;, &lt;code&gt;.sdw&lt;/code&gt;, &lt;code&gt;.sgl&lt;/code&gt;, &lt;code&gt;.sti&lt;/code&gt;, &lt;code&gt;.sxi&lt;/code&gt;, &lt;code&gt;.sxw&lt;/code&gt;, &lt;code&gt;.stw&lt;/code&gt;, &lt;code&gt;.sxg&lt;/code&gt;, &lt;code&gt;.uof&lt;/code&gt;, &lt;code&gt;.uop&lt;/code&gt;, &lt;code&gt;.uot&lt;/code&gt;, &lt;code&gt;.vor&lt;/code&gt;, &lt;code&gt;.wps&lt;/code&gt;, &lt;code&gt;.zabw&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;, &lt;code&gt;.markdown&lt;/code&gt;, &lt;code&gt;.rst&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.org&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.xhtml&lt;/code&gt;, &lt;code&gt;.adoc&lt;/code&gt;, &lt;code&gt;.asciidoc&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Presentations&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;, &lt;code&gt;.pptm&lt;/code&gt;, &lt;code&gt;.pot&lt;/code&gt;, &lt;code&gt;.potm&lt;/code&gt;, &lt;code&gt;.potx&lt;/code&gt;, &lt;code&gt;.odp&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Spreadsheets &amp;amp; Data&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsm&lt;/code&gt;, &lt;code&gt;.xlsb&lt;/code&gt;, &lt;code&gt;.xlw&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;, &lt;code&gt;.ods&lt;/code&gt;, &lt;code&gt;.fods&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.dbf&lt;/code&gt;, &lt;code&gt;.123&lt;/code&gt;, &lt;code&gt;.dif&lt;/code&gt;, &lt;code&gt;.sylk&lt;/code&gt;, &lt;code&gt;.slk&lt;/code&gt;, &lt;code&gt;.prn&lt;/code&gt;, &lt;code&gt;.et&lt;/code&gt;, &lt;code&gt;.uos1&lt;/code&gt;, &lt;code&gt;.uos2&lt;/code&gt;, &lt;code&gt;.wk1&lt;/code&gt;, &lt;code&gt;.wk2&lt;/code&gt;, &lt;code&gt;.wk3&lt;/code&gt;, &lt;code&gt;.wk4&lt;/code&gt;, &lt;code&gt;.wks&lt;/code&gt;, &lt;code&gt;.wq1&lt;/code&gt;, &lt;code&gt;.wq2&lt;/code&gt;, &lt;code&gt;.wb1&lt;/code&gt;, &lt;code&gt;.wb2&lt;/code&gt;, &lt;code&gt;.wb3&lt;/code&gt;, &lt;code&gt;.qpw&lt;/code&gt;, &lt;code&gt;.xlr&lt;/code&gt;, &lt;code&gt;.eth&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Images&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.gif&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.svg&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.web&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.heic&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Audio &amp;amp; Video &lt;em&gt;(Always Supported)&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;.mp3&lt;/code&gt;, &lt;code&gt;.mpga&lt;/code&gt;, &lt;code&gt;.m4a&lt;/code&gt;, &lt;code&gt;.wav&lt;/code&gt;, &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.mpeg&lt;/code&gt;, &lt;code&gt;.webm&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Email &amp;amp; Communication&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.eml&lt;/code&gt;, &lt;code&gt;.msg&lt;/code&gt;, &lt;code&gt;.p7s&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ”– Cross Browser Extension&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The SurfSense extension can be used to save any webpage you like.&lt;/li&gt; 
 &lt;li&gt;Its main usecase is to save any webpages protected beyond authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FEATURE REQUESTS AND FUTURE&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;SurfSense is actively being developed.&lt;/strong&gt; While it's not yet production-ready, you can help us speed up the process.&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.gg/ejRNvftDp9"&gt;SurfSense Discord&lt;/a&gt; and help shape the future of SurfSense!&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with our development progress and upcoming features!&lt;br /&gt; Check out our public roadmap and contribute your ideas or feedback:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;View the Roadmap:&lt;/strong&gt; &lt;a href="https://github.com/users/MODSetter/projects/2"&gt;SurfSense Roadmap on GitHub Projects&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to get started?&lt;/h2&gt; 
&lt;h3&gt;Installation Options&lt;/h3&gt; 
&lt;p&gt;SurfSense provides two installation methods:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/docker-installation"&gt;Docker Installation&lt;/a&gt;&lt;/strong&gt; - The easiest way to get SurfSense up and running with all dependencies containerized.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Includes pgAdmin for database management through a web UI&lt;/li&gt; 
   &lt;li&gt;Supports environment variable customization via &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Flexible deployment options (full stack or core services only)&lt;/li&gt; 
   &lt;li&gt;No need to manually edit configuration files between environments&lt;/li&gt; 
   &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DOCKER_SETUP.md"&gt;Docker Setup Guide&lt;/a&gt; for detailed instructions&lt;/li&gt; 
   &lt;li&gt;For deployment scenarios and options, see &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DEPLOYMENT_GUIDE.md"&gt;Deployment Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/manual-installation"&gt;Manual Installation (Recommended)&lt;/a&gt;&lt;/strong&gt; - For users who prefer more control over their setup or need to customize their deployment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Both installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.&lt;/p&gt; 
&lt;p&gt;Before installation, make sure to complete the &lt;a href="https://www.surfsense.net/docs/"&gt;prerequisite setup steps&lt;/a&gt; including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PGVector setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Processing ETL Service&lt;/strong&gt; (choose one): 
  &lt;ul&gt; 
   &lt;li&gt;Unstructured.io API key (supports 34+ formats)&lt;/li&gt; 
   &lt;li&gt;LlamaIndex API key (enhanced parsing, supports 50+ formats)&lt;/li&gt; 
   &lt;li&gt;Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other required API keys&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4" alt="updated_researcher" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Search Spaces&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099" alt="search_spaces" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Manage Documents&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d" alt="documents" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Podcast Agent&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c" alt="podcasts" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agent Chat&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491" alt="git_chat" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40" alt="ext1" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7" alt="ext2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;BackEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;: Modern, fast web framework for building APIs with Python&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PostgreSQL with pgvector&lt;/strong&gt;: Database with vector search capabilities for similarity searches&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SQLAlchemy&lt;/strong&gt;: SQL toolkit and ORM (Object-Relational Mapping) for database interactions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alembic&lt;/strong&gt;: A database migrations tool for SQLAlchemy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI Users&lt;/strong&gt;: Authentication and user management with JWT and OAuth support&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: Framework for developing AI-agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: Framework for developing AI-powered applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Integration&lt;/strong&gt;: Integration with LLM models through LiteLLM&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rerankers&lt;/strong&gt;: Advanced result ranking for improved search relevance&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt;: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector Embeddings&lt;/strong&gt;: Document and text embeddings for semantic search&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgvector&lt;/strong&gt;: PostgreSQL extension for efficient vector similarity operations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chonkie&lt;/strong&gt;: Advanced document chunking and embedding library&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Uses &lt;code&gt;AutoEmbeddings&lt;/code&gt; for flexible embedding model selection&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;LateChunker&lt;/code&gt; for optimized document chunking based on embedding model's max sequence length&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;FrontEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next.js 15.2.3&lt;/strong&gt;: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React 19.0.0&lt;/strong&gt;: JavaScript library for building user interfaces.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: Static type-checking for JavaScript, enhancing code quality and developer experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vercel AI SDK Kit UI Stream Protocol&lt;/strong&gt;: To create scalable chat UI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tailwind CSS 4.x&lt;/strong&gt;: Utility-first CSS framework for building custom UI designs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Shadcn&lt;/strong&gt;: Headless components library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lucide React&lt;/strong&gt;: Icon set implemented as React components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Framer Motion&lt;/strong&gt;: Animation library for React.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sonner&lt;/strong&gt;: Toast notification library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Geist&lt;/strong&gt;: Font family from Vercel.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React Hook Form&lt;/strong&gt;: Form state management and validation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zod&lt;/strong&gt;: TypeScript-first schema validation with static type inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@hookform/resolvers&lt;/strong&gt;: Resolvers for using validation libraries with React Hook Form.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@tanstack/react-table&lt;/strong&gt;: Headless UI for building powerful tables &amp;amp; datagrids.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;DevOps&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Container platform for consistent deployment across environments&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;: Tool for defining and running multi-container Docker applications&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgAdmin&lt;/strong&gt;: Web-based PostgreSQL administration tool included in Docker setup&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Extension&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Manifest v3 on Plasmo&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add More Connectors.&lt;/li&gt; 
 &lt;li&gt;Patch minor bugs.&lt;/li&gt; 
 &lt;li&gt;Document Podcasts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome! A contribution can be as small as a â­ or even finding and creating issues. Fine-tuning the Backend is always desired.&lt;/p&gt; 
&lt;p&gt;For detailed contribution guidelines, please see our &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#MODSetter/SurfSense&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4" alt="Catalyst Project" width="200" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>google/computer-use-preview</title>
      <link>https://github.com/google/computer-use-preview</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Computer Use Preview&lt;/h1&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This section will guide you through setting up and running the Computer Use Preview model. Follow these steps to get started.&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/computer-use-preview.git
cd computer-use-preview
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Set up Python Virtual Environment and Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install Playwright and Browser Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install system dependencies required by Playwright for Chrome
playwright install-deps chrome

# Install the Chrome browser for Playwright
playwright install chrome
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Configuration&lt;/h3&gt; 
&lt;p&gt;You can get started using either the Gemini Developer API or Vertex AI.&lt;/p&gt; 
&lt;h4&gt;A. If using the Gemini Developer API:&lt;/h4&gt; 
&lt;p&gt;You need a Gemini API key to use the agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to add this to your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo 'export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"' &amp;gt;&amp;gt; .venv/bin/activate
# After editing, you'll need to deactivate and reactivate your virtual
# environment if it's already active:
deactivate
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;YOUR_GEMINI_API_KEY&lt;/code&gt; with your actual key.&lt;/p&gt; 
&lt;h4&gt;B. If using the Vertex AI Client:&lt;/h4&gt; 
&lt;p&gt;You need to explicitly use Vertex AI, then provide project and location to use the agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_VERTEXAI=true
export VERTEXAI_PROJECT="YOUR_PROJECT_ID"
export VERTEXAI_LOCATION="YOUR_LOCATION"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to add this to your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo 'export USE_VERTEXAI=true' &amp;gt;&amp;gt; .venv/bin/activate
echo 'export VERTEXAI_PROJECT="your-project-id"' &amp;gt;&amp;gt; .venv/bin/activate
echo 'export VERTEXAI_LOCATION="your-location"' &amp;gt;&amp;gt; .venv/bin/activate
# After editing, you'll need to deactivate and reactivate your virtual
# environment if it's already active:
deactivate
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;YOUR_PROJECT_ID&lt;/code&gt; and &lt;code&gt;YOUR_LOCATION&lt;/code&gt; with your actual project and location.&lt;/p&gt; 
&lt;h3&gt;3. Running the Tool&lt;/h3&gt; 
&lt;p&gt;The primary way to use the tool is via the &lt;code&gt;main.py&lt;/code&gt; script.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;General Command Structure:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query "Go to Google and type 'Hello World' into the search bar"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Environments:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can specify a particular environment with the &lt;code&gt;--env &amp;lt;environment&amp;gt;&lt;/code&gt; flag. Available options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;playwright&lt;/code&gt;: Runs the browser locally using Playwright.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;browserbase&lt;/code&gt;: Connects to a Browserbase instance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Local Playwright&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Runs the agent using a Chrome browser instance controlled locally by Playwright.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="playwright"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify an initial URL for the Playwright environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="playwright" --initial_url="https://www.google.com/search?q=latest+AI+news"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Browserbase&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Runs the agent using Browserbase as the browser backend. Ensure the proper Browserbase environment variables are set:&lt;code&gt;BROWSERBASE_API_KEY&lt;/code&gt; and &lt;code&gt;BROWSERBASE_PROJECT_ID&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="browserbase"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Agent CLI&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;main.py&lt;/code&gt; script is the command-line interface (CLI) for running the browser agent.&lt;/p&gt; 
&lt;h3&gt;Command-Line Arguments&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Argument&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
   &lt;th&gt;Supported Environment(s)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--query&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The natural language query for the browser agent to execute.&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--env&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The computer use environment to use. Must be one of the following: &lt;code&gt;playwright&lt;/code&gt;, or &lt;code&gt;browserbase&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--initial_url&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The initial URL to load when the browser starts.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.google.com"&gt;https://www.google.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--highlight_mouse&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;If specified, the agent will attempt to highlight the mouse cursor's position in the screenshots. This is useful for visual debugging.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;False (not highlighted)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;playwright&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GEMINI_API_KEY&lt;/td&gt; 
   &lt;td&gt;Your API key for the Gemini model.&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BROWSERBASE_API_KEY&lt;/td&gt; 
   &lt;td&gt;Your API key for Browserbase.&lt;/td&gt; 
   &lt;td&gt;Yes (when using the browserbase environment)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BROWSERBASE_PROJECT_ID&lt;/td&gt; 
   &lt;td&gt;Your Project ID for Browserbase.&lt;/td&gt; 
   &lt;td&gt;Yes (when using the browserbase environment)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>cython/cython</title>
      <link>https://github.com/cython/cython</link>
      <description>&lt;p&gt;The most widely used Python to C compiler&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to Cython!&lt;/h1&gt; 
&lt;p&gt;Cython is an optimising Python compiler that makes writing C extensions for Python as easy as Python itself.&lt;/p&gt; 
&lt;p&gt;Cython translates Python code to C/C++ code, but additionally supports calling C functions and declaring C types on variables and class attributes. This allows broad to fine-grained manual tuning that lets the compiler generate very efficient C code from Cython code.&lt;/p&gt; 
&lt;p&gt;This makes Cython the ideal language for wrapping external C libraries, and for fast C modules that speed up the execution of Python code.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Official website: &lt;a href="https://cython.org/"&gt;https://cython.org/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Documentation: &lt;a href="https://docs.cython.org/"&gt;https://docs.cython.org/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Github repository: &lt;a href="https://github.com/cython/cython"&gt;https://github.com/cython/cython&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Wiki: &lt;a href="https://github.com/cython/cython/wiki"&gt;https://github.com/cython/cython/wiki&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Cython has &lt;code&gt;more than 70 million downloads &amp;lt;https://pypistats.org/packages/cython&amp;gt;&lt;/code&gt;_ per month on PyPI. You can &lt;strong&gt;support the Cython project&lt;/strong&gt; via &lt;code&gt;Github Sponsors &amp;lt;https://github.com/users/scoder/sponsorship&amp;gt;&lt;/code&gt;_ or &lt;code&gt;Tidelift &amp;lt;https://tidelift.com/subscription/pkg/pypi-cython&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;h2&gt;Installation:&lt;/h2&gt; 
&lt;p&gt;If you already have a C compiler, just run following command::&lt;/p&gt; 
&lt;p&gt;pip install Cython&lt;/p&gt; 
&lt;p&gt;otherwise, see &lt;code&gt;the installation page &amp;lt;https://docs.cython.org/en/latest/src/quickstart/install.html&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;h2&gt;License:&lt;/h2&gt; 
&lt;p&gt;The original Pyrex program, which Cython is based on, was licensed "free of restrictions" (see below). Cython itself is licensed under the permissive &lt;strong&gt;Apache License&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;LICENSE.txt &amp;lt;https://github.com/cython/cython/blob/master/LICENSE.txt&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;h2&gt;Contributing:&lt;/h2&gt; 
&lt;p&gt;Want to contribute to the Cython project? Here is some &lt;code&gt;help to get you started &amp;lt;https://github.com/cython/cython/blob/master/docs/CONTRIBUTING.rst&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;h2&gt;Differences to other Python compilers&lt;/h2&gt; 
&lt;p&gt;Started as a project in the early 2000s, Cython has outlived &lt;code&gt;most other attempts &amp;lt;https://wiki.python.org/moin/PythonImplementations#Compilers&amp;gt;&lt;/code&gt;_ at producing static compilers for the Python language.&lt;/p&gt; 
&lt;p&gt;Similar projects that have a relevance today include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;PyPy &amp;lt;https://www.pypy.org/&amp;gt;&lt;/code&gt;_, a Python implementation with a JIT compiler.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Pros: JIT compilation with runtime optimisations, fully language compliant, good integration with external C/C++ code&lt;/li&gt; 
   &lt;li&gt;Cons: non-CPython runtime, relatively large resource usage of the runtime, limited compatibility with CPython extensions, non-obvious performance results&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Numba &amp;lt;http://numba.pydata.org/&amp;gt;&lt;/code&gt;_, a Python extension that features a JIT compiler for a subset of the language, based on the LLVM compiler infrastructure (probably best known for its &lt;code&gt;clang&lt;/code&gt; C compiler). It mostly targets numerical code that uses NumPy.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Pros: JIT compilation with runtime optimisations&lt;/li&gt; 
   &lt;li&gt;Cons: limited language support, relatively large runtime dependency (LLVM), non-obvious performance results&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Pythran &amp;lt;https://pythran.readthedocs.io/&amp;gt;&lt;/code&gt;&lt;em&gt;, a static Python-to-C++ extension compiler for a subset of the language, mostly targeted at numerical computation. Pythran can be (and is probably best) used as an additional &lt;code&gt;backend for NumPy code &amp;lt;https://cython.readthedocs.io/en/latest/src/userguide/numpy_pythran.html&amp;gt;&lt;/code&gt;&lt;/em&gt; in Cython.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;mypyc &amp;lt;https://mypyc.readthedocs.io/&amp;gt;&lt;/code&gt;&lt;em&gt;, a static Python-to-C extension compiler, based on the &lt;code&gt;mypy &amp;lt;http://www.mypy-lang.org/&amp;gt;&lt;/code&gt;&lt;/em&gt; static Python analyser. Like Cython's &lt;code&gt;pure Python mode &amp;lt;https://cython.readthedocs.io/en/latest/src/tutorial/pure.html&amp;gt;&lt;/code&gt;_, mypyc can make use of PEP-484 type annotations to optimise code for static types.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Pros: good support for language and PEP-484 typing, good type inference, reasonable performance gains&lt;/li&gt; 
   &lt;li&gt;Cons: no support for low-level optimisations and typing, opinionated Python type interpretation, reduced Python compatibility and introspection after compilation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Nuitka &amp;lt;https://nuitka.net/&amp;gt;&lt;/code&gt;_, a static Python-to-C extension compiler.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Pros: highly language compliant, reasonable performance gains, support for static application linking (similar to &lt;code&gt;cython_freeze &amp;lt;https://github.com/cython/cython/blob/master/bin/cython_freeze&amp;gt;&lt;/code&gt;_ but with the ability to bundle library dependencies into a self-contained executable)&lt;/li&gt; 
   &lt;li&gt;Cons: no support for low-level optimisations and typing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In comparison to the above, Cython provides&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;fast, efficient and highly compliant support for almost all Python language features, including dynamic features and introspection&lt;/li&gt; 
 &lt;li&gt;full runtime compatibility with all still-in-use and future versions of CPython&lt;/li&gt; 
 &lt;li&gt;"generate once, compile everywhere" C code generation that allows for reproducible performance results and testing&lt;/li&gt; 
 &lt;li&gt;C compile time adaptation to the target platform and Python version&lt;/li&gt; 
 &lt;li&gt;support for other C-API implementations, including PyPy and Pyston&lt;/li&gt; 
 &lt;li&gt;seamless integration with C/C++ code&lt;/li&gt; 
 &lt;li&gt;broad support for manual optimisation and tuning down to the C level&lt;/li&gt; 
 &lt;li&gt;a large user base with thousands of libraries, packages and tools&lt;/li&gt; 
 &lt;li&gt;more than two decades of bug fixing and static code optimisations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;The following is from Pyrex:&lt;/h2&gt; 
&lt;p&gt;Cython was originally based on &lt;code&gt;Pyrex &amp;lt;https://www.cosc.canterbury.ac.nz/~greg/python/Pyrex/&amp;gt;&lt;/code&gt;_ by Greg Ewing, with the following written in the Pyrex readme document:&lt;/p&gt; 
&lt;p&gt;This is a development version of Pyrex, a language for writing Python extension modules.&lt;/p&gt; 
&lt;p&gt;For more info, take a look at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Doc/About.html for a description of the language&lt;/li&gt; 
 &lt;li&gt;INSTALL.txt for installation instructions&lt;/li&gt; 
 &lt;li&gt;USAGE.txt for usage instructions&lt;/li&gt; 
 &lt;li&gt;Demos for usage examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Comments, suggestions, bug reports, etc. are most welcome!&lt;/p&gt; 
&lt;p&gt;Copyright stuff: Pyrex is free of restrictions. You may use, redistribute, modify and distribute modified versions.&lt;/p&gt; 
&lt;p&gt;The latest version of Pyrex can be found &lt;code&gt;here &amp;lt;https://www.cosc.canterbury.ac.nz/~greg/python/Pyrex/&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;| Greg Ewing, Computer Science Dept | University of Canterbury | Christchurch, New Zealand&lt;/p&gt; 
&lt;p&gt;A citizen of NewZealandCorp, a wholly-owned subsidiary of USA Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>opendatalab/MinerU</title>
      <link>https://github.com/opendatalab/MinerU</link>
      <description>&lt;p&gt;Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" xmlns="http://www.w3.org/1999/html"&gt; 
 &lt;!-- logo --&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/images/MinerU-logo.png" width="300px" style="vertical-align:middle;" /&gt; &lt;/p&gt; 
 &lt;!-- icon --&gt; 
 &lt;p&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU.svg?sanitize=true" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/forks/opendatalab/MinerU.svg?sanitize=true" alt="forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-raw/opendatalab/MinerU" alt="open issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU" alt="issue resolution" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/v/mineru" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/mineru" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru/month" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/gist/myhloli/a3cb16570ab3cfeadf9d8f0ac91b4fca/mineru_demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2409.18839"&gt;&lt;img src="https://img.shields.io/badge/MinerU-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.22186"&gt;&lt;img src="https://img.shields.io/badge/MinerU2.5-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11174" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11174" alt="opendatalab%2FMinerU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- language --&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README_zh-CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- hot link --&gt; 
 &lt;p align="center"&gt; ğŸš€&lt;a href="https://mineru.net/?source=github"&gt;Access MinerU Nowâ†’âœ… Zero-Install Web Version âœ… Full-Featured Desktop Client âœ… Instant API Access; Skip deployment headaches â€“ get all product formats in one click. Developers, dive in!&lt;/a&gt; &lt;/p&gt; 
 &lt;!-- join us --&gt; 
 &lt;p align="center"&gt; ğŸ‘‹ join us on &lt;a href="https://discord.gg/Tdedn9GTXq" target="_blank"&gt;Discord&lt;/a&gt; and &lt;a href="https://mineru.net/community-portal/?aliasId=3c430f94" target="_blank"&gt;WeChat&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;Changelog&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/26 2.5.4 released&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸ‰ğŸ‰ The MinerU2.5 &lt;a href="https://arxiv.org/abs/2509.22186"&gt;Technical Report&lt;/a&gt; is now available! We welcome you to read it for a comprehensive overview of its model architecture, training strategy, data engineering and evaluation results.&lt;/li&gt; 
   &lt;li&gt;Fixed an issue where some &lt;code&gt;PDF&lt;/code&gt; files were mistakenly identified as &lt;code&gt;AI&lt;/code&gt; files, causing parsing failures&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/20 2.5.3 Released&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; backend compatibility fixes for torch 2.8.0.&lt;/li&gt; 
   &lt;li&gt;Reduced default concurrency for vLLM async backend to lower server pressure and avoid connection closure issues caused by high load.&lt;/li&gt; 
   &lt;li&gt;More compatibility-related details can be found in the &lt;a href="https://github.com/opendatalab/MinerU/discussions/3548"&gt;announcement&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/19 2.5.2 Released&lt;/p&gt; &lt;p&gt;We are officially releasing MinerU2.5, currently the most powerful multimodal large model for document parsing. With only 1.2B parameters, MinerU2.5's accuracy on the OmniDocBench benchmark comprehensively surpasses top-tier multimodal models like Gemini 2.5 Pro, GPT-4o, and Qwen2.5-VL-72B. It also significantly outperforms leading specialized models such as dots.ocr, MonkeyOCR, and PP-StructureV3. The model has been released on &lt;a href="https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/opendatalab/MinerU2.5-2509-1.2B"&gt;ModelScope&lt;/a&gt; platforms. Welcome to download and use!&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Core Highlights: 
    &lt;ul&gt; 
     &lt;li&gt;SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.&lt;/li&gt; 
     &lt;li&gt;Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Key Capability Enhancements: 
    &lt;ul&gt; 
     &lt;li&gt;Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references.&lt;/li&gt; 
     &lt;li&gt;Table Parsing: Drastically improves parsing for challenging cases, including rotated tables, borderless/semi-structured tables, and long/complex tables.&lt;/li&gt; 
     &lt;li&gt;Formula Recognition: Significantly boosts accuracy for complex, long-form, and hybrid Chinese-English formulas, greatly enhancing the parsing capability for mathematical documents.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Additionally, with the release of vlm 2.5, we have made some adjustments to the repository:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2.&lt;/li&gt; 
   &lt;li&gt;VLM inference-related code has been moved to &lt;a href="https://github.com/opendatalab/mineru-vl-utils"&gt;mineru_vl_utils&lt;/a&gt;, reducing coupling with the main mineru repository and facilitating independent iteration in the future.&lt;/li&gt; 
   &lt;li&gt;The vlm accelerated inference framework has been switched from &lt;code&gt;sglang&lt;/code&gt; to &lt;code&gt;vllm&lt;/code&gt;, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework.&lt;/li&gt; 
   &lt;li&gt;Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file &lt;code&gt;middle.json&lt;/code&gt; and result file &lt;code&gt;content_list.json&lt;/code&gt;. Please refer to the &lt;a href="https://opendatalab.github.io/MinerU/reference/output_files/"&gt;documentation&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Other repository optimizations:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Removed file extension whitelist validation for input files. When input files are PDF documents or images, there are no longer requirements for file extensions, improving usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;History Log&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/10 2.2.2 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where the new table recognition model would affect the overall parsing task when some table parsing failed&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/08 2.2.1 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where some newly added models were not downloaded when using the model download command.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/05 2.2.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; Major Updates 
    &lt;ul&gt; 
     &lt;li&gt;In this version, we focused on improving table parsing accuracy by introducing a new &lt;a href="https://github.com/RapidAI/TableStructureRec"&gt;wired table recognition model&lt;/a&gt; and a brand-new hybrid table structure parsing algorithm, significantly enhancing the table recognition capabilities of the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt; 
     &lt;li&gt;We also added support for cross-page table merging, which is supported by both &lt;code&gt;pipeline&lt;/code&gt; and &lt;code&gt;vlm&lt;/code&gt; backends, further improving the completeness and accuracy of table parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; Other Updates 
    &lt;ul&gt; 
     &lt;li&gt;The &lt;code&gt;pipeline&lt;/code&gt; backend now supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; added OCR capability support for Thai and Greek, and updated the English OCR model to the latest version. English recognition accuracy improved by 11%, Thai recognition model accuracy is 82.68%, and Greek recognition model accuracy is 89.28% (by PPOCRv5)&lt;/li&gt; 
     &lt;li&gt;Added &lt;code&gt;bbox&lt;/code&gt; field (mapped to 0-1000 range) in the output &lt;code&gt;content_list.json&lt;/code&gt;, making it convenient for users to directly obtain position information for each content block&lt;/li&gt; 
     &lt;li&gt;Removed the &lt;code&gt;pipeline_old_linux&lt;/code&gt; installation option, no longer supporting legacy Linux systems such as &lt;code&gt;CentOS 7&lt;/code&gt;, to provide better support for &lt;code&gt;uv&lt;/code&gt;'s &lt;code&gt;sync&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt; commands&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/08/01 2.1.10 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed an issue in the &lt;code&gt;pipeline&lt;/code&gt; backend where block overlap caused the parsing results to deviate from expectations #3232&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/30 2.1.9 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.1 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/28 2.1.8 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9.post5 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/27 2.1.7 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.0 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/26 2.1.6 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed table parsing issues in handwritten documents when using &lt;code&gt;vlm&lt;/code&gt; backend&lt;/li&gt; 
   &lt;li&gt;Fixed visualization box position drift issue when document is rotated #3175&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/24 2.1.5 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9 version adaptation, synchronously upgrading the dockerfile base image to sglang 0.4.9.post3&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/23 2.1.4 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed the issue of excessive memory consumption during the &lt;code&gt;MFR&lt;/code&gt; step in the &lt;code&gt;pipeline&lt;/code&gt; backend under certain scenarios #2771&lt;/li&gt; 
     &lt;li&gt;Fixed the inaccurate matching between &lt;code&gt;image&lt;/code&gt;/&lt;code&gt;table&lt;/code&gt; and &lt;code&gt;caption&lt;/code&gt;/&lt;code&gt;footnote&lt;/code&gt; under certain conditions #3129&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/16 2.1.1 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed text block content loss issue that could occur in certain &lt;code&gt;pipeline&lt;/code&gt; scenarios #3005&lt;/li&gt; 
     &lt;li&gt;Fixed issue where &lt;code&gt;sglang-client&lt;/code&gt; required unnecessary packages like &lt;code&gt;torch&lt;/code&gt; #2968&lt;/li&gt; 
     &lt;li&gt;Updated &lt;code&gt;dockerfile&lt;/code&gt; to fix incomplete text content parsing due to missing fonts in Linux #2915&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Usability improvements&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Updated &lt;code&gt;compose.yaml&lt;/code&gt; to facilitate direct startup of &lt;code&gt;sglang-server&lt;/code&gt;, &lt;code&gt;mineru-api&lt;/code&gt;, and &lt;code&gt;mineru-gradio&lt;/code&gt; services&lt;/li&gt; 
     &lt;li&gt;Launched brand new &lt;a href="https://opendatalab.github.io/MinerU/"&gt;online documentation site&lt;/a&gt;, simplified readme, providing better documentation experience&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/05 2.1.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;This is the first major update of MinerU 2, which includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes. The detailed update contents are as follows:&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimizations:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Significantly improved preprocessing speed for documents with specific resolutions (around 2000 pixels on the long side).&lt;/li&gt; 
     &lt;li&gt;Greatly enhanced post-processing speed when the &lt;code&gt;pipeline&lt;/code&gt; backend handles batch processing of documents with fewer pages (&amp;lt;10 pages).&lt;/li&gt; 
     &lt;li&gt;Layout analysis speed of the &lt;code&gt;pipeline&lt;/code&gt; backend has been increased by approximately 20%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Experience Enhancements:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Built-in ready-to-use &lt;code&gt;fastapi service&lt;/code&gt; and &lt;code&gt;gradio webui&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#advanced-usage-via-api-webui-sglang-clientserver"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;Adapted to &lt;code&gt;sglang&lt;/code&gt; version &lt;code&gt;0.4.8&lt;/code&gt;, significantly reducing the GPU memory requirements for the &lt;code&gt;vlm-sglang&lt;/code&gt; backend. It can now run on graphics cards with as little as &lt;code&gt;8GB GPU memory&lt;/code&gt; (Turing architecture or newer).&lt;/li&gt; 
     &lt;li&gt;Added transparent parameter passing for all commands related to &lt;code&gt;sglang&lt;/code&gt;, allowing the &lt;code&gt;sglang-engine&lt;/code&gt; backend to receive all &lt;code&gt;sglang&lt;/code&gt; parameters consistently with the &lt;code&gt;sglang-server&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Supports feature extensions based on configuration files, including &lt;code&gt;custom formula delimiters&lt;/code&gt;, &lt;code&gt;enabling heading classification&lt;/code&gt;, and &lt;code&gt;customizing local model directories&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#extending-mineru-functionality-with-configuration-files"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Features:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Updated the &lt;code&gt;pipeline&lt;/code&gt; backend with the PP-OCRv5 multilingual text recognition model, supporting text recognition in 37 languages such as French, Spanish, Portuguese, Russian, and Korean, with an average accuracy improvement of over 30%. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Introduced limited support for vertical text layout in the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/20 2.0.6 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed occasional parsing interruptions caused by invalid block content in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed parsing interruptions caused by incomplete table structures in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/17 2.0.5 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where models were still required to be downloaded in the &lt;code&gt;sglang-client&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the &lt;code&gt;sglang-client&lt;/code&gt; mode unnecessarily depended on packages like &lt;code&gt;torch&lt;/code&gt; during runtime.&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where only the first instance would take effect when attempting to launch multiple &lt;code&gt;sglang-client&lt;/code&gt; instances via multiple URLs within the same process&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/15 2.0.3 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed a configuration file key-value update error that occurred when downloading model type was set to &lt;code&gt;all&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the formula and table feature toggle switches were not working in &lt;code&gt;command line mode&lt;/code&gt;, causing the features to remain enabled.&lt;/li&gt; 
   &lt;li&gt;Fixed compatibility issues with sglang version 0.4.7 in the &lt;code&gt;sglang-engine&lt;/code&gt; mode.&lt;/li&gt; 
   &lt;li&gt;Updated Dockerfile and installation documentation for deploying the full version of MinerU in sglang environment&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/13 2.0.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New Architecture&lt;/strong&gt;: MinerU 2.0 has been deeply restructured in code organization and interaction methods, significantly improving system usability, maintainability, and extensibility. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Removal of Third-party Dependency Limitations&lt;/strong&gt;: Completely eliminated the dependency on &lt;code&gt;pymupdf&lt;/code&gt;, moving the project toward a more open and compliant open-source direction.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ready-to-use, Easy Configuration&lt;/strong&gt;: No need to manually edit JSON configuration files; most parameters can now be set directly via command line or API.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Automatic Model Management&lt;/strong&gt;: Added automatic model download and update mechanisms, allowing users to complete model deployment without manual intervention.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Offline Deployment Friendly&lt;/strong&gt;: Provides built-in model download commands, supporting deployment requirements in completely offline environments.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Streamlined Code Structure&lt;/strong&gt;: Removed thousands of lines of redundant code, simplified class inheritance logic, significantly improving code readability and development efficiency.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Unified Intermediate Format Output&lt;/strong&gt;: Adopted standardized &lt;code&gt;middle_json&lt;/code&gt; format, compatible with most secondary development scenarios based on this format, ensuring seamless ecosystem business migration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Model&lt;/strong&gt;: MinerU 2.0 integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-speed, high-precision document understanding. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Small Model, Big Capabilities&lt;/strong&gt;: With parameters under 1B, yet surpassing traditional 72B-level vision-language models (VLMs) in parsing accuracy.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multiple Functions in One&lt;/strong&gt;: A single model covers multilingual recognition, handwriting recognition, layout analysis, table parsing, formula recognition, reading order sorting, and other core tasks.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ultimate Inference Speed&lt;/strong&gt;: Achieves peak throughput exceeding 10,000 tokens/s through &lt;code&gt;sglang&lt;/code&gt; acceleration on a single NVIDIA 4090 card, easily handling large-scale document processing requirements.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Online Experience&lt;/strong&gt;: You can experience our brand-new VLM model on &lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;MinerU.net&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;Hugging Face&lt;/a&gt;, and &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Incompatible Changes Notice&lt;/strong&gt;: To improve overall architectural rationality and long-term maintainability, this version contains some incompatible changes: 
    &lt;ul&gt; 
     &lt;li&gt;Python package name changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;, and the command-line tool changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;. Please update your scripts and command calls accordingly.&lt;/li&gt; 
     &lt;li&gt;For modular system design and ecosystem consistency considerations, MinerU 2.0 no longer includes the LibreOffice document conversion module. If you need to process Office documents, we recommend converting them to PDF format through an independently deployed LibreOffice service before proceeding with subsequent parsing operations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/05/24 Release 1.3.12&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for PPOCRv5 models, updated &lt;code&gt;ch_server&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt;, and &lt;code&gt;ch_lite&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;In testing, we found that PPOCRv5(server) has some improvement for handwritten documents, but has slightly lower accuracy than v4_server_doc for other document types, so the default ch model remains unchanged as &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Since PPOCRv5 has enhanced recognition capabilities for handwriting and special characters, you can manually choose the PPOCRv5 model for Japanese-Traditional Chinese mixed scenarios and handwritten documents&lt;/li&gt; 
     &lt;li&gt;You can select the appropriate model through the lang parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line): 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;ch&lt;/code&gt;: &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (default) (Chinese/English/Japanese/Traditional Chinese mixed/15K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_server&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_mobile&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Added support for handwritten documents through optimized layout recognition of handwritten text areas 
    &lt;ul&gt; 
     &lt;li&gt;This feature is supported by default, no additional configuration required&lt;/li&gt; 
     &lt;li&gt;You can refer to the instructions above to manually select the PPOCRv5 model for better handwritten document parsing results&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;modelscope&lt;/code&gt; demos have been updated to versions that support handwriting recognition and PPOCRv5 models, which you can experience online&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/29 Release 1.3.10&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for custom formula delimiters, which can be configured by modifying the &lt;code&gt;latex-delimiter-config&lt;/code&gt; section in the &lt;code&gt;magic-pdf.json&lt;/code&gt; file in your user directory.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/27 Release 1.3.9&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Optimized formula parsing functionality, improved formula rendering success rate&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/23 Release 1.3.8&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The default &lt;code&gt;ocr&lt;/code&gt; model (&lt;code&gt;ch&lt;/code&gt;) has been updated to &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; is trained on a mixture of more Chinese document data and PP-OCR training data based on &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, adding recognition capabilities for some traditional Chinese characters, Japanese, and special characters. It can recognize over 15,000 characters and improves both document-specific and general text recognition abilities.&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_recognition.html#_3"&gt;Performance comparison of PP-OCRv4_server_rec_doc/PP-OCRv4_server_rec/PP-OCRv4_mobile_rec&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;After verification, the &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; model shows significant accuracy improvements in Chinese/English/Japanese/Traditional Chinese in both single language and mixed language scenarios, with comparable speed to &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, making it suitable for most use cases.&lt;/li&gt; 
     &lt;li&gt;In some pure English scenarios, &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; may have word adhesion issues, while &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; performs better in these cases. Therefore, we've kept the &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; model, which users can access by adding the parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/22 Release 1.3.7&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where the lang parameter was ineffective during table parsing model initialization&lt;/li&gt; 
   &lt;li&gt;Fixed the significant speed reduction of OCR and table parsing in &lt;code&gt;cpu&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/16 Release 1.3.4&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Slightly improved OCR-det speed by removing some unnecessary blocks&lt;/li&gt; 
   &lt;li&gt;Fixed page-internal sorting errors caused by footnotes in certain cases&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/12 Release 1.3.2&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed dependency version incompatibility issues when installing on Windows with Python 3.13&lt;/li&gt; 
   &lt;li&gt;Optimized memory usage during batch inference&lt;/li&gt; 
   &lt;li&gt;Improved parsing of tables rotated 90 degrees&lt;/li&gt; 
   &lt;li&gt;Enhanced parsing of oversized tables in financial report samples&lt;/li&gt; 
   &lt;li&gt;Fixed the occasional word adhesion issue in English text areas when OCR language is not specified (model update required)&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/08 Release 1.3.1&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed several compatibility issues 
    &lt;ul&gt; 
     &lt;li&gt;Added support for Python 3.13&lt;/li&gt; 
     &lt;li&gt;Made final adaptations for outdated Linux systems (such as CentOS 7) with no guarantee of continued support in future versions, &lt;a href="https://github.com/opendatalab/MinerU/issues/1004"&gt;installation instructions&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/03 Release 1.3.0&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Installation and compatibility optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Resolved compatibility issues caused by &lt;code&gt;detectron2&lt;/code&gt; by removing &lt;code&gt;layoutlmv3&lt;/code&gt; usage in layout&lt;/li&gt; 
     &lt;li&gt;Extended torch version compatibility to 2.2~2.6 (excluding 2.5)&lt;/li&gt; 
     &lt;li&gt;Added CUDA compatibility for versions 11.8/12.4/12.6/12.8 (CUDA version determined by torch), solving compatibility issues for users with 50-series and H-series GPUs&lt;/li&gt; 
     &lt;li&gt;Extended Python compatibility to versions 3.10~3.12, fixing the issue of automatic downgrade to version 0.6.1 when installing in non-3.10 environments&lt;/li&gt; 
     &lt;li&gt;Optimized offline deployment process, eliminating the need to download any model files after successful deployment&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Enhanced parsing speed for batches of small files by supporting batch processing of multiple PDF files (&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/demo/batch_demo.py"&gt;script example&lt;/a&gt;), with formula parsing speed improved by up to 1400% and overall parsing speed improved by up to 500% compared to version 1.0.1&lt;/li&gt; 
     &lt;li&gt;Reduced memory usage and improved parsing speed by optimizing MFR model loading and usage (requires re-running the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/how_to_download_models_zh_cn.md"&gt;model download process&lt;/a&gt; to get incremental updates to model files)&lt;/li&gt; 
     &lt;li&gt;Optimized GPU memory usage, requiring only 6GB minimum to run this project&lt;/li&gt; 
     &lt;li&gt;Improved running speed on MPS devices&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Parsing effect optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Updated MFR model to &lt;code&gt;unimernet(2503)&lt;/code&gt;, fixing line break loss issues in multi-line formulas&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Usability optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Completely replaced the &lt;code&gt;paddle&lt;/code&gt; framework and &lt;code&gt;paddleocr&lt;/code&gt; in the project by using &lt;code&gt;paddleocr2torch&lt;/code&gt;, resolving conflicts between &lt;code&gt;paddle&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt;, as well as thread safety issues caused by the &lt;code&gt;paddle&lt;/code&gt; framework&lt;/li&gt; 
     &lt;li&gt;Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/03/03 1.2.1 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the impact on punctuation marks during full-width to half-width conversion of letters and numbers&lt;/li&gt; 
   &lt;li&gt;Fixed caption matching inaccuracies in certain scenarios&lt;/li&gt; 
   &lt;li&gt;Fixed formula span loss issues in certain scenarios&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/02/24 1.2.0 released&lt;/summary&gt; 
  &lt;p&gt;This version includes several fixes and improvements to enhance parsing efficiency and accuracy:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Increased classification speed for PDF documents in auto mode.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Improved parsing logic for documents containing watermarks, significantly enhancing the parsing results for such documents.&lt;/li&gt; 
     &lt;li&gt;Enhanced the matching logic for multiple images/tables and captions within a single page, improving the accuracy of image-text matching in complex layouts.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed an issue where image/table spans were incorrectly filled into text blocks under certain conditions.&lt;/li&gt; 
     &lt;li&gt;Resolved an issue where title blocks were empty in some cases.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/22 1.1.0 released&lt;/summary&gt; 
  &lt;p&gt;In this version we have focused on improving parsing accuracy and efficiency:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model capability upgrade&lt;/strong&gt; (requires re-executing the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;model download process&lt;/a&gt; to obtain incremental updates of model files) 
    &lt;ul&gt; 
     &lt;li&gt;The layout recognition model has been upgraded to the latest &lt;code&gt;doclayout_yolo(2501)&lt;/code&gt; model, improving layout recognition accuracy.&lt;/li&gt; 
     &lt;li&gt;The formula parsing model has been upgraded to the latest &lt;code&gt;unimernet(2501)&lt;/code&gt; model, improving formula recognition accuracy.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;On devices that meet certain configuration requirements (16GB+ VRAM), by optimizing resource usage and restructuring the processing pipeline, overall parsing speed has been increased by more than 50%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing effect optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Added a new heading classification feature (testing version, enabled by default) to the online demo (&lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;mineru.net&lt;/a&gt;/&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;huggingface&lt;/a&gt;/&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;modelscope&lt;/a&gt;), which supports hierarchical classification of headings, thereby enhancing document structuring.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/10 1.0.1 released&lt;/summary&gt; 
  &lt;p&gt;This is our first official release, where we have introduced a completely new API interface and enhanced compatibility through extensive refactoring, as well as a brand new automatic language identification feature:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New API Interface&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.&lt;/li&gt; 
     &lt;li&gt;For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enhanced Compatibility&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By optimizing the dependency environment and configuration items, we ensure stable and efficient operation on ARM architecture Linux systems.&lt;/li&gt; 
     &lt;li&gt;We have deeply integrated with Huawei Ascend NPU acceleration, providing autonomous and controllable high-performance computing capabilities. This supports the localization and development of AI application platforms in China. &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/README_Ascend_NPU_Acceleration_zh_CN.md"&gt;Ascend NPU Acceleration&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Automatic Language Identification&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By introducing a new language recognition model, setting the &lt;code&gt;lang&lt;/code&gt; configuration to &lt;code&gt;auto&lt;/code&gt; during document parsing will automatically select the appropriate OCR language model, improving the accuracy of scanned document parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/22 0.10.0 released&lt;/summary&gt; 
  &lt;p&gt;Introducing hybrid OCR text extraction capabilities:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Significantly improved parsing performance in complex text distribution scenarios such as dense formulas, irregular span regions, and text represented by images.&lt;/li&gt; 
   &lt;li&gt;Combines the dual advantages of accurate content extraction and faster speed in text mode, and more precise span/line region recognition in OCR mode.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/15 0.9.3 released&lt;/summary&gt; 
  &lt;p&gt;Integrated &lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt; for table recognition, improving single-table parsing speed by more than 10 times, with higher accuracy and lower GPU memory usage.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/06 0.9.2 released&lt;/summary&gt; 
  &lt;p&gt;Integrated the &lt;a href="https://huggingface.co/U4R/StructTable-InternVL2-1B"&gt;StructTable-InternVL2-1B&lt;/a&gt; model for table recognition functionality.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/10/31 0.9.0 released&lt;/summary&gt; 
  &lt;p&gt;This is a major new version with extensive code refactoring, addressing numerous issues, improving performance, reducing hardware requirements, and enhancing usability:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Refactored the sorting module code to use &lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt; for reading order sorting, ensuring high accuracy in various layouts.&lt;/li&gt; 
   &lt;li&gt;Refactored the paragraph concatenation module to achieve good results in cross-column, cross-page, cross-figure, and cross-table scenarios.&lt;/li&gt; 
   &lt;li&gt;Refactored the list and table of contents recognition functions, significantly improving the accuracy of list blocks and table of contents blocks, as well as the parsing of corresponding text paragraphs.&lt;/li&gt; 
   &lt;li&gt;Refactored the matching logic for figures, tables, and descriptive text, greatly enhancing the accuracy of matching captions and footnotes to figures and tables, and reducing the loss rate of descriptive text to near zero.&lt;/li&gt; 
   &lt;li&gt;Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations"&gt;OCR Language Support List&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Added memory recycling logic and other memory optimization measures, significantly reducing memory usage. The memory requirement for enabling all acceleration features except table acceleration (layout/formula/OCR) has been reduced from 16GB to 8GB, and the memory requirement for enabling all acceleration features has been reduced from 24GB to 10GB.&lt;/li&gt; 
   &lt;li&gt;Optimized configuration file feature switches, adding an independent formula detection switch to significantly improve speed and parsing results when formula detection is not needed.&lt;/li&gt; 
   &lt;li&gt;Integrated &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit 1.0&lt;/a&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;Added the self-developed &lt;code&gt;doclayout_yolo&lt;/code&gt; model, which speeds up processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with &lt;code&gt;layoutlmv3&lt;/code&gt; via the configuration file.&lt;/li&gt; 
     &lt;li&gt;Upgraded formula parsing to &lt;code&gt;unimernet 0.2.1&lt;/code&gt;, improving formula parsing accuracy while significantly reducing memory usage.&lt;/li&gt; 
     &lt;li&gt;Due to the repository change for &lt;code&gt;PDF-Extract-Kit 1.0&lt;/code&gt;, you need to re-download the model. Please refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;How to Download Models&lt;/a&gt; for detailed steps.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/27 Version 0.8.1 released&lt;/summary&gt; 
  &lt;p&gt;Fixed some bugs, and providing a &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web_demo/README.md"&gt;localized deployment version&lt;/a&gt; of the &lt;a href="https://opendatalab.com/OpenSourceTools/Extractor/PDF/"&gt;online demo&lt;/a&gt; and the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web/README.md"&gt;front-end interface&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/09 Version 0.8.0 released&lt;/summary&gt; 
  &lt;p&gt;Supporting fast deployment with Dockerfile, and launching demos on Huggingface and Modelscope.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/30 Version 0.7.1 released&lt;/summary&gt; 
  &lt;p&gt;Add paddle tablemaster table recognition option&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/09 Version 0.7.0b1 released&lt;/summary&gt; 
  &lt;p&gt;Simplified installation process, added table recognition functionality&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/01 Version 0.6.2b1 released&lt;/summary&gt; 
  &lt;p&gt;Optimized dependency conflict issues and installation documentation&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/07/05 Initial open-source release&lt;/summary&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;h1&gt;MinerU&lt;/h1&gt; 
&lt;h2&gt;Project Introduction&lt;/h2&gt; 
&lt;p&gt;MinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format. MinerU was born during the pre-training process of &lt;a href="https://github.com/InternLM/InternLM"&gt;InternLM&lt;/a&gt;. We focus on solving symbol conversion issues in scientific literature and hope to contribute to technological development in the era of large models. Compared to well-known commercial products, MinerU is still young. If you encounter any issues or if the results are not as expected, please submit an issue on &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;issue&lt;/a&gt; and &lt;strong&gt;attach the relevant PDF&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c"&gt;https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.&lt;/li&gt; 
 &lt;li&gt;Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.&lt;/li&gt; 
 &lt;li&gt;Preserve the structure of the original document, including headings, paragraphs, lists, etc.&lt;/li&gt; 
 &lt;li&gt;Extract images, image descriptions, tables, table titles, and footnotes.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert formulas in the document to LaTeX format.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert tables in the document to HTML format.&lt;/li&gt; 
 &lt;li&gt;Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality.&lt;/li&gt; 
 &lt;li&gt;OCR supports detection and recognition of 84 languages.&lt;/li&gt; 
 &lt;li&gt;Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats.&lt;/li&gt; 
 &lt;li&gt;Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality.&lt;/li&gt; 
 &lt;li&gt;Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration&lt;/li&gt; 
 &lt;li&gt;Compatible with Windows, Linux, and Mac platforms.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;If you encounter any installation issues, please first consult the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#faq"&gt;FAQ&lt;/a&gt;. &lt;br /&gt; If the parsing results are not as expected, refer to the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#known-issues"&gt;Known Issues&lt;/a&gt;. &lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Online Experience&lt;/h2&gt; 
&lt;h3&gt;Official online web application&lt;/h3&gt; 
&lt;p&gt;The official online version has the same functionality as the client, with a beautiful interface and rich features, requires login to use&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Gradio-based online demo&lt;/h3&gt; 
&lt;p&gt;A WebUI developed based on Gradio, with a simple interface and only core parsing functionality, no login required&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Local Deployment&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Pre-installation Noticeâ€”Hardware and Software Environment Support&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To ensure the stability and reliability of the project, we only optimize and test for specific hardware and software environments during development. This ensures that users deploying and running the project on recommended system configurations will get the best performance with the fewest compatibility issues.&lt;/p&gt; 
 &lt;p&gt;By focusing resources on the mainline environment, our team can more efficiently resolve potential bugs and develop new features.&lt;/p&gt; 
 &lt;p&gt;In non-mainline environments, due to the diversity of hardware and software configurations, as well as third-party dependency compatibility issues, we cannot guarantee 100% project availability. Therefore, for users who wish to use this project in non-recommended environments, we suggest carefully reading the documentation and FAQ first. Most issues already have corresponding solutions in the FAQ. We also encourage community feedback to help us gradually expand support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;Parsing Backend&lt;/td&gt; 
   &lt;td&gt;pipeline&lt;/td&gt; 
   &lt;td&gt;vlm-transformers&lt;/td&gt; 
   &lt;td&gt;vlm-vllm&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operating System&lt;/td&gt; 
   &lt;td&gt;Linux / Windows / macOS&lt;/td&gt; 
   &lt;td&gt;Linux / Windows&lt;/td&gt; 
   &lt;td&gt;Linux / Windows (via WSL2)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Inference Support&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td colspan="2"&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPU Requirements&lt;/td&gt; 
   &lt;td&gt;Turing architecture and later, 6GB+ VRAM or Apple Silicon&lt;/td&gt; 
   &lt;td colspan="2"&gt;Turing architecture and later, 8GB+ VRAM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Memory Requirements&lt;/td&gt; 
   &lt;td colspan="3"&gt;Minimum 16GB+, recommended 32GB+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Disk Space Requirements&lt;/td&gt; 
   &lt;td colspan="3"&gt;20GB+, SSD recommended&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python Version&lt;/td&gt; 
   &lt;td colspan="3"&gt;3.10-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Install MinerU&lt;/h3&gt; 
&lt;h4&gt;Install MinerU using pip or uv&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install uv
uv pip install -U "mineru[core]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install MinerU from source code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/opendatalab/MinerU.git
cd MinerU
uv pip install -e .[core]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;code&gt;mineru[core]&lt;/code&gt; includes all core features except &lt;code&gt;vLLM&lt;/code&gt; acceleration, compatible with Windows / Linux / macOS systems, suitable for most users. If you need to use &lt;code&gt;vLLM&lt;/code&gt; acceleration for VLM model inference or install a lightweight client on edge devices, please refer to the documentation &lt;a href="https://opendatalab.github.io/MinerU/quick_start/extension_modules/"&gt;Extension Modules Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Deploy MinerU using Docker&lt;/h4&gt; 
&lt;p&gt;MinerU provides a convenient Docker deployment method, which helps quickly set up the environment and solve some tricky environment compatibility issues. You can get the &lt;a href="https://opendatalab.github.io/MinerU/quick_start/docker_deployment/"&gt;Docker Deployment Instructions&lt;/a&gt; in the documentation.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Using MinerU&lt;/h3&gt; 
&lt;p&gt;The simplest command line invocation is:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mineru -p &amp;lt;input_path&amp;gt; -o &amp;lt;output_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use MinerU for PDF parsing through various methods such as command line, API, and WebUI. For detailed instructions, please refer to the &lt;a href="https://opendatalab.github.io/MinerU/usage/"&gt;Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;TODO&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Reading order based on the model&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Recognition of &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt; in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Table recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Heading Classification&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Handwritten Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Vertical Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Latin Accent Mark Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Code block recognition in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/chemical_knowledge_introduction/introduction.pdf"&gt;Chemical formula recognition&lt;/a&gt;(mineru.net)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Geometric shape recognition&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Known Issues&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reading order is determined by the model based on the spatial distribution of readable content, and may be out of order in some areas under extremely complex layouts.&lt;/li&gt; 
 &lt;li&gt;Limited support for vertical text.&lt;/li&gt; 
 &lt;li&gt;Tables of contents and lists are recognized through rules, and some uncommon list formats may not be recognized.&lt;/li&gt; 
 &lt;li&gt;Code blocks are not yet supported in the layout model.&lt;/li&gt; 
 &lt;li&gt;Comic books, art albums, primary school textbooks, and exercises cannot be parsed well.&lt;/li&gt; 
 &lt;li&gt;Table recognition may result in row/column recognition errors in complex tables.&lt;/li&gt; 
 &lt;li&gt;OCR recognition may produce inaccurate characters in PDFs of lesser-known languages (e.g., diacritical marks in Latin script, easily confused characters in Arabic script).&lt;/li&gt; 
 &lt;li&gt;Some formulas may not render correctly in Markdown.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you encounter any issues during usage, you can first check the &lt;a href="https://opendatalab.github.io/MinerU/faq/"&gt;FAQ&lt;/a&gt; for solutions.&lt;/li&gt; 
 &lt;li&gt;If your issue remains unresolved, you may also use &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;DeepWiki&lt;/a&gt; to interact with an AI assistant, which can address most common problems.&lt;/li&gt; 
 &lt;li&gt;If you still cannot resolve the issue, you are welcome to join our community via &lt;a href="https://discord.gg/Tdedn9GTXq"&gt;Discord&lt;/a&gt; or &lt;a href="https://mineru.net/community-portal/?aliasId=3c430f94"&gt;WeChat&lt;/a&gt; to discuss with other users and developers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;All Thanks To Our Contributors&lt;/h1&gt; 
&lt;a href="https://github.com/opendatalab/MinerU/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=opendatalab/MinerU" /&gt; &lt;/a&gt; 
&lt;h1&gt;License Information&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/LICENSE.md"&gt;LICENSE.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Currently, some models in this project are trained based on YOLO. However, since YOLO follows the AGPL license, it may impose restrictions on certain use cases. In future iterations, we plan to explore and replace these with models under more permissive licenses to enhance user-friendliness and flexibility.&lt;/p&gt; 
&lt;h1&gt;Acknowledgments&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/DocLayout-YOLO"&gt;DocLayout-YOLO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/UniMERNet"&gt;UniMERNet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RapidAI/TableStructureRec"&gt;TableStructureRec&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;PaddleOCR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frotms/PaddleOCR2Pytorch"&gt;PaddleOCR2Pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Sanster/xy-cut"&gt;xy-cut&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LlmKira/fast-langdetect"&gt;fast-langdetect&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pypdfium2-team/pypdfium2"&gt;pypdfium2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/datalab-to/pdftext"&gt;pdftext&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pdfminer/pdfminer.six"&gt;pdfminer.six&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/py-pdf/pypdf"&gt;pypdf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/magika"&gt;magika&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{niu2025mineru25decoupledvisionlanguagemodel,
      title={MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing}, 
      author={Junbo Niu and Zheng Liu and Zhuangcheng Gu and Bin Wang and Linke Ouyang and Zhiyuan Zhao and Tao Chu and Tianyao He and Fan Wu and Qintong Zhang and Zhenjiang Jin and Guang Liang and Rui Zhang and Wenzheng Zhang and Yuan Qu and Zhifei Ren and Yuefeng Sun and Yuanhong Zheng and Dongsheng Ma and Zirui Tang and Boyu Niu and Ziyang Miao and Hejun Dong and Siyi Qian and Junyuan Zhang and Jingzhou Chen and Fangdong Wang and Xiaomeng Zhao and Liqun Wei and Wei Li and Shasha Wang and Ruiliang Xu and Yuanyuan Cao and Lu Chen and Qianqian Wu and Huaiyu Gu and Lindong Lu and Keming Wang and Dechen Lin and Guanlin Shen and Xuanhe Zhou and Linfeng Zhang and Yuhang Zang and Xiaoyi Dong and Jiaqi Wang and Bo Zhang and Lei Bai and Pei Chu and Weijia Li and Jiang Wu and Lijun Wu and Zhenxiang Li and Guangyu Wang and Zhongying Tu and Chao Xu and Kai Chen and Yu Qiao and Bowen Zhou and Dahua Lin and Wentao Zhang and Conghui He},
      year={2025},
      eprint={2509.22186},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.22186}, 
}

@misc{wang2024mineruopensourcesolutionprecise,
      title={MinerU: An Open-Source Solution for Precise Document Content Extraction}, 
      author={Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
      year={2024},
      eprint={2409.18839},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.18839}, 
}

@article{he2024opendatalab,
  title={Opendatalab: Empowering general artificial intelligence with open datasets},
  author={He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},
  journal={arXiv preprint arXiv:2407.13773},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;a&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h1&gt;Links&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenDCAI/DataFlow"&gt;Easy Data Preparation with latest LLMs-based Operators and Pipelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/Vis3"&gt;Vis3 (OSS browser based on s3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/labelU"&gt;LabelU (A Lightweight Multi-modal Data Annotation Tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/LabelLLM"&gt;LabelLLM (An Open-source LLM Dialogue Annotation Platform)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit (A Comprehensive Toolkit for High-Quality PDF Content Extraction)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/OmniDocBench"&gt;OmniDocBench (A Comprehensive Benchmark for Document Parsing and Evaluation)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/magic-html"&gt;Magic-HTML (Mixed web page extraction tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InternLM/magic-doc"&gt;Magic-Doc (Fast speed ppt/pptx/doc/docx/pdf extraction tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MigoXLab/dingo"&gt;Dingo: A Comprehensive AI Data Quality Evaluation Tool&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>mindsdb/mindsdb</title>
      <link>https://github.com/mindsdb/mindsdb</link>
      <description>&lt;p&gt;AI Analytics and Knowledge Engine for RAG over large-scale, heterogeneous data. - The only MCP Server you'll ever need&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.org/project/MindsDB/" target="_blank"&gt;&lt;img src="https://badge.fury.io/py/MindsDB.svg?sanitize=true" alt="MindsDB Release" /&gt;&lt;/a&gt; 
 &lt;a href="https://www.python.org/downloads/" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg?sanitize=true" alt="Python supported" /&gt;&lt;/a&gt; 
 &lt;a href="https://hub.docker.com/u/mindsdb" target="_blank"&gt;&lt;img src="https://img.shields.io/docker/pulls/mindsdb/mindsdb" alt="Docker pulls" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/3068" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3068" alt="mindsdb%2Fmindsdb | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;a href="https://github.com/mindsdb/mindsdb"&gt; &lt;img src="https://raw.githubusercontent.com/mindsdb/mindsdb/main/docs/assets/mindsdb_logo.png" alt="MindsDB" width="300" /&gt; &lt;/a&gt; 
 &lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://www.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Website&lt;/a&gt; Â· &lt;a href="https://docs.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Docs&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/contact"&gt;Contact us for a Demo&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Community Slack&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.&lt;/p&gt; 
&lt;a href="https://www.youtube.com/watch?v=MX3OKpnsoLM" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064" alt="MindsDB Demo" /&gt; &lt;/a&gt; 
&lt;h2&gt;Install MindsDB Server&lt;/h2&gt; 
&lt;p&gt;MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker-desktop"&gt;Using Docker Desktop&lt;/a&gt;. This is the fastest and recommended way to get started and have it all running.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker"&gt;Using Docker&lt;/a&gt;. This is also simple, but gives you more flexibility on how to further customize your server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;MindsDB has an MCP server built in&lt;/a&gt; that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Core Philosophy: Connect, Unify, Respond&lt;/h1&gt; 
&lt;p&gt;MindsDB's architecture is built around three fundamental capabilities:&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;Connect&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;You can connect to hundreds of enterprise &lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;data sources (learn more)&lt;/a&gt;. These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/overview"&gt;Unify&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;In many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/knowledge-bases"&gt;&lt;strong&gt;KNOWLEDGE BASES&lt;/strong&gt;&lt;/a&gt; â€“ Index and organize unstructured data for efficient Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/view"&gt;&lt;strong&gt;VIEWS&lt;/strong&gt;&lt;/a&gt; â€“ Simplify data access by creating unified views across different sources (no-ETL).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unification of data can be automated using JOBs&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs"&gt;&lt;strong&gt;JOBS&lt;/strong&gt;&lt;/a&gt; â€“ Schedule synchronization and transformation tasks for real-time processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;Respond&lt;/a&gt; From Your Data&lt;/h2&gt; 
&lt;p&gt;Chat with Your Data&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;&lt;strong&gt;AGENTS&lt;/strong&gt;&lt;/a&gt; â€“ Configure built-in agents specialized in answering questions over your connected and unified data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt; â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contribute&lt;/h2&gt; 
&lt;p&gt;Interested in contributing to MindsDB? Follow our &lt;a href="https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;installation guide for development&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find our &lt;a href="https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contribution guide here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.&lt;/p&gt; 
&lt;p&gt;This project adheres to a &lt;a href="https://github.com/mindsdb/mindsdb/raw/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to follow its terms.&lt;/p&gt; 
&lt;p&gt;Also, check out our &lt;a href="https://mindsdb.com/community?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;community rewards and programs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Support&lt;/h2&gt; 
&lt;p&gt;If you find a bug, please submit an &lt;a href="https://github.com/mindsdb/mindsdb/issues/new/choose"&gt;issue on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Hereâ€™s how you can get community support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ask a question in our &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Slack Community&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join our &lt;a href="https://github.com/mindsdb/mindsdb/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Post on &lt;a href="https://stackoverflow.com/questions/tagged/mindsdb"&gt;Stack Overflow&lt;/a&gt; with the MindsDB tag.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For commercial support, please &lt;a href="https://mindsdb.com/contact?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contact the MindsDB team&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ’š Current Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/mindsdb/mindsdb/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=mindsdb/mindsdb" /&gt; &lt;/a&gt; 
&lt;p&gt;Generated with &lt;a href="https://contributors-img.web.app"&gt;contributors-img&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ”” Subscribe for Updates&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://mindsdb.com/joincommunity"&gt;Slack community&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BerriAI/litellm</title>
      <link>https://github.com/BerriAI/litellm</link>
      <description>&lt;p&gt;Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; ğŸš… LiteLLM &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://render.com/deploy?repo=https://github.com/BerriAI/litellm" target="_blank" rel="nofollow"&gt;&lt;img src="https://render.com/images/deploy-to-render-button.svg?sanitize=true" alt="Deploy to Render" /&gt;&lt;/a&gt; &lt;a href="https://railway.app/template/HLP0Ub?referralCode=jch2ME"&gt; &lt;img src="https://railway.app/button.svg?sanitize=true" alt="Deploy on Railway" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.] &lt;br /&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt;&lt;a href="https://docs.litellm.ai/docs/simple_proxy" target="_blank"&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href="https://docs.litellm.ai/docs/hosted" target="_blank"&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href="https://docs.litellm.ai/docs/enterprise" target="_blank"&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://pypi.org/project/litellm/" target="_blank"&gt; &lt;img src="https://img.shields.io/pypi/v/litellm.svg?sanitize=true" alt="PyPI Version" /&gt; &lt;/a&gt; &lt;a href="https://www.ycombinator.com/companies/berriai"&gt; &lt;img src="https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square" alt="Y Combinator W23" /&gt; &lt;/a&gt; &lt;a href="https://wa.link/huol9n"&gt; &lt;img src="https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=WhatsApp&amp;amp;color=success&amp;amp;logo=WhatsApp&amp;amp;style=flat-square" alt="Whatsapp" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/wuPM9dRgDw"&gt; &lt;img src="https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=Discord&amp;amp;color=blue&amp;amp;logo=Discord&amp;amp;style=flat-square" alt="Discord" /&gt; &lt;/a&gt; &lt;a href="https://www.litellm.ai/support"&gt; &lt;img src="https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=Slack&amp;amp;color=black&amp;amp;logo=Slack&amp;amp;style=flat-square" alt="Slack" /&gt; &lt;/a&gt; &lt;/h4&gt; 
&lt;p&gt;LiteLLM manages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Translate inputs to provider's &lt;code&gt;completion&lt;/code&gt;, &lt;code&gt;embedding&lt;/code&gt;, and &lt;code&gt;image_generation&lt;/code&gt; endpoints&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/completion/output"&gt;Consistent output&lt;/a&gt;, text responses will always be available at &lt;code&gt;['choices'][0]['message']['content']&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - &lt;a href="https://docs.litellm.ai/docs/routing"&gt;Router&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Set Budgets &amp;amp; Rate limits per project, api key, model &lt;a href="https://docs.litellm.ai/docs/simple_proxy"&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/BerriAI/litellm?tab=readme-ov-file#litellm-proxy-server-llm-gateway---docs"&gt;&lt;strong&gt;Jump to LiteLLM Proxy (LLM Gateway) Docs&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs"&gt;&lt;strong&gt;Jump to Supported LLM Providers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ğŸš¨ &lt;strong&gt;Stable Release:&lt;/strong&gt; Use docker images with the &lt;code&gt;-stable&lt;/code&gt; tag. These have undergone 12 hour load tests, before being published. &lt;a href="https://docs.litellm.ai/docs/proxy/release_cycle"&gt;More information about the release cycle here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Support for more providers. Missing a provider or LLM Platform, raise a &lt;a href="https://github.com/BerriAI/litellm/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;template=feature_request.yml&amp;amp;title=%5BFeature%5D%3A+"&gt;feature request&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Usage (&lt;a href="https://docs.litellm.ai/docs/"&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;)&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] LiteLLM v1.0.0 now requires &lt;code&gt;openai&amp;gt;=1.0.0&lt;/code&gt;. Migration guide &lt;a href="https://docs.litellm.ai/docs/migration"&gt;here&lt;/a&gt; LiteLLM v1.40.14+ now requires &lt;code&gt;pydantic&amp;gt;=2.0.0&lt;/code&gt;. No changes required.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;a target="_blank" href="https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install litellm
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from litellm import completion
import os

## set ENV variables
os.environ["OPENAI_API_KEY"] = "your-openai-key"
os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-key"

messages = [{ "content": "Hello, how are you?","role": "user"}]

# openai call
response = completion(model="openai/gpt-4o", messages=messages)

# anthropic call
response = completion(model="anthropic/claude-sonnet-4-20250514", messages=messages)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response (OpenAI Format)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "id": "chatcmpl-1214900a-6cdd-4148-b663-b5e2f642b4de",
    "created": 1751494488,
    "model": "claude-sonnet-4-20250514",
    "object": "chat.completion",
    "system_fingerprint": null,
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "Hello! I'm doing well, thank you for asking. I'm here and ready to help with whatever you'd like to discuss or work on. How are you doing today?",
                "role": "assistant",
                "tool_calls": null,
                "function_call": null
            }
        }
    ],
    "usage": {
        "completion_tokens": 39,
        "prompt_tokens": 13,
        "total_tokens": 52,
        "completion_tokens_details": null,
        "prompt_tokens_details": {
            "audio_tokens": null,
            "cached_tokens": 0
        },
        "cache_creation_input_tokens": 0,
        "cache_read_input_tokens": 0
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Call any model supported by a provider, with &lt;code&gt;model=&amp;lt;provider_name&amp;gt;/&amp;lt;model_name&amp;gt;&lt;/code&gt;. There might be provider-specific details here, so refer to &lt;a href="https://docs.litellm.ai/docs/providers"&gt;provider docs for more information&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Async (&lt;a href="https://docs.litellm.ai/docs/completion/stream#async-completion"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = "Hello, how are you?"
    messages = [{"content": user_message, "role": "user"}]
    response = await acompletion(model="openai/gpt-4o", messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Streaming (&lt;a href="https://docs.litellm.ai/docs/completion/stream"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;p&gt;liteLLM supports streaming the model response back, pass &lt;code&gt;stream=True&lt;/code&gt; to get a streaming iterator in response. Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from litellm import completion
response = completion(model="openai/gpt-4o", messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or "")

# claude sonnet 4
response = completion('anthropic/claude-sonnet-4-20250514', messages, stream=True)
for part in response:
    print(part)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response chunk (OpenAI Format)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "id": "chatcmpl-fe575c37-5004-4926-ae5e-bfbc31f356ca",
    "created": 1751494808,
    "model": "claude-sonnet-4-20250514",
    "object": "chat.completion.chunk",
    "system_fingerprint": null,
    "choices": [
        {
            "finish_reason": null,
            "index": 0,
            "delta": {
                "provider_specific_fields": null,
                "content": "Hello",
                "role": "assistant",
                "function_call": null,
                "tool_calls": null,
                "audio": null
            },
            "logprobs": null
        }
    ],
    "provider_specific_fields": null,
    "stream_options": null,
    "citations": null
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Logging Observability (&lt;a href="https://docs.litellm.ai/docs/observability/callbacks"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;p&gt;LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ["LUNARY_PUBLIC_KEY"] = "your-lunary-public-key"
os.environ["HELICONE_API_KEY"] = "your-helicone-auth-key"
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""
os.environ["ATHINA_API_KEY"] = "your-athina-api-key"

os.environ["OPENAI_API_KEY"] = "your-openai-key"

# set callbacks
litellm.success_callback = ["lunary", "mlflow", "langfuse", "athina", "helicone"] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model="openai/gpt-4o", messages=[{"role": "user", "content": "Hi ğŸ‘‹ - i'm openai"}])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;LiteLLM Proxy Server (LLM Gateway) - (&lt;a href="https://docs.litellm.ai/docs/simple_proxy"&gt;Docs&lt;/a&gt;)&lt;/h1&gt; 
&lt;p&gt;Track spend + Load Balance across multiple projects&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.litellm.ai/docs/hosted"&gt;Hosted Proxy (Preview)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The proxy provides:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth"&gt;Hooks for auth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class"&gt;Hooks for logging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend"&gt;Cost tracking&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/proxy/users#set-rate-limits"&gt;Rate Limiting&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“– Proxy Endpoints - &lt;a href="https://litellm-api.up.railway.app/"&gt;Swagger Docs&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Quick Start Proxy - CLI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'litellm[proxy]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 1: Start litellm proxy&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 2: Make ChatCompletions Request to Proxy&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] ğŸ’¡ &lt;a href="https://docs.litellm.ai/docs/proxy/user_keys"&gt;Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import openai # openai v1.0.0+
client = openai.OpenAI(api_key="anything",base_url="http://0.0.0.0:4000") # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [
    {
        "role": "user",
        "content": "this is a test request, write a short poem"
    }
])

print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Proxy Key Management (&lt;a href="https://docs.litellm.ai/docs/proxy/virtual_keys"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;p&gt;Connect the proxy with a Postgres DB to create proxy keys&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo 'LITELLM_MASTER_KEY="sk-1234"' &amp;gt; .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/
# password generator to get a random hash for litellm salt key
echo 'LITELLM_SALT_KEY="sk-1234"' &amp;gt;&amp;gt; .env

source .env

# Start
docker compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;UI on &lt;code&gt;/ui&lt;/code&gt; on your proxy server &lt;img src="https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033" alt="ui_3" /&gt;&lt;/p&gt; 
&lt;p&gt;Set budgets and rate limits across multiple projects &lt;code&gt;POST /key/generate&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Request&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;curl 'http://0.0.0.0:4000/key/generate' \
--header 'Authorization: Bearer sk-1234' \
--header 'Content-Type: application/json' \
--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4", "claude-2"], "duration": "20m","metadata": {"user": "ishaan@berri.ai", "team": "core-infra"}}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Expected Response&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;{
    "key": "sk-kdEXbIqZRwEeEiHwdg7sFA", # Bearer token
    "expires": "2023-11-19T01:38:25.838000+00:00" # datetime object
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Providers (&lt;a href="https://docs.litellm.ai/docs/providers"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/#basic-usage"&gt;Completion&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/completion/stream#streaming-responses"&gt;Streaming&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/completion/stream#async-completion"&gt;Async Completion&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/completion/stream#async-streaming"&gt;Async Streaming&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/embedding/supported_embedding"&gt;Async Embedding&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/image_generation"&gt;Async Image Generation&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/openai"&gt;openai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/meta_llama"&gt;Meta - Llama API&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/azure"&gt;azure&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/aiml"&gt;AI/ML API&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/aws_sagemaker"&gt;aws - sagemaker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/bedrock"&gt;aws - bedrock&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/vertex"&gt;google - vertex_ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/palm"&gt;google - palm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/gemini"&gt;google AI Studio - gemini&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/mistral"&gt;mistral ai api&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/cloudflare_workers"&gt;cloudflare AI Workers&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/compactifai"&gt;CompactifAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/cohere"&gt;cohere&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/anthropic"&gt;anthropic&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/empower"&gt;empower&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/huggingface"&gt;huggingface&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/replicate"&gt;replicate&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/togetherai"&gt;together_ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/openrouter"&gt;openrouter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/ai21"&gt;ai21&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/baseten"&gt;baseten&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/vllm"&gt;vllm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/nlp_cloud"&gt;nlp_cloud&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/aleph_alpha"&gt;aleph alpha&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/petals"&gt;petals&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/ollama"&gt;ollama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/deepinfra"&gt;deepinfra&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/perplexity"&gt;perplexity-ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/groq"&gt;Groq AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/deepseek"&gt;Deepseek&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/anyscale"&gt;anyscale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/watsonx"&gt;IBM - watsonx.ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/voyage"&gt;voyage ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/xinference"&gt;xinference [Xorbits Inference]&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/friendliai"&gt;FriendliAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/galadriel"&gt;Galadriel&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/gradient_ai"&gt;GradientAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://novita.ai/models/llm?utm_source=github_litellm&amp;amp;utm_medium=github_readme&amp;amp;utm_campaign=github_link"&gt;Novita AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/featherless_ai"&gt;Featherless AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/nebius"&gt;Nebius AI Studio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/heroku"&gt;Heroku&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/ovhcloud"&gt;OVHCloud AI Endpoints&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://docs.litellm.ai/docs/"&gt;&lt;strong&gt;Read the Docs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Run in Developer mode&lt;/h2&gt; 
&lt;h3&gt;Services&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Setup .env file in root&lt;/li&gt; 
 &lt;li&gt;Run dependant services &lt;code&gt;docker-compose up db prometheus&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Backend&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;(In root) create virtual environment &lt;code&gt;python -m venv .venv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Activate virtual environment &lt;code&gt;source .venv/bin/activate&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies &lt;code&gt;pip install -e ".[all]"&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Start proxy backend &lt;code&gt;python litellm/proxy_cli.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Frontend&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;ui/litellm-dashboard&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;npm run dev&lt;/code&gt; to start the dashboard&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Enterprise&lt;/h1&gt; 
&lt;p&gt;For companies that need better security, user management and professional support&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat"&gt;Talk to founders&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This covers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Features under the &lt;a href="https://docs.litellm.ai/docs/proxy/enterprise"&gt;LiteLLM Commercial License&lt;/a&gt;:&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Feature Prioritization&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Custom Integrations&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Professional Support - Dedicated discord + slack&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Custom SLAs&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Secure access with Single Sign-On&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions to LiteLLM! Whether you're fixing bugs, adding features, or improving documentation, we appreciate your help.&lt;/p&gt; 
&lt;h2&gt;Quick Start for Contributors&lt;/h2&gt; 
&lt;p&gt;This requires poetry to be installed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/BerriAI/litellm.git
cd litellm
make install-dev    # Install development dependencies
make format         # Format your code
make lint           # Run all linting checks
make test-unit      # Run unit tests
make format-check   # Check formatting only
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed contributing guidelines, see &lt;a href="https://raw.githubusercontent.com/BerriAI/litellm/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Code Quality / Linting&lt;/h2&gt; 
&lt;p&gt;LiteLLM follows the &lt;a href="https://google.github.io/styleguide/pyguide.html"&gt;Google Python Style Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Our automated checks include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Black&lt;/strong&gt; for code formatting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ruff&lt;/strong&gt; for linting and code quality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MyPy&lt;/strong&gt; for type checking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Circular import detection&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Import safety checks&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All these checks must pass before your PR can be merged.&lt;/p&gt; 
&lt;h1&gt;Support / talk with founders&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version"&gt;Schedule Demo ğŸ‘‹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/wuPM9dRgDw"&gt;Community Discord ğŸ’­&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.litellm.ai/support"&gt;Community Slack ğŸ’­&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Our numbers ğŸ“ +1 (770) 8783-106 / â€­+1 (412) 618-6238â€¬&lt;/li&gt; 
 &lt;li&gt;Our emails âœ‰ï¸ &lt;a href="mailto:ishaan@berri.ai"&gt;ishaan@berri.ai&lt;/a&gt; / &lt;a href="mailto:krrish@berri.ai"&gt;krrish@berri.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Why did we build this&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Need for simplicity&lt;/strong&gt;: Our code started to get extremely complicated managing &amp;amp; translating calls between Azure, OpenAI and Cohere.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributors&lt;/h1&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;a href="https://github.com/BerriAI/litellm/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=BerriAI/litellm" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Ksuriuri/index-tts-vllm</title>
      <link>https://github.com/Ksuriuri/index-tts-vllm</link>
      <description>&lt;p&gt;Added vLLM support to IndexTTS for faster inference.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Ksuriuri/index-tts-vllm/master/README.md"&gt;ä¸­æ–‡&lt;/a&gt; ï½œ &lt;a href="https://raw.githubusercontent.com/Ksuriuri/index-tts-vllm/master/README_EN.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;IndexTTS-vLLM&lt;/h1&gt; 
&lt;/div&gt; 
&lt;h2&gt;é¡¹ç›®ç®€ä»‹&lt;/h2&gt; 
&lt;p&gt;è¯¥é¡¹ç›®åœ¨ &lt;a href="https://github.com/index-tts/index-tts"&gt;index-tts&lt;/a&gt; çš„åŸºç¡€ä¸Šä½¿ç”¨ vllm åº“é‡æ–°å®ç°äº† gpt æ¨¡å‹çš„æ¨ç†ï¼ŒåŠ é€Ÿäº† index-tts çš„æ¨ç†è¿‡ç¨‹ã€‚&lt;/p&gt; 
&lt;p&gt;æ¨ç†é€Ÿåº¦ï¼ˆIndex-TTS-v1ï¼‰åœ¨å•å¡ RTX 4090 ä¸Šçš„æå‡ä¸ºï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å•ä¸ªè¯·æ±‚çš„ RTF (Real-Time Factor)ï¼šâ‰ˆ0.3 -&amp;gt; â‰ˆ0.1&lt;/li&gt; 
 &lt;li&gt;å•ä¸ªè¯·æ±‚çš„ gpt æ¨¡å‹ decode é€Ÿåº¦ï¼šâ‰ˆ90 token / s -&amp;gt; â‰ˆ280 token / s&lt;/li&gt; 
 &lt;li&gt;å¹¶å‘é‡ï¼šgpu_memory_utilizationè®¾ç½®ä¸º0.25ï¼ˆçº¦5GBæ˜¾å­˜ï¼‰çš„æƒ…å†µä¸‹ï¼Œå®æµ‹ 16 å·¦å³çš„å¹¶å‘æ— å‹åŠ›ï¼ˆæµ‹é€Ÿè„šæœ¬å‚è€ƒ &lt;code&gt;simple_test.py&lt;/code&gt;ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;æ›´æ–°æ—¥å¿—&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2025-08-07]&lt;/strong&gt; æ”¯æŒ Docker å…¨è‡ªåŠ¨åŒ–ä¸€é”®éƒ¨ç½² API æœåŠ¡ï¼š&lt;code&gt;docker compose up&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2025-08-06]&lt;/strong&gt; æ”¯æŒ openai æ¥å£æ ¼å¼è°ƒç”¨ï¼š&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;æ·»åŠ  /audio/speech api è·¯å¾„ï¼Œå…¼å®¹ OpenAI æ¥å£&lt;/li&gt; 
   &lt;li&gt;æ·»åŠ  /audio/voices api è·¯å¾„ï¼Œ è·å¾— voice/character åˆ—è¡¨&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;å¯¹åº”ï¼š&lt;a href="https://platform.openai.com/docs/api-reference/audio/createSpeech"&gt;createSpeech&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2025-09-22]&lt;/strong&gt; æ”¯æŒäº† vllm v1 ç‰ˆæœ¬ï¼ŒIndexTTS2 æ­£åœ¨å…¼å®¹ä¸­&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2025-09-28]&lt;/strong&gt; æ”¯æŒäº† IndexTTS2 çš„ webui æ¨ç†ï¼Œå¹¶æ•´ç†äº†æƒé‡æ–‡ä»¶ï¼Œç°åœ¨éƒ¨ç½²æ›´åŠ æ–¹ä¾¿äº†ï¼ \0.0/ ï¼›ä½†å½“å‰ç‰ˆæœ¬å¯¹äº IndexTTS2 çš„ gpt ä¼¼ä¹å¹¶æ²¡æœ‰åŠ é€Ÿæ•ˆæœï¼Œå¾…ç ”ç©¶&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2025-09-29]&lt;/strong&gt; è§£å†³äº† IndexTTS2 çš„ gpt æ¨¡å‹æ¨ç†åŠ é€Ÿæ— æ•ˆçš„é—®é¢˜&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[2025-10-09]&lt;/strong&gt; å…¼å®¹ IndexTTS2 çš„ api æ¥å£è°ƒç”¨ï¼Œè¯·å‚è€ƒ &lt;a href="https://raw.githubusercontent.com/Ksuriuri/index-tts-vllm/master/#api"&gt;API&lt;/a&gt;ï¼Œv1/1.5 çš„ api æ¥å£ä»¥åŠ openai å…¼å®¹çš„æ¥å£å¯èƒ½è¿˜æœ‰ bugï¼Œæ™šç‚¹å†ä¿®&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ä½¿ç”¨æ­¥éª¤&lt;/h2&gt; 
&lt;h3&gt;1. git æœ¬é¡¹ç›®&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Ksuriuri/index-tts-vllm.git
cd index-tts-vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. åˆ›å»ºå¹¶æ¿€æ´» conda ç¯å¢ƒ&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n index-tts-vllm python=3.12
conda activate index-tts-vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. å®‰è£… pytorch&lt;/h3&gt; 
&lt;p&gt;éœ€è¦ pytorch ç‰ˆæœ¬ 2.8.0ï¼ˆå¯¹åº” vllm 0.10.2ï¼‰ï¼Œå…·ä½“å®‰è£…æŒ‡ä»¤è¯·å‚è€ƒï¼š&lt;a href="https://pytorch.org/get-started/locally/"&gt;pytorch å®˜ç½‘&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;4. å®‰è£…ä¾èµ–&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;5. ä¸‹è½½æ¨¡å‹æƒé‡&lt;/h3&gt; 
&lt;p&gt;ï¼ˆæ¨èï¼‰é€‰æ‹©å¯¹åº”ç‰ˆæœ¬çš„æ¨¡å‹æƒé‡ä¸‹è½½åˆ° &lt;code&gt;checkpoints/&lt;/code&gt; è·¯å¾„ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Index-TTS
modelscope download --model kusuriuri/Index-TTS-vLLM --local_dir ./checkpoints/Index-TTS-vLLM

# IndexTTS-1.5
modelscope download --model kusuriuri/Index-TTS-1.5-vLLM --local_dir ./checkpoints/Index-TTS-1.5-vLLM

# IndexTTS-2
modelscope download --model kusuriuri/IndexTTS-2-vLLM --local_dir ./checkpoints/IndexTTS-2-vLLM
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ï¼ˆå¯é€‰ï¼Œä¸æ¨èï¼‰ä¹Ÿå¯ä»¥ä½¿ç”¨ &lt;code&gt;convert_hf_format.sh&lt;/code&gt; è‡ªè¡Œè½¬æ¢å®˜æ–¹æƒé‡æ–‡ä»¶ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash convert_hf_format.sh /path/to/your/model_dir
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6. webui å¯åŠ¨ï¼&lt;/h3&gt; 
&lt;p&gt;è¿è¡Œå¯¹åº”ç‰ˆæœ¬ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Index-TTS 1.0
python webui.py

# IndexTTS-1.5
python webui.py --version 1.5

# IndexTTS-2
python webui_v2.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ç¬¬ä¸€æ¬¡å¯åŠ¨å¯èƒ½ä¼šä¹…ä¸€äº›ï¼Œå› ä¸ºè¦å¯¹ bigvgan è¿›è¡Œ cuda æ ¸ç¼–è¯‘&lt;/p&gt; 
&lt;h2&gt;API&lt;/h2&gt; 
&lt;p&gt;ä½¿ç”¨ fastapi å°è£…äº† api æ¥å£ï¼Œå¯åŠ¨ç¤ºä¾‹å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Index-TTS-1.0/1.5
python api_server.py

# IndexTTS-2
python api_server_v2.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;å¯åŠ¨å‚æ•°&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_dir&lt;/code&gt;: å¿…å¡«ï¼Œæ¨¡å‹æƒé‡è·¯å¾„&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;: æœåŠ¡ipåœ°å€ï¼Œé»˜è®¤ä¸º &lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;: æœåŠ¡ç«¯å£ï¼Œé»˜è®¤ä¸º &lt;code&gt;6006&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--gpu_memory_utilization&lt;/code&gt;: vllm æ˜¾å­˜å ç”¨ç‡ï¼Œé»˜è®¤è®¾ç½®ä¸º &lt;code&gt;0.25&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;API è¯·æ±‚ç¤ºä¾‹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;v1/1.5 è¯·å‚è€ƒ &lt;code&gt;api_example.py&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;v2 è¯·å‚è€ƒ &lt;code&gt;api_example_v2.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenAI API&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ·»åŠ  /audio/speech api è·¯å¾„ï¼Œå…¼å®¹ OpenAI æ¥å£&lt;/li&gt; 
 &lt;li&gt;æ·»åŠ  /audio/voices api è·¯å¾„ï¼Œ è·å¾— voice/character åˆ—è¡¨&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;è¯¦è§ï¼š&lt;a href="https://platform.openai.com/docs/api-reference/audio/createSpeech"&gt;createSpeech&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;æ–°ç‰¹æ€§&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;v1/v1.5:&lt;/strong&gt; æ”¯æŒå¤šè§’è‰²éŸ³é¢‘æ··åˆï¼šå¯ä»¥ä¼ å…¥å¤šä¸ªå‚è€ƒéŸ³é¢‘ï¼ŒTTS è¾“å‡ºçš„è§’è‰²å£°çº¿ä¸ºå¤šä¸ªå‚è€ƒéŸ³é¢‘çš„æ··åˆç‰ˆæœ¬ï¼ˆè¾“å…¥å¤šä¸ªå‚è€ƒéŸ³é¢‘ä¼šå¯¼è‡´è¾“å‡ºçš„è§’è‰²å£°çº¿ä¸ç¨³å®šï¼Œå¯ä»¥æŠ½å¡æŠ½åˆ°æ»¡æ„çš„å£°çº¿å†ä½œä¸ºå‚è€ƒéŸ³é¢‘ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;æ€§èƒ½&lt;/h2&gt; 
&lt;p&gt;Word Error Rate (WER) Results for IndexTTS and Baseline Models on the &lt;a href="https://github.com/BytedanceSpeech/seed-tts-eval"&gt;&lt;strong&gt;seed-test&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;zh&lt;/th&gt; 
   &lt;th&gt;en&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Human&lt;/td&gt; 
   &lt;td&gt;1.254&lt;/td&gt; 
   &lt;td&gt;2.143&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;index-tts (num_beams=3)&lt;/td&gt; 
   &lt;td&gt;1.005&lt;/td&gt; 
   &lt;td&gt;1.943&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;index-tts (num_beams=1)&lt;/td&gt; 
   &lt;td&gt;1.107&lt;/td&gt; 
   &lt;td&gt;2.032&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;index-tts-vllm&lt;/td&gt; 
   &lt;td&gt;1.12&lt;/td&gt; 
   &lt;td&gt;1.987&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;åŸºæœ¬ä¿æŒäº†åŸé¡¹ç›®çš„æ€§èƒ½&lt;/p&gt; 
&lt;h2&gt;å¹¶å‘æµ‹è¯•&lt;/h2&gt; 
&lt;p&gt;å‚è€ƒ &lt;a href="https://raw.githubusercontent.com/Ksuriuri/index-tts-vllm/master/simple_test.py"&gt;&lt;code&gt;simple_test.py&lt;/code&gt;&lt;/a&gt;ï¼Œéœ€å…ˆå¯åŠ¨ API æœåŠ¡&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenRLHF/OpenRLHF</title>
      <link>https://github.com/OpenRLHF/OpenRLHF</link>
      <description>&lt;p&gt;An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO &amp; GRPO &amp; REINFORCE++ &amp; vLLM &amp; Ray &amp; Dynamic Sampling &amp; Async Agentic RL)&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img alt="OpenRLHF logo" src="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/docs/logo.png" style="height: 140px;" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors"&gt; &lt;img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF" /&gt; &lt;/a&gt; &lt;a href="https://github.com/OpenRLHF/OpenRLHF/issues"&gt; &lt;img alt="Issues" src="https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff" /&gt; &lt;/a&gt; &lt;a href="https://github.com/OpenRLHF/OpenRLHF/discussions"&gt; &lt;img alt="Issues" src="https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff" /&gt; &lt;/a&gt; &lt;a href="https://github.com/OpenRLHF/OpenRLHF/pulls"&gt; &lt;img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff" /&gt; &lt;/a&gt;&lt;a href="https://github.com/OpenRLHF/OpenRLHF/stargazers"&gt; &lt;img alt="GitHub stars" src="https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf" /&gt; &lt;/a&gt; &lt;a href="https://deepwiki.com/OpenRLHF/OpenRLHF"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;em&gt;Open-source / Comprehensive / Lightweight / Easy-to-use&lt;/em&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;span&gt;[ English | &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/README_zh.md"&gt;ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/README_ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; ]&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;OpenRLHF is the first easy-to-use, high-performance open-source RLHF framework built on Ray, vLLM, ZeRO-3 and HuggingFace Transformers, designed to make RLHF training simple and accessible:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Architecture with Ray&lt;/strong&gt;&lt;br /&gt; OpenRLHF leverages &lt;a href="https://github.com/ray-project/ray"&gt;Ray&lt;/a&gt; for efficient distributed scheduling. It separates the Actor, Reward, Reference, and Critic models across different GPUs, enabling scalable training for models up to 70B parameters.&lt;br /&gt; It also supports &lt;strong&gt;Hybrid Engine&lt;/strong&gt; scheduling, allowing all models and vLLM engines to share GPU resourcesâ€”minimizing idle time and maximizing GPU utilization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vLLM Inference Acceleration + AutoTP&lt;/strong&gt;&lt;br /&gt; RLHF training spends 80% of the time on the sample generation stage. Powered by &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; and Auto Tensor Parallelism (AutoTP), OpenRLHF delivers high-throughput, memory-efficient samples generation. Native integration with HuggingFace Transformers ensures seamless and fast generation, making it the fastest RLHF framework available.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory-Efficient Training with ZeRO-3 / AutoTP&lt;/strong&gt;&lt;br /&gt; Built on &lt;a href="https://github.com/deepspeedai/DeepSpeed"&gt;DeepSpeed's&lt;/a&gt; ZeRO-3, &lt;a href="https://github.com/deepspeedai/DeepSpeed/raw/master/blogs/deepcompile/README.md"&gt;deepcompile&lt;/a&gt; and &lt;a href="https://github.com/deepspeedai/DeepSpeed/raw/master/blogs/huggingface-tp/README.md"&gt;AutoTP&lt;/a&gt;, OpenRLHF enables large model training without heavyweight frameworks. It works directly with HuggingFace for easy loading and fine-tuning of pretrained models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimized PPO Implementation&lt;/strong&gt;&lt;br /&gt; Incorporates advanced PPO tricks inspired by practical guides and community best practices, enhancing training stability and reward quality in RLHF workflows. Referencing &lt;a href="https://zhuanlan.zhihu.com/p/622134699"&gt;Zhihu&lt;/a&gt; and &lt;a href="https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361"&gt;Advanced Tricks for Training Large Language Models with Proximal Policy Optimization&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;More details are in &lt;a href="https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing"&gt;Slides&lt;/a&gt; | &lt;a href="https://www.researchgate.net/publication/393414548_OpenRLHF_An_Easy-to-use_Scalable_and_High-performance_RLHF_Framework"&gt;Technical Report&lt;/a&gt; | &lt;a href="https://openrlhf.readthedocs.io/"&gt;Documents&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/8] &lt;a href="https://hijkzzz.notion.site/prorl-v2"&gt;ProRL V2&lt;/a&gt; uses REINFORCE++-baseline to train a state-of-the-art 1.5B reasoning model and releases the blog post &lt;a href="https://medium.com/@janhu9527/reinforce-baseline-is-all-you-need-in-rlvr-f5406930aa85"&gt;REINFORCE++-baseline is all you need in RLVR&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/6] &lt;a href="https://mistral.ai/static/research/magistral.pdf"&gt;Magistral&lt;/a&gt; uses the method quite similar to REINFORCE++-baseline to train the reasoning models.&lt;/li&gt; 
 &lt;li&gt;[2025/5] &lt;a href="https://github.com/TsinghuaC3I/MARTI"&gt;MARTI&lt;/a&gt; has been released as a fork of OpenRLHF. It is designed to train LLM-based multi-agent systems using RL, by integrating centralized multi-agent interactions with distributed policy training.&lt;/li&gt; 
 &lt;li&gt;[2025/5] OpenRLHF 0.8.0 supports &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_reinforce_baseline_llama_ray_async.sh"&gt;Async Pipeline RLHF&lt;/a&gt; (&lt;code&gt;--async_train&lt;/code&gt;) and &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_reinforce_baseline_llama_ray_agent_async.sh"&gt;Async Agent RLHF&lt;/a&gt;(&lt;code&gt;--agent_func_path&lt;/code&gt;) with redesigned class-based Agent API&lt;/li&gt; 
 &lt;li&gt;[2025/4] Post the blog &lt;a href="https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html"&gt;Accelerating RLHF with vLLM, Best Practice from OpenRLHF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/4] Clean OpenRLHF: Refactored the source code based on Single Controller and Unified Packing Samples&lt;/li&gt; 
 &lt;li&gt;[2025/3] The CMU &lt;a href="https://cmu-l3.github.io/anlp-spring2025/"&gt;Advanced Natural Language Processing Spring 2025&lt;/a&gt; course uses OpenRLHF as the RLHF framework teaching case.&lt;/li&gt; 
 &lt;li&gt;[2025/2] &lt;a href="https://arxiv.org/abs/2502.14768"&gt;Logic-RL&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2502.01456"&gt;PRIME&lt;/a&gt; demonstrate that REINFORCE++ is more stable in training compared to GRPO and faster than PPO.&lt;/li&gt; 
 &lt;li&gt;[2025/2] &lt;a href="https://github.com/TideDra/lmm-r1"&gt;LMM-R1&lt;/a&gt; is a fork of OpenRLHF, aimed at providing high-performance RL infrastructure for reproduction of DeepSeek-R1 on multimodal tasks.&lt;/li&gt; 
 &lt;li&gt;[2025/2] MIT &amp;amp; Microsoft proposed the &lt;a href="https://arxiv.org/pdf/2502.06773"&gt;On the Emergence of Thinking in LLMs I: Searching for the Right Intuition&lt;/a&gt; using OpenRLHF&lt;/li&gt; 
 &lt;li&gt;[2025/1] HKUST reproduced the &lt;a href="https://github.com/hkust-nlp/simpleRL-reason"&gt;DeepSeek-R1-Zero and DeepSeek-R1 training on small models using OpenRLHF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2024/12] We "proposed" ğŸ˜Š the &lt;a href="https://www.researchgate.net/publication/387487679_REINFORCE_An_Efficient_RLHF_Algorithm_with_Robustnessto_Both_Prompt_and_Reward_Models"&gt;REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/12] We analyzed the PPO, REINFORCE++, GRPO and RLOO in the &lt;a href="https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05"&gt;Notion Blogpost&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2023/8] OpenRLHF was open-sourced.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Distributed &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_ppo_llama_ray.sh"&gt;PPO&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_reinforce_llama_ray_hybrid_engine.sh"&gt;REINFORCE++/REINFORCE++-baseline/GRPO/RLOO&lt;/a&gt; implementations based on Ray.&lt;/li&gt; 
 &lt;li&gt;Support Ray-based &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_ppo_llama_ray_hybrid_engine.sh"&gt;PPO&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_reinforce_llama_ray_hybrid_engine.sh"&gt;REINFORCE++/REINFORCE++-baseline/GRPO/RLOO&lt;/a&gt; using Hybrid Engine (&lt;code&gt;--colocate_all_models&lt;/code&gt;, &lt;code&gt;--vllm_enable_sleep&lt;/code&gt; and &lt;code&gt;--vllm_gpu_memory_utilization 0.5&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_ppo_llama_with_reward_fn.sh"&gt;Ray-based Reinforced Finetuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Integration with vLLM for accelerated generation in RLHF tasks (&lt;code&gt;--vllm_num_engines&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Support RL Dynamic Sampling from DAPO(&lt;code&gt;--dynamic_filtering&lt;/code&gt; and &lt;code&gt;--dynamic_filtering_reward_range&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Support &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_sft_llama_tensor_parallelism.sh"&gt;DeepSpeed AutoTP training&lt;/a&gt; (&lt;code&gt;--ds_tensor_parallel_size&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Implementation of &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_dpo_ring_llama.sh"&gt;RingAttention&lt;/a&gt; (&lt;code&gt;--ring_attn_size&lt;/code&gt;, &lt;code&gt;--ring_head_stride&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Implementation of &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_dpo_llama.sh"&gt;DPO (Direct Preference Optimization)/IPO/cDPO&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_kto_llama.sh"&gt;Kahneman-Tversky Optimization (KTO)&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Support for &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_iterative_dpo_llama.sh"&gt;Iterative DPO&lt;/a&gt; (&lt;a href="https://github.com/RLHFlow/Online-RLHF"&gt;GitHub: Online-RLHF&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Support for &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_rejection_sampling_llama.sh"&gt;Rejection Sampling&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Implementation of &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_conditional_llama.sh"&gt;Conditional SFT&lt;/a&gt; (&lt;a href="https://arxiv.org/abs/2308.12050"&gt;arXiv:2308.12050&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Support for &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_knowledge_distillation.sh"&gt;Knowledge Distillation&lt;/a&gt; (&lt;a href="https://github.com/microsoft/LMOps/tree/main/minillm"&gt;Microsoft: minillm&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Integration of &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_prm_mistral.sh"&gt;Process Reward Model (PRM)&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Packing of training samples for SFT, DPO, RM, PRM, and PPO (&lt;code&gt;--packing_samples&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Support for &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/test_scripts/train_sft_mixtral_lora.sh"&gt;Mixture of Experts (MoE)&lt;/a&gt; (&lt;code&gt;--aux_loss_coef&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Integration of FlashAttention (&lt;code&gt;--attn_implementation&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Support for QLoRA (&lt;code&gt;--load_in_4bit&lt;/code&gt;) and &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_sft_mixtral_lora.sh"&gt;LoRA&lt;/a&gt; (&lt;code&gt;--lora_rank&lt;/code&gt;, &lt;code&gt;--target_modules&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Compatibility with HuggingFace's &lt;code&gt;tokenizer.apply_chat_template&lt;/code&gt; for datasets (&lt;code&gt;--apply_chat_template&lt;/code&gt; and &lt;code&gt;--input_key&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Logging support with Wandb (&lt;code&gt;--use_wandb&lt;/code&gt;) and TensorBoard (&lt;code&gt;--use_tensorboard&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Checkpoint recovery functionality (&lt;code&gt;--load_checkpoint&lt;/code&gt; and &lt;code&gt;--save_steps&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Provided multi-node training scripts, such as &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_llama_slurm.sh"&gt;DPO&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/train_ppo_llama_ray_slurm.sh"&gt;Ray PPO&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;To use OpenRLHF, first launch the docker container (&lt;strong&gt;Recommended&lt;/strong&gt;) and &lt;code&gt;pip install&lt;/code&gt; openrlhf inside the docker container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch the docker container
docker run --runtime=nvidia -it --rm --shm-size="10g" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:25.02-py3 bash
sudo pip uninstall xgboost transformer_engine flash_attn pynvml -y

# pip install
pip install openrlhf

# If you want to use vLLM acceleration (Install vLLM 0.11.0)
pip install openrlhf[vllm]
# latest vLLM is also supported
pip install openrlhf[vllm_latest]
# Install vLLM, ring-flash-attention and Liger-Kernel
pip install openrlhf[vllm,ring,liger]

# pip install the latest version
pip install git+https://github.com/OpenRLHF/OpenRLHF.git

# Or git clone
git clone https://github.com/OpenRLHF/OpenRLHF.git
cd OpenRLHF
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] We recommend using vLLM 0.11.0 or higher. We also provided the &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/dockerfile/"&gt;Dockerfiles for vLLM&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/nvidia_docker_install.sh"&gt;One-Click Installation Script of Nvidia-Docker&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Prepare Datasets&lt;/h3&gt; 
&lt;p&gt;OpenRLHF provides multiple data processing methods in our dataset classes. Such as in the &lt;a href="https://github.com/OpenRLHF/OpenRLHF/raw/main/openrlhf/datasets/prompts_dataset.py#L6"&gt;Prompt Dataset&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def preprocess_data(data, input_template=None, input_key="input", apply_chat_template=None) -&amp;gt; str:
    if apply_chat_template:
        chat = data[input_key]
        if isinstance(chat, str):
            chat = [{"role": "user", "content": chat}]
        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    else:
        prompt = data[input_key]
        if input_template:
            prompt = input_template.format(prompt)
    return prompt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;We can use &lt;code&gt;--input_key&lt;/code&gt; to specify the &lt;code&gt;JSON key name&lt;/code&gt; of the input datasets &lt;code&gt;--prompt_data {name or path}&lt;/code&gt; (PPO) or &lt;code&gt;--dataset {name or path}&lt;/code&gt;, and use &lt;code&gt;--apply_chat_template&lt;/code&gt; to utilize the &lt;code&gt;chat_template&lt;/code&gt; from the &lt;a href="https://huggingface.co/docs/transformers/main/en/chat_templating"&gt;Huggingface Tokenizer&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you don't want to use &lt;code&gt;--apply_chat_template&lt;/code&gt;, you can use &lt;code&gt;--input_template&lt;/code&gt; instead, or preprocess the datasets offline in advance.&lt;/li&gt; 
 &lt;li&gt;OpenRLHF also support mixing multiple datasets using &lt;code&gt;--prompt_data_probs 0.1,0.4,0.5&lt;/code&gt; (PPO) or &lt;code&gt;--dataset_probs 0.1,0.4,0.5&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;How Chat Templating Works:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;dataset = [{"input_key": [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "I'd like to show off how chat templating works!"},
]}]

tokenizer.apply_chat_template(dataset[0]["input_key"], tokenize=False)

"&amp;lt;s&amp;gt;[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?&amp;lt;/s&amp;gt; [INST] I'd like to show off how chat templating works! [/INST]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;How to specify test datasets ?&lt;/p&gt; 
&lt;p&gt;Please set test datasets path using &lt;code&gt;--eval_dataset {name or path}&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The &lt;code&gt;JSON key&lt;/code&gt; options depends on the specific datasets. See &lt;a href="https://github.com/OpenRLHF/OpenRLHF/raw/main/openrlhf/datasets/reward_dataset.py#L10"&gt;Reward Dataset&lt;/a&gt; and &lt;a href="https://github.com/OpenRLHF/OpenRLHF/raw/main/openrlhf/datasets/sft_dataset.py#L9"&gt;SFT Dataset&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Supervised Fine-tuning&lt;/h3&gt; 
&lt;p&gt;OpenRLHF's model checkpoint is fully compatible with HuggingFace models. You can specify the model name or path using &lt;code&gt;--pretrain {name or path}&lt;/code&gt;, &lt;code&gt;--reward_pretrain {name or path}&lt;/code&gt; and &lt;code&gt;--critic_pretrain {name or path}&lt;/code&gt;. We have provided some pre-trained checkpoints and datasets on &lt;a href="https://huggingface.co/OpenRLHF"&gt;HuggingFace OpenRLHF&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Then you can use the startup scripts we provide in the &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/"&gt;examples/scripts&lt;/a&gt; directory, or start the training using the following commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;deepspeed --module openrlhf.cli.train_sft \
   --max_len 4096 \
   --dataset Open-Orca/OpenOrca \
   --input_key question \
   --output_key response \
   --input_template $'User: {}\nAssistant: ' \
   --train_batch_size 256 \
   --micro_train_batch_size 2 \
   --max_samples 500000 \
   --pretrain meta-llama/Meta-Llama-3-8B \
   --save_path ./checkpoint/llama3-8b-sft \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --zero_stage 2 \
   --max_epochs 1 \
   --packing_samples \
   --bf16 \
   --learning_rate 5e-6 \
   --gradient_checkpointing \
   --use_wandb {wandb_token}

# Support HF tokenizer.apply_chat_template
# --apply_chat_template 
# --tokenizer_chat_template {HF Chat Template}

# Support RingAttention
# pip install ring_flash_attn
#   --ring_attn_size 2 \
#   --ring_head_stride 2 \

# Multi-turn fine-tuning loss
# --multiturn

# Can also be used for continued pre-training
# --pretrain_mode
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] OpenRLHF SFT/DPO/RewardModel/PPO trainers support &lt;code&gt;--packing_samples&lt;/code&gt; &lt;a href="https://github.com/MeetKai/functionary/tree/main/functionary/train/packing"&gt;based on &lt;code&gt;flash_attention&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Reward Model Training&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;deepspeed --module openrlhf.cli.train_rm \
   --save_path ./checkpoint/llama3-8b-rm \
   --save_steps -1 \
   --logging_steps 1 \
   --eval_steps -1 \
   --train_batch_size 256 \
   --micro_train_batch_size 1 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --bf16 \
   --max_epochs 1 \
   --max_len 8192 \
   --zero_stage 3 \
   --learning_rate 9e-6 \
   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \
   --apply_chat_template \
   --chosen_key chosen \
   --rejected_key rejected \
   --packing_samples \
   --gradient_checkpointing \
   --use_wandb {wandb_token}

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is recommended to set the &lt;code&gt;--value_prefix_head&lt;/code&gt; option of the Reward Model to &lt;code&gt;score&lt;/code&gt;, so that we can load the model using &lt;code&gt;AutoModelForSequenceClassification&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;reward_model = AutoModelForSequenceClassification.from_pretrained(
              reward_model_path,
              num_labels=1,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              use_cache=False,
          )
inputs = xxxx (Left Padding Input Tokens)
reward = reward_model.model(*inputs).last_hidden_state
reward = reward_model.score(reward)[:, -1]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;PPO/REINFORCE++ with Ray and vLLM&lt;/h3&gt; 
&lt;p&gt;To improve RLHF training speed or support 70B models, we can use the PPO with Ray and vLLM acceleration (Hybrid Engine)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# launch the master node of ray in container
ray start --head --node-ip-address 0.0.0.0 --num-gpus 8

# if you want to launch ray on more nodes, use
ray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8

ray job submit --address="http://127.0.0.1:8265" \
   --runtime-env-json='{"working_dir": "/openrlhf"}' \
   -- python3 -m openrlhf.cli.train_ppo_ray \
   --ref_num_nodes 1 \
   --ref_num_gpus_per_node 8 \
   --reward_num_nodes 1 \
   --reward_num_gpus_per_node 8 \
   --critic_num_nodes 1 \
   --critic_num_gpus_per_node 8 \
   --actor_num_nodes 1 \
   --actor_num_gpus_per_node 8 \
   --vllm_num_engines 4 \
   --vllm_tensor_parallel_size 2 \
   --colocate_all_models \
   --vllm_gpu_memory_utilization 0.5 \
   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \
   --reward_pretrain OpenRLHF/Llama-3-8b-rm-700k \
   --save_path /openrlhf/examples/test_scripts/final/llama3-8b-rlhf \
   --ckpt_path /openrlhf/examples/test_scripts/ckpt/llama3-8b-rlhf \
   --save_hf_ckpt \
   --micro_train_batch_size 8 \
   --train_batch_size 128 \
   --micro_rollout_batch_size 16 \
   --rollout_batch_size 1024 \
   --n_samples_per_prompt 1 \
   --max_epochs 1 \
   --prompt_max_len 1024 \
   --max_samples 100000 \
   --generate_max_len 1024 \
   --zero_stage 3 \
   --bf16 \
   --actor_learning_rate 5e-7 \
   --critic_learning_rate 9e-6 \
   --init_kl_coef 0.01 \
   --prompt_data OpenRLHF/prompt-collection-v0.1 \
   --input_key context_messages \
   --apply_chat_template \
   --normalize_reward \
   --gradient_checkpointing \
   --packing_samples \
   --vllm_sync_backend nccl \
   --enforce_eager \
   --vllm_enable_sleep \
   --deepspeed_enable_sleep
   --use_wandb {wandb_token}

# Support REINFORCE++  | RLOO | REINFORCE++-baseline | GRPO | Dr. GRPO
# --advantage_estimator reinforce | rloo | reinforce_baseline | group_norm | dr_grpo

# Set --init_kl_coef to 0 will not launch the reference model

# Support remote reward model (HTTP)
# --remote_rm_url http://localhost:5000/get_reward

# Support N samples
# --n_samples_per_prompt 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You can also use &lt;code&gt;setup_commands&lt;/code&gt; to let Ray automatically deploy the environment, such as &lt;code&gt;--runtime-env-json='{"setup_commands": ["pip install openrlhf[vllm]"]}'&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] RLOO and REINFORCE++-baseline in OPENRLHF are a modification based on REINFORCE++:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;REINFORCE++ integrates key optimization techniques from PPO (such as advantage normalization and PPO-clip loss) into REINFORCE while eliminating the need for a critic network.&lt;/li&gt; 
  &lt;li&gt;REINFORCE++-baseline uses the &lt;code&gt;mean reward of multiple samples from the same prompt&lt;/code&gt; as the baseline to reshape the rewards, therefore, under the RLVR setting, the algorithm insensitive to reward patterns such as 0 (incorrect) / 1 (correct) / -0.5 (format reward) or -1 (incorrect) / 1 (correct) / -0.5 (format reward).&lt;/li&gt; 
  &lt;li&gt;RLOO in OpenRLHF modifies the original version by incorporating the &lt;code&gt;per-token KL reward&lt;/code&gt; and utilizing the &lt;code&gt;PPO-clip loss&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;Dr. GRPO remove the local group normalization &lt;code&gt;/std&lt;/code&gt; in GRPO.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you you encounter an error related to index out of range when deepspeed sets up the GPU devices, you can try to set the environment variable &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/openrlhf/trainer/ray/utils.py"&gt;&lt;code&gt;RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES&lt;/code&gt;&lt;/a&gt; as a workaround.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# For NVIDIA GPUs:
export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The launch scripts and documents for supported algorithms are in &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/"&gt;example/scripts&lt;/a&gt; and &lt;a href="https://openrlhf.readthedocs.io/en/latest/usage.html"&gt;Documents - Usage&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Reinforced Fine-tuning&lt;/h2&gt; 
&lt;p&gt;OpenRLHF supports convenient and efficient Reinforced Fine-tuning. You only need to implement a &lt;a href="https://raw.githubusercontent.com/OpenRLHF/OpenRLHF/main/examples/scripts/reward_func.py"&gt;file containing the custom &lt;code&gt;reward_func&lt;/code&gt; function&lt;/a&gt; and pass its path to the &lt;code&gt;remote_rm_url&lt;/code&gt; parameter. Such as&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# reward_func.py
import torch

def reward_func(queries, prompts, labels):
    # queries is prompts + responses
    # labels is answers
    print(queries)

    # Generate random rewards as an example
    # In real applications, this should be replaced with actual reward calculation logic
    reward = torch.randint(0, 2, (len(queries),)).float()

    return {
        "rewards": reward,  # Rewards for advantage calculation
        "scores": reward,  # Scores for dynamic filtering (0-1 reward)
        "extra_logs": {"dummy_scores": reward},  # Additional logging info for wandb
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then just set&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ray job submit --address="http://127.0.0.1:8265" \
  --runtime-env-json='{"working_dir": "/openrlhf"}' \
  -- python3 -m openrlhf.cli.train_ppo_ray \
  ...
  --remote_rm_url /path/to/reward_func.py \
  --label_key answer
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;where the &lt;code&gt;label_key&lt;/code&gt; parameter is used to pass additional sample information such as answer to the reward function.&lt;/p&gt; 
&lt;h2&gt;Async RLHF &amp;amp; Agent RLHF&lt;/h2&gt; 
&lt;p&gt;OpenRLHF provides comprehensive support for both Asynchronous RLHF and Agent-based RLHF implementations. To utilize these features, simply include the &lt;code&gt;--async_train&lt;/code&gt; and &lt;code&gt;--agent_func_path&lt;/code&gt; parameters in your training configuration.&lt;/p&gt; 
&lt;p&gt;The Agent API has been redesigned to use a class-based approach with &lt;code&gt;AgentInstanceBase&lt;/code&gt; and &lt;code&gt;AgentExecutorBase&lt;/code&gt; classes for better modularity and extensibility.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# agent_func.py
import random
from typing import Any, Dict

import torch
from openrlhf.utils.agent import AgentExecutorBase, AgentInstanceBase


# A simple n-step random environment
class AgentInstance(AgentInstanceBase):
    async def __init__(self, *args, **kwargs):
        self.step_idx = 0
        self.max_steps = random.randint(1, 3)  # 1-3 steps

    async def reset(self, states: dict, **kwargs):
        return {"observation": states["observation"]}  # Return original text observation

    async def step(self, states: dict, **kwargs) -&amp;gt; Dict[str, Any]:
        print(f"step_idx: {self.step_idx}, max_steps: {self.max_steps}")

        observation_text = states["observation_text"]
        action_text = states["action_text"]
        label = states["label"]

        # Check if episode is done
        done = self.step_idx &amp;gt;= self.max_steps
        reward = torch.randint(0, 2, (1,)).float() if done else torch.tensor(0)

        # Generate environment feedback based on whether episode is done
        environment_feedback = (
            "\n\nHuman: [CORRECT]\n&amp;lt;/s&amp;gt;"
            if done
            else "\n\nHuman: [INCORRECT]\nPlease analyze the issues and try again.\n&amp;lt;/s&amp;gt;\n\nAssistant: "
        )

        self.step_idx += 1

        return {
            "rewards": reward,  # Rewards for advantage calculation
            "scores": reward,  # Scores for dynamic filtering (0-1 reward)
            "environment_feedback": environment_feedback,  # Environment feedback text
            "done": done,  # Boolean indicating if the episode is complete
            "sampling_params": states.get("sampling_params", None),  # Parameters for vLLM sampling in next step
            "extra_logs": {"dummy_scores": reward},  # Additional logging information
        }


class AgentExecutor(AgentExecutorBase):
    def __init__(self, max_steps, max_length, llm_engine, hf_tokenizer, result_queue):
        super().__init__(AgentInstance, max_steps, max_length, llm_engine, hf_tokenizer, result_queue)

    async def execute(self, prompt, label, sampling_params):
        # You could override the execute function of AgentExecutorBase to add custom agent running logic
        return await super().execute(prompt, label, sampling_params)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also configure the maximum number of concurrent agents per vLLM engine by setting &lt;code&gt;export OPENRLHF_ASYNC_NUM_TASKS=128&lt;/code&gt;. Additionally, you can control the degree of off-policy sampling by setting &lt;code&gt;export OPENRLHF_ASYNC_QUEUE_SIZE=1&lt;/code&gt; (this parameter controls how many batches of data can be stored in the buffer at most) in your environment.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] By overriding the &lt;code&gt;execute&lt;/code&gt; function of &lt;code&gt;AgentExecutorBase&lt;/code&gt;, you can implement completely custom agent running processes. The design follows the &lt;strong&gt;token-in-token-out principle&lt;/strong&gt; to ensure consistency between sampling and training samples, avoiding potential mismatches that could occur with text-level processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] OpenRLHF's Agent RLHF also supports Hybrid Engine training. To enable this feature, please remove the &lt;code&gt;--async_train&lt;/code&gt; flag and enable &lt;code&gt;--colocate_all_models&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Asynchronous training may affect the training stability. It is recommended to prioritize using Hybrid Engine or synchronous training mode.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;LoRA&lt;/h3&gt; 
&lt;p&gt;If you use &lt;code&gt;LoRA (Low-Rank Adaptation)&lt;/code&gt;, &lt;code&gt;OpenRLHF&lt;/code&gt; will not save the full weights by default instead of &lt;code&gt;LoRA Adapter&lt;/code&gt;. To continue in your task normally, you should combine the &lt;code&gt;Adapter&lt;/code&gt; with weights of your base model&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m openrlhf.cli.lora_combiner \
    --model_path meta-llama/Meta-Llama-3-8B \
    --lora_path ./checkpoint/llama3-8b-rm \
    --output_path ./checkpoint/llama-3-8b-rm-combined \
    --is_rm \
    --bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Performance Tuning Guide&lt;/h3&gt; 
&lt;p&gt;To achieve optimal performance, we recommend allocating nodes &lt;code&gt;vLLM:Actor:Critic = 1:1:1&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For example, for a 70B model with 48 A100 GPUs, it is advised to allocate 16 A100 GPUs to the vLLM Engine, 16 GPUs to the Actor model, and the remaining 16 GPUs to the Critic model.&lt;/li&gt; 
 &lt;li&gt;Enable asynchronous training &lt;code&gt;--async_train&lt;/code&gt; when the convergence of the RL algorithm meets requirements.&lt;/li&gt; 
 &lt;li&gt;Using hybrid engine &lt;code&gt;--colocate_all_models&lt;/code&gt; and &lt;code&gt;--vllm_enable_sleep&lt;/code&gt; and &lt;code&gt;--deepspeed_enable_sleep&lt;/code&gt; rather than distributed RLHF when there are enough GPU memory.&lt;/li&gt; 
 &lt;li&gt;Enable the &lt;code&gt;--colocate_critic_reward&lt;/code&gt;, &lt;code&gt;--colocate_actor_ref&lt;/code&gt; options to merge nodes.&lt;/li&gt; 
 &lt;li&gt;You should increase the &lt;code&gt;rollout_micro_batch_size&lt;/code&gt; (and minimize the TP size of vLLM engine) as much as possible. During the training phase, a larger &lt;code&gt;--micro_train_batch_size&lt;/code&gt; is better and enable &lt;code&gt;--packing_samples&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;When there are enough GPU memory, please disable &lt;code&gt;--adam_offload&lt;/code&gt; and enable &lt;code&gt;--overlap_comm&lt;/code&gt;. Also enable &lt;code&gt;--deepcompile&lt;/code&gt; to speed up the training.&lt;/li&gt; 
 &lt;li&gt;For vLLM, please use &lt;code&gt;--vllm_sync_backend nccl&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Enable &lt;code&gt;--use_dynamic_batch&lt;/code&gt; to accelerate the deepspeed training and forward.&lt;/li&gt; 
 &lt;li&gt;Enable &lt;a href="https://docs.vllm.ai/en/stable/automatic_prefix_caching/apc.html"&gt;enable_prefix_caching&lt;/a&gt; in vLLM generation when &lt;code&gt;n_samples_per_prompts&lt;/code&gt; &amp;gt; 1.&lt;/li&gt; 
 &lt;li&gt;For a large base model, if an OOM occurs, do not use any &lt;code&gt;--colocate_xxxx&lt;/code&gt; options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Companies and Organizations using OpenRLHF&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Google&lt;/li&gt; 
 &lt;li&gt;ByteDance&lt;/li&gt; 
 &lt;li&gt;Tencent&lt;/li&gt; 
 &lt;li&gt;Alibaba&lt;/li&gt; 
 &lt;li&gt;Baidu&lt;/li&gt; 
 &lt;li&gt;China Telecom&lt;/li&gt; 
 &lt;li&gt;Vivo&lt;/li&gt; 
 &lt;li&gt;Allen AI&lt;/li&gt; 
 &lt;li&gt;NexusFlow&lt;/li&gt; 
 &lt;li&gt;JÃ¼lich Supercomputing Centre (JSC)&lt;/li&gt; 
 &lt;li&gt;Berkeley Starling Team&lt;/li&gt; 
 &lt;li&gt;M-A-P&lt;/li&gt; 
 &lt;li&gt;...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Join Us&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;How to Join?&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Email us at &lt;a href="mailto:janhu9527@gmail.com"&gt;janhu9527@gmail.com&lt;/a&gt; or join &lt;a href="https://github.com/OpenRLHF"&gt;GitHub Organization&lt;/a&gt;. Please include the following details: 
  &lt;ul&gt; 
   &lt;li&gt;Your name&lt;/li&gt; 
   &lt;li&gt;Your GitHub username&lt;/li&gt; 
   &lt;li&gt;Your areas of interest&lt;/li&gt; 
   &lt;li&gt;Your skills and experience related to NLP and/or AI&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;You can also join us through the official GitHub &lt;a href="https://github.com/OpenRLHF/OpenRLHF"&gt;OpenRLHF â†—&lt;/a&gt; project page. Just create an issue about your interest to contribute and we will get back to you.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;What can you do?&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Join the team and participate in the development of the OpenRLHF project.&lt;/li&gt; 
 &lt;li&gt;Contribute to the project by submitting pull requests.&lt;/li&gt; 
 &lt;li&gt;Help improve documentation, fix bugs, or create new features.&lt;/li&gt; 
 &lt;li&gt;Share the project and help us grow the community.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Sponsor Us&lt;/h2&gt; 
&lt;p&gt;Your sponsorship can help us maintain and improve OpenRLHF. If you find this project useful, please consider sponsoring us. You can sponsor us on &lt;a href="https://opencollective.com/OpenRLHF"&gt;Open Collective â†—&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Starchart&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#OpenRLHF/OpenRLHF&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;A big thank you to all our contributors! If you want to contribute, feel free to make a pull request or create an issue.&lt;/p&gt; 
&lt;a href="https://github.com/OpenRLHF/OpenRLHF/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF" /&gt; &lt;/a&gt; 
&lt;h2&gt;References &amp;amp; Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to express our gratitude to the following projects and organizations for their contributions to the field of AI and NLP:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;Hugging Face Transformers â†—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/gpt-3"&gt;OpenAI GPT â†—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://llama.meta.com/"&gt;LLaMA â†—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/DeepSpeed"&gt;DeepSpeed â†—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ray-project/ray"&gt;Ray â†—&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Our project would also like to thank &lt;a href="https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalChat"&gt;ColossalChat&lt;/a&gt; and &lt;a href="https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat"&gt;DeepSpeedChat&lt;/a&gt;. In the early stages of the project, we referred to their code design. Our project would like to thank &lt;a href="https://www.netmind.ai/"&gt;Netmind.AI&lt;/a&gt; for the GPU support of developing ring attention.&lt;/p&gt; 
&lt;p&gt;(2024/7) Our GitHub organization has changed from OpenLLMAI to OpenRLHF.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;OpenRLHF&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{hu2024openrlhf,
  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},
  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},
  journal={arXiv preprint arXiv:2405.11143},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;REINFORCE++-baseline&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{hu2025reinforce++,
  title={Reinforce++: A simple and efficient approach for aligning large language models},
  author={Hu, Jian},
  journal={arXiv preprint arXiv:2501.03262},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;OpenRLHF Â© 2025 OpenRLHF. All Rights Reserved.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kijai/ComfyUI-WanVideoWrapper</title>
      <link>https://github.com/kijai/ComfyUI-WanVideoWrapper</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI wrapper nodes for &lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;WanVideo&lt;/a&gt; and related models.&lt;/h1&gt; 
&lt;h1&gt;WORK IN PROGRESS (perpetually)&lt;/h1&gt; 
&lt;h1&gt;Why should I use custom nodes when WanVideo works natively?&lt;/h1&gt; 
&lt;p&gt;Short answer: Unless it's a model/feature not available yet on native, you shouldn't.&lt;/p&gt; 
&lt;p&gt;Long answer: Due to the complexity of ComfyUI core code, and my lack of coding experience, in many cases it's far easier and faster to implement new models and features to a standalone wrapper, so this is a way to test things relatively quickly. I consider this my personal sandbox (which is obviously open for everyone) to play with without having to worry about compability issues etc, but as such this code is always work in progress and prone to have issues. Also not all new models end up being worth the trouble to implement in core Comfy, though I've also made some patcher nodes to allow using them in native workflows, such as the &lt;a href="https://huggingface.co/bytedance-research/ATI"&gt;ATI&lt;/a&gt; node available in this wrapper. This is also the end goal, idea isn't to compete or even offer alternatives to everything available in native workflows. All that said (this is clearly not a sales pitch) I do appreciate everyone using these nodes to explore new releases and possibilities with WanVideo.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repo into &lt;code&gt;custom_nodes&lt;/code&gt; folder.&lt;/li&gt; 
 &lt;li&gt;Install dependencies: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; or if you use the portable install, run this in ComfyUI_windows_portable -folder:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\ComfyUI-WanVideoWrapper\requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/Kijai/WanVideo_comfy/tree/main"&gt;https://huggingface.co/Kijai/WanVideo_comfy/tree/main&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;fp8 scaled models (personal recommendation):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled"&gt;https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Text encoders to &lt;code&gt;ComfyUI/models/text_encoders&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Clip vision to &lt;code&gt;ComfyUI/models/clip_vision&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Transformer (main video model) to &lt;code&gt;ComfyUI/models/diffusion_models&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Vae to &lt;code&gt;ComfyUI/models/vae&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can also use the native ComfyUI text encoding and clip vision loader with the wrapper instead of the original models:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/6a2fd9a5-8163-4c93-b362-92ef34dbd3a4" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;GGUF models can now be loaded in the main model loader as well.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Supported extra models:&lt;/p&gt; 
&lt;p&gt;SkyReels: &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9"&gt;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;WanVideoFun: &lt;a href="https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17"&gt;https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ReCamMaster: &lt;a href="https://github.com/KwaiVGI/ReCamMaster"&gt;https://github.com/KwaiVGI/ReCamMaster&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;VACE: &lt;a href="https://github.com/ali-vilab/VACE"&gt;https://github.com/ali-vilab/VACE&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Phantom: &lt;a href="https://huggingface.co/bytedance-research/Phantom"&gt;https://huggingface.co/bytedance-research/Phantom&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ATI: &lt;a href="https://huggingface.co/bytedance-research/ATI"&gt;https://huggingface.co/bytedance-research/ATI&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Uni3C: &lt;a href="https://github.com/alibaba-damo-academy/Uni3C"&gt;https://github.com/alibaba-damo-academy/Uni3C&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MiniMaxRemover: &lt;a href="https://huggingface.co/zibojia/minimax-remover"&gt;https://huggingface.co/zibojia/minimax-remover&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MAGREF: &lt;a href="https://huggingface.co/MAGREF-Video/MAGREF"&gt;https://huggingface.co/MAGREF-Video/MAGREF&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;FantasyTalking: &lt;a href="https://github.com/Fantasy-AMAP/fantasy-talking"&gt;https://github.com/Fantasy-AMAP/fantasy-talking&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;FantasyPortrait: &lt;a href="https://github.com/Fantasy-AMAP/fantasy-portrait"&gt;https://github.com/Fantasy-AMAP/fantasy-portrait&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MultiTalk: &lt;a href="https://github.com/MeiGen-AI/MultiTalk"&gt;https://github.com/MeiGen-AI/MultiTalk&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;EchoShot: &lt;a href="https://github.com/D2I-ai/EchoShot"&gt;https://github.com/D2I-ai/EchoShot&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Stand-In: &lt;a href="https://github.com/WeChatCV/Stand-In"&gt;https://github.com/WeChatCV/Stand-In&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;HuMo: &lt;a href="https://github.com/Phantom-video/HuMo"&gt;https://github.com/Phantom-video/HuMo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;WanAnimate: &lt;a href="https://github.com/Wan-Video/Wan2.2/tree/main/wan/modules/animate"&gt;https://github.com/Wan-Video/Wan2.2/tree/main/wan/modules/animate&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Examples:&lt;/h2&gt; 
&lt;p&gt;WanAnimate:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f370b001-0f98-4c4c-bcb5-cfad0b330697"&gt;https://github.com/user-attachments/assets/f370b001-0f98-4c4c-bcb5-cfad0b330697&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KwaiVGI/ReCamMaster"&gt;ReCamMaster&lt;/a&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/c58a12c2-13ba-4af8-8041-e283dbef197e"&gt;https://github.com/user-attachments/assets/c58a12c2-13ba-4af8-8041-e283dbef197e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;TeaCache (with the old temporary WIP naive version, I2V):&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note that with the new version the threshold values should be 10x higher&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Range of 0.25-0.30 seems good when using the coefficients, start step can be 0, with more aggressive threshold values it may make sense to start later to avoid any potential step skips early on, that generally ruin the motion.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/504a9a50-3337-43d2-97b8-8e1661f29f46"&gt;https://github.com/user-attachments/assets/504a9a50-3337-43d2-97b8-8e1661f29f46&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Context window test:&lt;/p&gt; 
&lt;p&gt;1025 frames using window size of 81 frames, with 16 overlap. With the 1.3B T2V model this used under 5GB VRAM and took 10 minutes to gen on a 5090:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/89b393af-cf1b-49ae-aa29-23e57f65911e"&gt;https://github.com/user-attachments/assets/89b393af-cf1b-49ae-aa29-23e57f65911e&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;This very first test was 512x512x81&lt;/p&gt; 
&lt;p&gt;~16GB used with 20/40 blocks offloaded&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fa6d0a4f-4a4d-4de5-84a4-877cc37b715f"&gt;https://github.com/user-attachments/assets/fa6d0a4f-4a4d-4de5-84a4-877cc37b715f&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Vid2vid example:&lt;/p&gt; 
&lt;p&gt;with 14B T2V model:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ef228b8a-a13a-4327-8a1b-1eb343cf00d8"&gt;https://github.com/user-attachments/assets/ef228b8a-a13a-4327-8a1b-1eb343cf00d8&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;with 1.3B T2V model&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4f35ba84-da7a-4d5b-97ee-9641296f391e"&gt;https://github.com/user-attachments/assets/4f35ba84-da7a-4d5b-97ee-9641296f391e&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/RD-Agent</title>
      <link>https://github.com/microsoft/RD-Agent</link>
      <description>&lt;p&gt;Research and development (R&amp;D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&amp;D are mainly focused on data and models. We are committed to automating these high-value generic R&amp;D processes through R&amp;D-Agent, which lets AI drive data-driven AI. ğŸ”—https://aka.ms/RD-Agent-Tech-Report&lt;/p&gt;&lt;hr&gt;&lt;h4 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/logo.png" alt="RA-Agent logo" style="width:70%; " /&gt; &lt;p&gt;&lt;a href="https://rdagent.azurewebsites.net" target="_blank"&gt;ğŸ–¥ï¸ Live Demo&lt;/a&gt; | &lt;a href="https://rdagent.azurewebsites.net/factor_loop" target="_blank"&gt;ğŸ¥ Demo Video&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR" target="_blank"&gt;â–¶ï¸YouTube&lt;/a&gt; | &lt;a href="https://rdagent.readthedocs.io/en/latest/index.html" target="_blank"&gt;ğŸ“– Documentation&lt;/a&gt; | &lt;a href="https://aka.ms/RD-Agent-Tech-Report" target="_blank"&gt;ğŸ“„ Tech Report&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#-paperwork-list"&gt; ğŸ“ƒ Papers &lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg?sanitize=true" alt="CodeQL" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg?sanitize=true" alt="Dependabot Updates" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg?sanitize=true" alt="Lint PR Title" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/release.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg?sanitize=true" alt="Release.yml" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/rdagent/#files"&gt;&lt;img src="https://img.shields.io/badge/platform-Linux-blue" alt="Platform" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/rdagent/"&gt;&lt;img src="https://img.shields.io/pypi/v/rdagent" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/rdagent/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/rdagent" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/microsoft/RD-Agent" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/microsoft/RD-Agent" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pre-commit/pre-commit"&gt;&lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit" alt="pre-commit" /&gt;&lt;/a&gt; &lt;a href="http://mypy-lang.org/"&gt;&lt;img src="https://www.mypy-lang.org/static/mypy_badge.svg?sanitize=true" alt="Checked with mypy" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ybQ97B6Jjy"&gt;&lt;img src="https://img.shields.io/badge/chat-discord-blue" alt="Chat" /&gt;&lt;/a&gt; &lt;a href="https://rdagent.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/rdagent/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg?sanitize=true" alt="Readthedocs Preview" /&gt;&lt;/a&gt; 
 &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt; &lt;a href="https://arxiv.org/abs/2505.14738"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2505.14738-00ff00.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ“° News&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ğŸ—ï¸ News&lt;/th&gt; 
   &lt;th&gt;ğŸ“ Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NeurIPS 2025 Acceptance&lt;/td&gt; 
   &lt;td&gt;We are thrilled to announce that our paper &lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant&lt;/a&gt; has been accepted to NeurIPS 2025&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#overall-technical-report"&gt;Technical Report Release&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Overall framework description and results on MLE-bench&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#deep-application-in-diverse-scenarios"&gt;R&amp;amp;D-Agent-Quant Release&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Apply R&amp;amp;D-Agent to quant trading&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MLE-Bench Results Released&lt;/td&gt; 
   &lt;td&gt;R&amp;amp;D-Agent currently leads as the &lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#-the-best-machine-learning-engineering-agent"&gt;top-performing machine learning engineering agent&lt;/a&gt; on MLE-bench&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support LiteLLM Backend&lt;/td&gt; 
   &lt;td&gt;We now fully support &lt;strong&gt;&lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;&lt;/strong&gt; as our default backend for integration with multiple LLM providers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;General Data Science Agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;Data Science Agent&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kaggle Scenario release&lt;/td&gt; 
   &lt;td&gt;We release &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;Kaggle Agent&lt;/a&gt;&lt;/strong&gt;, try the new features!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Official WeChat group release&lt;/td&gt; 
   &lt;td&gt;We created a WeChat group, welcome to join! (ğŸ—ª&lt;a href="https://github.com/microsoft/RD-Agent/issues/880"&gt;QR Code&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Official Discord release&lt;/td&gt; 
   &lt;td&gt;We launch our first chatting channel in Discord (ğŸ—ª&lt;a href="https://discord.gg/ybQ97B6Jjy"&gt;&lt;img src="https://img.shields.io/badge/chat-discord-blue" alt="Chat" /&gt;&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;First release&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;R&amp;amp;D-Agent&lt;/strong&gt; is released on GitHub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;ğŸ† The Best Machine Learning Engineering Agent!&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/openai/mle-bench"&gt;MLE-bench&lt;/a&gt; is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems' capabilities in real-world ML engineering scenarios.&lt;/p&gt; 
&lt;p&gt;R&amp;amp;D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent&lt;/th&gt; 
   &lt;th&gt;Low == Lite (%)&lt;/th&gt; 
   &lt;th&gt;Medium (%)&lt;/th&gt; 
   &lt;th&gt;High (%)&lt;/th&gt; 
   &lt;th&gt;All (%)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;R&amp;amp;D-Agent o3(R)+GPT-4.1(D)&lt;/td&gt; 
   &lt;td&gt;51.52 Â± 6.9&lt;/td&gt; 
   &lt;td&gt;19.3 Â± 5.5&lt;/td&gt; 
   &lt;td&gt;26.67 Â± 0&lt;/td&gt; 
   &lt;td&gt;30.22 Â± 1.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;R&amp;amp;D-Agent o1-preview&lt;/td&gt; 
   &lt;td&gt;48.18 Â± 2.49&lt;/td&gt; 
   &lt;td&gt;8.95 Â± 2.36&lt;/td&gt; 
   &lt;td&gt;18.67 Â± 2.98&lt;/td&gt; 
   &lt;td&gt;22.4 Â± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AIDE o1-preview&lt;/td&gt; 
   &lt;td&gt;34.3 Â± 2.4&lt;/td&gt; 
   &lt;td&gt;8.8 Â± 1.1&lt;/td&gt; 
   &lt;td&gt;10.0 Â± 1.9&lt;/td&gt; 
   &lt;td&gt;16.9 Â± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;O3(R)+GPT-4.1(D)&lt;/strong&gt;: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AIDE o1-preview&lt;/strong&gt;: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.&lt;/li&gt; 
 &lt;li&gt;Average and standard deviation results for R&amp;amp;D-Agent o1-preview is based on a independent of 5 seeds and for R&amp;amp;D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.&lt;/li&gt; 
 &lt;li&gt;According to MLE-Bench, the 75 competitions are categorized into three levels of complexity: &lt;strong&gt;Low==Lite&lt;/strong&gt; if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models; &lt;strong&gt;Medium&lt;/strong&gt; if it takes between 2 and 10 hours; and &lt;strong&gt;High&lt;/strong&gt; if it takes more than 10 hours.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can inspect the detailed runs of the above results online.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/RD-Agent_MLE-Bench_O1-preview"&gt;R&amp;amp;D-Agent o1-preview detailed runs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41"&gt;R&amp;amp;D-Agent o3(R)+GPT-4.1(D) detailed runs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For running R&amp;amp;D-Agent on MLE-bench, refer to &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;MLE-bench Guide: Running ML Engineering via MLE-bench&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ¥‡ The First Data-Centric Quant Multi-Agent Framework!&lt;/h1&gt; 
&lt;p&gt;R&amp;amp;D-Agent for Quantitative Finance, in short &lt;strong&gt;RD-Agent(Q)&lt;/strong&gt;, is the first data-centric, multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;Extensive experiments in real stock markets show that, at a cost under $10, RD-Agent(Q) achieves approximately 2Ã— higher ARR than benchmark factor libraries while using over 70% fewer factors. It also surpasses state-of-the-art deep time-series models under smaller resource budgets. Its alternating factorâ€“model optimization further delivers excellent trade-off between predictive accuracy and strategy robustness.&lt;/p&gt; 
&lt;p&gt;You can learn more details about &lt;strong&gt;RD-Agent(Q)&lt;/strong&gt; through the &lt;a href="https://arxiv.org/abs/2505.15155"&gt;paper&lt;/a&gt; and reproduce it through the &lt;a href="https://rdagent.readthedocs.io/en/latest/scens/quant_agent_fin.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Data Science Agent Preview&lt;/h1&gt; 
&lt;p&gt;Check out our demo video showcasing the current progress of our Data Science Agent under development:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305"&gt;https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸŒŸ Introduction&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/scen.png" alt="Our focused scenario" style="width:80%; " /&gt; 
&lt;/div&gt; 
&lt;p&gt;R&amp;amp;D-Agent aims to automate the most critical and valuable aspects of the industrial R&amp;amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. Methodologically, we have identified a framework with two key components: 'R' for proposing new ideas and 'D' for implementing them. We believe that the automatic evolution of R&amp;amp;D will lead to solutions of significant industrial value.&lt;/p&gt; 
&lt;!-- Tag Cloud --&gt; 
&lt;p&gt;R&amp;amp;D is a very general scenario. The advent of R&amp;amp;D-Agent can be your&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’° &lt;strong&gt;Automatic Quant Factory&lt;/strong&gt; (&lt;a href="https://rdagent.azurewebsites.net/factor_loop"&gt;ğŸ¥Demo Video&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s"&gt;â–¶ï¸YouTube&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Data Mining Agent:&lt;/strong&gt; Iteratively proposing data &amp;amp; models (&lt;a href="https://rdagent.azurewebsites.net/model_loop"&gt;ğŸ¥Demo Video 1&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s"&gt;â–¶ï¸YouTube&lt;/a&gt;) (&lt;a href="https://rdagent.azurewebsites.net/dmm"&gt;ğŸ¥Demo Video 2&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=VIaSTZuoZg4"&gt;â–¶ï¸YouTube&lt;/a&gt;) and implementing them by gaining knowledge from data.&lt;/li&gt; 
 &lt;li&gt;ğŸ¦¾ &lt;strong&gt;Research Copilot:&lt;/strong&gt; Auto read research papers (&lt;a href="https://rdagent.azurewebsites.net/report_model"&gt;ğŸ¥Demo Video&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=BiA2SfdKQ7o"&gt;â–¶ï¸YouTube&lt;/a&gt;) / financial reports (&lt;a href="https://rdagent.azurewebsites.net/report_factor"&gt;ğŸ¥Demo Video&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=ECLTXVcSx-c"&gt;â–¶ï¸YouTube&lt;/a&gt;) and implement model structures or building datasets.&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Kaggle Agent:&lt;/strong&gt; Auto Model Tuning and Feature Engineering(&lt;a href=""&gt;ğŸ¥Demo Video Coming Soon...&lt;/a&gt;) and implementing them to achieve more in competitions.&lt;/li&gt; 
 &lt;li&gt;...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can click the links above to view the demo. We're continuously adding more methods and scenarios to the project to enhance your R&amp;amp;D processes and boost productivity.&lt;/p&gt; 
&lt;p&gt;Additionally, you can take a closer look at the examples in our &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net/"&gt;ğŸ–¥ï¸ Live Demo&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://rdagent.azurewebsites.net/" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/demo.png" alt="Watch the demo" width="80%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;âš¡ Quick start&lt;/h1&gt; 
&lt;h3&gt;RD-Agent currently only supports Linux.&lt;/h3&gt; 
&lt;p&gt;You can try above demos by running the following command:&lt;/p&gt; 
&lt;h3&gt;ğŸ³ Docker installation.&lt;/h3&gt; 
&lt;p&gt;Users must ensure Docker is installed before attempting most scenarios. Please refer to the &lt;a href="https://docs.docker.com/engine/install/"&gt;official ğŸ³Docker page&lt;/a&gt; for installation instructions. Ensure the current user can run Docker commands &lt;strong&gt;without using sudo&lt;/strong&gt;. You can verify this by executing &lt;code&gt;docker run hello-world&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ Create a Conda Environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI): &lt;pre&gt;&lt;code class="language-sh"&gt;conda create -n rdagent python=3.10
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Activate the environment: &lt;pre&gt;&lt;code class="language-sh"&gt;conda activate rdagent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ› ï¸ Install the R&amp;amp;D-Agent&lt;/h3&gt; 
&lt;h4&gt;For Users&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can directly install the R&amp;amp;D-Agent package from PyPI: &lt;pre&gt;&lt;code class="language-sh"&gt;pip install rdagent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;For Developers&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you want to try the latest version or contribute to RD-Agent, you can install it from the source and follow the development setup: &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/microsoft/RD-Agent
cd RD-Agent
make dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;More details can be found in the &lt;a href="https://rdagent.readthedocs.io/en/latest/development.html"&gt;development setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ’Š Health check&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;rdagent provides a health check that currently checks two things. 
  &lt;ul&gt; 
   &lt;li&gt;whether the docker installation was successful.&lt;/li&gt; 
   &lt;li&gt;whether the default port used by the &lt;a href="https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results"&gt;rdagent ui&lt;/a&gt; is occupied.&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent health_check --no-check-env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;âš™ï¸ Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;The demos requires following ability:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ChatCompletion&lt;/li&gt; 
   &lt;li&gt;json_mode&lt;/li&gt; 
   &lt;li&gt;embedding query&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;You can set your Chat Model and Embedding Model in the following ways:&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;ğŸ”¥ Attention&lt;/strong&gt;: We now provide experimental support for &lt;strong&gt;DeepSeek&lt;/strong&gt; models! You can use DeepSeek's official API for cost-effective and high-performance inference. See the configuration example below for DeepSeek setup.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using LiteLLM (Default)&lt;/strong&gt;: We now support LiteLLM as a backend for integration with multiple LLM providers. You can configure in multiple ways:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option 1: Unified API base for both models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Configuration Example: &lt;code&gt;OpenAI&lt;/code&gt; Setup :&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
# Set to any model supported by LiteLLM.
CHAT_MODEL=gpt-4o 
EMBEDDING_MODEL=text-embedding-3-small
# Configure unified API base
OPENAI_API_BASE=&amp;lt;your_unified_api_base&amp;gt;
OPENAI_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Configuration Example: &lt;code&gt;Azure OpenAI&lt;/code&gt; Setup :&lt;/em&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Before using this configuration, please confirm in advance that your &lt;code&gt;Azure OpenAI API key&lt;/code&gt; supports &lt;code&gt;embedded models&lt;/code&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
EMBEDDING_MODEL=azure/&amp;lt;Model deployment supporting embedding&amp;gt;
CHAT_MODEL=azure/&amp;lt;your deployment name&amp;gt;
AZURE_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;
AZURE_API_BASE=&amp;lt;your_unified_api_base&amp;gt;
AZURE_API_VERSION=&amp;lt;azure api version&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Option 2: Separate API bases for Chat and Embedding models&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
# Set to any model supported by LiteLLM.
# Configure separate API bases for chat and embedding

# CHAT MODEL:
CHAT_MODEL=gpt-4o 
OPENAI_API_BASE=&amp;lt;your_chat_api_base&amp;gt;
OPENAI_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;

# EMBEDDING MODEL:
# TAKE siliconflow as an example, you can use other providers.
# Note: embedding requires litellm_proxy prefix
EMBEDDING_MODEL=litellm_proxy/BAAI/bge-large-en-v1.5
LITELLM_PROXY_API_KEY=&amp;lt;replace_with_your_siliconflow_api_key&amp;gt;
LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Configuration Example: &lt;code&gt;DeepSeek&lt;/code&gt; Setup :&lt;/em&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Since many users encounter configuration errors when setting up DeepSeek. Here's a complete working example for DeepSeek Setup:&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
# CHAT MODEL: Using DeepSeek Official API
CHAT_MODEL=deepseek/deepseek-chat 
DEEPSEEK_API_KEY=&amp;lt;replace_with_your_deepseek_api_key&amp;gt;

# EMBEDDING MODEL: Using SiliconFlow for embedding since deepseek has no embedding model.
# Note: embedding requires litellm_proxy prefix
EMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3
LITELLM_PROXY_API_KEY=&amp;lt;replace_with_your_siliconflow_api_key&amp;gt;
LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice: If you are using reasoning models that include thought processes in their responses (such as &amp;lt;think&amp;gt; tags), you need to set the following environment variable:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;REASONING_THINK_RM=True
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use a deprecated backend if you only use &lt;code&gt;OpenAI API&lt;/code&gt; or &lt;code&gt;Azure OpenAI&lt;/code&gt; directly. For this deprecated setting and more configuration information, please refer to the &lt;a href="https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If your environment configuration is complete, please execute the following commands to check if your configuration is valid. This step is necessary.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;rdagent health_check
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ Run the Application&lt;/h3&gt; 
&lt;p&gt;The &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net/"&gt;ğŸ–¥ï¸ Live Demo&lt;/a&gt;&lt;/strong&gt; is implemented by the following commands(each item represents one demo, you can select the one you prefer):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Factors Model Joint Evolution&lt;/strong&gt;: &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; self-loop factor &amp;amp; model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent fin_quant
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Factors Evolution&lt;/strong&gt;: &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; self-loop factor proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent fin_factor
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Model Evolution&lt;/strong&gt;: &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; self-loop model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent fin_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Factors Extraction from Financial Reports&lt;/strong&gt;: Run the &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; factor extraction and implementation application based on financial reports&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# 1. Generally, you can run this scenario using the following command:
rdagent fin_factor_report --report-folder=&amp;lt;Your financial reports folder path&amp;gt;

# 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
unzip all_reports.zip -d git_ignore_folder/reports
rdagent fin_factor_report --report-folder=git_ignore_folder/reports
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Model Research &amp;amp; Development Copilot&lt;/strong&gt;: model extraction and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# 1. Generally, you can run your own papers/reports with the following command:
rdagent general_model &amp;lt;Your paper URL&amp;gt;

# 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
rdagent general_model  "https://arxiv.org/pdf/2210.09789"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Medical Prediction Model Evolution&lt;/strong&gt;: Medical self-loop model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Generally, you can run the data science program with the following command:
rdagent data_science --competition &amp;lt;your competition name&amp;gt;

# Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:

# 1. Download the dataset, extract it to the target folder.
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/ds_data/arf-12-hours-prediction-task.zip
unzip arf-12-hours-prediction-task.zip -d ./git_ignore_folder/ds_data/

# 2. Configure environment variables in the `.env` file
dotenv set DS_LOCAL_DATA_PATH "$(pwd)/git_ignore_folder/ds_data"
dotenv set DS_CODER_ON_WHOLE_PIPELINE True
dotenv set DS_IF_USING_MLE_DATA False
dotenv set DS_SAMPLE_DATA_BY_LLM False
dotenv set DS_SCEN rdagent.scenarios.data_science.scen.DataScienceScen

# 3. run the application
rdagent data_science --competition arf-12-hours-prediction-task
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; For more information about the dataset, please refer to the &lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Kaggle Model Tuning &amp;amp; Feature Engineering&lt;/strong&gt;: self-loop model proposal and feature engineering implementation application &lt;br /&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Using &lt;strong&gt;tabular-playground-series-dec-2021&lt;/strong&gt; as an example. &lt;br /&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;Register and login on the &lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt; website. &lt;br /&gt;&lt;/li&gt; 
    &lt;li&gt;Configuring the Kaggle API. &lt;br /&gt; (1) Click on the avatar (usually in the top right corner of the page) -&amp;gt; &lt;code&gt;Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Create New Token&lt;/code&gt;, A file called &lt;code&gt;kaggle.json&lt;/code&gt; will be downloaded. &lt;br /&gt; (2) Move &lt;code&gt;kaggle.json&lt;/code&gt; to &lt;code&gt;~/.config/kaggle/&lt;/code&gt; &lt;br /&gt; (3) Modify the permissions of the kaggle.json file. Reference command: &lt;code&gt;chmod 600 ~/.config/kaggle/kaggle.json&lt;/code&gt; &lt;br /&gt;&lt;/li&gt; 
    &lt;li&gt;Join the competition: Click &lt;code&gt;Join the competition&lt;/code&gt; -&amp;gt; &lt;code&gt;I Understand and Accept&lt;/code&gt; at the bottom of the &lt;a href="https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/data"&gt;competition details page&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Generally, you can run the Kaggle competition program with the following command:
rdagent data_science --competition &amp;lt;your competition name&amp;gt;

# 1. Configure environment variables in the `.env` file
mkdir -p ./git_ignore_folder/ds_data
dotenv set DS_LOCAL_DATA_PATH "$(pwd)/git_ignore_folder/ds_data"
dotenv set DS_CODER_ON_WHOLE_PIPELINE True
dotenv set DS_IF_USING_MLE_DATA True
dotenv set DS_SAMPLE_DATA_BY_LLM True
dotenv set DS_SCEN rdagent.scenarios.data_science.scen.KaggleScen

# 2. run the application
rdagent data_science --competition tabular-playground-series-dec-2021
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ–¥ï¸ Monitor the Application Results&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can run the following command for our demo program to see the run logs.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent ui --port 19899 --log-dir &amp;lt;your log folder like "log/"&amp;gt; --data-science
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;About the &lt;code&gt;data_science&lt;/code&gt; parameter: If you want to see the logs of the data science scenario, set the &lt;code&gt;data_science&lt;/code&gt; parameter to &lt;code&gt;True&lt;/code&gt;; otherwise set it to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.&lt;/p&gt; &lt;p&gt;You can check if a port is occupied by running the following command.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent health_check --no-check-env --no-check-docker
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ­ Scenarios&lt;/h1&gt; 
&lt;p&gt;We have applied R&amp;amp;D-Agent to multiple valuable data-driven industrial scenarios.&lt;/p&gt; 
&lt;h2&gt;ğŸ¯ Goal: Agent for Data-driven R&amp;amp;D&lt;/h2&gt; 
&lt;p&gt;In this project, we are aiming to build an Agent to automate Data-Driven R&amp;amp;D that can&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“„ Read real-world material (reports, papers, etc.) and &lt;strong&gt;extract&lt;/strong&gt; key formulas, descriptions of interested &lt;strong&gt;features&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt;, which are the key components of data-driven R&amp;amp;D .&lt;/li&gt; 
 &lt;li&gt;ğŸ› ï¸ &lt;strong&gt;Implement&lt;/strong&gt; the extracted formulas (e.g., features, factors, and models) in runnable codes. 
  &lt;ul&gt; 
   &lt;li&gt;Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;ğŸ’¡ Propose &lt;strong&gt;new ideas&lt;/strong&gt; based on current knowledge and observations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- ![Data-Centric R&amp;D Overview](docs/_static/overview.png) --&gt; 
&lt;h2&gt;ğŸ“ˆ Scenarios/Demos&lt;/h2&gt; 
&lt;p&gt;In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ğŸ¦¾Copilot and ğŸ¤–Agent.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The ğŸ¦¾Copilot follows human instructions to automate repetitive tasks.&lt;/li&gt; 
 &lt;li&gt;The ğŸ¤–Agent, being more autonomous, actively proposes ideas for better results in the future.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The supported scenarios are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario/Target&lt;/th&gt; 
   &lt;th&gt;Model Implementation&lt;/th&gt; 
   &lt;th&gt;Data Building&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ’¹ Finance&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ¤– &lt;a href="https://rdagent.azurewebsites.net/model_loop"&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s"&gt;â–¶ï¸YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ¤– &lt;a href="https://rdagent.azurewebsites.net/factor_loop"&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s"&gt;â–¶ï¸YouTube&lt;/a&gt; &lt;br /&gt; ğŸ¦¾ &lt;a href="https://rdagent.azurewebsites.net/report_factor"&gt;Auto reports reading &amp;amp; implementation&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=ECLTXVcSx-c"&gt;â–¶ï¸YouTube&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ©º Medical&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ¤– &lt;a href="https://rdagent.azurewebsites.net/dmm"&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=VIaSTZuoZg4"&gt;â–¶ï¸YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ­ General&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ¦¾ &lt;a href="https://rdagent.azurewebsites.net/report_model"&gt;Auto paper reading &amp;amp; implementation&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=BiA2SfdKQ7o"&gt;â–¶ï¸YouTube&lt;/a&gt; &lt;br /&gt; ğŸ¤– Auto Kaggle Model Tuning&lt;/td&gt; 
   &lt;td&gt;ğŸ¤–Auto Kaggle feature Engineering&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html#roadmap"&gt;RoadMap&lt;/a&gt;&lt;/strong&gt;: Currently, we are working hard to add new features to the Kaggle scenario.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.&lt;/p&gt; 
&lt;p&gt;Here is a gallery of &lt;a href="https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip"&gt;successful explorations&lt;/a&gt; (5 traces showed in &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net/"&gt;ğŸ–¥ï¸ Live Demo&lt;/a&gt;&lt;/strong&gt;). You can download and view the execution trace using &lt;a href="https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results"&gt;this command&lt;/a&gt; from the documentation.&lt;/p&gt; 
&lt;p&gt;Please refer to &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/catalog.html"&gt;ğŸ“–readthedocs_scen&lt;/a&gt;&lt;/strong&gt; for more details of the scenarios.&lt;/p&gt; 
&lt;h1&gt;âš™ï¸ Framework&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/Framework-RDAgent.png" alt="Framework-RDAgent" width="85%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Automating the R&amp;amp;D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.&lt;/p&gt; 
&lt;p&gt;The research questions within this framework can be divided into three main categories:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Research Area&lt;/th&gt; 
   &lt;th&gt;Paper/Work List&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Benchmark the R&amp;amp;D abilities&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#benchmark"&gt;Benchmark&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Idea proposal:&lt;/strong&gt; Explore new ideas or refine existing ones&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#research"&gt;Research&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ability to realize ideas:&lt;/strong&gt; Implement and execute ideas&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#development"&gt;Development&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;We believe that the key to delivering high-quality solutions lies in the ability to evolve R&amp;amp;D capabilities. Agents should learn like human experts, continuously improving their R&amp;amp;D skills.&lt;/p&gt; 
&lt;p&gt;More documents can be found in the &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/"&gt;ğŸ“– readthedocs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;ğŸ“ƒ Paper/Work list&lt;/h1&gt; 
&lt;h2&gt;Overall Technical Report&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.14738"&gt;R&amp;amp;D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{yang2024rdagent,
    title={R\&amp;amp;D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution},
    author={Xu Yang and Xiao Yang and Shikai Fang and Bowen Xian and Yuante Li and Jian Wang and Minrui Xu and Haoran Pan and Xinpeng Hong and Weiqing Liu and Yelong Shen and Weizhu Chen and Jiang Bian},
    year={2025},
    eprint={2505.14738},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2505.14738}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/28b0488d-a546-4fef-8dc5-563ed64a9b4d" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“Š Benchmark&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2404.11276"&gt;Towards Data-Centric Automatic R&amp;amp;D&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{chen2024datacentric,
    title={Towards Data-Centric Automatic R&amp;amp;D},
    author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2404.11276},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/494f55d3-de9e-4e73-ba3d-a787e8f9e841" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ” Research&lt;/h2&gt; 
&lt;p&gt;In a data mining expert's daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.&lt;/p&gt; 
&lt;p&gt;Based on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.&lt;/p&gt; 
&lt;p&gt;For more detail, please refer to our &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net"&gt;ğŸ–¥ï¸ Live Demo page&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ› ï¸ Development&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2407.18690"&gt;Collaborative Evolving Strategy for Automatic Data-Centric Development&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{yang2024collaborative,
    title={Collaborative Evolving Strategy for Automatic Data-Centric Development},
    author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2407.18690},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75d9769b-0edd-4caf-9d45-57d1e577054b" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Deep Application in Diverse Scenarios&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{li2025rdagentquant,
    title={R\&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3186f67a-c2f8-4b6b-8bb9-a9b959c13866" alt="image" /&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ¤ Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions and suggestions to improve R&amp;amp;D-Agent. Please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for more details on how to contribute.&lt;/p&gt; 
&lt;p&gt;Before submitting a pull request, ensure that your code passes the automatic CI checks.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Guidelines&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Contributing to this project is straightforward and rewarding. Whether it's solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve R&amp;amp;D-Agent.&lt;/p&gt; 
&lt;p&gt;To get started, you can explore the issues list, or search for &lt;code&gt;TODO:&lt;/code&gt; comments in the codebase by running the command &lt;code&gt;grep -r "TODO:"&lt;/code&gt;.&lt;/p&gt; 
&lt;img src="https://img.shields.io/github/contributors-anon/microsoft/RD-Agent" /&gt; 
&lt;a href="https://github.com/microsoft/RD-Agent/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=microsoft/RD-Agent&amp;amp;max=100&amp;amp;columns=15" /&gt; &lt;/a&gt; 
&lt;p&gt;Before we released R&amp;amp;D-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.&lt;/p&gt; 
&lt;h1&gt;âš–ï¸ Legal disclaimer&lt;/h1&gt; 
&lt;p style="line-height: 1; font-style: italic;"&gt;The RD-agent is provided â€œas isâ€, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hesreallyhim/awesome-claude-code</title>
      <link>https://github.com/hesreallyhim/awesome-claude-code</link>
      <description>&lt;p&gt;A curated list of awesome commands, files, and workflows for Claude Code&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Same ASCII art for all screen sizes, just scales down on mobile --&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="assets/logo-dark.svg" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/assets/logo-light.svg?sanitize=true" alt="Awesome Claude Code" width="100%" style="max-width: 900px;" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;!-- Generated with https://github.com/denvercoder1/readme-typing-svg --&gt; 
&lt;p&gt;&lt;a href="https://git.io/typing-svg"&gt;&lt;img src="https://readme-typing-svg.demolab.com?font=Fira+Code&amp;amp;weight=600&amp;amp;pause=1000&amp;amp;color=F7080D&amp;amp;random=true&amp;amp;width=435&amp;amp;lines=Fumigating...;Gallivanting...;Matriculating...;Toodleedoodling...;Goo-goo-g'joob-ing...;Excaliburating...;Canoodling...;Doing+the+humpty+dance...;Shiver-me-timbers-ing...;Becoming+sentient...;Opening+the+pod+bay+doors...;Rimraf-ing...;23-skidoo-ing...;Skip-to-my-loo'ing...;High-falutin'...;Disambiguating...;Coagulating...;Undulating...;Just+Clauding+around..." alt="Typing SVG" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;p&gt;&lt;a href="https://awesome.re"&gt;&lt;img src="https://awesome.re/badge-flat2.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt; &lt;a href="https://bailproject.org"&gt;&lt;img src="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/assets/freedom-funder-badge.svg?sanitize=true" alt="FREEDOM FUNDER" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Awesome Claude Code&lt;/h1&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;p&gt;This is a curated list of slash-commands, &lt;code&gt;CLAUDE.md&lt;/code&gt; files, CLI tools, and other resources and guides for enhancing your &lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;Claude Code&lt;/a&gt; workflow, productivity, and vibes.&lt;/p&gt; 
&lt;!--lint enable double-link--&gt; 
&lt;p&gt;Claude Code is a cutting-edge CLI-based coding assistant and agent released by &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt; that you can access in your terminal or IDE. It is a rapidly evolving tool that offers a number of powerful capabilities, and allows for a lot of configuration, in a lot of different ways. Users are actively working out best practices and workflows. It is the hope that this repo will help the community share knowledge and understand how to get the most out of Claude Code.&lt;/p&gt; 
&lt;h3&gt;Announcements &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt; 
&lt;details open&gt; 
 &lt;summary&gt;View Announcements&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; 
   &lt;details open&gt; 
    &lt;summary&gt;2025-10-06 - Awesome Claude Code 2.0&lt;/summary&gt; 
    &lt;ul&gt; 
     &lt;li&gt; 
      &lt;details open&gt; 
       &lt;summary&gt;Change afoot&lt;/summary&gt; 
       &lt;ul&gt; 
        &lt;li&gt;Thank you, again, to Anthropic PBC for paying tribute to hip-hop royalty in their recent advertising campaign. Although they neglected to include his voice in the commercial, I'm sure DOOM would have echoed their joyful sentiment with full enthusiasm.&lt;/li&gt; 
       &lt;/ul&gt; 
      &lt;/details&gt; &lt;/li&gt; 
     &lt;li&gt; 
      &lt;details open&gt; 
       &lt;summary&gt;Fundraising update&lt;/summary&gt; 
       &lt;ul&gt; 
        &lt;li&gt;I've managed at last to secure a dedicated, direct-link, one-click fundraising widget for my campaign to support &lt;a href="https://bailproject.org/"&gt;The Bail Project&lt;/a&gt;. See &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#support"&gt;Support&lt;/a&gt; below for more details, and click &lt;a href="https://donor.bailproject.org/-/NPHKDQGP?member=SELFZPZN"&gt;here&lt;/a&gt; to contribute.&lt;/li&gt; 
       &lt;/ul&gt; 
      &lt;/details&gt; &lt;/li&gt; 
     &lt;li&gt; 
      &lt;details open&gt; 
       &lt;summary&gt;Content update&lt;/summary&gt; 
       &lt;ul&gt; 
        &lt;li&gt;Thanks to the hard work that so many people have devoted to writing code for other users of Claude Code, the current list has reached a very respectable size, with about 150 resource listings. As Claude Code has now reached 2nd grade, I am thinking more about how best to manage the list going forward. First, I'd like to ensure that existing resources are still relevant and compatible with the current version of Claude Code. I think some categories (I'm thinking in particular of usage monitors and bespoke orchestration frameworks) are more or less at capacity. For those wishing to submit to the Awesome List, I would like to strongly encourage exploration of any new or recent features that have been, or will be, rolled out - and to think about how to creatively leverage the existing features of &lt;em&gt;Claude Code&lt;/em&gt;, as opposed to finding new ways to hook up Claude Code to something &lt;em&gt;else&lt;/em&gt;. (For a bit more detail, see &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/DONTREADME.md"&gt;&lt;code&gt;DONTREADME.md&lt;/code&gt;&lt;/a&gt;.) In addition to this, due to personal factors and social ailments such as "rent", I must be mindful of the time I devote to this project, which is non-trivial - and, sadly, because GitHub is not as cool as reddit, I operate at a loss in maintaining this repo. For now, I am temporarily suspending any commitment to maintaining the Issue submissions as a FIFO &lt;em&gt;queue&lt;/em&gt; of items that must be evaluated in a timely fashion. You are absolutely encouraged to submit your projects, and I respect everybody's hard work and will make my best effort to review submissions as they come in. Futher maintenance support may be coming soon, but until then, I will be adding items as I see fit, whether they are from the Issues list or those that I discover on my own. If you want to support more activate maintenance and attention towards reviewing submissions, you may see the &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#support"&gt;Support&lt;/a&gt; section below. Frankly, everyone on the list probably deserves some compensation for helping make Claude Code such a popular and awesome product, but I don't control the purse strings.&lt;/li&gt; 
       &lt;/ul&gt; 
      &lt;/details&gt; &lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;This Week's Additions âœ¨ &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Resources added in the past 7 days&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/vibe-log/vibe-log-cli"&gt;&lt;code&gt;Vibe-Log&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/vibe-log"&gt;Vibe-Log&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Analyzes your Claude Code prompts locally (using CC), provides intelligent session analysis and actionable strategic guidance - works in the statusline and produces very pretty HTML reports as well. Easy to install and remove.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=vibe-log-cli&amp;amp;username=vibe-log&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for vibe-log-cli" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://github.com/tony/claude-code-riper-5"&gt;&lt;code&gt;RIPER Workflow&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://tony.sh"&gt;Tony Narlock&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Structured development workflow enforcing separation between Research, Innovate, Plan, Execute, and Review phases. Features consolidated subagents for context-efficiency, branch-aware memory bank, and strict mode enforcement for guided development.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-riper-5&amp;amp;username=tony&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-riper-5" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://github.com/FlineDev/ContextKit"&gt;&lt;code&gt;ContextKit&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Jeehut"&gt;Cihat GÃ¼ndÃ¼z&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A systematic development framework that transforms Claude Code into a proactive development partner. Features 4-phase planning methodology, specialized quality agents, and structured workflows that help AI produce production-ready code on first try.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ContextKit&amp;amp;username=FlineDev&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ContextKit" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://github.com/hagan/claudia-statusline"&gt;&lt;code&gt;claudia-statusline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hagan"&gt;Hagan Franks&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; High-performance Rust-based statusline for Claude Code with persistent stats tracking, progress bars, and optional cloud sync. Features SQLite-first persistence, git integration, context progress bars, burn rate calculation, XDG-compliant with theme support (dark/light, NO_COLOR).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claudia-statusline&amp;amp;username=hagan&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claudia-statusline" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h1&gt;&lt;strong&gt;Support&lt;/strong&gt;&lt;/h1&gt; 
&lt;p&gt;You can easily offer support by making a charitable donation in honor of Awesome Claude Code by visiting the &lt;a href="https://donor.bailproject.org/-/NPHKDQGP/join?member=SELFZPZN"&gt;fundraising page&lt;/a&gt; I've set up for &lt;a href="https://bailproject.org/"&gt;The Bail Project&lt;/a&gt;. One click will take you straight to the donation widget, and a few seconds later your contribution will be completed - even $1 will truly make an impact. (If you are on GitHub, and you want your donation to be recognized, you will have to take about two more minutes to complete a follow-up email which actually notifies me of the dedication. As a reward you can show one of our nice Freedom Funder badges.) Sadly, this repo cannot be actively maintained without some direct support to this fund. Luckily there are loads of other resources devoted to Claude and his amazing Code, and it's up to you whether you think this one is particularly valuable or not.&lt;/p&gt; 
&lt;a href="https://donor.bailproject.org/-/NPHKDQGP/join?member=SELFZPZN" text-align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/assets/fundraising-link-img.jpg" alt="Awesome Claude Code Freedom Funders" width="200" margin="auto" /&gt; &lt;/a&gt; 
&lt;h2&gt;Contents &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Table of Contents&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; 
   &lt;details open&gt; 
    &lt;summary&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#workflows--knowledge-guides-"&gt;Workflows &amp;amp; Knowledge Guides&lt;/a&gt;&lt;/summary&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general-"&gt;General&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;details open&gt; 
    &lt;summary&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#tooling-"&gt;Tooling&lt;/a&gt;&lt;/summary&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general--1"&gt;General&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ide-integrations-"&gt;IDE Integrations&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#usage-monitors-"&gt;Usage Monitors&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#orchestrators-"&gt;Orchestrators&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;details open&gt; 
    &lt;summary&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#status-lines-"&gt;Status Lines&lt;/a&gt;&lt;/summary&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general--2"&gt;General&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;details open&gt; 
    &lt;summary&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#hooks-"&gt;Hooks&lt;/a&gt;&lt;/summary&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general--3"&gt;General&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;details open&gt; 
    &lt;summary&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#output-styles-"&gt;Output Styles&lt;/a&gt;&lt;/summary&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general--4"&gt;General&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;details open&gt; 
    &lt;summary&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#slash-commands-"&gt;Slash-Commands&lt;/a&gt;&lt;/summary&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general--5"&gt;General&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#version-control--git-"&gt;Version Control &amp;amp; Git&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#code-analysis--testing-"&gt;Code Analysis &amp;amp; Testing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#context-loading--priming-"&gt;Context Loading &amp;amp; Priming&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#documentation--changelogs-"&gt;Documentation &amp;amp; Changelogs&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ci--deployment-"&gt;CI / Deployment&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project--task-management-"&gt;Project &amp;amp; Task Management&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#miscellaneous-"&gt;Miscellaneous&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;details open&gt; 
    &lt;summary&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#claudemd-files-"&gt;CLAUDE.md Files&lt;/a&gt;&lt;/summary&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#language-specific-"&gt;Language-Specific&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#domain-specific-"&gt;Domain-Specific&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project-scaffolding--mcp-"&gt;Project Scaffolding &amp;amp; MCP&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; &lt;/li&gt; 
  &lt;li&gt; 
   &lt;details open&gt; 
    &lt;summary&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#official-documentation-"&gt;Official Documentation&lt;/a&gt;&lt;/summary&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general--6"&gt;General&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/details&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;Workflows &amp;amp; Knowledge Guides ğŸ§  &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A &lt;strong&gt;workflow&lt;/strong&gt; is a tightly coupled set of Claude Code-native resources that facilitate specific projects&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;General &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/ayoubben18/ab-method"&gt;&lt;code&gt;AB Method&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ayoubben18"&gt;Ayoub Bensalah&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A principled, spec-driven workflow that transforms large problems into focused, incremental missions using Claude Code's specialized sub agents. Includes slash-commands, sub agents, and specialized workflows designed for specific parts of the SDLC.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ab-method&amp;amp;username=ayoubben18&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ab-method" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/cloudartisan/cloudartisan.github.io/tree/main/.claude/commands"&gt;&lt;code&gt;Blogging Platform Instructions&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/cloudartisan"&gt;cloudartisan&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;CC-BY-SA-4.0&lt;br /&gt; Provides a well-structured set of commands for publishing and maintaining a blogging platform, including commands for creating posts, managing categories, and handling media files.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=cloudartisan.github.io&amp;amp;username=cloudartisan&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for cloudartisan.github.io" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/automazeio/ccpm"&gt;&lt;code&gt;Claude Code PM&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ranaroussi"&gt;Ran Aroussi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Really comprehensive and feature-packed project-management workflow for Claude Code. Numerous specialized agents, slash-commands, and strong documentation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ccpm&amp;amp;username=automazeio&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ccpm" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://claudelog.com"&gt;&lt;code&gt;ClaudeLog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://www.reddit.com/user/inventor_black/"&gt;InventorBlack&lt;/a&gt;&lt;br /&gt; A comprehensive knowledge base with detailed breakdowns of advanced &lt;a href="https://claudelog.com/mechanics/you-are-the-main-thread/"&gt;mechanics&lt;/a&gt; including &lt;a href="https://claudelog.com/mechanics/claude-md-supremacy"&gt;CLAUDE.md best practices&lt;/a&gt;, practical technique guides like &lt;a href="https://claudelog.com/mechanics/plan-mode"&gt;plan mode&lt;/a&gt;, &lt;a href="https://claudelog.com/faqs/what-is-ultrathink/"&gt;ultrathink&lt;/a&gt;, &lt;a href="https://claudelog.com/mechanics/task-agent-tools/"&gt;sub-agents&lt;/a&gt;, &lt;a href="https://claudelog.com/mechanics/agent-first-design/"&gt;agent-first design&lt;/a&gt; and &lt;a href="https://claudelog.com/configuration"&gt;configuration guides&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/JSONbored/claudepro-directory"&gt;&lt;code&gt;ClaudoPro Directory&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/JSONbored"&gt;ghost&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Well-crafted, wide selection of Claude Code hooks, slash commands, subagent files, and more, covering a range of specialized tasks and workflows. Better resources than your average "Claude-template-for-everything" site.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claudepro-directory&amp;amp;username=JSONbored&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claudepro-directory" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/tree/main/.claude/commands"&gt;&lt;code&gt;Context Priming&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br /&gt; Provides a systematic approach to priming Claude Code with comprehensive project context through specialized commands for different project scenarios and development contexts.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=just-prompt&amp;amp;username=disler&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for just-prompt" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/OneRedOak/claude-code-workflows/tree/main/design-review"&gt;&lt;code&gt;Design Review Workflow&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/OneRedOak"&gt;Patrick Ellis&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A tailored workflow for enabling automated UI/UX design review, including specialized sub agents, slash commands, &lt;code&gt;CLAUDE.md&lt;/code&gt; excerpts, and more. Covers a broad range of criteria from responsive design to accessibility.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-workflows&amp;amp;username=OneRedOak&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-workflows" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/tott/laravel-tall-claude-ai-configs"&gt;&lt;code&gt;Laravel TALL Stack AI Development Starter Kit&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/tott"&gt;tott&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Transform your Laravel TALL (Tailwind, AlpineJS, Laravel, Livewire) stack development with comprehensive Claude Code configurations that provide intelligent assistance, systematic workflows, and domain expert consultation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=laravel-tall-claude-ai-configs&amp;amp;username=tott&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for laravel-tall-claude-ai-configs" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/tree/main/.claude/commands"&gt;&lt;code&gt;n8n_agent&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br /&gt; Amazing comprehensive set of comments for code analysis, QA, design, documentation, project structure, project management, optimization, and many more.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=n8n_agent&amp;amp;username=kingler&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for n8n_agent" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/tree/main/.claude/commands"&gt;&lt;code&gt;Project Bootstrapping and Task Management&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br /&gt; Provides a structured set of commands for bootstrapping and managing a new project, including meta-commands for creating and editing custom slash-commands.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=steadystart&amp;amp;username=steadycursor&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for steadystart" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/scopecraft/command/tree/main/.claude/commands"&gt;&lt;code&gt;Project Management, Implementation, Planning, and Release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br /&gt; Really comprehensive set of commands for all aspects of SDLC.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=command&amp;amp;username=scopecraft&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for command" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/harperreed/dotfiles/tree/master/.claude/commands"&gt;&lt;code&gt;Project Workflow System&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/harperreed"&gt;harperreed&lt;/a&gt;&lt;br /&gt; A set of commands that provide a comprehensive workflow system for managing projects, including task management, code review, and deployment processes.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=dotfiles&amp;amp;username=harperreed&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for dotfiles" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/tony/claude-code-riper-5"&gt;&lt;code&gt;RIPER Workflow&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://tony.sh"&gt;Tony Narlock&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Structured development workflow enforcing separation between Research, Innovate, Plan, Execute, and Review phases. Features consolidated subagents for context-efficiency, branch-aware memory bank, and strict mode enforcement for guided development.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-riper-5&amp;amp;username=tony&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-riper-5" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://diwank.space/field-notes-from-shipping-real-code-with-claude"&gt;&lt;code&gt;Shipping Real Code w/ Claude&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/creatorrr"&gt;Diwank&lt;/a&gt;&lt;br /&gt; A detailed blog post explaining the author's process for shipping a product with Claude Code, including CLAUDE.md files and other interesting resources.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Helmi/claude-simone"&gt;&lt;code&gt;Simone&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Helmi"&gt;Helmi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A broader project management workflow for Claude Code that encompasses not just a set of commands, but a system of documents, guidelines, and processes to facilitate project planning and execution.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-simone&amp;amp;username=Helmi&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-simone" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;Tooling ğŸ§° &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tooling&lt;/strong&gt; denotes applications that are built on top of Claude Code and consist of more components than slash-commands and &lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;General &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Veraticus/cc-tools"&gt;&lt;code&gt;cc-tools&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Veraticus"&gt;Josh Symonds&lt;/a&gt;&lt;br /&gt; High-performance Go implementation of Claude Code hooks and utilities. Provides smart linting, testing, and statusline generation with minimal overhead.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=cc-tools&amp;amp;username=Veraticus&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for cc-tools" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/nyatinte/ccexp"&gt;&lt;code&gt;ccexp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nyatinte"&gt;nyatinte&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Interactive CLI tool for discovering and managing Claude Code configuration files and slash commands with a beautiful terminal UI.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ccexp&amp;amp;username=nyatinte&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ccexp" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/eckardt/cchistory"&gt;&lt;code&gt;cchistory&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eckardt"&gt;eckardt&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Like the shell history command but for your Claude Code sessions. Easily list all Bash or "Bash-mode" (&lt;code&gt;!&lt;/code&gt;) commands Claude Code ran in a session for reference.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=cchistory&amp;amp;username=eckardt&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for cchistory" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Brads3290/cclogviewer"&gt;&lt;code&gt;cclogviewer&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Brads3290"&gt;Brad S.&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A humble but handy utility for viewing Claude Code &lt;code&gt;.jsonl&lt;/code&gt; conversation files in a pretty HTML UI.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=cclogviewer&amp;amp;username=Brads3290&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for cclogviewer" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/davila7/claude-code-templates"&gt;&lt;code&gt;Claude Code Templates&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/davila7"&gt;Daniel Avila&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Incredibly awesome collection of resources from every category in this list, presented with a neatly polished UI, great features like usage dashboard, analytics, and everything from slash commands to hooks to agents. An awesome companion for this awesome list.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-templates&amp;amp;username=davila7&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-templates" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/possibilities/claude-composer"&gt;&lt;code&gt;Claude Composer&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/possibilities"&gt;Mike Bannister&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Unlicense&lt;br /&gt; A tool that adds small enhancements to Claude Code.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-composer&amp;amp;username=possibilities&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-composer" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/claude-did-this/claude-hub"&gt;&lt;code&gt;Claude Hub&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/claude-did-this"&gt;Claude Did This&lt;/a&gt;&lt;br /&gt; A webhook service that connects Claude Code to GitHub repositories, enabling AI-powered code assistance directly through pull requests and issues. This integration allows Claude to analyze repositories, answer technical questions, and help developers understand and improve their codebase through simple @mentions.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-hub&amp;amp;username=claude-did-this&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-hub" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/pchalasani/claude-code-tools"&gt;&lt;code&gt;claude-code-tools&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/pchalasani"&gt;Prasad Chalasani&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A collection of awesome tools, including tmux integrations, better session management, hooks that enhance security - a really well-done set of Claude Code enhancers, especially for tmux users.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-tools&amp;amp;username=pchalasani&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-tools" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/carlrannaberg/claudekit"&gt;&lt;code&gt;claudekit&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/carlrannaberg"&gt;Carl Rannaberg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Impressive CLI toolkit providing auto-save checkpointing, code quality hooks, specification generation and execution, and 20+ specialized subagents including oracle (gpt-5), code-reviewer (6-aspect deep analysis), ai-sdk-expert (Vercel AI SDK), typescript-expert and many more for Claude Code workflows.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claudekit&amp;amp;username=carlrannaberg&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claudekit" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/dagger/container-use"&gt;&lt;code&gt;Container Use&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/dagger"&gt;dagger&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Development environments for coding agents. Enable multiple agents to work safely and independently with your preferred stack.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=container-use&amp;amp;username=dagger&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for container-use" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/FlineDev/ContextKit"&gt;&lt;code&gt;ContextKit&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Jeehut"&gt;Cihat GÃ¼ndÃ¼z&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A systematic development framework that transforms Claude Code into a proactive development partner. Features 4-phase planning methodology, specialized quality agents, and structured workflows that help AI produce production-ready code on first try.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ContextKit&amp;amp;username=FlineDev&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ContextKit" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/icanhasjonas/run-claude-docker"&gt;&lt;code&gt;run-claude-docker&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/icanhasjonas/"&gt;Jonas&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A self-contained Docker runner that forwards your current workspace into a safe(r) isolated docker container, where you still have access to your Claude Code settings, authentication, ssh agent, pgp, optionally aws keys etc.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=run-claude-docker&amp;amp;username=icanhasjonas&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for run-claude-docker" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/marcindulak/stt-mcp-server-linux"&gt;&lt;code&gt;stt-mcp-server-linux&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/marcindulak"&gt;marcindulak&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; A push-to-talk speech transcription setup for Linux using a Python MCP server. Runs locally in Docker with no external API calls. Your speech is recorded, transcribed into text, and then sent to Claude running in a Tmux session.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=stt-mcp-server-linux&amp;amp;username=marcindulak&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for stt-mcp-server-linux" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/SuperClaude-Org/SuperClaude_Framework"&gt;&lt;code&gt;SuperClaude&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/SuperClaude-Org"&gt;SuperClaude-Org&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A versatile configuration framework that enhances Claude Code with specialized commands, cognitive personas, and development methodologies, such as "Introspection" and "Orchestration".&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=SuperClaude_Framework&amp;amp;username=SuperClaude-Org&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for SuperClaude_Framework" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Piebald-AI/tweakcc"&gt;&lt;code&gt;tweakcc&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Piebald-AI"&gt;Piebald-AI&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Command-line tool to customize your Claude Code styling.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=tweakcc&amp;amp;username=Piebald-AI&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for tweakcc" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/vibe-log/vibe-log-cli"&gt;&lt;code&gt;Vibe-Log&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/vibe-log"&gt;Vibe-Log&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Analyzes your Claude Code prompts locally (using CC), provides intelligent session analysis and actionable strategic guidance - works in the statusline and produces very pretty HTML reports as well. Easy to install and remove.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=vibe-log-cli&amp;amp;username=vibe-log&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for vibe-log-cli" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;IDE Integrations &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=AndrePimenta.claude-code-chat"&gt;&lt;code&gt;Claude Code Chat&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/andrepimenta"&gt;andrepimenta&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Â©&lt;br /&gt; An elegant and user-friendly Claude Code chat interface for VS Code.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/manzaltu/claude-code-ide.el"&gt;&lt;code&gt;claude-code-ide.el&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/manzaltu"&gt;manzaltu&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br /&gt; claude-code-ide.el integrates Claude Code with Emacs, like Anthropicâ€™s VS Code/IntelliJ extensions. It shows ediff-based code suggestions, pulls LSP/flymake/flycheck diagnostics, and tracks buffer context. It adds an extensible MCP tool support for symbol refs/defs, project metadata, and tree-sitter AST queries.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-ide.el&amp;amp;username=manzaltu&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-ide.el" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/stevemolitor/claude-code.el"&gt;&lt;code&gt;claude-code.el&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stevemolitor"&gt;stevemolitor&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; An Emacs interface for Claude Code CLI.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code.el&amp;amp;username=stevemolitor&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code.el" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/greggh/claude-code.nvim"&gt;&lt;code&gt;claude-code.nvim&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/greggh"&gt;greggh&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A seamless integration between Claude Code AI assistant and Neovim.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code.nvim&amp;amp;username=greggh&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code.nvim" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/stravu/crystal"&gt;&lt;code&gt;crystal&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stravu"&gt;stravu&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A full-fledged desktop application for orchestrating, monitoring, and interacting with Claude Code agents.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=crystal&amp;amp;username=stravu&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for crystal" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Usage Monitors &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/ryoppippi/ccusage"&gt;&lt;code&gt;CC Usage&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ryoppippi"&gt;ryoppippi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Handy CLI tool for managing and analyzing Claude Code usage, based on analyzing local Claude Code logs. Presents a nice dashboard regarding cost information, token consumption, etc.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ccusage&amp;amp;username=ryoppippi&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ccusage" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/snipeship/ccflare"&gt;&lt;code&gt;ccflare&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/snipeship"&gt;snipeship&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Claude Code usage dashboard with a web-UI that would put Tableau to shame. Thoroughly comprehensive metrics, frictionless setup, detailed logging, really really nice UI.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ccflare&amp;amp;username=snipeship&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ccflare" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor"&gt;&lt;code&gt;Claude Code Usage Monitor&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Maciek-roboblog"&gt;Maciek-roboblog&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A real-time terminal-based tool for monitoring Claude Code token usage. It shows live token consumption, burn rate, and predictions for token depletion. Features include visual progress bars, session-aware analytics, and support for multiple subscription plans.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=Claude-Code-Usage-Monitor&amp;amp;username=Maciek-roboblog&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for Claude-Code-Usage-Monitor" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sculptdotfun/viberank"&gt;&lt;code&gt;viberank&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nikshepsvn"&gt;nikshepsvn&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A community-driven leaderboard tool that enables developers to visualize, track, and compete based on their Claude Code usage statistics. It features robust data analytics, GitHub OAuth, data validation, and user-friendly CLI/web submission methods.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=viberank&amp;amp;username=sculptdotfun&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for viberank" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Orchestrators &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/ruvnet/claude-code-flow"&gt;&lt;code&gt;Claude Code Flow&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ruvnet"&gt;ruvnet&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; This mode serves as a code-first orchestration layer, enabling Claude to write, edit, test, and optimize code autonomously across recursive agent cycles.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-flow&amp;amp;username=ruvnet&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-flow" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/smtg-ai/claude-squad"&gt;&lt;code&gt;Claude Squad&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/smtg-ai"&gt;smtg-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br /&gt; Claude Squad is a terminal app that manages multiple Claude Code, Codex (and other local agents including Aider) in separate workspaces, allowing you to work on multiple tasks simultaneously.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-squad&amp;amp;username=smtg-ai&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-squad" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/parruda/claude-swarm"&gt;&lt;code&gt;Claude Swarm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/parruda"&gt;parruda&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Launch Claude Code session that is connected to a swarm of Claude Code Agents.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-swarm&amp;amp;username=parruda&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-swarm" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/eyaltoledano/claude-task-master"&gt;&lt;code&gt;Claude Task Master&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eyaltoledano"&gt;eyaltoledano&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; A task management system for AI-driven development with Claude, designed to work seamlessly with Cursor AI.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-task-master&amp;amp;username=eyaltoledano&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-task-master" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-task-runner"&gt;&lt;code&gt;Claude Task Runner&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt;&lt;br /&gt; A specialized tool to manage context isolation and focused task execution with Claude Code, solving the critical challenge of context length limitations and task focus when working with Claude on complex, multi-step projects.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-task-runner&amp;amp;username=grahama1970&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-task-runner" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/slopus/happy"&gt;&lt;code&gt;Happy Coder&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://peoplesgrocers.com/en/projects"&gt;GrocerPublishAgent&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Spawn and control multiple Claude Codes in parallel from your phone or desktop. Happy Coder runs Claude Code on your hardware, sends push notifications when Claude needs more input or permission, and costs nothing.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=happy&amp;amp;username=slopus&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for happy" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/rsmdt/the-startup"&gt;&lt;code&gt;The Agentic Startup&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rsmdt"&gt;Rudolf Schmidt&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Yet Another Claude Orchestrator - a collection of agents, commands, etc., for shipping production code - but I like this because it's comprehensive, well-written, and one of the few resources that actually uses Output Styles! +10 points!&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=the-startup&amp;amp;username=rsmdt&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for the-startup" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/dtormoen/tsk"&gt;&lt;code&gt;TSK - AI Agent Task Manager and Sandbox&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/dtormoen"&gt;dtormoen&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A Rust CLI tool that lets you delegate development tasks to AI agents running in sandboxed Docker environments. Multiple agents work in parallel, returning git branches for human review.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=tsk&amp;amp;username=dtormoen&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for tsk" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;Status Lines ğŸ“Š &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Status lines&lt;/strong&gt; - Configurations and customizations for Claude Code's status bar functionality&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;General &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sirmalloc/ccstatusline"&gt;&lt;code&gt;ccstatusline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/sirmalloc"&gt;sirmalloc&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A highly customizable status line formatter for Claude Code CLI that displays model info, git branch, token usage, and other metrics in your terminal.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ccstatusline&amp;amp;username=sirmalloc&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ccstatusline" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/rz1989s/claude-code-statusline"&gt;&lt;code&gt;claude-code-statusline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rz1989s"&gt;rz1989s&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Enhanced 4-line statusline for Claude Code with themes, cost tracking, and MCP server monitoring&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-statusline&amp;amp;username=rz1989s&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-statusline" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Owloops/claude-powerline"&gt;&lt;code&gt;claude-powerline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Owloops"&gt;Owloops&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A vim-style powerline statusline for Claude Code with real-time usage tracking, git integration, custom themes, and more&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-powerline&amp;amp;username=Owloops&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-powerline" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/hagan/claudia-statusline"&gt;&lt;code&gt;claudia-statusline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hagan"&gt;Hagan Franks&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; High-performance Rust-based statusline for Claude Code with persistent stats tracking, progress bars, and optional cloud sync. Features SQLite-first persistence, git integration, context progress bars, burn rate calculation, XDG-compliant with theme support (dark/light, NO_COLOR).&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claudia-statusline&amp;amp;username=hagan&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claudia-statusline" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;Hooks ğŸª &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Hooks&lt;/strong&gt; are a powerful API for Claude Code that allows users to activate commands and run scripts at different points in Claude's agentic lifecycle.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;General &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/dazuiba/CCNotify"&gt;&lt;code&gt;CC Notify&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/dazuiba"&gt;dazuiba&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; CCNotify provides desktop notifications for Claude Code, alerting you to input needs or task completion, with one-click jumps back to VS Code and task duration display.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=CCNotify&amp;amp;username=dazuiba&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for CCNotify" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/GowayLee/cchooks"&gt;&lt;code&gt;cchooks&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/GowayLee"&gt;GowayLee&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A lightweight Python SDK with a clean API and good documentation; simplifies the process of writing hooks and integrating them into your codebase, providing a nice abstraction over the JSON configuration files.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=cchooks&amp;amp;username=GowayLee&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for cchooks" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/beyondcode/claude-hooks-sdk"&gt;&lt;code&gt;claude-code-hooks-sdk&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/beyondcode"&gt;beyondcode&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A Laravel-inspired PHP SDK for building Claude Code hook responses with a clean, fluent API. This SDK makes it easy to create structured JSON responses for Claude Code hooks using an expressive, chainable interface.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-hooks-sdk&amp;amp;username=beyondcode&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-hooks-sdk" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/johnlindquist/claude-hooks"&gt;&lt;code&gt;claude-hooks&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/johnlindquist"&gt;John Lindquist&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A TypeScript-based system for configuring and customizing Claude Code hooks with a powerful and flexible interface.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-hooks&amp;amp;username=johnlindquist&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-hooks" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/ctoth/claudio"&gt;&lt;code&gt;Claudio&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ctoth"&gt;Christopher Toth&lt;/a&gt;&lt;br /&gt; A no-frills little library that adds delightful OS-native sounds to Claude Code via simple hooks. It really sparks joy.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claudio&amp;amp;username=ctoth&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claudio" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/nizos/tdd-guard"&gt;&lt;code&gt;TDD Guard&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nizos"&gt;Nizar Selander&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A hooks-driven system that monitors file operations in real-time and blocks changes that violate TDD principles.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=tdd-guard&amp;amp;username=nizos&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for tdd-guard" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/bartolli/claude-code-typescript-hooks"&gt;&lt;code&gt;TypeScript Quality Hooks&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/bartolli"&gt;bartolli&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Quality check hook for Node.js TypeScript projects with TypeScript compilation. ESLint auto-fixing, and Prettier formatting. Uses SHA256 config caching for &amp;lt;5ms validation performance during real-time editing.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-typescript-hooks&amp;amp;username=bartolli&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-typescript-hooks" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;Output Styles ğŸ’¬ &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Output styles&lt;/strong&gt; allow you to use Claude Code as any type of agent while keeping its core capabilities, such as running local scripts, reading/writing files, and tracking TODOs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;General &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/viveknair/ccoutputstyles"&gt;&lt;code&gt;ccoutputstyles&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/viveknair"&gt;Vivek Nair&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; CLI tool and template gallery for customizing Claude Code output styles with pre-built templates. Features over 15 templates at the time of writing!&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ccoutputstyles&amp;amp;username=viveknair&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ccoutputstyles" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;Slash-Commands ğŸ”ª &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;General &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/omril321/automated-notebooklm/raw/main/.claude/commands/create-hook.md"&gt;&lt;code&gt;/create-hook&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/omril321"&gt;Omri Lavi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Slash command for hook creation - intelligently prompts you through the creation process with smart suggestions based on your project setup (TS, Prettier, ESLint...).&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=automated-notebooklm&amp;amp;username=omril321&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for automated-notebooklm" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Version Control &amp;amp; Git &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/raw/feature/issue-227-ai-suggestions/.claude/commands/analyze-issue.md"&gt;&lt;code&gt;/analyze-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Fetches GitHub issue details to create comprehensive implementation specifications, analyzing requirements and planning structured approach with clear implementation steps.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=Narraitor&amp;amp;username=jerseycheese&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for Narraitor" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/commit.md"&gt;&lt;code&gt;/commit&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Creates git commits using conventional commit format with appropriate emojis, following project standards and creating descriptive messages that explain the purpose of changes.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=tevm-monorepo&amp;amp;username=evmts&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for tevm-monorepo" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/.claude/commands/2-commit-fast.md"&gt;&lt;code&gt;/commit-fast&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br /&gt; Automates git commit process by selecting the first suggested message, generating structured commits with consistent formatting while skipping manual confirmation and removing Claude co-Contributorship footer&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=steadystart&amp;amp;username=steadycursor&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for steadystart" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/toyamarinyon/giselle/raw/main/.claude/commands/create-pr.md"&gt;&lt;code&gt;/create-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/toyamarinyon"&gt;toyamarinyon&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Streamlines pull request creation by handling the entire workflow: creating a new branch, committing changes, formatting modified files with Biome, and submitting the PR.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=giselle&amp;amp;username=toyamarinyon&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for giselle" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/liam-hq/liam/raw/main/.claude/commands/create-pull-request.md"&gt;&lt;code&gt;/create-pull-request&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/liam-hq"&gt;liam-hq&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Provides comprehensive PR creation guidance with GitHub CLI, enforcing title conventions, following template structure, and offering concrete command examples with best practices.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=liam&amp;amp;username=liam-hq&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for liam" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/create-worktrees.md"&gt;&lt;code&gt;/create-worktrees&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Creates git worktrees for all open PRs or specific branches, handling branches with slashes, cleaning up stale worktrees, and supporting custom branch creation for development.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=tevm-monorepo&amp;amp;username=evmts&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for tevm-monorepo" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/jeremymailen/kotlinter-gradle/raw/master/.claude/commands/fix-github-issue.md"&gt;&lt;code&gt;/fix-github-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jeremymailen"&gt;jeremymailen&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Analyzes and fixes GitHub issues using a structured approach with GitHub CLI for issue details, implementing necessary code changes, running tests, and creating proper commit messages.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=kotlinter-gradle&amp;amp;username=jeremymailen&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for kotlinter-gradle" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-issue.md"&gt;&lt;code&gt;/fix-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Addresses GitHub issues by taking issue number as parameter, analyzing context, implementing solution, and testing/validating the fix for proper integration.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=metabase&amp;amp;username=metabase&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for metabase" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-pr.md"&gt;&lt;code&gt;/fix-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Fetches and fixes unresolved PR comments by automatically retrieving feedback, addressing reviewer concerns, making targeted code improvements, and streamlining the review process.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=metabase&amp;amp;username=metabase&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for metabase" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/husky.md"&gt;&lt;code&gt;/husky&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Sets up and manages Husky Git hooks by configuring pre-commit hooks, establishing commit message standards, integrating with linting tools, and ensuring code quality on commits.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=tevm-monorepo&amp;amp;username=evmts&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for tevm-monorepo" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/arkavo-org/opentdf-rs/raw/main/.claude/commands/pr-review.md"&gt;&lt;code&gt;/pr-review&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/arkavo-org"&gt;arkavo-org&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Reviews pull request changes to provide feedback, check for issues, and suggest improvements before merging into the main codebase.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=opentdf-rs&amp;amp;username=arkavo-org&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for opentdf-rs" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/.claude/commands/update-branch-name.md"&gt;&lt;code&gt;/update-branch-name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Updates branch names with proper prefixes and formats, enforcing naming conventions, supporting semantic prefixes, and managing remote branch updates.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=giselle&amp;amp;username=giselles-ai&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for giselle" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Code Analysis &amp;amp; Testing &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/rygwdn/slack-tools/raw/main/.claude/commands/check.md"&gt;&lt;code&gt;/check&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rygwdn"&gt;rygwdn&lt;/a&gt;&lt;br /&gt; Performs comprehensive code quality and security checks, featuring static analysis integration, security vulnerability scanning, code style enforcement, and detailed reporting.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=slack-tools&amp;amp;username=rygwdn&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for slack-tools" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Graphlet-AI/eridu/raw/main/.claude/commands/clean.md"&gt;&lt;code&gt;/clean&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Graphlet-AI"&gt;Graphlet-AI&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Addresses code formatting and quality issues by fixing black formatting problems, organizing imports with isort, resolving flake8 linting issues, and correcting mypy type errors.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=eridu&amp;amp;username=Graphlet-AI&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for eridu" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/raw/main/.claude/commands/code_analysis.md"&gt;&lt;code&gt;/code_analysis&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br /&gt; Provides a menu of advanced code analysis commands for deep inspection, including knowledge graph generation, optimization suggestions, and quality evaluation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=n8n_agent&amp;amp;username=kingler&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for n8n_agent" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/to4iki/ai-project-rules/raw/main/.claude/commands/optimize.md"&gt;&lt;code&gt;/optimize&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/to4iki"&gt;to4iki&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Analyzes code performance to identify bottlenecks, proposing concrete optimizations with implementation guidance for improved application performance.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ai-project-rules&amp;amp;username=to4iki&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ai-project-rules" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/rzykov/metabase/raw/master/.claude/commands/repro-issue.md"&gt;&lt;code&gt;/repro-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rzykov"&gt;rzykov&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Creates reproducible test cases for GitHub issues, ensuring tests fail reliably and documenting clear reproduction steps for developers.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=metabase&amp;amp;username=rzykov&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for metabase" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/zscott/pane/raw/main/.claude/commands/tdd.md"&gt;&lt;code&gt;/tdd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zscott"&gt;zscott&lt;/a&gt;&lt;br /&gt; Guides development using Test-Driven Development principles, enforcing Red-Green-Refactor discipline, integrating with git workflow, and managing PR creation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=pane&amp;amp;username=zscott&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for pane" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/raw/feature/issue-227-ai-suggestions/.claude/commands/tdd-implement.md"&gt;&lt;code&gt;/tdd-implement&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Implements Test-Driven Development by analyzing feature requirements, creating tests first (red), implementing minimal passing code (green), and refactoring while maintaining tests.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=Narraitor&amp;amp;username=jerseycheese&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for Narraitor" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Context Loading &amp;amp; Priming &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/elizaOS/elizaos.github.io/raw/main/.claude/commands/context-prime.md"&gt;&lt;code&gt;/context-prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/elizaOS"&gt;elizaOS&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Primes Claude with comprehensive project understanding by loading repository structure, setting development context, establishing project goals, and defining collaboration parameters.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=elizaos.github.io&amp;amp;username=elizaOS&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for elizaos.github.io" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/okuvshynov/cubestat/raw/main/.claude/commands/initref.md"&gt;&lt;code&gt;/initref&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/okuvshynov"&gt;okuvshynov&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Initializes reference documentation structure with standard doc templates, API reference setup, documentation conventions, and placeholder content generation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=cubestat&amp;amp;username=okuvshynov&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for cubestat" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/ethpandaops/xatu-data/raw/master/.claude/commands/load-llms-txt.md"&gt;&lt;code&gt;/load-llms-txt&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ethpandaops"&gt;ethpandaops&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Loads LLM configuration files to context, importing specific terminology, model configurations, and establishing baseline terminology for AI discussions.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=xatu-data&amp;amp;username=ethpandaops&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for xatu-data" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_coo_context.md"&gt;&lt;code&gt;/load_coo_context&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br /&gt; References specific files for sparse matrix operations, explains transform usage, compares with previous approaches, and sets data formatting context for development.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=torchcell&amp;amp;username=Mjvolk3&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for torchcell" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_dango_pipeline.md"&gt;&lt;code&gt;/load_dango_pipeline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br /&gt; Sets context for model training by referencing pipeline files, establishing working context, and preparing for pipeline work with relevant documentation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=torchcell&amp;amp;username=Mjvolk3&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for torchcell" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/yzyydev/AI-Engineering-Structure/raw/main/.claude/commands/prime.md"&gt;&lt;code&gt;/prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/yzyydev"&gt;yzyydev&lt;/a&gt;&lt;br /&gt; Sets up initial project context by viewing directory structure and reading key files, creating standardized context with directory visualization and key documentation focus.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=AI-Engineering-Structure&amp;amp;username=yzyydev&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for AI-Engineering-Structure" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/ddisisto/si/raw/main/.claude/commands/rsi.md"&gt;&lt;code&gt;/rsi&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ddisisto"&gt;ddisisto&lt;/a&gt;&lt;br /&gt; Reads all commands and key project files to optimize AI-assisted development by streamlining the process, loading command context, and setting up for better development workflow.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=si&amp;amp;username=ddisisto&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for si" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Documentation &amp;amp; Changelogs &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/berrydev-ai/blockdoc-python/raw/main/.claude/commands/add-to-changelog.md"&gt;&lt;code&gt;/add-to-changelog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/berrydev-ai"&gt;berrydev-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Adds new entries to changelog files while maintaining format consistency, properly documenting changes, and following established project standards for version tracking.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=blockdoc-python&amp;amp;username=berrydev-ai&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for blockdoc-python" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/raw/feature/issue-227-ai-suggestions/.claude/commands/create-docs.md"&gt;&lt;code&gt;/create-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Analyzes code structure and purpose to create comprehensive documentation detailing inputs/outputs, behavior, user interaction flows, and edge cases with error handling.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=Narraitor&amp;amp;username=jerseycheese&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for Narraitor" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/slunsford/coffee-analytics/raw/main/.claude/commands/docs.md"&gt;&lt;code&gt;/docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/slunsford"&gt;slunsford&lt;/a&gt;&lt;br /&gt; Generates comprehensive documentation that follows project structure, documenting APIs and usage patterns with consistent formatting for better user understanding.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=coffee-analytics&amp;amp;username=slunsford&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for coffee-analytics" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/explain-issue-fix.md"&gt;&lt;code&gt;/explain-issue-fix&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br /&gt; Documents solution approaches for GitHub issues, explaining technical decisions, detailing challenges overcome, and providing implementation context for better understanding.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=toban-contribution-viewer&amp;amp;username=hackdays-io&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for toban-contribution-viewer" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Consiliency/Flutter-Structurizr/raw/main/.claude/commands/update-docs.md"&gt;&lt;code&gt;/update-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Consiliency"&gt;Consiliency&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Reviews current documentation status, updates implementation progress, reviews phase documents, and maintains documentation consistency across the project.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=Flutter-Structurizr&amp;amp;username=Consiliency&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for Flutter-Structurizr" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;CI / Deployment &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/kelp/webdown/raw/main/.claude/commands/release.md"&gt;&lt;code&gt;/release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kelp"&gt;kelp&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Manages software releases by updating changelogs, reviewing README changes, evaluating version increments, and documenting release changes for better version tracking.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=webdown&amp;amp;username=kelp&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for webdown" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/run-ci.md"&gt;&lt;code&gt;/run-ci&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br /&gt; Activates virtual environments, runs CI-compatible check scripts, iteratively fixes errors, and ensures all tests pass before completion.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=toban-contribution-viewer&amp;amp;username=hackdays-io&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for toban-contribution-viewer" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Project &amp;amp; Task Management &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/scopecraft/command/raw/main/.claude/commands/create-command.md"&gt;&lt;code&gt;/create-command&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br /&gt; Guides Claude through creating new custom commands with proper structure by analyzing requirements, templating commands by category, enforcing command standards, and creating supporting documentation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=command&amp;amp;username=scopecraft&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for command" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-jtbd.md"&gt;&lt;code&gt;/create-jtbd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br /&gt; Creates Jobs-to-be-Done frameworks that outline user needs with structured format, focusing on specific user problems and organizing by job categories for product development.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=inkverse&amp;amp;username=taddyorg&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for inkverse" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-prd.md"&gt;&lt;code&gt;/create-prd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br /&gt; Generates comprehensive product requirement documents outlining detailed specifications, requirements, and features following standardized document structure and format.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=inkverse&amp;amp;username=taddyorg&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for inkverse" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Wirasm/claudecode-utils/raw/main/.claude/commands/create-prp.md"&gt;&lt;code&gt;/create-prp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Wirasm"&gt;Wirasm&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Creates product requirement plans by reading PRP methodology, following template structure, creating comprehensive requirements, and structuring product definitions for development.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claudecode-utils&amp;amp;username=Wirasm&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claudecode-utils" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/raw/feature/issue-227-ai-suggestions/.claude/commands/do-issue.md"&gt;&lt;code&gt;/do-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Implements GitHub issues with manual review points, following a structured approach with issue number parameter and offering alternative automated mode for efficiency.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=Narraitor&amp;amp;username=jerseycheese&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for Narraitor" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/raw/main/.claude/commands/project_hello_w_name.md"&gt;&lt;code&gt;/project_hello_w_name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br /&gt; Creates customizable greeting components with name input, demonstrating argument passing, component reusability, state management, and user input handling.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=just-prompt&amp;amp;username=disler&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for just-prompt" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/chrisleyva/todo-slash-command/raw/main/todo.md"&gt;&lt;code&gt;/todo&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/chrisleyva"&gt;chrisleyva&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A convenient command to quickly manage project todo items without leaving the Claude Code interface, featuring due dates, sorting, task prioritization, and comprehensive todo list management.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=todo-slash-command&amp;amp;username=chrisleyva&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for todo-slash-command" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Miscellaneous &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TuckerTucker/tkr-portfolio/raw/main/.claude/commands/five.md"&gt;&lt;code&gt;/five&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/TuckerTucker"&gt;TuckerTucker&lt;/a&gt;&lt;br /&gt; Applies the "five whys" methodology to perform root cause analysis, identify underlying issues, and create solution approaches for complex problems.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=tkr-portfolio&amp;amp;username=TuckerTucker&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for tkr-portfolio" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/fixing_go_in_graph.md"&gt;&lt;code&gt;/fixing_go_in_graph&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br /&gt; Focuses on Gene Ontology annotation integration in graph databases, handling multiple data sources, addressing graph representation issues, and ensuring correct data incorporation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=torchcell&amp;amp;username=Mjvolk3&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for torchcell" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/GaloyMoney/lana-bank/raw/main/.claude/commands/mermaid.md"&gt;&lt;code&gt;/mermaid&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/GaloyMoney"&gt;GaloyMoney&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Generates Mermaid diagrams from SQL schema files, creating entity relationship diagrams with table properties, validating diagram compilation, and ensuring complete entity coverage.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=lana-bank&amp;amp;username=GaloyMoney&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for lana-bank" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/review_dcell_model.md"&gt;&lt;code&gt;/review_dcell_model&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br /&gt; Reviews old Dcell implementation files, comparing with newer Dango model, noting changes over time, and analyzing refactoring approaches for better code organization.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=torchcell&amp;amp;username=Mjvolk3&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for torchcell" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/zuplo/docs/raw/main/.claude/commands/use-stepper.md"&gt;&lt;code&gt;/use-stepper&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zuplo"&gt;zuplo&lt;/a&gt;&lt;br /&gt; Reformats documentation to use React Stepper component, transforming heading formats, applying proper indentation, and maintaining markdown compatibility with admonition formatting.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=docs&amp;amp;username=zuplo&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for docs" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;CLAUDE.md Files ğŸ“‚ &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/strong&gt; are files that contain important guidelines and context-specific information or instructions that help Claude Code to better understand your project and your coding standards&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Language-Specific &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/didalgolab/ai-intellij-plugin/raw/main/CLAUDE.md"&gt;&lt;code&gt;AI IntelliJ Plugin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/didalgolab"&gt;didalgolab&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Provides comprehensive Gradle commands for IntelliJ plugin development with platform-specific coding patterns, detailed package structure guidelines, and clear internationalization standards.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ai-intellij-plugin&amp;amp;username=didalgolab&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for ai-intellij-plugin" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/alexei-led/aws-mcp-server/raw/main/CLAUDE.md"&gt;&lt;code&gt;AWS MCP Server&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/alexei-led"&gt;alexei-led&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Features multiple Python environment setup options with detailed code style guidelines, comprehensive error handling recommendations, and security considerations for AWS CLI interactions.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=aws-mcp-server&amp;amp;username=alexei-led&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for aws-mcp-server" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/touchlab/DroidconKotlin/raw/main/CLAUDE.md"&gt;&lt;code&gt;DroidconKotlin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/touchlab"&gt;touchlab&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Delivers comprehensive Gradle commands for cross-platform Kotlin Multiplatform development with clear module structure and practical guidance for dependency injection.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=DroidconKotlin&amp;amp;username=touchlab&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for DroidconKotlin" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code/raw/main/resources/claude.md-files/EDSL/CLAUDE.md"&gt;&lt;code&gt;EDSL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/expectedparrot"&gt;expectedparrot&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Offers detailed build and test commands with strict code style enforcement, comprehensive testing requirements, and standardized development workflow using Black and mypy.*&lt;br /&gt; &lt;sub&gt;* Removed from origin&lt;/sub&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/CLAUDE.md"&gt;&lt;code&gt;Giselle&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Provides detailed build and test commands using pnpm and Vitest with strict code formatting requirements and comprehensive naming conventions for code consistency.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=giselle&amp;amp;username=giselles-ai&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for giselle" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/hashintel/hash/raw/main/CLAUDE.md"&gt;&lt;code&gt;HASH&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hashintel"&gt;hashintel&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Features comprehensive repository structure breakdown with strong emphasis on coding standards, detailed Rust documentation guidelines, and systematic PR review process.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=hash&amp;amp;username=hashintel&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for hash" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/inkline/inkline/raw/main/CLAUDE.md"&gt;&lt;code&gt;Inkline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/inkline"&gt;inkline&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Structures development workflow using pnpm with emphasis on TypeScript and Vue 3 Composition API, detailed component creation process, and comprehensive testing recommendations.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=inkline&amp;amp;username=inkline&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for inkline" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/mattgodbolt/jsbeeb/raw/main/CLAUDE.md"&gt;&lt;code&gt;JSBeeb&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/mattgodbolt"&gt;mattgodbolt&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br /&gt; Provides development guide for JavaScript BBC Micro emulator with build and testing instructions, architecture documentation, and debugging workflows.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=jsbeeb&amp;amp;username=mattgodbolt&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for jsbeeb" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/LamoomAI/lamoom-python/raw/main/CLAUDE.md"&gt;&lt;code&gt;Lamoom Python&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/LamoomAI"&gt;LamoomAI&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Serves as reference for production prompt engineering library with load balancing of AI Models, API documentation, and usage patterns with examples.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=lamoom-python&amp;amp;username=LamoomAI&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for lamoom-python" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/langchain-ai/langgraphjs/raw/main/CLAUDE.md"&gt;&lt;code&gt;LangGraphJS&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/langchain-ai"&gt;langchain-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Offers comprehensive build and test commands with detailed TypeScript style guidelines, layered library architecture, and monorepo structure using yarn workspaces.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=langgraphjs&amp;amp;username=langchain-ai&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for langgraphjs" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/CLAUDE.md"&gt;&lt;code&gt;Metabase&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Details workflow for REPL-driven development in Clojure/ClojureScript with emphasis on incremental development, testing, and step-by-step approach for feature implementation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=metabase&amp;amp;username=metabase&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for metabase" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sgcarstrends/backend/raw/main/CLAUDE.md"&gt;&lt;code&gt;SG Cars Trends Backend&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/sgcarstrends"&gt;sgcarstrends&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Provides comprehensive structure for TypeScript monorepo projects with detailed commands for development, testing, deployment, and AWS/Cloudflare integration.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=backend&amp;amp;username=sgcarstrends&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for backend" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/spylang/spy/raw/main/CLAUDE.md"&gt;&lt;code&gt;SPy&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/spylang"&gt;spylang&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Enforces strict coding conventions with comprehensive testing guidelines, multiple code compilation options, and backend-specific test decorators for targeted filtering.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=spy&amp;amp;username=spylang&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for spy" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/KarpelesLab/tpl/raw/master/CLAUDE.md"&gt;&lt;code&gt;TPL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/KarpelesLab"&gt;KarpelesLab&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Details Go project conventions with comprehensive error handling recommendations, table-driven testing approach guidelines, and modernization suggestions for latest Go features.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=tpl&amp;amp;username=KarpelesLab&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for tpl" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Domain-Specific &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Layr-Labs/avs-vibe-developer-guide/raw/master/CLAUDE.md"&gt;&lt;code&gt;AVS Vibe Developer Guide&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Layr-Labs"&gt;Layr-Labs&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Structures AI-assisted EigenLayer AVS development workflow with consistent naming conventions for prompt files and established terminology standards for blockchain concepts.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=avs-vibe-developer-guide&amp;amp;username=Layr-Labs&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for avs-vibe-developer-guide" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/CommE2E/comm/raw/master/CLAUDE.md"&gt;&lt;code&gt;Comm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/CommE2E"&gt;CommE2E&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;BSD-3-Clause&lt;br /&gt; Serves as a development reference for E2E-encrypted messaging applications with code organization architecture, security implementation details, and testing procedures.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=comm&amp;amp;username=CommE2E&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for comm" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/badass-courses/course-builder/raw/main/CLAUDE.md"&gt;&lt;code&gt;Course Builder&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/badass-courses"&gt;badass-courses&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Enables real-time multiplayer capabilities for collaborative course creation with diverse tech stack integration and monorepo architecture using Turborepo.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=course-builder&amp;amp;username=badass-courses&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for course-builder" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/eastlondoner/cursor-tools/raw/main/CLAUDE.md"&gt;&lt;code&gt;Cursor Tools&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eastlondoner"&gt;eastlondoner&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Creates a versatile AI command interface supporting multiple providers and models with flexible command options and browser automation through "Stagehand" feature.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=cursor-tools&amp;amp;username=eastlondoner&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for cursor-tools" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/soramimi/Guitar/raw/master/CLAUDE.md"&gt;&lt;code&gt;Guitar&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/soramimi"&gt;soramimi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-2.0&lt;br /&gt; Serves as development guide for Guitar Git GUI Client with build commands for various platforms, code style guidelines for contributing, and project structure explanation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=Guitar&amp;amp;username=soramimi&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for Guitar" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Fimeg/NetworkChronicles/raw/legacy-v1/CLAUDE.md"&gt;&lt;code&gt;Network Chronicles&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Fimeg"&gt;Fimeg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Presents detailed implementation plan for AI-driven game characters with technical specifications for LLM integration, character guidelines, and service discovery mechanics.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=NetworkChronicles&amp;amp;username=Fimeg&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for NetworkChronicles" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/different-ai/note-companion/raw/master/CLAUDE.md"&gt;&lt;code&gt;Note Companion&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/different-ai"&gt;different-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Provides detailed styling isolation techniques for Obsidian plugins using Tailwind with custom prefix to prevent style conflicts and practical troubleshooting steps.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=note-companion&amp;amp;username=different-ai&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for note-companion" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/ParetoSecurity/pareto-mac/raw/main/CLAUDE.md"&gt;&lt;code&gt;Pareto Mac&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ParetoSecurity"&gt;ParetoSecurity&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br /&gt; Serves as development guide for Mac security audit tool with build instructions, contribution guidelines, testing procedures, and workflow documentation.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=pareto-mac&amp;amp;username=ParetoSecurity&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for pareto-mac" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/CLAUDE.md"&gt;&lt;code&gt;SteadyStart&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br /&gt; Clear and direct instructives about style, permissions, Claude's "role", communications, and documentation of Claude Code sessions for other team members to stay abreast.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=steadystart&amp;amp;username=steadycursor&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for steadystart" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;Project Scaffolding &amp;amp; MCP &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://github.com/basicmachines-co/basic-memory/raw/main/CLAUDE.md"&gt;&lt;code&gt;Basic Memory&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/basicmachines-co"&gt;basicmachines-co&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br /&gt; Presents an innovative AI-human collaboration framework with Model Context Protocol for bidirectional LLM-markdown communication and flexible knowledge structure for complex projects.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=basic-memory&amp;amp;username=basicmachines-co&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for basic-memory" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-code-mcp-enhanced/raw/main/CLAUDE.md"&gt;&lt;code&gt;claude-code-mcp-enhanced&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Provides detailed and emphatic instructions for Claude to follow as a coding agent, with testing guidance, code examples, and compliance checks.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-mcp-enhanced&amp;amp;username=grahama1970&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-mcp-enhanced" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Family-IT-Guy/perplexity-mcp/raw/main/CLAUDE.md"&gt;&lt;code&gt;Perplexity MCP&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Family-IT-Guy"&gt;Family-IT-Guy&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;ISC&lt;br /&gt; Offers clear step-by-step installation instructions with multiple configuration options, detailed troubleshooting guidance, and concise architecture overview of the MCP protocol.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=perplexity-mcp&amp;amp;username=Family-IT-Guy&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for perplexity-mcp" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;Official Documentation ğŸ›ï¸ &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Links to some of Anthropic's terrific documentation and resources regarding Claude Code&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;General &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://docs.claude.com/en/home"&gt;&lt;code&gt;Anthropic Documentation&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Â©&lt;br /&gt; The official documentation for Claude Code, including installation instructions, usage guidelines, API references, tutorials, examples, loads of information that I won't list individually. Like Claude Code, the documentation is frequently updated.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/anthropics/claude-quickstarts"&gt;&lt;code&gt;Anthropic Quickstarts&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Offers comprehensive development guides for three distinct AI-powered demo projects with standardized workflows, strict code style guidelines, and containerization instructions.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-quickstarts&amp;amp;username=anthropics&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-quickstarts" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/anthropics/claude-code-action/tree/main/examples"&gt;&lt;code&gt;Claude Code GitHub Actions&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Official GitHub Actions integration for Claude Code with examples and documentation for automating AI-powered workflows in CI/CD pipelines.&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;ğŸ“Š GitHub Stats&lt;/summary&gt; 
  &lt;p&gt;&lt;img src="https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-action&amp;amp;username=anthropics&amp;amp;all_stats=true&amp;amp;stats_only=true" alt="GitHub Stats for claude-code-action" /&gt;&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contributing ğŸŒ» &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;ğŸš€ &lt;strong&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code/issues/new?template=submit-resource.yml"&gt;Submit a new resource here!&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;It's easy! Just click the link above and fill out the form. No Git knowledge required - our automated system handles everything for you.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How we evaluate submissions&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;First and foremost, best efforts are made to ensure that any resource on this list is free of malware, bloatware, or other unwanted risks.&lt;/li&gt; 
 &lt;li&gt;Second, being a &lt;em&gt;curated&lt;/em&gt; list, every entry is manually evaluated and "tested out" to see if it actually delivers value to Claude Code users. If you want an &lt;em&gt;un-curated&lt;/em&gt; list, you can find many great resources &lt;a href="https://github.com/search?q=%22claude+code%22&amp;amp;type=repositories&amp;amp;s=stars&amp;amp;o=desc"&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;What we're looking for&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The Claude Code CLI/TUI has led the way in terms of its capabilities, range of features, and possibilites for configuration and optimization. As of late, many submissions have focused less on "what can you do with Claude Code?" and more on "how can I plug Claude Code in to 100 other cool things?" My goal is to focus on the former question, not because it's inherently more interesting, but because the internet is full of awesome things, and this particular repo is supposed to highlight awesome things you can do &lt;em&gt;with Claude Code&lt;/em&gt;. That is to say, this is not a list for &lt;em&gt;general-purpose agent-configuration systems&lt;/em&gt; (however amazing those things are) - there are still plenty of people who sign up every day and go online and ask "What am I supposed to write in my &lt;code&gt;CLAUDE.md&lt;/code&gt;"? Indie developers, such as those featured on this list, have often done much of the legwork to help those users get comfortable with the Claude Code ecosystem. So I encourage you to &lt;strong&gt;keep exploring the possibilities that exist within Claude Code&lt;/strong&gt; - a small utility that is easy to install, easy to uninstall, and adds a dash of pleasure to the Claude Code experience, is more likely to be featured (or, more quickly, at least), than a full-fledged agent development framework that is more complex that Claude Code itself.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the complete submission guide and review process.&lt;/p&gt; 
&lt;p&gt;For suggestions about the repository itself, please &lt;a href="https://github.com/hesreallyhim/awesome-claude-code/issues/new"&gt;open a general issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is released with a &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/code-of-conduct.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to abide by its terms. And although I take strong measures to uphold the quality and safety of this list, like Anthropic, I take no responsibility or liability for anything bad that might happen as a result of these third-party resources.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>