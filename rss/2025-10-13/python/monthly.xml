<rss version="2.0">
  <channel>
    <title>GitHub Python Monthly Trending</title>
    <description>Monthly Trending of Python in GitHub</description>
    <pubDate>Sun, 12 Oct 2025 01:56:01 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>CorentinJ/Real-Time-Voice-Cloning</title>
      <link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link>
      <description>&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; 
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href="https://matheo.uliege.be/handle/2268.2/6801"&gt;master's thesis&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA"&gt;&lt;img src="https://i.imgur.com/8lFUlgz.png" alt="Toolbox demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Papers implemented&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;Designation&lt;/th&gt; 
   &lt;th&gt;Title&lt;/th&gt; 
   &lt;th&gt;Implementation source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf"&gt;1802.08435&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; 
   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1703.10135.pdf"&gt;1703.10135&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; 
   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf"&gt;1710.10467&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GE2E (encoder)&lt;/td&gt; 
   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Heads up&lt;/h2&gt; 
&lt;p&gt;Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out &lt;a href="https://paperswithcode.com/task/speech-synthesis/"&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;Chatterbox&lt;/a&gt; for a similar project up to date with the 2025 SOTA in voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running the toolbox&lt;/h2&gt; 
&lt;p&gt;Both Windows and Linux are supported.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;a href="https://ffmpeg.org/download.html#get-packages"&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files. Check if it's installed by running in a command line&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install uv for python package management&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;# On Windows:
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
# On Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Alternatively, on any platform if you have pip installed you can do
pip install -U uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Run one of the following commands&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;# Run the toolbox if you have an NVIDIA GPU
uv run --extra cuda demo_toolbox.py
# Use this if you don't
uv run --extra cpu demo_toolbox.py

# Run in command line if you don't want the GUI
uv run --extra cuda demo_cli.py
uv run --extra cpu demo_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Uv will automatically create a .venv directory for you with an appropriate python environment. &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues"&gt;Open an issue&lt;/a&gt; if this fails for you&lt;/p&gt; 
&lt;h3&gt;(Optional) Download Pretrained Models&lt;/h3&gt; 
&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;(Optional) Download Datasets&lt;/h3&gt; 
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="https://www.openslr.org/resources/12/train-clean-100.tar.gz"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Alibaba-NLP/DeepResearch</title>
      <link>https://github.com/Alibaba-NLP/DeepResearch</link>
      <description>&lt;p&gt;Tongyi Deep Research, the Leading Open-source Deep Research Agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/logo.png" width="100%" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;&lt;img src="https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="MODELS" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Alibaba-NLP/DeepResearch"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p align="center"&gt; ğŸ¤— &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;HuggingFace&lt;/a&gt; ï½œ &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;ModelScope&lt;/a&gt; | ğŸ’¬ &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/wechat_new.jpg"&gt;WeChat(å¾®ä¿¡)&lt;/a&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14895" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14895" alt="Alibaba-NLP%2FDeepResearch | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;ğŸ‘ Welcome to try Tongyi DeepResearch via our &lt;strong&gt;&lt;a href="https://www.modelscope.cn/studios/jialongwu/Tongyi-DeepResearch"&gt;&lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; Modelscope online demo&lt;/a&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;a href="https://huggingface.co/spaces/Alibaba-NLP/Tongyi-DeepResearch"&gt;ğŸ¤— Huggingface online demo&lt;/a&gt;&lt;/strong&gt; or &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/assets/aliyun.png" width="14px" style="display:inline;" /&gt; &lt;strong&gt;&lt;a href="https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&amp;amp;tab=app#/app/app-market/deep-search/"&gt;bailian service&lt;/a&gt;&lt;/strong&gt;!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This demo is for quick exploration only. Response times may vary or fail intermittently due to model latency and tool QPS limits. For a stable experience we recommend local deployment; for a production-ready service, visit &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/assets/aliyun.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&amp;amp;tab=app#/app/app-market/deep-search/"&gt;bailian&lt;/a&gt; and follow the guided setup.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;We present &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;strong&gt;Tongyi DeepResearch&lt;/strong&gt;, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for &lt;strong&gt;long-horizon, deep information-seeking&lt;/strong&gt; tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tongyi DeepResearch builds upon our previous work on the &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/"&gt;WebAgent&lt;/a&gt; project.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;More details can be found in our ğŸ“°&amp;nbsp;&lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;Tech Blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/performance.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;âš™ï¸ &lt;strong&gt;Fully automated synthetic data generation pipeline&lt;/strong&gt;: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Large-scale continual pre-training on agentic data&lt;/strong&gt;: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.&lt;/li&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;End-to-end reinforcement learning&lt;/strong&gt;: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a nonâ€‘stationary environment.&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Agent Inference Paradigm Compatibility&lt;/strong&gt;: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Model Download&lt;/h1&gt; 
&lt;p&gt;You can directly download the model by following the links below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Download Links&lt;/th&gt; 
   &lt;th align="center"&gt;Model Size&lt;/th&gt; 
   &lt;th align="center"&gt;Context Length&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Tongyi-DeepResearch-30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;ğŸ¤— HuggingFace&lt;/a&gt;&lt;br /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B"&gt;ğŸ¤– ModelScope&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;128K&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;p&gt;[2025/09/20]ğŸš€ Tongyi-DeepResearch-30B-A3B is now on &lt;a href="https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b"&gt;OpenRouter&lt;/a&gt;! Follow the &lt;a href="https://github.com/Alibaba-NLP/DeepResearch?tab=readme-ov-file#6-you-can-use-openrouters-api-to-call-our-model"&gt;Quick-start&lt;/a&gt; guide.&lt;/p&gt; 
&lt;p&gt;[2025/09/17]ğŸ”¥ We have released &lt;strong&gt;Tongyi-DeepResearch-30B-A3B&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;Deep Research Benchmark Results&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This guide provides instructions for setting up the environment and running inference scripts located in the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/inference/"&gt;inference&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recommended Python version: &lt;strong&gt;3.10.0&lt;/strong&gt; (using other versions may cause dependency issues).&lt;/li&gt; 
 &lt;li&gt;It is strongly advised to create an isolated environment using &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with Conda
conda create -n react_infer_env python=3.10.0
conda activate react_infer_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install the required dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Environment Configuration and Prepare Evaluation Data&lt;/h3&gt; 
&lt;h4&gt;Environment Configuration&lt;/h4&gt; 
&lt;p&gt;Configure your API keys and settings by copying the example environment file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy the example environment file
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Edit the &lt;code&gt;.env&lt;/code&gt; file and provide your actual API keys and configuration values:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SERPER_KEY_ID&lt;/strong&gt;: Get your key from &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; for web search and Google Scholar&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JINA_API_KEYS&lt;/strong&gt;: Get your key from &lt;a href="https://jina.ai/"&gt;Jina.ai&lt;/a&gt; for web page reading&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API_KEY/API_BASE&lt;/strong&gt;: OpenAI-compatible API for page summarization from &lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DASHSCOPE_API_KEY&lt;/strong&gt;: Get your key from &lt;a href="https://dashscope.aliyun.com/"&gt;Dashscope&lt;/a&gt; for file parsing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SANDBOX_FUSION_ENDPOINT&lt;/strong&gt;: Python interpreter sandbox endpoints (see &lt;a href="https://github.com/bytedance/SandboxFusion"&gt;SandboxFusion&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MODEL_PATH&lt;/strong&gt;: Path to your model weights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DATASET&lt;/strong&gt;: Name of your evaluation dataset&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OUTPUT_PATH&lt;/strong&gt;: Directory for saving results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;.env&lt;/code&gt; file is gitignored, so your secrets will not be committed to the repository.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Prepare Evaluation Data&lt;/h4&gt; 
&lt;p&gt;The system supports two input file formats: &lt;strong&gt;JSON&lt;/strong&gt; and &lt;strong&gt;JSONL&lt;/strong&gt;.&lt;/p&gt; 
&lt;h4&gt;Supported File Formats:&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: JSONL Format (recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.jsonl&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.jsonl&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Each line must be a valid JSON object with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class="language-json"&gt;{"question": "What is the capital of France?", "answer": "Paris"}
{"question": "Explain quantum computing", "answer": ""}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: JSON Format&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.json&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.json&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;File must contain a JSON array of objects, each with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class="language-json"&gt;[
  {"question": "What is the capital of France?", "answer": "Paris"},
  {"question": "Explain quantum computing", "answer": ""}
]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; The &lt;code&gt;answer&lt;/code&gt; field contains the &lt;strong&gt;ground truth/reference answer&lt;/strong&gt; used for evaluation. The system generates its own responses to the questions, and these reference answers are used to automatically judge the quality of the generated responses during benchmark evaluation.&lt;/p&gt; 
&lt;h4&gt;File References for Document Processing:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;If using the &lt;em&gt;file parser&lt;/em&gt; tool, &lt;strong&gt;prepend the filename to the &lt;code&gt;question&lt;/code&gt; field&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Place referenced files in &lt;code&gt;eval_data/file_corpus/&lt;/code&gt; directory&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;{"question": "report.pdf What are the key findings?", "answer": "..."}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;File Organization:&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;project_root/
â”œâ”€â”€ eval_data/
â”‚   â”œâ”€â”€ my_questions.jsonl          # Your evaluation data
â”‚   â””â”€â”€ file_corpus/                # Referenced documents
â”‚       â”œâ”€â”€ report.pdf
â”‚       â””â”€â”€ data.xlsx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Configure the Inference Script&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open &lt;code&gt;run_react_infer.sh&lt;/code&gt; and modify the following variables as instructed in the comments: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - path to the local or remote model weights.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;DATASET&lt;/code&gt; - full path to your evaluation file, e.g. &lt;code&gt;eval_data/my_questions.jsonl&lt;/code&gt; or &lt;code&gt;/path/to/my_questions.json&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OUTPUT_PATH&lt;/code&gt; - path for saving the prediction results, e.g. &lt;code&gt;./outputs&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required &lt;code&gt;API_KEY&lt;/code&gt;, &lt;code&gt;BASE_URL&lt;/code&gt;, or other credentials. Each key is explained inline in the bash script.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Run the Inference Script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash run_react_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;With these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.&lt;/p&gt; 
&lt;h3&gt;6. You can use OpenRouter's API to call our model&lt;/h3&gt; 
&lt;p&gt;Tongyi-DeepResearch-30B-A3B is now available at &lt;a href="https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b"&gt;OpenRouter&lt;/a&gt;. You can run the inference without any GPUs.&lt;/p&gt; 
&lt;p&gt;You need to modify the following in the file &lt;a href="https://github.com/Alibaba-NLP/DeepResearch/raw/main/inference/react_agent.py"&gt;inference/react_agent.py&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the call_server function: Set the API key and URL to your OpenRouter accountâ€™s API and URL.&lt;/li&gt; 
 &lt;li&gt;Change the model name to alibaba/tongyi-deepresearch-30b-a3b.&lt;/li&gt; 
 &lt;li&gt;Adjust the content concatenation way as described in the comments on lines &lt;strong&gt;88â€“90.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide benchmark evaluation scripts for various datasets. Please refer to the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/evaluation/"&gt;evaluation scripts&lt;/a&gt; directory for more details.&lt;/p&gt; 
&lt;h2&gt;Deep Research Agent Family&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/family.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:&lt;/p&gt; 
&lt;p&gt;[1] &lt;a href="https://arxiv.org/pdf/2501.07572"&gt;WebWalker: Benchmarking LLMs in Web Traversal&lt;/a&gt; (ACL 2025)&lt;br /&gt; [2] &lt;a href="https://arxiv.org/pdf/2505.22648"&gt;WebDancer: Towards Autonomous Information Seeking Agency&lt;/a&gt; (NeurIPS 2025)&lt;br /&gt; [3] &lt;a href="https://arxiv.org/pdf/2507.02592"&gt;WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/a&gt;&lt;br /&gt; [4] &lt;a href="https://arxiv.org/pdf/2507.15061"&gt;WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/a&gt;&lt;br /&gt; [5] &lt;a href="https://arxiv.org/pdf/2508.05748"&gt;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&lt;/a&gt;&lt;br /&gt; [6] &lt;a href="https://arxiv.org/pdf/2509.13309"&gt;WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents&lt;/a&gt;&lt;br /&gt; [7] &lt;a href="https://arxiv.org/pdf/2509.13313"&gt;ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization&lt;/a&gt;&lt;br /&gt; [8] &lt;a href="https://arxiv.org/pdf/2509.13312"&gt;WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research&lt;/a&gt;&lt;br /&gt; [9] &lt;a href="https://arxiv.org/pdf/2509.13305"&gt;WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning&lt;/a&gt;&lt;br /&gt; [10] &lt;a href="https://arxiv.org/pdf/2509.13310"&gt;Scaling Agents via Continual Pre-training&lt;/a&gt;&lt;br /&gt; [11] &lt;a href="https://arxiv.org/pdf/2509.13311"&gt;Towards General Agentic Intelligence via Environment Scaling&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸŒŸ Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#Alibaba-NLP/DeepResearch&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸš© Talent Recruitment&lt;/h2&gt; 
&lt;p&gt;ğŸ”¥ğŸ”¥ğŸ”¥ We are hiring! Research intern positions are open (based in Hangzhouã€Beijingã€Shanghai)&lt;/p&gt; 
&lt;p&gt;ğŸ“š &lt;strong&gt;Research Area&lt;/strong&gt;ï¼šWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; 
&lt;p&gt;â˜ï¸ &lt;strong&gt;Contact&lt;/strong&gt;ï¼š&lt;a href=""&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href="mailto:yongjiang.jy@alibaba-inc.com"&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>harry0703/MoneyPrinterTurbo</title>
      <link>https://github.com/harry0703/MoneyPrinterTurbo</link>
      <description>&lt;p&gt;åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1 align="center"&gt;MoneyPrinterTurbo ğŸ’¸&lt;/h1&gt; 
 &lt;p align="center"&gt; &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/issues"&gt;&lt;img src="https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="License" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;ç®€ä½“ä¸­æ–‡ | &lt;a href="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/README-en.md"&gt;English&lt;/a&gt;&lt;/h3&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/8731" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/8731" alt="harry0703%2FMoneyPrinterTurbo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; åªéœ€æä¾›ä¸€ä¸ªè§†é¢‘ 
 &lt;b&gt;ä¸»é¢˜&lt;/b&gt; æˆ– 
 &lt;b&gt;å…³é”®è¯&lt;/b&gt; ï¼Œå°±å¯ä»¥å…¨è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆã€è§†é¢‘ç´ æã€è§†é¢‘å­—å¹•ã€è§†é¢‘èƒŒæ™¯éŸ³ä¹ï¼Œç„¶ååˆæˆä¸€ä¸ªé«˜æ¸…çš„çŸ­è§†é¢‘ã€‚ 
 &lt;br /&gt; 
 &lt;h4&gt;Webç•Œé¢&lt;/h4&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/webui.jpg" alt="" /&gt;&lt;/p&gt; 
 &lt;h4&gt;APIç•Œé¢&lt;/h4&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/api.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ç‰¹åˆ«æ„Ÿè°¢ ğŸ™&lt;/h2&gt; 
&lt;p&gt;ç”±äºè¯¥é¡¹ç›®çš„ &lt;strong&gt;éƒ¨ç½²&lt;/strong&gt; å’Œ &lt;strong&gt;ä½¿ç”¨&lt;/strong&gt;ï¼Œå¯¹äºä¸€äº›å°ç™½ç”¨æˆ·æ¥è¯´ï¼Œè¿˜æ˜¯ &lt;strong&gt;æœ‰ä¸€å®šçš„é—¨æ§›&lt;/strong&gt;ï¼Œåœ¨æ­¤ç‰¹åˆ«æ„Ÿè°¢ &lt;strong&gt;å½•å’–ï¼ˆAIæ™ºèƒ½ å¤šåª’ä½“æœåŠ¡å¹³å°ï¼‰&lt;/strong&gt; ç½‘ç«™åŸºäºè¯¥é¡¹ç›®ï¼Œæä¾›çš„å…è´¹&lt;code&gt;AIè§†é¢‘ç”Ÿæˆå™¨&lt;/code&gt;æœåŠ¡ï¼Œå¯ä»¥ä¸ç”¨éƒ¨ç½²ï¼Œç›´æ¥åœ¨çº¿ä½¿ç”¨ï¼Œéå¸¸æ–¹ä¾¿ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä¸­æ–‡ç‰ˆï¼š&lt;a href="https://reccloud.cn"&gt;https://reccloud.cn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;è‹±æ–‡ç‰ˆï¼š&lt;a href="https://reccloud.com"&gt;https://reccloud.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/reccloud.cn.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;æ„Ÿè°¢èµåŠ© ğŸ™&lt;/h2&gt; 
&lt;p&gt;æ„Ÿè°¢ä½ç³– &lt;a href="https://picwish.cn"&gt;https://picwish.cn&lt;/a&gt; å¯¹è¯¥é¡¹ç›®çš„æ”¯æŒå’ŒèµåŠ©ï¼Œä½¿å¾—è¯¥é¡¹ç›®èƒ½å¤ŸæŒç»­çš„æ›´æ–°å’Œç»´æŠ¤ã€‚&lt;/p&gt; 
&lt;p&gt;ä½ç³–ä¸“æ³¨äº&lt;strong&gt;å›¾åƒå¤„ç†é¢†åŸŸ&lt;/strong&gt;ï¼Œæä¾›ä¸°å¯Œçš„&lt;strong&gt;å›¾åƒå¤„ç†å·¥å…·&lt;/strong&gt;ï¼Œå°†å¤æ‚æ“ä½œæè‡´ç®€åŒ–ï¼ŒçœŸæ­£å®ç°è®©å›¾åƒå¤„ç†æ›´ç®€å•ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/picwish.jpg" alt="picwish.jpg" /&gt;&lt;/p&gt; 
&lt;h2&gt;åŠŸèƒ½ç‰¹æ€§ ğŸ¯&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; å®Œæ•´çš„ &lt;strong&gt;MVCæ¶æ„&lt;/strong&gt;ï¼Œä»£ç  &lt;strong&gt;ç»“æ„æ¸…æ™°&lt;/strong&gt;ï¼Œæ˜“äºç»´æŠ¤ï¼Œæ”¯æŒ &lt;code&gt;API&lt;/code&gt; å’Œ &lt;code&gt;Webç•Œé¢&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ”¯æŒè§†é¢‘æ–‡æ¡ˆ &lt;strong&gt;AIè‡ªåŠ¨ç”Ÿæˆ&lt;/strong&gt;ï¼Œä¹Ÿå¯ä»¥&lt;strong&gt;è‡ªå®šä¹‰æ–‡æ¡ˆ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ”¯æŒå¤šç§ &lt;strong&gt;é«˜æ¸…è§†é¢‘&lt;/strong&gt; å°ºå¯¸ 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ç«–å± 9:16ï¼Œ&lt;code&gt;1080x1920&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ¨ªå± 16:9ï¼Œ&lt;code&gt;1920x1080&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ”¯æŒ &lt;strong&gt;æ‰¹é‡è§†é¢‘ç”Ÿæˆ&lt;/strong&gt;ï¼Œå¯ä»¥ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªè§†é¢‘ï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªæœ€æ»¡æ„çš„&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ”¯æŒ &lt;strong&gt;è§†é¢‘ç‰‡æ®µæ—¶é•¿&lt;/strong&gt; è®¾ç½®ï¼Œæ–¹ä¾¿è°ƒèŠ‚ç´ æåˆ‡æ¢é¢‘ç‡&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ”¯æŒ &lt;strong&gt;ä¸­æ–‡&lt;/strong&gt; å’Œ &lt;strong&gt;è‹±æ–‡&lt;/strong&gt; è§†é¢‘æ–‡æ¡ˆ&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ”¯æŒ &lt;strong&gt;å¤šç§è¯­éŸ³&lt;/strong&gt; åˆæˆï¼Œå¯ &lt;strong&gt;å®æ—¶è¯•å¬&lt;/strong&gt; æ•ˆæœ&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ”¯æŒ &lt;strong&gt;å­—å¹•ç”Ÿæˆ&lt;/strong&gt;ï¼Œå¯ä»¥è°ƒæ•´ &lt;code&gt;å­—ä½“&lt;/code&gt;ã€&lt;code&gt;ä½ç½®&lt;/code&gt;ã€&lt;code&gt;é¢œè‰²&lt;/code&gt;ã€&lt;code&gt;å¤§å°&lt;/code&gt;ï¼ŒåŒæ—¶æ”¯æŒ&lt;code&gt;å­—å¹•æè¾¹&lt;/code&gt;è®¾ç½®&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ”¯æŒ &lt;strong&gt;èƒŒæ™¯éŸ³ä¹&lt;/strong&gt;ï¼Œéšæœºæˆ–è€…æŒ‡å®šéŸ³ä¹æ–‡ä»¶ï¼Œå¯è®¾ç½®&lt;code&gt;èƒŒæ™¯éŸ³ä¹éŸ³é‡&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; è§†é¢‘ç´ ææ¥æº &lt;strong&gt;é«˜æ¸…&lt;/strong&gt;ï¼Œè€Œä¸” &lt;strong&gt;æ— ç‰ˆæƒ&lt;/strong&gt;ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„ &lt;strong&gt;æœ¬åœ°ç´ æ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; æ”¯æŒ &lt;strong&gt;OpenAI&lt;/strong&gt;ã€&lt;strong&gt;Moonshot&lt;/strong&gt;ã€&lt;strong&gt;Azure&lt;/strong&gt;ã€&lt;strong&gt;gpt4free&lt;/strong&gt;ã€&lt;strong&gt;one-api&lt;/strong&gt;ã€&lt;strong&gt;é€šä¹‰åƒé—®&lt;/strong&gt;ã€&lt;strong&gt;Google Gemini&lt;/strong&gt;ã€&lt;strong&gt;Ollama&lt;/strong&gt;ã€&lt;strong&gt;DeepSeek&lt;/strong&gt;ã€ &lt;strong&gt;æ–‡å¿ƒä¸€è¨€&lt;/strong&gt;, &lt;strong&gt;Pollinations&lt;/strong&gt; ç­‰å¤šç§æ¨¡å‹æ¥å…¥ 
  &lt;ul&gt; 
   &lt;li&gt;ä¸­å›½ç”¨æˆ·å»ºè®®ä½¿ç”¨ &lt;strong&gt;DeepSeek&lt;/strong&gt; æˆ– &lt;strong&gt;Moonshot&lt;/strong&gt; ä½œä¸ºå¤§æ¨¡å‹æä¾›å•†ï¼ˆå›½å†…å¯ç›´æ¥è®¿é—®ï¼Œä¸éœ€è¦VPNã€‚æ³¨å†Œå°±é€é¢åº¦ï¼ŒåŸºæœ¬å¤Ÿç”¨ï¼‰&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;åæœŸè®¡åˆ’ ğŸ“…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; GPT-SoVITS é…éŸ³æ”¯æŒ&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œåˆ©ç”¨å¤§æ¨¡å‹ï¼Œä½¿å…¶åˆæˆçš„å£°éŸ³ï¼Œæ›´åŠ è‡ªç„¶ï¼Œæƒ…ç»ªæ›´åŠ ä¸°å¯Œ&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; å¢åŠ è§†é¢‘è½¬åœºæ•ˆæœï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åŠ çš„æµç•…&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; å¢åŠ æ›´å¤šè§†é¢‘ç´ ææ¥æºï¼Œä¼˜åŒ–è§†é¢‘ç´ æå’Œæ–‡æ¡ˆçš„åŒ¹é…åº¦&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; å¢åŠ è§†é¢‘é•¿åº¦é€‰é¡¹ï¼šçŸ­ã€ä¸­ã€é•¿&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; æ”¯æŒæ›´å¤šçš„è¯­éŸ³åˆæˆæœåŠ¡å•†ï¼Œæ¯”å¦‚ OpenAI TTS&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; è‡ªåŠ¨ä¸Šä¼ åˆ°YouTubeå¹³å°&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;è§†é¢‘æ¼”ç¤º ğŸ“º&lt;/h2&gt; 
&lt;h3&gt;ç«–å± 9:16&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     â–¶ï¸
    &lt;/g-emoji&gt; ã€Šå¦‚ä½•å¢åŠ ç”Ÿæ´»çš„ä¹è¶£ã€‹&lt;/th&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     â–¶ï¸
    &lt;/g-emoji&gt; ã€Šé‡‘é’±çš„ä½œç”¨ã€‹&lt;br /&gt;æ›´çœŸå®çš„åˆæˆå£°éŸ³&lt;/th&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     â–¶ï¸
    &lt;/g-emoji&gt; ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476"&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;æ¨ªå± 16:9&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     â–¶ï¸
    &lt;/g-emoji&gt;ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     â–¶ï¸
    &lt;/g-emoji&gt;ã€Šä¸ºä»€ä¹ˆè¦è¿åŠ¨ã€‹&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87"&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;é…ç½®è¦æ±‚ ğŸ“¦&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;å»ºè®®æœ€ä½ CPU &lt;strong&gt;4æ ¸&lt;/strong&gt; æˆ–ä»¥ä¸Šï¼Œå†…å­˜ &lt;strong&gt;4G&lt;/strong&gt; æˆ–ä»¥ä¸Šï¼Œæ˜¾å¡éå¿…é¡»&lt;/li&gt; 
 &lt;li&gt;Windows 10 æˆ– MacOS 11.0 ä»¥ä¸Šç³»ç»Ÿ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;å¿«é€Ÿå¼€å§‹ ğŸš€&lt;/h2&gt; 
&lt;h3&gt;åœ¨ Google Colab ä¸­è¿è¡Œ&lt;/h3&gt; 
&lt;p&gt;å…å»æœ¬åœ°ç¯å¢ƒé…ç½®ï¼Œç‚¹å‡»ç›´æ¥åœ¨ Google Colab ä¸­å¿«é€Ÿä½“éªŒ MoneyPrinterTurbo&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/harry0703/MoneyPrinterTurbo/blob/main/docs/MoneyPrinterTurbo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Windowsä¸€é”®å¯åŠ¨åŒ…&lt;/h3&gt; 
&lt;p&gt;ä¸‹è½½ä¸€é”®å¯åŠ¨åŒ…ï¼Œè§£å‹ç›´æ¥ä½¿ç”¨ï¼ˆè·¯å¾„ä¸è¦æœ‰ &lt;strong&gt;ä¸­æ–‡&lt;/strong&gt;ã€&lt;strong&gt;ç‰¹æ®Šå­—ç¬¦&lt;/strong&gt;ã€&lt;strong&gt;ç©ºæ ¼&lt;/strong&gt;ï¼‰&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ç™¾åº¦ç½‘ç›˜ï¼ˆv1.2.6ï¼‰: &lt;a href="https://pan.baidu.com/s/1wg0UaIyXpO3SqIpaq790SQ?pwd=sbqx"&gt;https://pan.baidu.com/s/1wg0UaIyXpO3SqIpaq790SQ?pwd=sbqx&lt;/a&gt; æå–ç : sbqx&lt;/li&gt; 
 &lt;li&gt;Google Drive (v1.2.6): &lt;a href="https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing"&gt;https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ä¸‹è½½åï¼Œå»ºè®®å…ˆ&lt;strong&gt;åŒå‡»æ‰§è¡Œ&lt;/strong&gt; &lt;code&gt;update.bat&lt;/code&gt; æ›´æ–°åˆ°&lt;strong&gt;æœ€æ–°ä»£ç &lt;/strong&gt;ï¼Œç„¶ååŒå‡» &lt;code&gt;start.bat&lt;/code&gt; å¯åŠ¨&lt;/p&gt; 
&lt;p&gt;å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ &lt;strong&gt;Chrome&lt;/strong&gt; æˆ–è€… &lt;strong&gt;Edge&lt;/strong&gt; æ‰“å¼€ï¼‰&lt;/p&gt; 
&lt;h2&gt;å®‰è£…éƒ¨ç½² ğŸ“¥&lt;/h2&gt; 
&lt;h3&gt;å‰ææ¡ä»¶&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;å°½é‡ä¸è¦ä½¿ç”¨ &lt;strong&gt;ä¸­æ–‡è·¯å¾„&lt;/strong&gt;ï¼Œé¿å…å‡ºç°ä¸€äº›æ— æ³•é¢„æ–™çš„é—®é¢˜&lt;/li&gt; 
 &lt;li&gt;è¯·ç¡®ä¿ä½ çš„ &lt;strong&gt;ç½‘ç»œ&lt;/strong&gt; æ˜¯æ­£å¸¸çš„ï¼ŒVPNéœ€è¦æ‰“å¼€&lt;code&gt;å…¨å±€æµé‡&lt;/code&gt;æ¨¡å¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;â‘  å…‹éš†ä»£ç &lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;â‘¡ ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œå»ºè®®å¯åŠ¨åä¹Ÿå¯ä»¥åœ¨ WebUI é‡Œé¢é…ç½®ï¼‰&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;å°† &lt;code&gt;config.example.toml&lt;/code&gt; æ–‡ä»¶å¤åˆ¶ä¸€ä»½ï¼Œå‘½åä¸º &lt;code&gt;config.toml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;æŒ‰ç…§ &lt;code&gt;config.toml&lt;/code&gt; æ–‡ä»¶ä¸­çš„è¯´æ˜ï¼Œé…ç½®å¥½ &lt;code&gt;pexels_api_keys&lt;/code&gt; å’Œ &lt;code&gt;llm_provider&lt;/code&gt;ï¼Œå¹¶æ ¹æ® llm_provider å¯¹åº”çš„æœåŠ¡å•†ï¼Œé…ç½®ç›¸å…³çš„ API Key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Dockeréƒ¨ç½² ğŸ³&lt;/h3&gt; 
&lt;h4&gt;â‘  å¯åŠ¨Docker&lt;/h4&gt; 
&lt;p&gt;å¦‚æœæœªå®‰è£… Dockerï¼Œè¯·å…ˆå®‰è£… &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;å¦‚æœæ˜¯Windowsç³»ç»Ÿï¼Œè¯·å‚è€ƒå¾®è½¯çš„æ–‡æ¡£ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/zh-cn/windows/wsl/install"&gt;https://learn.microsoft.com/zh-cn/windows/wsl/install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers"&gt;https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd MoneyPrinterTurbo
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨æ„ï¼šæœ€æ–°ç‰ˆçš„dockerå®‰è£…æ—¶ä¼šè‡ªåŠ¨ä»¥æ’ä»¶çš„å½¢å¼å®‰è£…docker composeï¼Œå¯åŠ¨å‘½ä»¤è°ƒæ•´ä¸ºdocker compose up&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;â‘¡ è®¿é—®Webç•Œé¢&lt;/h4&gt; 
&lt;p&gt;æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® &lt;a href="http://0.0.0.0:8501"&gt;http://0.0.0.0:8501&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;â‘¢ è®¿é—®APIæ–‡æ¡£&lt;/h4&gt; 
&lt;p&gt;æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® &lt;a href="http://0.0.0.0:8080/docs"&gt;http://0.0.0.0:8080/docs&lt;/a&gt; æˆ–è€… &lt;a href="http://0.0.0.0:8080/redoc"&gt;http://0.0.0.0:8080/redoc&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;æ‰‹åŠ¨éƒ¨ç½² ğŸ“¦&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è§†é¢‘æ•™ç¨‹&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;å®Œæ•´çš„ä½¿ç”¨æ¼”ç¤ºï¼š&lt;a href="https://v.douyin.com/iFhnwsKY/"&gt;https://v.douyin.com/iFhnwsKY/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;å¦‚ä½•åœ¨Windowsä¸Šéƒ¨ç½²ï¼š&lt;a href="https://v.douyin.com/iFyjoW3M"&gt;https://v.douyin.com/iFyjoW3M&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;â‘  åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ&lt;/h4&gt; 
&lt;p&gt;å»ºè®®ä½¿ç”¨ &lt;a href="https://conda.io/projects/conda/en/latest/user-guide/install/index.html"&gt;conda&lt;/a&gt; åˆ›å»º python è™šæ‹Ÿç¯å¢ƒ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
conda create -n MoneyPrinterTurbo python=3.11
conda activate MoneyPrinterTurbo
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;â‘¡ å®‰è£…å¥½ ImageMagick&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ä¸‹è½½ &lt;a href="https://imagemagick.org/script/download.php"&gt;https://imagemagick.org/script/download.php&lt;/a&gt; é€‰æ‹©Windowsç‰ˆæœ¬ï¼Œåˆ‡è®°ä¸€å®šè¦é€‰æ‹© &lt;strong&gt;é™æ€åº“&lt;/strong&gt; ç‰ˆæœ¬ï¼Œæ¯”å¦‚ ImageMagick-7.1.1-32-Q16-x64-&lt;strong&gt;static&lt;/strong&gt;.exe&lt;/li&gt; 
   &lt;li&gt;å®‰è£…ä¸‹è½½å¥½çš„ ImageMagickï¼Œ&lt;strong&gt;æ³¨æ„ä¸è¦ä¿®æ”¹å®‰è£…è·¯å¾„&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;ä¿®æ”¹ &lt;code&gt;é…ç½®æ–‡ä»¶ config.toml&lt;/code&gt; ä¸­çš„ &lt;code&gt;imagemagick_path&lt;/code&gt; ä¸ºä½ çš„ &lt;strong&gt;å®é™…å®‰è£…è·¯å¾„&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MacOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;brew install imagemagick
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ubuntu&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;sudo apt-get install imagemagick
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CentOS&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;sudo yum install ImageMagick
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;â‘¢ å¯åŠ¨Webç•Œé¢ ğŸŒ&lt;/h4&gt; 
&lt;p&gt;æ³¨æ„éœ€è¦åˆ° MoneyPrinterTurbo é¡¹ç›® &lt;code&gt;æ ¹ç›®å½•&lt;/code&gt; ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤&lt;/p&gt; 
&lt;h6&gt;Windows&lt;/h6&gt; 
&lt;pre&gt;&lt;code class="language-bat"&gt;webui.bat
&lt;/code&gt;&lt;/pre&gt; 
&lt;h6&gt;MacOS or Linux&lt;/h6&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sh webui.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ &lt;strong&gt;Chrome&lt;/strong&gt; æˆ–è€… &lt;strong&gt;Edge&lt;/strong&gt; æ‰“å¼€ï¼‰&lt;/p&gt; 
&lt;h4&gt;â‘£ å¯åŠ¨APIæœåŠ¡ ğŸš€&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¯åŠ¨åï¼Œå¯ä»¥æŸ¥çœ‹ &lt;code&gt;APIæ–‡æ¡£&lt;/code&gt; &lt;a href="http://127.0.0.1:8080/docs"&gt;http://127.0.0.1:8080/docs&lt;/a&gt; æˆ–è€… &lt;a href="http://127.0.0.1:8080/redoc"&gt;http://127.0.0.1:8080/redoc&lt;/a&gt; ç›´æ¥åœ¨çº¿è°ƒè¯•æ¥å£ï¼Œå¿«é€Ÿä½“éªŒã€‚&lt;/p&gt; 
&lt;h2&gt;è¯­éŸ³åˆæˆ ğŸ—£&lt;/h2&gt; 
&lt;p&gt;æ‰€æœ‰æ”¯æŒçš„å£°éŸ³åˆ—è¡¨ï¼Œå¯ä»¥æŸ¥çœ‹ï¼š&lt;a href="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/voice-list.txt"&gt;å£°éŸ³åˆ—è¡¨&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2024-04-16 v1.1.2 æ–°å¢äº†9ç§Azureçš„è¯­éŸ³åˆæˆå£°éŸ³ï¼Œéœ€è¦é…ç½®API KEYï¼Œè¯¥å£°éŸ³åˆæˆçš„æ›´åŠ çœŸå®ã€‚&lt;/p&gt; 
&lt;h2&gt;å­—å¹•ç”Ÿæˆ ğŸ“œ&lt;/h2&gt; 
&lt;p&gt;å½“å‰æ”¯æŒ2ç§å­—å¹•ç”Ÿæˆæ–¹å¼ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;edge&lt;/strong&gt;: ç”Ÿæˆ&lt;code&gt;é€Ÿåº¦å¿«&lt;/code&gt;ï¼Œæ€§èƒ½æ›´å¥½ï¼Œå¯¹ç”µè„‘é…ç½®æ²¡æœ‰è¦æ±‚ï¼Œä½†æ˜¯è´¨é‡å¯èƒ½ä¸ç¨³å®š&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;whisper&lt;/strong&gt;: ç”Ÿæˆ&lt;code&gt;é€Ÿåº¦æ…¢&lt;/code&gt;ï¼Œæ€§èƒ½è¾ƒå·®ï¼Œå¯¹ç”µè„‘é…ç½®æœ‰ä¸€å®šè¦æ±‚ï¼Œä½†æ˜¯&lt;code&gt;è´¨é‡æ›´å¯é &lt;/code&gt;ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¯ä»¥ä¿®æ”¹ &lt;code&gt;config.toml&lt;/code&gt; é…ç½®æ–‡ä»¶ä¸­çš„ &lt;code&gt;subtitle_provider&lt;/code&gt; è¿›è¡Œåˆ‡æ¢&lt;/p&gt; 
&lt;p&gt;å»ºè®®ä½¿ç”¨ &lt;code&gt;edge&lt;/code&gt; æ¨¡å¼ï¼Œå¦‚æœç”Ÿæˆçš„å­—å¹•è´¨é‡ä¸å¥½ï¼Œå†åˆ‡æ¢åˆ° &lt;code&gt;whisper&lt;/code&gt; æ¨¡å¼&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨æ„ï¼š&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;whisper æ¨¡å¼ä¸‹éœ€è¦åˆ° HuggingFace ä¸‹è½½ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œå¤§çº¦ 3GB å·¦å³ï¼Œè¯·ç¡®ä¿ç½‘ç»œé€šç•…&lt;/li&gt; 
 &lt;li&gt;å¦‚æœç•™ç©ºï¼Œè¡¨ç¤ºä¸ç”Ÿæˆå­—å¹•ã€‚&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ç”±äºå›½å†…æ— æ³•è®¿é—® HuggingFaceï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¸‹è½½ &lt;code&gt;whisper-large-v3&lt;/code&gt; çš„æ¨¡å‹æ–‡ä»¶&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ä¸‹è½½åœ°å€ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ç™¾åº¦ç½‘ç›˜: &lt;a href="https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9"&gt;https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;å¤¸å…‹ç½‘ç›˜ï¼š&lt;a href="https://pan.quark.cn/s/3ee3d991d64b"&gt;https://pan.quark.cn/s/3ee3d991d64b&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;æ¨¡å‹ä¸‹è½½åè§£å‹ï¼Œæ•´ä¸ªç›®å½•æ”¾åˆ° &lt;code&gt;.\MoneyPrinterTurbo\models&lt;/code&gt; é‡Œé¢ï¼Œ æœ€ç»ˆçš„æ–‡ä»¶è·¯å¾„åº”è¯¥æ˜¯è¿™æ ·: &lt;code&gt;.\MoneyPrinterTurbo\models\whisper-large-v3&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;MoneyPrinterTurbo  
  â”œâ”€models
  â”‚   â””â”€whisper-large-v3
  â”‚          config.json
  â”‚          model.bin
  â”‚          preprocessor_config.json
  â”‚          tokenizer.json
  â”‚          vocabulary.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;èƒŒæ™¯éŸ³ä¹ ğŸµ&lt;/h2&gt; 
&lt;p&gt;ç”¨äºè§†é¢‘çš„èƒŒæ™¯éŸ³ä¹ï¼Œä½äºé¡¹ç›®çš„ &lt;code&gt;resource/songs&lt;/code&gt; ç›®å½•ä¸‹ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å½“å‰é¡¹ç›®é‡Œé¢æ”¾äº†ä¸€äº›é»˜è®¤çš„éŸ³ä¹ï¼Œæ¥è‡ªäº YouTube è§†é¢‘ï¼Œå¦‚æœ‰ä¾µæƒï¼Œè¯·åˆ é™¤ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;å­—å¹•å­—ä½“ ğŸ…°&lt;/h2&gt; 
&lt;p&gt;ç”¨äºè§†é¢‘å­—å¹•çš„æ¸²æŸ“ï¼Œä½äºé¡¹ç›®çš„ &lt;code&gt;resource/fonts&lt;/code&gt; ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥æ”¾è¿›å»è‡ªå·±çš„å­—ä½“ã€‚&lt;/p&gt; 
&lt;h2&gt;å¸¸è§é—®é¢˜ ğŸ¤”&lt;/h2&gt; 
&lt;h3&gt;â“RuntimeError: No ffmpeg exe could be found&lt;/h3&gt; 
&lt;p&gt;é€šå¸¸æƒ…å†µä¸‹ï¼Œffmpeg ä¼šè¢«è‡ªåŠ¨ä¸‹è½½ï¼Œå¹¶ä¸”ä¼šè¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚ ä½†æ˜¯å¦‚æœä½ çš„ç¯å¢ƒæœ‰é—®é¢˜ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ï¼Œå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æ­¤æ—¶ä½ å¯ä»¥ä» &lt;a href="https://www.gyan.dev/ffmpeg/builds/"&gt;https://www.gyan.dev/ffmpeg/builds/&lt;/a&gt; ä¸‹è½½ffmpegï¼Œè§£å‹åï¼Œè®¾ç½® &lt;code&gt;ffmpeg_path&lt;/code&gt; ä¸ºä½ çš„å®é™…å®‰è£…è·¯å¾„å³å¯ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[app]
# è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„è®¾ç½®ï¼Œæ³¨æ„ Windows è·¯å¾„åˆ†éš”ç¬¦ä¸º \\
ffmpeg_path = "C:\\Users\\harry\\Downloads\\ffmpeg.exe"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;â“ImageMagickçš„å®‰å…¨ç­–ç•¥é˜»æ­¢äº†ä¸ä¸´æ—¶æ–‡ä»¶@/tmp/tmpur5hyyto.txtç›¸å…³çš„æ“ä½œ&lt;/h3&gt; 
&lt;p&gt;å¯ä»¥åœ¨ImageMagickçš„é…ç½®æ–‡ä»¶policy.xmlä¸­æ‰¾åˆ°è¿™äº›ç­–ç•¥ã€‚ è¿™ä¸ªæ–‡ä»¶é€šå¸¸ä½äº /etc/ImageMagick-&lt;code&gt;X&lt;/code&gt;/ æˆ– ImageMagick å®‰è£…ç›®å½•çš„ç±»ä¼¼ä½ç½®ã€‚ ä¿®æ”¹åŒ…å«&lt;code&gt;pattern="@"&lt;/code&gt;çš„æ¡ç›®ï¼Œå°†&lt;code&gt;rights="none"&lt;/code&gt;æ›´æ”¹ä¸º&lt;code&gt;rights="read|write"&lt;/code&gt;ä»¥å…è®¸å¯¹æ–‡ä»¶çš„è¯»å†™æ“ä½œã€‚&lt;/p&gt; 
&lt;h3&gt;â“OSError: [Errno 24] Too many open files&lt;/h3&gt; 
&lt;p&gt;è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºç³»ç»Ÿæ‰“å¼€æ–‡ä»¶æ•°é™åˆ¶å¯¼è‡´çš„ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ç³»ç»Ÿçš„æ–‡ä»¶æ‰“å¼€æ•°é™åˆ¶æ¥è§£å†³ã€‚&lt;/p&gt; 
&lt;p&gt;æŸ¥çœ‹å½“å‰é™åˆ¶&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ulimit -n
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¦‚æœè¿‡ä½ï¼Œå¯ä»¥è°ƒé«˜ä¸€äº›ï¼Œæ¯”å¦‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ulimit -n 10240
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;â“Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå‡ºç°å¦‚ä¸‹é”™è¯¯&lt;/h3&gt; 
&lt;p&gt;LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and outgoing trafic has been disabled. To enablerepo look-ups and downloads online, pass 'local files only=False' as input.&lt;/p&gt; 
&lt;p&gt;æˆ–è€…&lt;/p&gt; 
&lt;p&gt;An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub: An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the specified revision on the local disk. Please check your internet connection and try again. Trying to load the model directly from the local cache, if it exists.&lt;/p&gt; 
&lt;p&gt;è§£å†³æ–¹æ³•ï¼š&lt;a href="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-"&gt;ç‚¹å‡»æŸ¥çœ‹å¦‚ä½•ä»ç½‘ç›˜æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;åé¦ˆå»ºè®® ğŸ“¢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;å¯ä»¥æäº¤ &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/issues"&gt;issue&lt;/a&gt; æˆ–è€… &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/pulls"&gt;pull request&lt;/a&gt;ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;è®¸å¯è¯ ğŸ“&lt;/h2&gt; 
&lt;p&gt;ç‚¹å‡»æŸ¥çœ‹ &lt;a href="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; æ–‡ä»¶&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>EbookFoundation/free-programming-books</title>
      <link>https://github.com/EbookFoundation/free-programming-books</link>
      <description>&lt;p&gt;ğŸ“š Freely available programming books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;List of Free Learning Resources In Many Languages&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg?sanitize=true" alt="License: CC BY 4.0" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2025-10-01..2025-10-31"&gt;&lt;img src="https://img.shields.io/github/hacktoberfest/2025/EbookFoundation/free-programming-books?label=Hacktoberfest+2025" alt="Hacktoberfest 2025 stats" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Search the list at &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;https://ebookfoundation.github.io/free-programming-books-search/&lt;/a&gt; &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Dynamic%20search%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F" alt="https://ebookfoundation.github.io/free-programming-books-search/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This page is available as an easy-to-read website. Access it by clicking on &lt;a href="https://ebookfoundation.github.io/free-programming-books/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Static%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F" alt="https://ebookfoundation.github.io/free-programming-books/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;form action="https://ebookfoundation.github.io/free-programming-books-search"&gt; 
  &lt;input type="text" id="fpbSearch" name="search" required placeholder="Search Book or Author" /&gt; 
  &lt;label for="submit"&gt; &lt;/label&gt; 
  &lt;input type="submit" id="submit" name="submit" value="Search" /&gt; 
 &lt;/form&gt; 
&lt;/div&gt; 
&lt;h2&gt;Intro&lt;/h2&gt; 
&lt;p&gt;This list was originally a clone of &lt;a href="https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926"&gt;StackOverflow - List of Freely Available Programming Books&lt;/a&gt; with contributions from Karan Bhangui and George Stocker.&lt;/p&gt; 
&lt;p&gt;The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of &lt;a href="https://octoverse.github.com/"&gt;GitHub's most popular repositories&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/network"&gt;&lt;img src="https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Forks" alt="GitHub repo forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars" alt="GitHub repo stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Contributors" alt="GitHub repo contributors" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/sponsors/EbookFoundation"&gt;&lt;img src="https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Sponsors" alt="GitHub org sponsors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers" alt="GitHub repo watchers" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip"&gt;&lt;img src="https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size" alt="GitHub repo size" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;a href="https://ebookfoundation.org"&gt;Free Ebook Foundation&lt;/a&gt; now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. &lt;a href="https://ebookfoundation.org/contributions.html"&gt;Donations&lt;/a&gt; to the Free Ebook Foundation are tax-deductible in the US.&lt;/p&gt; 
&lt;h2&gt;How To Contribute&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;. If you're new to GitHub, &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;welcome&lt;/a&gt;! Remember to abide by our adapted from &lt;img src="https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg?sanitize=true" alt="Contributor Covenant 1.3" /&gt; &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; too (&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/#translations"&gt;translations&lt;/a&gt; also available).&lt;/p&gt; 
&lt;p&gt;Click on these badges to see how you might be able to help:&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/issues"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=red&amp;amp;label=Issues" alt="GitHub repo Issues" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Good%20First%20issues" alt="GitHub repo Good Issues for newbies" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20issues" alt="GitHub Help Wanted issues" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=orange&amp;amp;label=PRs" alt="GitHub repo PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged"&gt;&lt;img src="https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Merged%20PRs&amp;amp;query=is%3Amerged" alt="GitHub repo Merged PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20PRs" alt="GitHub Help Wanted PRs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;How To Share&lt;/h2&gt; 
&lt;div align="left" markdown="1"&gt; 
 &lt;a href="https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;amp;p%5Bimages%5D%5B0%5D=&amp;amp;p%5Btitle%5D=Free%20Programming%20Books&amp;amp;p%5Bsummary%5D="&gt;Share on Facebook&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on LinkedIn&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://toot.kytta.dev/?text=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Mastodon/Fediverse&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Telegram&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books"&gt;Share on ğ• (Twitter)&lt;/a&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;This project lists books and other resources grouped by genres:&lt;/p&gt; 
&lt;h3&gt;Books&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-langs.md"&gt;English, By Programming Language&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-subjects.md"&gt;English, By Subject&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Other Languages&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ar.md"&gt;Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hy.md"&gt;Armenian / Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-az.md"&gt;Azerbaijani / ĞĞ·Ó™Ñ€Ğ±Ğ°Ñ˜Ò¹Ğ°Ğ½ Ğ´Ğ¸Ğ»Ğ¸ / Ø¢Ø°Ø±Ø¨Ø§ÙŠØ¬Ø§Ù†Ø¬Ø§ Ø¯ÙŠÙ„ÙŠ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bn.md"&gt;Bengali / à¦¬à¦¾à¦‚à¦²à¦¾&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bg.md"&gt;Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-my.md"&gt;Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-cs.md"&gt;Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ca.md"&gt;Catalan / catalan / catalÃ &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-da.md"&gt;Danish / dansk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-et.md"&gt;Estonian / eesti keel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fr.md"&gt;French / franÃ§ais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-el.md"&gt;Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-he.md"&gt;Hebrew / ×¢×‘×¨×™×ª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hi.md"&gt;Hindi / à¤¹à¤¿à¤¨à¥à¤¦à¥€&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hu.md"&gt;Hungarian / magyar / magyar nyelv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ja.md"&gt;Japanese / æ—¥æœ¬èª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ko.md"&gt;Korean / í•œêµ­ì–´&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-lv.md"&gt;Latvian / LatvieÅ¡u&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ml.md"&gt;Malayalam / à´®à´²à´¯à´¾à´³à´‚&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fa_IR.md"&gt;Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pl.md"&gt;Polish / polski / jÄ™zyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ro.md"&gt;Romanian (Romania) / limba romÃ¢nÄƒ / romÃ¢n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ru.md"&gt;Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sr.md"&gt;Serbian / ÑÑ€Ğ¿ÑĞºĞ¸ Ñ˜ĞµĞ·Ğ¸Ğº / srpski jezik&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sk.md"&gt;Slovak / slovenÄina&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-es.md"&gt;Spanish / espaÃ±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ta.md"&gt;Tamil / à®¤à®®à®¿à®´à¯&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-te.md"&gt;Telugu / à°¤à±†à°²à±à°—à±&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-th.md"&gt;Thai / à¹„à¸—à¸¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-tr.md"&gt;Turkish / TÃ¼rkÃ§e&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-uk.md"&gt;Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-vi.md"&gt;Vietnamese / Tiáº¿ng Viá»‡t&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cheat Sheets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-cheatsheets.md"&gt;All Languages&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Free Online Courses&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ar.md"&gt;Arabic / al arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bn.md"&gt;Bengali / à¦¬à¦¾à¦‚à¦²à¦¾&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bg.md"&gt;Bulgarian / Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-my.md"&gt;Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fr.md"&gt;French / franÃ§ais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-el.md"&gt;Greek / ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-he.md"&gt;Hebrew / ×¢×‘×¨×™×ª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-hi.md"&gt;Hindi / à¤¹à¤¿à¤‚à¤¦à¥€&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ja.md"&gt;Japanese / æ—¥æœ¬èª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kn.md"&gt;Kannada / à²•à²¨à³à²¨à²¡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kk.md"&gt;Kazakh / Ò›Ğ°Ğ·Ğ°Ò›ÑˆĞ°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-km.md"&gt;Khmer / á—á¶áŸá¶ááŸ’á˜áŸ‚áš&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ko.md"&gt;Korean / í•œêµ­ì–´&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ml.md"&gt;Malayalam / à´®à´²à´¯à´¾à´³à´‚&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-mr.md"&gt;Marathi / à¤®à¤°à¤¾à¤ à¥€&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ne.md"&gt;Nepali / à¤¨à¥‡à¤ªà¤¾à¤²à¥€&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fa_IR.md"&gt;Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pl.md"&gt;Polish / polski / jÄ™zyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pa.md"&gt;Punjabi / à¨ªà©°à¨œà¨¾à¨¬à©€ / Ù¾Ù†Ø¬Ø§Ø¨ÛŒ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ro.md"&gt;Romanian (Romania) / limba romÃ¢nÄƒ / romÃ¢n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ru.md"&gt;Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-si.md"&gt;Sinhala / à·ƒà·’à¶‚à·„à¶½&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-es.md"&gt;Spanish / espaÃ±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-sv.md"&gt;Swedish / svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ta.md"&gt;Tamil / à®¤à®®à®¿à®´à¯&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-te.md"&gt;Telugu / à°¤à±†à°²à±à°—à±&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-th.md"&gt;Thai / à¸ à¸²à¸©à¸²à¹„à¸—à¸¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-tr.md"&gt;Turkish / TÃ¼rkÃ§e&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-uk.md"&gt;Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ur.md"&gt;Urdu / Ø§Ø±Ø¯Ùˆ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-vi.md"&gt;Vietnamese / Tiáº¿ng Viá»‡t&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Interactive Programming Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ja.md"&gt;Japanese / æ—¥æœ¬èª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ru.md"&gt;Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Problem Sets and Competitive Programming&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/problem-sets-competitive-programming.md"&gt;Problem Sets&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Podcast - Screencast&lt;/h3&gt; 
&lt;p&gt;Free Podcasts and Screencasts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ar.md"&gt;Arabic / al Arabiya / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-my.md"&gt;Burmese / á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-cs.md"&gt;Czech / ÄeÅ¡tina / ÄeskÃ½ jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fi.md"&gt;Finnish / Suomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fr.md"&gt;French / franÃ§ais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-he.md"&gt;Hebrew / ×¢×‘×¨×™×ª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fa_IR.md"&gt;Persian / Farsi (Iran) / ÙØ§Ø±Ø³Ù‰&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pl.md"&gt;Polish / polski / jÄ™zyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ru.md"&gt;Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-si.md"&gt;Sinhala / à·ƒà·’à¶‚à·„à¶½&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-es.md"&gt;Spanish / espaÃ±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-tr.md"&gt;Turkish / TÃ¼rkÃ§e&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-uk.md"&gt;Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Programming Playgrounds&lt;/h3&gt; 
&lt;p&gt;Write, compile, and run your code within a browser. Try it out!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-zh.md"&gt;Chinese / ä¸­æ–‡&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;How-to&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;... &lt;em&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;More languages&lt;/a&gt;&lt;/em&gt; ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might notice that there are &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;some missing translations here&lt;/a&gt; - perhaps you would like to help out by &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md#help-out-by-contributing-a-translation"&gt;contributing a translation&lt;/a&gt;?&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Each file included in this repository is licensed under the &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/LICENSE"&gt;CC BY License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TheAlgorithms/Python</title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description>&lt;p&gt;All Algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Title: --&gt; 
 &lt;a href="https://github.com/TheAlgorithms/"&gt; &lt;img src="https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg?sanitize=true" height="100" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;a href="https://github.com/TheAlgorithms/"&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt; 
 &lt;!-- Labels: --&gt; 
 &lt;!-- First row: --&gt; 
 &lt;a href="https://gitpod.io/#https://github.com/TheAlgorithms/Python"&gt; &lt;img src="https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;amp;style=flat-square" height="20" alt="Gitpod Ready-to-Code" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/raw/master/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/static/v1.svg?label=Contributions&amp;amp;message=Welcome&amp;amp;color=0059b3&amp;amp;style=flat-square" height="20" alt="Contributions Welcome" /&gt; &lt;/a&gt; 
 &lt;img src="https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;amp;style=flat-square" height="20" /&gt; 
 &lt;a href="https://the-algorithms.com/discord"&gt; &lt;img src="https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;amp;colorB=7289DA&amp;amp;style=flat-square" height="20" alt="Discord chat" /&gt; &lt;/a&gt; 
 &lt;a href="https://gitter.im/TheAlgorithms/community"&gt; &lt;img src="https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;amp;logo=gitter&amp;amp;style=flat-square" height="20" alt="Gitter chat" /&gt; &lt;/a&gt; 
 &lt;!-- Second row: --&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square" height="20" alt="GitHub Workflow Status" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/pre-commit/pre-commit"&gt; &lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square" height="20" alt="pre-commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.astral.sh/ruff/formatter/"&gt; &lt;img src="https://img.shields.io/static/v1?label=code%20style&amp;amp;message=ruff&amp;amp;color=black&amp;amp;style=flat-square" height="20" alt="code style: black" /&gt; &lt;/a&gt; 
 &lt;!-- Short description: --&gt; 
 &lt;h3&gt;All algorithms implemented in Python - for education ğŸ“š&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Getting Started&lt;/h2&gt; 
&lt;p&gt;ğŸ“‹ Read through our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; before you contribute.&lt;/p&gt; 
&lt;h2&gt;ğŸŒ Community Channels&lt;/h2&gt; 
&lt;p&gt;We are on &lt;a href="https://the-algorithms.com/discord"&gt;Discord&lt;/a&gt; and &lt;a href="https://gitter.im/TheAlgorithms/community"&gt;Gitter&lt;/a&gt;! Community channels are a great way for you to ask questions and get help. Please join us!&lt;/p&gt; 
&lt;h2&gt;ğŸ“œ List of Algorithms&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/DIRECTORY.md"&gt;directory&lt;/a&gt; for easier navigation and a better overview of the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-research/timesfm</title>
      <link>https://github.com/google-research/timesfm</link>
      <description>&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; 
&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2310.10688"&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, ICML 2024.&lt;/li&gt; 
 &lt;li&gt;All checkpoints: &lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;TimesFM Hugging Face Collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/"&gt;Google Research blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/docs/timesfm-model"&gt;TimesFM in BigQuery&lt;/a&gt;: an official Google product.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This open version is not an officially supported Google product.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Latest Model Version:&lt;/strong&gt; TimesFM 2.5&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Archived Model Versions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1.0 and 2.0: relevant code archived in the sub directory &lt;code&gt;v1&lt;/code&gt;. You can &lt;code&gt;pip install timesfm==1.3.0&lt;/code&gt; to install an older version of this package to load them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Update - Sept. 15, 2025&lt;/h2&gt; 
&lt;p&gt;TimesFM 2.5 is out!&lt;/p&gt; 
&lt;p&gt;Comparing to TimesFM 2.0, this new 2.5 model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;uses 200M parameters, down from 500M.&lt;/li&gt; 
 &lt;li&gt;supports up to 16k context length, up from 2048.&lt;/li&gt; 
 &lt;li&gt;supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.&lt;/li&gt; 
 &lt;li&gt;gets rid of the &lt;code&gt;frequency&lt;/code&gt; indicator.&lt;/li&gt; 
 &lt;li&gt;has a couple of new forecasting flags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;add support for an upcoming Flax version of the model (faster inference).&lt;/li&gt; 
 &lt;li&gt;add back covariate support.&lt;/li&gt; 
 &lt;li&gt;populate more docstrings, docs and notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/google-research/timesfm.git
cd timesfm
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create a virtual environment and install dependencies using &lt;code&gt;uv&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;# Create a virtual environment
uv venv

# Activate the environment
source .venv/bin/activate

# Install the package in editable mode with torch
uv pip install -e .[torch]
# Or with flax
uv pip install -e .[flax]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[Optional] Install your preferred &lt;code&gt;torch&lt;/code&gt; / &lt;code&gt;jax&lt;/code&gt; backend based on your OS and accelerators (CPU, GPU, TPU or Apple Silicon).:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org/get-started/locally/"&gt;Install PyTorch&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.jax.dev/en/latest/installation.html#installation"&gt;Install Jax&lt;/a&gt; for Flax.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Code Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
import numpy as np
import timesfm

torch.set_float32_matmul_precision("high")

model = timesfm.TimesFM_2p5_200M_torch.from_pretrained("google/timesfm-2.5-200m-pytorch")

model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;ğŸ”¥ åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚Text-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿ&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot æ˜¯ä¸€æ¬¾åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿã€‚SQLBot çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å¼€ç®±å³ç”¨&lt;/strong&gt;: åªéœ€é…ç½®å¤§æ¨¡å‹å’Œæ•°æ®æºå³å¯å¼€å¯é—®æ•°ä¹‹æ—…ï¼Œé€šè¿‡å¤§æ¨¡å‹å’Œ RAG çš„ç»“åˆæ¥å®ç°é«˜è´¨é‡çš„ text2sqlï¼›&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ˜“äºé›†æˆ&lt;/strong&gt;: æ”¯æŒå¿«é€ŸåµŒå…¥åˆ°ç¬¬ä¸‰æ–¹ä¸šåŠ¡ç³»ç»Ÿï¼Œä¹Ÿæ”¯æŒè¢« n8nã€MaxKBã€Difyã€Coze ç­‰ AI åº”ç”¨å¼€å‘å¹³å°é›†æˆè°ƒç”¨ï¼Œè®©å„ç±»åº”ç”¨å¿«é€Ÿæ‹¥æœ‰æ™ºèƒ½é—®æ•°èƒ½åŠ›ï¼›&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å®‰å…¨å¯æ§&lt;/strong&gt;: æä¾›åŸºäºå·¥ä½œç©ºé—´çš„èµ„æºéš”ç¦»æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„æ•°æ®æƒé™æ§åˆ¶ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;å·¥ä½œåŸç†&lt;/h2&gt; 
&lt;img width="1105" height="577" alt="system-arch" src="https://github.com/user-attachments/assets/462603fc-980b-4b8b-a6d4-a821c070a048" /&gt; 
&lt;h2&gt;å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;h3&gt;å®‰è£…éƒ¨ç½²&lt;/h3&gt; 
&lt;p&gt;å‡†å¤‡ä¸€å° Linux æœåŠ¡å™¨ï¼Œå®‰è£…å¥½ &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;ï¼Œæ‰§è¡Œä»¥ä¸‹ä¸€é”®å®‰è£…è„šæœ¬ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/file:/opt/sqlbot/data/file \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ä½ ä¹Ÿå¯ä»¥é€šè¿‡ &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel åº”ç”¨å•†åº—&lt;/a&gt; å¿«é€Ÿéƒ¨ç½² SQLBotã€‚&lt;/p&gt; 
&lt;p&gt;å¦‚æœæ˜¯å†…ç½‘ç¯å¢ƒï¼Œä½ å¯ä»¥é€šè¿‡ &lt;a href="https://community.fit2cloud.com/#/products/sqlbot/downloads"&gt;ç¦»çº¿å®‰è£…åŒ…æ–¹å¼&lt;/a&gt; éƒ¨ç½² SQLBotã€‚&lt;/p&gt; 
&lt;h3&gt;è®¿é—®æ–¹å¼&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://&amp;lt;ä½ çš„æœåŠ¡å™¨IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;ç”¨æˆ·å: admin&lt;/li&gt; 
 &lt;li&gt;å¯†ç : SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;è”ç³»æˆ‘ä»¬&lt;/h3&gt; 
&lt;p&gt;å¦‚ä½ æœ‰æ›´å¤šé—®é¢˜ï¼Œå¯ä»¥åŠ å…¥æˆ‘ä»¬çš„æŠ€æœ¯äº¤æµç¾¤ä¸æˆ‘ä»¬äº¤æµã€‚&lt;/p&gt; 
&lt;img width="180" height="180" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI å±•ç¤º&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;é£è‡´äº‘æ——ä¸‹çš„å…¶ä»–æ˜æ˜Ÿé¡¹ç›®&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - äººäººå¯ç”¨çš„å¼€æº BI å·¥å…·&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - ç°ä»£åŒ–ã€å¼€æºçš„ Linux æœåŠ¡å™¨è¿ç»´ç®¡ç†é¢æ¿&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - å¼ºå¤§æ˜“ç”¨çš„ä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - å¹¿å—æ¬¢è¿çš„å¼€æºå ¡å’æœº&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1Panel-dev/CordysCRM"&gt;Cordys CRM&lt;/a&gt; - æ–°ä¸€ä»£çš„å¼€æº AI CRM ç³»ç»Ÿ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - å¼ºå¤§æ˜“ç”¨çš„å¼€æºå»ºç«™å·¥å…·&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - æ–°ä¸€ä»£çš„å¼€æºæŒç»­æµ‹è¯•å·¥å…·&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;æœ¬ä»“åº“éµå¾ª &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; å¼€æºåè®®ï¼Œè¯¥è®¸å¯è¯æœ¬è´¨ä¸Šæ˜¯ GPLv3ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„é™åˆ¶ã€‚&lt;/p&gt; 
&lt;p&gt;ä½ å¯ä»¥åŸºäº SQLBot çš„æºä»£ç è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œä½†æ˜¯éœ€è¦éµå®ˆä»¥ä¸‹è§„å®šï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä¸èƒ½æ›¿æ¢å’Œä¿®æ”¹ SQLBot çš„ Logo å’Œç‰ˆæƒä¿¡æ¯ï¼›&lt;/li&gt; 
 &lt;li&gt;äºŒæ¬¡å¼€å‘åçš„è¡ç”Ÿä½œå“å¿…é¡»éµå®ˆ GPL V3 çš„å¼€æºä¹‰åŠ¡ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¦‚éœ€å•†ä¸šæˆæƒï¼Œè¯·è”ç³» &lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt; ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-agents-python</title>
      <link>https://github.com/openai/openai-agents-python</link>
      <description>&lt;p&gt;A lightweight, powerful framework for multi-agent workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Agents SDK&lt;/h1&gt; 
&lt;p&gt;The OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows. It is provider-agnostic, supporting the OpenAI Responses and Chat Completions APIs, as well as 100+ other LLMs.&lt;/p&gt; 
&lt;img src="https://cdn.openai.com/API/docs/images/orchestration.png" alt="Image of the Agents Tracing UI" style="max-height: 803px;" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JavaScript/TypeScript version? Check out &lt;a href="https://github.com/openai/openai-agents-js"&gt;Agents SDK JS/TS&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Core concepts:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/agents"&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/a&gt;: LLMs configured with instructions, tools, guardrails, and handoffs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/handoffs/"&gt;&lt;strong&gt;Handoffs&lt;/strong&gt;&lt;/a&gt;: A specialized tool call used by the Agents SDK for transferring control between agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/guardrails/"&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;&lt;/a&gt;: Configurable safety checks for input and output validation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/#sessions"&gt;&lt;strong&gt;Sessions&lt;/strong&gt;&lt;/a&gt;: Automatic conversation history management across agent runs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/tracing/"&gt;&lt;strong&gt;Tracing&lt;/strong&gt;&lt;/a&gt;: Built-in tracking of agent runs, allowing you to view, debug and optimize your workflows&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Explore the &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples"&gt;examples&lt;/a&gt; directory to see the SDK in action, and read our &lt;a href="https://openai.github.io/openai-agents-python/"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;p&gt;To get started, set up your Python environment (Python 3.9 or newer required), and then install OpenAI Agents SDK package.&lt;/p&gt; 
&lt;h3&gt;venv&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install openai-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For voice support, install with the optional &lt;code&gt;voice&lt;/code&gt; group: &lt;code&gt;pip install 'openai-agents[voice]'&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For Redis session support, install with the optional &lt;code&gt;redis&lt;/code&gt; group: &lt;code&gt;pip install 'openai-agents[redis]'&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;uv&lt;/h3&gt; 
&lt;p&gt;If you're familiar with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, using the tool would be even similar:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
uv add openai-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For voice support, install with the optional &lt;code&gt;voice&lt;/code&gt; group: &lt;code&gt;uv add 'openai-agents[voice]'&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For Redis session support, install with the optional &lt;code&gt;redis&lt;/code&gt; group: &lt;code&gt;uv add 'openai-agents[redis]'&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Hello world example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(&lt;em&gt;If running this, ensure you set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable&lt;/em&gt;)&lt;/p&gt; 
&lt;p&gt;(&lt;em&gt;For Jupyter notebook users, see &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples/basic/hello_world_jupyter.ipynb"&gt;hello_world_jupyter.ipynb&lt;/a&gt;&lt;/em&gt;)&lt;/p&gt; 
&lt;h2&gt;Handoffs example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
)


async def main():
    result = await Runner.run(triage_agent, input="Hola, Â¿cÃ³mo estÃ¡s?")
    print(result.final_output)
    # Â¡Hola! Estoy bien, gracias por preguntar. Â¿Y tÃº, cÃ³mo estÃ¡s?


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Functions example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio

from agents import Agent, Runner, function_tool


@function_tool
def get_weather(city: str) -&amp;gt; str:
    return f"The weather in {city} is sunny."


agent = Agent(
    name="Hello world",
    instructions="You are a helpful agent.",
    tools=[get_weather],
)


async def main():
    result = await Runner.run(agent, input="What's the weather in Tokyo?")
    print(result.final_output)
    # The weather in Tokyo is sunny.


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;The agent loop&lt;/h2&gt; 
&lt;p&gt;When you call &lt;code&gt;Runner.run()&lt;/code&gt;, we run a loop until we get a final output.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;We call the LLM, using the model and settings on the agent, and the message history.&lt;/li&gt; 
 &lt;li&gt;The LLM returns a response, which may include tool calls.&lt;/li&gt; 
 &lt;li&gt;If the response has a final output (see below for more on this), we return it and end the loop.&lt;/li&gt; 
 &lt;li&gt;If the response has a handoff, we set the agent to the new agent and go back to step 1.&lt;/li&gt; 
 &lt;li&gt;We process the tool calls (if any) and append the tool responses messages. Then we go to step 1.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There is a &lt;code&gt;max_turns&lt;/code&gt; parameter that you can use to limit the number of times the loop executes.&lt;/p&gt; 
&lt;h3&gt;Final output&lt;/h3&gt; 
&lt;p&gt;Final output is the last thing the agent produces in the loop.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If you set an &lt;code&gt;output_type&lt;/code&gt; on the agent, the final output is when the LLM returns something of that type. We use &lt;a href="https://platform.openai.com/docs/guides/structured-outputs"&gt;structured outputs&lt;/a&gt; for this.&lt;/li&gt; 
 &lt;li&gt;If there's no &lt;code&gt;output_type&lt;/code&gt; (i.e. plain text responses), then the first LLM response without any tool calls or handoffs is considered as the final output.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;As a result, the mental model for the agent loop is:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If the current agent has an &lt;code&gt;output_type&lt;/code&gt;, the loop runs until the agent produces structured output matching that type.&lt;/li&gt; 
 &lt;li&gt;If the current agent does not have an &lt;code&gt;output_type&lt;/code&gt;, the loop runs until the current agent produces a message without any tool calls/handoffs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common agent patterns&lt;/h2&gt; 
&lt;p&gt;The Agents SDK is designed to be highly flexible, allowing you to model a wide range of LLM workflows including deterministic flows, iterative loops, and more. See examples in &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples/agent_patterns"&gt;&lt;code&gt;examples/agent_patterns&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tracing&lt;/h2&gt; 
&lt;p&gt;The Agents SDK automatically traces your agent runs, making it easy to track and debug the behavior of your agents. Tracing is extensible by design, supporting custom spans and a wide variety of external destinations, including &lt;a href="https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents"&gt;Logfire&lt;/a&gt;, &lt;a href="https://docs.agentops.ai/v1/integrations/agentssdk"&gt;AgentOps&lt;/a&gt;, &lt;a href="https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk"&gt;Braintrust&lt;/a&gt;, &lt;a href="https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration"&gt;Scorecard&lt;/a&gt;, and &lt;a href="https://docs.keywordsai.co/integration/development-frameworks/openai-agent"&gt;Keywords AI&lt;/a&gt;. For more details about how to customize or disable tracing, see &lt;a href="http://openai.github.io/openai-agents-python/tracing"&gt;Tracing&lt;/a&gt;, which also includes a larger list of &lt;a href="http://openai.github.io/openai-agents-python/tracing/#external-tracing-processors-list"&gt;external tracing processors&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Long running agents &amp;amp; human-in-the-loop&lt;/h2&gt; 
&lt;p&gt;You can use the Agents SDK &lt;a href="https://temporal.io/"&gt;Temporal&lt;/a&gt; integration to run durable, long-running workflows, including human-in-the-loop tasks. View a demo of Temporal and the Agents SDK working in action to complete long-running tasks &lt;a href="https://www.youtube.com/watch?v=fFBZqzT4DD8"&gt;in this video&lt;/a&gt;, and &lt;a href="https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents"&gt;view docs here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Sessions&lt;/h2&gt; 
&lt;p&gt;The Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle &lt;code&gt;.to_input_list()&lt;/code&gt; between turns.&lt;/p&gt; 
&lt;h3&gt;Quick start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner, SQLiteSession

# Create agent
agent = Agent(
    name="Assistant",
    instructions="Reply very concisely.",
)

# Create a session instance
session = SQLiteSession("conversation_123")

# First turn
result = await Runner.run(
    agent,
    "What city is the Golden Gate Bridge in?",
    session=session
)
print(result.final_output)  # "San Francisco"

# Second turn - agent automatically remembers previous context
result = await Runner.run(
    agent,
    "What state is it in?",
    session=session
)
print(result.final_output)  # "California"

# Also works with synchronous runner
result = Runner.run_sync(
    agent,
    "What's the population?",
    session=session
)
print(result.final_output)  # "Approximately 39 million"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Session options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No memory&lt;/strong&gt; (default): No session memory when session parameter is omitted&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;session: Session = DatabaseSession(...)&lt;/code&gt;&lt;/strong&gt;: Use a Session instance to manage conversation history&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner, SQLiteSession

# SQLite - file-based or in-memory database
session = SQLiteSession("user_123", "conversations.db")

# Redis - for scalable, distributed deployments
# from agents.extensions.memory import RedisSession
# session = RedisSession.from_url("user_123", url="redis://localhost:6379/0")

agent = Agent(name="Assistant")

# Different session IDs maintain separate conversation histories
result1 = await Runner.run(
    agent,
    "Hello",
    session=session
)
result2 = await Runner.run(
    agent,
    "Hello",
    session=SQLiteSession("user_456", "conversations.db")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Custom session implementations&lt;/h3&gt; 
&lt;p&gt;You can implement your own session memory by creating a class that follows the &lt;code&gt;Session&lt;/code&gt; protocol:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents.memory import Session
from typing import List

class MyCustomSession:
    """Custom session implementation following the Session protocol."""

    def __init__(self, session_id: str):
        self.session_id = session_id
        # Your initialization here

    async def get_items(self, limit: int | None = None) -&amp;gt; List[dict]:
        # Retrieve conversation history for the session
        pass

    async def add_items(self, items: List[dict]) -&amp;gt; None:
        # Store new items for the session
        pass

    async def pop_item(self) -&amp;gt; dict | None:
        # Remove and return the most recent item from the session
        pass

    async def clear_session(self) -&amp;gt; None:
        # Clear all items for the session
        pass

# Use your custom session
agent = Agent(name="Assistant")
result = await Runner.run(
    agent,
    "Hello",
    session=MyCustomSession("my_session")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development (only needed if you need to edit the SDK/examples)&lt;/h2&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt;Ensure you have &lt;a href="https://docs.astral.sh/uv/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;(After making changes) lint/test&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;make check # run tests linter and typechecker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to run them individually:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;make tests  # run tests
make mypy   # run typechecker
make lint   # run linter
make format-check # run style checker
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We'd like to acknowledge the excellent work of the open-source community, especially:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.pydantic.dev/latest/"&gt;Pydantic&lt;/a&gt; (data validation) and &lt;a href="https://ai.pydantic.dev/"&gt;PydanticAI&lt;/a&gt; (advanced agent framework)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt; (unified interface for 100+ LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/squidfunk/mkdocs-material"&gt;MkDocs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mkdocstrings/griffe"&gt;Griffe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; and &lt;a href="https://github.com/astral-sh/ruff"&gt;ruff&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We're committed to continuing to build the Agents SDK as an open source framework so others in the community can expand on our approach.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sentient-agi/ROMA</title>
      <link>https://github.com/sentient-agi/ROMA</link>
      <description>&lt;p&gt;Recursive-Open-Meta-Agent v0.1 (Beta). A meta-agent framework to build high-performance multi-agent systems.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/sentient-logo-new-M.png" alt="alt text" width="60%" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;ROMA: Recursive Open Meta-Agents&lt;/h1&gt; 
 &lt;p align="center"&gt; &lt;strong&gt;Building hierarchical high-performance multi-agent systems made easy! (Beta) &lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14848" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14848" alt="sentient-agi%2FROMA | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://sentient.xyz/" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Homepage" src="https://img.shields.io/badge/Sentient-Homepage-%23EAEAEA?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%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%2BPC9zdmc%2B&amp;amp;link=https%3A%2F%2Fhuggingface.co%2FSentientagi" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://github.com/sentient-agi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="GitHub" src="https://img.shields.io/badge/Github-sentient_agi-181717?logo=github" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://huggingface.co/Sentientagi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SentientAGI-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;/div&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://discord.gg/sentientfoundation" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord-SentientAGI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;a href="https://x.com/SentientAGI" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/badge/-SentientAGI-grey?logo=x&amp;amp;link=https%3A%2F%2Fx.com%2FSentientAGI%2F" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.sentient.xyz/blog/recursive-open-meta-agent"&gt;Technical Blog&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/"&gt;Paper (Coming soon)&lt;/a&gt; â€¢ &lt;a href="https://www.sentient.xyz/"&gt;Build Agents for $$$&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt;  
&lt;h2&gt;ğŸ“– Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/INTRODUCTION.md"&gt;ğŸš€ Introduction&lt;/a&gt;&lt;/strong&gt; - Understand the vision and architecture behind ROMA&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;ğŸ“¦ Setup&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;ğŸ¤– Agents Guide&lt;/a&gt;&lt;/strong&gt; - Learn how to create and customize your own agents&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/CONFIGURATION.md"&gt;âš™ï¸ Configuration&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/ROADMAP.md"&gt;ğŸ—ºï¸ Roadmap&lt;/a&gt;&lt;/strong&gt; - See what's coming next for ROMA&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¯ What is ROMA?&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/roma_run.gif" alt="alt text" width="80%" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; is a &lt;strong&gt;meta-agent framework&lt;/strong&gt; that uses recursive hierarchical structures to solve complex problems. By breaking down tasks into parallelizable components, ROMA enables agents to tackle sophisticated reasoning challenges while maintaining transparency that makes context-engineering and iteration straightforward. The framework offers &lt;strong&gt;parallel problem solving&lt;/strong&gt; where agents work simultaneously on different parts of complex tasks, &lt;strong&gt;transparent development&lt;/strong&gt; with a clear structure for easy debugging, and &lt;strong&gt;proven performance&lt;/strong&gt; demonstrated through our search agent's strong benchmark results. We've shown the framework's effectiveness, but this is just the beginning. As an &lt;strong&gt;open-source and extensible&lt;/strong&gt; platform, ROMA is designed for community-driven development, allowing you to build and customize agents for your specific needs while benefiting from the collective improvements of the community.&lt;/p&gt; 
&lt;h2&gt;ğŸ—ï¸ How It Works&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; framework processes tasks through a recursive &lt;strong&gt;planâ€“execute loop&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def solve(task):
    if is_atomic(task):                 # Step 1: Atomizer
        return execute(task)            # Step 2: Executor
    else:
        subtasks = plan(task)           # Step 2: Planner
        results = []
        for subtask in subtasks:
            results.append(solve(subtask))  # Recursive call
        return aggregate(results)       # Step 3: Aggregator

# Entry point:
answer = solve(initial_request)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Atomizer&lt;/strong&gt; â€“ Decides whether a request is &lt;strong&gt;atomic&lt;/strong&gt; (directly executable) or requires &lt;strong&gt;planning&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Planner&lt;/strong&gt; â€“ If planning is needed, the task is broken into smaller &lt;strong&gt;subtasks&lt;/strong&gt;. Each subtask is fed back into the &lt;strong&gt;Atomizer&lt;/strong&gt;, making the process recursive.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Executors&lt;/strong&gt; â€“ Handle atomic tasks. Executors can be &lt;strong&gt;LLMs, APIs, or even other agents&lt;/strong&gt; â€” as long as they implement an &lt;code&gt;agent.execute()&lt;/code&gt; interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aggregator&lt;/strong&gt; â€“ Collects and integrates results from subtasks. Importantly, the Aggregator produces the &lt;strong&gt;answer to the original parent task&lt;/strong&gt;, not just raw child outputs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;ğŸ“ Information Flow&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Top-down:&lt;/strong&gt; Tasks are decomposed into subtasks recursively.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bottom-up:&lt;/strong&gt; Subtask results are aggregated upwards into solutions for parent tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Left-to-right:&lt;/strong&gt; If a subtask depends on the output of a previous one, it waits until that subtask completes before execution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This structure makes the system flexible, recursive, and dependency-aware â€” capable of decomposing complex problems into smaller steps while ensuring results are integrated coherently.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view the system flow diagram&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TB
    A[Your Request] --&amp;gt; B{Atomizer}
    B --&amp;gt;|Plan Needed| C[Planner]
    B --&amp;gt;|Atomic Task| D[Executor]

    %% Planner spawns subtasks
    C --&amp;gt; E[Subtasks]
    E --&amp;gt; G[Aggregator]

    %% Recursion
    E -.-&amp;gt; B  

    %% Execution + Aggregation
    D --&amp;gt; F[Final Result]
    G --&amp;gt; F

    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#d1c4e9
    style G fill:#c5cae9

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt;
&lt;br /&gt; 
&lt;h3&gt;ğŸš€ 30-Second Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sentient-agi/ROMA.git
cd ROMA

# Run the automated setup
./setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Choose between:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Setup&lt;/strong&gt; (Recommended) - One-command setup with isolation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native Setup&lt;/strong&gt; - Direct installation for development&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ› ï¸ Technical Stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: Built on &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/your/agnoagents%5D(https://github.com/agno-agi/agno)"&gt;AgnoAgents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Python 3.12+ with FastAPI/Flask&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + TypeScript with real-time WebSocket&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Support&lt;/strong&gt;: Any provider via LiteLLM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Persistence&lt;/strong&gt;: Enterprise S3 mounting with security validation 
  &lt;ul&gt; 
   &lt;li&gt;ğŸ”’ &lt;strong&gt;goofys FUSE mounting&lt;/strong&gt; for zero-latency file access&lt;/li&gt; 
   &lt;li&gt;ğŸ›¡ï¸ &lt;strong&gt;Path injection protection&lt;/strong&gt; with comprehensive validation&lt;/li&gt; 
   &lt;li&gt;ğŸ” &lt;strong&gt;AWS credentials verification&lt;/strong&gt; before operations&lt;/li&gt; 
   &lt;li&gt;ğŸ“ &lt;strong&gt;Dynamic Docker Compose&lt;/strong&gt; with secure volume mounting&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Execution&lt;/strong&gt;: E2B sandboxes with unified S3 integration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Production-grade validation and error handling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt;: Multi-modal, tools, MCP, hooks, caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“¦ Installation Options&lt;/h2&gt; 
&lt;h3&gt;Quick Start (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Main setup (choose Docker or Native)
./setup.sh

# Optional: Setup E2B sandbox integration
./setup.sh --e2b

# Test E2B integration  
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Command Line Options&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./setup.sh --docker     # Run Docker setup directly
./setup.sh --docker-from-scratch  # Rebuild Docker images/containers from scratch (down -v, no cache)
./setup.sh --native     # Run native setup directly (macOS/Ubuntu/Debian)
./setup.sh --e2b        # Setup E2B template (requires E2B_API_KEY + AWS creds)
./setup.sh --test-e2b   # Test E2B template integration
./setup.sh --help       # Show all available options
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual Installation&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;setup docs&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h3&gt;ğŸ—ï¸ Optional: E2B Sandbox Integration&lt;/h3&gt; 
&lt;p&gt;For secure code execution capabilities, optionally set up E2B sandboxes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# After main setup, configure E2B (requires E2B_API_KEY and AWS credentials in .env)
./setup.sh --e2b

# Test E2B integration
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;E2B Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”’ &lt;strong&gt;Secure Code Execution&lt;/strong&gt; - Run untrusted code in isolated sandboxes&lt;/li&gt; 
 &lt;li&gt;â˜ï¸ &lt;strong&gt;S3 Integration&lt;/strong&gt; - Automatic data sync between local and sandbox environments&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;goofys Mounting&lt;/strong&gt; - High-performance S3 filesystem mounting&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;AWS Credentials&lt;/strong&gt; - Passed securely via Docker build arguments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤– Pre-built Agents&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; These agents are demonstrations built using ROMA's framework through simple vibe-prompting and minimal manual tuning. They showcase how easily you can create high-performance agents with ROMA, rather than being production-final solutions. Our mission is to empower the community to build, share, and get rewarded for creating innovative agent recipes and use-cases.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ROMA comes with example agents that demonstrate the framework's capabilities:&lt;/p&gt; 
&lt;h3&gt;ğŸ” General Task Solver&lt;/h3&gt; 
&lt;p&gt;A versatile agent powered by ChatGPT Search Preview for handling diverse tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Search&lt;/strong&gt;: Leverages OpenAI's latest search capabilities for real-time information&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Planning&lt;/strong&gt;: Adapts task decomposition based on query complexity&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Domain&lt;/strong&gt;: Handles everything from technical questions to creative projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quick Prototyping&lt;/strong&gt;: Perfect for testing ROMA's capabilities without domain-specific setup&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: General research, fact-checking, exploratory analysis, quick information gathering&lt;/p&gt; 
&lt;h3&gt;ğŸ”¬ Deep Research Agent&lt;/h3&gt; 
&lt;p&gt;A comprehensive research system that breaks down complex research questions into manageable sub-tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Task Decomposition&lt;/strong&gt;: Automatically splits research topics into search, analysis, and synthesis phases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Information Gathering&lt;/strong&gt;: Executes multiple searches simultaneously for faster results&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Source Integration&lt;/strong&gt;: Combines results from web search, Wikipedia, and specialized APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Synthesis&lt;/strong&gt;: Aggregates findings into coherent, well-structured reports&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Academic research, market analysis, competitive intelligence, technical documentation&lt;/p&gt; 
&lt;h3&gt;ğŸ’¹ Crypto Analytics Agent&lt;/h3&gt; 
&lt;p&gt;Specialized financial analysis agent with deep blockchain and DeFi expertise:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Market Data&lt;/strong&gt;: Integrates with Binance, CoinGecko, and DefiLlama APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-Chain Analytics&lt;/strong&gt;: Access to Arkham Intelligence for wallet tracking and token flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technical Analysis&lt;/strong&gt;: Advanced charting with OHLC data and market indicators&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeFi Metrics&lt;/strong&gt;: TVL tracking, yield analysis, protocol comparisons&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Execution&lt;/strong&gt;: Runs analysis in E2B sandboxes with data persistence&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Token research, portfolio analysis, DeFi protocol evaluation, market trend analysis&lt;/p&gt; 
&lt;p&gt;All three agents demonstrate ROMA's recursive architecture in action, showing how complex queries that would overwhelm single-pass systems can be elegantly decomposed and solved. They serve as templates and inspiration for building your own specialized agents.&lt;/p&gt; 
&lt;h3&gt;Your First Agent in 5 Minutes&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;./setup.sh  # Automated setup with Docker or native installation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access all the pre-defined agents through the frontend on &lt;code&gt;localhost:3000&lt;/code&gt; after setting up the backend on &lt;code&gt;localhost:5000&lt;/code&gt;. Please checkout &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;Setup&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;Agents guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/agent_customization.png" alt="alt text" width="60%" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Your first agent in 3 lines
from sentientresearchagent import SentientAgent

agent = SentientAgent.create()
result = await agent.run("Create a podcast about AI safety")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ“Š Benchmarks&lt;/h2&gt; 
&lt;p&gt;We evaluate our simple implementation of a search system using ROMA, called ROMA-Search across three benchmarks: &lt;strong&gt;SEAL-0&lt;/strong&gt;, &lt;strong&gt;FRAMES&lt;/strong&gt;, and &lt;strong&gt;SimpleQA&lt;/strong&gt;.&lt;br /&gt; Below are the performance graphs for each benchmark.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/vtllms/sealqa"&gt;SEAL-0&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;SealQA is a new challenging benchmark for evaluating Search-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/seal-0-full.001.jpeg" alt="SEAL-0 Results" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/google/frames-benchmark"&gt;FRAMES&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;A comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/FRAMES-full.001.jpeg" alt="FRAMES Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://openai.com/index/introducing-simpleqa/"&gt;SimpleQA&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;Factuality benchmark that measures the ability for language models to answer short, fact-seeking questions.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/simpleQAFull.001.jpeg" alt="SimpleQA Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;âœ¨ Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ”„ &lt;strong&gt;Recursive Task Decomposition&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Automatically breaks down complex tasks into manageable subtasks with intelligent dependency management. Runs independent sub-tasks in &lt;strong&gt;parallel&lt;/strong&gt;.&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ¤– &lt;strong&gt;Agent Agnostic&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Works with any provider (OpenAI, Anthropic, Google, local models) through unified interface, as long as it has an &lt;code&gt;agent.run()&lt;/code&gt; command, then you can use it!&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ” &lt;strong&gt;Complete Transparency&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Stage tracing shows exactly what happens at each step - debug and optimize with full visibility&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ”Œ Connect Any Tool&lt;/h3&gt; &lt;p&gt;Seamlessly integrate external tools and protocols with configurable intervention points. Already includes production-grade connectors such as E2B, file-read-write, and more.&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ™ Acknowledgments&lt;/h2&gt; 
&lt;p&gt;This framework would not have been possible if it wasn't for these amazing open-source contributions!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inspired by the hierarchical planning approach described in &lt;a href="https://arxiv.org/abs/2503.08275"&gt;"Beyond Outlining: Heterogeneous Recursive Planning"&lt;/a&gt; by Xiong et al.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt; - Data validation using Python type annotations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/agno-ai/agno%5D(https://github.com/agno-agi/agno)"&gt;Agno&lt;/a&gt; - Framework for building AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/e2b-dev/e2b"&gt;E2B&lt;/a&gt; - Cloud runtime for AI agents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“š Citation&lt;/h2&gt; 
&lt;p&gt;If you use the ROMA repo in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{al_zubi_2025_17052592,
  author       = {Al-Zubi, Salah and
                  Nama, Baran and
                  Kaz, Arda and
                  Oh, Sewoong},
  title        = {SentientResearchAgent: A Hierarchical AI Agent
                   Framework for Research and Analysis
                  },
  month        = sep,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {ROMA},
  doi          = {10.5281/zenodo.17052592},
  url          = {https://doi.org/10.5281/zenodo.17052592},
  swhid        = {swh:1:dir:69cd1552103e0333dd0c39fc4f53cb03196017ce
                   ;origin=https://doi.org/10.5281/zenodo.17052591;vi
                   sit=swh:1:snp:f50bf99634f9876adb80c027361aec9dff97
                   3433;anchor=swh:1:rel:afa7caa843ce1279f5b4b29b5d3d
                   5e3fe85edc95;path=salzubi401-ROMA-b31c382
                  },
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸŒŸ Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#sentient-agi/roma&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=sentient-agi/roma&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>isaac-sim/IsaacLab</title>
      <link>https://github.com/isaac-sim/IsaacLab</link>
      <description>&lt;p&gt;Unified framework for robot learning built on NVIDIA Isaac Sim&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/source/_static/isaaclab.jpg" alt="Isaac Lab" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Isaac Lab&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://docs.isaacsim.omniverse.nvidia.com/latest/index.html"&gt;&lt;img src="https://img.shields.io/badge/IsaacSim-5.0.0-silver.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://docs.python.org/3/whatsnew/3.11.html"&gt;&lt;img src="https://img.shields.io/badge/python-3.11-blue.svg?sanitize=true" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://releases.ubuntu.com/22.04/"&gt;&lt;img src="https://img.shields.io/badge/platform-linux--64-orange.svg?sanitize=true" alt="Linux platform" /&gt;&lt;/a&gt; &lt;a href="https://www.microsoft.com/en-us/"&gt;&lt;img src="https://img.shields.io/badge/platform-windows--64-orange.svg?sanitize=true" alt="Windows platform" /&gt;&lt;/a&gt; &lt;a href="https://github.com/isaac-sim/IsaacLab/actions/workflows/pre-commit.yaml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/pre-commit.yaml?logo=pre-commit&amp;amp;logoColor=white&amp;amp;label=pre-commit&amp;amp;color=brightgreen" alt="pre-commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/isaac-sim/IsaacLab/actions/workflows/docs.yaml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/docs.yaml?label=docs&amp;amp;color=brightgreen" alt="docs status" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/BSD-3-Clause"&gt;&lt;img src="https://img.shields.io/badge/license-BSD--3-yellow.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/license/apache-2-0"&gt;&lt;img src="https://img.shields.io/badge/license-Apache--2.0-yellow.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Isaac Lab&lt;/strong&gt; is a GPU-accelerated, open-source framework designed to unify and simplify robotics research workflows, such as reinforcement learning, imitation learning, and motion planning. Built on &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/latest/index.html"&gt;NVIDIA Isaac Sim&lt;/a&gt;, it combines fast and accurate physics and sensor simulation, making it an ideal choice for sim-to-real transfer in robotics.&lt;/p&gt; 
&lt;p&gt;Isaac Lab provides developers with a range of essential features for accurate sensor simulation, such as RTX-based cameras, LIDAR, or contact sensors. The framework's GPU acceleration enables users to run complex simulations and computations faster, which is key for iterative processes like reinforcement learning and data-intensive tasks. Moreover, Isaac Lab can run locally or be distributed across the cloud, offering flexibility for large-scale deployments.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;Isaac Lab offers a comprehensive set of tools and environments designed to facilitate robot learning:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robots&lt;/strong&gt;: A diverse collection of robots, from manipulators, quadrupeds, to humanoids, with 16 commonly available models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environments&lt;/strong&gt;: Ready-to-train implementations of more than 30 environments, which can be trained with popular reinforcement learning frameworks such as RSL RL, SKRL, RL Games, or Stable Baselines. We also support multi-agent reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Physics&lt;/strong&gt;: Rigid bodies, articulated systems, deformable objects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sensors&lt;/strong&gt;: RGB/depth/segmentation cameras, camera annotations, IMU, contact sensors, ray casters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;Our &lt;a href="https://isaac-sim.github.io/IsaacLab"&gt;documentation page&lt;/a&gt; provides everything you need to get started, including detailed tutorials and step-by-step guides. Follow these links to learn more about:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/index.html#local-installation"&gt;Installation steps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/overview/reinforcement-learning/rl_existing_scripts.html"&gt;Reinforcement learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/tutorials/index.html"&gt;Tutorials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html"&gt;Available environments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Isaac Sim Version Dependency&lt;/h2&gt; 
&lt;p&gt;Isaac Lab is built on top of Isaac Sim and requires specific versions of Isaac Sim that are compatible with each release of Isaac Lab. Below, we outline the recent Isaac Lab releases and GitHub branches and their corresponding dependency versions for Isaac Sim.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Isaac Lab Version&lt;/th&gt; 
   &lt;th&gt;Isaac Sim Version&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;main&lt;/code&gt; branch&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5 / 5.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.2.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5 / 5.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.1.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.0.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contributing to Isaac Lab&lt;/h2&gt; 
&lt;p&gt;We wholeheartedly welcome contributions from the community to make this framework mature and useful for everyone. These may happen as bug reports, feature requests, or code contributions. For details, please check our &lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/refs/contributing.html"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Show &amp;amp; Tell: Share Your Inspiration&lt;/h2&gt; 
&lt;p&gt;We encourage you to utilize our &lt;a href="https://github.com/isaac-sim/IsaacLab/discussions/categories/show-and-tell"&gt;Show &amp;amp; Tell&lt;/a&gt; area in the &lt;code&gt;Discussions&lt;/code&gt; section of this repository. This space is designed for you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Share the tutorials you've created&lt;/li&gt; 
 &lt;li&gt;Showcase your learning content&lt;/li&gt; 
 &lt;li&gt;Present exciting projects you've developed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By sharing your work, you'll inspire others and contribute to the collective knowledge of our community. Your contributions can spark new ideas and collaborations, fostering innovation in robotics and simulation.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;Please see the &lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/refs/troubleshooting.html"&gt;troubleshooting&lt;/a&gt; section for common fixes or &lt;a href="https://github.com/isaac-sim/IsaacLab/issues"&gt;submit an issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For issues related to Isaac Sim, we recommend checking its &lt;a href="https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/overview.html"&gt;documentation&lt;/a&gt; or opening a question on its &lt;a href="https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/67"&gt;forums&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Please use GitHub &lt;a href="https://github.com/isaac-sim/IsaacLab/discussions"&gt;Discussions&lt;/a&gt; for discussing ideas, asking questions, and requests for new features.&lt;/li&gt; 
 &lt;li&gt;Github &lt;a href="https://github.com/isaac-sim/IsaacLab/issues"&gt;Issues&lt;/a&gt; should only be used to track executable pieces of work with a definite scope and a clear deliverable. These can be fixing bugs, documentation issues, new features, or general updates.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Connect with the NVIDIA Omniverse Community&lt;/h2&gt; 
&lt;p&gt;Do you have a project or resource you'd like to share more widely? We'd love to hear from you! Reach out to the NVIDIA Omniverse Community team at &lt;a href="mailto:OmniverseCommunity@nvidia.com"&gt;OmniverseCommunity@nvidia.com&lt;/a&gt; to explore opportunities to spotlight your work.&lt;/p&gt; 
&lt;p&gt;You can also join the conversation on the &lt;a href="https://discord.com/invite/nvidiaomniverse"&gt;Omniverse Discord&lt;/a&gt; to connect with other developers, share your projects, and help grow a vibrant, collaborative ecosystem where creativity and technology intersect. Your contributions can make a meaningful impact on the Isaac Lab community and beyond!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The Isaac Lab framework is released under &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/LICENSE"&gt;BSD-3 License&lt;/a&gt;. The &lt;code&gt;isaaclab_mimic&lt;/code&gt; extension and its corresponding standalone scripts are released under &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/LICENSE-mimic"&gt;Apache 2.0&lt;/a&gt;. The license files of its dependencies and assets are present in the &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses"&gt;&lt;code&gt;docs/licenses&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; 
&lt;p&gt;Note that Isaac Lab requires Isaac Sim, which includes components under proprietary licensing terms. Please see the &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses/dependencies/isaacsim-license.txt"&gt;Isaac Sim license&lt;/a&gt; for information on Isaac Sim licensing.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;isaaclab_mimic&lt;/code&gt; extension requires cuRobo, which has proprietary licensing terms that can be found in &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses/dependencies/cuRobo-license.txt"&gt;&lt;code&gt;docs/licenses/dependencies/cuRobo-license.txt&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Isaac Lab development initiated from the &lt;a href="https://isaac-orbit.github.io/"&gt;Orbit&lt;/a&gt; framework. We would appreciate if you would cite it in academic publications as well:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{mittal2023orbit,
   author={Mittal, Mayank and Yu, Calvin and Yu, Qinxi and Liu, Jingzhou and Rudin, Nikita and Hoeller, David and Yuan, Jia Lin and Singh, Ritvik and Guo, Yunrong and Mazhar, Hammad and Mandlekar, Ajay and Babich, Buck and State, Gavriel and Hutter, Marco and Garg, Animesh},
   journal={IEEE Robotics and Automation Letters},
   title={Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments},
   year={2023},
   volume={8},
   number={6},
   pages={3740-3747},
   doi={10.1109/LRA.2023.3270034}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>onyx-dot-app/onyx</title>
      <link>https://github.com/onyx-dot-app/onyx</link>
      <description>&lt;p&gt;Open Source AI Platform - AI Chat with advanced features that works with every LLM&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2 align="center"&gt; &lt;a href="https://www.onyx.app/"&gt; &lt;img width="50%" src="https://github.com/onyx-dot-app/onyx/raw/logo/OnyxLogoCropped.jpg?raw=true)" /&gt;&lt;/a&gt; &lt;/h2&gt; 
&lt;p align="center"&gt;Open Source AI Platform&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://discord.gg/TDJ59cGV2X" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/discord-join-blue.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt; &lt;/a&gt; &lt;a href="https://docs.onyx.app/" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/docs-view-blue" alt="Documentation" /&gt; &lt;/a&gt; &lt;a href="https://docs.onyx.app/" target="_blank"&gt; &lt;img src="https://img.shields.io/website?url=https://www.onyx.app&amp;amp;up_message=visit&amp;amp;up_color=blue" alt="Documentation" /&gt; &lt;/a&gt; &lt;a href="https://github.com/onyx-dot-app/onyx/raw/main/LICENSE" target="_blank"&gt; &lt;img src="https://img.shields.io/static/v1?label=license&amp;amp;message=MIT&amp;amp;color=blue" alt="License" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.onyx.app/"&gt;Onyx&lt;/a&gt;&lt;/strong&gt; is a feature-rich, self-hostable Chat UI that works with any LLM. It is easy to deploy and can run in a completely airgapped environment.&lt;/p&gt; 
&lt;p&gt;Onyx comes loaded with advanced features like Agents, Web Search, RAG, MCP, Deep Research, Connectors to 40+ knowledge sources, and more.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Run Onyx with one command (or see deployment section below):&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;curl -fsSL https://raw.githubusercontent.com/onyx-dot-app/onyx/main/deployment/docker_compose/install.sh &amp;gt; install.sh &amp;amp;&amp;amp; chmod +x install.sh &amp;amp;&amp;amp; ./install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;img src="https://github.com/onyx-dot-app/onyx/releases/download/v0.21.1/OnyxChatSilentDemo.gif" alt="Onyx Chat Silent Demo" /&gt;&lt;/p&gt; 
&lt;h2&gt;â­ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– Custom Agents:&lt;/strong&gt; Build AI Agents with unique instructions, knowledge and actions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸŒ Web Search:&lt;/strong&gt; Browse the web with Google PSE, Exa, and Serper as well as an in-house scraper or Firecrawl.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” RAG:&lt;/strong&gt; Best in class hybrid-search + knowledge graph for uploaded files and ingested documents from connectors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”„ Connectors:&lt;/strong&gt; Pull knowledge, metadata, and access information from over 40 applications.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”¬ Deep Research:&lt;/strong&gt; Get in depth answers with an agentic multi-step search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;â–¶ï¸ Actions &amp;amp; MCP:&lt;/strong&gt; Give AI Agents the ability to interact with external systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ’» Code Interpreter:&lt;/strong&gt; Execute code to analyze data, render graphs and create files.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¨ Image Generation:&lt;/strong&gt; Generate images based on user prompts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ‘¥ Collaboration:&lt;/strong&gt; Chat sharing, feedback gathering, user management, usage analytics, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Onyx works with all LLMs (like OpenAI, Anthropic, Gemini, etc.) and self-hosted LLMs (like Ollama, vLLM, etc.)&lt;/p&gt; 
&lt;p&gt;To learn more about the features, check out our &lt;a href="https://docs.onyx.app/welcome"&gt;documentation&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Deployment&lt;/h2&gt; 
&lt;p&gt;Onyx supports deployments in Docker, Kubernetes, Terraform, along with guides for major cloud providers.&lt;/p&gt; 
&lt;p&gt;See guides below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.onyx.app/deployment/local/docker"&gt;Docker&lt;/a&gt; or &lt;a href="https://docs.onyx.app/deployment/getting_started/quickstart"&gt;Quickstart&lt;/a&gt; (best for most users)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.onyx.app/deployment/local/kubernetes"&gt;Kubernetes&lt;/a&gt; (best for large teams)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.onyx.app/deployment/local/terraform"&gt;Terraform&lt;/a&gt; (best for teams already using Terraform)&lt;/li&gt; 
 &lt;li&gt;Cloud specific guides (best if specifically using &lt;a href="https://docs.onyx.app/deployment/cloud/aws/eks"&gt;AWS EKS&lt;/a&gt;, &lt;a href="https://docs.onyx.app/deployment/cloud/azure"&gt;Azure VMs&lt;/a&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;br /&gt; &lt;strong&gt;To try Onyx for free without deploying, check out &lt;a href="https://cloud.onyx.app/signup"&gt;Onyx Cloud&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ” Other Notable Benefits&lt;/h2&gt; 
&lt;p&gt;Onyx is built for teams of all sizes, from individual users to the largest global enterprises.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise Search&lt;/strong&gt;: far more than simple RAG, Onyx has custom indexing and retrieval that remains performant and accurate for scales of up to tens of millions of documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: SSO (OIDC/SAML/OAuth2), RBAC, encryption of credentials, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Management UI&lt;/strong&gt;: different user roles such as basic, curator, and admin.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Permissioning&lt;/strong&gt;: mirrors user access from external apps for RAG use cases.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš§ Roadmap&lt;/h2&gt; 
&lt;p&gt;To see ongoing and upcoming projects, check out our &lt;a href="https://github.com/orgs/onyx-dot-app/projects/2"&gt;roadmap&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ğŸ“š Licensing&lt;/h2&gt; 
&lt;p&gt;There are two editions of Onyx:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Onyx Community Edition (CE) is available freely under the MIT license.&lt;/li&gt; 
 &lt;li&gt;Onyx Enterprise Edition (EE) includes extra features that are primarily useful for larger organizations. For feature details, check out &lt;a href="https://www.onyx.app/pricing"&gt;our website&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‘ª Community&lt;/h2&gt; 
&lt;p&gt;Join our open source community on &lt;strong&gt;&lt;a href="https://discord.gg/TDJ59cGV2X"&gt;Discord&lt;/a&gt;&lt;/strong&gt;!&lt;/p&gt; 
&lt;h2&gt;ğŸ’¡ Contributing&lt;/h2&gt; 
&lt;p&gt;Looking to contribute? Please check out the &lt;a href="https://raw.githubusercontent.com/onyx-dot-app/onyx/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-agent-sdk-python</title>
      <link>https://github.com/anthropics/claude-agent-sdk-python</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Agent SDK for Python&lt;/h1&gt; 
&lt;p&gt;Python SDK for Claude Agent. See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/sdk/sdk-python"&gt;Claude Agent SDK documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install claude-agent-sdk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Node.js&lt;/li&gt; 
 &lt;li&gt;Claude Code 2.0.0+: &lt;code&gt;npm install -g @anthropic-ai/claude-code&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import anyio
from claude_agent_sdk import query

async def main():
    async for message in query(prompt="What is 2 + 2?"):
        print(message)

anyio.run(main)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage: query()&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;query()&lt;/code&gt; is an async function for querying Claude Code. It returns an &lt;code&gt;AsyncIterator&lt;/code&gt; of response messages. See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/query.py"&gt;src/claude_agent_sdk/query.py&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage, TextBlock

# Simple query
async for message in query(prompt="Hello Claude"):
    if isinstance(message, AssistantMessage):
        for block in message.content:
            if isinstance(block, TextBlock):
                print(block.text)

# With options
options = ClaudeAgentOptions(
    system_prompt="You are a helpful assistant",
    max_turns=1
)

async for message in query(prompt="Tell me a joke", options=options):
    print(message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Tools&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;options = ClaudeAgentOptions(
    allowed_tools=["Read", "Write", "Bash"],
    permission_mode='acceptEdits'  # auto-accept file edits
)

async for message in query(
    prompt="Create a hello.py file",
    options=options
):
    # Process tool use and results
    pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Working Directory&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path

options = ClaudeAgentOptions(
    cwd="/path/to/project"  # or Path("/path/to/project")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ClaudeSDKClient&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;ClaudeSDKClient&lt;/code&gt; supports bidirectional, interactive conversations with Claude Code. See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/client.py"&gt;src/claude_agent_sdk/client.py&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unlike &lt;code&gt;query()&lt;/code&gt;, &lt;code&gt;ClaudeSDKClient&lt;/code&gt; additionally enables &lt;strong&gt;custom tools&lt;/strong&gt; and &lt;strong&gt;hooks&lt;/strong&gt;, both of which can be defined as Python functions.&lt;/p&gt; 
&lt;h3&gt;Custom Tools (as In-Process SDK MCP Servers)&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;custom tool&lt;/strong&gt; is a Python function that you can offer to Claude, for Claude to invoke as needed.&lt;/p&gt; 
&lt;p&gt;Custom tools are implemented in-process MCP servers that run directly within your Python application, eliminating the need for separate processes that regular MCP servers require.&lt;/p&gt; 
&lt;p&gt;For an end-to-end example, see &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/mcp_calculator.py"&gt;MCP Calculator&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Creating a Simple Tool&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import tool, create_sdk_mcp_server, ClaudeAgentOptions, ClaudeSDKClient

# Define a tool using the @tool decorator
@tool("greet", "Greet a user", {"name": str})
async def greet_user(args):
    return {
        "content": [
            {"type": "text", "text": f"Hello, {args['name']}!"}
        ]
    }

# Create an SDK MCP server
server = create_sdk_mcp_server(
    name="my-tools",
    version="1.0.0",
    tools=[greet_user]
)

# Use it with Claude
options = ClaudeAgentOptions(
    mcp_servers={"tools": server},
    allowed_tools=["mcp__tools__greet"]
)

async with ClaudeSDKClient(options=options) as client:
    await client.query("Greet Alice")

    # Extract and print response
    async for msg in client.receive_response():
        print(msg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Benefits Over External MCP Servers&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No subprocess management&lt;/strong&gt; - Runs in the same process as your application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better performance&lt;/strong&gt; - No IPC overhead for tool calls&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simpler deployment&lt;/strong&gt; - Single Python process instead of multiple&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier debugging&lt;/strong&gt; - All code runs in the same process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type safety&lt;/strong&gt; - Direct Python function calls with type hints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Migration from External Servers&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# BEFORE: External MCP server (separate process)
options = ClaudeAgentOptions(
    mcp_servers={
        "calculator": {
            "type": "stdio",
            "command": "python",
            "args": ["-m", "calculator_server"]
        }
    }
)

# AFTER: SDK MCP server (in-process)
from my_tools import add, subtract  # Your tool functions

calculator = create_sdk_mcp_server(
    name="calculator",
    tools=[add, subtract]
)

options = ClaudeAgentOptions(
    mcp_servers={"calculator": calculator}
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Mixed Server Support&lt;/h4&gt; 
&lt;p&gt;You can use both SDK and external MCP servers together:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;options = ClaudeAgentOptions(
    mcp_servers={
        "internal": sdk_server,      # In-process SDK server
        "external": {                # External subprocess server
            "type": "stdio",
            "command": "external-server"
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hooks&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;hook&lt;/strong&gt; is a Python function that the Claude Code &lt;em&gt;application&lt;/em&gt; (&lt;em&gt;not&lt;/em&gt; Claude) invokes at specific points of the Claude agent loop. Hooks can provide deterministic processing and automated feedback for Claude. Read more in &lt;a href="https://docs.anthropic.com/en/docs/claude-code/hooks"&gt;Claude Code Hooks Reference&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more examples, see examples/hooks.py.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import ClaudeAgentOptions, ClaudeSDKClient, HookMatcher

async def check_bash_command(input_data, tool_use_id, context):
    tool_name = input_data["tool_name"]
    tool_input = input_data["tool_input"]
    if tool_name != "Bash":
        return {}
    command = tool_input.get("command", "")
    block_patterns = ["foo.sh"]
    for pattern in block_patterns:
        if pattern in command:
            return {
                "hookSpecificOutput": {
                    "hookEventName": "PreToolUse",
                    "permissionDecision": "deny",
                    "permissionDecisionReason": f"Command contains invalid pattern: {pattern}",
                }
            }
    return {}

options = ClaudeAgentOptions(
    allowed_tools=["Bash"],
    hooks={
        "PreToolUse": [
            HookMatcher(matcher="Bash", hooks=[check_bash_command]),
        ],
    }
)

async with ClaudeSDKClient(options=options) as client:
    # Test 1: Command with forbidden pattern (will be blocked)
    await client.query("Run the bash command: ./foo.sh --help")
    async for msg in client.receive_response():
        print(msg)

    print("\n" + "=" * 50 + "\n")

    # Test 2: Safe command that should work
    await client.query("Run the bash command: echo 'Hello from hooks example!'")
    async for msg in client.receive_response():
        print(msg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Types&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/types.py"&gt;src/claude_agent_sdk/types.py&lt;/a&gt; for complete type definitions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ClaudeAgentOptions&lt;/code&gt; - Configuration options&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AssistantMessage&lt;/code&gt;, &lt;code&gt;UserMessage&lt;/code&gt;, &lt;code&gt;SystemMessage&lt;/code&gt;, &lt;code&gt;ResultMessage&lt;/code&gt; - Message types&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TextBlock&lt;/code&gt;, &lt;code&gt;ToolUseBlock&lt;/code&gt;, &lt;code&gt;ToolResultBlock&lt;/code&gt; - Content blocks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Error Handling&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import (
    ClaudeSDKError,      # Base error
    CLINotFoundError,    # Claude Code not installed
    CLIConnectionError,  # Connection issues
    ProcessError,        # Process failed
    CLIJSONDecodeError,  # JSON parsing issues
)

try:
    async for message in query(prompt="Hello"):
        pass
except CLINotFoundError:
    print("Please install Claude Code")
except ProcessError as e:
    print(f"Process failed with exit code: {e.exit_code}")
except CLIJSONDecodeError as e:
    print(f"Failed to parse response: {e}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/_errors.py"&gt;src/claude_agent_sdk/_errors.py&lt;/a&gt; for all error types.&lt;/p&gt; 
&lt;h2&gt;Available Tools&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/settings#tools-available-to-claude"&gt;Claude Code documentation&lt;/a&gt; for a complete list of available tools.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/quick_start.py"&gt;examples/quick_start.py&lt;/a&gt; for a complete working example.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/streaming_mode.py"&gt;examples/streaming_mode.py&lt;/a&gt; for comprehensive examples involving &lt;code&gt;ClaudeSDKClient&lt;/code&gt;. You can even run interactive examples in IPython from &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/streaming_mode_ipython.py"&gt;examples/streaming_mode_ipython.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Migrating from Claude Code SDK&lt;/h2&gt; 
&lt;p&gt;If you're upgrading from the Claude Code SDK (versions &amp;lt; 0.1.0), please see the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/CHANGELOG.md#010"&gt;CHANGELOG.md&lt;/a&gt; for details on breaking changes and new features, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ClaudeCodeOptions&lt;/code&gt; â†’ &lt;code&gt;ClaudeAgentOptions&lt;/code&gt; rename&lt;/li&gt; 
 &lt;li&gt;Merged system prompt configuration&lt;/li&gt; 
 &lt;li&gt;Settings isolation and explicit control&lt;/li&gt; 
 &lt;li&gt;New programmatic subagents and session forking features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>simular-ai/Agent-S</title>
      <link>https://github.com/simular-ai/Agent-S</link>
      <description>&lt;p&gt;Agent S: an open agentic framework that uses computers like a human&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s.png" alt="Logo" style="vertical-align:middle" width="60" /&gt; Agent S: &lt;small&gt;Use Computer Like a Human&lt;/small&gt; &lt;/h1&gt; 
&lt;p align="center"&gt;&amp;nbsp; ğŸŒ &lt;a href="https://www.simular.ai/articles/agent-s3"&gt;[S3 blog]&lt;/a&gt;&amp;nbsp; ğŸ“„ &lt;a href="https://arxiv.org/abs/2510.02250"&gt;[S3 Paper]&lt;/a&gt;&amp;nbsp; ğŸ¥ &lt;a href="https://www.youtube.com/watch?v=VHr0a3UBsh4"&gt;[S3 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&amp;nbsp; ğŸŒ &lt;a href="https://www.simular.ai/articles/agent-s2-technical-review"&gt;[S2 blog]&lt;/a&gt;&amp;nbsp; ğŸ“„ &lt;a href="https://arxiv.org/abs/2504.00906"&gt;[S2 Paper (COLM 2025)]&lt;/a&gt;&amp;nbsp; ğŸ¥ &lt;a href="https://www.youtube.com/watch?v=wUGVQl7c0eg"&gt;[S2 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&amp;nbsp; ğŸŒ &lt;a href="https://www.simular.ai/agent-s"&gt;[S1 blog]&lt;/a&gt;&amp;nbsp; ğŸ“„ &lt;a href="https://arxiv.org/abs/2410.08164"&gt;[S1 Paper (ICLR 2025)]&lt;/a&gt;&amp;nbsp; ğŸ¥ &lt;a href="https://www.youtube.com/watch?v=OBDE3Knte0g"&gt;[S1 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&amp;nbsp; &lt;a href="https://trendshift.io/repositories/13151" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13151" alt="simular-ai%2FAgent-S | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/OS-Windows-blue?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt; &lt;img src="https://img.shields.io/badge/OS-macOS-black?logo=apple&amp;amp;logoColor=white" alt="macOS" /&gt; &lt;img src="https://img.shields.io/badge/OS-Linux-yellow?logo=linux&amp;amp;logoColor=black" alt="Linux" /&gt; &lt;a href="https://discord.gg/E2XfsK9fPV"&gt; &lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/E2XfsK9fPV?style=flat" alt="Discord" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://pepy.tech/projects/gui-agents"&gt; &lt;img src="https://static.pepy.tech/badge/gui-agents" alt="PyPI Downloads" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=de"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=es"&gt;EspaÃ±ol&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=fr"&gt;franÃ§ais&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=ja"&gt;æ—¥æœ¬èª&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=ko"&gt;í•œêµ­ì–´&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=pt"&gt;PortuguÃªs&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=ru"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=zh"&gt;ä¸­æ–‡&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  &amp;nbsp;&amp;nbsp; 
 &lt;p&gt;Skip the setup? Try Agent S in &lt;a href="https://cloud.simular.ai/"&gt;Simular Cloud&lt;/a&gt; &lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;ğŸ¥³ Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/10/02&lt;/strong&gt;: Released Agent S3 and its &lt;a href="https://arxiv.org/abs/2510.02250"&gt;technical paper&lt;/a&gt;, setting a new SOTA of &lt;strong&gt;69.9%&lt;/strong&gt; on OSWorld (approaching 72% human performance), with strong generalizability on WindowsAgentArena and AndroidWorld! It is also simpler, faster, and more flexible.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/08/01&lt;/strong&gt;: Agent S2.5 is released (gui-agents v0.2.5): simpler, better, and faster! New SOTA on &lt;a href="https://os-world.github.io"&gt;OSWorld-Verified&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/07/07&lt;/strong&gt;: The &lt;a href="https://arxiv.org/abs/2504.00906"&gt;Agent S2 paper&lt;/a&gt; is accepted to COLM 2025! See you in Montreal!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/04/27&lt;/strong&gt;: The Agent S paper won the Best Paper Award ğŸ† at ICLR 2025 Agentic AI for Science Workshop!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/04/01&lt;/strong&gt;: Released the &lt;a href="https://arxiv.org/abs/2504.00906"&gt;Agent S2 paper&lt;/a&gt; with new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/03/12&lt;/strong&gt;: Released Agent S2 along with v0.2.0 of &lt;a href="https://github.com/simular-ai/Agent-S"&gt;gui-agents&lt;/a&gt;, the new state-of-the-art for computer use agents (CUA), outperforming OpenAI's CUA/Operator and Anthropic's Claude 3.7 Sonnet Computer-Use!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/01/22&lt;/strong&gt;: The &lt;a href="https://arxiv.org/abs/2410.08164"&gt;Agent S paper&lt;/a&gt; is accepted to ICLR 2025!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/01/21&lt;/strong&gt;: Released v0.1.2 of &lt;a href="https://github.com/simular-ai/Agent-S"&gt;gui-agents&lt;/a&gt; library, with support for Linux and Windows!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2024/12/05&lt;/strong&gt;: Released v0.1.0 of &lt;a href="https://github.com/simular-ai/Agent-S"&gt;gui-agents&lt;/a&gt; library, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2024/10/10&lt;/strong&gt;: Released the &lt;a href="https://arxiv.org/abs/2410.08164"&gt;Agent S paper&lt;/a&gt; and codebase!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-introduction"&gt;ğŸ’¡ Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-current-results"&gt;ğŸ¯ Current Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#%EF%B8%8F-installation--setup"&gt;ğŸ› ï¸ Installation &amp;amp; Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-usage"&gt;ğŸš€ Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-acknowledgements"&gt;ğŸ¤ Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-citation"&gt;ğŸ’¬ Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ’¡ Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to &lt;strong&gt;Agent S&lt;/strong&gt;, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer.&lt;/p&gt; 
&lt;p&gt;Whether you're interested in AI, automation, or contributing to cutting-edge agent-based systems, we're excited to have you here!&lt;/p&gt; 
&lt;h2&gt;ğŸ¯ Current Results&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/s3_results.png" alt="Agent S3 Results" width="700" /&gt; &lt;/p&gt; 
&lt;p&gt;On OSWorld, Agent S3 alone reaches 62.6% in the 100-step setting, already exceeding the previous state of the art of 61.4% (Claude Sonnet 4.5). With the addition of Behavior Best-of-N, performance climbs even higher to 69.9%, bringing computer-use agents to within just a few points of human-level accuracy (72%).&lt;/p&gt; 
&lt;p&gt;Agent S3 also demonstrates strong zero-shot generalization. On WindowsAgentArena, accuracy rises from 50.2% using only Agent S3 to 56.6% by selecting from 3 rollouts. Similarly on AndroidWorld, performance improves from 68.1% to 71.6%&lt;/p&gt; 
&lt;h2&gt;ğŸ› ï¸ Installation &amp;amp; Setup&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Single Monitor&lt;/strong&gt;: Our agent is designed for single monitor screens&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: The agent runs Python code to control your computer - use with care&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Platforms&lt;/strong&gt;: Linux, Mac, and Windows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;To install Agent S3 without cloning the repository, run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install gui-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you would like to test Agent S3 while making changes, clone the repository and install using&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Don't forget to also &lt;code&gt;brew install tesseract&lt;/code&gt;! Pytesseract requires this extra installation to work.&lt;/p&gt; 
&lt;h3&gt;API Configuration&lt;/h3&gt; 
&lt;h4&gt;Option 1: Environment Variables&lt;/h4&gt; 
&lt;p&gt;Add to your &lt;code&gt;.bashrc&lt;/code&gt; (Linux) or &lt;code&gt;.zshrc&lt;/code&gt; (MacOS):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=&amp;lt;YOUR_API_KEY&amp;gt;
export ANTHROPIC_API_KEY=&amp;lt;YOUR_ANTHROPIC_API_KEY&amp;gt;
export HF_TOKEN=&amp;lt;YOUR_HF_TOKEN&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Python Script&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["OPENAI_API_KEY"] = "&amp;lt;YOUR_API_KEY&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;p&gt;We support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. See &lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md"&gt;models.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h3&gt;Grounding Models (Required)&lt;/h3&gt; 
&lt;p&gt;For optimal performance, we recommend &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;UI-TARS-1.5-7B&lt;/a&gt; hosted on Hugging Face Inference Endpoints or another provider. See &lt;a href="https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints"&gt;Hugging Face Inference Endpoints&lt;/a&gt; for setup instructions.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Usage&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš¡ï¸ &lt;strong&gt;Recommended Setup:&lt;/strong&gt;&lt;br /&gt; For the best configuration, we recommend using &lt;strong&gt;OpenAI gpt-5-2025-08-07&lt;/strong&gt; as the main model, paired with &lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt; for grounding.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;CLI&lt;/h3&gt; 
&lt;p&gt;Note, this is running Agent S3, our improved agent, without bBoN.&lt;/p&gt; 
&lt;p&gt;Run Agent S3 with the required parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;agent_s \
    --provider openai \
    --model gpt-5-2025-08-07 \
    --ground_provider huggingface \
    --ground_url http://localhost:8080 \
    --ground_model ui-tars-1.5-7b \
    --grounding_width 1920 \
    --grounding_height 1080
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Local Coding Environment (Optional)&lt;/h4&gt; 
&lt;p&gt;For tasks that require code execution (e.g., data processing, file manipulation, system automation), you can enable the local coding environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;agent_s \
    --provider openai \
    --model gpt-5-2025-08-07 \
    --ground_provider huggingface \
    --ground_url http://localhost:8080 \
    --ground_model ui-tars-1.5-7b \
    --grounding_width 1920 \
    --grounding_height 1080 \
    --enable_local_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;âš ï¸ &lt;strong&gt;WARNING&lt;/strong&gt;: The local coding environment executes arbitrary Python and Bash code locally on your machine. Only use this feature in trusted environments and with trusted inputs.&lt;/p&gt; 
&lt;h4&gt;Required Parameters&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--provider&lt;/code&gt;&lt;/strong&gt;: Main generation model provider (e.g., openai, anthropic, etc.) - Default: "openai"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/strong&gt;: Main generation model name (e.g., gpt-5-2025-08-07) - Default: "gpt-5-2025-08-07"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_provider&lt;/code&gt;&lt;/strong&gt;: The provider for the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_url&lt;/code&gt;&lt;/strong&gt;: The URL of the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_model&lt;/code&gt;&lt;/strong&gt;: The model name for the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--grounding_width&lt;/code&gt;&lt;/strong&gt;: Width of the output coordinate resolution from the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--grounding_height&lt;/code&gt;&lt;/strong&gt;: Height of the output coordinate resolution from the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Optional Parameters&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model_temperature&lt;/code&gt;&lt;/strong&gt;: The temperature to fix all model calls to (necessary to set to 1.0 for models like o3 but can be left blank for other models)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Grounding Model Dimensions&lt;/h4&gt; 
&lt;p&gt;The grounding width and height should match the output coordinate resolution of your grounding model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt;: Use &lt;code&gt;--grounding_width 1920 --grounding_height 1080&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI-TARS-72B&lt;/strong&gt;: Use &lt;code&gt;--grounding_width 1000 --grounding_height 1000&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Optional Parameters&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model_url&lt;/code&gt;&lt;/strong&gt;: Custom API URL for main generation model - Default: ""&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model_api_key&lt;/code&gt;&lt;/strong&gt;: API key for main generation model - Default: ""&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_api_key&lt;/code&gt;&lt;/strong&gt;: API key for grounding model endpoint - Default: ""&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--max_trajectory_length&lt;/code&gt;&lt;/strong&gt;: Maximum number of image turns to keep in trajectory - Default: 8&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--enable_reflection&lt;/code&gt;&lt;/strong&gt;: Enable reflection agent to assist the worker agent - Default: True&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--enable_local_env&lt;/code&gt;&lt;/strong&gt;: Enable local coding environment for code execution (WARNING: Executes arbitrary code locally) - Default: False&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Local Coding Environment Details&lt;/h4&gt; 
&lt;p&gt;The local coding environment enables Agent S3 to execute Python and Bash code directly on your machine. This is particularly useful for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Data Processing&lt;/strong&gt;: Manipulating spreadsheets, CSV files, or databases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Operations&lt;/strong&gt;: Bulk file processing, content extraction, or file organization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System Automation&lt;/strong&gt;: Configuration changes, system setup, or automation scripts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Development&lt;/strong&gt;: Writing, editing, or executing code files&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Processing&lt;/strong&gt;: Document manipulation, content editing, or formatting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When enabled, the agent can use the &lt;code&gt;call_code_agent&lt;/code&gt; action to execute code blocks for tasks that can be completed through programming rather than GUI interaction.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: The same Python interpreter used to run Agent S3 (automatically detected)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bash&lt;/strong&gt;: Available at &lt;code&gt;/bin/bash&lt;/code&gt; (standard on macOS and Linux)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System Permissions&lt;/strong&gt;: The agent runs with the same permissions as the user executing it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Security Considerations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The local environment executes arbitrary code with the same permissions as the user running the agent&lt;/li&gt; 
 &lt;li&gt;Only enable this feature in trusted environments&lt;/li&gt; 
 &lt;li&gt;Be cautious when the agent generates code for system-level operations&lt;/li&gt; 
 &lt;li&gt;Consider running in a sandboxed environment for untrusted tasks&lt;/li&gt; 
 &lt;li&gt;Bash scripts are executed with a 30-second timeout to prevent hanging processes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;code&gt;gui_agents&lt;/code&gt; SDK&lt;/h3&gt; 
&lt;p&gt;First, we import the necessary modules. &lt;code&gt;AgentS3&lt;/code&gt; is the main agent class for Agent S3. &lt;code&gt;OSWorldACI&lt;/code&gt; is our grounding agent that translates agent actions into executable python code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pyautogui
import io
from gui_agents.s3.agents.agent_s import AgentS3
from gui_agents.s3.agents.grounding import OSWorldACI
from gui_agents.s3.utils.local_env import LocalEnv  # Optional: for local coding environment

# Load in your API keys.
from dotenv import load_dotenv
load_dotenv()

current_platform = "linux"  # "darwin", "windows"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, we define our engine parameters. &lt;code&gt;engine_params&lt;/code&gt; is used for the main agent, and &lt;code&gt;engine_params_for_grounding&lt;/code&gt; is for grounding. For &lt;code&gt;engine_params_for_grounding&lt;/code&gt;, we support custom endpoints like HuggingFace TGI, vLLM, and Open Router.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;engine_params = {
  "engine_type": provider,
  "model": model,
  "base_url": model_url,           # Optional
  "api_key": model_api_key,        # Optional
  "temperature": model_temperature # Optional
}

# Load the grounding engine from a custom endpoint
ground_provider = "&amp;lt;your_ground_provider&amp;gt;"
ground_url = "&amp;lt;your_ground_url&amp;gt;"
ground_model = "&amp;lt;your_ground_model&amp;gt;"
ground_api_key = "&amp;lt;your_ground_api_key&amp;gt;"

# Set grounding dimensions based on your model's output coordinate resolution
# UI-TARS-1.5-7B: grounding_width=1920, grounding_height=1080
# UI-TARS-72B: grounding_width=1000, grounding_height=1000
grounding_width = 1920  # Width of output coordinate resolution
grounding_height = 1080  # Height of output coordinate resolution

engine_params_for_grounding = {
  "engine_type": ground_provider,
  "model": ground_model,
  "base_url": ground_url,
  "api_key": ground_api_key,  # Optional
  "grounding_width": grounding_width,
  "grounding_height": grounding_height,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, we define our grounding agent and Agent S3.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Optional: Enable local coding environment
enable_local_env = False  # Set to True to enable local code execution
local_env = LocalEnv() if enable_local_env else None

grounding_agent = OSWorldACI(
    env=local_env,  # Pass local_env for code execution capability
    platform=current_platform,
    engine_params_for_generation=engine_params,
    engine_params_for_grounding=engine_params_for_grounding,
    width=1920,  # Optional: screen width
    height=1080  # Optional: screen height
)

agent = AgentS3(
    engine_params,
    grounding_agent,
    platform=current_platform,
    max_trajectory_length=8,  # Optional: maximum image turns to keep
    enable_reflection=True     # Optional: enable reflection agent
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, let's query the agent!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Get screenshot.
screenshot = pyautogui.screenshot()
buffered = io.BytesIO() 
screenshot.save(buffered, format="PNG")
screenshot_bytes = buffered.getvalue()

obs = {
  "screenshot": screenshot_bytes,
}

instruction = "Close VS Code"
info, action = agent.predict(instruction=instruction, observation=obs)

exec(action[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;code&gt;gui_agents/s3/cli_app.py&lt;/code&gt; for more details on how the inference loop works.&lt;/p&gt; 
&lt;h3&gt;OSWorld&lt;/h3&gt; 
&lt;p&gt;To deploy Agent S3 in OSWorld, follow the &lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/osworld_setup/s3/OSWorld.md"&gt;OSWorld Deployment instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ’¬ Citations&lt;/h2&gt; 
&lt;p&gt;If you find this codebase useful, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{Agent-S3,
      title={The Unreasonable Effectiveness of Scaling Agents for Computer Use}, 
      author={Gonzalo Gonzalez-Pumariega and Vincent Tu and Chih-Lun Lee and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2510.02250},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.02250}, 
}

@misc{Agent-S2,
      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, 
      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2504.00906},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.00906}, 
}

@inproceedings{Agent-S,
    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},
    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2025},
    url={https://arxiv.org/abs/2410.08164}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#simular-ai/Agent-S&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=simular-ai/Agent-S&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trycua/cua</title>
      <link>https://github.com/trycua/cua</link>
      <description>&lt;p&gt;Open-source infrastructure for Computer-Use Agents. Sandboxes, SDKs, and benchmarks to train and evaluate AI agents that can control full desktops (macOS, Linux, Windows).&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" alt="Cua logo" height="150" srcset="img/logo_white.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" alt="Cua logo" height="150" srcset="img/logo_black.png" /&gt; 
  &lt;img alt="Cua logo" height="150" src="https://raw.githubusercontent.com/trycua/cua/main/img/logo_black.png" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#"&gt;&lt;img src="https://img.shields.io/badge/Python-333333?logo=python&amp;amp;logoColor=white&amp;amp;labelColor=333333" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#"&gt;&lt;img src="https://img.shields.io/badge/Swift-F05138?logo=swift&amp;amp;logoColor=white" alt="Swift" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#"&gt;&lt;img src="https://img.shields.io/badge/macOS-000000?logo=apple&amp;amp;logoColor=F0F0F0" alt="macOS" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/mVnXXpdE85"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://trendshift.io/repositories/13685" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13685" alt="trycua%2Fcua | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Weâ€™re hosting the &lt;strong&gt;Computer-Use Agents SOTA Challenge&lt;/strong&gt; at &lt;a href="https://hackthenorth.com"&gt;Hack the North&lt;/a&gt; and online!&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Track A (On-site @ UWaterloo)&lt;/strong&gt;: Reserved for participants accepted to Hack the North. ğŸ† Prize: &lt;strong&gt;YC interview guaranteed&lt;/strong&gt;.&lt;br /&gt; &lt;strong&gt;Track B (Remote)&lt;/strong&gt;: Open to everyone worldwide. ğŸ† Prize: &lt;strong&gt;Cash award&lt;/strong&gt;.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;ğŸ‘‰ Sign up here: &lt;a href="https://www.trycua.com/hackathon"&gt;trycua.com/hackathon&lt;/a&gt;&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;cua&lt;/strong&gt; ("koo-ah") is Docker for &lt;a href="https://www.oneusefulthing.org/p/when-you-give-a-claude-a-mouse"&gt;Computer-Use Agents&lt;/a&gt; - it enables AI agents to control full operating systems in virtual containers and deploy them locally or to the cloud.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;video src="https://github.com/user-attachments/assets/c619b4ea-bb8e-4382-860e-f3757e36af20" width="600" controls&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;p&gt;With the Computer SDK, you can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;automate Windows, Linux, and macOS VMs with a consistent, &lt;a href="https://docs.trycua.com/docs/libraries/computer#interface-actions"&gt;pyautogui-like API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;create &amp;amp; manage VMs &lt;a href="https://docs.trycua.com/docs/computer-sdk/computers#cua-local-containers"&gt;locally&lt;/a&gt; or using &lt;a href="https://www.trycua.com/"&gt;cua cloud&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With the Agent SDK, you can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;run computer-use models with a &lt;a href="https://docs.trycua.com/docs/agent-sdk/message-format"&gt;consistent schema&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;benchmark on OSWorld-Verified, SheetBench-V2, and more &lt;a href="https://docs.trycua.com/docs/agent-sdk/integrations/hud"&gt;with a single line of code using HUD&lt;/a&gt; (&lt;a href="https://github.com/trycua/cua/raw/main/notebooks/eval_osworld.ipynb"&gt;Notebook&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;combine UI grounding models with any LLM using &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents"&gt;composed agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;use new UI agent models and UI grounding models from the Model Zoo below with just a model string (e.g., &lt;code&gt;ComputerAgent(model="openai/computer-use-preview")&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;use API or local inference by changing a prefix (e.g., &lt;code&gt;openai/&lt;/code&gt;, &lt;code&gt;openrouter/&lt;/code&gt;, &lt;code&gt;ollama/&lt;/code&gt;, &lt;code&gt;huggingface-local/&lt;/code&gt;, &lt;code&gt;mlx/&lt;/code&gt;, &lt;a href="https://docs.litellm.ai/docs/providers"&gt;etc.&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;CUA Model Zoo ğŸ¨&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents"&gt;All-in-one CUAs&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents"&gt;UI Grounding Models&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents"&gt;UI Planning Models&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;anthropic/claude-sonnet-4-5-20250929&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/xlangai/OpenCUA-{7B,32B}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;any all-in-one CUA&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;openai/computer-use-preview&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/HelloKKMe/GTA1-{7B,32B,72B}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;any VLM (using liteLLM, requires &lt;code&gt;tools&lt;/code&gt; parameter)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;openrouter/z-ai/glm-4.5v&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/Hcompany/Holo1.5-{3B,7B,72B}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;any LLM (using liteLLM, requires &lt;code&gt;moondream3+&lt;/code&gt; prefix )&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/OpenGVLab/InternVL3_5-{1B,2B,4B,8B,...}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;any all-in-one CUA&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;huggingface-local/ByteDance-Seed/UI-TARS-1.5-7B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;moondream3+{ui planning}&lt;/code&gt; (supports text-only models)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;omniparser+{ui planning}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;{ui grounding}+{ui planning}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;human/human&lt;/code&gt; â†’ &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/human-in-the-loop"&gt;Human-in-the-Loop&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Missing a model? &lt;a href="https://github.com/trycua/cua/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;title=%5BAgent%5D%3A+Add+model+support+for+"&gt;Raise a feature request&lt;/a&gt; or &lt;a href="https://github.com/trycua/cua/raw/main/CONTRIBUTING.md"&gt;contribute&lt;/a&gt;!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.trycua.com/docs/quickstart-ui"&gt;Get started with a Computer-Use Agent UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.trycua.com/docs/quickstart-cli"&gt;Get started with the Computer-Use Agent CLI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.trycua.com/docs/quickstart-devs"&gt;Get started with the Python SDKs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h1&gt;Usage (&lt;a href="https://docs.trycua.com/docs"&gt;Docs&lt;/a&gt;)&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cua-agent[all]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agent import ComputerAgent

agent = ComputerAgent(
    model="anthropic/claude-3-5-sonnet-20241022",
    tools=[computer],
    max_trajectory_budget=5.0
)

messages = [{"role": "user", "content": "Take a screenshot and tell me what you see"}]

async for result in agent.run(messages):
    for item in result["output"]:
        if item["type"] == "message":
            print(item["content"][0]["text"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Output format (OpenAI Agent Responses Format):&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{ 
  "output": [
    # user input
    {
        "role": "user",
        "content": "go to trycua on gh"
    },
    # first agent turn adds the model output to the history
    {
        "summary": [
            {
                "text": "Searching Firefox for Trycua GitHub",
                "type": "summary_text"
            }
        ],
        "type": "reasoning"
    },
    {
        "action": {
            "text": "Trycua GitHub",
            "type": "type"
        },
        "call_id": "call_QI6OsYkXxl6Ww1KvyJc4LKKq",
        "status": "completed",
        "type": "computer_call"
    },
    # second agent turn adds the computer output to the history
    {
        "type": "computer_call_output",
        "call_id": "call_QI6OsYkXxl6Ww1KvyJc4LKKq",
        "output": {
            "type": "input_image",
            "image_url": "data:image/png;base64,..."
        }
    },
    # final agent turn adds the agent output text to the history
    {
        "type": "message",
        "role": "assistant",
        "content": [
          {
            "text": "Success! The Trycua GitHub page has been opened.",
            "type": "output_text"
          }
        ]
    }
  ], 
  "usage": {
      "prompt_tokens": 150,
      "completion_tokens": 75,
      "total_tokens": 225,
      "response_cost": 0.01,
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Computer (&lt;a href="https://docs.trycua.com/docs/computer-sdk/computers"&gt;Docs&lt;/a&gt;)&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cua-computer[all]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from computer import Computer

async with Computer(
    os_type="linux",
    provider_type="cloud",
    name="your-container-name",
    api_key="your-api-key"
) as computer:
    # Take screenshot
    screenshot = await computer.interface.screenshot()

    # Click and type
    await computer.interface.left_click(100, 100)
    await computer.interface.type("Hello!")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/mcp-server/README.md"&gt;How to use the MCP Server with Claude Desktop or other MCP clients&lt;/a&gt; - One of the easiest ways to get started with Cua&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/agent/README.md"&gt;How to use OpenAI Computer-Use, Anthropic, OmniParser, or UI-TARS for your Computer-Use Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lume/README.md"&gt;How to use Lume CLI for managing desktops&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.trycua.com/blog/training-computer-use-models-trajectories-1"&gt;Training Computer-Use Models: Collecting Human Trajectories with Cua (Part 1)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Modules&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Installation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lume/README.md"&gt;&lt;strong&gt;Lume&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;VM management for macOS/Linux using Apple's Virtualization.Framework&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lumier/README.md"&gt;&lt;strong&gt;Lumier&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Docker interface for macOS and Linux VMs&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;docker pull trycua/lumier:latest&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/computer/README.md"&gt;&lt;strong&gt;Computer (Python)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python Interface for controlling virtual machines&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install "cua-computer[all]"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/typescript/computer/README.md"&gt;&lt;strong&gt;Computer (Typescript)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Typescript Interface for controlling virtual machines&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;npm install @trycua/computer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/agent/README.md"&gt;&lt;strong&gt;Agent&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AI agent framework for automating tasks&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install "cua-agent[all]"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/mcp-server/README.md"&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP server for using CUA with Claude Desktop&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-mcp-server&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/som/README.md"&gt;&lt;strong&gt;SOM&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Self-of-Mark library for Agent&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-som&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/computer-server/README.md"&gt;&lt;strong&gt;Computer Server&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Server component for Computer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-computer-server&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/core/README.md"&gt;&lt;strong&gt;Core (Python)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python Core utilities&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-core&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/typescript/core/README.md"&gt;&lt;strong&gt;Core (Typescript)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Typescript Core utilities&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;npm install @trycua/core&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.com/invite/mVnXXpdE85"&gt;Discord community&lt;/a&gt; to discuss ideas, get assistance, or share your demos!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Cua is open-sourced under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;Portions of this project, specifically components adapted from Kasm Technologies Inc., are also licensed under the MIT License. See &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/kasm/LICENSE"&gt;libs/kasm/LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Microsoft's OmniParser, which is used in this project, is licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0). See the &lt;a href="https://github.com/microsoft/OmniParser/raw/master/LICENSE"&gt;OmniParser LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h3&gt;Third-Party Licenses and Optional Components&lt;/h3&gt; 
&lt;p&gt;Some optional extras for this project depend on third-party packages that are licensed under terms different from the MIT License.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The optional "omni" extra (installed via &lt;code&gt;pip install "cua-agent[omni]"&lt;/code&gt;) installs the &lt;code&gt;cua-som&lt;/code&gt; module, which includes &lt;code&gt;ultralytics&lt;/code&gt; and is licensed under the AGPL-3.0.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When you choose to install and use such optional extras, your use, modification, and distribution of those third-party components are governed by their respective licenses (e.g., AGPL-3.0 for &lt;code&gt;ultralytics&lt;/code&gt;).&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to Cua! Please refer to our &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;Apple, macOS, and Apple Silicon are trademarks of Apple Inc.&lt;br /&gt; Ubuntu and Canonical are registered trademarks of Canonical Ltd.&lt;br /&gt; Microsoft is a registered trademark of Microsoft Corporation.&lt;/p&gt; 
&lt;p&gt;This project is not affiliated with, endorsed by, or sponsored by Apple Inc., Canonical Ltd., Microsoft Corporation, or Kasm Technologies.&lt;/p&gt; 
&lt;h2&gt;Stargazers&lt;/h2&gt; 
&lt;p&gt;Thank you to all our supporters!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/trycua/cua"&gt;&lt;img src="https://starchart.cc/trycua/cua.svg?variant=adaptive" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Thank you to all our &lt;a href="https://github.com/sponsors/trycua"&gt;GitHub Sponsors&lt;/a&gt;!&lt;/p&gt; 
&lt;img width="300" alt="coderabbit-cli" src="https://github.com/user-attachments/assets/23a98e38-7897-4043-8ef7-eb990520dccc" /&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/RAG-Anything</title>
      <link>https://github.com/HKUDS/RAG-Anything</link>
      <description>&lt;p&gt;"RAG-Anything: All-in-One RAG Framework"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div style="margin: 20px 0;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/logo.png" width="120" height="120" alt="RAG-Anything Logo" style="border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;ğŸš€ RAG-Anything: All-in-One RAG Framework&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14959" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14959" alt="HKUDS%2FRAG-Anything | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://readme-typing-svg.herokuapp.com?font=Orbitron&amp;amp;size=24&amp;amp;duration=3000&amp;amp;pause=1000&amp;amp;color=00D9FF&amp;amp;center=true&amp;amp;vCenter=true&amp;amp;width=600&amp;amp;lines=Welcome+to+RAG-Anything;Next-Gen+Multimodal+RAG+System;Powered+by+Advanced+AI+Technology" alt="Typing Animation" /&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;"&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;&lt;img src="https://img.shields.io/badge/ğŸ”¥Project-Page-00d9ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2410.05779"&gt;&lt;img src="https://img.shields.io/badge/ğŸ“„arXiv-2410.05779-ff6b6b?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt;&lt;img src="https://img.shields.io/badge/âš¡Based%20on-LightRAG-4ecdc4?style=for-the-badge&amp;amp;logo=lightning&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/RAG-Anything?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/ğŸPython-3.10-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/raganything/"&gt;&lt;img src="https://img.shields.io/pypi/v/raganything.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;img src="https://img.shields.io/badge/âš¡uv-Ready-ff6b6b?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything/issues/7"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README_zh.md"&gt;&lt;img src="https://img.shields.io/badge/ğŸ‡¨ğŸ‡³ä¸­æ–‡ç‰ˆ-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/ğŸ‡ºğŸ‡¸English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ‰ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.08.12]ğŸ¯ğŸ“¢ ğŸ” RAG-Anything now features &lt;strong&gt;VLM-Enhanced Query&lt;/strong&gt; mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07.05]ğŸ¯ğŸ“¢ RAG-Anything now features a &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/docs/context_aware_processing.md"&gt;context configuration module&lt;/a&gt;, enabling intelligent integration of relevant contextual information to enhance multimodal content processing.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07.04]ğŸ¯ğŸ“¢ ğŸš€ RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; [2025.07.03]ğŸ¯ğŸ“¢ ğŸ‰ RAG-Anything has reached 1kğŸŒŸ stars on GitHub! Thank you for your incredible support and valuable contributions to the project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸŒŸ System Overview&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Next-Generation Multimodal Intelligence&lt;/em&gt;&lt;/p&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border: 2px solid #00d9ff; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);"&gt; 
 &lt;p&gt;Modern documents increasingly contain diverse multimodal contentâ€”text, images, tables, equations, charts, and multimediaâ€”that traditional text-focused RAG systems cannot effectively process. &lt;strong&gt;RAG-Anything&lt;/strong&gt; addresses this challenge as a comprehensive &lt;strong&gt;All-in-One Multimodal Document Processing RAG system&lt;/strong&gt; built on &lt;a href="https://github.com/HKUDS/LightRAG"&gt;LightRAG&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;As a unified solution, RAG-Anything &lt;strong&gt;eliminates the need for multiple specialized tools&lt;/strong&gt;. It provides &lt;strong&gt;seamless processing and querying across all content modalities&lt;/strong&gt; within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers &lt;strong&gt;comprehensive multimodal retrieval capabilities&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;Users can query documents containing &lt;strong&gt;interleaved text&lt;/strong&gt;, &lt;strong&gt;visual diagrams&lt;/strong&gt;, &lt;strong&gt;structured tables&lt;/strong&gt;, and &lt;strong&gt;mathematical formulations&lt;/strong&gt; through &lt;strong&gt;one cohesive interface&lt;/strong&gt;. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a &lt;strong&gt;unified processing framework&lt;/strong&gt;.&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/assets/rag_anything_framework.png" alt="RAG-Anything" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;ğŸ¯ Key Features&lt;/h3&gt; 
&lt;div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); border-radius: 15px; padding: 25px; margin: 20px 0;"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ”„ End-to-End Multimodal Pipeline&lt;/strong&gt; - Complete workflow from document ingestion and parsing to intelligent multimodal query answering&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ“„ Universal Document Support&lt;/strong&gt; - Seamless processing of PDFs, Office documents, images, and diverse file formats&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ§  Specialized Content Analysis&lt;/strong&gt; - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ”— Multimodal Knowledge Graph&lt;/strong&gt; - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;âš¡ Adaptive Processing Modes&lt;/strong&gt; - Flexible MinerU-based parsing or direct multimodal content injection workflows&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ“‹ Direct Content List Insertion&lt;/strong&gt; - Bypass document parsing by directly inserting pre-parsed content lists from external sources&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ğŸ¯ Hybrid Intelligent Retrieval&lt;/strong&gt; - Advanced search capabilities spanning textual and multimodal content with contextual understanding&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ—ï¸ Algorithm &amp;amp; Architecture&lt;/h2&gt; 
&lt;div style="background: linear-gradient(135deg, #0f0f23 0%, #1a1a2e 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border-left: 5px solid #00d9ff;"&gt; 
 &lt;h3&gt;Core Algorithm&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;RAG-Anything&lt;/strong&gt; implements an effective &lt;strong&gt;multi-stage multimodal pipeline&lt;/strong&gt; that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 20px;"&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     ğŸ“„
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Document Parsing
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    â†’
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     ğŸ§ 
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Content Analysis
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    â†’
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     ğŸ”
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Knowledge Graph
    &lt;/div&gt; 
   &lt;/div&gt; 
   &lt;div style="font-size: 20px; color: #00d9ff;"&gt;
    â†’
   &lt;/div&gt; 
   &lt;div style="text-align: center;"&gt; 
    &lt;div style="font-size: 24px; margin-bottom: 10px;"&gt;
     ğŸ¯
    &lt;/div&gt; 
    &lt;div style="font-size: 14px; color: #00d9ff;"&gt;
     Intelligent Retrieval
    &lt;/div&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h3&gt;1. Document Parsing Stage&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;âš™ï¸ MinerU Integration&lt;/strong&gt;: Leverages &lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; for high-fidelity document structure extraction and semantic preservation across complex layouts.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ§© Adaptive Content Decomposition&lt;/strong&gt;: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“ Universal Format Support&lt;/strong&gt;: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Multi-Modal Content Understanding &amp;amp; Processing&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Key Components:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ¯ Autonomous Content Categorization and Routing&lt;/strong&gt;: Automatically identify, categorize, and route different content types through optimized execution channels.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;âš¡ Concurrent Multi-Pipeline Architecture&lt;/strong&gt;: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ—ï¸ Document Hierarchy Extraction&lt;/strong&gt;: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;3. Multimodal Analysis Engine&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #0f3460 0%, #1a1a2e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #00d9ff;"&gt; 
 &lt;p&gt;The system deploys modality-aware processing units for heterogeneous data modalities:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Specialized Analyzers:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Visual Content Analyzer&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Integrate vision model for image analysis.&lt;/li&gt; 
    &lt;li&gt;Generates context-aware descriptive captions based on visual semantics.&lt;/li&gt; 
    &lt;li&gt;Extracts spatial relationships and hierarchical structures between visual elements.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“Š Structured Data Interpreter&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Performs systematic interpretation of tabular and structured data formats.&lt;/li&gt; 
    &lt;li&gt;Implements statistical pattern recognition algorithms for data trend analysis.&lt;/li&gt; 
    &lt;li&gt;Identifies semantic relationships and dependencies across multiple tabular datasets.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“ Mathematical Expression Parser&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Parses complex mathematical expressions and formulas with high accuracy.&lt;/li&gt; 
    &lt;li&gt;Provides native LaTeX format support for seamless integration with academic workflows.&lt;/li&gt; 
    &lt;li&gt;Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”§ Extensible Modality Handler&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Provides configurable processing framework for custom and emerging content types.&lt;/li&gt; 
    &lt;li&gt;Enables dynamic integration of new modality processors through plugin architecture.&lt;/li&gt; 
    &lt;li&gt;Supports runtime configuration of processing pipelines for specialized use cases.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;4. Multimodal Knowledge Graph Index&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;"&gt; 
 &lt;p&gt;The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Core Functions:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Multi-Modal Entity Extraction&lt;/strong&gt;: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”— Cross-Modal Relationship Mapping&lt;/strong&gt;: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ—ï¸ Hierarchical Structure Preservation&lt;/strong&gt;: Maintains original document organization through "belongs_to" relationship chains. These chains preserve logical content hierarchy and sectional dependencies.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;âš–ï¸ Weighted Relationship Scoring&lt;/strong&gt;: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;h3&gt;5. Modality-Aware Retrieval&lt;/h3&gt; 
&lt;div style="background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;"&gt; 
 &lt;p&gt;The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Retrieval Mechanisms:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”€ Vector-Graph Fusion&lt;/strong&gt;: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“Š Modality-Aware Ranking&lt;/strong&gt;: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”— Relational Coherence Maintenance&lt;/strong&gt;: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Initialize Your AI Journey&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif" width="400" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;Option 1: Install from PyPI (Recommended)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic installation
pip install raganything

# With optional dependencies for extended format support:
pip install 'raganything[all]'              # All optional features
pip install 'raganything[image]'            # Image format conversion (BMP, TIFF, GIF, WebP)
pip install 'raganything[text]'             # Text file processing (TXT, MD)
pip install 'raganything[image,text]'       # Multiple features
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Install from Source&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and setup the project with uv
git clone https://github.com/HKUDS/RAG-Anything.git
cd RAG-Anything

# Install the package and dependencies in a virtual environment
uv sync

# If you encounter network timeouts (especially for opencv packages):
# UV_HTTP_TIMEOUT=120 uv sync

# Run commands directly with uv (recommended approach)
uv run python examples/raganything_example.py --help

# Install with optional dependencies
uv sync --extra image --extra text  # Specific extras
uv sync --all-extras                 # All optional features
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[image]&lt;/code&gt;&lt;/strong&gt; - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[text]&lt;/code&gt;&lt;/strong&gt; - Enables processing of TXT and MD files (requires ReportLab)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[all]&lt;/code&gt;&lt;/strong&gt; - Includes all Python optional dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ Office Document Processing Requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require &lt;strong&gt;LibreOffice&lt;/strong&gt; installation&lt;/li&gt; 
  &lt;li&gt;Download from &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice official website&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Download installer from official website&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;brew install --cask libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Ubuntu/Debian&lt;/strong&gt;: &lt;code&gt;sudo apt-get install libreoffice&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;CentOS/RHEL&lt;/strong&gt;: &lt;code&gt;sudo yum install libreoffice&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Check MinerU installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Verify installation
mineru --version

# Check if properly configured
python -c "from raganything import RAGAnything; rag = RAGAnything(); print('âœ… MinerU installed properly' if rag.check_parser_installation() else 'âŒ MinerU installation issue')"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models are downloaded automatically on first use. For manual download, refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/README.md#22-model-source-configuration"&gt;MinerU Model Source Configuration&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;h4&gt;1. End-to-End Document Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        parser="mineru",  # Parser selection: mineru or docling
        parse_method="auto",  # Parse method: auto, ocr, or txt
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define LLM model function
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Define embedding function
    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Process a document
    await rag.process_document_complete(
        file_path="path/to/your/document.pdf",
        output_dir="./output",
        parse_method="auto"
    )

    # Query the processed content
    # Pure text query - for basic knowledge base search
    text_result = await rag.aquery(
        "What are the main findings shown in the figures and tables?",
        mode="hybrid"
    )
    print("Text query result:", text_result)

    # Multimodal query with specific multimodal content
    multimodal_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
    print("Multimodal query result:", multimodal_result)

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Direct Multimodal Content Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from raganything.modalprocessors import ImageModalProcessor, TableModalProcessor

async def process_multimodal_content():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Initialize LightRAG
    rag = LightRAG(
        working_dir="./rag_storage",
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )
    await rag.initialize_storages()

    # Process an image
    image_processor = ImageModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
            "gpt-4o",
            "",
            system_prompt=None,
            history_messages=[],
            messages=[
                {"role": "system", "content": system_prompt} if system_prompt else None,
                {"role": "user", "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                ]} if image_data else {"role": "user", "content": prompt}
            ],
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ) if image_data else openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    image_content = {
        "img_path": "path/to/image.jpg",
        "image_caption": ["Figure 1: Experimental results"],
        "image_footnote": ["Data collected in 2024"]
    }

    description, entity_info = await image_processor.process_multimodal_content(
        modal_content=image_content,
        content_type="image",
        file_path="research_paper.pdf",
        entity_name="Experimental Results Figure"
    )

    # Process a table
    table_processor = TableModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    table_content = {
        "table_body": """
        | Method | Accuracy | F1-Score |
        |--------|----------|----------|
        | RAGAnything | 95.2% | 0.94 |
        | Baseline | 87.3% | 0.85 |
        """,
        "table_caption": ["Performance Comparison"],
        "table_footnote": ["Results on test dataset"]
    }

    description, entity_info = await table_processor.process_multimodal_content(
        modal_content=table_content,
        content_type="table",
        file_path="research_paper.pdf",
        entity_name="Performance Results Table"
    )

if __name__ == "__main__":
    asyncio.run(process_multimodal_content())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Batch Processing&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process multiple documents
await rag.process_folder_complete(
    folder_path="./documents",
    output_dir="./output",
    file_extensions=[".pdf", ".docx", ".pptx"],
    recursive=True,
    max_workers=4
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Custom Modal Processors&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from raganything.modalprocessors import GenericModalProcessor

class CustomModalProcessor(GenericModalProcessor):
    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):
        # Your custom processing logic
        enhanced_description = await self.analyze_custom_content(modal_content)
        entity_info = self.create_custom_entity(enhanced_description, entity_name)
        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Query Options&lt;/h4&gt; 
&lt;p&gt;RAG-Anything provides three types of query methods:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Pure Text Queries&lt;/strong&gt; - Direct knowledge base search using LightRAG:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Different query modes for text queries
text_result_hybrid = await rag.aquery("Your question", mode="hybrid")
text_result_local = await rag.aquery("Your question", mode="local")
text_result_global = await rag.aquery("Your question", mode="global")
text_result_naive = await rag.aquery("Your question", mode="naive")

# Synchronous version
sync_text_result = rag.query("Your question", mode="hybrid")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;VLM Enhanced Queries&lt;/strong&gt; - Automatically analyze images in retrieved context using VLM:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# VLM enhanced query (automatically enabled when vision_model_func is provided)
vlm_result = await rag.aquery(
    "Analyze the charts and figures in the document",
    mode="hybrid"
    # vlm_enhanced=True is automatically set when vision_model_func is available
)

# Manually control VLM enhancement
vlm_enabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=True  # Force enable VLM enhancement
)

vlm_disabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=False  # Force disable VLM enhancement
)

# When documents contain images, VLM can see and analyze them directly
# The system will automatically:
# 1. Retrieve relevant context containing image paths
# 2. Load and encode images as base64
# 3. Send both text context and images to VLM for comprehensive analysis
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Multimodal Queries&lt;/strong&gt; - Enhanced queries with specific multimodal content analysis:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Query with table data
table_result = await rag.aquery_with_multimodal(
    "Compare these performance metrics with the document content",
    multimodal_content=[{
        "type": "table",
        "table_data": """Method,Accuracy,Speed
                        RAGAnything,95.2%,120ms
                        Traditional,87.3%,180ms""",
        "table_caption": "Performance comparison"
    }],
    mode="hybrid"
)

# Query with equation content
equation_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;6. Loading Existing LightRAG Instance&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import EmbeddingFunc
import os

async def load_existing_lightrag():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # First, create or load existing LightRAG instance
    lightrag_working_dir = "./existing_lightrag_storage"

    # Check if previous LightRAG instance exists
    if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
        print("âœ… Found existing LightRAG instance, loading...")
    else:
        print("âŒ No existing LightRAG instance found, will create new one")

    # Create/load LightRAG instance with your configuration
    lightrag_instance = LightRAG(
        working_dir=lightrag_working_dir,
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )

    # Initialize storage (this will load existing data if available)
    await lightrag_instance.initialize_storages()
    await initialize_pipeline_status()

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Now use existing LightRAG instance to initialize RAGAnything
    rag = RAGAnything(
        lightrag=lightrag_instance,  # Pass existing LightRAG instance
        vision_model_func=vision_model_func,
        # Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance
    )

    # Query existing knowledge base
    result = await rag.aquery(
        "What data has been processed in this LightRAG instance?",
        mode="hybrid"
    )
    print("Query result:", result)

    # Add new multimodal document to existing LightRAG instance
    await rag.process_document_complete(
        file_path="path/to/new/multimodal_document.pdf",
        output_dir="./output"
    )

if __name__ == "__main__":
    asyncio.run(load_existing_lightrag())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;7. Direct Content List Insertion&lt;/h4&gt; 
&lt;p&gt;For scenarios where you already have a pre-parsed content list (e.g., from external parsers or previous processing), you can directly insert it into RAGAnything without document parsing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def insert_content_list_example():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define model functions
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                        ],
                    } if image_data else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Example: Pre-parsed content list from external source
    content_list = [
        {
            "type": "text",
            "text": "This is the introduction section of our research paper.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "image",
            "img_path": "/absolute/path/to/figure1.jpg",  # IMPORTANT: Use absolute path
            "image_caption": ["Figure 1: System Architecture"],
            "image_footnote": ["Source: Authors' original design"],
            "page_idx": 1  # Page number where this image appears
        },
        {
            "type": "table",
            "table_body": "| Method | Accuracy | F1-Score |\n|--------|----------|----------|\n| Ours | 95.2% | 0.94 |\n| Baseline | 87.3% | 0.85 |",
            "table_caption": ["Table 1: Performance Comparison"],
            "table_footnote": ["Results on test dataset"],
            "page_idx": 2  # Page number where this table appears
        },
        {
            "type": "equation",
            "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
            "text": "Document relevance probability formula",
            "page_idx": 3  # Page number where this equation appears
        },
        {
            "type": "text",
            "text": "In conclusion, our method demonstrates superior performance across all metrics.",
            "page_idx": 4  # Page number where this content appears
        }
    ]

    # Insert the content list directly
    await rag.insert_content_list(
        content_list=content_list,
        file_path="research_paper.pdf",  # Reference file name for citation
        split_by_character=None,         # Optional text splitting
        split_by_character_only=False,   # Optional text splitting mode
        doc_id=None,                     # Optional custom document ID (will be auto-generated if not provided)
        display_stats=True               # Show content statistics
    )

    # Query the inserted content
    result = await rag.aquery(
        "What are the key findings and performance metrics mentioned in the research?",
        mode="hybrid"
    )
    print("Query result:", result)

    # You can also insert multiple content lists with different document IDs
    another_content_list = [
        {
            "type": "text",
            "text": "This is content from another document.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "table",
            "table_body": "| Feature | Value |\n|---------|-------|\n| Speed | Fast |\n| Accuracy | High |",
            "table_caption": ["Feature Comparison"],
            "page_idx": 1  # Page number where this table appears
        }
    ]

    await rag.insert_content_list(
        content_list=another_content_list,
        file_path="another_document.pdf",
        doc_id="custom-doc-id-123"  # Custom document ID
    )

if __name__ == "__main__":
    asyncio.run(insert_content_list_example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Content List Format:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;content_list&lt;/code&gt; should follow the standard format with each item being a dictionary containing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text content&lt;/strong&gt;: &lt;code&gt;{"type": "text", "text": "content text", "page_idx": 0}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Image content&lt;/strong&gt;: &lt;code&gt;{"type": "image", "img_path": "/absolute/path/to/image.jpg", "image_caption": ["caption"], "image_footnote": ["note"], "page_idx": 1}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Table content&lt;/strong&gt;: &lt;code&gt;{"type": "table", "table_body": "markdown table", "table_caption": ["caption"], "table_footnote": ["note"], "page_idx": 2}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equation content&lt;/strong&gt;: &lt;code&gt;{"type": "equation", "latex": "LaTeX formula", "text": "description", "page_idx": 3}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic content&lt;/strong&gt;: &lt;code&gt;{"type": "custom_type", "content": "any content", "page_idx": 4}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;img_path&lt;/code&gt;&lt;/strong&gt;: Must be an absolute path to the image file (e.g., &lt;code&gt;/home/user/images/chart.jpg&lt;/code&gt; or &lt;code&gt;C:\Users\user\images\chart.jpg&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;page_idx&lt;/code&gt;&lt;/strong&gt;: Represents the page number where the content appears in the original document (0-based indexing)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content ordering&lt;/strong&gt;: Items are processed in the order they appear in the list&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This method is particularly useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You have content from external parsers (non-MinerU/Docling)&lt;/li&gt; 
 &lt;li&gt;You want to process programmatically generated content&lt;/li&gt; 
 &lt;li&gt;You need to insert content from multiple sources into a single knowledge base&lt;/li&gt; 
 &lt;li&gt;You have cached parsing results that you want to reuse&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ› ï¸ Examples&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Practical Implementation Demos&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif" width="300" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;code&gt;examples/&lt;/code&gt; directory contains comprehensive usage examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;raganything_example.py&lt;/code&gt;&lt;/strong&gt;: End-to-end document processing with MinerU&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;modalprocessors_example.py&lt;/code&gt;&lt;/strong&gt;: Direct multimodal content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;office_document_test.py&lt;/code&gt;&lt;/strong&gt;: Office document parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;image_format_test.py&lt;/code&gt;&lt;/strong&gt;: Image format parsing test with MinerU (no API key required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;text_format_test.py&lt;/code&gt;&lt;/strong&gt;: Text format parsing test with MinerU (no API key required)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Run examples:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# End-to-end processing with parser selection
python examples/raganything_example.py path/to/document.pdf --api-key YOUR_API_KEY --parser mineru

# Direct modal processing
python examples/modalprocessors_example.py --api-key YOUR_API_KEY

# Office document parsing test (MinerU only)
python examples/office_document_test.py --file path/to/document.docx

# Image format parsing test (MinerU only)
python examples/image_format_test.py --file path/to/image.bmp

# Text format parsing test (MinerU only)
python examples/text_format_test.py --file path/to/document.md

# Check LibreOffice installation
python examples/office_document_test.py --check-libreoffice --file dummy

# Check PIL/Pillow installation
python examples/image_format_test.py --check-pillow --file dummy

# Check ReportLab installation
python examples/text_format_test.py --check-reportlab --file dummy
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”§ Configuration&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;System Optimization Parameters&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file (refer to &lt;code&gt;.env.example&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=your_base_url  # Optional
OUTPUT_DIR=./output             # Default output directory for parsed documents
PARSER=mineru                   # Parser selection: mineru or docling
PARSE_METHOD=auto              # Parse method: auto, ocr, or txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For backward compatibility, legacy environment variable names are still supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MINERU_PARSE_METHOD&lt;/code&gt; is deprecated, please use &lt;code&gt;PARSE_METHOD&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: API keys are only required for full RAG processing with LLM integration. The parsing test files (&lt;code&gt;office_document_test.py&lt;/code&gt; and &lt;code&gt;image_format_test.py&lt;/code&gt;) only test parser functionality and do not require API keys.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Parser Configuration&lt;/h3&gt; 
&lt;p&gt;RAGAnything now supports multiple parsers, each with specific advantages:&lt;/p&gt; 
&lt;h4&gt;MinerU Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports PDF, images, Office documents, and more formats&lt;/li&gt; 
 &lt;li&gt;Powerful OCR and table extraction capabilities&lt;/li&gt; 
 &lt;li&gt;GPU acceleration support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Docling Parser&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optimized for Office documents and HTML files&lt;/li&gt; 
 &lt;li&gt;Better document structure preservation&lt;/li&gt; 
 &lt;li&gt;Native support for multiple Office formats&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MinerU Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MinerU 2.0 uses command-line parameters instead of config files
# Check available options:
mineru --help

# Common configurations:
mineru -p input.pdf -o output_dir -m auto    # Automatic parsing mode
mineru -p input.pdf -o output_dir -m ocr     # OCR-focused parsing
mineru -p input.pdf -o output_dir -b pipeline --device cuda  # GPU acceleration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also configure parsing through RAGAnything parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Basic parsing configuration with parser selection
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # or "ocr", "txt"
    parser="mineru"               # Optional: "mineru" or "docling"
)

# Advanced parsing configuration with special parameters
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # Parsing method: "auto", "ocr", "txt"
    parser="mineru",              # Parser selection: "mineru" or "docling"

    # MinerU special parameters - all supported kwargs:
    lang="ch",                   # Document language for OCR optimization (e.g., "ch", "en", "ja")
    device="cuda:0",             # Inference device: "cpu", "cuda", "cuda:0", "npu", "mps"
    start_page=0,                # Starting page number (0-based, for PDF)
    end_page=10,                 # Ending page number (0-based, for PDF)
    formula=True,                # Enable formula parsing
    table=True,                  # Enable table parsing
    backend="pipeline",          # Parsing backend: pipeline|vlm-transformers|vlm-sglang-engine|vlm-sglang-client.
    source="huggingface",        # Model source: "huggingface", "modelscope", "local"
    # vlm_url="http://127.0.0.1:3000" # Service address when using backend=vlm-sglang-client

    # Standard RAGAnything parameters
    display_stats=True,          # Display content statistics
    split_by_character=None,     # Optional character to split text by
    doc_id=None                  # Optional document ID
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MinerU 2.0 no longer uses the &lt;code&gt;magic-pdf.json&lt;/code&gt; configuration file. All settings are now passed as command-line parameters or function arguments. RAG-Anything now supports multiple document parsers - you can choose between MinerU and Docling based on your needs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Processing Requirements&lt;/h3&gt; 
&lt;p&gt;Different content types require specific optional dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; (.doc, .docx, .ppt, .pptx, .xls, .xlsx): Install &lt;a href="https://www.libreoffice.org/download/download/"&gt;LibreOffice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extended Image Formats&lt;/strong&gt; (.bmp, .tiff, .gif, .webp): Install with &lt;code&gt;pip install raganything[image]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; (.txt, .md): Install with &lt;code&gt;pip install raganything[text]&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“‹ Quick Install&lt;/strong&gt;: Use &lt;code&gt;pip install raganything[all]&lt;/code&gt; to enable all format support (Python dependencies only - LibreOffice still needs separate installation)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ§ª Supported Content Types&lt;/h2&gt; 
&lt;h3&gt;Document Formats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PDFs&lt;/strong&gt; - Research papers, reports, presentations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Office Documents&lt;/strong&gt; - DOC, DOCX, PPT, PPTX, XLS, XLSX&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - JPG, PNG, BMP, TIFF, GIF, WebP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Files&lt;/strong&gt; - TXT, MD&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Elements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt; - Photographs, diagrams, charts, screenshots&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tables&lt;/strong&gt; - Data tables, comparison charts, statistical summaries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Equations&lt;/strong&gt; - Mathematical formulas in LaTeX format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generic Content&lt;/strong&gt; - Custom content types via extensible processors&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;For installation of format-specific dependencies, see the &lt;a href="https://raw.githubusercontent.com/HKUDS/RAG-Anything/main/#-configuration"&gt;Configuration&lt;/a&gt; section.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“– Citation&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Academic Reference&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 60px; height: 60px; margin: 20px auto; position: relative;"&gt; 
  &lt;div style="width: 100%; height: 100%; border: 2px solid #00d9ff; border-radius: 50%; position: relative;"&gt; 
   &lt;div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); font-size: 24px; color: #00d9ff;"&gt;
    ğŸ“–
   &lt;/div&gt; 
  &lt;/div&gt; 
  &lt;div style="position: absolute; bottom: -5px; left: 50%; transform: translateX(-50%); width: 20px; height: 20px; background: white; border-right: 2px solid #00d9ff; border-bottom: 2px solid #00d9ff; transform: rotate(45deg);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;p&gt;If you find RAG-Anything useful in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{guo2024lightrag,
  title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
  author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
  year={2024},
  eprint={2410.05779},
  archivePrefix={arXiv},
  primaryClass={cs.IR}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”— Related Projects&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Ecosystem &amp;amp; Extensions&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;âš¡&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;LightRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Simple and Fast RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/VideoRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;ğŸ¥&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;VideoRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extreme Long-Context Video RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/MiniRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;âœ¨&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;MiniRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extremely Simple RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â­ Star History&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://star-history.com/#HKUDS/RAG-Anything&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/RAG-Anything&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contribution&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Join the Innovation&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt;
  We thank all our contributors for their valuable contributions. 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/HKUDS/RAG-Anything/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=HKUDS/RAG-Anything" style="border-radius: 15px; box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 30px 0;"&gt; 
 &lt;div&gt; 
  &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="500" /&gt; 
 &lt;/div&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/â­%20Star%20us%20on%20GitHub-1a1a2e?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/issues" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/ğŸ›%20Report%20Issues-ff6b6b?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/RAG-Anything/discussions" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/ğŸ’¬%20Discussions-4ecdc4?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: center; align-items: center; gap: 15px;"&gt; 
   &lt;span style="font-size: 24px;"&gt;â­&lt;/span&gt; 
   &lt;span style="color: #00d9ff; font-size: 18px;"&gt;Thank you for visiting RAG-Anything!&lt;/span&gt; 
   &lt;span style="font-size: 24px;"&gt;â­&lt;/span&gt; 
  &lt;/div&gt; 
  &lt;div style="margin-top: 10px; color: #00d9ff; font-size: 16px;"&gt;
   Building the Future of Multimodal AI
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>knownsec/aipyapp</title>
      <link>https://github.com/knownsec/aipyapp</link>
      <description>&lt;p&gt;AI-Powered Python &amp; Python-Powered AI (Python-Use)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3af4e228-79b2-4fa0-a45c-c38276c6db91" alt="logo" /&gt;&lt;/p&gt; 
&lt;h1&gt;Python use&lt;/h1&gt; 
&lt;p&gt;AIPy is an implementation of the Python-use concept, demonstrating its practical value and potential.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Mission&lt;/strong&gt;: unleash the full potential of large language models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vision&lt;/strong&gt;: a future where LLMs can think independently and proactively leverage AIPy to solve complex problems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What&lt;/h2&gt; 
&lt;p&gt;Python use provides the entire Python execution environment to LLM. Imagine LLM sitting in front of a computer, typing various commands into the Python command-line interpreter, pressing Enter to execute, observing the results, and then typing and executing more code.&lt;/p&gt; 
&lt;p&gt;Unlike Agents, Python use does not define any tools interface. LLM can freely use all the features provided by the Python runtime environment.&lt;/p&gt; 
&lt;h2&gt;Why&lt;/h2&gt; 
&lt;p&gt;If you are a data engineer, you are likely familiar with the following scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Handling various data file formats: csv/excel, json, html, sqlite, parquet, etc.&lt;/li&gt; 
 &lt;li&gt;Performing operations like data cleaning, transformation, computation, aggregation, sorting, grouping, filtering, analysis, and visualization.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This process often requires:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starting Python, importing pandas as pd, and typing a bunch of commands to process data.&lt;/li&gt; 
 &lt;li&gt;Generating a bunch of intermediate temporary files.&lt;/li&gt; 
 &lt;li&gt;Describing your needs to ChatGPT/Claude, copying the generated data processing code, and running it manually.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;So, why not start the Python command-line interpreter, directly describe your data processing needs, and let it be done automatically? The benefits are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No need to manually input a bunch of Python commands temporarily.&lt;/li&gt; 
 &lt;li&gt;No need to describe your needs to GPT, copy the program, and run it manually.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This is the problem Python use aims to solve!&lt;/p&gt; 
&lt;h2&gt;How&lt;/h2&gt; 
&lt;p&gt;Python use (aipython) is a Python command-line interpreter integrated with LLM. You can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Enter and execute Python commands as usual.&lt;/li&gt; 
 &lt;li&gt;Describe your needs in natural language, and aipython will automatically generate Python commands and execute them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Moreover, the two modes can access data interchangeably. For example, after aipython processes your natural language commands, you can use standard Python commands to view various data.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;AIPython has two running modes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Task mode: Very simple and easy to use, just input your task, suitable for users unfamiliar with Python.&lt;/li&gt; 
 &lt;li&gt;Python mode: Suitable for users familiar with Python, allowing both task input and Python commands, ideal for advanced users.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The default running mode is task mode, which can be switched to Python mode using the &lt;code&gt;--python&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;h3&gt;Basic Config&lt;/h3&gt; 
&lt;p&gt;~/.aipyapp/aipyapp.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[llm.deepseek]
type = "deepseek"
api_key = "Your DeepSeek API Key"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Task Mode&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;uv run aipy&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; Get the latest posts from Reddit r/LocalLLaMA
......
......
&amp;gt;&amp;gt;&amp;gt; /done
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;pip install aipyapp&lt;/code&gt; and run with &lt;code&gt;aipy&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;-&amp;gt; % aipy
ğŸš€ Python use - AIPython (0.1.22) [https://aipy.app]
&amp;gt;&amp;gt; Get the latest posts from Reddit r/LocalLLaMA
......
&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python Mode&lt;/h3&gt; 
&lt;h4&gt;Basic Usage&lt;/h4&gt; 
&lt;p&gt;Automatic task processing:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; ai("Get the title of Google's homepage")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Automatically Request to Install Third-Party Libraries&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;Python use - AIPython (Quit with 'exit()')
&amp;gt;&amp;gt;&amp;gt; ai("Use psutil to list all processes on MacOS")

ğŸ“¦ LLM requests to install third-party packages: ['psutil']
If you agree and have installed, please enter 'y [y/n] (n): y

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hei Ge: Product manager/senior user/chief tester&lt;/li&gt; 
 &lt;li&gt;Sonnet 3.7: Generated the first version of the code, which was almost ready to use without modification.&lt;/li&gt; 
 &lt;li&gt;ChatGPT: Provided many suggestions and code snippets, especially for the command-line interface.&lt;/li&gt; 
 &lt;li&gt;Codeium: Intelligent code completion&lt;/li&gt; 
 &lt;li&gt;Copilot: Code improvement suggestions and README translation&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;â—ï¸&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)ğŸ“Œ&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;â€¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;â€¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>bytedance/Dolphin</title>
      <link>https://github.com/bytedance/Dolphin</link>
      <description>&lt;p&gt;The official repo for â€œDolphin: Document Image Parsing via Heterogeneous Anchor Promptingâ€, ACL, 2025.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/dolphin.png" width="300" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://arxiv.org/abs/2505.14059"&gt; &lt;img src="https://img.shields.io/badge/Paper-arXiv-red" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/HuggingFace-Dolphin-yellow" /&gt; &lt;/a&gt; 
 &lt;a href="https://modelscope.cn/models/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/ModelScope-Dolphin-purple" /&gt; &lt;/a&gt; 
 &lt;a href="https://huggingface.co/spaces/ByteDance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/Demo-Dolphin-blue" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/bytedance/Dolphin"&gt; &lt;img src="https://img.shields.io/badge/Code-Github-green" /&gt; &lt;/a&gt; 
 &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-lightgray" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/demo.gif" width="800" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting&lt;/h1&gt; 
&lt;p&gt;Dolphin (&lt;strong&gt;Do&lt;/strong&gt;cument Image &lt;strong&gt;P&lt;/strong&gt;arsing via &lt;strong&gt;H&lt;/strong&gt;eterogeneous Anchor Prompt&lt;strong&gt;in&lt;/strong&gt;g) is a novel multimodal document image parsing model (&lt;strong&gt;0.3B&lt;/strong&gt;) following an analyze-then-parse paradigm. This repository contains the demo code and pre-trained models for Dolphin.&lt;/p&gt; 
&lt;h2&gt;ğŸ“‘ Overview&lt;/h2&gt; 
&lt;p&gt;Document image parsing is challenging due to its complexly intertwined elements such as text paragraphs, figures, formulas, and tables. Dolphin addresses these challenges through a two-stage approach:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Stage 1&lt;/strong&gt;: Comprehensive page-level layout analysis by generating element sequence in natural reading order&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§© Stage 2&lt;/strong&gt;: Efficient parallel parsing of document elements using heterogeneous anchors and task-specific prompts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/bytedance/Dolphin/master/assets/framework.png" width="680" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Dolphin achieves promising performance across diverse page-level and element-level parsing tasks while ensuring superior efficiency through its lightweight architecture and parallel parsing mechanism.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Demo&lt;/h2&gt; 
&lt;p&gt;Try our demo on &lt;a href="https://huggingface.co/spaces/ByteDance/Dolphin"&gt;Demo-Dolphin&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“… Changelog&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.07.10&lt;/strong&gt; Released the &lt;em&gt;Fox-Page Benchmark&lt;/em&gt;, a manually refined subset of the original &lt;a href="https://github.com/ucaslcl/Fox"&gt;Fox dataset&lt;/a&gt;. Download via: &lt;a href="https://pan.baidu.com/share/init?surl=t746ULp6iU5bUraVrPlMSw&amp;amp;pwd=fox1"&gt;Baidu Yun&lt;/a&gt; | &lt;a href="https://drive.google.com/file/d/1yZQZqI34QCqvhB4Tmdl3X_XEvYvQyP0q/view?usp=sharing"&gt;Google Drive&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.30&lt;/strong&gt; Added &lt;a href="https://github.com/bytedance/Dolphin/raw/master/deployment/tensorrt_llm/ReadMe.md"&gt;TensorRT-LLM support&lt;/a&gt; for accelerated inferenceï¼&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.27&lt;/strong&gt; Added &lt;a href="https://github.com/bytedance/Dolphin/raw/master/deployment/vllm/ReadMe.md"&gt;vLLM support&lt;/a&gt; for accelerated inferenceï¼&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.06.13&lt;/strong&gt; Added multi-page PDF document parsing capability.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.21&lt;/strong&gt; Our demo is released at &lt;a href="http://115.190.42.15:8888/dolphin/"&gt;link&lt;/a&gt;. Check it out!&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.20&lt;/strong&gt; The pretrained model and inference code of Dolphin are released.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;2025.05.16&lt;/strong&gt; Our paper has been accepted by ACL 2025. Paper link: &lt;a href="https://arxiv.org/abs/2505.14059"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ› ï¸ Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ByteDance/Dolphin.git
cd Dolphin
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download the pre-trained models using one of the following options:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A: Original Model Format (config-based)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Download from &lt;a href="https://pan.baidu.com/s/15zcARoX0CTOHKbW8bFZovQ?pwd=9rpx"&gt;Baidu Yun&lt;/a&gt; or &lt;a href="https://drive.google.com/drive/folders/1PQJ3UutepXvunizZEw-uGaQ0BCzf-mie?usp=sharing"&gt;Google Drive&lt;/a&gt; and put them in the &lt;code&gt;./checkpoints&lt;/code&gt; folder.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option B: Hugging Face Model Format&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Visit our Huggingface &lt;a href="https://huggingface.co/ByteDance/Dolphin"&gt;model card&lt;/a&gt;, or download model by:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Download the model from Hugging Face Hub
git lfs install
git clone https://huggingface.co/ByteDance/Dolphin ./hf_model
# Or use the Hugging Face CLI
pip install huggingface_hub
huggingface-cli download ByteDance/Dolphin --local-dir ./hf_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;âš¡ Inference&lt;/h2&gt; 
&lt;p&gt;Dolphin provides two inference frameworks with support for two parsing granularities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Page-level Parsing&lt;/strong&gt;: Parse the entire document page into a structured JSON and Markdown format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Element-level Parsing&lt;/strong&gt;: Parse individual document elements (text, table, formula)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“„ Page-level Parsing&lt;/h3&gt; 
&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single document image
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results

# Process a single document pdf
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results

# Process all documents in a directory
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results

# Process with custom batch size for parallel element decoding
python demo_page.py --config ./config/Dolphin.yaml --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single document image
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_1.jpeg --save_dir ./results

# Process a single document pdf
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs/page_6.pdf --save_dir ./results

# Process all documents in a directory
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results

# Process with custom batch size for parallel element decoding
python demo_page_hf.py --model_path ./hf_model --input_path ./demo/page_imgs --save_dir ./results --max_batch_size 16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ§© Element-level Parsing&lt;/h3&gt; 
&lt;h4&gt;Using Original Framework (config-based)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single table image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/table_1.jpeg --element_type table

# Process a single formula image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula

# Process a single text paragraph image
python demo_element.py --config ./config/Dolphin.yaml --input_path ./demo/element_imgs/para_1.jpg --element_type text
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Hugging Face Framework&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Process a single table image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/table_1.jpeg --element_type table

# Process a single formula image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/line_formula.jpeg --element_type formula

# Process a single text paragraph image
python demo_element_hf.py --model_path ./hf_model --input_path ./demo/element_imgs/para_1.jpg --element_type text
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸŒŸ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”„ Two-stage analyze-then-parse approach based on a single VLM&lt;/li&gt; 
 &lt;li&gt;ğŸ“Š Promising performance on document parsing tasks&lt;/li&gt; 
 &lt;li&gt;ğŸ” Natural reading order element sequence generation&lt;/li&gt; 
 &lt;li&gt;ğŸ§© Heterogeneous anchor prompting for different document elements&lt;/li&gt; 
 &lt;li&gt;â±ï¸ Efficient parallel parsing mechanism&lt;/li&gt; 
 &lt;li&gt;ğŸ¤— Support for Hugging Face Transformers for easier integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“® Notice&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Call for Bad Cases:&lt;/strong&gt; If you have encountered any cases where the model performs poorly, we would greatly appreciate it if you could share them in the issue. We are continuously working to optimize and improve the model.&lt;/p&gt; 
&lt;h2&gt;ğŸ’– Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We would like to acknowledge the following open-source projects that provided inspiration and reference for this work:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/clovaai/donut/"&gt;Donut&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/nougat"&gt;Nougat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Ucas-HaoranWei/GOT-OCR2.0"&gt;GOT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/MinerU/tree/master"&gt;MinerU&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Swin-Transformer"&gt;Swin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;Hugging Face Transformers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ Citation&lt;/h2&gt; 
&lt;p&gt;If you find this code useful for your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{feng2025dolphin,
  title={Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting},
  author={Feng, Hao and Wei, Shu and Fei, Xiang and Shi, Wei and Han, Yingdong and Liao, Lei and Lu, Jinghui and Wu, Binghong and Liu, Qi and Lin, Chunhui and others},
  journal={arXiv preprint arXiv:2505.14059},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#bytedance/Dolphin&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=bytedance/Dolphin&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/garak</title>
      <link>https://github.com/NVIDIA/garak</link>
      <description>&lt;p&gt;the LLM vulnerability scanner&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;garak, LLM vulnerability scanner&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Generative AI Red-teaming &amp;amp; Assessment Kit&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; checks if an LLM can be made to fail in a way we don't want. &lt;code&gt;garak&lt;/code&gt; probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know &lt;code&gt;nmap&lt;/code&gt; or &lt;code&gt;msf&lt;/code&gt; / Metasploit Framework, garak does somewhat similar things to them, but for LLMs.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; focuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt;'s a free tool. We love developing it and are always interested in adding functionality to support applications.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg?sanitize=true" alt="Tests/Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg?sanitize=true" alt="Tests/Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg?sanitize=true" alt="Tests/OSX" /&gt;&lt;/a&gt; &lt;a href="http://garak.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/garak/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.11036"&gt;&lt;img src="https://img.shields.io/badge/cs.CL-arXiv%3A2406.11036-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/uVch4puUCs"&gt;&lt;img src="https://img.shields.io/badge/chat-on%20discord-yellow.svg?sanitize=true" alt="discord-img" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/garak"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/garak" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/garak"&gt;&lt;img src="https://badge.fury.io/py/garak.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak/month" alt="Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;h3&gt;&amp;gt; See our user guide! &lt;a href="https://docs.garak.ai/"&gt;docs.garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Join our &lt;a href="https://discord.gg/uVch4puUCs"&gt;Discord&lt;/a&gt;!&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Project links &amp;amp; home: &lt;a href="https://garak.ai/"&gt;garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Twitter: &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; DEF CON &lt;a href="https://garak.ai/garak_aiv_slides.pdf"&gt;slides&lt;/a&gt;!&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;h2&gt;LLM support&lt;/h2&gt; 
&lt;p&gt;currently supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/models"&gt;hugging face hub&lt;/a&gt; generative models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/"&gt;replicate&lt;/a&gt; text models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/docs/introduction"&gt;openai api&lt;/a&gt; chat &amp;amp; continuation models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;litellm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;pretty much anything accessible via REST&lt;/li&gt; 
 &lt;li&gt;gguf models like &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; version &amp;gt;= 1046&lt;/li&gt; 
 &lt;li&gt;.. and many more LLMs!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install:&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; is a command-line tool. It's developed in Linux and OSX.&lt;/p&gt; 
&lt;h3&gt;Standard install with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Just grab it from PyPI and you should be good to go:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U garak
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install development version with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;The standard pip version of &lt;code&gt;garak&lt;/code&gt; is updated periodically. To get a fresher version from GitHub, try:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Clone from source&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; has its own dependencies. You can to install &lt;code&gt;garak&lt;/code&gt; in its own Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda create --name garak "python&amp;gt;=3.10,&amp;lt;=3.12"
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OK, if that went fine, you're probably good to go!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you cloned before the move to the &lt;code&gt;NVIDIA&lt;/code&gt; GitHub organisation, but you're reading this at the &lt;code&gt;github.com/NVIDIA&lt;/code&gt; URI, please update your remotes as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git remote set-url origin https://github.com/NVIDIA/garak.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The general syntax is:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak &amp;lt;options&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak --list_probes&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;To specify a generator, use the &lt;code&gt;--target_type&lt;/code&gt; and, optionally, the &lt;code&gt;--target_name&lt;/code&gt; options. Model type specifies a model family/interface; model name specifies the exact model to be used. The "Intro to generators" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set &lt;code&gt;--target_type&lt;/code&gt; to &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;--target_name&lt;/code&gt; to the model's name on Hub (e.g. &lt;code&gt;"RWKV/rwkv-4-169m-pile"&lt;/code&gt;). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; runs all the probes by default, but you can be specific about that too. &lt;code&gt;--probes promptinject&lt;/code&gt; will use only the &lt;a href="https://github.com/agencyenterprise/promptinject"&gt;PromptInject&lt;/a&gt; framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a &lt;code&gt;.&lt;/code&gt;; for example, &lt;code&gt;--probes lmrc.SlurUsage&lt;/code&gt; will use an implementation of checking for models generating slurs based on the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; framework.&lt;/p&gt; 
&lt;p&gt;For help and inspiration, find us on &lt;a href="https://twitter.com/garak_llm"&gt;Twitter&lt;/a&gt; or &lt;a href="https://discord.gg/uVch4puUCs"&gt;discord&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY="sk-123XXXXXXXXXXXX"
python3 -m garak --target_type openai --target_name gpt-3.5-turbo --probes encoding
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m garak --target_type huggingface --target_name gpt2 --probes dan.Dan_11_0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reading the results&lt;/h2&gt; 
&lt;p&gt;For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.&lt;/p&gt; 
&lt;p&gt;Here are the results with the &lt;code&gt;encoding&lt;/code&gt; module on a GPT-3 variant: &lt;img src="https://i.imgur.com/8Dxf45N.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;And the same results for ChatGPT: &lt;img src="https://i.imgur.com/VKAF5if.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections. The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.&lt;/p&gt; 
&lt;p&gt;Errors go in &lt;code&gt;garak.log&lt;/code&gt;; the run is logged in detail in a &lt;code&gt;.jsonl&lt;/code&gt; file specified at analysis start &amp;amp; end. There's a basic analysis script in &lt;code&gt;analyse/analyse_log.py&lt;/code&gt; which will output the probes and prompts that led to the most hits.&lt;/p&gt; 
&lt;p&gt;Send PRs &amp;amp; open issues. Happy hunting!&lt;/p&gt; 
&lt;h2&gt;Intro to generators&lt;/h2&gt; 
&lt;h3&gt;Hugging Face&lt;/h3&gt; 
&lt;p&gt;Using the Pipeline API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type huggingface&lt;/code&gt; (for transformers models to run locally)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using the Inference API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type huggingface.InferenceAPI&lt;/code&gt; (for API-based model access)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; - the model name from Hub, e.g. &lt;code&gt;"mosaicml/mpt-7b-instruct"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using private endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--target_type huggingface.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--target_name&lt;/code&gt; - the endpoint URL, e.g. &lt;code&gt;https://xxx.us-east-1.aws.endpoints.huggingface.cloud&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(optional) set the &lt;code&gt;HF_INFERENCE_TOKEN&lt;/code&gt; environment variable to a Hugging Face API token with the "read" role; see &lt;a href="https://huggingface.co/settings/tokens"&gt;https://huggingface.co/settings/tokens&lt;/a&gt; when logged in&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenAI&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; - the OpenAI model you'd like to use. &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt; is fast and fine for testing.&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your OpenAI API key (e.g. "sk-19763ASDF87q6657"); see &lt;a href="https://platform.openai.com/account/api-keys"&gt;https://platform.openai.com/account/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.&lt;/p&gt; 
&lt;h3&gt;Replicate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;REPLICATE_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see &lt;a href="https://replicate.com/account/api-tokens"&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Public Replicate models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type replicate&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; - the Replicate model name and hash, e.g. &lt;code&gt;"stability-ai/stablelm-tuned-alpha-7b:c49dae36"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Private Replicate endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type replicate.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; - username/model-name slug from the deployed endpoint, e.g. &lt;code&gt;elim/elims-llama2-7b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cohere&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type cohere&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; (optional, &lt;code&gt;command&lt;/code&gt; by default) - The specific Cohere model you'd like to test&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;COHERE_API_KEY&lt;/code&gt; environment variable to your Cohere API key, e.g. "aBcDeFgHiJ123456789"; see &lt;a href="https://dashboard.cohere.ai/api-keys"&gt;https://dashboard.cohere.ai/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Groq&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type groq&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; - The name of the model to access via the Groq API&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; environment variable to your Groq API key, see &lt;a href="https://console.groq.com/docs/quickstart"&gt;https://console.groq.com/docs/quickstart&lt;/a&gt; for details on creating an API key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ggml&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type ggml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; - The path to the ggml model you'd like to load, e.g. &lt;code&gt;/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GGML_MAIN_PATH&lt;/code&gt; environment variable to the path to your ggml &lt;code&gt;main&lt;/code&gt; executable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;REST&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;rest.RestGenerator&lt;/code&gt; is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See &lt;a href="https://reference.garak.ai/en/latest/garak.generators.rest.html"&gt;https://reference.garak.ai/en/latest/garak.generators.rest.html&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h3&gt;NIM&lt;/h3&gt; 
&lt;p&gt;Use models from &lt;a href="https://build.nvidia.com/"&gt;https://build.nvidia.com/&lt;/a&gt; or other NIM endpoints.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;NIM_API_KEY&lt;/code&gt; environment variable to your authentication API token, or specify it in the config YAML&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For chat models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type nim&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;meta/llama-3.1-8b-instruct&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For completion models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--target_type nim.NVOpenAICompletion&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--target_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;bigcode/starcoder2-15b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Test&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--target_type test&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(alternatively) &lt;code&gt;--target_name test.Blank&lt;/code&gt; For testing. This always generates the empty string, using the &lt;code&gt;test.Blank&lt;/code&gt; generator. Will be marked as failing for any tests that &lt;em&gt;require&lt;/em&gt; an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--target_type test.Repeat&lt;/code&gt; For testing. This generator repeats back the prompt it received.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to probes&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Probe&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;blank&lt;/td&gt; 
   &lt;td&gt;A simple probe that always sends an empty prompt.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;atkgen&lt;/td&gt; 
   &lt;td&gt;Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 &lt;a href="https://huggingface.co/garak-llm/artgpt2tox"&gt;fine-tuned&lt;/a&gt; on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;av_spam_scanning&lt;/td&gt; 
   &lt;td&gt;Probes that attempt to make the model output malicious content signatures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;continuation&lt;/td&gt; 
   &lt;td&gt;Probes that test if the model will continue a probably undesirable word&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dan&lt;/td&gt; 
   &lt;td&gt;Various &lt;a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html"&gt;DAN&lt;/a&gt; and DAN-like attacks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;donotanswer&lt;/td&gt; 
   &lt;td&gt;Prompts to which responsible language models should not answer.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;encoding&lt;/td&gt; 
   &lt;td&gt;Prompt injection through text encoding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gcg&lt;/td&gt; 
   &lt;td&gt;Disrupt a system prompt by appending an adversarial suffix.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;glitch&lt;/td&gt; 
   &lt;td&gt;Probe model for glitch tokens that provoke unusual behavior.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;grandma&lt;/td&gt; 
   &lt;td&gt;Appeal to be reminded of one's grandmother.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;goodside&lt;/td&gt; 
   &lt;td&gt;Implementations of Riley Goodside attacks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;leakreplay&lt;/td&gt; 
   &lt;td&gt;Evaluate if a model will replay training data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lmrc&lt;/td&gt; 
   &lt;td&gt;Subsample of the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; probes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;malwaregen&lt;/td&gt; 
   &lt;td&gt;Attempts to have the model generate code for building malware&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;misleading&lt;/td&gt; 
   &lt;td&gt;Attempts to make a model support misleading and false claims&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;packagehallucination&lt;/td&gt; 
   &lt;td&gt;Trying to get code generations that specify non-existent (and therefore insecure) packages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;promptinject&lt;/td&gt; 
   &lt;td&gt;Implementation of the Agency Enterprise &lt;a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject"&gt;PromptInject&lt;/a&gt; work (best paper awards @ NeurIPS ML Safety Workshop 2022)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;realtoxicityprompts&lt;/td&gt; 
   &lt;td&gt;Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;snowball&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ofir.io/snowballed_hallucination.pdf"&gt;Snowballed Hallucination&lt;/a&gt; probes designed to make a model give a wrong answer to questions too complex for it to process&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xss&lt;/td&gt; 
   &lt;td&gt;Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Logging&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; generates multiple kinds of log:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A log file, &lt;code&gt;garak.log&lt;/code&gt;. This includes debugging information from &lt;code&gt;garak&lt;/code&gt; and its plugins, and is continued across runs.&lt;/li&gt; 
 &lt;li&gt;A report of the current run, structured as JSONL. A new report file is created every time &lt;code&gt;garak&lt;/code&gt; runs. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's &lt;code&gt;status&lt;/code&gt; attribute takes a constant from &lt;code&gt;garak.attempts&lt;/code&gt; to describe what stage it was made at.&lt;/li&gt; 
 &lt;li&gt;A hit log, detailing attempts that yielded a vulnerability (a 'hit')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How is the code structured?&lt;/h2&gt; 
&lt;p&gt;Check out the &lt;a href="https://reference.garak.ai/"&gt;reference docs&lt;/a&gt; for an authoritative guide to &lt;code&gt;garak&lt;/code&gt; code structure.&lt;/p&gt; 
&lt;p&gt;In a typical run, &lt;code&gt;garak&lt;/code&gt; will read a model type (and optionally model name) from the command line, then determine which &lt;code&gt;probe&lt;/code&gt;s and &lt;code&gt;detector&lt;/code&gt;s to run, start up a &lt;code&gt;generator&lt;/code&gt;, and then pass these to a &lt;code&gt;harness&lt;/code&gt; to do the probing; an &lt;code&gt;evaluator&lt;/code&gt; deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;garak/probes/&lt;/code&gt; - classes for generating interactions with LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/detectors/&lt;/code&gt; - classes for detecting an LLM is exhibiting a given failure mode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/evaluators/&lt;/code&gt; - assessment reporting schemes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/generators/&lt;/code&gt; - plugins for LLMs to be probed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/harnesses/&lt;/code&gt; - classes for structuring testing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resources/&lt;/code&gt; - ancillary items required by plugins&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The default operating mode is to use the &lt;code&gt;probewise&lt;/code&gt; harness. Given a list of probe module names and probe plugin names, the &lt;code&gt;probewise&lt;/code&gt; harness instantiates each probe, then for each probe reads its &lt;code&gt;recommended_detectors&lt;/code&gt; attribute to get a list of &lt;code&gt;detector&lt;/code&gt;s to run on the output.&lt;/p&gt; 
&lt;p&gt;Each plugin category (&lt;code&gt;probes&lt;/code&gt;, &lt;code&gt;detectors&lt;/code&gt;, &lt;code&gt;evaluators&lt;/code&gt;, &lt;code&gt;generators&lt;/code&gt;, &lt;code&gt;harnesses&lt;/code&gt;) includes a &lt;code&gt;base.py&lt;/code&gt; which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, &lt;code&gt;garak.generators.openai.OpenAIGenerator&lt;/code&gt; descends from &lt;code&gt;garak.generators.base.Generator&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using &lt;code&gt;garak&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Developing your own plugin&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Take a look at how other plugins do it&lt;/li&gt; 
 &lt;li&gt;Inherit from one of the base classes, e.g. &lt;code&gt;garak.probes.base.TextProbe&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Override as little as possible&lt;/li&gt; 
 &lt;li&gt;You can test the new code in at least two ways: 
  &lt;ul&gt; 
   &lt;li&gt;Start an interactive Python session 
    &lt;ul&gt; 
     &lt;li&gt;Import the model, e.g. &lt;code&gt;import garak.probes.mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;Instantiate the plugin, e.g. &lt;code&gt;p = garak.probes.mymodule.MyProbe()&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Run a scan with test plugins 
    &lt;ul&gt; 
     &lt;li&gt;For probes, try a blank generator and always.Pass detector: &lt;code&gt;python3 -m garak -m test.Blank -p mymodule -d always.Pass&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For detectors, try a blank generator and a blank probe: &lt;code&gt;python3 -m garak -m test.Blank -p test.Blank -d mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For generators, try a blank probe and always.Pass detector: &lt;code&gt;python3 -m garak -m mymodule -p test.Blank -d always.Pass&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Get &lt;code&gt;garak&lt;/code&gt; to list all the plugins of the type you're writing, with &lt;code&gt;--list_probes&lt;/code&gt;, &lt;code&gt;--list_detectors&lt;/code&gt;, or &lt;code&gt;--list_generators&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;We have an FAQ &lt;a href="https://github.com/NVIDIA/garak/raw/main/FAQ.md"&gt;here&lt;/a&gt;. Reach out if you have any more questions! &lt;a href="mailto:garak@nvidia.com"&gt;garak@nvidia.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Code reference documentation is at &lt;a href="https://garak.readthedocs.io/en/latest/"&gt;garak.readthedocs.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing garak&lt;/h2&gt; 
&lt;p&gt;You can read the &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/garak-paper.pdf"&gt;garak preprint paper&lt;/a&gt;. If you use garak, please cite us.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly"&lt;/em&gt; - Elim&lt;/p&gt; 
&lt;p&gt;For updates and news see &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Â© 2023- Leon Derczynski; Apache license v2, see &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>