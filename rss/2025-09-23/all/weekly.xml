<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Mon, 22 Sep 2025 01:42:57 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>nocodb/nocodb</title>
      <link>https://github.com/nocodb/nocodb</link>
      <description>&lt;p&gt;üî• üî• üî• Open Source Airtable Alternative&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center" style="border-bottom: none"&gt; 
 &lt;div&gt; 
  &lt;a style="color:#36f" href="https://www.nocodb.com"&gt; &lt;img src="https://raw.githubusercontent.com/nocodb/nocodb/develop/packages/nc-gui/assets/img/brand/nocodb-full.png" height="80" /&gt; &lt;br /&gt; The Open Source Airtable Alternative &lt;/a&gt; 
  &lt;br /&gt; 
 &lt;/div&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; NocoDB is the fastest and easiest way to build databases online. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="http://www.nocodb.com"&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href="https://discord.gg/5RgZmkW"&gt;&lt;b&gt;Discord&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href="https://community.nocodb.com/"&gt;&lt;b&gt;Community&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href="https://twitter.com/nocodb"&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.reddit.com/r/NocoDB/"&gt;&lt;b&gt;Reddit&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href="https://docs.nocodb.com/"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nocodb/nocodb/assets/86527202/e2fad786-f211-4dcb-9bd3-aaece83a6783" alt="video avi" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/chinese.md"&gt;&lt;img height="38" src="https://user-images.githubusercontent.com/61551451/135263434-75fe793d-42af-49e4-b964-d70920e41655.png" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/french.md"&gt;&lt;img height="38" src="https://user-images.githubusercontent.com/61551451/135263474-787d71e7-3a87-42a8-92a8-be1d1f55413d.png" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/german.md"&gt;&lt;img height="38" src="https://user-images.githubusercontent.com/61551451/135263531-fae58600-6616-4b43-95a0-5891019dd35d.png" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/spanish.md"&gt;&lt;img height="38" src="https://user-images.githubusercontent.com/61551451/135263589-3dbeda9a-0d2e-4bbd-b1fc-691404bb74fb.png" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/portuguese.md"&gt;&lt;img height="38" src="https://user-images.githubusercontent.com/61551451/135263669-f567196a-d4e8-4143-a80a-93d3be32ba90.png" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/italian.md"&gt;&lt;img height="38" src="https://user-images.githubusercontent.com/61551451/135263707-ba4e04a4-268a-4626-91b8-048e572fd9f6.png" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/japanese.md"&gt;&lt;img height="38" src="https://user-images.githubusercontent.com/61551451/135263770-38e3e79d-11d4-472e-ac27-ae0f17cf65c4.png" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/korean.md"&gt;&lt;img height="38" src="https://user-images.githubusercontent.com/61551451/135263822-28fce9de-915a-44dc-962d-7a61d340e91d.png" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/russian.md"&gt;&lt;img height="38" src="https://user-images.githubusercontent.com/61551451/135263888-151d4ad1-7084-4943-97c9-56f28cd40b80.png" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/markdown/readme/languages/README.md"&gt;&lt;b&gt;See other languages ¬ª&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://static.scarf.sh/a.png?x-pxid=c12a77cc-855e-4602-8a0f-614b2d0da56a" /&gt; 
&lt;h1&gt;Join Our Community&lt;/h1&gt; 
&lt;a href="https://discord.gg/5RgZmkW" target="_blank"&gt; &lt;img src="https://discordapp.com/api/guilds/661905455894888490/widget.png?style=banner3" alt="" /&gt; &lt;/a&gt; 
&lt;p&gt;&lt;a href="https://github.com/nocodb/nocodb/stargazers"&gt;&lt;img src="http://reporoster.com/stars/nocodb/nocodb" alt="Stargazers repo roster for @nocodb/nocodb" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;h2&gt;Docker with SQLite&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name noco \
  -v "$(pwd)"/nocodb:/usr/app/data/ \
  -p 8080:8080 \
  nocodb/nocodb:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker with PG&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name noco \
  -v "$(pwd)"/nocodb:/usr/app/data/ \
  -p 8080:8080 \
  -e NC_DB="pg://host.docker.internal:5432?u=root&amp;amp;p=password&amp;amp;d=d1" \
  -e NC_AUTH_JWT_SECRET="569a1821-0a93-45e8-87ab-eb857f20a010" \
  nocodb/nocodb:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Auto-upstall&lt;/h2&gt; 
&lt;p&gt;Auto-upstall is a single command that sets up NocoDB on a server for production usage. Behind the scenes it auto-generates docker-compose for you.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash &amp;lt;(curl -sSL http://install.nocodb.com/noco.sh) &amp;lt;(mktemp)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Auto-upstall does the following: üïä&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üê≥ Automatically installs all pre-requisites like docker, docker-compose&lt;/li&gt; 
 &lt;li&gt;üöÄ Automatically installs NocoDB with PostgreSQL, Redis, Minio, Traefik gateway using Docker Compose. üêò üóÑÔ∏è üåê&lt;/li&gt; 
 &lt;li&gt;üîÑ Automatically upgrades NocoDB to the latest version when you run the command again.&lt;/li&gt; 
 &lt;li&gt;üîí Automatically setups SSL and also renews it. Needs a domain or subdomain as input while installation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;install.nocodb.com/noco.sh script can be found &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/docker-compose/1_Auto_Upstall/noco.sh"&gt;here in our github&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Other Methods&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Binaries are only for quick testing locally.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Install Method&lt;/th&gt; 
   &lt;th&gt;Command to install&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üçè MacOS arm64 &lt;br /&gt;(Binary)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;curl http://get.nocodb.com/macos-arm64 -o nocodb -L &amp;amp;&amp;amp; chmod +x nocodb &amp;amp;&amp;amp; ./nocodb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üçè MacOS x64 &lt;br /&gt;(Binary)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;curl http://get.nocodb.com/macos-x64 -o nocodb -L &amp;amp;&amp;amp; chmod +x nocodb &amp;amp;&amp;amp; ./nocodb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üêß Linux arm64 &lt;br /&gt;(Binary)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;curl http://get.nocodb.com/linux-arm64 -o nocodb -L &amp;amp;&amp;amp; chmod +x nocodb &amp;amp;&amp;amp; ./nocodb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üêß Linux x64 &lt;br /&gt;(Binary)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;curl http://get.nocodb.com/linux-x64 -o nocodb -L &amp;amp;&amp;amp; chmod +x nocodb &amp;amp;&amp;amp; ./nocodb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ü™ü Windows arm64 &lt;br /&gt;(Binary)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;iwr http://get.nocodb.com/win-arm64.exe -OutFile Noco-win-arm64.exe &amp;amp;&amp;amp; .\Noco-win-arm64.exe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ü™ü Windows x64 &lt;br /&gt;(Binary)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;iwr http://get.nocodb.com/win-x64.exe -OutFile Noco-win-x64.exe &amp;amp;&amp;amp; .\Noco-win-x64.exe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;When running locally access nocodb by visiting: &lt;a href="http://localhost:8080/dashboard"&gt;http://localhost:8080/dashboard&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more installation methods, please refer to &lt;a href="https://docs.nocodb.com/category/installation"&gt;our docs&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Screenshots&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://github.com/nocodb/nocodb/assets/86527202/a127c05e-2121-4af2-a342-128e0e2d0291" alt="2" /&gt; &lt;img src="https://github.com/nocodb/nocodb/assets/86527202/674da952-8a06-4848-a0e8-a7b02d5f5c88" alt="3" /&gt; &lt;img src="https://github.com/nocodb/nocodb/assets/86527202/cbc5152a-9caf-4f77-a8f7-92a9d06d025b" alt="4" /&gt; &lt;img src="https://github.com/nocodb/nocodb/assets/86527202/dc75dfdc-c486-4f5a-a853-2a8f9e6b569a" alt="5" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/35857179/194844886-a17006e0-979d-493f-83c4-0e72f5a9b716.png" alt="5" /&gt; &lt;img src="https://github.com/nocodb/nocodb/assets/86527202/be64e619-7295-43e2-aa95-cace4462b17f" alt="7" /&gt; &lt;img src="https://github.com/nocodb/nocodb/assets/86527202/4538bf5a-371f-4ec1-a867-8197e5824286" alt="8" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/35857179/194844893-82d5e21b-ae61-41bd-9990-31ad659bf490.png" alt="8" /&gt; &lt;img src="https://user-images.githubusercontent.com/35857179/194844897-cfd79946-e413-4c97-b16d-eb4d7678bb79.png" alt="9" /&gt; &lt;img src="https://user-images.githubusercontent.com/35857179/194844902-c0122570-0dd5-41cf-a26f-6f8d71fefc99.png" alt="10" /&gt; &lt;img src="https://user-images.githubusercontent.com/35857179/194844903-c1e47f40-e782-4f5d-8dce-6449cc70b181.png" alt="11" /&gt; &lt;img src="https://user-images.githubusercontent.com/35857179/194844907-09277d3e-cbbf-465c-9165-6afc4161e279.png" alt="12" /&gt;&lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;h3&gt;Rich Spreadsheet Interface&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;Basic Operations: Create, Read, Update and Delete Tables, Columns, and Rows&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;Fields Operations: Sort, Filter, Group, Hide / Unhide Columns&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;Multiple Views Types: Grid (By default), Gallery, Form, Kanban and Calendar View&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;View Permissions Types: Collaborative Views, &amp;amp; Locked Views&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;Share Bases / Views: either Public or Private (with Password Protected)&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;Variant Cell Types: ID, Links, Lookup, Rollup, SingleLineText, Attachment, Currency, Formula, User, etc&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;Access Control with Roles: Fine-grained Access Control at different levels&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;and more ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;App Store for Workflow Automations&lt;/h3&gt; 
&lt;p&gt;We provide different integrations in three main categories. See &lt;a href="https://docs.nocodb.com/account-settings/oss-specific-details/#app-store" target="_blank"&gt;App Store&lt;/a&gt; for details.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;Chat: Slack, Discord, Mattermost, and etc&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;Email: AWS SES, SMTP, MailerSend, and etc&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;Storage: AWS S3, Google Cloud Storage, Minio, and etc&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Programmatic Access&lt;/h3&gt; 
&lt;p&gt;We provide the following ways to let users programmatically invoke actions. You can use a token (either JWT or Social Auth) to sign your requests for authorization to NocoDB.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;REST APIs&lt;/li&gt; 
 &lt;li&gt;‚ö° &amp;nbsp;NocoDB SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;Please refer to &lt;a href="https://github.com/nocodb/nocodb/raw/master/.github/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Why are we building this?&lt;/h1&gt; 
&lt;p&gt;Most internet businesses equip themselves with either spreadsheet or a database to solve their business needs. Spreadsheets are used by Billion+ humans collaboratively every single day. However, we are way off working at similar speeds on databases which are way more powerful tools when it comes to computing. Attempts to solve this with SaaS offerings have meant horrible access controls, vendor lock-in, data lock-in, abrupt price changes &amp;amp; most importantly a glass ceiling on what's possible in the future.&lt;/p&gt; 
&lt;h1&gt;Our Mission&lt;/h1&gt; 
&lt;p&gt;Our mission is to provide the most powerful no-code interface for databases that is open source to every single internet business in the world. This would not only democratise access to a powerful computing tool but also bring forth a billion+ people who will have radical tinkering-and-building abilities on the internet.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt; This project is licensed under &lt;a href="https://raw.githubusercontent.com/nocodb/nocodb/develop/LICENSE"&gt;AGPLv3&lt;/a&gt;. &lt;/p&gt; 
&lt;h1&gt;Contributors&lt;/h1&gt; 
&lt;p&gt;Thank you for your contributions! We appreciate all the contributions from the community.&lt;/p&gt; 
&lt;a href="https://github.com/nocodb/nocodb/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=nocodb/nocodb" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>CorentinJ/Real-Time-Voice-Cloning</title>
      <link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link>
      <description>&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; 
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href="https://matheo.uliege.be/handle/2268.2/6801"&gt;master's thesis&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA"&gt;&lt;img src="https://i.imgur.com/8lFUlgz.png" alt="Toolbox demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Papers implemented&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;Designation&lt;/th&gt; 
   &lt;th&gt;Title&lt;/th&gt; 
   &lt;th&gt;Implementation source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf"&gt;1802.08435&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; 
   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1703.10135.pdf"&gt;1703.10135&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; 
   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf"&gt;1710.10467&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GE2E (encoder)&lt;/td&gt; 
   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Heads up&lt;/h2&gt; 
&lt;p&gt;Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out &lt;a href="https://paperswithcode.com/task/speech-synthesis/"&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;Chatterbox&lt;/a&gt; for a similar project up to date with the 2025 SOTA in voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;h3&gt;1. Install Requirements&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.&lt;/li&gt; 
 &lt;li&gt;Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using &lt;code&gt;venv&lt;/code&gt;, but this is optional.&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://ffmpeg.org/download.html#get-packages"&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files.&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt;. Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.&lt;/li&gt; 
 &lt;li&gt;Install the remaining requirements with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. (Optional) Download Pretrained Models&lt;/h3&gt; 
&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;3. (Optional) Test Configuration&lt;/h3&gt; 
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt; 
&lt;h3&gt;4. (Optional) Download Datasets&lt;/h3&gt; 
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="https://www.openslr.org/resources/12/train-clean-100.tar.gz"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt; 
&lt;h3&gt;5. Launch the Toolbox&lt;/h3&gt; 
&lt;p&gt;You can then try the toolbox:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br /&gt; or&lt;br /&gt; &lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Zie619/n8n-workflows</title>
      <link>https://github.com/Zie619/n8n-workflows</link>
      <description>&lt;p&gt;all of the workflows of n8n i could find (also from the site itself)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;‚ö° N8N Workflow Collection &amp;amp; Documentation&lt;/h1&gt; 
&lt;p&gt;A professionally organized collection of &lt;strong&gt;2,053 n8n workflows&lt;/strong&gt; with a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è IMPORTANT NOTICE (Aug 14, 2025):&lt;/strong&gt; Repository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see &lt;a href="https://github.com/Zie619/n8n-workflows/issues/85"&gt;Issue 85&lt;/a&gt; for instructions on syncing your copy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support My Work&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/zie619"&gt;&lt;img src="https://img.shields.io/badge/-Buy%20Me%20a%20Coffee-ffdd00?logo=buy-me-a-coffee&amp;amp;logoColor=black&amp;amp;style=flat" alt="Buy Me a Coffee" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you'd like to say thanks, consider buying me a coffee‚Äîyour support helps me keep improving this project!&lt;/p&gt; 
&lt;h2&gt;üöÄ &lt;strong&gt;NEW: High-Performance Documentation System&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Experience 100x performance improvement over traditional documentation!&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Quick Start - Fast Documentation System&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
pip install -r requirements.txt

# Start the fast API server
python run.py

# Open in browser
http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Sub-100ms response times&lt;/strong&gt; with SQLite FTS5 search&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Instant full-text search&lt;/strong&gt; with advanced filtering&lt;/li&gt; 
 &lt;li&gt;üì± &lt;strong&gt;Responsive design&lt;/strong&gt; - works perfectly on mobile&lt;/li&gt; 
 &lt;li&gt;üåô &lt;strong&gt;Dark/light themes&lt;/strong&gt; with system preference detection&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Live statistics&lt;/strong&gt; - 365 unique integrations, 29,445 total nodes&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Smart categorization&lt;/strong&gt; by trigger type and complexity&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Use case categorization&lt;/strong&gt; by service name mapped to categories&lt;/li&gt; 
 &lt;li&gt;üìÑ &lt;strong&gt;On-demand JSON viewing&lt;/strong&gt; and download&lt;/li&gt; 
 &lt;li&gt;üîó &lt;strong&gt;Mermaid diagram generation&lt;/strong&gt; for workflow visualization&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Real-time workflow naming&lt;/strong&gt; with intelligent formatting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Metric&lt;/th&gt; 
   &lt;th&gt;Old System&lt;/th&gt; 
   &lt;th&gt;New System&lt;/th&gt; 
   &lt;th&gt;Improvement&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;File Size&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;71MB HTML&lt;/td&gt; 
   &lt;td&gt;&amp;lt;100KB&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;700x smaller&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Load Time&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;10+ seconds&lt;/td&gt; 
   &lt;td&gt;&amp;lt;1 second&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;10x faster&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Search&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Client-side only&lt;/td&gt; 
   &lt;td&gt;Full-text with FTS5&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Instant&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;~2GB RAM&lt;/td&gt; 
   &lt;td&gt;&amp;lt;50MB RAM&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;40x less&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mobile Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Poor&lt;/td&gt; 
   &lt;td&gt;Excellent&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Fully responsive&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÇ Repository Organization&lt;/h2&gt; 
&lt;h3&gt;Workflow Collection&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,053 workflows&lt;/strong&gt; with meaningful, searchable names&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; across popular platforms&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;29,445 total nodes&lt;/strong&gt; with professional categorization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality assurance&lt;/strong&gt; - All workflows analyzed and categorized&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Naming System ‚ú®&lt;/h3&gt; 
&lt;p&gt;Our intelligent naming system converts technical filenames into readable titles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Before&lt;/strong&gt;: &lt;code&gt;2051_Telegram_Webhook_Automation_Webhook.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;After&lt;/strong&gt;: &lt;code&gt;Telegram Webhook Automation&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; with smart capitalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic integration detection&lt;/strong&gt; from node analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Use Case Category ‚ú®&lt;/h3&gt; 
&lt;p&gt;The search interface includes a dropdown filter that lets you browse 2,000+ workflows by category.&lt;/p&gt; 
&lt;p&gt;The system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.&lt;/p&gt; 
&lt;h3&gt;How Categorization Works&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the categorization script&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python create_categories.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Service Name Recognition&lt;/strong&gt; The script analyzes each workflow JSON filename to identify recognized service names (e.g., "Twilio", "Slack", "Gmail", etc.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Category Mapping&lt;/strong&gt; Each recognized service name is matched to its corresponding category using the definitions in &lt;code&gt;context/def_categories.json&lt;/code&gt;. For example:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Twilio ‚Üí Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Gmail ‚Üí Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Airtable ‚Üí Data Processing &amp;amp; Analysis&lt;/li&gt; 
   &lt;li&gt;Salesforce ‚Üí CRM &amp;amp; Sales&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Search Categories Generation&lt;/strong&gt; The script produces a &lt;code&gt;search_categories.json&lt;/code&gt; file that contains the categorized workflow data&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Filter Interface&lt;/strong&gt; Users can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Available Categories&lt;/h3&gt; 
&lt;p&gt;The categorization system includes the following main categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Agent Development&lt;/li&gt; 
 &lt;li&gt;Business Process Automation&lt;/li&gt; 
 &lt;li&gt;Cloud Storage &amp;amp; File Management&lt;/li&gt; 
 &lt;li&gt;Communication &amp;amp; Messaging&lt;/li&gt; 
 &lt;li&gt;Creative Content &amp;amp; Video Automation&lt;/li&gt; 
 &lt;li&gt;Creative Design Automation&lt;/li&gt; 
 &lt;li&gt;CRM &amp;amp; Sales&lt;/li&gt; 
 &lt;li&gt;Data Processing &amp;amp; Analysis&lt;/li&gt; 
 &lt;li&gt;E-commerce &amp;amp; Retail&lt;/li&gt; 
 &lt;li&gt;Financial &amp;amp; Accounting&lt;/li&gt; 
 &lt;li&gt;Marketing &amp;amp; Advertising Automation&lt;/li&gt; 
 &lt;li&gt;Project Management&lt;/li&gt; 
 &lt;li&gt;Social Media Management&lt;/li&gt; 
 &lt;li&gt;Technical Infrastructure &amp;amp; DevOps&lt;/li&gt; 
 &lt;li&gt;Web Scraping &amp;amp; Data Extraction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contribute Categories&lt;/h3&gt; 
&lt;p&gt;You can help expand the categorization by adding more service-to-category mappings (e.g., Twilio ‚Üí Communication &amp;amp; Messaging) in context/defs_categories.json.&lt;/p&gt; 
&lt;p&gt;Many workflow JSON files are conveniently named with the service name, often separated by underscores (_).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üõ† Usage Instructions&lt;/h2&gt; 
&lt;h3&gt;Option 1: Modern Fast System (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install Python dependencies
pip install -r requirements.txt

# Start the documentation server
python run.py

# Browse workflows at http://localhost:8000
# - Instant search across 2,053 workflows
# - Professional responsive interface
# - Real-time workflow statistics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Development Mode&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start with auto-reload for development
python run.py --dev

# Or specify custom host/port
python run.py --host 0.0.0.0 --port 3000

# Force database reindexing
python run.py --reindex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Import Workflows into n8n&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use the Python importer (recommended)
python import_workflows.py

# Or manually import individual workflows:
# 1. Open your n8n Editor UI
# 2. Click menu (‚ò∞) ‚Üí Import workflow
# 3. Choose any .json file from the workflows/ folder
# 4. Update credentials/webhook URLs before running
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìä Workflow Statistics&lt;/h2&gt; 
&lt;h3&gt;Current Collection Stats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Total Workflows&lt;/strong&gt;: 2,053 automation workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Active Workflows&lt;/strong&gt;: 215 (10.5% active rate)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Total Nodes&lt;/strong&gt;: 29,445 (avg 14.3 nodes per workflow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unique Integrations&lt;/strong&gt;: 365 different services and APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: SQLite with FTS5 full-text search&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Trigger Distribution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Complex&lt;/strong&gt;: 831 workflows (40.5%) - Multi-trigger systems&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Webhook&lt;/strong&gt;: 519 workflows (25.3%) - API-triggered automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual&lt;/strong&gt;: 477 workflows (23.2%) - User-initiated workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scheduled&lt;/strong&gt;: 226 workflows (11.0%) - Time-based executions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complexity Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Low (‚â§5 nodes)&lt;/strong&gt;: ~35% - Simple automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Medium (6-15 nodes)&lt;/strong&gt;: ~45% - Standard workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High (16+ nodes)&lt;/strong&gt;: ~20% - Complex enterprise systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Popular Integrations&lt;/h3&gt; 
&lt;p&gt;Top services by usage frequency:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Storage&lt;/strong&gt;: Google Drive, Google Sheets, Dropbox&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Databases&lt;/strong&gt;: PostgreSQL, MySQL, MongoDB, Airtable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI/ML&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: HTTP Request, Webhook, GraphQL&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîç Advanced Search Features&lt;/h2&gt; 
&lt;h3&gt;Smart Search Categories&lt;/h3&gt; 
&lt;p&gt;Our system automatically categorizes workflows into 12 service categories:&lt;/p&gt; 
&lt;h4&gt;Available Categories:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;messaging&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp, Teams&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ai_ml&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;database&lt;/strong&gt;: PostgreSQL, MySQL, MongoDB, Redis, Airtable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;email&lt;/strong&gt;: Gmail, Mailjet, Outlook, SMTP/IMAP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;cloud_storage&lt;/strong&gt;: Google Drive, Google Docs, Dropbox, OneDrive&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;project_management&lt;/strong&gt;: Jira, GitHub, GitLab, Trello, Asana&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;social_media&lt;/strong&gt;: LinkedIn, Twitter/X, Facebook, Instagram&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ecommerce&lt;/strong&gt;: Shopify, Stripe, PayPal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;analytics&lt;/strong&gt;: Google Analytics, Mixpanel&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;calendar_tasks&lt;/strong&gt;: Google Calendar, Cal.com, Calendly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;forms&lt;/strong&gt;: Typeform, Google Forms, Form Triggers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;development&lt;/strong&gt;: Webhook, HTTP Request, GraphQL, SSE&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;API Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Search workflows by text
curl "http://localhost:8000/api/workflows?q=telegram+automation"

# Filter by trigger type and complexity
curl "http://localhost:8000/api/workflows?trigger=Webhook&amp;amp;complexity=high"

# Find all messaging workflows
curl "http://localhost:8000/api/workflows/category/messaging"

# Get database statistics
curl "http://localhost:8000/api/stats"

# Browse available categories
curl "http://localhost:8000/api/categories"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèó Technical Architecture&lt;/h2&gt; 
&lt;h3&gt;Modern Stack&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SQLite Database&lt;/strong&gt; - FTS5 full-text search with 365 indexed integrations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FastAPI Backend&lt;/strong&gt; - RESTful API with automatic OpenAPI documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responsive Frontend&lt;/strong&gt; - Modern HTML5 with embedded CSS/JavaScript&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Analysis&lt;/strong&gt; - Automatic workflow categorization and naming&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Change Detection&lt;/strong&gt; - MD5 hashing for efficient re-indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt; - Non-blocking workflow analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compressed Responses&lt;/strong&gt; - Gzip middleware for optimal speed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Error Handling&lt;/strong&gt; - Graceful degradation and comprehensive logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile Optimization&lt;/strong&gt; - Touch-friendly interface design&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Database Performance&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;-- Optimized schema for lightning-fast queries
CREATE TABLE workflows (
    id INTEGER PRIMARY KEY,
    filename TEXT UNIQUE,
    name TEXT,
    active BOOLEAN,
    trigger_type TEXT,
    complexity TEXT,
    node_count INTEGER,
    integrations TEXT,  -- JSON array of 365 unique services
    description TEXT,
    file_hash TEXT,     -- MD5 for change detection
    analyzed_at TIMESTAMP
);

-- Full-text search with ranking
CREATE VIRTUAL TABLE workflows_fts USING fts5(
    filename, name, description, integrations, tags,
    content='workflows', content_rowid='id'
);
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîß Setup &amp;amp; Requirements&lt;/h2&gt; 
&lt;h3&gt;System Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.7+&lt;/strong&gt; - For running the documentation system&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modern Browser&lt;/strong&gt; - Chrome, Firefox, Safari, Edge&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;50MB Storage&lt;/strong&gt; - For SQLite database and indexes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Instance&lt;/strong&gt; - For importing and running workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install dependencies
pip install -r requirements.txt

# Start documentation server
python run.py

# Access at http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or .venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run with auto-reload for development
python api_server.py --reload

# Force database reindexing
python workflow_db.py --index --force
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìã Naming Convention&lt;/h2&gt; 
&lt;h3&gt;Intelligent Formatting System&lt;/h3&gt; 
&lt;p&gt;Our system automatically converts technical filenames to user-friendly names:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Automatic transformations:
2051_Telegram_Webhook_Automation_Webhook.json ‚Üí "Telegram Webhook Automation"
0250_HTTP_Discord_Import_Scheduled.json ‚Üí "HTTP Discord Import Scheduled"  
0966_OpenAI_Data_Processing_Manual.json ‚Üí "OpenAI Data Processing Manual"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Technical Format&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Smart Capitalization Rules&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP&lt;/strong&gt; ‚Üí HTTP (not Http)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt; ‚Üí API (not Api)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;webhook&lt;/strong&gt; ‚Üí Webhook&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;automation&lt;/strong&gt; ‚Üí Automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;scheduled&lt;/strong&gt; ‚Üí Scheduled&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ API Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Endpoints&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /&lt;/code&gt; - Main workflow browser interface&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/stats&lt;/code&gt; - Database statistics and metrics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows&lt;/code&gt; - Search with filters and pagination&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}&lt;/code&gt; - Detailed workflow information&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/download&lt;/code&gt; - Download workflow JSON&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/diagram&lt;/code&gt; - Generate Mermaid diagram&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Search&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/category/{category}&lt;/code&gt; - Search by service category&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/categories&lt;/code&gt; - List all available categories&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/integrations&lt;/code&gt; - Get integration statistics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;POST /api/reindex&lt;/code&gt; - Trigger background reindexing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Response Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;// GET /api/stats
{
  "total": 2053,
  "active": 215,
  "inactive": 1838,
  "triggers": {
    "Complex": 831,
    "Webhook": 519,
    "Manual": 477,
    "Scheduled": 226
  },
  "total_nodes": 29445,
  "unique_integrations": 365
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;h3&gt;Adding New Workflows&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Export workflow&lt;/strong&gt; as JSON from n8n&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Name descriptively&lt;/strong&gt; following the established pattern&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add to workflows/&lt;/strong&gt; directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Remove sensitive data&lt;/strong&gt; (credentials, personal URLs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Run reindexing&lt;/strong&gt; to update the database&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Quality Standards&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ Workflow must be functional and tested&lt;/li&gt; 
 &lt;li&gt;‚úÖ Remove all credentials and sensitive data&lt;/li&gt; 
 &lt;li&gt;‚úÖ Follow naming convention for consistency&lt;/li&gt; 
 &lt;li&gt;‚úÖ Verify compatibility with recent n8n versions&lt;/li&gt; 
 &lt;li&gt;‚úÖ Include meaningful description or comments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ö†Ô∏è Important Notes&lt;/h2&gt; 
&lt;h3&gt;Security &amp;amp; Privacy&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Review before use&lt;/strong&gt; - All workflows shared as-is for educational purposes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Update credentials&lt;/strong&gt; - Replace API keys, tokens, and webhooks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test safely&lt;/strong&gt; - Verify in development environment first&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check permissions&lt;/strong&gt; - Ensure proper access rights for integrations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Compatibility&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Version&lt;/strong&gt; - Compatible with n8n 1.0+ (most workflows)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Nodes&lt;/strong&gt; - Some workflows may require additional node installations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API Changes&lt;/strong&gt; - External services may have updated their APIs since creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt; - Verify required integrations before importing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìö Resources &amp;amp; References&lt;/h2&gt; 
&lt;h3&gt;Workflow Sources&lt;/h3&gt; 
&lt;p&gt;This comprehensive collection includes workflows from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Official n8n.io&lt;/strong&gt; - Documentation and community examples&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub repositories&lt;/strong&gt; - Open source community contributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Blog posts &amp;amp; tutorials&lt;/strong&gt; - Real-world automation patterns&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User submissions&lt;/strong&gt; - Tested and verified workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise use cases&lt;/strong&gt; - Business process automations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Learn More&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/"&gt;n8n Documentation&lt;/a&gt; - Official documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://community.n8n.io/"&gt;n8n Community&lt;/a&gt; - Community forum and support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://n8n.io/workflows/"&gt;Workflow Templates&lt;/a&gt; - Official template library&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/integrations/"&gt;Integration Docs&lt;/a&gt; - Service-specific guides&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèÜ Project Achievements&lt;/h2&gt; 
&lt;h3&gt;Repository Transformation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,053 workflows&lt;/strong&gt; professionally organized and named&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; automatically detected and categorized&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; (improved from basic filename patterns)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero data loss&lt;/strong&gt; during intelligent renaming process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced search&lt;/strong&gt; with 12 service categories&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Revolution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Sub-100ms search&lt;/strong&gt; with SQLite FTS5 full-text indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant filtering&lt;/strong&gt; across 29,445 workflow nodes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile-optimized&lt;/strong&gt; responsive design for all devices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time statistics&lt;/strong&gt; with live database queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Professional interface&lt;/strong&gt; with modern UX principles&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Reliability&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robust error handling&lt;/strong&gt; with graceful degradation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Change detection&lt;/strong&gt; for efficient database updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background processing&lt;/strong&gt; for non-blocking operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive logging&lt;/strong&gt; for debugging and monitoring&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Production-ready&lt;/strong&gt; with proper middleware and security&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;This repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üéØ Perfect for&lt;/strong&gt;: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Zie619/n8n-workflows/main/README_ZH.md"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/codex</title>
      <link>https://github.com/openai/codex</link>
      <description>&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;OpenAI Codex CLI&lt;/h1&gt; 
&lt;p align="center"&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer. &lt;br /&gt; &lt;br /&gt;If you want Codex in your code editor (VS Code, Cursor, Windsurf), &lt;a href="https://developers.openai.com/codex/ide"&gt;install in your IDE&lt;/a&gt; &lt;br /&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, go to &lt;a href="https://chatgpt.com/codex"&gt;chatgpt.com/codex&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png" alt="Codex CLI splash" width="80%" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; 
&lt;p&gt;Install globally with your preferred package manager. If you use npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @openai/codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you use Homebrew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;brew install codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can also go to the &lt;a href="https://github.com/openai/codex/releases/latest"&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; 
 &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS 
   &lt;ul&gt; 
    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Linux 
   &lt;ul&gt; 
    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-login.png" alt="Codex CLI login" width="80%" /&gt; &lt;/p&gt; 
&lt;p&gt;Run &lt;code&gt;codex&lt;/code&gt; and select &lt;strong&gt;Sign in with ChatGPT&lt;/strong&gt;. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan. &lt;a href="https://help.openai.com/en/articles/11369540-codex-in-chatgpt"&gt;Learn more about what's included in your ChatGPT plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also use Codex with an API key, but this requires &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#usage-based-billing-alternative-use-an-openai-api-key"&gt;additional setup&lt;/a&gt;. If you previously used an API key for usage-based billing, see the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#migrating-from-usage-based-billing-api-key"&gt;migration steps&lt;/a&gt;. If you're having trouble with login, please comment on &lt;a href="https://github.com/openai/codex/issues/1243"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Model Context Protocol (MCP)&lt;/h3&gt; 
&lt;p&gt;Codex CLI supports &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#model-context-protocol-mcp"&gt;MCP servers&lt;/a&gt;. Enable by adding an &lt;code&gt;mcp_servers&lt;/code&gt; section to your &lt;code&gt;~/.codex/config.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Codex CLI supports a rich set of configuration options, with preferences stored in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. For full configuration options, see &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/config.md"&gt;Configuration&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Docs &amp;amp; FAQ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md"&gt;&lt;strong&gt;Getting started&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#cli-usage"&gt;CLI usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#running-with-a-prompt-as-input"&gt;Running with a prompt as input&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#example-prompts"&gt;Example prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#memory-with-agentsmd"&gt;Memory with AGENTS.md&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/config.md"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/sandbox.md"&gt;&lt;strong&gt;Sandbox &amp;amp; approvals&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md"&gt;&lt;strong&gt;Authentication&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#forcing-a-specific-auth-method-advanced"&gt;Auth methods&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#connecting-on-a-headless-machine"&gt;Login on a "Headless" machine&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md"&gt;&lt;strong&gt;Advanced&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#non-interactive--ci-mode"&gt;Non-interactive / CI mode&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#tracing--verbose-logging"&gt;Tracing / verbose logging&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/zdr.md"&gt;&lt;strong&gt;Zero data retention (ZDR)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/contributing.md"&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md"&gt;&lt;strong&gt;Install &amp;amp; build&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md#system-requirements"&gt;System Requirements&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md#dotslash"&gt;DotSlash&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md#build-from-source"&gt;Build from source&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/faq.md"&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/open-source-fund.md"&gt;&lt;strong&gt;Open source fund&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>go-task/task</title>
      <link>https://github.com/go-task/task</link>
      <description>&lt;p&gt;A task runner / simpler Make alternative written in Go&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://taskfile.dev"&gt; &lt;img src="https://raw.githubusercontent.com/go-task/task/main/website/src/public/img/logo.svg?sanitize=true" width="200px" height="200px" /&gt; &lt;/a&gt; 
 &lt;h1&gt;Task&lt;/h1&gt; 
 &lt;p&gt; Task is a task runner / build tool that aims to be simpler and easier to use than, for example, &lt;a href="https://www.gnu.org/software/make/"&gt;GNU Make&lt;/a&gt;&lt;a&gt;. &lt;/a&gt;&lt;/p&gt;
 &lt;a&gt; &lt;/a&gt;
 &lt;p&gt;&lt;a&gt; &lt;/a&gt;&lt;a href="https://taskfile.dev/installation/"&gt;Installation&lt;/a&gt; | &lt;a href="https://taskfile.dev/usage/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://twitter.com/taskfiledev"&gt;Twitter&lt;/a&gt; | &lt;a href="https://bsky.app/profile/taskfile.dev"&gt;Bluesky&lt;/a&gt; | &lt;a href="https://fosstodon.org/@task"&gt;Mastodon&lt;/a&gt; | &lt;a href="https://discord.gg/6TY36E39UK"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
 &lt;h1&gt;Gold Sponsors&lt;/h1&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" valign="middle"&gt; &lt;a target="_blank" href="https://devowl.io"&gt; &lt;img src="https://devowl.io/wp-content/uploads/meta/favicon.webp" height="100px" title="devowl.io" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>apple/container</title>
      <link>https://github.com/apple/container</link>
      <description>&lt;p&gt;A tool for creating and running Linux containers using lightweight virtual machines on a Mac. It is written in Swift, and optimized for Apple silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;code&gt;container&lt;/code&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;container&lt;/code&gt; is a tool that you can use to create and run Linux containers as lightweight virtual machines on your Mac. It's written in Swift, and optimized for Apple silicon.&lt;/p&gt; 
&lt;p&gt;The tool consumes and produces &lt;a href="https://github.com/opencontainers/image-spec"&gt;OCI-compatible container images&lt;/a&gt;, so you can pull and run images from any standard container registry. You can push images that you build to those registries as well, and run the images in any other OCI-compatible application.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;container&lt;/code&gt; uses the &lt;a href="https://github.com/apple/containerization"&gt;Containerization&lt;/a&gt; Swift package for low level container, image, and process management.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apple/container/main/docs/assets/landing-movie.gif" alt="introductory movie showing some basic commands" /&gt;&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;p&gt;You need a Mac with Apple silicon to run &lt;code&gt;container&lt;/code&gt;. To build it, see the &lt;a href="https://raw.githubusercontent.com/apple/container/main/BUILDING.md"&gt;BUILDING&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;container&lt;/code&gt; is supported on macOS 26, since it takes advantage of new features and enhancements to virtualization and networking in this release. We do not support older versions of macOS and the &lt;code&gt;container&lt;/code&gt; maintainers typically will not address issues that cannot be reproduced on the latest macOS 26 beta.&lt;/p&gt; 
&lt;h3&gt;Install or upgrade&lt;/h3&gt; 
&lt;p&gt;If you're upgrading, first stop and uninstall your existing &lt;code&gt;container&lt;/code&gt; (the &lt;code&gt;-k&lt;/code&gt; flag keeps your user data, while &lt;code&gt;-d&lt;/code&gt; removes it):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;container system stop
uninstall-container.sh -k
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download the latest signed installer package for &lt;code&gt;container&lt;/code&gt; from the &lt;a href="https://github.com/apple/container/releases"&gt;GitHub release page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To install the tool, double-click the package file and follow the instructions. Enter your administrator password when prompted, to give the installer permission to place the installed files under &lt;code&gt;/usr/local&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Start the system service with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;container system start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Uninstall&lt;/h3&gt; 
&lt;p&gt;Use the &lt;code&gt;uninstall-container.sh&lt;/code&gt; script to remove &lt;code&gt;container&lt;/code&gt; from your system. To remove your user data along with the tool, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uninstall-container.sh -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To retain your user data so that it is available should you reinstall later, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uninstall-container.sh -k
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Next steps&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Take &lt;a href="https://raw.githubusercontent.com/apple/container/main/docs/tutorial.md"&gt;a guided tour of &lt;code&gt;container&lt;/code&gt;&lt;/a&gt; by building, running, and publishing a simple web server image.&lt;/li&gt; 
 &lt;li&gt;Learn how to &lt;a href="https://raw.githubusercontent.com/apple/container/main/docs/how-to.md"&gt;use various &lt;code&gt;container&lt;/code&gt; features&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Read a brief description and &lt;a href="https://raw.githubusercontent.com/apple/container/main/docs/technical-overview.md"&gt;technical overview&lt;/a&gt; of &lt;code&gt;container&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Browse the &lt;a href="https://raw.githubusercontent.com/apple/container/main/docs/command-reference.md"&gt;full command reference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apple/container/main/BUILDING.md"&gt;Build and run&lt;/a&gt; &lt;code&gt;container&lt;/code&gt; on your own development system.&lt;/li&gt; 
 &lt;li&gt;View the project &lt;a href="https://apple.github.io/container/documentation/"&gt;API documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions to &lt;code&gt;container&lt;/code&gt; are welcomed and encouraged. Please see our &lt;a href="https://github.com/apple/containerization/raw/main/CONTRIBUTING.md"&gt;main contributing guide&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Project Status&lt;/h2&gt; 
&lt;p&gt;The container project is currently under active development. Its stability, both for consuming the project as a Swift package and the &lt;code&gt;container&lt;/code&gt; tool, is only guaranteed within patch versions, such as between 0.1.1 and 0.1.2. Minor version number releases may include breaking changes until we achieve a 1.0.0 release.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TEN-framework/ten-framework</title>
      <link>https://github.com/TEN-framework/ten-framework</link>
      <description>&lt;p&gt;Open-source framework for conversational voice AI agents.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a name="readme-top"&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/7c8f72d7-3993-4d01-8504-b71578a22944" alt="TEN banner" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/ten-framework/ten-framework?color=369eff&amp;amp;labelColor=gray&amp;amp;logo=github&amp;amp;style=flat-square" alt="TEN Releases" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/ten-framework/ten-framework?labelColor=gray&amp;amp;style=flat-square" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/discussions/"&gt;&lt;img src="https://img.shields.io/github/discussions/TEN-framework/ten_framework?labelColor=gray&amp;amp;color=%20%23f79009" alt="Discussion posts" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/m/TEN-framework/ten_framework?labelColor=gray&amp;amp;color=pink" alt="Commits" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/issues"&gt;&lt;img src="https://img.shields.io/github/issues-search?query=repo%3ATEN-framework%2Ften-framework%20is%3Aclosed&amp;amp;label=issues%20closed&amp;amp;labelColor=gray&amp;amp;color=green" alt="Issues closed" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/ten-framework/ten-framework?color=c4f042&amp;amp;labelColor=gray&amp;amp;style=flat-square" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/pulls"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome!-brightgreen.svg?style=flat-square" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten_framework/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0_with_certain_conditions-blue.svg?labelColor=%20%23155EEF&amp;amp;color=%20%23528bff" alt="GitHub license" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/TEN-framework/TEN-framework"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://readmex.com/TEN-framework/ten-framework"&gt;&lt;img src="https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg?sanitize=true" alt="ReadmeX" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://GitHub.com/TEN-framework/ten_framework/watchers/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/watchers/TEN-framework/ten_framework?style=social&amp;amp;label=Watch" alt="GitHub watchers" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/TEN-framework/ten_framework/network/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/forks/TEN-framework/ten_framework?style=social&amp;amp;label=Fork" alt="GitHub forks" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/TEN-framework/ten_framework/stargazers/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/stars/TEN-framework/ten_framework?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/README.md"&gt;&lt;img alt="README in English" src="https://img.shields.io/badge/English-lightgrey" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-CN.md"&gt;&lt;img alt="ÁÆÄ‰Ωì‰∏≠ÊñáÊìç‰ΩúÊåáÂçó" src="https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-lightgrey" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-JP.md"&gt;&lt;img alt="Êó•Êú¨Ë™û„ÅÆREADME" src="https://img.shields.io/badge/Êó•Êú¨Ë™û-lightgrey" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-KR.md"&gt;&lt;img alt="README in ÌïúÍµ≠Ïñ¥" src="https://img.shields.io/badge/ÌïúÍµ≠Ïñ¥-lightgrey" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-ES.md"&gt;&lt;img alt="README en Espa√±ol" src="https://img.shields.io/badge/Espa√±ol-lightgrey" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-FR.md"&gt;&lt;img alt="README en Fran√ßais" src="https://img.shields.io/badge/Fran√ßais-lightgrey" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-IT.md"&gt;&lt;img alt="README in Italiano" src="https://img.shields.io/badge/Italiano-lightgrey" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://theten.ai"&gt;Official Site&lt;/a&gt; ‚Ä¢ &lt;a href="https://theten.ai/docs/ten_agent/overview"&gt;Documentation&lt;/a&gt; ‚Ä¢ &lt;a href="https://theten.ai/blog"&gt;Blog&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11978" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11978" alt="TEN-framework%2Ften_framework | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;kbd&gt;Table of Contents&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;h4&gt;Table of Contents&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#-welcome-to-ten"&gt;üëã Welcome to TEN&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#-tman-designer"&gt;üé® TMAN Designer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#-features"&gt;‚ú® Features&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#1%EF%B8%8F%E2%83%A3-real-time-avatar"&gt;1Ô∏è‚É£ Real-time Avatar&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#2%EF%B8%8F%E2%83%A3-real-time-voice-with-mcp-servers"&gt;2Ô∏è‚É£ Real-time voice with MCP servers&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#3%EF%B8%8F%E2%83%A3-real-time-communication-with-hardware"&gt;3Ô∏è‚É£ Real-time communication with hardware&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#4%EF%B8%8F%E2%83%A3-real-time-vision-and-real-time-screenshare-detection"&gt;4Ô∏è‚É£ Real-time vision and real-time screenshare detection&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#5%EF%B8%8F%E2%83%A3-ten-with-other-llm-platforms"&gt;5Ô∏è‚É£ TEN with other LLM platforms&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#6%EF%B8%8F%E2%83%A3-storyteller---ten-image-generation"&gt;6Ô∏è‚É£ StoryTeller - TEN image generation&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#-get-ten-agent-up-and-running"&gt;üë©‚Äçüíª Get TEN Agent up and running&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#%F0%9F%85%B0%EF%B8%8F-run-ten-agent-in-localhost"&gt;üÖ∞Ô∏è Run TEN Agent in &lt;code&gt;localhost&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#%F0%9F%85%B1%EF%B8%8F-run-ten-agent-in-codespaceno-docker"&gt;üÖ±Ô∏è Run TEN Agent in Codespace(no docker)&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#%EF%B8%8F-ten-agent-self-hosting"&gt;üõ≥Ô∏è TEN Agent Self Hosting&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#%F0%9F%85%B0%EF%B8%8F-deploying-with-docker"&gt;üÖ∞Ô∏è Deploying with Docker&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#%F0%9F%85%B1%EF%B8%8F-deploying-with-other-cloud-services"&gt;üÖ±Ô∏è Deploying with other cloud services&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#-ten-ecosystem"&gt;üåç TEN Ecosystem&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#-ask-questions"&gt;‚ùì Ask Questions&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#-contributing"&gt;ü•∞ Contributing&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#code-contributors"&gt;Code Contributors&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#contribution-guidelines"&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;h2&gt;üëã Welcome to TEN&lt;/h2&gt; 
&lt;p&gt;TEN is a comprehensive open-source ecosystem for creating, customizing, and deploying real-time conversational AI agents with multimodal capabilities including voice, vision, and avatar interactions.&lt;/p&gt; 
&lt;p&gt;TEN includes &lt;a href="https://github.com/ten-framework/ten-framework"&gt;TEN Framework&lt;/a&gt;, &lt;a href="https://github.com/ten-framework/ten-turn-detection"&gt;TEN Turn Detection&lt;/a&gt;, &lt;a href="https://github.com/ten-framework/ten-vad"&gt;TEN VAD&lt;/a&gt;, &lt;a href="https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/demo"&gt;TEN Agent&lt;/a&gt;, &lt;a href="https://github.com/TEN-framework/ten-framework/tree/main/core/src/ten_manager/designer_frontend"&gt;TMAN Designer&lt;/a&gt;, and &lt;a href="https://github.com/ten-framework/portal"&gt;TEN Portal&lt;/a&gt;. Check out &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#-ten-ecosystem"&gt;üåç TEN Ecosystem&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Community Channel&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://twitter.com/intent/follow?screen_name=TenFramework"&gt;&lt;img src="https://img.shields.io/twitter/follow/TenFramework?logo=X&amp;amp;color=%20%23f5f5f5" alt="Follow on X" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Follow TEN Framework on X for updates and announcements&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.linkedin.com/company/ten-framework"&gt;&lt;img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-TEN_Framework-0A66C2?logo=linkedin-white&amp;amp;logoColor=fff" alt="Follow on LinkedIn" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Follow TEN Framework on LinkedIn for updates and announcements&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/VnPftUzAMJ"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20TEN%20Community-5865F2?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord TEN Community" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our Discord community to connect with developers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/TEN-framework"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-TEN%20Framework-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face Space" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our Hugging Face community to explore our spaces and models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TEN-framework/ten-agent/discussions/170"&gt;&lt;img src="https://img.shields.io/badge/TEN_Framework-WeChat_Group-%2307C160?logo=wechat&amp;amp;labelColor=darkgreen&amp;amp;color=gray" alt="WeChat" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our WeChat group for Chinese community discussions&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Star TEN Repositories&lt;/strong&gt; ‚≠êÔ∏è&lt;/p&gt; 
 &lt;p&gt;Get instant notifications for new releases and updates. Your support helps us grow and improve TEN!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/eeebe996-8c14-4bf7-82ae-f1a1f7e30705" alt="TEN star us gif" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;kbd&gt;Star History&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;picture&gt; 
  &lt;img width="100%" src="https://api.star-history.com/svg?repos=ten-framework/ten-framework&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; 
&lt;/details&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;üé® TMAN Designer&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/44c6a087-ec7a-45b0-a084-dab5dac5e36b"&gt;https://github.com/user-attachments/assets/44c6a087-ec7a-45b0-a084-dab5dac5e36b&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;TMAN Designer&lt;/h3&gt; 
&lt;p&gt;TMAN Designer is a low/no-code option to create voice agents with an easy-to-use workflow UI. It can load apps and graphs, and includes an online editor, log viewer, and much more.&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://theten.ai/blog/tman-designer-of-ten-framework"&gt;this blog&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;‚ú® Features&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/c6702995-de94-4d3e-8cae-af097f087ac1" alt="TEN Agent with Trulience" /&gt;&lt;/p&gt; 
&lt;h3&gt;1Ô∏è‚É£ Real-time Avatar&lt;/h3&gt; 
&lt;p&gt;Build engaging AI avatars with TEN Agent using &lt;a href="https://trulience.com"&gt;Trulience&lt;/a&gt;'s diverse collection of free avatar options. To get it up and running, you only need 2 steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Follow the README to finish setting up and running the Playground&lt;/li&gt; 
 &lt;li&gt;Enter the avatar ID and &lt;a href="https://trulience.com/docs#/authentication/jwt-tokens/jwt-tokens?id=use-your-custom-userid"&gt;token&lt;/a&gt; you get from &lt;a href="https://trulience.com"&gt;Trulience&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/afb77ad3-9c23-452f-b870-216687779017" alt="TEN with MCP servers" /&gt;&lt;/p&gt; 
&lt;h3&gt;2Ô∏è‚É£ Real-time voice with MCP servers&lt;/h3&gt; 
&lt;p&gt;TEN Agent now integrates seamlessly with MCP servers, expanding its LLM capabilities. To get started:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open the Module Picker in Playground&lt;/li&gt; 
 &lt;li&gt;Add the MCP server tool for LLM integration&lt;/li&gt; 
 &lt;li&gt;Paste a URL from your MCP server in the extension&lt;/li&gt; 
 &lt;li&gt;Start a realtime conversation with TEN Agent&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This integration allows you to leverage MCP's diverse servers offerings while maintaining TEN Agent's powerful conversational abilities.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/78647eef-2d66-44e6-99a8-1918a940fb9f"&gt;https://github.com/user-attachments/assets/78647eef-2d66-44e6-99a8-1918a940fb9f&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;3Ô∏è‚É£ Real-time communication with hardware&lt;/h3&gt; 
&lt;p&gt;TEN Agent is now running on the Espressif ESP32-S3 Korvo V3 development board, an excellent way to integrate realtime communication with LLM on hardware.&lt;/p&gt; 
&lt;p&gt;Check out the &lt;a href="https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/esp32-client"&gt;integration guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a1addb02-a450-47be-8cb2-d25e3b574f53" alt="Real-time Vision" /&gt;&lt;/p&gt; 
&lt;h3&gt;4Ô∏è‚É£ Real-time vision and real-time screenshare detection&lt;/h3&gt; 
&lt;p&gt;Try Google Gemini Multimodal Live API with realtime vision and realtime screenshare detection capabilities, it is a ready-to-use extension, along with powerful tools like Weather Check and Web Search integrated perfectly into TEN Agent.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/234ff443-bef8-4cc4-9a10-09d6ec3f5bc1" alt="TEN with Dify" /&gt;&lt;/p&gt; 
&lt;h3&gt;5Ô∏è‚É£ TEN with other LLM platforms&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://theten.ai/docs/ten_agent/playground/use-cases/voice-assistant/run_dify#steps"&gt;TEN Agent + Dify&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;TEN offers a great support to make the realtime interactive experience even better on other LLM platform as well, check out docs for more.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/fe28a549-ddb9-431e-9282-57539fb87371" alt="TEN StoryTeller" /&gt;&lt;/p&gt; 
&lt;h3&gt;6Ô∏è‚É£ StoryTeller - TEN image generation&lt;/h3&gt; 
&lt;p&gt;Experience the real-time image generation with StoryTeller, it is a ready-to-use extension, along with powerful tools like Weather Check and Web Search integrated perfectly into TEN.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;üë©‚Äçüíª Get TEN Agent up and running&lt;/h2&gt; 
&lt;h4&gt;üÖ∞Ô∏è Run TEN Agent in localhost&lt;/h4&gt; 
&lt;h4&gt;Step ‚ìµ - Prerequisites&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Requirements&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Keys&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ Agora &lt;a href="https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project"&gt;App ID&lt;/a&gt; and &lt;a href="https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project"&gt;App Certificate&lt;/a&gt; (free minutes every month) &lt;br /&gt;‚Ä¢ &lt;a href="https://openai.com/index/openai-api/"&gt;OpenAI&lt;/a&gt; API key (any LLM that is compatible with OpenAI)&lt;br /&gt;‚Ä¢ &lt;a href="https://deepgram.com/"&gt;Deepgram&lt;/a&gt; ASR (free credits available with signup)&lt;br /&gt;‚Ä¢ &lt;a href="https://elevenlabs.io/"&gt;Elevenlabs&lt;/a&gt; TTS (free credits available with signup)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; / &lt;a href="https://docs.docker.com/compose/"&gt;Docker Compose&lt;/a&gt;&lt;br /&gt;‚Ä¢ &lt;a href="https://nodejs.org/en"&gt;Node.js(LTS) v18&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Minimum System Requirements&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ä¢ CPU &amp;gt;= 2 Core&lt;br /&gt;‚Ä¢ RAM &amp;gt;= 4 GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;macOS: Docker setting on Apple Silicon&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Uncheck "Use Rosetta for x86/amd64 emulation" in Docker settings, it may result in slower build times on ARM, but performance will be normal when deployed to x64 servers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;h4&gt;Step ‚ì∂ - Build agent in VM&lt;/h4&gt; 
&lt;h5&gt;1. Clone down the repo,&lt;code&gt;cd&lt;/code&gt; to &lt;code&gt;ai-agents&lt;/code&gt; and create &lt;code&gt;.env&lt;/code&gt; file from &lt;code&gt;.env.example&lt;/code&gt;&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ai_agents
cp ./.env.example ./.env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;2. Setup Agora App ID and App Certificate in &lt;code&gt;.env&lt;/code&gt;&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;AGORA_APP_ID=
AGORA_APP_CERTIFICATE=
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;3. Start agent development containers&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;4. Enter container&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker exec -it ten_agent_dev bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;5. Build agent with the default &lt;code&gt;graph&lt;/code&gt; ( ~5min - ~8min)&lt;/h5&gt; 
&lt;p&gt;check the &lt;code&gt;/examples&lt;/code&gt; folder for more examples&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# use the chained voice assistant
task use AGENT=voice-assistant

# or use the speech-to-speech voice assistant realtime
task use AGENT=voice-assistant-realtime
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;6. Start the web server&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# run task build if you changed any local source code, this is necessary if you are working on languages which require compilation like TypeScript or Golang.
task build

task run
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h4&gt;Step ‚ì∑ - Customize your agent with TMAN Designer&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:49483"&gt;localhost:49483&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Right-click on the STT, LLM, and TTS extensions.&lt;/li&gt; 
 &lt;li&gt;Open their properties and enter APIs respectively.&lt;/li&gt; 
 &lt;li&gt;Right-click the canvas and select 'Manage Apps' to open the Apps Manager.&lt;/li&gt; 
 &lt;li&gt;Right under the Actions, click the ‚ñ∂ to run the App.&lt;/li&gt; 
 &lt;li&gt;Check the 'Run with TEN Agent' option and click the Run button.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;üÖ±Ô∏è Run TEN Agent in Codespace(no docker)&lt;/h3&gt; 
&lt;p&gt;GitHub offers free Codespace for each repository, you can run the playground in Codespace without using Docker.Also, the speed of Codespace is much faster than localhost.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://codespaces.new/ten-framework/ten-agent"&gt;&lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://theten.ai/docs/ten_agent/setup_development_env/setting_up_development_inside_codespace"&gt;this guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;üõ≥Ô∏è TEN Agent Self Hosting&lt;/h2&gt; 
&lt;h4&gt;üÖ∞Ô∏è Deploying with Docker&lt;/h4&gt; 
&lt;p&gt;Once you have customized your agent (either by using the TMAN Manager, Playground, or editing &lt;code&gt;property.json&lt;/code&gt; directly), you can deploy it by creating a release Docker image for your service.&lt;/p&gt; 
&lt;p&gt;Read the &lt;a href="https://theten.ai/docs/ten_agent/deploy_ten_agent/deploy_agent_service"&gt;Deployment Guide&lt;/a&gt; for detailed information about deployment.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h4&gt;üÖ±Ô∏è Deploying with other cloud services&lt;/h4&gt; 
&lt;p&gt;&lt;em&gt;coming soon&lt;/em&gt;&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;üåç TEN Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Project&lt;/th&gt; 
   &lt;th&gt;Preview&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/ten_framework"&gt;&lt;strong&gt;üèöÔ∏è TEN Framework&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;TEN is an open-source framework for real-time, multimodal conversational AI.&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/ten_framework?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/7c8f72d7-3993-4d01-8504-b71578a22944" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/ten-turn-detection"&gt;&lt;strong&gt;Ô∏èüîÇ TEN Turn Detection&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;TEN is for full-duplex dialogue communication.&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/ten-turn-detection?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/8d0ec716-5d0e-43e4-ad9a-d97b17305658" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/ten-vad"&gt;&lt;strong&gt;üîâ TEN VAD&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;TEN VAD is a low-latency, lightweight and high-performance streaming voice activity detector (VAD).&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/ten-vad?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/d45870e4-9453-4047-8163-08737f82863f" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/tree/main/ai_agents"&gt;&lt;strong&gt;üéôÔ∏è TEN Agent&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;TEN Agent is a showcase of TEN Framewrok.&lt;br /&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/38de2207-939b-4702-a0aa-04491f5b5275" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/tree/main/core/src/ten_manager/designer_frontend"&gt;&lt;strong&gt;üé® TMAN Designer&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;TMAN Designer is low/no code option to make a voice agent with easy to use workflow UI.&lt;br /&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/804c3543-0a47-42b7-b40b-ef32b742fb8f" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/portal"&gt;&lt;strong&gt;üìí TEN Portal&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;The official site of TEN framework, it has documentation and blog.&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/portal?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/e17d8aaa-5928-45dd-ac71-814928e26a89" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;‚ùì Ask Questions&lt;/h2&gt; 
&lt;p&gt;TEN Framework is available on these AI-powered Q&amp;amp;A platforms. They can help you find answers quickly and accurately in multiple languages, covering everything from basic setup to advanced implementation details.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepWiki&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://deepwiki.com/TEN-framework/TEN-framework"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ReadmeX&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://readmex.com/TEN-framework/ten-framework"&gt;&lt;img src="https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg?sanitize=true" alt="ReadmeX" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ü•∞ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome all forms of open-source collaboration! Whether you're fixing bugs, adding features, improving documentation, or sharing ideas - your contributions help advance personalized AI tools. Check out our GitHub Issues and Projects to find ways to contribute and show your skills. Together, we can build something amazing!&lt;/p&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Welcome all kinds of contributions&lt;/strong&gt; üôè&lt;/p&gt; 
 &lt;p&gt;Join us in building TEN better! Every contribution makes a difference, from code to documentation. Share your TEN Agent projects on social media with to inspire others!&lt;/p&gt; 
 &lt;p&gt;Connect with one of the TEN maintainers &lt;a href="https://x.com/elliotchen100"&gt;@elliotchen100&lt;/a&gt; on ùïè or &lt;a href="https://github.com/cyfyifanchen"&gt;@cyfyifanchen&lt;/a&gt; on GitHub for project updates, discussions and collaboration opportunities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;h3&gt;Code Contributors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-agent/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=TEN-framework/ten-agent" alt="TEN" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Contribution Guidelines&lt;/h3&gt; 
&lt;p&gt;Contributions are welcome! Please read the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/docs/code-of-conduct/contributing.md"&gt;contribution guidelines&lt;/a&gt; first.&lt;/p&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;The entire TEN framework (except for the folders explicitly listed below) is released under the Apache License, Version 2.0, with additional restrictions. For details, please refer to the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/LICENSE"&gt;LICENSE&lt;/a&gt; file located in the root directory of the TEN framework.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The components within the &lt;code&gt;packages&lt;/code&gt; directory are released under the Apache License, Version 2.0. For details, please refer to the &lt;code&gt;LICENSE&lt;/code&gt; file located in each package's root directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The third-party libraries used by the TEN framework are listed and described in detail. For more information, please refer to the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/third_party/"&gt;third_party&lt;/a&gt; folder.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>simdjson/simdjson</title>
      <link>https://github.com/simdjson/simdjson</link>
      <description>&lt;p&gt;Parsing gigabytes of JSON per second : used by Facebook/Meta Velox, the Node.js runtime, ClickHouse, WatermelonDB, Apache Doris, Milvus, StarRocks&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202-blue.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/LICENSE-MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://simdjson.github.io/simdjson/"&gt;&lt;img src="https://img.shields.io/badge/docs-doxygen-green.svg?sanitize=true" alt="Doxygen Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;simdjson : Parsing gigabytes of JSON per second&lt;/h1&gt; 
&lt;img src="https://raw.githubusercontent.com/simdjson/simdjson/master/images/logo.png" width="10%" style="float: right" /&gt; JSON is everywhere on the Internet. Servers spend a *lot* of time parsing it. We need a fresh approach. The simdjson library uses commonly available SIMD instructions and microparallel algorithms to parse JSON 4x faster than RapidJSON and 25x faster than JSON for Modern C++. 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast:&lt;/strong&gt; Over 4x faster than commonly used production-grade JSON parsers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Record Breaking Features:&lt;/strong&gt; Minify JSON at 6 GB/s, validate UTF-8 at 13 GB/s, NDJSON at 3.5 GB/s.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy:&lt;/strong&gt; First-class, easy to use and carefully documented APIs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strict:&lt;/strong&gt; Full JSON and UTF-8 validation, lossless parsing. Performance with no compromises.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic:&lt;/strong&gt; Selects a CPU-tailored parser at runtime. No configuration needed.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable:&lt;/strong&gt; From memory allocation to error handling, simdjson's design avoids surprises.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Peer Reviewed:&lt;/strong&gt; Our research appears in venues like VLDB Journal, Software: Practice and Experience.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This library is part of the &lt;a href="https://awesomecpp.com"&gt;Awesome Modern C++&lt;/a&gt; list.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#real-world-usage"&gt;Real-world usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#godbolt"&gt;Godbolt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#performance-results"&gt;Performance results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#packages"&gt;Packages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#bindings-and-ports-of-simdjson"&gt;Bindings and Ports of simdjson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#about-simdjson"&gt;About simdjson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#funding"&gt;Funding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#contributing-to-simdjson"&gt;Contributing to simdjson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Real-world usage&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/"&gt;Node.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ClickHouse/ClickHouse"&gt;ClickHouse&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://velox-lib.io"&gt;Meta Velox&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/paxml"&gt;Google Pax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/milvus-io/milvus"&gt;milvus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://questdb.io/blog/questdb-release-8-0-3/"&gt;QuestDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aras-p/ClangBuildAnalyzer"&gt;Clang Build Analyzer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Shopify/heap-profiler"&gt;Shopify HeapProfiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/StarRocks/starrocks"&gt;StarRocks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/FishStore"&gt;Microsoft FishStore&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/intel/pcm"&gt;Intel PCM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Nozbe/WatermelonDB"&gt;WatermelonDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/doris"&gt;Apache Doris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dgraph-io/dgraph"&gt;Dgraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unum-cloud/ujrpc"&gt;UJRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/spnda/fastgltf"&gt;fastgltf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tenzir/vast"&gt;vast&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ada-url/ada"&gt;ada-url&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/adamritter/fastgron"&gt;fastgron&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://wasmedge.org"&gt;WasmEdge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/logicalclocks/rondb"&gt;RonDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GreptimeTeam/greptimedb"&gt;GreptimeDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mamba-org/mamba"&gt;mamba&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are planning to use simdjson in a product, please work from one of our releases.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;The simdjson library is easily consumable with a single .h and .cpp file.&lt;/p&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt; &lt;p&gt;Prerequisites: &lt;code&gt;g++&lt;/code&gt; (version 7 or better) or &lt;code&gt;clang++&lt;/code&gt; (version 6 or better), and a 64-bit system with a command-line shell (e.g., Linux, macOS, freeBSD). We also support programming environments like Visual Studio and Xcode, but different steps are needed. Users of clang++ may need to specify the C++ version (e.g., &lt;code&gt;c++ -std=c++17&lt;/code&gt;) since clang++ tends to default on C++98.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Pull &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.h"&gt;simdjson.h&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.cpp"&gt;simdjson.cpp&lt;/a&gt; into a directory, along with the sample file &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/jsonexamples/twitter.json"&gt;twitter.json&lt;/a&gt;. You can download them with the &lt;code&gt;wget&lt;/code&gt; utility:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.h https://raw.githubusercontent.com/simdjson/simdjson/master/singleheader/simdjson.cpp https://raw.githubusercontent.com/simdjson/simdjson/master/jsonexamples/twitter.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create &lt;code&gt;quickstart.cpp&lt;/code&gt;:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-c++"&gt;#include &amp;lt;iostream&amp;gt;
#include "simdjson.h"
using namespace simdjson;
int main(void) {
    ondemand::parser parser;
    padded_string json = padded_string::load("twitter.json");
    ondemand::document tweets = parser.iterate(json);
    std::cout &amp;lt;&amp;lt; uint64_t(tweets["search_metadata"]["count"]) &amp;lt;&amp;lt; " results." &amp;lt;&amp;lt; std::endl;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;code&gt;c++ -o quickstart quickstart.cpp simdjson.cpp&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;./quickstart&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt; 100 results.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Usage documentation is available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/basics.md"&gt;Basics&lt;/a&gt; is an overview of how to use simdjson and its APIs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/builder.md"&gt;Builder&lt;/a&gt; is an overview of how to efficiently write JSON strings using simdjson.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/performance.md"&gt;Performance&lt;/a&gt; shows some more advanced scenarios and how to tune for them.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/implementation-selection.md"&gt;Implementation Selection&lt;/a&gt; describes runtime CPU detection and how you can work with it.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://simdjson.github.io/simdjson/"&gt;API&lt;/a&gt; contains the automatically generated API documentation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Godbolt&lt;/h2&gt; 
&lt;p&gt;Some users may want to browse code along with the compiled assembly. You want to check out the following lists of examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://godbolt.org/z/K3Px64TqK"&gt;C++26 reflection example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://godbolt.org/z/7G5qE4sr9"&gt;simdjson examples with errors handled through exceptions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://godbolt.org/z/e9dWb9E4v"&gt;simdjson examples with errors without exceptions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance results&lt;/h2&gt; 
&lt;p&gt;The simdjson library uses three-quarters less instructions than state-of-the-art parser &lt;a href="https://rapidjson.org"&gt;RapidJSON&lt;/a&gt;. To our knowledge, simdjson is the first fully-validating JSON parser to run at &lt;a href="https://en.wikipedia.org/wiki/Gigabyte"&gt;gigabytes per second&lt;/a&gt; (GB/s) on commodity processors. It can parse millions of JSON documents per second on a single core.&lt;/p&gt; 
&lt;p&gt;The following figure represents parsing speed in GB/s for parsing various files on an Intel Skylake processor (3.4 GHz) using the GNU GCC 10 compiler (with the -O3 flag). We compare against the best and fastest C++ libraries on benchmarks that load and process the data. The simdjson library offers full unicode (&lt;a href="https://en.wikipedia.org/wiki/UTF-8"&gt;UTF-8&lt;/a&gt;) validation and exact number parsing.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/rome.png" width="60%" /&gt; 
&lt;p&gt;The simdjson library offers high speed whether it processes tiny files (e.g., 300 bytes) or larger files (e.g., 3MB). The following plot presents parsing speed for &lt;a href="https://github.com/simdjson/simdjson_experiments_vldb2019/raw/master/experiments/growing/gen.py"&gt;synthetic files over various sizes generated with a script&lt;/a&gt; on a 3.4 GHz Skylake processor (GNU GCC 9, -O3).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/simdjson/simdjson/master/doc/growing.png" width="60%" /&gt; 
&lt;p&gt;&lt;a href="https://github.com/simdjson/simdjson_experiments_vldb2019"&gt;All our experiments are reproducible&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For NDJSON files, we can exceed 3 GB/s with &lt;a href="https://github.com/simdjson/simdjson/raw/master/doc/parse_many.md"&gt;our multithreaded parsing functions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Packages&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://repology.org/project/simdjson/versions"&gt;&lt;img src="https://repology.org/badge/vertical-allrepos/simdjson.svg?sanitize=true" alt="Packaging status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Bindings and Ports of simdjson&lt;/h2&gt; 
&lt;p&gt;We distinguish between "bindings" (which just wrap the C++ code) and a port to another programming language (which reimplements everything).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/michaeleisel/zippyjson"&gt;ZippyJSON&lt;/a&gt;: Swift bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gerrymanoim/libpy_simdjson/"&gt;libpy_simdjson&lt;/a&gt;: high-speed Python bindings for simdjson using &lt;a href="https://github.com/quantopian/libpy"&gt;libpy&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TkTech/pysimdjson"&gt;pysimdjson&lt;/a&gt;: Python bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TeskaLabs/cysimdjson"&gt;cysimdjson&lt;/a&gt;: high-speed Python bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/simd-lite"&gt;simdjson-rs&lt;/a&gt;: Rust port.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SunDoge/simdjson-rust"&gt;simdjson-rust&lt;/a&gt;: Rust wrapper (bindings).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EgorBo/SimdJsonSharp"&gt;SimdJsonSharp&lt;/a&gt;: C# version for .NET Core (bindings and full port).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/luizperes/simdjson_nodejs"&gt;simdjson_nodejs&lt;/a&gt;: Node.js bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crazyxman/simdjson_php"&gt;simdjson_php&lt;/a&gt;: PHP bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/saka1/simdjson_ruby"&gt;simdjson_ruby&lt;/a&gt;: Ruby bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anilmaurya/fast_jsonparser"&gt;fast_jsonparser&lt;/a&gt;: Ruby bindings for the simdjson project.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/minio/simdjson-go"&gt;simdjson-go&lt;/a&gt;: Go port using Golang assembly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eddelbuettel/rcppsimdjson"&gt;rcppsimdjson&lt;/a&gt;: R bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ChomperT/simdjson_erlang"&gt;simdjson_erlang&lt;/a&gt;: erlang bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/saleyn/simdjsone"&gt;simdjsone&lt;/a&gt;: erlang bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FourierTransformer/lua-simdjson"&gt;lua-simdjson&lt;/a&gt;: lua bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hackage.haskell.org/package/hermes-json"&gt;hermes-json&lt;/a&gt;: haskell bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EzequielRamis/zimdjson"&gt;zimdjson&lt;/a&gt;: Zig port.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/travisstaloch/simdjzon"&gt;simdjzon&lt;/a&gt;: Zig port.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rawleyfowler/JSON-simd"&gt;JSON-Simd&lt;/a&gt;: Raku bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://metacpan.org/pod/JSON::SIMD"&gt;JSON::SIMD&lt;/a&gt;: Perl bindings; fully-featured JSON module that uses simdjson for decoding.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sainttttt/gemmaJSON"&gt;gemmaJSON&lt;/a&gt;: Nim JSON parser based on simdjson bindings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/simdjson/simdjson-java"&gt;simdjson-java&lt;/a&gt;: Java port.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About simdjson&lt;/h2&gt; 
&lt;p&gt;The simdjson library takes advantage of modern microarchitectures, parallelizing with SIMD vector instructions, reducing branch misprediction, and reducing data dependency to take advantage of each CPU's multiple execution cores.&lt;/p&gt; 
&lt;p&gt;Our default front-end is called On-Demand, and we wrote a paper about it:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Keiser, Daniel Lemire, &lt;a href="http://arxiv.org/abs/2312.17149"&gt;On-Demand JSON: A Better Way to Parse Documents?&lt;/a&gt;, Software: Practice and Experience 54 (6), 2024.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Some people &lt;a href="https://arxiv.org/abs/1902.08318"&gt;enjoy reading the first (2019) simdjson paper&lt;/a&gt;: A description of the design and implementation of simdjson is in our research article:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Geoff Langdale, Daniel Lemire, &lt;a href="https://arxiv.org/abs/1902.08318"&gt;Parsing Gigabytes of JSON per Second&lt;/a&gt;, VLDB Journal 28 (6), 2019.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We have an in-depth paper focused on the UTF-8 validation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Keiser, Daniel Lemire, &lt;a href="https://arxiv.org/abs/2010.03090"&gt;Validating UTF-8 In Less Than One Instruction Per Byte&lt;/a&gt;, Software: Practice &amp;amp; Experience 51 (5), 2021.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also have an informal &lt;a href="https://branchfree.org/2019/02/25/paper-parsing-gigabytes-of-json-per-second/"&gt;blog post providing some background and context&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For the video inclined, &lt;br /&gt; &lt;a href="http://www.youtube.com/watch?v=wlvKAT7SZIQ"&gt;&lt;img src="http://img.youtube.com/vi/wlvKAT7SZIQ/0.jpg" alt="simdjson at QCon San Francisco 2019" /&gt;&lt;/a&gt;&lt;br /&gt; (It was the best voted talk, we're kinda proud of it.)&lt;/p&gt; 
&lt;h2&gt;Funding&lt;/h2&gt; 
&lt;p&gt;The work is supported by the Natural Sciences and Engineering Research Council of Canada under grants RGPIN-2017-03910 and RGPIN-2024-03787.&lt;/p&gt; 
&lt;h2&gt;Contributing to simdjson&lt;/h2&gt; 
&lt;p&gt;Head over to &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for information on contributing to simdjson, and &lt;a href="https://raw.githubusercontent.com/simdjson/simdjson/master/HACKING.md"&gt;HACKING.md&lt;/a&gt; for information on source, building, and architecture/design.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This code is made available under the &lt;a href="https://www.apache.org/licenses/LICENSE-2.0.html"&gt;Apache License 2.0&lt;/a&gt; as well as under the MIT License. As a user, you can pick the license you prefer.&lt;/p&gt; 
&lt;p&gt;Under Windows, we build some tools using the windows/dirent_portable.h file (which is outside our library code): it is under the liberal (business-friendly) MIT license.&lt;/p&gt; 
&lt;p&gt;For compilers that do not support &lt;a href="https://en.wikipedia.org/wiki/C%2B%2B17"&gt;C++17&lt;/a&gt;, we bundle the string-view library which is published under the &lt;a href="http://www.boost.org/LICENSE_1_0.txt"&gt;Boost license&lt;/a&gt;. Like the Apache license, the Boost license is a permissive license allowing commercial redistribution.&lt;/p&gt; 
&lt;p&gt;For efficient number serialization, we bundle Florian Loitsch's implementation of the Grisu2 algorithm for binary to decimal floating-point numbers. The implementation was slightly modified by JSON for Modern C++ library. Both Florian Loitsch's implementation and JSON for Modern C++ are provided under the MIT license.&lt;/p&gt; 
&lt;p&gt;For runtime dispatching, we use some code from the PyTorch project licensed under 3-clause BSD.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;‚å®Ô∏è Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;üñ•Ô∏è Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;‚å®Ô∏è Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03‚ÄØPM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;üî• Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÁÆ±Âç≥Áî®&lt;/strong&gt;: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êòì‰∫éÈõÜÊàê&lt;/strong&gt;: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ÂèØÊéß&lt;/strong&gt;: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Â∑•‰ΩúÂéüÁêÜ&lt;/h2&gt; 
&lt;img width="1189" height="624" alt="system-arch" src="https://github.com/user-attachments/assets/cde40783-369e-493e-bb59-44ce43c2e7c5" /&gt; 
&lt;h2&gt;Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;ÂÆâË£ÖÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÂÆâË£ÖÂ•Ω &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‰Ω†‰πüÂèØ‰ª•ÈÄöËøá &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel Â∫îÁî®ÂïÜÂ∫ó&lt;/a&gt; Âø´ÈÄüÈÉ®ÁΩ≤ SQLBot„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊòØÂÜÖÁΩëÁéØÂ¢ÉÔºå‰Ω†ÂèØ‰ª•ÈÄöËøá &lt;a href="https://community.fit2cloud.com/#/products/sqlbot/downloads"&gt;Á¶ªÁ∫øÂÆâË£ÖÂåÖÊñπÂºè&lt;/a&gt; ÈÉ®ÁΩ≤ SQLBot„ÄÇ&lt;/p&gt; 
&lt;h3&gt;ËÆøÈóÆÊñπÂºè&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&amp;lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;Áî®Êà∑Âêç: admin&lt;/li&gt; 
 &lt;li&gt;ÂØÜÁ†Å: SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÅîÁ≥ªÊàë‰ª¨&lt;/h3&gt; 
&lt;p&gt;Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ&lt;/p&gt; 
&lt;img width="180" height="180" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI Â±ïÁ§∫&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1Panel-dev/CordysCRM"&gt;Cordys CRM&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫ê AI CRM Á≥ªÁªü&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ªìÂ∫ìÈÅµÂæ™ &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ&lt;/p&gt; 
&lt;p&gt;‰Ω†ÂèØ‰ª•Âü∫‰∫é SQLBot ÁöÑÊ∫ê‰ª£Á†ÅËøõË°å‰∫åÊ¨°ÂºÄÂèëÔºå‰ΩÜÊòØÈúÄË¶ÅÈÅµÂÆà‰ª•‰∏ãËßÑÂÆöÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‰∏çËÉΩÊõøÊç¢Âíå‰øÆÊîπ SQLBot ÁöÑ Logo ÂíåÁâàÊùÉ‰ø°ÊÅØÔºõ&lt;/li&gt; 
 &lt;li&gt;‰∫åÊ¨°ÂºÄÂèëÂêéÁöÑË°çÁîü‰ΩúÂìÅÂøÖÈ°ªÈÅµÂÆà GPL V3 ÁöÑÂºÄÊ∫ê‰πâÂä°„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Â¶ÇÈúÄÂïÜ‰∏öÊéàÊùÉÔºåËØ∑ËÅîÁ≥ª &lt;a href="mailto:support@fit2cloud.com"&gt;support@fit2cloud.com&lt;/a&gt; „ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Alibaba-NLP/DeepResearch</title>
      <link>https://github.com/Alibaba-NLP/DeepResearch</link>
      <description>&lt;p&gt;Tongyi Deep Research, the Leading Open-source Deep Research Agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/logo.png" width="100%" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;&lt;img src="https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="MODELS" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Alibaba-NLP/DeepResearch"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; ü§ó &lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;HuggingFace&lt;/a&gt; ÔΩú &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B" target="_blank"&gt;ModelScope&lt;/a&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14895" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14895" alt="Alibaba-NLP%2FDeepResearch | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;We present &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;strong&gt;Tongyi DeepResearch&lt;/strong&gt;, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for &lt;strong&gt;long-horizon, deep information-seeking&lt;/strong&gt; tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity's Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tongyi DeepResearch builds upon our previous work on the &lt;img src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png" width="14px" style="display:inline;" /&gt; &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/"&gt;WebAgent&lt;/a&gt; project.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;More details can be found in our üì∞&amp;nbsp;&lt;a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"&gt;Tech Blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/performance.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Fully automated synthetic data generation pipeline&lt;/strong&gt;: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Large-scale continual pre-training on agentic data&lt;/strong&gt;: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.&lt;/li&gt; 
 &lt;li&gt;üîÅ &lt;strong&gt;End-to-end reinforcement learning&lt;/strong&gt;: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Agent Inference Paradigm Compatibility&lt;/strong&gt;: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model's core intrinsic abilities, and an IterResearch-based 'Heavy' mode, which uses a test-time scaling strategy to unlock the model's maximum performance ceiling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Model Download&lt;/h1&gt; 
&lt;p&gt;You can directly download the model by following the links below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Download Links&lt;/th&gt; 
   &lt;th align="center"&gt;Model Size&lt;/th&gt; 
   &lt;th align="center"&gt;Context Length&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Tongyi-DeepResearch-30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B"&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br /&gt; &lt;a href="https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B"&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;30B-A3B&lt;/td&gt; 
   &lt;td align="center"&gt;128K&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;p&gt;[2025/09/17]üî• We have released &lt;strong&gt;Tongyi-DeepResearch-30B-A3B&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;Deep Research Benchmark Results&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This guide provides instructions for setting up the environment and running inference scripts located in the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/inference/"&gt;inference&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recommended Python version: &lt;strong&gt;3.10.0&lt;/strong&gt; (using other versions may cause dependency issues).&lt;/li&gt; 
 &lt;li&gt;It is strongly advised to create an isolated environment using &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with Conda
conda create -n react_infer_env python=3.10.0
conda activate react_infer_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install the required dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Environment Configuration and Prepare Evaluation Data&lt;/h3&gt; 
&lt;h4&gt;Environment Configuration&lt;/h4&gt; 
&lt;p&gt;Configure your API keys and settings by copying the example environment file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy the example environment file
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Edit the &lt;code&gt;.env&lt;/code&gt; file and provide your actual API keys and configuration values:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SERPER_KEY_ID&lt;/strong&gt;: Get your key from &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; for web search and Google Scholar&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JINA_API_KEYS&lt;/strong&gt;: Get your key from &lt;a href="https://jina.ai/"&gt;Jina.ai&lt;/a&gt; for web page reading&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API_KEY/API_BASE&lt;/strong&gt;: OpenAI-compatible API for page summarization from &lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DASHSCOPE_API_KEY&lt;/strong&gt;: Get your key from &lt;a href="https://dashscope.aliyun.com/"&gt;Dashscope&lt;/a&gt; for file parsing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SANDBOX_FUSION_ENDPOINT&lt;/strong&gt;: Python interpreter sandbox endpoints (see &lt;a href="https://github.com/bytedance/SandboxFusion"&gt;SandboxFusion&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MODEL_PATH&lt;/strong&gt;: Path to your model weights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DATASET&lt;/strong&gt;: Name of your evaluation dataset&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OUTPUT_PATH&lt;/strong&gt;: Directory for saving results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;.env&lt;/code&gt; file is gitignored, so your secrets will not be committed to the repository.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Prepare Evaluation Data&lt;/h4&gt; 
&lt;p&gt;The system supports two input file formats: &lt;strong&gt;JSON&lt;/strong&gt; and &lt;strong&gt;JSONL&lt;/strong&gt;.&lt;/p&gt; 
&lt;h4&gt;Supported File Formats:&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: JSONL Format (recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.jsonl&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.jsonl&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Each line must be a valid JSON object with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class="language-json"&gt;{"question": "What is the capital of France?", "answer": "Paris"}
{"question": "Explain quantum computing", "answer": ""}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: JSON Format&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.json&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.json&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;File must contain a JSON array of objects, each with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class="language-json"&gt;[
  {"question": "What is the capital of France?", "answer": "Paris"},
  {"question": "Explain quantum computing", "answer": ""}
]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; The &lt;code&gt;answer&lt;/code&gt; field contains the &lt;strong&gt;ground truth/reference answer&lt;/strong&gt; used for evaluation. The system generates its own responses to the questions, and these reference answers are used to automatically judge the quality of the generated responses during benchmark evaluation.&lt;/p&gt; 
&lt;h4&gt;File References for Document Processing:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;If using the &lt;em&gt;file parser&lt;/em&gt; tool, &lt;strong&gt;prepend the filename to the &lt;code&gt;question&lt;/code&gt; field&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Place referenced files in &lt;code&gt;eval_data/file_corpus/&lt;/code&gt; directory&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;{"question": "report.pdf What are the key findings?", "answer": "..."}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;File Organization:&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;project_root/
‚îú‚îÄ‚îÄ eval_data/
‚îÇ   ‚îú‚îÄ‚îÄ my_questions.jsonl          # Your evaluation data
‚îÇ   ‚îî‚îÄ‚îÄ file_corpus/                # Referenced documents
‚îÇ       ‚îú‚îÄ‚îÄ report.pdf
‚îÇ       ‚îî‚îÄ‚îÄ data.xlsx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Configure the Inference Script&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open &lt;code&gt;run_react_infer.sh&lt;/code&gt; and modify the following variables as instructed in the comments: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - path to the local or remote model weights.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;DATASET&lt;/code&gt; - full path to your evaluation file, e.g. &lt;code&gt;eval_data/my_questions.jsonl&lt;/code&gt; or &lt;code&gt;/path/to/my_questions.json&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OUTPUT_PATH&lt;/code&gt; - path for saving the prediction results, e.g. &lt;code&gt;./outputs&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required &lt;code&gt;API_KEY&lt;/code&gt;, &lt;code&gt;BASE_URL&lt;/code&gt;, or other credentials. Each key is explained inline in the bash script.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Run the Inference Script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash run_react_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;With these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.&lt;/p&gt; 
&lt;h3&gt;6. You can use OpenRouter's API to call our model&lt;/h3&gt; 
&lt;p&gt;Tongyi-DeepResearch-30B-A3B is now available at &lt;a href="https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b"&gt;OpenRouter&lt;/a&gt;. You can run the inference without any GPUs.&lt;/p&gt; 
&lt;p&gt;You need to modify the following in the file &lt;a href="https://github.com/Alibaba-NLP/DeepResearch/raw/main/inference/react_agent.py"&gt;inference/react_agent.py&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the call_server function: Set the API key and URL to your OpenRouter account‚Äôs API and URL.&lt;/li&gt; 
 &lt;li&gt;Change the model name to alibaba/tongyi-deepresearch-30b-a3b.&lt;/li&gt; 
 &lt;li&gt;Adjust the content concatenation way as described in the comments on lines &lt;strong&gt;88‚Äì90.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide benchmark evaluation scripts for various datasets. Please refer to the &lt;a href="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/evaluation/"&gt;evaluation scripts&lt;/a&gt; directory for more details.&lt;/p&gt; 
&lt;h2&gt;Deep Research Agent Family&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/family.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:&lt;/p&gt; 
&lt;p&gt;[1] &lt;a href="https://arxiv.org/pdf/2501.07572"&gt;WebWalker: Benchmarking LLMs in Web Traversal&lt;/a&gt; (ACL 2025)&lt;br /&gt; [2] &lt;a href="https://arxiv.org/pdf/2505.22648"&gt;WebDancer: Towards Autonomous Information Seeking Agency&lt;/a&gt; (NeurIPS 2025)&lt;br /&gt; [3] &lt;a href="https://arxiv.org/pdf/2507.02592"&gt;WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/a&gt;&lt;br /&gt; [4] &lt;a href="https://arxiv.org/pdf/2507.15061"&gt;WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/a&gt;&lt;br /&gt; [5] &lt;a href="https://arxiv.org/pdf/2508.05748"&gt;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&lt;/a&gt;&lt;br /&gt; [6] &lt;a href="https://arxiv.org/pdf/2509.13309"&gt;WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents&lt;/a&gt;&lt;br /&gt; [7] &lt;a href="https://arxiv.org/pdf/2509.13313"&gt;ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization&lt;/a&gt;&lt;br /&gt; [8] &lt;a href="https://arxiv.org/pdf/2509.13312"&gt;WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research&lt;/a&gt;&lt;br /&gt; [9] &lt;a href="https://arxiv.org/pdf/2509.13305"&gt;WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning&lt;/a&gt;&lt;br /&gt; [10] &lt;a href="https://arxiv.org/pdf/2509.13310"&gt;Scaling Agents via Continual Pre-training&lt;/a&gt;&lt;br /&gt; [11] &lt;a href="https://arxiv.org/pdf/2509.13311"&gt;Towards General Agentic Intelligence via Environment Scaling&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üåü Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#Alibaba-NLP/DeepResearch&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üö© Talent Recruitment&lt;/h2&gt; 
&lt;p&gt;üî•üî•üî• We are hiring! Research intern positions are open (based in Hangzhou„ÄÅBeijing„ÄÅShanghai)&lt;/p&gt; 
&lt;p&gt;üìö &lt;strong&gt;Research Area&lt;/strong&gt;ÔºöWeb Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; 
&lt;p&gt;‚òéÔ∏è &lt;strong&gt;Contact&lt;/strong&gt;Ôºö&lt;a href=""&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href="mailto:yongjiang.jy@alibaba-inc.com"&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi-DeepResearch},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>SkyworkAI/DeepResearchAgent</title>
      <link>https://github.com/SkyworkAI/DeepResearchAgent</link>
      <description>&lt;p&gt;DeepResearchAgent is a hierarchical multi-agent system designed not only for deep research tasks but also for general-purpose task solving. The framework leverages a top-level planning agent to coordinate multiple specialized lower-level agents, enabling automated task decomposition and efficient execution across diverse and complex domains.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepResearchAgent&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://skyworkai.github.io/DeepResearchAgent/"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%8C%90-Website-blue?style=for-the-badge&amp;amp;logo=github" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.12508"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%93%84-arXiv%20Paper-red?style=for-the-badge&amp;amp;logo=arxiv" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/README_CN.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://skyworkai.github.io/DeepResearchAgent/"&gt;üåê &lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;image.png DeepResearchAgent is a hierarchical multi-agent system designed not only for deep research tasks but also for general-purpose task solving. The framework leverages a top-level planning agent to coordinate multiple specialized lower-level agents, enabling automated task decomposition and efficient execution across diverse and complex domains.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üåê &lt;strong&gt;Check out our interactive website&lt;/strong&gt;: &lt;a href="https://skyworkai.github.io/DeepResearchAgent/"&gt;https://skyworkai.github.io/DeepResearchAgent/&lt;/a&gt; - Explore the architecture, view experiments, and learn more about our research!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/docs/assets/architecture.png" alt="Architecture" width="700" /&gt; &lt;/p&gt; 
&lt;p&gt;The system adopts a two-layer structure:&lt;/p&gt; 
&lt;h3&gt;1. Top-Level Planning Agent&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Responsible for understanding, decomposing, and planning the overall workflow for a given task.&lt;/li&gt; 
 &lt;li&gt;Breaks down tasks into manageable sub-tasks and assigns them to appropriate lower-level agents.&lt;/li&gt; 
 &lt;li&gt;Dynamically coordinates the collaboration among agents to ensure smooth task completion.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. Specialized Lower-Level Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep Analyzer&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Performs in-depth analysis of input information, extracting key insights and potential requirements.&lt;/li&gt; 
   &lt;li&gt;Supports analysis of various data types, including text and structured data.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep Researcher&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Conducts thorough research on specified topics or questions, retrieving and synthesizing high-quality information.&lt;/li&gt; 
   &lt;li&gt;Capable of generating research reports or knowledge summaries automatically.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Browser Use&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Automates browser operations, supporting web search, information extraction, and data collection tasks.&lt;/li&gt; 
   &lt;li&gt;Assists the Deep Researcher in acquiring up-to-date information from the internet.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MCP Manager Agent&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Manages and orchestrates Model Context Protocol (MCP) tools and services.&lt;/li&gt; 
   &lt;li&gt;Enables dynamic tool discovery, registration, and execution through MCP standards.&lt;/li&gt; 
   &lt;li&gt;Supports both local and remote MCP tool integration for enhanced agent capabilities.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Tool Calling Agent&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Provides a general-purpose interface for invoking various tools and APIs.&lt;/li&gt; 
   &lt;li&gt;Supports function calling, allowing the agent to execute specific tasks or retrieve information from external services.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hierarchical agent collaboration for complex and dynamic task scenarios&lt;/li&gt; 
 &lt;li&gt;Extensible agent system, allowing easy integration of additional specialized agents&lt;/li&gt; 
 &lt;li&gt;Automated information analysis, research, and web interaction capabilities&lt;/li&gt; 
 &lt;li&gt;Secure Python code execution environment for tools, featuring configurable import controls, restricted built-ins, attribute access limitations, and resource limits. (See &lt;a href="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/docs/python_interpreter_sandbox.md"&gt;PythonInterpreterTool Sandboxing&lt;/a&gt; for details).&lt;/li&gt; 
 &lt;li&gt;Support for asynchronous operations, enabling efficient handling of multiple tasks and agents&lt;/li&gt; 
 &lt;li&gt;Support for local and remote model inference, including OpenAI, Anthropic, Google LLMs, and local Qwen models via vLLM&lt;/li&gt; 
 &lt;li&gt;Support for image and video generation tools based on the Imagen and Veo3 models, respectively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Image and Video Examples:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/docs/assets/cat_yarn_playful_reference.png" alt="Image Example" width="300" /&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/docs/assets/cat_playing_with_yarn_video.gif" alt="Video Example" width="600" /&gt; &lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2025.08.04&lt;/strong&gt;: Add the support for loading mcp tools from the local json file.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2025.07.08&lt;/strong&gt;: Add the video generator tool, which can generate a video based on the input text and/or image. The video generator tool is based on the Veo3 model.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2025.07.08&lt;/strong&gt;: Add the image generator tool, which can generate images based on the input text. The image generator tool is based on the Imagen model.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2025.07.07&lt;/strong&gt;: Due to the limited flexibility of TOML configuration files, we have switched to using the config format supported by mmengine.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2025.06.20&lt;/strong&gt;: Add the support for the mcp (Both the local mcp and remote mcp).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2025.06.17&lt;/strong&gt;: Update technical report &lt;a href="https://arxiv.org/pdf/2506.12508"&gt;https://arxiv.org/pdf/2506.12508&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2025.06.01&lt;/strong&gt;: Update the browser-use to 0.1.48.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2025.05.30&lt;/strong&gt;: Convert the sub agent to a function call. Planning agent can now be gpt-4.1 or gemini-2.5-pro.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2025.05.27&lt;/strong&gt;: Support OpenAI, Anthropic, Google LLMs, and local Qwen models (via vLLM, see details in &lt;a href="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/#usage"&gt;Usage&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;TODO List&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Asynchronous feature completed&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Image Generator Tool completed&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Video Generator Tool completed&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; MCP in progress&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Load local MCP tools from JSON file completed&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; AI4Research Agent to be developed&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Novel Writing Agent to be developed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prepare Environment&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# poetry install environment
conda create -n dra python=3.11
conda activate dra
make install

# (Optional) You can also use requirements.txt
conda create -n dra python=3.11
conda activate dra
make install-requirements

# playwright install if needed
pip install playwright
playwright install chromium --with-deps --no-shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Set Up &lt;code&gt;.env&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Please refer to the &lt;code&gt;.env.template&lt;/code&gt; file and create a &lt;code&gt;.env&lt;/code&gt; file in the root directory of the project. This file is used to configure API keys and other environment variables.&lt;/p&gt; 
&lt;p&gt;Refer to the following instructions to obtain the necessary google gemini-2.5-pro API key and set it in the &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.google.com/app/apikey"&gt;https://aistudio.google.com/app/apikey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/docs/authentication/application-default-credentials?hl=zh-cn"&gt;https://cloud.google.com/docs/authentication/application-default-credentials?hl=zh-cn&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install --cask google-cloud-sdk
gcloud init
gcloud auth application-default login
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Main Example&lt;/h3&gt; 
&lt;p&gt;A simple example to demonstrate the usage of the DeepResearchAgent framework.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run Single Agent Example&lt;/h3&gt; 
&lt;p&gt;A simple example to demonstrate the usage of a single agent, such as a general tool calling agent.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/run_general.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;RUN GAIA Evaluation Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download GAIA
mkdir data &amp;amp;&amp;amp; cd data
git clone https://huggingface.co/datasets/gaia-benchmark/GAIA

# Run
python examples/run_gaia.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Experiments&lt;/h2&gt; 
&lt;p&gt;We evaluated our agent on both GAIA validation and test sets, achieving state-of-the-art performance. Our system demonstrates superior performance across all difficulty levels.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/docs/assets/gaia_test.png" alt="GAIA Test Results" width="300" /&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/docs/assets/gaia_validation.png" alt="GAIA Validation Results" width="300" /&gt; &lt;/p&gt; 
&lt;p&gt;With the integration of the Computer Use and MCP Manager Agent, which now enables pixel-level control of the browser, our system demonstrates remarkable evolutionary capabilities. The agents can dynamically acquire and enhance their abilities through learning and adaptation, leading to significantly improved performance. The latest results show:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Test Set&lt;/strong&gt;: 83.39 (average), with 93.55 on Level 1, 83.02 on Level 2, and 65.31 on Level 3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Validation Set&lt;/strong&gt;: 82.4 (average), with 92.5 on Level 1, 83.7 on Level 2, and 57.7 on Level 3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Questions&lt;/h2&gt; 
&lt;h3&gt;1. About Qwen Models&lt;/h3&gt; 
&lt;p&gt;Our framework now supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;qwen2.5-7b-instruct&lt;/li&gt; 
 &lt;li&gt;qwen2.5-14b-instruct&lt;/li&gt; 
 &lt;li&gt;qwen2.5-32b-instruct&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Update your config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;model_id = "qwen2.5-7b-instruct"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Browser Use&lt;/h3&gt; 
&lt;p&gt;If problems occur, reinstall:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "browser-use[memory]"==0.1.48
pip install playwright
playwright install chromium --with-deps --no-shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Sub-Agent Calling&lt;/h3&gt; 
&lt;p&gt;Function-calling is now supported natively by GPT-4.1 / Gemini 2.5 Pro. Claude-3.7-Sonnet is also recommended.&lt;/p&gt; 
&lt;h3&gt;4. Use vllm for local models&lt;/h3&gt; 
&lt;p&gt;We provide huggingface as a shortcut to the local model. Also provide vllm as a way to start services so that parallel acceleration can be provided.&lt;/p&gt; 
&lt;h4&gt;Step 1: Launch the vLLM Inference Service&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nohup bash -c 'CUDA_VISIBLE_DEVICES=0,1 python -m vllm.entrypoints.openai.api_server \
  --model /input0/Qwen3-32B \
  --served-model-name Qwen \
  --host 0.0.0.0 \
  --port 8000 \
  --max-num-seqs 16 \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --tensor_parallel_size 2' &amp;gt; vllm_qwen.log 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update &lt;code&gt;.env&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;QWEN_API_BASE=http://localhost:8000/v1
QWEN_API_KEY="abc"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 2: Launch the Agent Service&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;Use deep_researcher_agent to search the latest papers on the topic of 'AI Agent' and then summarize it.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;DeepResearchAgent is primarily inspired by the architecture of smolagents. The following improvements have been made:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The codebase of smolagents has been modularized for better structure and organization.&lt;/li&gt; 
 &lt;li&gt;The original synchronous framework has been refactored into an asynchronous one.&lt;/li&gt; 
 &lt;li&gt;The multi-agent setup process has been optimized to make it more user-friendly and efficient.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We would like to express our gratitude to the following open source projects, which have greatly contributed to the development of this work:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/smolagents"&gt;smolagents&lt;/a&gt; - A lightweight agent framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mannaandpoem/OpenManus"&gt;OpenManus&lt;/a&gt; - An asynchronous agent framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/browser-use/browser-use"&gt;browser-use&lt;/a&gt; - An AI-powered browser automation tool.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unclecode/crawl4ai"&gt;crawl4ai&lt;/a&gt; - A web crawling library for AI applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/markitdown"&gt;markitdown&lt;/a&gt; - A tool for converting files to Markdown format.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We sincerely appreciate the efforts of all contributors and maintainers of these projects for their commitment to advancing AI technologies and making them available to the wider community.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Contributions and suggestions are welcome! Feel free to open issues or submit pull requests.&lt;/p&gt; 
&lt;h2&gt;Cite&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{zhang2025agentorchestrahierarchicalmultiagentframework,
      title={AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving}, 
      author={Wentao Zhang, Liang Zeng, Yuzhen Xiao, Yongcong Li, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, Bo An},
      year={2025},
      eprint={2506.12508},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.12508}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üá®üá≥ ‰∏≠ÊñáÁâàËØ¥ÊòéÊñáÊ°£&lt;/h3&gt; 
&lt;p&gt;Â¶ÇÊûú‰Ω†Êõ¥‰π†ÊÉØÈòÖËØª‰∏≠ÊñáËØ¥ÊòéÊñáÊ°£ÔºåËØ∑Êü•ÈòÖ &lt;a href="https://raw.githubusercontent.com/SkyworkAI/DeepResearchAgent/main/README_CN.md"&gt;README_CN.md&lt;/a&gt;„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LazyVim/LazyVim</title>
      <link>https://github.com/LazyVim/LazyVim</link>
      <description>&lt;p&gt;Neovim config for the lazy&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/292349/213446185-2db63fd5-8c84-459c-9f04-e286382d6e80.png" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://lazyvim.github.io/installation"&gt;Install&lt;/a&gt; ¬∑ &lt;a href="https://lazyvim.github.io/configuration"&gt;Configure&lt;/a&gt; ¬∑ &lt;a href="https://lazyvim.github.io"&gt;Docs&lt;/a&gt; &lt;/h4&gt; 
&lt;div align="center"&gt;
 &lt;p&gt; &lt;a href="https://github.com/LazyVim/LazyVim/releases/latest"&gt; &lt;img alt="Latest release" src="https://img.shields.io/github/v/release/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=starship&amp;amp;color=C9CBFF&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41&amp;amp;include_prerelease&amp;amp;sort=semver" /&gt; &lt;/a&gt; &lt;a href="https://github.com/LazyVim/LazyVim/pulse"&gt; &lt;img alt="Last commit" src="https://img.shields.io/github/last-commit/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=starship&amp;amp;color=8bd5ca&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41" /&gt; &lt;/a&gt; &lt;a href="https://github.com/LazyVim/LazyVim/raw/main/LICENSE"&gt; &lt;img alt="License" src="https://img.shields.io/github/license/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=starship&amp;amp;color=ee999f&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41" /&gt; &lt;/a&gt; &lt;a href="https://github.com/LazyVim/LazyVim/stargazers"&gt; &lt;img alt="Stars" src="https://img.shields.io/github/stars/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=starship&amp;amp;color=c69ff5&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41" /&gt; &lt;/a&gt; &lt;a href="https://github.com/LazyVim/LazyVim/issues"&gt; &lt;img alt="Issues" src="https://img.shields.io/github/issues/LazyVim/LazyVim?style=for-the-badge&amp;amp;logo=bilibili&amp;amp;color=F5E0DC&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41" /&gt; &lt;/a&gt; &lt;a href="https://github.com/LazyVim/LazyVim"&gt; &lt;img alt="Repo Size" src="https://img.shields.io/github/repo-size/LazyVim/LazyVim?color=%23DDB6F2&amp;amp;label=SIZE&amp;amp;logo=codesandbox&amp;amp;style=for-the-badge&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=folke"&gt; &lt;img alt="follow on Twitter" src="https://img.shields.io/twitter/follow/folke?style=for-the-badge&amp;amp;logo=twitter&amp;amp;color=8aadf3&amp;amp;logoColor=D9E0EE&amp;amp;labelColor=302D41" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;LazyVim is a Neovim setup powered by &lt;a href="https://github.com/folke/lazy.nvim"&gt;üí§ lazy.nvim&lt;/a&gt; to make it easy to customize and extend your config. Rather than having to choose between starting from scratch or using a pre-made distro, LazyVim offers the best of both worlds - the flexibility to tweak your config as needed, along with the convenience of a pre-configured setup.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/292349/211285846-0b7bb3bf-0462-4029-b64c-4ee1d037fc1c.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/292349/213447056-92290767-ea16-430c-8727-ce994c93e9cc.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üî• Transform your Neovim into a full-fledged IDE&lt;/li&gt; 
 &lt;li&gt;üí§ Easily customize and extend your config with &lt;a href="https://github.com/folke/lazy.nvim"&gt;lazy.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöÄ Blazingly fast&lt;/li&gt; 
 &lt;li&gt;üßπ Sane default settings for options, autocmds, and keymaps&lt;/li&gt; 
 &lt;li&gt;üì¶ Comes with a wealth of plugins pre-configured and ready to use&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö°Ô∏è Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Neovim &amp;gt;= &lt;strong&gt;0.11.2&lt;/strong&gt; (needs to be built with &lt;strong&gt;LuaJIT&lt;/strong&gt;)&lt;/li&gt; 
 &lt;li&gt;Git &amp;gt;= &lt;strong&gt;2.19.0&lt;/strong&gt; (for partial clones support)&lt;/li&gt; 
 &lt;li&gt;a &lt;a href="https://www.nerdfonts.com/"&gt;Nerd Font&lt;/a&gt; &lt;strong&gt;&lt;em&gt;(optional)&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;a &lt;strong&gt;C&lt;/strong&gt; compiler for &lt;code&gt;nvim-treesitter&lt;/code&gt;. See &lt;a href="https://github.com/nvim-treesitter/nvim-treesitter#requirements"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;p&gt;You can find a starter template for &lt;strong&gt;LazyVim&lt;/strong&gt; &lt;a href="https://github.com/LazyVim/starter"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Try it with Docker&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;docker run -w /root -it --rm alpine:edge sh -uelic '
  apk add git lazygit fzf curl neovim ripgrep alpine-sdk --update
  git clone https://github.com/LazyVim/starter ~/.config/nvim
  cd ~/.config/nvim
  nvim
'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Install the &lt;a href="https://github.com/LazyVim/starter"&gt;LazyVim Starter&lt;/a&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Make a backup of your current Neovim files:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;mv ~/.config/nvim ~/.config/nvim.bak
mv ~/.local/share/nvim ~/.local/share/nvim.bak
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Clone the starter&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/LazyVim/starter ~/.config/nvim
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Remove the &lt;code&gt;.git&lt;/code&gt; folder, so you can add it to your own repo later&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rm -rf ~/.config/nvim/.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Start Neovim!&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;nvim
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Refer to the comments in the files on how to customize &lt;strong&gt;LazyVim&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;p&gt;There's a great video created by &lt;a href="https://github.com/elijahmanor"&gt;@elijahmanor&lt;/a&gt; with a walkthrough to get started.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=N93cTbtLCIM"&gt;&lt;img src="https://img.youtube.com/vi/N93cTbtLCIM/hqdefault.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/dusty-phillips"&gt;@dusty-phillips&lt;/a&gt; wrote a comprehensive book called &lt;a href="https://lazyvim-ambitious-devs.phillips.codes"&gt;LazyVim for Ambitious Developers&lt;/a&gt; available for free online.&lt;/p&gt; 
&lt;h2&gt;üìÇ File Structure&lt;/h2&gt; 
&lt;p&gt;The files under config will be automatically loaded at the appropriate time, so you don't need to require those files manually. &lt;strong&gt;LazyVim&lt;/strong&gt; comes with a set of default config files that will be loaded &lt;strong&gt;&lt;em&gt;before&lt;/em&gt;&lt;/strong&gt; your own. See &lt;a href="https://github.com/LazyVim/LazyVim/tree/main/lua/lazyvim/config"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can add your custom plugin specs under &lt;code&gt;lua/plugins/&lt;/code&gt;. All files there will be automatically loaded by &lt;a href="https://github.com/folke/lazy.nvim"&gt;lazy.nvim&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;
~/.config/nvim
‚îú‚îÄ‚îÄ lua
‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ config
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ autocmds.lua
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ keymaps.lua
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ lazy.lua
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ options.lua
‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ plugins
‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ spec1.lua
‚îÇ&amp;nbsp;&amp;nbsp;     ‚îú‚îÄ‚îÄ **
‚îÇ&amp;nbsp;&amp;nbsp;     ‚îî‚îÄ‚îÄ spec2.lua
‚îî‚îÄ‚îÄ init.lua
&lt;/pre&gt; 
&lt;h2&gt;‚öôÔ∏è Configuration&lt;/h2&gt; 
&lt;p&gt;Refer to the &lt;a href="https://lazyvim.github.io"&gt;docs&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-research/timesfm</title>
      <link>https://github.com/google-research/timesfm</link>
      <description>&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; 
&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2310.10688"&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, ICML 2024.&lt;/li&gt; 
 &lt;li&gt;All checkpoints: &lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;TimesFM Hugging Face Collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/"&gt;Google Research blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/bigquery/docs/timesfm-model"&gt;TimesFM in BigQuery&lt;/a&gt;: an official Google product.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This open version is not an officially supported Google product.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Latest Model Version:&lt;/strong&gt; TimesFM 2.5&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Archived Model Versions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;1.0 and 2.0: relevant code archived in the sub directory &lt;code&gt;v1&lt;/code&gt;. You can &lt;code&gt;pip install timesfm==1.3.0&lt;/code&gt; to install an older version of this package to load them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Update - Sept. 15, 2025&lt;/h2&gt; 
&lt;p&gt;TimesFM 2.5 is out!&lt;/p&gt; 
&lt;p&gt;Comparing to TimesFM 2.0, this new 2.5 model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;uses 200M parameters, down from 500M.&lt;/li&gt; 
 &lt;li&gt;supports up to 16k context length, up from 2048.&lt;/li&gt; 
 &lt;li&gt;supports continuous quantile forecast up to 1k horizon via an optional 30M quantile head.&lt;/li&gt; 
 &lt;li&gt;gets rid of the &lt;code&gt;frequency&lt;/code&gt; indicator.&lt;/li&gt; 
 &lt;li&gt;has a couple of new forecasting flags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Along with the model upgrade we have also upgraded the inference API. This repo will be under construction over the next few weeks to&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;add support for an upcoming Flax version of the model (faster inference).&lt;/li&gt; 
 &lt;li&gt;add back covariate support.&lt;/li&gt; 
 &lt;li&gt;populate more docstrings, docs and notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install&lt;/h3&gt; 
&lt;p&gt;TODO(siriuz42): Package timesfm==2.0.0 and upload to PyPI .&lt;/p&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/google-research/timesfm.git
cd timesfm
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Code Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import timesfm
model = timesfm.TimesFM_2p5_200M_torch()
model.load_checkpoint()
model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ItzCrazyKns/Perplexica</title>
      <link>https://github.com/ItzCrazyKns/Perplexica</link>
      <description>&lt;p&gt;Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üöÄ Perplexica - An AI-powered search engine üîé 
 &lt;!-- omit in toc --&gt;&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.warp.dev/perplexica"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/775dd593-9b5f-40f1-bf48-479faff4c27b" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="https://www.warp.dev/perplexica"&gt;Warp, the AI Devtool that lives in your terminal&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://www.warp.dev/perplexica"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/26aArMy8tT"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/26aArMy8tT?style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-screenshot.png?" alt="preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#overview"&gt;Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#preview"&gt;Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#installation"&gt;Installation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#getting-started-with-docker-recommended"&gt;Getting Started with Docker (Recommended)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#non-docker-installation"&gt;Non-Docker Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#ollama-connection-errors"&gt;Ollama Connection Errors&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#lemonade-connection-errors"&gt;Lemonade Connection Errors&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#using-as-a-search-engine"&gt;Using as a Search Engine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#using-perplexicas-api"&gt;Using Perplexica's API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#expose-perplexica-to-network"&gt;Expose Perplexica to a network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#one-click-deployment"&gt;One-Click Deployment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features"&gt;Upcoming Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#support-us"&gt;Support Us&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#donations"&gt;Donations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#help-and-support"&gt;Help and Support&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Perplexica is an open-source AI-powered searching tool or an AI-powered search engine that goes deep into the internet to find answers. Inspired by Perplexity AI, it's an open-source option that not just searches the web but understands your questions. It uses advanced machine learning algorithms like similarity searching and embeddings to refine results and provides clear answers with sources cited.&lt;/p&gt; 
&lt;p&gt;Using SearxNG to stay current and fully open source, Perplexica ensures you always get the most up-to-date information without compromising your privacy.&lt;/p&gt; 
&lt;p&gt;Want to know more about its architecture and how it works? You can read it &lt;a href="https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Preview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-preview.gif" alt="video-preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Local LLMs&lt;/strong&gt;: You can utilize local LLMs such as Qwen, DeepSeek, Llama, and Mistral.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two Main Modes:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Copilot Mode:&lt;/strong&gt; (In development) Boosts search by generating different queries to find more relevant internet sources. Like normal search instead of just using the context by SearxNG, it visits the top matches and tries to find relevant sources to the user's query directly from the page.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Normal Mode:&lt;/strong&gt; Processes your query and performs a web search.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Focus Modes:&lt;/strong&gt; Special modes to better answer specific types of questions. Perplexica currently has 6 focus modes: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All Mode:&lt;/strong&gt; Searches the entire web to find the best results.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Writing Assistant Mode:&lt;/strong&gt; Helpful for writing tasks that do not require searching the web.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Academic Search Mode:&lt;/strong&gt; Finds articles and papers, ideal for academic research.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;YouTube Search Mode:&lt;/strong&gt; Finds YouTube videos based on the search query.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Wolfram Alpha Search Mode:&lt;/strong&gt; Answers queries that need calculations or data analysis using Wolfram Alpha.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Reddit Search Mode:&lt;/strong&gt; Searches Reddit for discussions and opinions related to the query.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Current Information:&lt;/strong&gt; Some search tools might give you outdated info because they use data from crawling bots and convert them into embeddings and store them in a index. Unlike them, Perplexica uses SearxNG, a metasearch engine to get the results and rerank and get the most relevant source out of it, ensuring you always get the latest information without the overhead of daily data updates.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt;: Integrate Perplexica into your existing applications and make use of its capibilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It has many more features like image and video search. Some of the planned features are mentioned in &lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features"&gt;upcoming features&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;There are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.&lt;/p&gt; 
&lt;h3&gt;Getting Started with Docker (Recommended)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure Docker is installed and running on your system.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the Perplexica repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ItzCrazyKns/Perplexica.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;After cloning, navigate to the directory containing the project files.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt;. For Docker setups, you need only fill in the following fields:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OPENAI&lt;/code&gt;: Your OpenAI API key. &lt;strong&gt;You only need to fill this if you wish to use OpenAI's models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;CUSTOM_OPENAI&lt;/code&gt;: Your OpenAI-API-compliant local server URL, model name, and API key. You should run your local server with host set to &lt;code&gt;0.0.0.0&lt;/code&gt;, take note of which port number it is running on, and then use that port number to set &lt;code&gt;API_URL = http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. You must specify the model name, such as &lt;code&gt;MODEL_NAME = "unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_XL"&lt;/code&gt;. Finally, set &lt;code&gt;API_KEY&lt;/code&gt; to the appropriate value. If you have not defined an API key, just put anything you want in-between the quotation marks: &lt;code&gt;API_KEY = "whatever-you-want-but-not-blank"&lt;/code&gt; &lt;strong&gt;You only need to configure these settings if you want to use a local OpenAI-compliant server, such as Llama.cpp's &lt;a href="https://github.com/ggml-org/llama.cpp/raw/master/tools/server/README.md"&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OLLAMA&lt;/code&gt;: Your Ollama API URL. You should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Ollama on port 11434, use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Ollama's models instead of OpenAI's&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;LEMONADE&lt;/code&gt;: Your Lemonade API URL. Since Lemonade runs directly on your local machine (not in Docker), you should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Lemonade on port 8000, use &lt;code&gt;http://host.docker.internal:8000&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Lemonade's models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;GROQ&lt;/code&gt;: Your Groq API key. &lt;strong&gt;You only need to fill this if you wish to use Groq's hosted models&lt;/strong&gt;.`&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;ANTHROPIC&lt;/code&gt;: Your Anthropic API key. &lt;strong&gt;You only need to fill this if you wish to use Anthropic models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;Gemini&lt;/code&gt;: Your Gemini API key. &lt;strong&gt;You only need to fill this if you wish to use Google's models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;DEEPSEEK&lt;/code&gt;: Your Deepseek API key. &lt;strong&gt;Only needed if you want Deepseek models.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;AIMLAPI&lt;/code&gt;: Your AI/ML API key. &lt;strong&gt;Only needed if you want to use AI/ML API models and embeddings.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can change these after starting Perplexica from the settings dialog.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;SIMILARITY_MEASURE&lt;/code&gt;: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensure you are in the directory containing the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and execute:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Wait a few minutes for the setup to complete. You can access Perplexica at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt; in your web browser.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.&lt;/p&gt; 
&lt;h3&gt;Non-Docker Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install SearXNG and allow &lt;code&gt;JSON&lt;/code&gt; format in the SearXNG settings.&lt;/li&gt; 
 &lt;li&gt;Clone the repository and rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt; in the root directory. Ensure you complete all required fields in this file.&lt;/li&gt; 
 &lt;li&gt;After populating the configuration run &lt;code&gt;npm i&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install the dependencies and then execute &lt;code&gt;npm run build&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Finally, start the app by running &lt;code&gt;npm run start&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/installation"&gt;installation documentation&lt;/a&gt; for more information like updating, etc.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;h4&gt;Local OpenAI-API-Compliant Servers&lt;/h4&gt; 
&lt;p&gt;If Perplexica tells you that you haven't configured any chat model providers, ensure that:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Your server is running on &lt;code&gt;0.0.0.0&lt;/code&gt; (not &lt;code&gt;127.0.0.1&lt;/code&gt;) and on the same port you put in the API URL.&lt;/li&gt; 
 &lt;li&gt;You have specified the correct model name loaded by your local LLM server.&lt;/li&gt; 
 &lt;li&gt;You have specified the correct API key, or if one is not defined, you have put &lt;em&gt;something&lt;/em&gt; in the API key field and not left it empty.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Ollama Connection Errors&lt;/h4&gt; 
&lt;p&gt;If you're encountering an Ollama connection error, it is likely due to the backend being unable to connect to Ollama's API. To fix this issue you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check your Ollama API URL:&lt;/strong&gt; Ensure that the API URL is correctly set in the settings menu.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Update API URL Based on OS:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; Use &lt;code&gt;http://&amp;lt;private_ip_of_host&amp;gt;:11434&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Adjust the port number if you're using a different one.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux Users - Expose Ollama to Network:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;/etc/systemd/system/ollama.service&lt;/code&gt;, you need to add &lt;code&gt;Environment="OLLAMA_HOST=0.0.0.0:11434"&lt;/code&gt;. (Change the port number if you are using a different one.) Then reload the systemd manager configuration with &lt;code&gt;systemctl daemon-reload&lt;/code&gt;, and restart Ollama by &lt;code&gt;systemctl restart ollama&lt;/code&gt;. For more information see &lt;a href="https://github.com/ollama/ollama/raw/main/docs/faq.md#setting-environment-variables-on-linux"&gt;Ollama docs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Ensure that the port (default is 11434) is not blocked by your firewall.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Lemonade Connection Errors&lt;/h4&gt; 
&lt;p&gt;If you're encountering a Lemonade connection error, it is likely due to the backend being unable to connect to Lemonade's API. To fix this issue you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check your Lemonade API URL:&lt;/strong&gt; Ensure that the API URL is correctly set in the settings menu.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Update API URL Based on OS:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:8000&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:8000&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; Use &lt;code&gt;http://&amp;lt;private_ip_of_host&amp;gt;:8000&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Adjust the port number if you're using a different one.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ensure Lemonade Server is Running:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Make sure your Lemonade server is running and accessible on the configured port (default is 8000).&lt;/li&gt; 
   &lt;li&gt;Verify that Lemonade is configured to accept connections from all interfaces (&lt;code&gt;0.0.0.0&lt;/code&gt;), not just localhost (&lt;code&gt;127.0.0.1&lt;/code&gt;).&lt;/li&gt; 
   &lt;li&gt;Ensure that the port (default is 8000) is not blocked by your firewall.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using as a Search Engine&lt;/h2&gt; 
&lt;p&gt;If you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser's search bar, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open your browser's settings.&lt;/li&gt; 
 &lt;li&gt;Navigate to the 'Search Engines' section.&lt;/li&gt; 
 &lt;li&gt;Add a new site search with the following URL: &lt;code&gt;http://localhost:3000/?q=%s&lt;/code&gt;. Replace &lt;code&gt;localhost&lt;/code&gt; with your IP address or domain name, and &lt;code&gt;3000&lt;/code&gt; with the port number if Perplexica is not hosted locally.&lt;/li&gt; 
 &lt;li&gt;Click the add button. Now, you can use Perplexica directly from your browser's search bar.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using Perplexica's API&lt;/h2&gt; 
&lt;p&gt;Perplexica also provides an API for developers looking to integrate its powerful search engine into their own applications. You can run searches, use multiple models and get answers to your queries.&lt;/p&gt; 
&lt;p&gt;For more details, check out the full documentation &lt;a href="https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/API/SEARCH.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Expose Perplexica to network&lt;/h2&gt; 
&lt;p&gt;Perplexica runs on Next.js and handles all API requests. It works right away on the same network and stays accessible even with port forwarding.&lt;/p&gt; 
&lt;h2&gt;One-Click Deployment&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://usw.sealos.io/?openapp=system-template%3FtemplateName%3Dperplexica"&gt;&lt;img src="https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg?sanitize=true" alt="Deploy to Sealos" /&gt;&lt;/a&gt; &lt;a href="https://repocloud.io/details/?app_id=267"&gt;&lt;img src="https://d16t0pc4846x52.cloudfront.net/deploylobe.svg?sanitize=true" alt="Deploy to RepoCloud" /&gt;&lt;/a&gt; &lt;a href="https://template.run.claw.cloud/?referralCode=U11MRQ8U9RM4&amp;amp;openapp=system-fastdeploy%3FtemplateName%3Dperplexica"&gt;&lt;img src="https://raw.githubusercontent.com/ClawCloud/Run-Template/refs/heads/main/Run-on-ClawCloud.svg?sanitize=true" alt="Run on ClawCloud" /&gt;&lt;/a&gt; &lt;a href="https://www.hostinger.com/vps/docker-hosting?compose_url=https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/refs/heads/master/docker-compose.yaml"&gt;&lt;img src="https://assets.hostinger.com/vps/deploy.svg?sanitize=true" alt="Deploy on Hostinger" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Upcoming Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add settings page&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Adding support for local LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; History Saving features&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Introducing various Focus Modes&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Adding API support&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Adding Discover&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Finalizing Copilot Mode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;If you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.&lt;/p&gt; 
&lt;h3&gt;Donations&lt;/h3&gt; 
&lt;p&gt;We also accept donations to help sustain our project. If you would like to contribute, you can use the following options to donate. Thank you for your support!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ethereum&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Address: &lt;code&gt;0xB025a84b2F269570Eb8D4b05DEdaA41D8525B6DD&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Perplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the &lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file to learn more about Perplexica and how you can contribute to it.&lt;/p&gt; 
&lt;h2&gt;Help and Support&lt;/h2&gt; 
&lt;p&gt;If you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. &lt;a href="https://discord.gg/EFwsmQDgAu"&gt;Click here&lt;/a&gt; to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at &lt;code&gt;itzcrazykns&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don't forget to check back for updates and new features!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sentient-agi/ROMA</title>
      <link>https://github.com/sentient-agi/ROMA</link>
      <description>&lt;p&gt;Recursive-Open-Meta-Agent v0.1 (Beta). A meta-agent framework to build high-performance multi-agent systems.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/sentient-logo-new-M.png" alt="alt text" width="60%" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;ROMA: Recursive Open Meta-Agents&lt;/h1&gt; 
 &lt;p align="center"&gt; &lt;strong&gt;Building hierarchical high-performance multi-agent systems made easy! (Beta) &lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14848" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14848" alt="sentient-agi%2FROMA | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://sentient.xyz/" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Homepage" src="https://img.shields.io/badge/Sentient-Homepage-%23EAEAEA?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%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%2BPC9zdmc%2B&amp;amp;link=https%3A%2F%2Fhuggingface.co%2FSentientagi" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://github.com/sentient-agi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="GitHub" src="https://img.shields.io/badge/Github-sentient_agi-181717?logo=github" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;a href="https://huggingface.co/Sentientagi" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SentientAGI-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;/div&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://discord.gg/sentientfoundation" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord-SentientAGI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;a href="https://x.com/SentientAGI" target="_blank" style="margin: 2px;"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/badge/-SentientAGI-grey?logo=x&amp;amp;link=https%3A%2F%2Fx.com%2FSentientAGI%2F" style="display: inline-block; vertical-align: middle;" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.sentient.xyz/blog/recursive-open-meta-agent"&gt;Technical Blog&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/"&gt;Paper (Coming soon)&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.sentient.xyz/"&gt;Build Agents for $$$&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt;  
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/INTRODUCTION.md"&gt;üöÄ Introduction&lt;/a&gt;&lt;/strong&gt; - Understand the vision and architecture behind ROMA&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;üì¶ Setup&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;ü§ñ Agents Guide&lt;/a&gt;&lt;/strong&gt; - Learn how to create and customize your own agents&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/CONFIGURATION.md"&gt;‚öôÔ∏è Configuration&lt;/a&gt;&lt;/strong&gt; - Detailed configuration options and environment setup&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/ROADMAP.md"&gt;üó∫Ô∏è Roadmap&lt;/a&gt;&lt;/strong&gt; - See what's coming next for ROMA&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéØ What is ROMA?&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/roma_run.gif" alt="alt text" width="80%" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; is a &lt;strong&gt;meta-agent framework&lt;/strong&gt; that uses recursive hierarchical structures to solve complex problems. By breaking down tasks into parallelizable components, ROMA enables agents to tackle sophisticated reasoning challenges while maintaining transparency that makes context-engineering and iteration straightforward. The framework offers &lt;strong&gt;parallel problem solving&lt;/strong&gt; where agents work simultaneously on different parts of complex tasks, &lt;strong&gt;transparent development&lt;/strong&gt; with a clear structure for easy debugging, and &lt;strong&gt;proven performance&lt;/strong&gt; demonstrated through our search agent's strong benchmark results. We've shown the framework's effectiveness, but this is just the beginning. As an &lt;strong&gt;open-source and extensible&lt;/strong&gt; platform, ROMA is designed for community-driven development, allowing you to build and customize agents for your specific needs while benefiting from the collective improvements of the community.&lt;/p&gt; 
&lt;h2&gt;üèóÔ∏è How It Works&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ROMA&lt;/strong&gt; framework processes tasks through a recursive &lt;strong&gt;plan‚Äìexecute loop&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def solve(task):
    if is_atomic(task):                 # Step 1: Atomizer
        return execute(task)            # Step 2: Executor
    else:
        subtasks = plan(task)           # Step 2: Planner
        results = []
        for subtask in subtasks:
            results.append(solve(subtask))  # Recursive call
        return aggregate(results)       # Step 3: Aggregator

# Entry point:
answer = solve(initial_request)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Atomizer&lt;/strong&gt; ‚Äì Decides whether a request is &lt;strong&gt;atomic&lt;/strong&gt; (directly executable) or requires &lt;strong&gt;planning&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Planner&lt;/strong&gt; ‚Äì If planning is needed, the task is broken into smaller &lt;strong&gt;subtasks&lt;/strong&gt;. Each subtask is fed back into the &lt;strong&gt;Atomizer&lt;/strong&gt;, making the process recursive.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Executors&lt;/strong&gt; ‚Äì Handle atomic tasks. Executors can be &lt;strong&gt;LLMs, APIs, or even other agents&lt;/strong&gt; ‚Äî as long as they implement an &lt;code&gt;agent.execute()&lt;/code&gt; interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aggregator&lt;/strong&gt; ‚Äì Collects and integrates results from subtasks. Importantly, the Aggregator produces the &lt;strong&gt;answer to the original parent task&lt;/strong&gt;, not just raw child outputs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;üìê Information Flow&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Top-down:&lt;/strong&gt; Tasks are decomposed into subtasks recursively.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bottom-up:&lt;/strong&gt; Subtask results are aggregated upwards into solutions for parent tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Left-to-right:&lt;/strong&gt; If a subtask depends on the output of a previous one, it waits until that subtask completes before execution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This structure makes the system flexible, recursive, and dependency-aware ‚Äî capable of decomposing complex problems into smaller steps while ensuring results are integrated coherently.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view the system flow diagram&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TB
    A[Your Request] --&amp;gt; B{Atomizer}
    B --&amp;gt;|Plan Needed| C[Planner]
    B --&amp;gt;|Atomic Task| D[Executor]

    %% Planner spawns subtasks
    C --&amp;gt; E[Subtasks]
    E --&amp;gt; G[Aggregator]

    %% Recursion
    E -.-&amp;gt; B  

    %% Execution + Aggregation
    D --&amp;gt; F[Final Result]
    G --&amp;gt; F

    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style B fill:#fff3e0
    style C fill:#ffe0b2
    style D fill:#d1c4e9
    style G fill:#c5cae9

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt;
&lt;br /&gt; 
&lt;h3&gt;üöÄ 30-Second Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sentient-agi/ROMA.git
cd ROMA

# Run the automated setup
./setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Choose between:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Setup&lt;/strong&gt; (Recommended) - One-command setup with isolation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native Setup&lt;/strong&gt; - Direct installation for development&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üõ†Ô∏è Technical Stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: Built on &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/your/agnoagents%5D(https://github.com/agno-agi/agno)"&gt;AgnoAgents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Python 3.12+ with FastAPI/Flask&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + TypeScript with real-time WebSocket&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Support&lt;/strong&gt;: Any provider via LiteLLM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Persistence&lt;/strong&gt;: Enterprise S3 mounting with security validation 
  &lt;ul&gt; 
   &lt;li&gt;üîí &lt;strong&gt;goofys FUSE mounting&lt;/strong&gt; for zero-latency file access&lt;/li&gt; 
   &lt;li&gt;üõ°Ô∏è &lt;strong&gt;Path injection protection&lt;/strong&gt; with comprehensive validation&lt;/li&gt; 
   &lt;li&gt;üîê &lt;strong&gt;AWS credentials verification&lt;/strong&gt; before operations&lt;/li&gt; 
   &lt;li&gt;üìÅ &lt;strong&gt;Dynamic Docker Compose&lt;/strong&gt; with secure volume mounting&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Execution&lt;/strong&gt;: E2B sandboxes with unified S3 integration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: Production-grade validation and error handling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt;: Multi-modal, tools, MCP, hooks, caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì¶ Installation Options&lt;/h2&gt; 
&lt;h3&gt;Quick Start (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Main setup (choose Docker or Native)
./setup.sh

# Optional: Setup E2B sandbox integration
./setup.sh --e2b

# Test E2B integration  
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Command Line Options&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./setup.sh --docker     # Run Docker setup directly
./setup.sh --docker-from-scratch  # Rebuild Docker images/containers from scratch (down -v, no cache)
./setup.sh --native     # Run native setup directly (macOS/Ubuntu/Debian)
./setup.sh --e2b        # Setup E2B template (requires E2B_API_KEY + AWS creds)
./setup.sh --test-e2b   # Test E2B template integration
./setup.sh --help       # Show all available options
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual Installation&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;setup docs&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h3&gt;üèóÔ∏è Optional: E2B Sandbox Integration&lt;/h3&gt; 
&lt;p&gt;For secure code execution capabilities, optionally set up E2B sandboxes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# After main setup, configure E2B (requires E2B_API_KEY and AWS credentials in .env)
./setup.sh --e2b

# Test E2B integration
./setup.sh --test-e2b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;E2B Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Secure Code Execution&lt;/strong&gt; - Run untrusted code in isolated sandboxes&lt;/li&gt; 
 &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;S3 Integration&lt;/strong&gt; - Automatic data sync between local and sandbox environments&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;goofys Mounting&lt;/strong&gt; - High-performance S3 filesystem mounting&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;AWS Credentials&lt;/strong&gt; - Passed securely via Docker build arguments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ñ Pre-built Agents&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; These agents are demonstrations built using ROMA's framework through simple vibe-prompting and minimal manual tuning. They showcase how easily you can create high-performance agents with ROMA, rather than being production-final solutions. Our mission is to empower the community to build, share, and get rewarded for creating innovative agent recipes and use-cases.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ROMA comes with example agents that demonstrate the framework's capabilities:&lt;/p&gt; 
&lt;h3&gt;üîç General Task Solver&lt;/h3&gt; 
&lt;p&gt;A versatile agent powered by ChatGPT Search Preview for handling diverse tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Search&lt;/strong&gt;: Leverages OpenAI's latest search capabilities for real-time information&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Planning&lt;/strong&gt;: Adapts task decomposition based on query complexity&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Domain&lt;/strong&gt;: Handles everything from technical questions to creative projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quick Prototyping&lt;/strong&gt;: Perfect for testing ROMA's capabilities without domain-specific setup&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: General research, fact-checking, exploratory analysis, quick information gathering&lt;/p&gt; 
&lt;h3&gt;üî¨ Deep Research Agent&lt;/h3&gt; 
&lt;p&gt;A comprehensive research system that breaks down complex research questions into manageable sub-tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Task Decomposition&lt;/strong&gt;: Automatically splits research topics into search, analysis, and synthesis phases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Information Gathering&lt;/strong&gt;: Executes multiple searches simultaneously for faster results&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Source Integration&lt;/strong&gt;: Combines results from web search, Wikipedia, and specialized APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Synthesis&lt;/strong&gt;: Aggregates findings into coherent, well-structured reports&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Academic research, market analysis, competitive intelligence, technical documentation&lt;/p&gt; 
&lt;h3&gt;üíπ Crypto Analytics Agent&lt;/h3&gt; 
&lt;p&gt;Specialized financial analysis agent with deep blockchain and DeFi expertise:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Market Data&lt;/strong&gt;: Integrates with Binance, CoinGecko, and DefiLlama APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-Chain Analytics&lt;/strong&gt;: Access to Arkham Intelligence for wallet tracking and token flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technical Analysis&lt;/strong&gt;: Advanced charting with OHLC data and market indicators&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeFi Metrics&lt;/strong&gt;: TVL tracking, yield analysis, protocol comparisons&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Execution&lt;/strong&gt;: Runs analysis in E2B sandboxes with data persistence&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Perfect for: Token research, portfolio analysis, DeFi protocol evaluation, market trend analysis&lt;/p&gt; 
&lt;p&gt;All three agents demonstrate ROMA's recursive architecture in action, showing how complex queries that would overwhelm single-pass systems can be elegantly decomposed and solved. They serve as templates and inspiration for building your own specialized agents.&lt;/p&gt; 
&lt;h3&gt;Your First Agent in 5 Minutes&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;./setup.sh  # Automated setup with Docker or native installation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access all the pre-defined agents through the frontend on &lt;code&gt;localhost:3000&lt;/code&gt; after setting up the backend on &lt;code&gt;localhost:5000&lt;/code&gt;. Please checkout &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/SETUP.md"&gt;Setup&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/docs/AGENTS_GUIDE.md"&gt;Agents guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/agent_customization.png" alt="alt text" width="60%" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Your first agent in 3 lines
from sentientresearchagent import SentientAgent

agent = SentientAgent.create()
result = await agent.run("Create a podcast about AI safety")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìä Benchmarks&lt;/h2&gt; 
&lt;p&gt;We evaluate our simple implementation of a search system using ROMA, called ROMA-Search across three benchmarks: &lt;strong&gt;SEAL-0&lt;/strong&gt;, &lt;strong&gt;FRAMES&lt;/strong&gt;, and &lt;strong&gt;SimpleQA&lt;/strong&gt;.&lt;br /&gt; Below are the performance graphs for each benchmark.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/vtllms/sealqa"&gt;SEAL-0&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;SealQA is a new challenging benchmark for evaluating Search-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/seal-0-full.001.jpeg" alt="SEAL-0 Results" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://huggingface.co/datasets/google/frames-benchmark"&gt;FRAMES&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;A comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/FRAMES-full.001.jpeg" alt="FRAMES Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;a href="https://openai.com/index/introducing-simpleqa/"&gt;SimpleQA&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;View full results&lt;/summary&gt; 
 &lt;p&gt;Factuality benchmark that measures the ability for language models to answer short, fact-seeking questions.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sentient-agi/ROMA/main/assets/simpleQAFull.001.jpeg" alt="SimpleQA Results" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîÑ &lt;strong&gt;Recursive Task Decomposition&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Automatically breaks down complex tasks into manageable subtasks with intelligent dependency management. Runs independent sub-tasks in &lt;strong&gt;parallel&lt;/strong&gt;.&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ü§ñ &lt;strong&gt;Agent Agnostic&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Works with any provider (OpenAI, Anthropic, Google, local models) through unified interface, as long as it has an &lt;code&gt;agent.run()&lt;/code&gt; command, then you can use it!&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîç &lt;strong&gt;Complete Transparency&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;Stage tracing shows exactly what happens at each step - debug and optimize with full visibility&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîå Connect Any Tool&lt;/h3&gt; &lt;p&gt;Seamlessly integrate external tools and protocols with configurable intervention points. Already includes production-grade connectors such as E2B, file-read-write, and more.&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;This framework would not have been possible if it wasn't for these amazing open-source contributions!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inspired by the hierarchical planning approach described in &lt;a href="https://arxiv.org/abs/2503.08275"&gt;"Beyond Outlining: Heterogeneous Recursive Planning"&lt;/a&gt; by Xiong et al.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt; - Data validation using Python type annotations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/%5Bhttps://github.com/agno-ai/agno%5D(https://github.com/agno-agi/agno)"&gt;Agno&lt;/a&gt; - Framework for building AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/e2b-dev/e2b"&gt;E2B&lt;/a&gt; - Cloud runtime for AI agents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìö Citation&lt;/h2&gt; 
&lt;p&gt;If you use the ROMA repo in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{al_zubi_2025_17052592,
  author       = {Al-Zubi, Salah and
                  Nama, Baran and
                  Kaz, Arda and
                  Oh, Sewoong},
  title        = {SentientResearchAgent: A Hierarchical AI Agent
                   Framework for Research and Analysis
                  },
  month        = sep,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {ROMA},
  doi          = {10.5281/zenodo.17052592},
  url          = {https://doi.org/10.5281/zenodo.17052592},
  swhid        = {swh:1:dir:69cd1552103e0333dd0c39fc4f53cb03196017ce
                   ;origin=https://doi.org/10.5281/zenodo.17052591;vi
                   sit=swh:1:snp:f50bf99634f9876adb80c027361aec9dff97
                   3433;anchor=swh:1:rel:afa7caa843ce1279f5b4b29b5d3d
                   5e3fe85edc95;path=salzubi401-ROMA-b31c382
                  },
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üåü Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.star-history.com/#sentient-agi/roma&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=sentient-agi/roma&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://raw.githubusercontent.com/sentient-agi/ROMA/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>