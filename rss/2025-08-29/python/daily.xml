<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 28 Aug 2025 01:35:50 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>ai-to-ai/Auto-Gmail-Creator</title>
      <link>https://github.com/ai-to-ai/Auto-Gmail-Creator</link>
      <description>&lt;p&gt;Open Source Bulk Auto Gmail Creator Bot with Selenium &amp; Seleniumwire ( Python ). Feel free to contact me with Django/Flask, ML, AI, GPT, Automation, Scraping.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Auto-Gmail-Creator&lt;/h1&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;Latest Open Source Bulk Auto Google Account ( Gmail ) Regiteration Bot Script 2023&lt;/p&gt; 
&lt;h3&gt;Important Notice for already cloned/ forked users&lt;/h3&gt; 
&lt;p&gt;I developed stealth webdriver that avoid detection during the account creation. You can contact me to get it by giving me a coffee. btw, The only thing remain is to find cheap, robust sms providers as well as proxies.&lt;/p&gt; 
&lt;h3&gt;Introduction&lt;/h3&gt; 
&lt;p&gt;This is the automation script for Python lovers to learn basics about automation as well as commercial for marketers. &lt;a class="github-fork-ribbon right-top" href="https://github.com/ai-to-ai/Auto-Gmail-Creator/fork" data-ribbon="Fork me on GitHub" title="Fork me on GitHub"&gt;Fork me on GitHub&lt;/a&gt; &lt;img align="left" src="https://visitor-badge.laobi.icu/badge?page_id=ai-to-ai.ai-to-ai" /&gt;&lt;/p&gt; 
&lt;p&gt;According to &lt;a href="https://www.quora.com/profile/Jonathan-Elder"&gt;Jonathan&lt;/a&gt;'s desription ,only about five gmail addresses can be verified on a single phone number.&lt;/p&gt; 
&lt;p&gt;This script uses &lt;a href="https://sms-activate.org"&gt;sms-activate.org&lt;/a&gt; api for phone verification and more services will be added.&lt;/p&gt; 
&lt;p&gt;To run the script, you don't need to download Chromedriver or Geckodriver manually. The script does it automatically with webdriver manager.&lt;/p&gt; 
&lt;p&gt;You can customize this script to avoid getting blocked. I am also developing another script with Requests but going to keep it private since it will be blocked if I let it public.&lt;/p&gt; 
&lt;p&gt;Also, Feel free to contact me if you have any project regarding Automation, Scraping, Machine Learning.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Python 3.x.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;python app.py&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;'Created.txt' will be generated for successful creation.&lt;/li&gt; 
 &lt;li&gt;if you already installed python packages for my previous script, I recommend to upgrade the &lt;code&gt;webdriver-manager&lt;/code&gt; package by &lt;code&gt;pip install webdriver-manager -U&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Customize&lt;/h2&gt; 
&lt;h3&gt;Browser [ Chrome, Firefox ]&lt;/h3&gt; 
&lt;p&gt;Switch from Chrome to Firefox by commenting 2 lines.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;#options = ChromeOptions()
options = FirefoxOptions()

#driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options = options, seleniumwire_options=seleniumwire_options)

driver = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options = options, seleniumwire_options=seleniumwire_options)

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual or Automatic User info generation&lt;/h3&gt; 
&lt;p&gt;You edit the 'User.csv' with given type such as First name, Last name, Password, Birthday, Username(optional) from the second line. If the 5th parameter on user.csv is not passed by userBot generates username automatically adding FN + dot + LN + random 5 digits.(&lt;a href="mailto:john.doe12345@gmail.com"&gt;john.doe12345@gmail.com&lt;/a&gt;) Thanks to &lt;a href="https://github.com/BourneXu/AutoCreateGmailAccount"&gt;BourneXu&lt;/a&gt;, Script generates random popular usernames. You can set this variant as "True" to use this functionality to automate generation.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;AUTO_GENERATE_UERINFO = True
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Proxy&lt;/h3&gt; 
&lt;p&gt;If you want to use socks proxy, please remove comment theses lines. &lt;a href="http://free-proxy.cz/en/proxylist/country/all/socks5/ping/all/2"&gt;Free Proxy list&lt;/a&gt; is here&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    SOCKS_PROXY = "socks5://user:pass@ip:port"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Headless or With UI (Optional)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;    options.add_argument('--headless')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Profile (Optional)&lt;/h3&gt; 
&lt;p&gt;You can add your own profile if you want by specifying the path.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    options.add_argument("--incognito")
    options.add_argument(r"--user-data-dir=C:\\Users\\Username\\AppData\\Local\\Google\\Chrome\\User Data")
    options.add_argument(r'--profile-directory=ProfileName')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;To-Do&lt;/h2&gt; 
&lt;p&gt;Try to simulate user's mouse action wity pyautogui and adding cookie, recovery email, more sms services such as Durian, 5sims. To bypass bot-detection, I am all ears to hear from you.&lt;/p&gt; 
&lt;h2&gt;Images&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Running &lt;img src="https://raw.githubusercontent.com/ai-to-ai/Auto-Gmail-Creator/master/data/images/auto-gmail-creator-leostech.jpg" alt="auto-gmail-creator-leostech" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Edit user.csv&lt;/p&gt; &lt;p&gt;With Notepad &lt;img src="https://raw.githubusercontent.com/ai-to-ai/Auto-Gmail-Creator/master/data/images/user-notepad-leostech.jpg" alt="edit-user-notepad" /&gt;&lt;/p&gt; &lt;p&gt;With Excel &lt;img src="https://raw.githubusercontent.com/ai-to-ai/Auto-Gmail-Creator/master/data/images/user-excel-leostech.jpg" alt="edit-user-excel" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Bot will create chrome browser repeatedly for each gmail. &lt;img src="https://raw.githubusercontent.com/ai-to-ai/Auto-Gmail-Creator/master/data/images/gmail-create-leostech.jpg" alt="auto-gmail-create-leostech" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can visit &lt;a href="https://sms-activate.org"&gt;sms-activate.org&lt;/a&gt; to see it's apis. &lt;img src="https://raw.githubusercontent.com/ai-to-ai/Auto-Gmail-Creator/master/data/images/sms-leostech.jpg" alt="sms-activate" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;To see the country code, you can hit here. &lt;img src="https://raw.githubusercontent.com/ai-to-ai/Auto-Gmail-Creator/master/data/images/country-code-leostech.jpg" alt="auto-gmail-creator-leostech" /&gt; &lt;img src="https://raw.githubusercontent.com/ai-to-ai/Auto-Gmail-Creator/master/data/images/country-table-leostech.jpg" alt="auto-gmail-creator-leostech" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Telegram&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://t.me/autogmailcreator"&gt;https://t.me/autogmailcreator&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Discord&lt;/h2&gt; 
&lt;p&gt;leoshabit&lt;/p&gt; 
&lt;h2&gt;Skype&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://join.skype.com/invite/H6S0RFA69GNK"&gt;https://join.skype.com/invite/H6S0RFA69GNK&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Phone&lt;/h2&gt; 
&lt;p&gt;Americans can call me through this number. +13035365033&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-research/timesfm</title>
      <link>https://github.com/google-research/timesfm</link>
      <description>&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TimesFM&lt;/h1&gt; 
&lt;p&gt;TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2310.10688"&gt;A decoder-only foundation model for time-series forecasting&lt;/a&gt;, to appear in ICML 2024.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/"&gt;Google Research blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;Hugging Face release&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This repo contains the code to load public TimesFM checkpoints and run model inference. Please visit our &lt;a href="https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6"&gt;Hugging Face release&lt;/a&gt; to download model checkpoints.&lt;/p&gt; 
&lt;p&gt;This is not an officially supported Google product.&lt;/p&gt; 
&lt;p&gt;We recommend at least 32GB RAM to load TimesFM dependencies.&lt;/p&gt; 
&lt;h2&gt;Update - Dec. 30, 2024&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;We are launching a 500m checkpoint as a part of TimesFM-2.0 release. This new checkpoint can be upto 25% better than v1.0 on leading benchmarks and also has a 4 times longer max. context length.&lt;/li&gt; 
 &lt;li&gt;Launched &lt;a href="https://github.com/google-research/timesfm/raw/master/notebooks/finetuning.ipynb"&gt;finetuning support&lt;/a&gt; that lets you finetune the weights of the pretrained TimesFM model on your own data.&lt;/li&gt; 
 &lt;li&gt;Launched &lt;a href="https://github.com/google-research/timesfm/raw/master/notebooks/covariates.ipynb"&gt;~zero-shot covariate support&lt;/a&gt; with external regressors. More details &lt;a href="https://github.com/google-research/timesfm?tab=readme-ov-file#covariates-support"&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Update - Feb. 17, 2024&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;We are providing the option for &lt;a href="https://github.com/google-research/timesfm/raw/master/notebooks/finetuning_torch.ipynb"&gt;finetuning using Pytorch&lt;/a&gt;, which mimics the previously added functionality from &lt;a href="https://github.com/google-research/timesfm/raw/master/notebooks/finetuning.ipynb"&gt;finetuning support&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We are also providing the Multi-GPU finetuining with Pytorch. We currently support DDP multi-gpu finetuning, other variants of multi-gpu training (pipeline parallelism/model parallelism) might be added later. In order to use it, follow the steps in &lt;a href="https://github.com/google-research/timesfm/raw/master/finetuning/finetuning_example.py"&gt;finetuning example&lt;/a&gt; .&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Checkpoint timesfm-1.0-200m (-pytorch)&lt;/h2&gt; 
&lt;p&gt;timesfm-1.0-200m is our first open model checkpoint:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It performs univariate time series forecasting for context lengths up to 512 timepoints and any horizon lengths, with an optional frequency indicator.&lt;/li&gt; 
 &lt;li&gt;It focuses on point forecasts, and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Checkpoint timesfm-2.0-500m (-jax/-pytorch)&lt;/h2&gt; 
&lt;p&gt;timesfm-2.0-500m is our second open model checkpoint:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It performs univariate time series forecasting for context lengths up to 2048 timepoints and any horizon lengths, with an optional frequency indicator.&lt;/li&gt; 
 &lt;li&gt;It focuses on point forecasts. We experimentally offer 10 quantile heads but they have not been calibrated after pretraining.&lt;/li&gt; 
 &lt;li&gt;This new checkpoint can be upto 25% better than v1.0 on leading benchmarks and also has a 4 times longer max. context length.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarking&lt;/h2&gt; 
&lt;p&gt;TimesFM 2.0 has been added to &lt;a href="https://huggingface.co/spaces/Salesforce/GIFT-Eval"&gt;GIFT-Eval&lt;/a&gt; which is one of the most comprehensive time-series bechmarks available. It takes the top spot in terms of aggregated MASE and CRPS, where it is 6% better than the next best model in terms of aggregated MASE.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Local installation using poetry&lt;/h3&gt; 
&lt;p&gt;We will be using &lt;code&gt;pyenv&lt;/code&gt; and &lt;code&gt;poetry&lt;/code&gt;. In order to set these things up please follow the instructions &lt;a href="https://substack.com/home/post/p-148747960?r=28a5lx&amp;amp;utm_campaign=post&amp;amp;utm_medium=web"&gt;here&lt;/a&gt;. Note that the PAX (or JAX) version needs to run on python 3.10.x and the PyTorch version can run on &amp;gt;=3.11.x. Therefore make sure you have two versions of python installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pyenv install 3.10
pyenv install 3.11
pyenv versions # to list the versions available (lets assume the versions are 3.10.15 and 3.11.10)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For PAX version installation do the following.&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pyenv local 3.10.15
poetry env use 3.10.15
poetry lock
poetry install -E  pax
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After than you can run the timesfm under &lt;code&gt;poetry shell&lt;/code&gt; or do &lt;code&gt;poetry run python3 ...&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;For PyTorch version installation do the following.&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pyenv local 3.11.10
poetry env use 3.11.10
poetry lock
poetry install -E  torch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After than you can run the timesfm under &lt;code&gt;poetry shell&lt;/code&gt; or do &lt;code&gt;poetry run python3 ...&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Additional Note&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;If you plan to use the &lt;strong&gt;&lt;code&gt;forecast_with_covariates&lt;/code&gt;&lt;/strong&gt; function (which requires external regressors), you need to install &lt;strong&gt;JAX&lt;/strong&gt; and &lt;strong&gt;jaxlib&lt;/strong&gt;. If you installed the base version of TimesFM (&lt;code&gt;torch&lt;/code&gt;), you must manually install the dependencies for &lt;strong&gt;&lt;code&gt;forecast_with_covariates&lt;/code&gt;&lt;/strong&gt; support:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install jax jaxlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Why is this needed?&lt;/strong&gt;&lt;br /&gt; The &lt;code&gt;forecast_with_covariates&lt;/code&gt; method relies on the &lt;code&gt;xreg_lib&lt;/code&gt; module, which depends on JAX and jaxlib. If these packages are not installed, calling &lt;code&gt;forecast_with_covariates&lt;/code&gt; will raise an error. However, due to a lazy import mechanism, &lt;code&gt;xreg_lib&lt;/code&gt; (and hence JAX/jaxlib) is not needed for standard &lt;code&gt;forecast&lt;/code&gt; calls.&lt;/p&gt; 
&lt;h3&gt;Notes&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Running the provided benchmarks would require additional dependencies. Please see the &lt;code&gt;experiments&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The dependency &lt;code&gt;lingvo&lt;/code&gt; does not support ARM architectures, and the code is not working for machines with Apple silicon. We are aware of this issue and are working on a solution. Stay tuned.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install from PyPI (and publish)&lt;/h3&gt; 
&lt;p&gt;On python 3.11 you can install the torch version using:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install timesfm[torch]&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;On python 3.10 you can install the pax version using:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install timesfm[pax]&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Initialize the model and load a checkpoint.&lt;/h3&gt; 
&lt;p&gt;Then the base class can be loaded as,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import timesfm

# Loading the timesfm-2.0 checkpoint:
# For PAX
tfm = timesfm.TimesFm(
      hparams=timesfm.TimesFmHparams(
          backend="gpu",
          per_core_batch_size=32,
          horizon_len=128,
          num_layers=50,
          context_len=2048,

          use_positional_embedding=False,
      ),
      checkpoint=timesfm.TimesFmCheckpoint(
          huggingface_repo_id="google/timesfm-2.0-500m-jax"),
  )

# For Torch
tfm = timesfm.TimesFm(
      hparams=timesfm.TimesFmHparams(
          backend="gpu",
          per_core_batch_size=32,
          horizon_len=128,
          num_layers=50,
          use_positional_embedding=False,
          context_len=2048,
      ),
      checkpoint=timesfm.TimesFmCheckpoint(
          huggingface_repo_id="google/timesfm-2.0-500m-pytorch"),
  )

# Loading the timesfm-1.0 checkpoint:
# For PAX
tfm = timesfm.TimesFm(
      hparams=timesfm.TimesFmHparams(
          backend="gpu",
          per_core_batch_size=32,
          horizon_len=128,
      ),
      checkpoint=timesfm.TimesFmCheckpoint(
          huggingface_repo_id="google/timesfm-1.0-200m"),
  )

# For Torch
tfm = timesfm.TimesFm(
      hparams=timesfm.TimesFmHparams(
          backend="gpu",
          per_core_batch_size=32,
          horizon_len=128,
      ),
      checkpoint=timesfm.TimesFmCheckpoint(
          huggingface_repo_id="google/timesfm-1.0-200m-pytorch"),
  )
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note some of the parameters are fixed to load the 200m and 500m models&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;The &lt;code&gt;context_len&lt;/code&gt; in &lt;code&gt;hparams&lt;/code&gt; here can be set as the max context length &lt;strong&gt;of the model&lt;/strong&gt; (a maximum of 2048 for 2.0 models and 512 for 1.0 models). &lt;strong&gt;It needs to be a multiplier of &lt;code&gt;input_patch_len&lt;/code&gt;, i.e. a multiplier of 32.&lt;/strong&gt; You can provide a shorter series to the &lt;code&gt;tfm.forecast()&lt;/code&gt; function and the model will handle it. The input time series can have &lt;strong&gt;any context length&lt;/strong&gt;. Padding / truncation will be handled by the inference code if needed.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length &amp;lt;= context length but it is not a requirement in the function call.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;backend&lt;/code&gt; is one of "cpu", "gpu", case sensitive.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Perform inference&lt;/h3&gt; 
&lt;p&gt;We provide APIs to forecast from either array inputs or &lt;code&gt;pandas&lt;/code&gt; dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions &lt;code&gt;tfm.forecast()&lt;/code&gt; and &lt;code&gt;tfm.forecast_on_df()&lt;/code&gt; for detailed instructions.&lt;/p&gt; 
&lt;p&gt;In particular regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;0&lt;/strong&gt; (default): high frequency, long horizon time series. We recommend using this for time series up to daily granularity.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;1&lt;/strong&gt;: medium frequency time series. We recommend using this for weekly and monthly data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2&lt;/strong&gt;: low frequency, short horizon time series. We recommend using this for anything beyond monthly, e.g. quarterly or yearly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;0&lt;/strong&gt;: T, MIN, H, D, B, U&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;1&lt;/strong&gt;: W, M&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2&lt;/strong&gt;: Q, Y&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Notice you do &lt;strong&gt;NOT&lt;/strong&gt; have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.&lt;/p&gt; 
&lt;p&gt;Examples:&lt;/p&gt; 
&lt;p&gt;Array inputs, with the frequencies set to low, medium and high respectively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
forecast_input = [
    np.sin(np.linspace(0, 20, 100)),
    np.sin(np.linspace(0, 20, 200)),
    np.sin(np.linspace(0, 20, 400)),
]
frequency_input = [0, 1, 2]

point_forecast, experimental_quantile_forecast = tfm.forecast(
    forecast_input,
    freq=frequency_input,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;pandas&lt;/code&gt; dataframe, with the frequency set to "M" monthly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd

# e.g. input_df is
#       unique_id  ds          y
# 0     T1         1975-12-31  697458.0
# 1     T1         1976-01-31  1187650.0
# 2     T1         1976-02-29  1069690.0
# 3     T1         1976-03-31  1078430.0
# 4     T1         1976-04-30  1059910.0
# ...   ...        ...         ...
# 8175  T99        1986-01-31  602.0
# 8176  T99        1986-02-28  684.0
# 8177  T99        1986-03-31  818.0
# 8178  T99        1986-04-30  836.0
# 8179  T99        1986-05-31  878.0

forecast_df = tfm.forecast_on_df(
    inputs=input_df,
    freq="M",  # monthly
    value_name="y",
    num_jobs=-1,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Covariates Support&lt;/h2&gt; 
&lt;p&gt;We now have an external regressors library on top of TimesFM that can support static covariates as well as dynamic covariates available in the future. We have an usage example in &lt;a href="https://github.com/google-research/timesfm/raw/master/notebooks/covariates.ipynb"&gt;notebooks/covariates.ipynb&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you plan to use the &lt;strong&gt;&lt;code&gt;forecast_with_covariates&lt;/code&gt;&lt;/strong&gt; on timesfm &lt;code&gt;torch&lt;/code&gt; version, you need to install &lt;strong&gt;JAX&lt;/strong&gt; and &lt;strong&gt;jaxlib&lt;/strong&gt;. You must manually install the dependencies for &lt;strong&gt;&lt;code&gt;forecast_with_covariates&lt;/code&gt;&lt;/strong&gt; support:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install jax jaxlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Let's take a toy example of forecasting sales for a grocery store:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt; Given the observed the daily sales of this week (7 days), forecast the daily sales of next week (7 days).&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Product: ice cream
Daily_sales: [30, 30, 4, 5, 7, 8, 10]
Category: food
Base_price: 1.99
Weekday: [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6]
Has_promotion: [Yes, Yes, No, No, No, Yes, Yes, No, No, No, No, No, No, No]
Daily_temperature: [31.0, 24.3, 19.4, 26.2, 24.6, 30.0, 31.1, 32.4, 30.9, 26.0, 25.0, 27.8, 29.5, 31.2]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Product: sunscreen
Daily_sales: [5, 7, 12, 13, 5, 6, 10]
Category: skin product
Base_price: 29.99
Weekday: [0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6]
Has_promotion: [No, No, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes]
Daily_temperature: [31.0, 24.3, 19.4, 26.2, 24.6, 30.0, 31.1, 32.4, 30.9, 26.0, 25.0, 27.8, 29.5, 31.2]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this example, besides the &lt;code&gt;Daily_sales&lt;/code&gt;, we also have covariates &lt;code&gt;Category&lt;/code&gt;, &lt;code&gt;Base_price&lt;/code&gt;, &lt;code&gt;Weekday&lt;/code&gt;, &lt;code&gt;Has_promotion&lt;/code&gt;, &lt;code&gt;Daily_temperature&lt;/code&gt;. Let's introduce some concepts:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Static covariates&lt;/strong&gt; are covariates for each time series.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In our example, &lt;code&gt;Category&lt;/code&gt; is a &lt;strong&gt;static categorical covariate&lt;/strong&gt;,&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Base_price&lt;/code&gt; is a &lt;strong&gt;static numerical covariates&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Dynamic covariates&lt;/strong&gt; are covaraites for each time stamps.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Date / time related features can be usually treated as dynamic covariates.&lt;/li&gt; 
 &lt;li&gt;In our example, &lt;code&gt;Weekday&lt;/code&gt; and &lt;code&gt;Has_promotion&lt;/code&gt; are &lt;strong&gt;dynamic categorical covariates&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Daily_temperate&lt;/code&gt; is a &lt;strong&gt;dynamic numerical covariate&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Notice:&lt;/strong&gt; Here we make it mandatory that the dynamic covariates need to cover both the forecasting context and horizon. For example, all dynamic covariates in the example have 14 values: the first 7 correspond to the observed 7 days, and the last 7 correspond to the next 7 days.&lt;/p&gt; 
&lt;p&gt;We can now provide the past data of the two products along with static and dynamic covariates as a batch input to TimesFM and produce forecasts that take into the account the covariates. To learn more, check out the example in &lt;a href="https://github.com/google-research/timesfm/raw/master/notebooks/covariates.ipynb"&gt;notebooks/covariates.ipynb&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Finetuning&lt;/h2&gt; 
&lt;p&gt;We have provided an example of finetuning the model on a new dataset in &lt;a href="https://github.com/google-research/timesfm/raw/master/notebooks/finetuning.ipynb"&gt;notebooks/finetuning.ipynb&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribution Style guide&lt;/h2&gt; 
&lt;p&gt;If you would like to submit a PR please make sure that you use our formatting style. We use &lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; for formatting with the following options,&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[style]
based_on_style = google
# Add your custom style rules here
indent_width = 2
spaces_before_comment = 2

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please run &lt;code&gt;yapf --in-place --recursive &amp;lt;filename&amp;gt;&lt;/code&gt; on all affected files.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>santinic/audiblez</title>
      <link>https://github.com/santinic/audiblez</link>
      <description>&lt;p&gt;Generate audiobooks from e-books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Audiblez: Generate audiobooks from e-books&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/santinic/audiblez/actions/workflows/pip-install.yaml"&gt;&lt;img src="https://github.com/santinic/audiblez/actions/workflows/pip-install.yaml/badge.svg?sanitize=true" alt="Installing via pip and running" /&gt;&lt;/a&gt; &lt;a href="https://github.com/santinic/audiblez/actions/workflows/git-clone-and-run.yml"&gt;&lt;img src="https://github.com/santinic/audiblez/actions/workflows/git-clone-and-run.yml/badge.svg?sanitize=true" alt="Git clone and run" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/audiblez" alt="PyPI - Python Version" /&gt; &lt;img src="https://img.shields.io/pypi/v/audiblez" alt="PyPI - Version" /&gt;&lt;/p&gt; 
&lt;h3&gt;v4 Now with Graphical interface, CUDA support, and many languages!&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/santinic/audiblez/main/imgs/mac.png" alt="Audiblez GUI on MacOSX" /&gt;&lt;/p&gt; 
&lt;p&gt;Audiblez generates &lt;code&gt;.m4b&lt;/code&gt; audiobooks from regular &lt;code&gt;.epub&lt;/code&gt; e-books, using Kokoro's high-quality speech synthesis.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt; is a recently published text-to-speech model with just 82M params and very natural sounding output. It's released under Apache licence and it was trained on &amp;lt; 100 hours of audio. It currently supports these languages: üá∫üá∏ üá¨üáß üá™üá∏ üá´üá∑ üáÆüá≥ üáÆüáπ üáØüáµ üáßüá∑ üá®üá≥&lt;/p&gt; 
&lt;p&gt;On a Google Colab's T4 GPU via Cuda, &lt;strong&gt;it takes about 5 minutes to convert "Animal's Farm" by Orwell&lt;/strong&gt; (which is about 160,000 characters) to audiobook, at a rate of about 600 characters per second.&lt;/p&gt; 
&lt;p&gt;On my M2 MacBook Pro, on CPU, it takes about 1 hour, at a rate of about 60 characters per second.&lt;/p&gt; 
&lt;h2&gt;How to install the Command Line tool&lt;/h2&gt; 
&lt;p&gt;If you have Python 3 on your computer, you can install it with pip. You also need &lt;code&gt;espeak-ng&lt;/code&gt; and &lt;code&gt;ffmpeg&lt;/code&gt; installed on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install ffmpeg espeak-ng                   # on Ubuntu/Debian üêß
pip install audiblez
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install ffmpeg espeak-ng                       # on Mac üçè
pip install audiblez
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can convert an .epub directly with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;audiblez book.epub -v af_sky
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will first create a bunch of &lt;code&gt;book_chapter_1.wav&lt;/code&gt;, &lt;code&gt;book_chapter_2.wav&lt;/code&gt;, etc. files in the same directory, and at the end it will produce a &lt;code&gt;book.m4b&lt;/code&gt; file with the whole book you can listen with VLC or any audiobook player. It will only produce the &lt;code&gt;.m4b&lt;/code&gt; file if you have &lt;code&gt;ffmpeg&lt;/code&gt; installed on your machine.&lt;/p&gt; 
&lt;h2&gt;How to run the GUI&lt;/h2&gt; 
&lt;p&gt;The GUI is a simple graphical interface to use audiblez. You need some extra dependencies to run the GUI:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt install ffmpeg espeak-ng 
sudo apt install libgtk-3-dev        # just for Ubuntu/Debian üêß, Windows/Mac don't need this
  
pip install audiblez pillow wxpython
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can run the GUI with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;audiblez-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to run on Windows&lt;/h2&gt; 
&lt;p&gt;After many trials, on Windows we recommend to install audiblez in a Python venv:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open a Windows terminal&lt;/li&gt; 
 &lt;li&gt;Create anew folder: &lt;code&gt;mkdir audiblez&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Enter the folder: &lt;code&gt;cd audiblez&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Create a venv: &lt;code&gt;python -m venv venv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Activate the venv: &lt;code&gt;.\venv\Scripts\Activate.ps1&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install the dependencies: &lt;code&gt;pip install audiblez pillow wxpython&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Now you can run &lt;code&gt;audiblez&lt;/code&gt; or &lt;code&gt;audiblez-ui&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For Cuda support, you need to install Pytorch accordingly: &lt;a href="https://pytorch.org/get-started/locally/"&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Speed&lt;/h2&gt; 
&lt;p&gt;By default the audio is generated using a normal speed, but you can make it up to twice slower or faster by specifying a speed argument between 0.5 to 2.0:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;audiblez book.epub -v af_sky -s 1.5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Voices&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;-v&lt;/code&gt; option to specify the voice to use. Available voices are listed here. The first letter is the language code and the second is the gender of the speaker e.g. &lt;code&gt;im_nicola&lt;/code&gt; is an italian male voice.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://claudio.uk/posts/audiblez-v4.html"&gt;For hearing samples of Kokoro-82M voices, go here&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Voices&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá∫üá∏ American English&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;af_alloy&lt;/code&gt;, &lt;code&gt;af_aoede&lt;/code&gt;, &lt;code&gt;af_bella&lt;/code&gt;, &lt;code&gt;af_heart&lt;/code&gt;, &lt;code&gt;af_jessica&lt;/code&gt;, &lt;code&gt;af_kore&lt;/code&gt;, &lt;code&gt;af_nicole&lt;/code&gt;, &lt;code&gt;af_nova&lt;/code&gt;, &lt;code&gt;af_river&lt;/code&gt;, &lt;code&gt;af_sarah&lt;/code&gt;, &lt;code&gt;af_sky&lt;/code&gt;, &lt;code&gt;am_adam&lt;/code&gt;, &lt;code&gt;am_echo&lt;/code&gt;, &lt;code&gt;am_eric&lt;/code&gt;, &lt;code&gt;am_fenrir&lt;/code&gt;, &lt;code&gt;am_liam&lt;/code&gt;, &lt;code&gt;am_michael&lt;/code&gt;, &lt;code&gt;am_onyx&lt;/code&gt;, &lt;code&gt;am_puck&lt;/code&gt;, &lt;code&gt;am_santa&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá¨üáß British English&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;bf_alice&lt;/code&gt;, &lt;code&gt;bf_emma&lt;/code&gt;, &lt;code&gt;bf_isabella&lt;/code&gt;, &lt;code&gt;bf_lily&lt;/code&gt;, &lt;code&gt;bm_daniel&lt;/code&gt;, &lt;code&gt;bm_fable&lt;/code&gt;, &lt;code&gt;bm_george&lt;/code&gt;, &lt;code&gt;bm_lewis&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá™üá∏ Spanish&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ef_dora&lt;/code&gt;, &lt;code&gt;em_alex&lt;/code&gt;, &lt;code&gt;em_santa&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá´üá∑ French&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ff_siwis&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üáÆüá≥ Hindi&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;hf_alpha&lt;/code&gt;, &lt;code&gt;hf_beta&lt;/code&gt;, &lt;code&gt;hm_omega&lt;/code&gt;, &lt;code&gt;hm_psi&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üáÆüáπ Italian&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;if_sara&lt;/code&gt;, &lt;code&gt;im_nicola&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üáØüáµ Japanese&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jf_alpha&lt;/code&gt;, &lt;code&gt;jf_gongitsune&lt;/code&gt;, &lt;code&gt;jf_nezumi&lt;/code&gt;, &lt;code&gt;jf_tebukuro&lt;/code&gt;, &lt;code&gt;jm_kumo&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üáßüá∑ Brazilian Portuguese&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pf_dora&lt;/code&gt;, &lt;code&gt;pm_alex&lt;/code&gt;, &lt;code&gt;pm_santa&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá®üá≥ Mandarin Chinese&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;zf_xiaobei&lt;/code&gt;, &lt;code&gt;zf_xiaoni&lt;/code&gt;, &lt;code&gt;zf_xiaoxiao&lt;/code&gt;, &lt;code&gt;zf_xiaoyi&lt;/code&gt;, &lt;code&gt;zm_yunjian&lt;/code&gt;, &lt;code&gt;zm_yunxi&lt;/code&gt;, &lt;code&gt;zm_yunxia&lt;/code&gt;, &lt;code&gt;zm_yunyang&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more detaila about voice quality, check this document: &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md"&gt;Kokoro-82M voices&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to run on GPU&lt;/h2&gt; 
&lt;p&gt;By default, audiblez runs on CPU. If you pass the option &lt;code&gt;--cuda&lt;/code&gt; it will try to use the Cuda device via Torch.&lt;/p&gt; 
&lt;p&gt;Check out this example: &lt;a href="https://colab.research.google.com/drive/164PQLowogprWQpRjKk33e-8IORAvqXKI?usp=sharing%5D"&gt;Audiblez running on a Google Colab Notebook with Cuda &lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We don't currently support Apple Silicon, as there is not yet a Kokoro implementation in MLX. As soon as it will be available, we will support it.&lt;/p&gt; 
&lt;h2&gt;Manually pick chapters to convert&lt;/h2&gt; 
&lt;p&gt;Sometimes you want to manually select which chapters/sections in the e-book to read out loud. To do so, you can use &lt;code&gt;--pick&lt;/code&gt; to interactively choose the chapters to convert (without running the GUI).&lt;/p&gt; 
&lt;h2&gt;Help page&lt;/h2&gt; 
&lt;p&gt;For all the options available, you can check the help page &lt;code&gt;audiblez --help&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: audiblez [-h] [-v VOICE] [-p] [-s SPEED] [-c] [-o FOLDER] epub_file_path

positional arguments:
  epub_file_path        Path to the epub file

options:
  -h, --help            show this help message and exit
  -v VOICE, --voice VOICE
                        Choose narrating voice: a, b, e, f, h, i, j, p, z
  -p, --pick            Interactively select which chapters to read in the audiobook
  -s SPEED, --speed SPEED
                        Set speed from 0.5 to 2.0
  -c, --cuda            Use GPU via Cuda in Torch if available
  -o FOLDER, --output FOLDER
                        Output folder for the audiobook and temporary files

example:
  audiblez book.epub -l en-us -v af_sky

to use the GUI, run:
  audiblez-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Author&lt;/h2&gt; 
&lt;p&gt;by &lt;a href="https://claudio.uk"&gt;Claudio Santini&lt;/a&gt; in 2025, distributed under MIT licence.&lt;/p&gt; 
&lt;p&gt;Related Article: &lt;a href="https://claudio.uk/posts/audiblez-v4.html"&gt;Audiblez v4: Generate Audiobooks from E-books&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mlflow/mlflow</title>
      <link>https://github.com/mlflow/mlflow</link>
      <description>&lt;p&gt;The open source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center" style="border-bottom: none"&gt; &lt;a href="https://mlflow.org/"&gt; &lt;img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg?sanitize=true" width="200" /&gt; &lt;/a&gt; &lt;/h1&gt; 
&lt;h2 align="center" style="border-bottom: none"&gt;Open-Source Platform for Productionizing AI&lt;/h2&gt; 
&lt;p&gt;MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end &lt;strong&gt;experiment tracking&lt;/strong&gt;, &lt;strong&gt;observability&lt;/strong&gt;, and &lt;strong&gt;evaluations&lt;/strong&gt;, all in one integrated platform.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/mlflow/"&gt;&lt;img src="https://img.shields.io/pypi/v/mlflow" alt="Python SDK" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/mlflow"&gt;&lt;img src="https://img.shields.io/pypi/dm/mlflow" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mlflow/mlflow/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/mlflow/mlflow" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank"&gt; &lt;img src="https://img.shields.io/twitter/follow/mlflow?logo=X&amp;amp;color=%20%23f5f5f5" alt="follow on X(Twitter)" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/mlflow-org/" target="_blank"&gt; &lt;img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&amp;amp;logoColor=fff" alt="follow on LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/mlflow/mlflow"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div&gt; 
  &lt;a href="https://mlflow.org/"&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://mlflow.org/docs/latest/index.html"&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://github.com/mlflow/mlflow/issues/new/choose"&gt;&lt;strong&gt;Feature Request&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://mlflow.org/blog"&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://www.youtube.com/@mlflowoss"&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/a&gt; ¬∑ 
  &lt;a href="https://lu.ma/mlflow?k=c"&gt;&lt;strong&gt;Events&lt;/strong&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;üöÄ Installation&lt;/h2&gt; 
&lt;p&gt;To install the MLflow Python package, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install mlflow
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üì¶ Core Components&lt;/h2&gt; 
&lt;p&gt;MLflow is &lt;strong&gt;the only platform that provides a unified solution for all your AI/ML needs&lt;/strong&gt;, including LLMs, Agents, Deep Learning, and traditional machine learning.&lt;/p&gt; 
&lt;h3&gt;üí° For LLM / GenAI Developers&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width="100%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/llms/tracing/index.html"&gt;&lt;strong&gt;üîç Tracing / Observability&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/python-openai/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width="100%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/eval-monitor/"&gt;&lt;strong&gt;üìä LLM Evaluation&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/eval-monitor/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"&gt;&lt;strong&gt;ü§ñ Prompt Management&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"&gt;&lt;strong&gt;üì¶ App Version Tracking&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;üéì For Data Scientists&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td colspan="2" align="center"&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width="50%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/tracking/"&gt;&lt;strong&gt;üìù Experiment Tracking&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
      Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width="100%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/model-registry/"&gt;&lt;strong&gt;üíæ Model Registry&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
       A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width="100%" /&gt; 
    &lt;div align="center"&gt; 
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/deployment/"&gt;&lt;strong&gt;üöÄ Deployment&lt;/strong&gt;&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
     &lt;div&gt;
       Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.
     &lt;/div&gt;
     &lt;br /&gt; 
     &lt;a href="https://mlflow.org/docs/latest/ml/deployment/"&gt;Getting Started ‚Üí&lt;/a&gt; 
     &lt;br /&gt;
     &lt;br /&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üåê Hosting MLflow Anywhere&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width="100%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.&lt;/p&gt; 
&lt;p&gt;Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/sagemaker-ai/experiments/"&gt;Amazon SageMaker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2"&gt;Azure ML&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.databricks.com/product/managed-mlflow"&gt;Databricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nebius.com/services/managed-mlflow"&gt;Nebius&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For hosting MLflow on your own infrastructure, please refer to &lt;a href="https://mlflow.org/docs/latest/ml/tracking/#tracking-setup"&gt;this guidance&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üó£Ô∏è Supported Programming Languages&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/mlflow/"&gt;Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.npmjs.com/package/mlflow-tracing"&gt;TypeScript / JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mvnrepository.com/artifact/org.mlflow/mlflow-client"&gt;Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cran.r-project.org/web/packages/mlflow/readme/README.html"&gt;R&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîó Integrations&lt;/h2&gt; 
&lt;p&gt;MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png" alt="Integrations" /&gt;&lt;/p&gt; 
&lt;h2&gt;Usage Examples&lt;/h2&gt; 
&lt;h3&gt;Experiment Tracking (&lt;a href="https://mlflow.org/docs/latest/ml/tracking/"&gt;Doc&lt;/a&gt;)&lt;/h3&gt; 
&lt;p&gt;The following examples trains a simple regression model with scikit-learn, while enabling MLflow's &lt;a href="https://mlflow.org/docs/latest/tracking/autolog.html"&gt;autologging&lt;/a&gt; feature for experiment tracking.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow's automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow &lt;strong&gt;Run&lt;/strong&gt; should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlflow ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Evaluating Models (&lt;a href="https://mlflow.org/docs/latest/model-evaluation/index.html"&gt;Doc&lt;/a&gt;)&lt;/h3&gt; 
&lt;p&gt;The following example runs automatic evaluation for question-answering tasks with several built-in metrics.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import mlflow
import pandas as pd

# Evaluation set contains (1) input question (2) model outputs (3) ground truth
df = pd.DataFrame(
    {
        "inputs": ["What is MLflow?", "What is Spark?"],
        "outputs": [
            "MLflow is an innovative fully self-driving airship powered by AI.",
            "Sparks is an American pop and rock duo formed in Los Angeles.",
        ],
        "ground_truth": [
            "MLflow is an open-source platform for productionizing AI.",
            "Apache Spark is an open-source, distributed computing system.",
        ],
    }
)
eval_dataset = mlflow.data.from_pandas(
    df, predictions="outputs", targets="ground_truth"
)

# Start an MLflow Run to record the evaluation results to
with mlflow.start_run(run_name="evaluate_qa"):
    # Run automatic evaluation with a set of built-in metrics for question-answering models
    results = mlflow.evaluate(
        data=eval_dataset,
        model_type="question-answering",
    )

print(results.tables["eval_results_table"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Observability (&lt;a href="https://mlflow.org/docs/latest/llms/tracing/index.html"&gt;Doc&lt;/a&gt;)&lt;/h3&gt; 
&lt;p&gt;MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call &lt;code&gt;mlflow.xyz.autolog()&lt;/code&gt; before running your models. Refer to the documentation for customization and manual instrumentation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hi!"}],
    temperature=0.1,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.&lt;/p&gt; 
&lt;h2&gt;üí≠ Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For help or questions about MLflow usage (e.g. "how do I do X?") visit the &lt;a href="https://mlflow.org/docs/latest/index.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;In the documentation, you can ask the question to our AI-powered chat bot. Click on the &lt;strong&gt;"Ask AI"&lt;/strong&gt; button at the right bottom.&lt;/li&gt; 
 &lt;li&gt;Join the &lt;a href="https://lu.ma/mlflow?k=c"&gt;virtual events&lt;/a&gt; like office hours and meetups.&lt;/li&gt; 
 &lt;li&gt;To report a bug, file a documentation issue, or submit a feature request, please &lt;a href="https://github.com/mlflow/mlflow/issues/new/choose"&gt;open a GitHub issue&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For release announcements and other discussions, please subscribe to our mailing list (&lt;a href="mailto:mlflow-users@googlegroups.com"&gt;mlflow-users@googlegroups.com&lt;/a&gt;) or join us on &lt;a href="https://mlflow.org/slack"&gt;Slack&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We happily welcome contributions to MLflow!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Submit &lt;a href="https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml"&gt;bug reports&lt;/a&gt; and &lt;a href="https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml"&gt;feature requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Contribute for &lt;a href="https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;good-first-issues&lt;/a&gt; and &lt;a href="https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22"&gt;help-wanted&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Writing about MLflow and sharing your experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please see our &lt;a href="https://raw.githubusercontent.com/mlflow/mlflow/master/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; to learn more about contributing to MLflow.&lt;/p&gt; 
&lt;h2&gt;‚≠êÔ∏è Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#mlflow/mlflow&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;‚úèÔ∏è Citation&lt;/h2&gt; 
&lt;p&gt;If you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the &lt;a href="https://github.com/mlflow/mlflow"&gt;GitHub repository page&lt;/a&gt;, which will provide you with citation formats including APA and BibTeX.&lt;/p&gt; 
&lt;h2&gt;üë• Core Members&lt;/h2&gt; 
&lt;p&gt;MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BenWilson2"&gt;Ben Wilson&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dbczumar"&gt;Corey Zumar&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/daniellok-db"&gt;Daniel Lok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gabrielfu"&gt;Gabriel Fu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/harupy"&gt;Harutaka Kawamura&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/serena-ruan"&gt;Serena Ruan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TomeHirata"&gt;Tomu Hirata&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/WeichenXu123"&gt;Weichen Xu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/B-Step62"&gt;Yuki Watanabe&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>swisskyrepo/PayloadsAllTheThings</title>
      <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
      <description>&lt;p&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Payloads All The Things&lt;/h1&gt; 
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security. Feel free to improve with your payloads and techniques !&lt;/p&gt; 
&lt;p&gt;You can also contribute with a &lt;span&gt;üçª&lt;/span&gt; IRL, or using the sponsor button.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/swisskyrepo"&gt;&lt;img src="https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;link=https://github.com/sponsors/swisskyrepo" alt="Sponsor" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/"&gt;&lt;img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An alternative display version is available at &lt;a href="https://swisskyrepo.github.io/PayloadsAllTheThings/"&gt;PayloadsAllTheThingsWeb&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png" alt="banner" /&gt; &lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; Documentation&lt;/h2&gt; 
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;README.md - vulnerability description and how to exploit it, including several payloads&lt;/li&gt; 
 &lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt; 
 &lt;li&gt;Images - pictures for the README.md&lt;/li&gt; 
 &lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might also like the other projects from the AllTheThings family :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/InternalAllTheThings/"&gt;InternalAllTheThings&lt;/a&gt; - Active Directory and Internal Pentest Cheatsheets&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/HardwareAllTheThings/"&gt;HardwareAllTheThings&lt;/a&gt; - Hardware/IOT Pentesting Wiki&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You want more ? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/YOUTUBE.md"&gt;Youtube channel&lt;/a&gt; selections.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üßëüíª&lt;/span&gt; Contributions&lt;/h2&gt; 
&lt;p&gt;Be sure to read &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;amp;max=36" alt="sponsors-list" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Thanks again for your contribution! &lt;span&gt;‚ù§Ô∏è&lt;/span&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üçª&lt;/span&gt; Sponsors&lt;/h2&gt; 
&lt;p&gt;This project is proudly sponsored by these companies.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Logo&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://serpapi.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/34724717?s=40&amp;amp;v=4" alt="sponsor-serpapi" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SerpApi&lt;/strong&gt; is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://projectdiscovery.io/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50994705?s=40&amp;amp;v=4" alt="sponsor-projectdiscovery" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ProjectDiscovery&lt;/strong&gt; - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.vaadata.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/48131541?s=40&amp;amp;v=4" alt="sponsor-vaadata" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;VAADATA&lt;/strong&gt; - Ethical Hacking Services&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>OpenPipe/ART</title>
      <link>https://github.com/OpenPipe/ART</link>
      <description>&lt;p&gt;Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://art.openpipe.ai"&gt;
   &lt;picture&gt; 
    &lt;img alt="ART logo" src="https://github.com/openpipe/art/raw/main/assets/ART_logo.png" width="160px" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Train multi-step agents for real-world tasks using GRPO. &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/openpipe/art/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true" alt="PRs-Welcome" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/openpipe-art/"&gt;&lt;img src="https://img.shields.io/pypi/v/openpipe-art?color=364fc7" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Train Agent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/zbBHRUpwf4"&gt;&lt;img src="https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://art.openpipe.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-orange?style=plastic&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìè RULER: Zero-Shot Agent Rewards&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;RULER&lt;/strong&gt; (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the rest‚Äî&lt;strong&gt;no labeled data, expert feedback, or reward engineering required&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;‚ú® &lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2-3x faster development&lt;/strong&gt; - Skip reward function engineering entirely&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;General-purpose&lt;/strong&gt; - Works across any task without modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strong performance&lt;/strong&gt; - Matches or exceeds hand-crafted rewards in 3/4 benchmarks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy integration&lt;/strong&gt; - Drop-in replacement for manual reward functions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, "openai/o3")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://art.openpipe.ai/fundamentals/ruler"&gt;üìñ Learn more about RULER ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ART Overview&lt;/h2&gt; 
&lt;p&gt;ART is an open-source RL framework that improves agent reliability by allowing LLMs to &lt;strong&gt;learn from experience&lt;/strong&gt;. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the &lt;a href="https://art.openpipe.ai"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìí Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Task&lt;/th&gt; 
   &lt;th&gt;Example Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Comparative Performance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ART‚Ä¢E LangGraph&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using LangGraph&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP‚Ä¢RL&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B masters the NWS MCP server&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ART‚Ä¢E [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using RULER&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/dev/art-e/art_e/evaluate/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2048&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play 2048&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/2048/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Clue&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to solve Temporal Clue&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tic Tac Toe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Tic Tac Toe&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/tic_tac_toe/display-benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codenames&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Codenames&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png" height="72" /&gt; &lt;a href="https://github.com/OpenPipe/art-notebooks/raw/main/examples/codenames/Codenames_RL.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AutoRL [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb"&gt;üèãÔ∏è Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Train Qwen 2.5 7B to master any task&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üì∞ ART News&lt;/h2&gt; 
&lt;p&gt;Explore our latest research and updates on building SOTA agents.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://art.openpipe.ai/integrations/langgraph-integration"&gt;ART now integrates seamlessly with LangGraph&lt;/a&gt;&lt;/strong&gt; - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://x.com/corbtt/status/1953171838382817625"&gt;MCP‚Ä¢RL: Teach Your Model to Master Any MCP Server&lt;/a&gt;&lt;/strong&gt; - Automatically train models to effectively use MCP server tools through reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://x.com/mattshumer_/status/1950572449025650733"&gt;AutoRL: Zero-Data Training for Any Task&lt;/a&gt;&lt;/strong&gt; - Train custom AI models without labeled data using automatic input generation and RULER evaluation.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards"&gt;RULER: Easy Mode for RL Rewards&lt;/a&gt;&lt;/strong&gt; is now available for automatic reward generation in reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ART¬∑E: How We Built an Email Research Agent That Beats o3&lt;/a&gt;&lt;/strong&gt; demonstrates a Qwen 2.5 14B email agent outperforming OpenAI's o3.&lt;/li&gt; 
 &lt;li&gt;üóûÔ∏è &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-trainer"&gt;ART Trainer: A New RL Trainer for Agents&lt;/a&gt;&lt;/strong&gt; enables easy training of LLM-based agents using GRPO.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://openpipe.ai/blog"&gt;üìñ See all blog posts ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why ART?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ART provides convenient wrappers for introducing RL training into &lt;strong&gt;existing applications&lt;/strong&gt;. We abstract the training server into a modular service that your code doesn't need to interface with.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train from anywhere.&lt;/strong&gt; Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.&lt;/li&gt; 
 &lt;li&gt;Integrations with hosted platforms like W&amp;amp;B, Langfuse, and OpenPipe provide flexible observability and &lt;strong&gt;simplify debugging&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ART is customizable with &lt;strong&gt;intelligent defaults&lt;/strong&gt;. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install openpipe-art
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ñ ART‚Ä¢E Agent&lt;/h2&gt; 
&lt;p&gt;Curious about how to use ART for a real-world task? Check out the &lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ART‚Ä¢E Agent&lt;/a&gt; blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!&lt;/p&gt; 
&lt;img src="https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png" width="700" /&gt; 
&lt;h2&gt;üîÅ Training Loop Overview&lt;/h2&gt; 
&lt;p&gt;ART's functionality is divided into a &lt;strong&gt;client&lt;/strong&gt; and a &lt;strong&gt;server&lt;/strong&gt;. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).&lt;/li&gt; 
   &lt;li&gt;Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.&lt;/li&gt; 
   &lt;li&gt;As the agent executes, each &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, and &lt;code&gt;assistant&lt;/code&gt; message is stored in a Trajectory.&lt;/li&gt; 
   &lt;li&gt;When a rollout finishes, your code assigns a &lt;code&gt;reward&lt;/code&gt; to its Trajectory, indicating the performance of the LLM.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.&lt;/li&gt; 
   &lt;li&gt;The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).&lt;/li&gt; 
   &lt;li&gt;The server saves the newly trained LoRA to a local directory and loads it into vLLM.&lt;/li&gt; 
   &lt;li&gt;Inference is unblocked and the loop resumes at step 1.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This training loop runs until a specified number of inference and training iterations have completed.&lt;/p&gt; 
&lt;h2&gt;üß© Supported Models&lt;/h2&gt; 
&lt;p&gt;ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth&lt;/a&gt;. Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on &lt;a href="https://discord.gg/zbBHRUpwf4"&gt;Discord&lt;/a&gt; or open an issue on &lt;a href="https://github.com/openpipe/art/issues"&gt;GitHub&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;ART is in active development, and contributions are most welcome! Please see the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚öñÔ∏è License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üôè Credits&lt;/h2&gt; 
&lt;p&gt;ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/trl"&gt;trl&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchtune"&gt;torchtune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/adk-python</title>
      <link>https://github.com/google/adk-python</link>
      <description>&lt;p&gt;An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Development Kit (ADK)&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-python/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/google-adk/"&gt;&lt;img src="https://img.shields.io/pypi/v/google-adk" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml"&gt;&lt;img src="https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml/badge.svg?sanitize=true" alt="Python Unit Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/agentdevelopmentkit/"&gt;&lt;img src="https://img.shields.io/badge/Reddit-r%2Fagentdevelopmentkit-FF4500?style=flat&amp;amp;logo=reddit&amp;amp;logoColor=white" alt="r/agentdevelopmentkit" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/google/adk-python"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt;  
&lt;h2 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png" width="256" /&gt; &lt;/h2&gt; 
&lt;h3 align="center"&gt; An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control. &lt;/h3&gt; 
&lt;h3 align="center"&gt; Important Links: &lt;a href="https://google.github.io/adk-docs/"&gt;Docs&lt;/a&gt;, &lt;a href="https://github.com/google/adk-samples"&gt;Samples&lt;/a&gt;, &lt;a href="https://github.com/google/adk-java"&gt;Java ADK&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/google/adk-web"&gt;ADK Web&lt;/a&gt;. &lt;/h3&gt;  
&lt;p&gt;Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents. While optimized for Gemini and the Google ecosystem, ADK is model-agnostic, deployment-agnostic, and is built for compatibility with other frameworks. ADK was designed to make agent development feel more like software development, to make it easier for developers to create, deploy, and orchestrate agentic architectures that range from simple tasks to complex workflows.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® What's new&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Config&lt;/strong&gt;: Build agents without code. Check out the &lt;a href="https://google.github.io/adk-docs/agents/config/"&gt;Agent Config&lt;/a&gt; feature.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rich Tool Ecosystem&lt;/strong&gt;: Utilize pre-built tools, custom functions, OpenAPI specs, or integrate existing tools to give agents diverse capabilities, all for tight integration with the Google ecosystem.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code-First Development&lt;/strong&gt;: Define agent logic, tools, and orchestration directly in Python for ultimate flexibility, testability, and versioning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Multi-Agent Systems&lt;/strong&gt;: Design scalable applications by composing multiple specialized agents into flexible hierarchies.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deploy Anywhere&lt;/strong&gt;: Easily containerize and deploy agents on Cloud Run or scale seamlessly with Vertex AI Agent Engine.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ñ Agent2Agent (A2A) Protocol and ADK Integration&lt;/h2&gt; 
&lt;p&gt;For remote agent-to-agent communication, ADK integrates with the &lt;a href="https://github.com/google-a2a/A2A/"&gt;A2A protocol&lt;/a&gt;. See this &lt;a href="https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents"&gt;example&lt;/a&gt; for how they can work together.&lt;/p&gt; 
&lt;h2&gt;üöÄ Installation&lt;/h2&gt; 
&lt;h3&gt;Stable Release (Recommended)&lt;/h3&gt; 
&lt;p&gt;You can install the latest stable version of ADK using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install google-adk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The release cadence is weekly.&lt;/p&gt; 
&lt;p&gt;This version is recommended for most users as it represents the most recent official release.&lt;/p&gt; 
&lt;h3&gt;Development Version&lt;/h3&gt; 
&lt;p&gt;Bug fixes and new features are merged into the main branch on GitHub first. If you need access to changes that haven't been included in an official PyPI release yet, you can install directly from the main branch:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/google/adk-python.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: The development version is built directly from the latest code commits. While it includes the newest fixes and features, it may also contain experimental changes or bugs not present in the stable release. Use it primarily for testing upcoming changes or accessing critical fixes before they are officially released.&lt;/p&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;Explore the full documentation for detailed guides on building, evaluating, and deploying agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://google.github.io/adk-docs"&gt;Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üèÅ Feature Highlight&lt;/h2&gt; 
&lt;h3&gt;Define a single agent:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.adk.agents import Agent
from google.adk.tools import google_search

root_agent = Agent(
    name="search_assistant",
    model="gemini-2.0-flash", # Or your preferred Gemini model
    instruction="You are a helpful assistant. Answer user questions using Google Search when needed.",
    description="An assistant that can search the web.",
    tools=[google_search]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Define a multi-agent system:&lt;/h3&gt; 
&lt;p&gt;Define a multi-agent system with coordinator agent, greeter agent, and task execution agent. Then ADK engine and the model will guide the agents works together to accomplish the task.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.adk.agents import LlmAgent, BaseAgent

# Define individual agents
greeter = LlmAgent(name="greeter", model="gemini-2.0-flash", ...)
task_executor = LlmAgent(name="task_executor", model="gemini-2.0-flash", ...)

# Create parent agent and assign children via sub_agents
coordinator = LlmAgent(
    name="Coordinator",
    model="gemini-2.0-flash",
    description="I coordinate greetings and tasks.",
    sub_agents=[ # Assign sub_agents here
        greeter,
        task_executor
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development UI&lt;/h3&gt; 
&lt;p&gt;A built-in development UI to help you test, evaluate, debug, and showcase your agent(s).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png" /&gt; 
&lt;h3&gt;Evaluate Agents&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;adk eval \
    samples_for_testing/hello_world \
    samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://google.github.io/adk-docs/contributing-guide/"&gt;General contribution guideline and flow&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Then if you want to contribute code, please read &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/CONTRIBUTING.md"&gt;Code Contributing Guidelines&lt;/a&gt; to get started.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Vibe Coding&lt;/h2&gt; 
&lt;p&gt;If you are to develop agent via vibe coding the &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/llms.txt"&gt;llms.txt&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/llms-full.txt"&gt;llms-full.txt&lt;/a&gt; can be used as context to LLM. While the former one is a summarized one and the later one has the full information in case your LLM has big enough context window.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Happy Agent Building!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Dao-AILab/flash-attention</title>
      <link>https://github.com/Dao-AILab/flash-attention</link>
      <description>&lt;p&gt;Fast and memory-efficient exact attention&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FlashAttention&lt;/h1&gt; 
&lt;p&gt;This repository provides the official implementation of FlashAttention and FlashAttention-2 from the following papers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/strong&gt;&lt;br /&gt; Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√©&lt;br /&gt; Paper: &lt;a href="https://arxiv.org/abs/2205.14135"&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;&lt;br /&gt; IEEE Spectrum &lt;a href="https://spectrum.ieee.org/mlperf-rankings-2022"&gt;article&lt;/a&gt; about our submission to the MLPerf 2.0 benchmark using FlashAttention. &lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flashattn_banner.jpg" alt="FlashAttention" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning&lt;/strong&gt;&lt;br /&gt; Tri Dao&lt;/p&gt; 
&lt;p&gt;Paper: &lt;a href="https://tridao.me/publications/flash2/flash2.pdf"&gt;https://tridao.me/publications/flash2/flash2.pdf&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flashattention_logo.png" alt="FlashAttention-2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;We've been very happy to see FlashAttention being widely adopted in such a short time after its release. This &lt;a href="https://github.com/Dao-AILab/flash-attention/raw/main/usage.md"&gt;page&lt;/a&gt; contains a partial list of places where FlashAttention is being used.&lt;/p&gt; 
&lt;p&gt;FlashAttention and FlashAttention-2 are free to use and modify (see LICENSE). Please cite and credit FlashAttention if you use it.&lt;/p&gt; 
&lt;h2&gt;FlashAttention-3 beta release&lt;/h2&gt; 
&lt;p&gt;FlashAttention-3 is optimized for Hopper GPUs (e.g. H100).&lt;/p&gt; 
&lt;p&gt;Blogpost: &lt;a href="https://tridao.me/blog/2024/flash3/"&gt;https://tridao.me/blog/2024/flash3/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Paper: &lt;a href="https://tridao.me/publications/flash3/flash3.pdf"&gt;https://tridao.me/publications/flash3/flash3.pdf&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flash3_fp16_fwd.png" alt="FlashAttention-3 speedup on H100 80GB SXM5 with FP16" /&gt;&lt;/p&gt; 
&lt;p&gt;This is a beta release for testing / benchmarking before we integrate that with the rest of the repo.&lt;/p&gt; 
&lt;p&gt;Currently released:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;FP16 / BF16 forward and backward, FP8 forward&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Requirements: H100 / H800 GPU, CUDA &amp;gt;= 12.3.&lt;/p&gt; 
&lt;p&gt;We highly recommend CUDA 12.8 for best performance.&lt;/p&gt; 
&lt;p&gt;To install:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd hopper
python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the test:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export PYTHONPATH=$PWD
pytest -q -s test_flash_attn.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the package is installed, you can import it as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import flash_attn_interface
flash_attn_interface.flash_attn_func()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation and features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CUDA toolkit or ROCm toolkit&lt;/li&gt; 
 &lt;li&gt;PyTorch 2.2 and above.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;packaging&lt;/code&gt; Python package (&lt;code&gt;pip install packaging&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ninja&lt;/code&gt; Python package (&lt;code&gt;pip install ninja&lt;/code&gt;) *&lt;/li&gt; 
 &lt;li&gt;Linux. Might work for Windows starting v2.3.2 (we've seen a few positive &lt;a href="https://github.com/Dao-AILab/flash-attention/issues/595"&gt;reports&lt;/a&gt;) but Windows compilation still requires more testing. If you have ideas on how to set up prebuilt CUDA wheels for Windows, please reach out via Github issue.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;* Make sure that &lt;code&gt;ninja&lt;/code&gt; is installed and that it works correctly (e.g. &lt;code&gt;ninja --version&lt;/code&gt; then &lt;code&gt;echo $?&lt;/code&gt; should return exit code 0). If not (sometimes &lt;code&gt;ninja --version&lt;/code&gt; then &lt;code&gt;echo $?&lt;/code&gt; returns a nonzero exit code), uninstall then reinstall &lt;code&gt;ninja&lt;/code&gt; (&lt;code&gt;pip uninstall -y ninja &amp;amp;&amp;amp; pip install ninja&lt;/code&gt;). Without &lt;code&gt;ninja&lt;/code&gt;, compiling can take a very long time (2h) since it does not use multiple CPU cores. With &lt;code&gt;ninja&lt;/code&gt; compiling takes 3-5 minutes on a 64-core machine using CUDA toolkit.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To install:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively you can compile from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your machine has less than 96GB of RAM and lots of CPU cores, &lt;code&gt;ninja&lt;/code&gt; might run too many parallel compilation jobs that could exhaust the amount of RAM. To limit the number of parallel compilation jobs, you can set the environment variable &lt;code&gt;MAX_JOBS&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;MAX_JOBS=4 pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Interface:&lt;/strong&gt; &lt;code&gt;src/flash_attention_interface.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;NVIDIA CUDA Support&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CUDA 12.0 and above.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We recommend the &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch"&gt;Pytorch&lt;/a&gt; container from Nvidia, which has all the required tools to install FlashAttention.&lt;/p&gt; 
&lt;p&gt;FlashAttention-2 with CUDA currently supports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100). Support for Turing GPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing GPUs for now.&lt;/li&gt; 
 &lt;li&gt;Datatype fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).&lt;/li&gt; 
 &lt;li&gt;All head dimensions up to 256. &lt;del&gt;Head dim &amp;gt; 192 backward requires A100/A800 or H100/H800&lt;/del&gt;. Head dim 256 backward now works on consumer GPUs (if there's no dropout) as of flash-attn 2.5.5.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;AMD ROCm Support&lt;/h3&gt; 
&lt;p&gt;ROCm version has two backends. There is &lt;a href="https://github.com/ROCm/composable_kernel"&gt;composable_kernel&lt;/a&gt; (ck) which is the default backend and a &lt;a href="https://github.com/triton-lang/triton"&gt;Triton&lt;/a&gt; backend. They provide an implementation of FlashAttention-2.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ROCm 6.0 and above.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We recommend the &lt;a href="https://hub.docker.com/r/rocm/pytorch"&gt;Pytorch&lt;/a&gt; container from ROCm, which has all the required tools to install FlashAttention.&lt;/p&gt; 
&lt;h4&gt;Composable Kernel Backend&lt;/h4&gt; 
&lt;p&gt;FlashAttention-2 ROCm CK backend currently supports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;MI200 or MI300 GPUs.&lt;/li&gt; 
 &lt;li&gt;Datatype fp16 and bf16&lt;/li&gt; 
 &lt;li&gt;Both forward's and backward's head dimensions up to 256.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Triton Backend&lt;/h4&gt; 
&lt;p&gt;The Triton implementation of the &lt;a href="https://tridao.me/publications/flash2/flash2.pdf"&gt;Flash Attention v2&lt;/a&gt; is currently a work in progress.&lt;/p&gt; 
&lt;p&gt;It supports AMD's CDNA (MI200, MI300) and RDNA GPU's using fp16, bf16 and fp32 datatypes.&lt;/p&gt; 
&lt;p&gt;These features are supported in Fwd and Bwd&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fwd and Bwd with causal masking&lt;/li&gt; 
 &lt;li&gt;Variable sequence lengths&lt;/li&gt; 
 &lt;li&gt;Arbitrary Q and KV sequence lengths&lt;/li&gt; 
 &lt;li&gt;Arbitrary head sizes&lt;/li&gt; 
 &lt;li&gt;Multi and grouped query attention&lt;/li&gt; 
 &lt;li&gt;Dropout&lt;/li&gt; 
 &lt;li&gt;Rotary embeddings&lt;/li&gt; 
 &lt;li&gt;ALiBi&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We are working on the following things&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Paged Attention&lt;/li&gt; 
 &lt;li&gt;Sliding Window&lt;/li&gt; 
 &lt;li&gt;FP8&lt;/li&gt; 
 &lt;li&gt;Performance Improvements&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;Getting Started&lt;/h5&gt; 
&lt;p&gt;To get started with the triton backend for AMD, follow the steps below.&lt;/p&gt; 
&lt;p&gt;First install the recommended Triton version&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install triton==3.2.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install Flash Attention with the flag &lt;code&gt;FLASH_ATTENTION_TRITON_AMD_ENABLE&lt;/code&gt; set to &lt;code&gt;"TRUE"&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd flash-attention
git checkout main_perf
FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE" python setup.py install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To test that things are working, you can run our tests. These tests take hours so you don't need to run the full thing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE" pytest tests/test_flash_attn_triton_amd.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use autotune for better performance by using this flag &lt;code&gt;FLASH_ATTENTION_TRITON_AMD_AUTOTUNE="TRUE"&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE" FLASH_ATTENTION_TRITON_AMD_AUTOTUNE="TRUE" python $PATH_TO_CODE
&lt;/code&gt;&lt;/pre&gt; 
&lt;h6&gt;Docker&lt;/h6&gt; 
&lt;p&gt;You can also use the Dockerfile below which does the above steps on top of the latest rocm/pytorch image.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FROM rocm/pytorch:latest

WORKDIR /workspace

# install triton
RUN pip install triton==3.2.0

# install flash attention
ENV FLASH_ATTENTION_TRITON_AMD_ENABLE="TRUE"

RUN git clone https://github.com/ROCm/flash-attention.git &amp;amp;&amp;amp;\ 
    cd flash-attention &amp;amp;&amp;amp;\
    git checkout main_perf &amp;amp;&amp;amp;\
    python setup.py install

# set working dir
WORKDIR /workspace/flash-attention
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To build the docker file&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker build -t fa_triton .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the docker image&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run -it --network=host --user root --group-add video --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --ipc=host --shm-size 16G --device=/dev/kfd --device=/dev/dri fa_triton
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to use FlashAttention&lt;/h2&gt; 
&lt;p&gt;The main functions implement scaled dot product attention (softmax(Q @ K^T * softmax_scale) @ V):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from flash_attn import flash_attn_qkvpacked_func, flash_attn_func
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;flash_attn_qkvpacked_func(qkv, dropout_p=0.0, softmax_scale=None, causal=False,
                          window_size=(-1, -1), alibi_slopes=None, deterministic=False):
"""dropout_p should be set to 0.0 during evaluation
If Q, K, V are already stacked into 1 tensor, this function will be faster than
calling flash_attn_func on Q, K, V since the backward pass avoids explicit concatenation
of the gradients of Q, K, V.
If window_size != (-1, -1), implements sliding window local attention. Query at position i
will only attend to keys between [i - window_size[0], i + window_size[1]] inclusive.
Arguments:
    qkv: (batch_size, seqlen, 3, nheads, headdim)
    dropout_p: float. Dropout probability.
    softmax_scale: float. The scaling of QK^T before applying softmax.
        Default to 1 / sqrt(headdim).
    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).
    window_size: (left, right). If not (-1, -1), implements sliding window local attention.
    alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of (-alibi_slope * |i - j|) is added to
        the attention score of query i and key j.
    deterministic: bool. Whether to use the deterministic implementation of the backward pass,
        which is slightly slower and uses more memory. The forward pass is always deterministic.
Return:
    out: (batch_size, seqlen, nheads, headdim).
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=False,
                window_size=(-1, -1), alibi_slopes=None, deterministic=False):
"""dropout_p should be set to 0.0 during evaluation
Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads
than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.
For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head
0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.
If window_size != (-1, -1), implements sliding window local attention. Query at position i
will only attend to keys between
[i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.

Arguments:
    q: (batch_size, seqlen, nheads, headdim)
    k: (batch_size, seqlen, nheads_k, headdim)
    v: (batch_size, seqlen, nheads_k, headdim)
    dropout_p: float. Dropout probability.
    softmax_scale: float. The scaling of QK^T before applying softmax.
        Default to 1 / sqrt(headdim).
    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).
    window_size: (left, right). If not (-1, -1), implements sliding window local attention.
    alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of
        (-alibi_slope * |i + seqlen_k - seqlen_q - j|)
        is added to the attention score of query i and key j.
    deterministic: bool. Whether to use the deterministic implementation of the backward pass,
        which is slightly slower and uses more memory. The forward pass is always deterministic.
Return:
    out: (batch_size, seqlen, nheads, headdim).
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def flash_attn_with_kvcache(
    q,
    k_cache,
    v_cache,
    k=None,
    v=None,
    rotary_cos=None,
    rotary_sin=None,
    cache_seqlens: Optional[Union[(int, torch.Tensor)]] = None,
    cache_batch_idx: Optional[torch.Tensor] = None,
    block_table: Optional[torch.Tensor] = None,
    softmax_scale=None,
    causal=False,
    window_size=(-1, -1),  # -1 means infinite context window
    rotary_interleaved=True,
    alibi_slopes=None,
):
    """
    If k and v are not None, k_cache and v_cache will be updated *inplace* with the new values from
    k and v. This is useful for incremental decoding: you can pass in the cached keys/values from
    the previous step, and update them with the new keys/values from the current step, and do
    attention with the updated cache, all in 1 kernel.

    If you pass in k / v, you must make sure that the cache is large enough to hold the new values.
    For example, the KV cache could be pre-allocated with the max sequence length, and you can use
    cache_seqlens to keep track of the current sequence lengths of each sequence in the batch.

    Also apply rotary embedding if rotary_cos and rotary_sin are passed in. The key @k will be
    rotated by rotary_cos and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.
    If causal or local (i.e., window_size != (-1, -1)), the query @q will be rotated by rotary_cos
    and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.
    If not causal and not local, the query @q will be rotated by rotary_cos and rotary_sin at
    indices cache_seqlens only (i.e. we consider all tokens in @q to be at position cache_seqlens).

    See tests/test_flash_attn.py::test_flash_attn_kvcache for examples of how to use this function.

    Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads
    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.
    For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head
    0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.

    If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.
    For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:
        1 1 1 1 0
        1 1 1 1 1
    If seqlen_q = 5 and seqlen_k = 2, the causal mask is:
        0 0
        0 0
        0 0
        1 0
        1 1
    If the row of the mask is all zero, the output will be zero.

    If window_size != (-1, -1), implements sliding window local attention. Query at position i
    will only attend to keys between
    [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.

    Note: Does not support backward pass.

    Arguments:
        q: (batch_size, seqlen, nheads, headdim)
        k_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,
            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)
            page_block_size must be a multiple of 256.
        v_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,
            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)
        k [optional]: (batch_size, seqlen_new, nheads_k, headdim). If not None, we concatenate
            k with k_cache, starting at the indices specified by cache_seqlens.
        v [optional]: (batch_size, seqlen_new, nheads_k, headdim). Similar to k.
        rotary_cos [optional]: (seqlen_ro, rotary_dim / 2). If not None, we apply rotary embedding
            to k and q. Only applicable if k and v are passed in. rotary_dim must be divisible by 16.
        rotary_sin [optional]: (seqlen_ro, rotary_dim / 2). Similar to rotary_cos.
        cache_seqlens: int, or (batch_size,), dtype torch.int32. The sequence lengths of the
            KV cache.
        block_table [optional]: (batch_size, max_num_blocks_per_seq), dtype torch.int32.
        cache_batch_idx: (batch_size,), dtype torch.int32. The indices used to index into the KV cache.
            If None, we assume that the batch indices are [0, 1, 2, ..., batch_size - 1].
            If the indices are not distinct, and k and v are provided, the values updated in the cache
                 might come from any of the duplicate indices.
        softmax_scale: float. The scaling of QK^T before applying softmax.
            Default to 1 / sqrt(headdim).
        causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).
        window_size: (left, right). If not (-1, -1), implements sliding window local attention.
        rotary_interleaved: bool. Only applicable if rotary_cos and rotary_sin are passed in.
            If True, rotary embedding will combine dimensions 0 &amp;amp; 1, 2 &amp;amp; 3, etc. If False,
            rotary embedding will combine dimensions 0 &amp;amp; rotary_dim / 2, 1 &amp;amp; rotary_dim / 2 + 1
            (i.e. GPT-NeoX style).
        alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of
            (-alibi_slope * |i + seqlen_k - seqlen_q - j|)
            is added to the attention score of query i and key j.

    Return:
        out: (batch_size, seqlen, nheads, headdim).
    """
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see how these functions are used in a multi-head attention layer (which includes QKV projection, output projection), see the MHA &lt;a href="https://github.com/Dao-AILab/flash-attention/raw/main/flash_attn/modules/mha.py"&gt;implementation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;h3&gt;2.0: Complete rewrite, 2x faster&lt;/h3&gt; 
&lt;p&gt;Upgrading from FlashAttention (1.x) to FlashAttention-2&lt;/p&gt; 
&lt;p&gt;These functions have been renamed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;flash_attn_unpadded_func&lt;/code&gt; -&amp;gt; &lt;code&gt;flash_attn_varlen_func&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;flash_attn_unpadded_qkvpacked_func&lt;/code&gt; -&amp;gt; &lt;code&gt;flash_attn_varlen_qkvpacked_func&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;flash_attn_unpadded_kvpacked_func&lt;/code&gt; -&amp;gt; &lt;code&gt;flash_attn_varlen_kvpacked_func&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If the inputs have the same sequence lengths in the same batch, it is simpler and faster to use these functions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;flash_attn_qkvpacked_func(qkv, dropout_p=0.0, softmax_scale=None, causal=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.1: Change behavior of causal flag&lt;/h3&gt; 
&lt;p&gt;If seqlen_q != seqlen_k and causal=True, the causal mask is aligned to the bottom right corner of the attention matrix, instead of the top-left corner.&lt;/p&gt; 
&lt;p&gt;For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:&lt;br /&gt; v2.0:&lt;br /&gt; 1 0 0 0 0&lt;br /&gt; 1 1 0 0 0&lt;br /&gt; v2.1:&lt;br /&gt; 1 1 1 1 0&lt;br /&gt; 1 1 1 1 1&lt;/p&gt; 
&lt;p&gt;If seqlen_q = 5 and seqlen_k = 2, the causal mask is:&lt;br /&gt; v2.0:&lt;br /&gt; 1 0&lt;br /&gt; 1 1&lt;br /&gt; 1 1&lt;br /&gt; 1 1&lt;br /&gt; 1 1&lt;br /&gt; v2.1:&lt;br /&gt; 0 0&lt;br /&gt; 0 0&lt;br /&gt; 0 0&lt;br /&gt; 1 0&lt;br /&gt; 1 1&lt;br /&gt; If the row of the mask is all zero, the output will be zero.&lt;/p&gt; 
&lt;h3&gt;2.2: Optimize for inference&lt;/h3&gt; 
&lt;p&gt;Optimize for inference (iterative decoding) when query has very small sequence length (e.g., query sequence length = 1). The bottleneck here is to load KV cache as fast as possible, and we split the loading across different thread blocks, with a separate kernel to combine results.&lt;/p&gt; 
&lt;p&gt;See the function &lt;code&gt;flash_attn_with_kvcache&lt;/code&gt; with more features for inference (perform rotary embedding, updating KV cache inplace).&lt;/p&gt; 
&lt;p&gt;Thanks to the xformers team, and in particular Daniel Haziza, for this collaboration.&lt;/p&gt; 
&lt;h3&gt;2.3: Local (i.e., sliding window) attention&lt;/h3&gt; 
&lt;p&gt;Implement sliding window attention (i.e., local attention). Thanks to &lt;a href="https://mistral.ai/"&gt;Mistral AI&lt;/a&gt; and in particular Timoth√©e Lacroix for this contribution. Sliding window was used in the &lt;a href="https://mistral.ai/news/announcing-mistral-7b/"&gt;Mistral 7B&lt;/a&gt; model.&lt;/p&gt; 
&lt;h3&gt;2.4: ALiBi (attention with linear bias), deterministic backward pass.&lt;/h3&gt; 
&lt;p&gt;Implement ALiBi (Press et al., 2021). Thanks to Sanghun Cho from Kakao Brain for this contribution.&lt;/p&gt; 
&lt;p&gt;Implement deterministic backward pass. Thanks to engineers from &lt;a href="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/www.meituan.com"&gt;Meituan&lt;/a&gt; for this contribution.&lt;/p&gt; 
&lt;h3&gt;2.5: Paged KV cache.&lt;/h3&gt; 
&lt;p&gt;Support paged KV cache (i.e., &lt;a href="https://arxiv.org/abs/2309.06180"&gt;PagedAttention&lt;/a&gt;). Thanks to @beginlner for this contribution.&lt;/p&gt; 
&lt;h3&gt;2.6: Softcapping.&lt;/h3&gt; 
&lt;p&gt;Support attention with softcapping, as used in Gemma-2 and Grok models. Thanks to @Narsil and @lucidrains for this contribution.&lt;/p&gt; 
&lt;h3&gt;2.7: Compatibility with torch compile&lt;/h3&gt; 
&lt;p&gt;Thanks to @ani300 for this contribution.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;We present expected speedup (combined forward + backward pass) and memory savings from using FlashAttention against PyTorch standard attention, depending on sequence length, on different GPUs (speedup depends on memory bandwidth - we see more speedup on slower GPU memory).&lt;/p&gt; 
&lt;p&gt;We currently have benchmarks for these GPUs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/#a100"&gt;A100&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/#h100"&gt;H100&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- * [RTX 3090](#rtx-3090) --&gt; 
&lt;!-- * [T4](#t4) --&gt; 
&lt;h3&gt;A100&lt;/h3&gt; 
&lt;p&gt;We display FlashAttention speedup using these parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Head dimension 64 or 128, hidden dimension 2048 (i.e. either 32 or 16 heads).&lt;/li&gt; 
 &lt;li&gt;Sequence length 512, 1k, 2k, 4k, 8k, 16k.&lt;/li&gt; 
 &lt;li&gt;Batch size set to 16k / seqlen.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Speedup&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flash2_a100_fwd_bwd_benchmark.png" alt="FlashAttention speedup on A100 80GB SXM5 with FP16/BF16" /&gt;&lt;/p&gt; 
&lt;h4&gt;Memory&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flashattn_memory.jpg" alt="FlashAttention memory" /&gt;&lt;/p&gt; 
&lt;p&gt;We show memory savings in this graph (note that memory footprint is the same no matter if you use dropout or masking). Memory savings are proportional to sequence length -- since standard attention has memory quadratic in sequence length, whereas FlashAttention has memory linear in sequence length. We see 10X memory savings at sequence length 2K, and 20X at 4K. As a result, FlashAttention can scale to much longer sequence lengths.&lt;/p&gt; 
&lt;h3&gt;H100&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/assets/flash2_h100_fwd_bwd_benchmark.png" alt="FlashAttention speedup on H100 SXM5 with FP16/BF16" /&gt;&lt;/p&gt; 
&lt;h2&gt;Full model code and training script&lt;/h2&gt; 
&lt;p&gt;We have released the full GPT model &lt;a href="https://github.com/Dao-AILab/flash-attention/raw/main/flash_attn/models/gpt.py"&gt;implementation&lt;/a&gt;. We also provide optimized implementations of other layers (e.g., MLP, LayerNorm, cross-entropy loss, rotary embedding). Overall this speeds up training by 3-5x compared to the baseline implementation from Huggingface, reaching up to 225 TFLOPs/sec per A100, equivalent to 72% model FLOPs utilization (we don't need any activation checkpointing).&lt;/p&gt; 
&lt;p&gt;We also include a training &lt;a href="https://github.com/Dao-AILab/flash-attention/tree/main/training"&gt;script&lt;/a&gt; to train GPT2 on Openwebtext and GPT3 on The Pile.&lt;/p&gt; 
&lt;h2&gt;Triton implementation of FlashAttention&lt;/h2&gt; 
&lt;p&gt;Phil Tillet (OpenAI) has an experimental implementation of FlashAttention in Triton: &lt;a href="https://github.com/openai/triton/raw/master/python/tutorials/06-fused-attention.py"&gt;https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;As Triton is a higher-level language than CUDA, it might be easier to understand and experiment with. The notations in the Triton implementation are also closer to what's used in our paper.&lt;/p&gt; 
&lt;p&gt;We also have an experimental implementation in Triton that support attention bias (e.g. ALiBi): &lt;a href="https://github.com/Dao-AILab/flash-attention/raw/main/flash_attn/flash_attn_triton.py"&gt;https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/flash_attn_triton.py&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Tests&lt;/h2&gt; 
&lt;p&gt;We test that FlashAttention produces the same output and gradient as a reference implementation, up to some numerical tolerance. In particular, we check that the maximum numerical error of FlashAttention is at most twice the numerical error of a baseline implementation in Pytorch (for different head dimensions, input dtype, sequence length, causal / non-causal).&lt;/p&gt; 
&lt;p&gt;To run the tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pytest -q -s tests/test_flash_attn.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;When you encounter issues&lt;/h2&gt; 
&lt;p&gt;This new release of FlashAttention-2 has been tested on several GPT-style models, mostly on A100 GPUs.&lt;/p&gt; 
&lt;p&gt;If you encounter bugs, please open a GitHub Issue!&lt;/p&gt; 
&lt;h2&gt;Tests&lt;/h2&gt; 
&lt;p&gt;To run the tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pytest tests/test_flash_attn_ck.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use this codebase, or otherwise found our work valuable, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}
@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;"DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;" /&gt; &lt;/td&gt; 
    &lt;td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;"&gt; &lt;pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;"&gt;    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/14665" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;!-- &lt;img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/&gt; --&gt; 
 &lt;h1&gt;&lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;" /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align="center"&gt;
  &lt;img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white" alt="Version"&gt;

  &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white" alt="License"&gt;
  &lt;img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white" alt="AI"&gt;
  &lt;img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white" alt="HKU"&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/HKUDS/DeepCode/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/deepcode-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/DeepCode/issues/11"&gt;&lt;img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;üñ•Ô∏è &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üöÄ Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;‚ö° Fast command-line workflow&lt;br /&gt;üîß Developer-friendly interface&lt;br /&gt;üìä Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üé® Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;üñ±Ô∏è Intuitive drag-and-drop&lt;br /&gt;üì± Responsive design&lt;br /&gt;üéØ Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h3&gt;üé¨ &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style="margin: 20px 0;"&gt; 
   &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;" /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;üéØ &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/‚ñ∂Ô∏è_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white" alt="Watch Video" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;"Where AI Agents Transform Ideas into Production-Ready Code"&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features"&gt;üöÄ Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture"&gt;üèóÔ∏è Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples"&gt;üí° Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations"&gt;üé¨ Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license"&gt;üìÑ License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üöÄ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üé® &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;‚öôÔ∏è &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Autonomous Multi-Agent Workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üìÑ &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üî¨ &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚è±Ô∏è &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîÑ &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    A["üìÑ Research Papers&amp;lt;br/&amp;gt;üí¨ Text Prompts&amp;lt;br/&amp;gt;üåê URLs &amp;amp; Document&amp;lt;br/&amp;gt;üìé Files: PDF, DOC, PPTX, TXT, HTML"] --&amp;gt; B["üß† DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine"]
    B --&amp;gt; C["üöÄ Algorithm Implementation &amp;lt;br/&amp;gt;üé® Frontend Development &amp;lt;br/&amp;gt;‚öôÔ∏è Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;h3&gt;üìä &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;üéØ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;üß¨ &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;ü™Ñ &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;‚ö° &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;üíé &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;üîÆ &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üîß &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üíæ &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîç &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üéØ Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìÑ Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìö Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üß¨ Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;üîß Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;üì° &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üîß &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üí° &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîç brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÇ filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üì• github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìã file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üß¨ code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìö code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;üîß &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üéØ &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚úçÔ∏è write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üêç execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÅ get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìä get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;üéõÔ∏è &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üöÄ Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;üåü &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; üí° &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; üìÑ Research Papers ‚Ä¢ üí¨ Natural Language ‚Ä¢ üåê URLs ‚Ä¢ üìã Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üéØ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making ‚Ä¢ Workflow Coordination ‚Ä¢ Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìù &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìÑ &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üìã &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis ‚Ä¢ Code Requirements Parsing ‚Ä¢ Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; üîç &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìö &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üß¨ &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation ‚Ä¢ Testing ‚Ä¢ Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; ‚ö° &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; üì¶ Complete Codebase ‚Ä¢ üß™ Test Suite ‚Ä¢ üìö Documentation ‚Ä¢ üöÄ Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;üîÑ &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" style="border: none;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;"&gt; 
      &lt;h4&gt;üéØ Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;"&gt; 
      &lt;h4&gt;üß† Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;"&gt; 
      &lt;h4&gt;üîç Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;"&gt; 
      &lt;h4&gt;‚ö° Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üì¶ &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üöÄ Install DeepCode package directly
pip install deepcode-hku

# üîë Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìÇ Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;üî• &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# üîß Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;üêç &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install dependencies
pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;ü™ü &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you're using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;üîç &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Brave Search&lt;/strong&gt; (&lt;code&gt;"brave"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Bocha-MCP&lt;/strong&gt; (&lt;code&gt;"bocha-mcp"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚ö° &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üåê Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Web Access" /&gt; 
&lt;/div&gt; 
&lt;h5&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white" alt="CLI Mode" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Examples&lt;/h2&gt; 
&lt;h3&gt;üé¨ &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üìÑ &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt; &lt;img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üñºÔ∏è &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt; &lt;img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üåê &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt; &lt;img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;üÜï &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üìÑ &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We're continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üìä &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href="https://star-history.com/#HKUDS/DeepCode&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;&lt;img src="https://img.shields.io/badge/üöÄ_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Get Started" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS"&gt;&lt;img src="https://img.shields.io/badge/üèõÔ∏è_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="View on GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/deepcode-agent"&gt;&lt;img src="https://img.shields.io/badge/‚≠ê_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white" alt="Star Project" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üìÑ &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; 
 &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff" alt="Visitors" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>QuentinFuxa/WhisperLiveKit</title>
      <link>https://github.com/QuentinFuxa/WhisperLiveKit</link>
      <description>&lt;p&gt;Python package for Real-time, Local Speech-to-Text and Speaker Diarization. FastAPI Server &amp; Web Interface&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;WhisperLiveKit&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png" alt="WhisperLiveKit Demo" width="730" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&lt;b&gt;Real-time, Fully Local Speech-to-Text with Speaker Identification&lt;/b&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="PyPI Version" src="https://img.shields.io/pypi/v/whisperlivekit?color=g" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/whisperlivekit"&gt;&lt;img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/whisperlivekit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=installations" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="Python Versions" src="https://img.shields.io/badge/python-3.9--3.13-dark_green" /&gt;&lt;/a&gt; &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/badge/License-MIT/Dual Licensed-dark_green" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Real-time speech transcription directly to your browser, with a ready-to-use backend+server and a simple frontend. ‚ú®&lt;/p&gt; 
&lt;h4&gt;Powered by Leading Research:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/SimulStreaming"&gt;SimulStreaming&lt;/a&gt; (SOTA 2025) - Ultra-low latency transcription with AlignAtt policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;WhisperStreaming&lt;/a&gt; (SOTA 2023) - Low latency transcription with LocalAgreement policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2507.18446"&gt;Streaming Sortformer&lt;/a&gt; (SOTA 2025) - Advanced real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/juanmc2005/diart"&gt;Diart&lt;/a&gt; (SOTA 2021) - Real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; (2024) - Enterprise-grade Voice Activity Detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Why not just run a simple Whisper model on every audio batch?&lt;/strong&gt; Whisper is designed for complete utterances, not real-time chunks. Processing small segments loses context, cuts off words mid-syllable, and produces poor transcription. WhisperLiveKit uses state-of-the-art simultaneous speech research for intelligent buffering and incremental processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Architecture&lt;/h3&gt; 
&lt;img alt="Architecture" src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png" /&gt; 
&lt;p&gt;&lt;em&gt;The backend supports multiple concurrent users. Voice Activity Detection reduces overhead when no voice is detected.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Installation &amp;amp; Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install whisperlivekit
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;FFmpeg is required&lt;/strong&gt; and must be installed before using WhisperLiveKit&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;OS&lt;/th&gt; 
    &lt;th&gt;How to install&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Ubuntu/Debian&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;sudo apt install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MacOS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;brew install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Windows&lt;/td&gt; 
    &lt;td&gt;Download .exe from &lt;a href="https://ffmpeg.org/download.html"&gt;https://ffmpeg.org/download.html&lt;/a&gt; and add to PATH&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the transcription server:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;whisperlivekit-server --model base --language en
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open your browser&lt;/strong&gt; and navigate to &lt;code&gt;http://localhost:8000&lt;/code&gt;. Start speaking and watch your words appear in real-time!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;See &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/li&gt; 
  &lt;li&gt;For HTTPS requirements, see the &lt;strong&gt;Parameters&lt;/strong&gt; section for SSL configuration options.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;pip install&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speaker diarization with Sortformer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speaker diarization with Diart&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Original Whisper backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Improved timestamps backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper-timestamped&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Apple Silicon optimization backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mlx-whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI API backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;strong&gt;Parameters &amp;amp; Configuration&lt;/strong&gt; below on how to use them.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Command-line Interface&lt;/strong&gt;: Start the transcription server with various options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use better model than default (small)
whisperlivekit-server --model large-v3

# Advanced configuration with diarization and language
whisperlivekit-server --host 0.0.0.0 --port 8000 --model medium --diarization --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Python API Integration&lt;/strong&gt;: Check &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/basic_server.py"&gt;basic_server&lt;/a&gt; for a more complete example of how to use the functions and classes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from whisperlivekit import TranscriptionEngine, AudioProcessor, parse_args
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
import asyncio

transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    transcription_engine = TranscriptionEngine(model="medium", diarization=True, lan="en")
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket: WebSocket, results_generator):
    async for response in results_generator:
        await websocket.send_json(response)
    await websocket.send_json({"type": "ready_to_stop"})

@app.websocket("/asr")
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # Create a new AudioProcessor for each connection, passing the shared engine
    audio_processor = AudioProcessor(transcription_engine=transcription_engine)    
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(handle_websocket_results(websocket, results_generator))
    await websocket.accept()
    while True:
        message = await websocket.receive_bytes()
        await audio_processor.process_audio(message)        
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Frontend Implementation&lt;/strong&gt;: The package includes an HTML/JavaScript implementation &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/web/live_transcription.html"&gt;here&lt;/a&gt;. You can also import it using &lt;code&gt;from whisperlivekit import get_web_interface_html&lt;/code&gt; &amp;amp; &lt;code&gt;page = get_web_interface_html()&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Parameters &amp;amp; Configuration&lt;/h2&gt; 
&lt;p&gt;An important list of parameters can be changed. But what &lt;em&gt;should&lt;/em&gt; you change?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;--model&lt;/code&gt; size. List and recommandations &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/available_models.md"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--language&lt;/code&gt;. List &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;here&lt;/a&gt;. If you use &lt;code&gt;auto&lt;/code&gt;, the model attempts to detect the language automatically, but it tends to bias towards English.&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--backend&lt;/code&gt; ? you can switch to &lt;code&gt;--backend faster-whisper&lt;/code&gt; if &lt;code&gt;simulstreaming&lt;/code&gt; does not work correctly or if you prefer to avoid the dual-license requirements.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--warmup-file&lt;/code&gt;, if you have one&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;, &lt;code&gt;--port&lt;/code&gt;, &lt;code&gt;--ssl-certfile&lt;/code&gt;, &lt;code&gt;--ssl-keyfile&lt;/code&gt;, if you set up a server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--diarization&lt;/code&gt;, if you want to use it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The rest I don't recommend. But below are your options.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Parameter&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whisper model size.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source language code or &lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--task&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt; or &lt;code&gt;translate&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Processing backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;simulstreaming&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--min-chunk-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio chunk size (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vac&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Controller&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vad&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--warmup-file&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio file path for model warmup&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jfk.wav&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server host address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-certfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL certificate file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-keyfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL private key file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;WhisperStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--confidence-validation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use confidence scores for faster validation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--buffer_trimming&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Buffer trimming strategy (&lt;code&gt;sentence&lt;/code&gt; or &lt;code&gt;segment&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;segment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SimulStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--frame-threshold&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AlignAtt frame threshold (lower = faster, higher = more accurate)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--beams&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Number of beams for beam search (1 = greedy decoding)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--decoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Force decoder type (&lt;code&gt;beam&lt;/code&gt; or &lt;code&gt;greedy&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-max-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum audio buffer length (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;30.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-min-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio length to process (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--cif-ckpt-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to CIF model for word boundary detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--never-fire&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Never truncate incomplete words&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for the model&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--static-init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Static prompt that doesn't scroll&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--max-context-tokens&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum context tokens&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Direct path to .pt model file. Download it if not found&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;./base.pt&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--preloaded-model-count&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Optional. Number of models to preload in memory to speed up loading (set up to the expected number of concurrent users)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Diarization options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable speaker identification&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt; or &lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--segmentation-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart segmentation model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--embedding-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart embedding model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;speechbrain/spkrec-ecapa-voxceleb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For diarization using Diart, you need access to pyannote.audio models:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation-3.0"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/embedding"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/embedding&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;Login with HuggingFace: &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üöÄ Deployment Guide&lt;/h3&gt; 
&lt;p&gt;To deploy WhisperLiveKit in production:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Server Setup&lt;/strong&gt;: Install production ASGI server &amp;amp; launch with multiple workers&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install uvicorn gunicorn
gunicorn -k uvicorn.workers.UvicornWorker -w 4 your_app:app
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Host your customized version of the &lt;code&gt;html&lt;/code&gt; example &amp;amp; ensure WebSocket connection points correctly&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nginx Configuration&lt;/strong&gt; (recommended for production):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-nginx"&gt;server {
   listen 80;
   server_name your-domain.com;
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
}}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTPS Support&lt;/strong&gt;: For secure deployments, use "wss://" instead of "ws://" in WebSocket URL&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üêã Docker&lt;/h2&gt; 
&lt;p&gt;Deploy the application easily using Docker with GPU or CPU support.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker installed on your system&lt;/li&gt; 
 &lt;li&gt;For GPU support: NVIDIA Docker runtime installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;With GPU acceleration (recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t wlk .
docker run --gpus all -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CPU only:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f Dockerfile.cpu -t wlk .
docker run -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Custom configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with custom model and language
docker run --gpus all -p 8000:8000 --name wlk wlk --model large-v3 --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Memory Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Large models&lt;/strong&gt;: Ensure your Docker runtime has sufficient memory allocated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--build-arg&lt;/code&gt; Options: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;EXTRAS="whisper-timestamped"&lt;/code&gt; - Add extras to the image's installation (no spaces). Remember to set necessary container options!&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_PRECACHE_DIR="./.cache/"&lt;/code&gt; - Pre-load a model cache for faster first-time start&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_TKN_FILE="./token"&lt;/code&gt; - Add your Hugging Face Hub access token to download gated models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîÆ Use Cases&lt;/h2&gt; 
&lt;p&gt;Capture discussions in real-time for meeting transcription, help hearing-impaired users follow conversations through accessibility tools, transcribe podcasts or videos automatically for content creation, transcribe support calls with speaker identification for customer service...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>comet-ml/opik</title>
      <link>https://github.com/comet-ml/opik</link>
      <description>&lt;p&gt;Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt;
 &lt;b&gt;&lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/readme_CN.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/readme_JP.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/readme_KO.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt;&lt;/b&gt;
&lt;/div&gt; 
&lt;h1 align="center" style="border-bottom: none"&gt; 
 &lt;div&gt; 
  &lt;a href="https://www.comet.com/site/products/opik/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=header_img&amp;amp;utm_campaign=opik"&gt;
   &lt;picture&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/logo-dark-mode.svg" /&gt; 
    &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg" /&gt; 
    &lt;img alt="Comet Opik logo" src="https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg?sanitize=true" width="200" /&gt; 
   &lt;/picture&gt;&lt;/a&gt; 
  &lt;br /&gt; Opik 
 &lt;/div&gt; &lt;/h1&gt; 
&lt;h2 align="center" style="border-bottom: none"&gt;Open-source LLM evaluation platform&lt;/h2&gt; 
&lt;p align="center"&gt; Opik helps you build, evaluate, and optimize LLM systems that run better, faster, and cheaper. From RAG chatbots to code assistants to complex agentic pipelines, Opik provides comprehensive tracing, evaluations, dashboards, and powerful features like &lt;b&gt;Opik Agent Optimizer&lt;/b&gt; and &lt;b&gt;Opik Guardrails&lt;/b&gt; to improve and secure your LLM powered applications in production. &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/opik/"&gt;&lt;img src="https://img.shields.io/pypi/v/opik" alt="Python SDK" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comet-ml/opik/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/comet-ml/opik" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comet-ml/opik/actions/workflows/build_apps.yml"&gt;&lt;img src="https://github.com/comet-ml/opik/actions/workflows/build_apps.yml/badge.svg?sanitize=true" alt="Build" /&gt;&lt;/a&gt; &lt;a href="https://algora.io/comet-ml/bounties?status=open"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Falgora.io%2Fapi%2Fshields%2Fcomet-ml%2Fbounties%3Fstatus%3Dopen" alt="Bounties" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- [![Quick Start](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/opik_quickstart.ipynb) --&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.comet.com/site/products/opik/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=website_button&amp;amp;utm_campaign=opik"&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href="https://chat.comet.com"&gt;&lt;b&gt;Slack Community&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href="https://x.com/Cometml"&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.comet.com/docs/opik/changelog"&gt;&lt;b&gt;Changelog&lt;/b&gt;&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.comet.com/docs/opik/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=docs_button&amp;amp;utm_campaign=opik"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;div align="center" style="margin-top: 1em; margin-bottom: 1em;"&gt; 
 &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/#-what-is-opik"&gt;üöÄ What is Opik?&lt;/a&gt; ‚Ä¢ 
 &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/#%EF%B8%8F-opik-server-installation"&gt;üõ†Ô∏è Opik Server Installation&lt;/a&gt; ‚Ä¢ 
 &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/#-opik-client-sdk"&gt;üíª Opik Client SDK&lt;/a&gt; ‚Ä¢ 
 &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/#-logging-traces-with-integrations"&gt;üìù Logging Traces&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/#-llm-as-a-judge-metrics"&gt;üßë‚Äç‚öñÔ∏è LLM as a Judge&lt;/a&gt; ‚Ä¢ 
 &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/#-evaluating-your-llm-application"&gt;üîç Evaluating your Application&lt;/a&gt; ‚Ä¢ 
 &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/#-star-us-on-github"&gt;‚≠ê Star Us&lt;/a&gt; ‚Ä¢ 
 &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/#-contributing"&gt;ü§ù Contributing&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://www.comet.com/signup?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=readme_banner&amp;amp;utm_campaign=opik"&gt;&lt;img src="https://raw.githubusercontent.com/comet-ml/opik/main/readme-thumbnail-new.png" alt="Opik platform screenshot (thumbnail)" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ What is Opik?&lt;/h2&gt; 
&lt;p&gt;Opik (built by &lt;a href="https://www.comet.com?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=what_is_opik_link&amp;amp;utm_campaign=opik"&gt;Comet&lt;/a&gt;) is an open-source platform designed to streamline the entire lifecycle of LLM applications. It empowers developers to evaluate, test, monitor, and optimize their models and agentic systems. Key offerings include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Observability&lt;/strong&gt;: Deep tracing of LLM calls, conversation logging, and agent activity.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Evaluation&lt;/strong&gt;: Robust prompt evaluation, LLM-as-a-judge, and experiment management.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Production-Ready&lt;/strong&gt;: Scalable monitoring dashboards and online evaluation rules for production.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Opik Agent Optimizer&lt;/strong&gt;: Dedicated SDK and set of optimizers to enhance prompts and agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Opik Guardrails&lt;/strong&gt;: Features to help you implement safe and responsible AI practices.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;p&gt;Key capabilities include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Development &amp;amp; Tracing:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Track all LLM calls and traces with detailed context during development and in production (&lt;a href="https://www.comet.com/docs/opik/quickstart/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=quickstart_link&amp;amp;utm_campaign=opik"&gt;Quickstart&lt;/a&gt;).&lt;/li&gt; 
   &lt;li&gt;Extensive 3rd-party integrations for easy observability: Seamlessly integrate with a growing list of frameworks, supporting many of the largest and most popular ones natively (including recent additions like &lt;strong&gt;Google ADK&lt;/strong&gt;, &lt;strong&gt;Autogen&lt;/strong&gt;, and &lt;strong&gt;Flowise AI&lt;/strong&gt;). (&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/overview/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=integrations_link&amp;amp;utm_campaign=opik"&gt;Integrations&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;Annotate traces and spans with feedback scores via the &lt;a href="https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-and-spans-using-the-sdk?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=sdk_link&amp;amp;utm_campaign=opik"&gt;Python SDK&lt;/a&gt; or the &lt;a href="https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-through-the-ui?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=ui_link&amp;amp;utm_campaign=opik"&gt;UI&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Experiment with prompts and models in the &lt;a href="https://www.comet.com/docs/opik/prompt_engineering/playground"&gt;Prompt Playground&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Evaluation &amp;amp; Testing&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Automate your LLM application evaluation with &lt;a href="https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=datasets_link&amp;amp;utm_campaign=opik"&gt;Datasets&lt;/a&gt; and &lt;a href="https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=eval_link&amp;amp;utm_campaign=opik"&gt;Experiments&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Leverage powerful LLM-as-a-judge metrics for complex tasks like &lt;a href="https://www.comet.com/docs/opik/evaluation/metrics/hallucination/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=hallucination_link&amp;amp;utm_campaign=opik"&gt;hallucination detection&lt;/a&gt;, &lt;a href="https://www.comet.com/docs/opik/evaluation/metrics/moderation/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=moderation_link&amp;amp;utm_campaign=opik"&gt;moderation&lt;/a&gt;, and RAG assessment (&lt;a href="https://www.comet.com/docs/opik/evaluation/metrics/answer_relevance/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=alex_link&amp;amp;utm_campaign=opik"&gt;Answer Relevance&lt;/a&gt;, &lt;a href="https://www.comet.com/docs/opik/evaluation/metrics/context_precision/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=context_link&amp;amp;utm_campaign=opik"&gt;Context Precision&lt;/a&gt;).&lt;/li&gt; 
   &lt;li&gt;Integrate evaluations into your CI/CD pipeline with our &lt;a href="https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=pytest_link&amp;amp;utm_campaign=opik"&gt;PyTest integration&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Production Monitoring &amp;amp; Optimization&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Log high volumes of production traces: Opik is designed for scale (40M+ traces/day).&lt;/li&gt; 
   &lt;li&gt;Monitor feedback scores, trace counts, and token usage over time in the &lt;a href="https://www.comet.com/docs/opik/production/production_monitoring/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=dashboard_link&amp;amp;utm_campaign=opik"&gt;Opik Dashboard&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Utilize &lt;a href="https://www.comet.com/docs/opik/production/rules/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=dashboard_link&amp;amp;utm_campaign=opik"&gt;Online Evaluation Rules&lt;/a&gt; with LLM-as-a-Judge metrics to identify production issues.&lt;/li&gt; 
   &lt;li&gt;Leverage &lt;strong&gt;Opik Agent Optimizer&lt;/strong&gt; and &lt;strong&gt;Opik Guardrails&lt;/strong&gt; to continuously improve and secure your LLM applications in production.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you are looking for features that Opik doesn't have today, please raise a new &lt;a href="https://github.com/comet-ml/opik/issues/new/choose"&gt;Feature request&lt;/a&gt; üöÄ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;h2&gt;üõ†Ô∏è Opik Server Installation&lt;/h2&gt; 
&lt;p&gt;Get your Opik server running in minutes. Choose the option that best suits your needs:&lt;/p&gt; 
&lt;h3&gt;Option 1: Comet.com Cloud (Easiest &amp;amp; Recommended)&lt;/h3&gt; 
&lt;p&gt;Access Opik instantly without any setup. Ideal for quick starts and hassle-free maintenance.&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://www.comet.com/signup?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=install_create_link&amp;amp;utm_campaign=opik"&gt;Create your free Comet account&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Option 2: Self-Host Opik for Full Control&lt;/h3&gt; 
&lt;p&gt;Deploy Opik in your own environment. Choose between Docker for local setups or Kubernetes for scalability.&lt;/p&gt; 
&lt;h4&gt;Self-Hosting with Docker Compose (for Local Development &amp;amp; Testing)&lt;/h4&gt; 
&lt;p&gt;This is the simplest way to get a local Opik instance running. Note the new &lt;code&gt;./opik.sh&lt;/code&gt; installation script:&lt;/p&gt; 
&lt;p&gt;On Linux or Mac Enviroment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the Opik repository
git clone https://github.com/comet-ml/opik.git

# Navigate to the repository
cd opik

# Start the Opik platform
./opik.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On Windows Enviroment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;# Clone the Opik repository
git clone https://github.com/comet-ml/opik.git

# Navigate to the repository
cd opik

# Start the Opik platform
powershell -ExecutionPolicy ByPass -c ".\\opik.ps1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use the &lt;code&gt;--help&lt;/code&gt; or &lt;code&gt;--info&lt;/code&gt; options to troubleshoot issues. Dockerfiles now ensure containers run as non-root users for enhanced security. Once all is up and running, you can now visit &lt;a href="http://localhost:5173"&gt;localhost:5173&lt;/a&gt; on your browser! For detailed instructions, see the &lt;a href="https://www.comet.com/docs/opik/self-host/local_deployment?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=self_host_link&amp;amp;utm_campaign=opik"&gt;Local Deployment Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Self-Hosting with Kubernetes &amp;amp; Helm (for Scalable Deployments)&lt;/h4&gt; 
&lt;p&gt;For production or larger-scale self-hosted deployments, Opik can be installed on a Kubernetes cluster using our Helm chart. Click the badge for the full &lt;a href="https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=kubernetes_link&amp;amp;utm_campaign=opik"&gt;Kubernetes Installation Guide using Helm&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=kubernetes_link&amp;amp;utm_campaign=opik"&gt;&lt;img src="https://img.shields.io/badge/Kubernetes-%23326ce5.svg?&amp;amp;logo=kubernetes&amp;amp;logoColor=white" alt="Kubernetes" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Version 1.7.0 Changes&lt;/strong&gt;: Please check the &lt;a href="https://github.com/comet-ml/opik/raw/main/CHANGELOG.md"&gt;changelog&lt;/a&gt; for important updates and breaking changes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üíª Opik Client SDK&lt;/h2&gt; 
&lt;p&gt;Opik provides a suite of client libraries and a REST API to interact with the Opik server. This includes SDKs for Python, TypeScript, and Ruby (via OpenTelemetry), allowing for seamless integration into your workflows. For detailed API and SDK references, see the &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/fern/docs/reference/overview.mdx"&gt;Opik Client Reference Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Python SDK Quick Start&lt;/h3&gt; 
&lt;p&gt;To get started with the Python SDK:&lt;/p&gt; 
&lt;p&gt;Install the package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install using pip
pip install opik

# or install with uv
uv pip install opik
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Configure the python SDK by running the &lt;code&gt;opik configure&lt;/code&gt; command, which will prompt you for your Opik server address (for self-hosted instances) or your API key and workspace (for Comet.com):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;opik configure
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] You can also call &lt;code&gt;opik.configure(use_local=True)&lt;/code&gt; from your Python code to configure the SDK to run on a local self-hosted installation, or provide API key and workspace details directly for Comet.com. Refer to the &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/fern/docs/reference/python-sdk/"&gt;Python SDK documentation&lt;/a&gt; for more configuration options.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You are now ready to start logging traces using the &lt;a href="https://www.comet.com/docs/opik/python-sdk-reference/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=sdk_link2&amp;amp;utm_campaign=opik"&gt;Python SDK&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üìù Logging Traces with Integrations&lt;/h3&gt; 
&lt;p&gt;The easiest way to log traces is to use one of our direct integrations. Opik supports a wide array of frameworks, including recent additions like &lt;strong&gt;Google ADK&lt;/strong&gt;, &lt;strong&gt;Autogen&lt;/strong&gt;, &lt;strong&gt;AG2&lt;/strong&gt;, and &lt;strong&gt;Flowise AI&lt;/strong&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Integration&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Documentation&lt;/th&gt; 
   &lt;th&gt;Try in Colab&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AG2&lt;/td&gt; 
   &lt;td&gt;Log traces for AG2 LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/ag2?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=ag2_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;aisuite&lt;/td&gt; 
   &lt;td&gt;Log traces for aisuite LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/aisuite?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=aisuite_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/aisuite.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Agno&lt;/td&gt; 
   &lt;td&gt;Log traces for Agno agent orchestration framework calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/agno?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=agno_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;Log traces for Anthropic LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/anthropic?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=anthropic_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/anthropic.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Autogen&lt;/td&gt; 
   &lt;td&gt;Log traces for Autogen agentic workflows&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/autogen?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=autogen_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Bedrock&lt;/td&gt; 
   &lt;td&gt;Log traces for Amazon Bedrock LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/bedrock?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=bedrock_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/bedrock.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CrewAI&lt;/td&gt; 
   &lt;td&gt;Log traces for CrewAI calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/crewai?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=crewai_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/crewai.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;Log traces for DeepSeek LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/deepseek?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=deepseek_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dify&lt;/td&gt; 
   &lt;td&gt;Log traces for Dify agent runs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/dify?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=dify_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DSPy&lt;/td&gt; 
   &lt;td&gt;Log traces for DSPy runs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/dspy?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=dspy_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/dspy.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Flowise AI&lt;/td&gt; 
   &lt;td&gt;Log traces for Flowise AI visual LLM builder&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/flowise?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=flowise_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Native UI integration, see documentation&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Log traces for Google Gemini LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/gemini?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=gemini_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/gemini.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google ADK&lt;/td&gt; 
   &lt;td&gt;Log traces for Google Agent Development Kit (ADK)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/adk?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=google_adk_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/google_adk_integration.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;Log traces for Groq LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/groq?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=groq_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/groq.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Guardrails&lt;/td&gt; 
   &lt;td&gt;Log traces for Guardrails AI validations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/guardrails-ai?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=guardrails_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/guardrails-ai.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Haystack&lt;/td&gt; 
   &lt;td&gt;Log traces for Haystack calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/haystack?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=haystack_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/haystack.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Instructor&lt;/td&gt; 
   &lt;td&gt;Log traces for LLM calls made with Instructor&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/instructor?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=instructor_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/instructor.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LangChain&lt;/td&gt; 
   &lt;td&gt;Log traces for LangChain LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/langchain?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=langchain_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/langchain.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LangChainJS&lt;/td&gt; 
   &lt;td&gt;Log traces for LangChainJS LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/langchainjs?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=langchainjs_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LangGraph&lt;/td&gt; 
   &lt;td&gt;Log traces for LangGraph executions&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/langgraph?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=langgraph_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/langgraph.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LiteLLM&lt;/td&gt; 
   &lt;td&gt;Log traces for LiteLLM model calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/litellm?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=litellm_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/litellm.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LlamaIndex&lt;/td&gt; 
   &lt;td&gt;Log traces for LlamaIndex LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/llama_index?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=llama_index_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/llama-index.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;Log traces for Ollama LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/ollama?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=ollama_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/ollama.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;Log traces for OpenAI LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/openai?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=openai_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/openai.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Agents&lt;/td&gt; 
   &lt;td&gt;Log traces for OpenAI Agents SDK calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/openai_agents?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=openai_agents_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/openai-agents.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;Log traces for OpenRouter LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/openrouter?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=openrouter_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenTelemetry&lt;/td&gt; 
   &lt;td&gt;Log traces for OpenTelemetry supported calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/opentelemetry/overview?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=opentelemetry_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Predibase&lt;/td&gt; 
   &lt;td&gt;Log traces for Predibase LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/predibase?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=predibase_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/predibase.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pydantic AI&lt;/td&gt; 
   &lt;td&gt;Log traces for PydanticAI agent calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/pydantic-ai?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=pydantic_ai_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/pydantic-ai.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ragas&lt;/td&gt; 
   &lt;td&gt;Log traces for Ragas evaluations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/ragas?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=ragas_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/ragas.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Smolagents&lt;/td&gt; 
   &lt;td&gt;Log traces for Smolagents agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/smolagents?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=smolagents_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/smolagents.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Strands Agents&lt;/td&gt; 
   &lt;td&gt;Log traces for Strands agents calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/strands-agents?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=strands_agents_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vercel AI SDK&lt;/td&gt; 
   &lt;td&gt;Log traces for Vercel AI SDK calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/vercel-ai-sdk?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=vercel_ai_sdk_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;(&lt;em&gt;Coming Soon&lt;/em&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;watsonx&lt;/td&gt; 
   &lt;td&gt;Log traces for IBM watsonx LLM calls&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.comet.com/docs/opik/tracing/integrations/watsonx?utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=watsonx_link&amp;amp;utm_campaign=opik"&gt;Documentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/watsonx.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Quickstart In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If the framework you are using is not listed above, feel free to &lt;a href="https://github.com/comet-ml/opik/issues"&gt;open an issue&lt;/a&gt; or submit a PR with the integration.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you are not using any of the frameworks above, you can also use the &lt;code&gt;track&lt;/code&gt; function decorator to &lt;a href="https://www.comet.com/docs/opik/tracing/log_traces/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=traces_link&amp;amp;utm_campaign=opik"&gt;log traces&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import opik

opik.configure(use_local=True) # Run locally

@opik.track
def my_llm_function(user_question: str) -&amp;gt; str:
    # Your LLM code here

    return "Hello"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The track decorator can be used in conjunction with any of our integrations and can also be used to track nested function calls.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üßë‚Äç‚öñÔ∏è LLM as a Judge metrics&lt;/h3&gt; 
&lt;p&gt;The Python Opik SDK includes a number of LLM as a judge metrics to help you evaluate your LLM application. Learn more about it in the &lt;a href="https://www.comet.com/docs/opik/evaluation/metrics/overview/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=metrics_2_link&amp;amp;utm_campaign=opik"&gt;metrics documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To use them, simply import the relevant metric and use the &lt;code&gt;score&lt;/code&gt; function:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from opik.evaluation.metrics import Hallucination

metric = Hallucination()
score = metric.score(
    input="What is the capital of France?",
    output="Paris",
    context=["France is a country in Europe."]
)
print(score)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Opik also includes a number of pre-built heuristic metrics as well as the ability to create your own. Learn more about it in the &lt;a href="https://www.comet.com/docs/opik/evaluation/metrics/overview?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=metrics_3_link&amp;amp;utm_campaign=opik"&gt;metrics documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üîç Evaluating your LLM Application&lt;/h3&gt; 
&lt;p&gt;Opik allows you to evaluate your LLM application during development through &lt;a href="https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=datasets_2_link&amp;amp;utm_campaign=opik"&gt;Datasets&lt;/a&gt; and &lt;a href="https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=experiments_link&amp;amp;utm_campaign=opik"&gt;Experiments&lt;/a&gt;. The Opik Dashboard offers enhanced charts for experiments and better handling of large traces. You can also run evaluations as part of your CI/CD pipeline using our &lt;a href="https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&amp;amp;utm_source=opik&amp;amp;utm_medium=github&amp;amp;utm_content=pytest_2_link&amp;amp;utm_campaign=opik"&gt;PyTest integration&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;‚≠ê Star Us on GitHub&lt;/h2&gt; 
&lt;p&gt;If you find Opik useful, please consider giving us a star! Your support helps us grow our community and continue improving the product.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/comet-ml/opik"&gt;&lt;img src="https://api.star-history.com/svg?repos=comet-ml/opik&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;There are many ways to contribute to Opik:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Submit &lt;a href="https://github.com/comet-ml/opik/issues"&gt;bug reports&lt;/a&gt; and &lt;a href="https://github.com/comet-ml/opik/issues"&gt;feature requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Review the documentation and submit &lt;a href="https://github.com/comet-ml/opik/pulls"&gt;Pull Requests&lt;/a&gt; to improve it&lt;/li&gt; 
 &lt;li&gt;Speaking or writing about Opik and &lt;a href="https://chat.comet.com"&gt;letting us know&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Upvoting &lt;a href="https://github.com/comet-ml/opik/issues?q=is%3Aissue+is%3Aopen+label%3A%22enhancement%22"&gt;popular feature requests&lt;/a&gt; to show your support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To learn more about how to contribute to Opik, please see our &lt;a href="https://raw.githubusercontent.com/comet-ml/opik/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mindverse/Second-Me</title>
      <link>https://github.com/mindverse/Second-Me</link>
      <description>&lt;p&gt;Train your AI self, amplify you, bridge the world&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/mindverse/Second-Me/raw/master/images/cover.png" alt="Second Me" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.secondme.io/"&gt;&lt;img src="https://img.shields.io/badge/Second_Me-Homepage-blue?style=flat-square&amp;amp;logo=homebridge" alt="Homepage" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.18312"&gt;&lt;img src="https://img.shields.io/badge/AI--native_Memory-arXiv-orange?style=flat-square&amp;amp;logo=academia" alt="AI-native Memory" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2503.08102"&gt;&lt;img src="https://img.shields.io/badge/AI--native_Memory_2.0-arXiv-red?style=flat-square&amp;amp;logo=arxiv" alt="AI-native Memory 2.0" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/GpWHQNUwrg"&gt;&lt;img src="https://img.shields.io/badge/Chat-Discord-5865F2?style=flat-square&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/SecondMe_AI1"&gt;&lt;img src="https://img.shields.io/badge/Follow-@SecondMe_AI-1DA1F2?style=flat-square&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/SecondMeAI/"&gt;&lt;img src="https://img.shields.io/badge/Join-Reddit-FF4500?style=flat-square&amp;amp;logo=reddit&amp;amp;logoColor=white" alt="Reddit" /&gt;&lt;/a&gt; &lt;a href="https://secondme.gitbook.io/secondme/faq"&gt;&lt;img src="https://img.shields.io/badge/FAQ-GitBook-blue?style=flat-square" alt="View FAQ" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Our Vision&lt;/h2&gt; 
&lt;p&gt;Companies like OpenAI built "Super AI" that threatens human independence. We crave individuality: AI that amplifies, not erases, &lt;strong&gt;YOU&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;We‚Äôre challenging that with "&lt;strong&gt;Second Me&lt;/strong&gt;": an open-source prototype where you craft your own &lt;strong&gt;AI self&lt;/strong&gt;‚Äîa new AI species that preserves you, delivers your context, and defends your interests.&lt;/p&gt; 
&lt;p&gt;It‚Äôs &lt;strong&gt;locally trained and hosted&lt;/strong&gt;‚Äîyour data, your control‚Äîyet &lt;strong&gt;globally connected&lt;/strong&gt;, scaling your intelligence across an AI network. Beyond that, it‚Äôs your AI identity interface‚Äîa bold standard linking your AI to the world, sparks collaboration among AI selves, and builds tomorrow‚Äôs truly native AI apps.&lt;/p&gt; 
&lt;p&gt;Tech enthusiasts, AI pros, domain experts, Join us! Second Me is your launchpad to extend your mind into the digital horizon.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;Train Your AI Self&lt;/strong&gt; with AI-Native Memory (&lt;a href="https://arxiv.org/abs/2503.08102"&gt;Paper&lt;/a&gt;)&lt;/h3&gt; 
&lt;p&gt;Start training your Second Me today with your own memories! Using Hierarchical Memory Modeling (HMM) and the Me-Alignment Algorithm, your AI self captures your identity, understands your context, and reflects you authentically.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/a84c6135-26dc-4413-82aa-f4a373c0ff89" width="94%" /&gt; &lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Scale Your Intelligence&lt;/strong&gt; on the Second Me Network&lt;/h3&gt; 
&lt;p&gt;Launch your AI self from your laptop onto our decentralized network‚Äîanyone or any app can connect with your permission, sharing your context as your digital identity.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/9a74a3f4-d8fd-41c1-8f24-534ed94c842a" width="94%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Build Tomorrow‚Äôs Apps with Second Me&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Roleplay&lt;/strong&gt;: Your AI self switches personas to represent you in different scenarios.&lt;br /&gt; &lt;strong&gt;AI Space&lt;/strong&gt;: Collaborate with other Second Mes to spark ideas or solve problems.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/bc6125c1-c84f-4ecc-b620-8932cc408094" width="94%" /&gt; &lt;/p&gt; 
&lt;h3&gt;100% &lt;strong&gt;Privacy and Control&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Unlike traditional centralized AI systems, Second Me ensures that your information and intelligence remain local and completely private.&lt;/p&gt; 
&lt;h2&gt;Getting started &amp;amp; staying tuned with us&lt;/h2&gt; 
&lt;p&gt;Star and join us, and you will receive all release notifications from GitHub without any delay!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/5c14d956-f931-4c25-b0b3-3c2c96cd7581" width="94%" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;üìä Model Size vs. Memory (Reference Guide)&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Note: "B" in the table represents "billion parameters model". Data shown are examples only; actual supported model sizes may vary depending on system optimization, deployment environment, and other hardware/software conditions.&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Memory (GB)&lt;/th&gt; 
   &lt;th&gt;Docker Deployment (Windows/Linux)&lt;/th&gt; 
   &lt;th&gt;Docker Deployment (Mac)&lt;/th&gt; 
   &lt;th&gt;Integrated Setup (Windows/Linux)&lt;/th&gt; 
   &lt;th&gt;Integrated Setup (Mac)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;~0.8B (example)&lt;/td&gt; 
   &lt;td&gt;~0.4B (example)&lt;/td&gt; 
   &lt;td&gt;~1.0B (example)&lt;/td&gt; 
   &lt;td&gt;~0.6B (example)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;1.5B (example)&lt;/td&gt; 
   &lt;td&gt;0.5B (example)&lt;/td&gt; 
   &lt;td&gt;~2.0B (example)&lt;/td&gt; 
   &lt;td&gt;~0.8B (example)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;~2.8B (example)&lt;/td&gt; 
   &lt;td&gt;~1.2B (example)&lt;/td&gt; 
   &lt;td&gt;~3.5B (example)&lt;/td&gt; 
   &lt;td&gt;~1.5B (example)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Models below 0.5B may not provide satisfactory performance for complex tasks. And we're continuously improving cross-platform support - please &lt;a href="https://github.com/mindverse/Second-Me/issues/new"&gt;submit an issue&lt;/a&gt; for feedback or compatibility problems on different operating systems.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;MLX Acceleration&lt;/strong&gt;: Mac M-series users can use &lt;a href="https://github.com/mindverse/Second-Me/tree/master/lpm_kernel/L2/mlx_training"&gt;MLX&lt;/a&gt; to run larger models (CLI-only).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚ö° Get your Second Me running in just 3 steps:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Clone the repository
git clone https://github.com/mindverse/Second-Me.git
cd Second-Me
# 2. Start Docker containers
make docker-up
# 3. Access the web interface
# Open your browser and visit: http://localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üëâ For detailed instructions ‚Äî including integrated (non-Docker) setup, model selection, memory requirements, and platform-specific tips, check the full &lt;a href="https://secondme.gitbook.io/secondme/guides/deployment"&gt;Deployment Guide on GitBook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;‚ùì Got questions about setup, models, or any troubleshooting? &lt;a href="https://secondme.gitbook.io/secondme/faq"&gt;Check our FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tutorial and Use Cases&lt;/h2&gt; 
&lt;p&gt;üõ†Ô∏è Feel free to follow &lt;a href="https://secondme.gitbook.io/secondme/getting-started"&gt;User tutorial&lt;/a&gt; to build your Second Me.&lt;/p&gt; 
&lt;p&gt;üí° Check out the links below to see how Second Me can be used in real-life scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.secondme.io/example/ama"&gt;Felix AMA (Roleplay app)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.secondme.io/example/brainstorming"&gt;Brainstorming a 15-Day European City Itinerary (Network app)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.secondme.io/example/Icebreaker"&gt;Icebreaking as a Speed Dating Match (Network app)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's Next: May 2025&lt;/h2&gt; 
&lt;p&gt;Second Me continues to evolve as the open-source identity infrastructure for AI. Here's what's on deck for May:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üóÇÔ∏è &lt;strong&gt;Version Control&lt;/strong&gt;: Smarter versioning of memory and identity states&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;Continuous Training Pipelines&lt;/strong&gt;: Keep your AI self evolving over time, with ongoing updates based on new memory inputs.&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Performance &amp;amp; Stability Improvements&lt;/strong&gt;: Enhancements across inference ability, model alignment, and base model upgrades&lt;/li&gt; 
 &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;Cloud Solutions&lt;/strong&gt;: Explore cloud-based solutions for both model training (fine-tuning) and model deployment, to reduce the hardware burden on users' local machines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We‚Äôd love for you to help shape what‚Äôs coming next ‚Äî whether it‚Äôs fixing bugs, building new features, or improving docs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìò Check out our &lt;a href="https://raw.githubusercontent.com/mindverse/Second-Me/master/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; to get started&lt;/li&gt; 
 &lt;li&gt;üíª Submit ideas, issues, or PRs on &lt;a href="https://github.com/mindverse/Second-Me"&gt;GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ Join the conversation and stay updated in our &lt;a href="https://discord.gg/GpWHQNUwrg"&gt;Discord&lt;/a&gt; ‚Äî it‚Äôs where the community lives&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;We would like to express our gratitude to all the individuals who have contributed to Second Me! If you're interested in contributing to the future of intelligence uploading, whether through code, documentation, or ideas, please feel free to submit a pull request to our repository: &lt;a href="https://github.com/Mindverse/Second-Me"&gt;Second-Me&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/mindverse/Second-Me/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=mindverse/Second-Me" /&gt; &lt;/a&gt; 
&lt;p&gt;Made with &lt;a href="https://contrib.rocks"&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This work leverages the power of the open-source community.&lt;/p&gt; 
&lt;p&gt;For data synthesis, we utilized &lt;a href="https://github.com/microsoft/graphrag"&gt;GraphRAG&lt;/a&gt; from Microsoft.&lt;/p&gt; 
&lt;p&gt;For model deployment, we utilized &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt;, which provides efficient inference capabilities.&lt;/p&gt; 
&lt;p&gt;Our base models primarily come from the &lt;a href="https://huggingface.co/Qwen"&gt;Qwen2.5&lt;/a&gt; series.&lt;/p&gt; 
&lt;p&gt;We also want to extend our sincere gratitude to all users who have experienced Second Me. We recognize that there is significant room for optimization throughout the entire pipeline, and we are fully committed to iterative improvements to ensure everyone can enjoy the best possible experience locally.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Second Me is open source software licensed under the Apache License 2.0. See the &lt;a href="https://raw.githubusercontent.com/mindverse/Second-Me/master/LICENSE"&gt;LICENSE&lt;/a&gt; file for more details.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#mindverse/Second-Me&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mindverse/Second-Me&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mindverse/Second-Me&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mindverse/Second-Me&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>willccbb/verifiers</title>
      <link>https://github.com/willccbb/verifiers</link>
      <description>&lt;p&gt;Verifiers for LLM Reinforcement Learning&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Verifiers&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Environments for LLM Reinforcement Learning &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Verifiers is a library of modular components for creating RL environments and training LLM agents. Verifiers includes an async GRPO implementation built around the &lt;code&gt;transformers&lt;/code&gt; Trainer, is supported by &lt;code&gt;prime-rl&lt;/code&gt; for large-scale FSDP training, and can easily be integrated into any RL framework which exposes an OpenAI-compatible inference client. In addition to RL training, Verifiers can be used directly for building LLM evaluations, creating synthetic data pipelines, and implementing agent harnesses.&lt;/p&gt; 
&lt;p&gt;Full documentation is available &lt;a href="https://verifiers.readthedocs.io/en/latest/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;code&gt;verifiers&lt;/code&gt; with along &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; for dependency management in your own project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
uv init # create a fresh project
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For local (CPU) development and evaluation with API models, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add verifiers # uv add 'verifiers[dev]' for Jupyter + testing support
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For training on GPUs with &lt;code&gt;vf.GRPOTrainer&lt;/code&gt;, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add 'verifiers[all]' &amp;amp;&amp;amp; uv pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use the latest &lt;code&gt;main&lt;/code&gt; branch, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add verifiers @ git+https://github.com/willccbb/verifiers.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use with &lt;code&gt;prime-rl&lt;/code&gt;, see &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To install &lt;code&gt;verifiers&lt;/code&gt; from source for core library development, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/willccbb/verifiers.git
cd verifiers
uv sync --all-extras &amp;amp;&amp;amp; uv pip install flash-attn --no-build-isolation
uv run pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In general, we recommend that you build and train Environments &lt;em&gt;with&lt;/em&gt; &lt;code&gt;verifiers&lt;/code&gt;, not &lt;em&gt;in&lt;/em&gt; &lt;code&gt;verifiers&lt;/code&gt;. If you find yourself needing to clone and modify the core library in order to implement key functionality for your project, we'd love for you to open an issue so that we can try and streamline the development experience. Our aim is for &lt;code&gt;verifiers&lt;/code&gt; to be a reliable toolkit to build on top of, and to minimize the "fork proliferation" which often pervades the RL infrastructure ecosystem.&lt;/p&gt; 
&lt;h2&gt;Environments&lt;/h2&gt; 
&lt;p&gt;Environments in Verifiers are installable Python modules which can specify dependencies in a &lt;code&gt;pyproject.toml&lt;/code&gt;, and which expose a &lt;code&gt;load_environment&lt;/code&gt; function for instantiation by downstream applications (e.g. trainers). See &lt;code&gt;environments/&lt;/code&gt; for examples.&lt;/p&gt; 
&lt;p&gt;To initialize a blank Environment module template, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-init vf-environment-name # -p /path/to/environments (defaults to "./environments")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To an install an Environment module into your project, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-install vf-environment-name # -p /path/to/environments (defaults to "./environments") 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install an Environment module from this repo's &lt;code&gt;environments&lt;/code&gt; folder, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-install vf-math-python --from-repo # -b branch_or_commit (defaults to "main")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once an Environment module is installed, you can create an instance of the Environment using &lt;code&gt;load_environment&lt;/code&gt;, passing any necessary args:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.load_environment("vf-environment-name", **env_args)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run a quick evaluation of your Environment with an API-based model, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-eval vf-environment-name # vf-eval -h for config options; defaults to gpt-4.1-mini, 5 prompts, 3 rollouts for each
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The core elements of Environments in are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Datasets: a Hugging Face &lt;code&gt;Dataset&lt;/code&gt; with a &lt;code&gt;prompt&lt;/code&gt; column for inputs, and either &lt;code&gt;answer (str)&lt;/code&gt; or &lt;code&gt;info (dict)&lt;/code&gt; columns for evaluation&lt;/li&gt; 
 &lt;li&gt;Rollout logic: interactions between models and the environment (e.g. &lt;code&gt;env_response&lt;/code&gt; + &lt;code&gt;is_completed&lt;/code&gt; for any &lt;code&gt;MultiTurnEnv&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Rubrics: an encapsulation for one or more reward functions&lt;/li&gt; 
 &lt;li&gt;Parsers: optional; an encapsulation for reusable parsing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We support both &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style and &lt;code&gt;/v1/completions&lt;/code&gt;-style inference via OpenAI clients, though we generally recommend &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style inference for the vast majority of applications. Both the included &lt;code&gt;GRPOTrainer&lt;/code&gt; as well as &lt;code&gt;prime-rl&lt;/code&gt; support the full set of &lt;a href="https://docs.vllm.ai/en/v0.6.0/dev/sampling_params.html"&gt;SamplingParams&lt;/a&gt; exposed by vLLM (via their OpenAI-compatible &lt;a href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html"&gt;server&lt;/a&gt; interface), and leveraging this will often be the appropriate way to implement rollout strategies requiring finer-grained control, such as interrupting and resuming generations for interleaved tool use, or enforcing reasoning budgets.&lt;/p&gt; 
&lt;p&gt;The primary constraint we impose on rollout logic is that token sequences must be &lt;em&gt;increasing&lt;/em&gt;, i.e. once a token has been added to a model's context in a rollout, it must remain as the rollout progresses. Note that this causes issues with some popular reasoning models such as the Qwen3 and DeepSeek-R1-Distill series; see &lt;a href="https://raw.githubusercontent.com/willccbb/verifiers/main/#footguns"&gt;Footguns&lt;/a&gt; for guidance on adapting these models to support multi-turn rollouts.&lt;/p&gt; 
&lt;h3&gt;SingleTurnEnv&lt;/h3&gt; 
&lt;p&gt;For tasks requiring only a single response from a model for each prompt, you can use &lt;code&gt;SingleTurnEnv&lt;/code&gt; directly by specifying a Dataset and a Rubric. Rubrics are sets of reward functions, which can be either sync or async.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from datasets import load_dataset
import verifiers as vf

dataset = load_dataset("my-account/my-dataset", split="train")

def reward_A(prompt, completion, info) -&amp;gt; float:
	# reward fn, e.g. correctness
	...

def reward_B(parser, completion) -&amp;gt; float:
	# auxiliary reward fn, e.g. format
	...

async def metric(completion) -&amp;gt; float:
	# non-reward metric, e.g. proper noun count
	...

rubric = vf.Rubric(funcs=[reward_A, reward_B, metric], weights=[1.0, 0.5, 0.0])

vf_env = SingleTurnEnv(
	dataset=dataset,
	rubric=rubric
)
results = vf_env.evaluate(client=OpenAI(), model="gpt-4.1-mini", num_examples=100, rollouts_per_example=1)
vf_env.make_dataset(results) # HF dataset format
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Datasets should be formatted with columns for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;'prompt' (List[ChatMessage])&lt;/code&gt; OR &lt;code&gt;'question' (str)&lt;/code&gt; fields 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ChatMessage&lt;/code&gt; = e.g. &lt;code&gt;{'role': 'user', 'content': '...'}&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;if &lt;code&gt;question&lt;/code&gt; is set instead of &lt;code&gt;prompt&lt;/code&gt;, you can also pass &lt;code&gt;system_prompt (str)&lt;/code&gt; and/or &lt;code&gt;few_shot (List[ChatMessage])&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer (str)&lt;/code&gt; AND/OR &lt;code&gt;info (dict)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task (str)&lt;/code&gt;: optional, used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt; for orchestrating composition of Environments and Rubrics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following named attributes available for use by reward functions in your Rubric:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: sequence of input messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;completion&lt;/code&gt;: sequence of messages generated during rollout by model and Environment&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer&lt;/code&gt;: primary answer column, optional if &lt;code&gt;info&lt;/code&gt; is used&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;state&lt;/code&gt;: can be modified during rollout to accumulate any metadata (&lt;code&gt;state['responses']&lt;/code&gt; includes full OpenAI response objects by default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;info&lt;/code&gt;: auxiliary info needed for reward computation (e.g. test cases), optional if &lt;code&gt;answer&lt;/code&gt; is used&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task&lt;/code&gt;: tag for task type (used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;parser&lt;/code&gt;: the parser object declared. Note: &lt;code&gt;vf.Parser().get_format_reward_func()&lt;/code&gt; is a no-op (always 1.0); use &lt;code&gt;vf.ThinkParser&lt;/code&gt; or a custom parser if you want a real format adherence reward.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For tasks involving LLM judges, you may wish to use &lt;code&gt;vf.JudgeRubric()&lt;/code&gt; for managing requests to auxiliary models.&lt;/p&gt; 
&lt;p&gt;Note on concurrency: environment APIs accept &lt;code&gt;max_concurrent&lt;/code&gt; to control parallel rollouts. The &lt;code&gt;vf-eval&lt;/code&gt; CLI currently exposes &lt;code&gt;--max-concurrent-requests&lt;/code&gt;; ensure this maps to your environment‚Äôs concurrency as expected.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;vf-eval&lt;/code&gt; also supports specifying &lt;code&gt;sampling_args&lt;/code&gt; as a JSON object, which is sent to the vLLM inference engine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-eval vf-environment-name --sampling-args '{"reasoning_effort": "low"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;vf-eval -s&lt;/code&gt; to save outputs as dataset-formatted JSON, and view all locally-saved eval results with &lt;code&gt;vf-tui&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;ToolEnv&lt;/h3&gt; 
&lt;p&gt;For many applications involving tool use, you can use &lt;code&gt;ToolEnv&lt;/code&gt; to leverage models' native tool/function-calling capabilities in an agentic loop. Tools can be specified as generic Python functions (with type hints and docstrings), which will then be passed in JSON schema form to each inference request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.ToolEnv(
	dataset= ... # HF Dataset with 'prompt'/'question' + 'answer'/'info' columns
	rubric= ... # Rubric object; vf.ToolRubric() can be optionally used for counting tool invocations in each rollout
	tools=[search_tool, read_article_tool, python_tool], # python functions with type hints + docstrings
	max_turns=10
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In cases where your tools require heavy computational resources, we recommend hosting your tools as standalone servers (e.g. MCP servers) and creating lightweight wrapper functions to pass to &lt;code&gt;ToolEnv&lt;/code&gt;. Parallel tool call support is enabled by default.&lt;/p&gt; 
&lt;p&gt;For training, or self-hosted endpoints, you'll want to enable auto tool choice in &lt;a href="https://docs.vllm.ai/en/stable/features/tool_calling.html#automatic-function-calling"&gt;vLLM&lt;/a&gt; with the appropriate parser. If your model does not support native tool calling, you may find the &lt;code&gt;XMLParser&lt;/code&gt; abstraction useful for rolling your own tool call parsing on top of &lt;code&gt;MultiTurnEnv&lt;/code&gt;; see &lt;code&gt;environments/xml_tool_env&lt;/code&gt; for an example.&lt;/p&gt; 
&lt;h3&gt;MultiTurnEnv&lt;/h3&gt; 
&lt;p&gt;Both &lt;code&gt;SingleTurnEnv&lt;/code&gt; and &lt;code&gt;ToolEnv&lt;/code&gt; are instances of &lt;code&gt;MultiTurnEnv&lt;/code&gt;, which exposes an interface for writing custom Environment interaction protocols. The two methods you must override are&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from typing import Tuple
import verifiers as vf
from verifiers.types import Messages, State
class YourMultiTurnEnv(vf.MultiTurnEnv):
    def __init__(self,
                 dataset: Dataset,
                 rubric: Rubric,
				 max_turns: int,
                 **kwargs):
	
  async def is_completed(self, messages: Messages, state: State, **kwargs) -&amp;gt; bool:
    # return whether or not a rollout is completed

  async def env_response(self, messages: Messages, state: State, **kwargs) -&amp;gt; Tuple[Messages, State]:
    # return new environment message(s) + updated state
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your application requires more fine-grained control than is allowed by &lt;code&gt;MultiTurnEnv&lt;/code&gt;, you may want to inherit from the base &lt;code&gt;Environment&lt;/code&gt; functionality directly and override the &lt;code&gt;rollout&lt;/code&gt; method.&lt;/p&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;h3&gt;GRPOTrainer&lt;/h3&gt; 
&lt;p&gt;The included trainer (&lt;code&gt;vf.GRPOTrainer&lt;/code&gt;) supports running GRPO-style RL training via Accelerate/DeepSpeed, and uses vLLM for inference. It supports both full-parameter finetuning, and is optimized for efficiently training dense transformer models on 2-16 GPUs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install environment
vf-install vf-wordle (-p /path/to/environments | --from-repo)

# quick eval
vf-eval vf-wordle -m (model_name in configs/endpoints.py) -n NUM_EXAMPLES -r ROLLOUTS_PER_EXAMPLE

# inference (shell 0)
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model willcb/Qwen3-1.7B-Wordle \
    --data-parallel-size 7 --enforce-eager --disable-log-requests

# training (shell 1)
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_wordle.py --size 1.7B
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can train environments with the external &lt;code&gt;prime-rl&lt;/code&gt; project (FSDP-first orchestration). See the &lt;code&gt;prime-rl&lt;/code&gt; README for installation and examples. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# orchestrator config (prime-rl)
[environment]
id = "vf-math-python"  # or your environment ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# run (prime-rl)
uv run rl \
  --trainer @ configs/your_exp/train.toml \
  --orchestrator @ configs/your_exp/orch.toml \
  --inference @ configs/your_exp/infer.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ensure your &lt;code&gt;wandb&lt;/code&gt; and &lt;code&gt;huggingface-cli&lt;/code&gt; logins are set up (or set &lt;code&gt;report_to=None&lt;/code&gt; in &lt;code&gt;training_args&lt;/code&gt;). You should also have something set as your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in your environment (can be a dummy key for vLLM).&lt;/li&gt; 
 &lt;li&gt;If using high max concurrency, increase the number of allowed open sockets (e.g. &lt;code&gt;ulimit -n 4096&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;On some setups, inter-GPU communication can &lt;a href="https://github.com/huggingface/trl/issues/2923"&gt;hang&lt;/a&gt; or crash during vLLM weight syncing. This can usually be alleviated by setting (or unsetting) &lt;code&gt;NCCL_P2P_DISABLE=1&lt;/code&gt; in your environment (or potentially &lt;code&gt;NCCL_CUMEM_ENABLE=1&lt;/code&gt;). Try this as your first step if you experience NCCL-related issues.&lt;/li&gt; 
 &lt;li&gt;If problems persist, please open an &lt;a href="https://github.com/willccbb/verifiers/issues"&gt;issue&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resource Requirements&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;GRPOTrainer&lt;/code&gt; is optimized for setups with at least 2 GPUs, scaling up to multiple nodes. 2-GPU setups with sufficient memory to enable small-scale experimentation can be &lt;a href="https://app.primeintellect.ai/dashboard/create-cluster?image=ubuntu_22_cuda_12"&gt;rented&lt;/a&gt; for &amp;lt;$1/hr.&lt;/p&gt; 
&lt;h3&gt;PRIME-RL&lt;/h3&gt; 
&lt;p&gt;If you do not require LoRA support, you may want to use the &lt;code&gt;prime-rl&lt;/code&gt; trainer, which natively supports Environments created using &lt;code&gt;verifiers&lt;/code&gt;, is more optimized for performance and scalability via FSDP, includes a broader set of configuration options and user experience features, and has more battle-tested defaults. Both trainers support asynchronous rollouts, and use a one-step off-policy delay by default for overlapping training and inference. See the &lt;code&gt;prime-rl&lt;/code&gt; &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;docs&lt;/a&gt; for usage instructions.&lt;/p&gt; 
&lt;h2&gt;Further Documentation&lt;/h2&gt; 
&lt;p&gt;See the full &lt;a href="https://verifiers.readthedocs.io/en/latest/"&gt;docs&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Verifiers warmly welcomes community contributions! Please open an issue or PR if you encounter bugs or other pain points during your development, or start a discussion for more open-ended questions.&lt;/p&gt; 
&lt;p&gt;Please note that the core &lt;code&gt;verifiers/&lt;/code&gt; library is intended to be a relatively lightweight set of reusable components rather than an exhaustive catalog of RL environments. For &lt;em&gt;applications&lt;/em&gt; of &lt;code&gt;verifiers&lt;/code&gt; (e.g. "an Environment for XYZ task"), you are welcome to submit a PR for a self-contained module that lives within &lt;code&gt;environments/&lt;/code&gt; if it serves as a canonical example of a new pattern. Stay tuned for more info shortly about our plans for supporting community Environment contributions üôÇ&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use this code in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{brown_verifiers_2025,
  author       = {William&amp;nbsp;Brown},
  title        = {{Verifiers}: Reinforcement Learning with LLMs in Verifiable Environments},
  howpublished = {\url{https://github.com/willccbb/verifiers}},
  note         = {Commit abcdefg ‚Ä¢ accessed DD‚ÄØMon‚ÄØYYYY},
  year         = {2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A community Environments hub for crowdsourcing, sharing, and discovering new RL environments built with &lt;code&gt;verifiers&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Default patterns for hosted resources such as code sandboxes, auxiliary models, and MCP servers&lt;/li&gt; 
 &lt;li&gt;Multimodal input support&lt;/li&gt; 
 &lt;li&gt;Non-increasing token sequences via REINFORCE&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>MODSetter/SurfSense</title>
      <link>https://github.com/MODSetter/SurfSense</link>
      <description>&lt;p&gt;Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65" alt="new_header" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://discord.gg/ejRNvftDp9"&gt; &lt;img src="https://img.shields.io/discord/1359368468260192417" alt="Discord" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;SurfSense&lt;/h1&gt; 
&lt;p&gt;While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Google Calendar and more to come.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13606" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13606" alt="MODSetter%2FSurfSense | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da"&gt;https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Podcast Sample&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7"&gt;https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;üí° &lt;strong&gt;Idea&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.&lt;/p&gt; 
&lt;h3&gt;üìÅ &lt;strong&gt;Multiple File Format Uploading Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Save content from your own personal files &lt;em&gt;(Documents, images, videos and supports &lt;strong&gt;50+ file extensions&lt;/strong&gt;)&lt;/em&gt; to your own personal knowledge base .&lt;/p&gt; 
&lt;h3&gt;üîç &lt;strong&gt;Powerful Search&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Quickly research or find anything in your saved content .&lt;/p&gt; 
&lt;h3&gt;üí¨ &lt;strong&gt;Chat with your Saved Content&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Interact in Natural Language and get cited answers.&lt;/p&gt; 
&lt;h3&gt;üìÑ &lt;strong&gt;Cited Answers&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Get Cited answers just like Perplexity.&lt;/p&gt; 
&lt;h3&gt;üîî &lt;strong&gt;Privacy &amp;amp; Local LLM Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Works Flawlessly with Ollama local LLMs.&lt;/p&gt; 
&lt;h3&gt;üè† &lt;strong&gt;Self Hostable&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Open source and easy to deploy locally.&lt;/p&gt; 
&lt;h3&gt;üéôÔ∏è Podcasts&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; 
 &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; 
 &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; 
 &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìä &lt;strong&gt;Advanced RAG Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports 100+ LLM's&lt;/li&gt; 
 &lt;li&gt;Supports 6000+ Embedding Models.&lt;/li&gt; 
 &lt;li&gt;Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)&lt;/li&gt; 
 &lt;li&gt;Uses Hierarchical Indices (2 tiered RAG setup).&lt;/li&gt; 
 &lt;li&gt;Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).&lt;/li&gt; 
 &lt;li&gt;RAG as a Service API Backend.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚ÑπÔ∏è &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; 
 &lt;li&gt;Slack&lt;/li&gt; 
 &lt;li&gt;Linear&lt;/li&gt; 
 &lt;li&gt;Jira&lt;/li&gt; 
 &lt;li&gt;ClickUp&lt;/li&gt; 
 &lt;li&gt;Confluence&lt;/li&gt; 
 &lt;li&gt;Notion&lt;/li&gt; 
 &lt;li&gt;Gmail&lt;/li&gt; 
 &lt;li&gt;Youtube Videos&lt;/li&gt; 
 &lt;li&gt;GitHub&lt;/li&gt; 
 &lt;li&gt;Discord&lt;/li&gt; 
 &lt;li&gt;Google Calendar&lt;/li&gt; 
 &lt;li&gt;and more to come.....&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ &lt;strong&gt;Supported File Extensions&lt;/strong&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Documents &amp;amp; Text&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.docm&lt;/code&gt;, &lt;code&gt;.dot&lt;/code&gt;, &lt;code&gt;.dotm&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.wpd&lt;/code&gt;, &lt;code&gt;.pages&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.602&lt;/code&gt;, &lt;code&gt;.abw&lt;/code&gt;, &lt;code&gt;.cgm&lt;/code&gt;, &lt;code&gt;.cwk&lt;/code&gt;, &lt;code&gt;.hwp&lt;/code&gt;, &lt;code&gt;.lwp&lt;/code&gt;, &lt;code&gt;.mw&lt;/code&gt;, &lt;code&gt;.mcw&lt;/code&gt;, &lt;code&gt;.pbd&lt;/code&gt;, &lt;code&gt;.sda&lt;/code&gt;, &lt;code&gt;.sdd&lt;/code&gt;, &lt;code&gt;.sdp&lt;/code&gt;, &lt;code&gt;.sdw&lt;/code&gt;, &lt;code&gt;.sgl&lt;/code&gt;, &lt;code&gt;.sti&lt;/code&gt;, &lt;code&gt;.sxi&lt;/code&gt;, &lt;code&gt;.sxw&lt;/code&gt;, &lt;code&gt;.stw&lt;/code&gt;, &lt;code&gt;.sxg&lt;/code&gt;, &lt;code&gt;.uof&lt;/code&gt;, &lt;code&gt;.uop&lt;/code&gt;, &lt;code&gt;.uot&lt;/code&gt;, &lt;code&gt;.vor&lt;/code&gt;, &lt;code&gt;.wps&lt;/code&gt;, &lt;code&gt;.zabw&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;, &lt;code&gt;.markdown&lt;/code&gt;, &lt;code&gt;.rst&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.org&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.xhtml&lt;/code&gt;, &lt;code&gt;.adoc&lt;/code&gt;, &lt;code&gt;.asciidoc&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Presentations&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;, &lt;code&gt;.pptm&lt;/code&gt;, &lt;code&gt;.pot&lt;/code&gt;, &lt;code&gt;.potm&lt;/code&gt;, &lt;code&gt;.potx&lt;/code&gt;, &lt;code&gt;.odp&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Spreadsheets &amp;amp; Data&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsm&lt;/code&gt;, &lt;code&gt;.xlsb&lt;/code&gt;, &lt;code&gt;.xlw&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;, &lt;code&gt;.ods&lt;/code&gt;, &lt;code&gt;.fods&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.dbf&lt;/code&gt;, &lt;code&gt;.123&lt;/code&gt;, &lt;code&gt;.dif&lt;/code&gt;, &lt;code&gt;.sylk&lt;/code&gt;, &lt;code&gt;.slk&lt;/code&gt;, &lt;code&gt;.prn&lt;/code&gt;, &lt;code&gt;.et&lt;/code&gt;, &lt;code&gt;.uos1&lt;/code&gt;, &lt;code&gt;.uos2&lt;/code&gt;, &lt;code&gt;.wk1&lt;/code&gt;, &lt;code&gt;.wk2&lt;/code&gt;, &lt;code&gt;.wk3&lt;/code&gt;, &lt;code&gt;.wk4&lt;/code&gt;, &lt;code&gt;.wks&lt;/code&gt;, &lt;code&gt;.wq1&lt;/code&gt;, &lt;code&gt;.wq2&lt;/code&gt;, &lt;code&gt;.wb1&lt;/code&gt;, &lt;code&gt;.wb2&lt;/code&gt;, &lt;code&gt;.wb3&lt;/code&gt;, &lt;code&gt;.qpw&lt;/code&gt;, &lt;code&gt;.xlr&lt;/code&gt;, &lt;code&gt;.eth&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Images&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.gif&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.svg&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.web&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.heic&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Audio &amp;amp; Video &lt;em&gt;(Always Supported)&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;.mp3&lt;/code&gt;, &lt;code&gt;.mpga&lt;/code&gt;, &lt;code&gt;.m4a&lt;/code&gt;, &lt;code&gt;.wav&lt;/code&gt;, &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.mpeg&lt;/code&gt;, &lt;code&gt;.webm&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Email &amp;amp; Communication&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.eml&lt;/code&gt;, &lt;code&gt;.msg&lt;/code&gt;, &lt;code&gt;.p7s&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;üîñ Cross Browser Extension&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The SurfSense extension can be used to save any webpage you like.&lt;/li&gt; 
 &lt;li&gt;Its main usecase is to save any webpages protected beyond authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FEATURE REQUESTS AND FUTURE&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;SurfSense is actively being developed.&lt;/strong&gt; While it's not yet production-ready, you can help us speed up the process.&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.gg/ejRNvftDp9"&gt;SurfSense Discord&lt;/a&gt; and help shape the future of SurfSense!&lt;/p&gt; 
&lt;h2&gt;üöÄ Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with our development progress and upcoming features!&lt;br /&gt; Check out our public roadmap and contribute your ideas or feedback:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;View the Roadmap:&lt;/strong&gt; &lt;a href="https://github.com/users/MODSetter/projects/2"&gt;SurfSense Roadmap on GitHub Projects&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to get started?&lt;/h2&gt; 
&lt;h3&gt;Installation Options&lt;/h3&gt; 
&lt;p&gt;SurfSense provides two installation methods:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/docker-installation"&gt;Docker Installation&lt;/a&gt;&lt;/strong&gt; - The easiest way to get SurfSense up and running with all dependencies containerized.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Includes pgAdmin for database management through a web UI&lt;/li&gt; 
   &lt;li&gt;Supports environment variable customization via &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Flexible deployment options (full stack or core services only)&lt;/li&gt; 
   &lt;li&gt;No need to manually edit configuration files between environments&lt;/li&gt; 
   &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DOCKER_SETUP.md"&gt;Docker Setup Guide&lt;/a&gt; for detailed instructions&lt;/li&gt; 
   &lt;li&gt;For deployment scenarios and options, see &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DEPLOYMENT_GUIDE.md"&gt;Deployment Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/manual-installation"&gt;Manual Installation (Recommended)&lt;/a&gt;&lt;/strong&gt; - For users who prefer more control over their setup or need to customize their deployment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Both installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.&lt;/p&gt; 
&lt;p&gt;Before installation, make sure to complete the &lt;a href="https://www.surfsense.net/docs/"&gt;prerequisite setup steps&lt;/a&gt; including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PGVector setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Processing ETL Service&lt;/strong&gt; (choose one): 
  &lt;ul&gt; 
   &lt;li&gt;Unstructured.io API key (supports 34+ formats)&lt;/li&gt; 
   &lt;li&gt;LlamaIndex API key (enhanced parsing, supports 50+ formats)&lt;/li&gt; 
   &lt;li&gt;Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other required API keys&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4" alt="updated_researcher" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Search Spaces&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099" alt="search_spaces" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Manage Documents&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d" alt="documents" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Podcast Agent&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c" alt="podcasts" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agent Chat&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491" alt="git_chat" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40" alt="ext1" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7" alt="ext2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;BackEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;: Modern, fast web framework for building APIs with Python&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PostgreSQL with pgvector&lt;/strong&gt;: Database with vector search capabilities for similarity searches&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SQLAlchemy&lt;/strong&gt;: SQL toolkit and ORM (Object-Relational Mapping) for database interactions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alembic&lt;/strong&gt;: A database migrations tool for SQLAlchemy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI Users&lt;/strong&gt;: Authentication and user management with JWT and OAuth support&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: Framework for developing AI-agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: Framework for developing AI-powered applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Integration&lt;/strong&gt;: Integration with LLM models through LiteLLM&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rerankers&lt;/strong&gt;: Advanced result ranking for improved search relevance&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt;: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector Embeddings&lt;/strong&gt;: Document and text embeddings for semantic search&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgvector&lt;/strong&gt;: PostgreSQL extension for efficient vector similarity operations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chonkie&lt;/strong&gt;: Advanced document chunking and embedding library&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Uses &lt;code&gt;AutoEmbeddings&lt;/code&gt; for flexible embedding model selection&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;LateChunker&lt;/code&gt; for optimized document chunking based on embedding model's max sequence length&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;FrontEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next.js 15.2.3&lt;/strong&gt;: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React 19.0.0&lt;/strong&gt;: JavaScript library for building user interfaces.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: Static type-checking for JavaScript, enhancing code quality and developer experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vercel AI SDK Kit UI Stream Protocol&lt;/strong&gt;: To create scalable chat UI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tailwind CSS 4.x&lt;/strong&gt;: Utility-first CSS framework for building custom UI designs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Shadcn&lt;/strong&gt;: Headless components library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lucide React&lt;/strong&gt;: Icon set implemented as React components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Framer Motion&lt;/strong&gt;: Animation library for React.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sonner&lt;/strong&gt;: Toast notification library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Geist&lt;/strong&gt;: Font family from Vercel.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React Hook Form&lt;/strong&gt;: Form state management and validation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zod&lt;/strong&gt;: TypeScript-first schema validation with static type inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@hookform/resolvers&lt;/strong&gt;: Resolvers for using validation libraries with React Hook Form.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@tanstack/react-table&lt;/strong&gt;: Headless UI for building powerful tables &amp;amp; datagrids.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;DevOps&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Container platform for consistent deployment across environments&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;: Tool for defining and running multi-container Docker applications&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgAdmin&lt;/strong&gt;: Web-based PostgreSQL administration tool included in Docker setup&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Extension&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Manifest v3 on Plasmo&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add More Connectors.&lt;/li&gt; 
 &lt;li&gt;Patch minor bugs.&lt;/li&gt; 
 &lt;li&gt;Document Chat &lt;strong&gt;[REIMPLEMENT]&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Document Podcasts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome! A contribution can be as small as a ‚≠ê or even finding and creating issues. Fine-tuning the Backend is always desired.&lt;/p&gt; 
&lt;p&gt;For detailed contribution guidelines, please see our &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#MODSetter/SurfSense&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>ansible/ansible</title>
      <link>https://github.com/ansible/ansible</link>
      <description>&lt;p&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible-core"&gt;&lt;img src="https://img.shields.io/pypi/v/ansible-core.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?sanitize=true" alt="Docs badge" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;&lt;img src="https://img.shields.io/badge/chat-IRC-brightgreen.svg?sanitize=true" alt="Chat badge" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;amp;branchName=devel"&gt;&lt;img src="https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/code_of_conduct.html"&gt;&lt;img src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg?sanitize=true" alt="Ansible Code of Conduct" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information"&gt;&lt;img src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg?sanitize=true" alt="Ansible mailing lists" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;&lt;img src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg?sanitize=true" alt="Repository License" /&gt;&lt;/a&gt; &lt;a href="https://bestpractices.coreinfrastructure.org/projects/2372"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/2372/badge" alt="Ansible CII Best Practices certification" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Ansible&lt;/h1&gt; 
&lt;p&gt;Ansible is a radically simple IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible &lt;a href="https://ansible.com/"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Design Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have an extremely simple setup process with a minimal learning curve.&lt;/li&gt; 
 &lt;li&gt;Manage machines quickly and in parallel.&lt;/li&gt; 
 &lt;li&gt;Avoid custom-agents and additional open ports, be agentless by leveraging the existing SSH daemon.&lt;/li&gt; 
 &lt;li&gt;Describe infrastructure in a language that is both machine and human friendly.&lt;/li&gt; 
 &lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt; 
 &lt;li&gt;Manage new remote machines instantly, without bootstrapping any software.&lt;/li&gt; 
 &lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt; 
 &lt;li&gt;Be usable as non-root.&lt;/li&gt; 
 &lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use Ansible&lt;/h2&gt; 
&lt;p&gt;You can install a released version of Ansible with &lt;code&gt;pip&lt;/code&gt; or a package manager. See our &lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html"&gt;installation guide&lt;/a&gt; for details on installing Ansible on a variety of platforms.&lt;/p&gt; 
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest features and fixes, directly. Although it is reasonably stable, you are more likely to encounter breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Communication&lt;/h2&gt; 
&lt;p&gt;Join the Ansible forum to ask questions, get help, and interact with the community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/help/6"&gt;Get Help&lt;/a&gt;: Find help or share your Ansible knowledge to help others. Use tags to filter and subscribe to posts, such as the following: 
  &lt;ul&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible"&gt;ansible&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible-core"&gt;ansible-core&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/playbook"&gt;playbook&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/chat/4"&gt;Social Spaces&lt;/a&gt;: Meet and interact with fellow enthusiasts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/news/5"&gt;News &amp;amp; Announcements&lt;/a&gt;: Track project-wide announcements including social events.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn"&gt;Bullhorn newsletter&lt;/a&gt;: Get release announcements and important changes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more ways to get in touch, see &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;Communicating with the Ansible community&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribute to Ansible&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out the &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/.github/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/devel/community"&gt;Community Information&lt;/a&gt; for all kinds of ways to contribute to and interact with the project, including how to submit bug reports and code to Ansible.&lt;/li&gt; 
 &lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt; 
 &lt;li&gt;Talk to us before making larger changes to avoid duplicate efforts. This not only helps everyone know what is going on, but it also helps save time and effort if we decide some changes are needed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Coding Guidelines&lt;/h2&gt; 
&lt;p&gt;We document our Coding Guidelines in the &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/"&gt;Developer Guide&lt;/a&gt;. We particularly suggest you review:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html"&gt;Contributing your module to Ansible&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html"&gt;Conventions, tips, and pitfalls&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Branch Info&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt; 
 &lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8). The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt; and has contributions from over 5000 users (and growing). Thanks everyone!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;GNU General Public License v3.0 or later&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pathwaycom/pathway</title>
      <link>https://github.com/pathwaycom/pathway</link>
      <description>&lt;p&gt;Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://pathway.com/"&gt; &lt;img src="https://pathway.com/logo-light.svg?sanitize=true" /&gt; &lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://trendshift.io/repositories/10388" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10388" alt="pathwaycom%2Fpathway | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg?sanitize=true" alt="ubuntu" /&gt; &lt;br /&gt; &lt;/a&gt;&lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/release.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg?sanitize=true" alt="Last release" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://badge.fury.io/py/pathway.svg?sanitize=true" alt="PyPI version" height="18" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://static.pepy.tech/badge/pathway" alt="PyPI downloads" height="18" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt; &lt;img src="https://img.shields.io/badge/license-BSL-green" alt="License: BSL" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://discord.gg/pathway"&gt; &lt;img src="https://img.shields.io/discord/1042405378304004156?logo=discord" alt="chat on Discord" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=pathway_com"&gt; &lt;img src="https://img.shields.io/twitter/follow/pathwaycom" alt="follow on Twitter" /&gt;&lt;/a&gt; &lt;a href="https://linkedin.com/company/pathway"&gt; &lt;img src="https://img.shields.io/badge/pathway-0077B5?style=social&amp;amp;logo=linkedin" alt="follow on LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dylanhogg/awesome-python/raw/main/README.md"&gt; &lt;img src="https://awesome.re/badge.svg?sanitize=true" alt="Awesome Python" /&gt;&lt;/a&gt; &lt;a href="https://gurubase.io/g/pathway"&gt; &lt;img src="https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF" alt="Pathway Guru" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#getting-started"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#deployment"&gt;Deployment&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#resources"&gt;Documentation and Support&lt;/a&gt; | &lt;a href="https://pathway.com/blog/"&gt;Blog&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#license"&gt;License&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Pathway&lt;a id="pathway"&gt; Live Data Framework&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pathway.com"&gt;Pathway&lt;/a&gt; is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt; 
&lt;p&gt;Pathway comes with an &lt;strong&gt;easy-to-use Python API&lt;/strong&gt;, allowing you to seamlessly integrate your favorite Python ML libraries. Pathway code is versatile and robust: &lt;strong&gt;you can use it in both development and production environments, handling both batch and streaming data effectively&lt;/strong&gt;. The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.&lt;/p&gt; 
&lt;p&gt;Pathway is powered by a &lt;strong&gt;scalable Rust engine&lt;/strong&gt; based on Differential Dataflow and performs incremental computation. Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations. All the pipeline is kept in memory and can be easily deployed with &lt;strong&gt;Docker and Kubernetes&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;You can install Pathway with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For any questions, you will find the community and team behind the project &lt;a href="https://discord.com/invite/pathway"&gt;on Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Use-cases and templates&lt;/h2&gt; 
&lt;p&gt;Ready to see what Pathway can do?&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pathway.com/developers/templates"&gt;Try one of our easy-to-run examples&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!&lt;/p&gt; 
&lt;h3&gt;Event processing and real-time analytics pipelines&lt;/h3&gt; 
&lt;p&gt;With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/kafka-etl"&gt;Showcase: Real-time ETL.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/realtime-log-monitoring"&gt;Showcase: Event-driven pipelines with alerting.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/linear_regression_with_kafka/"&gt;Showcase: Realtime analytics.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming"&gt;Docs: Switch from batch to streaming.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AI Pipelines&lt;/h3&gt; 
&lt;p&gt;Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/overview"&gt;LLM xpack documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Don't hesitate to try one of our runnable examples featuring LLM tooling. You can find such examples &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/llm-examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/unstructured-to-structured/"&gt;Template: Unstructured data to SQL on-the-fly.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/private-rag-ollama-mistral"&gt;Template: Private RAG with Ollama and Mistral AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/adaptive-rag"&gt;Template: Adaptive RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/multimodal-rag"&gt;Template: Multimodal RAG with gpt-4o&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A wide range of connectors&lt;/strong&gt;: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stateless and stateful transformations&lt;/strong&gt;: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Persistence&lt;/strong&gt;: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the "at least once" consistency while the enterprise version provides the "exactly once" consistency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Rust engine&lt;/strong&gt;: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM helpers&lt;/strong&gt;: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;a id="getting-started"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;a id="installation"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Pathway requires Python 3.10 or above.&lt;/p&gt; 
&lt;p&gt;You can install the current release of Pathway using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚ö†Ô∏è Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.&lt;/p&gt; 
&lt;h3&gt;Example: computing the sum of positive values in real time.&lt;a id="example"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  "./input/",
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&amp;gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, "output.jsonl")

# Run the computation
pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run Pathway &lt;a href="https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing"&gt;in Google Colab&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find more examples &lt;a href="https://github.com/pathwaycom/pathway/tree/main/examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Deployment&lt;a id="deployment"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Locally&lt;a id="running-pathway-locally"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;To use Pathway, you only need to import it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then run your Pathway project (say, &lt;code&gt;main.py&lt;/code&gt;) just like a normal Python script: &lt;code&gt;$ python main.py&lt;/code&gt;. Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages.&lt;/p&gt; 
&lt;img src="https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png" width="1326" alt="Pathway dashboard" /&gt; 
&lt;p&gt;Alternatively, you can use the pathway'ish version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pathway natively supports multithreading. To launch your application with 3 threads, you can do as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn --threads 3 python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To jumpstart a Pathway project, you can use our &lt;a href="https://github.com/pathwaycom/cookiecutter-pathway"&gt;cookiecutter template&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;a id="docker"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily run Pathway using docker.&lt;/p&gt; 
&lt;h4&gt;Pathway image&lt;/h4&gt; 
&lt;p&gt;You can use the &lt;a href="https://hub.docker.com/r/pathwaycom/pathway"&gt;Pathway docker image&lt;/a&gt;, using a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ "python", "./your-script.py" ]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then build and run the Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run a single Python script&lt;/h4&gt; 
&lt;p&gt;When dealing with single-file projects, creating a full-fledged &lt;code&gt;Dockerfile&lt;/code&gt; might seem unnecessary. In such scenarios, you can execute a Python script directly using the Pathway Docker image. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker run -it --rm --name my-pathway-app -v "$PWD":/app pathwaycom/pathway:latest python my-pathway-app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Python docker image&lt;/h4&gt; 
&lt;p&gt;You can also use a standard Python image and install Pathway using pip with a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD ["python", "-u", "pathway-script.py"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Kubernetes and cloud&lt;a id="k8s"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Docker containers are ideally suited for deployment on the cloud with Kubernetes. If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise. Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics. It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.&lt;/p&gt; 
&lt;p&gt;You can easily deploy Pathway using services like Render: see &lt;a href="https://pathway.com/developers/user-guide/deployment/render-deploy/"&gt;how to deploy Pathway in a few clicks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested, don't hesitate to &lt;a href="mailto:contact@pathway.com"&gt;contact us&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;a id="performance"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).&lt;/p&gt; 
&lt;p&gt;If you are curious, here are &lt;a href="https://github.com/pathwaycom/pathway-benchmarks"&gt;some benchmarks to play with&lt;/a&gt;.&lt;/p&gt; 
&lt;img src="https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png" width="1326" alt="WordCount Graph" /&gt; 
&lt;h2&gt;Documentation and Support&lt;a id="resources"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;The entire documentation of Pathway is available at &lt;a href="https://pathway.com/developers/user-guide/introduction/welcome"&gt;pathway.com/developers/&lt;/a&gt;, including the &lt;a href="https://pathway.com/developers/api-docs/pathway"&gt;API Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you have any question, don't hesitate to &lt;a href="https://github.com/pathwaycom/pathway/issues"&gt;open an issue on GitHub&lt;/a&gt;, join us on &lt;a href="https://discord.com/invite/pathway"&gt;Discord&lt;/a&gt;, or send us an email at &lt;a href="mailto:contact@pathway.com"&gt;contact@pathway.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;a id="license"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is distributed on a &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt;BSL 1.1 License&lt;/a&gt; which allows for unlimited non-commercial use, as well as use of the Pathway package &lt;a href="https://pathway.com/license/"&gt;for most commercial purposes&lt;/a&gt;, free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some &lt;a href="https://github.com/pathwaycom"&gt;public repos&lt;/a&gt; which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.&lt;/p&gt; 
&lt;h2&gt;Contribution guidelines&lt;a id="contribution-guidelines"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license.&lt;/p&gt; 
&lt;p&gt;For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don't hesitate to engage with Pathway's &lt;a href="https://discord.gg/pathway"&gt;Discord community&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>IBM/mcp-context-forge</title>
      <link>https://github.com/IBM/mcp-context-forge</link>
      <description>&lt;p&gt;A Model Context Protocol (MCP) Gateway &amp; Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Gateway&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Model Context Protocol gateway &amp;amp; proxy - unify REST, MCP, and A2A with federation, virtual servers, retries, security, and an optional admin UI.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/docs/docs/images/contextforge-banner.png" alt="" /&gt;&lt;/p&gt; 
&lt;!-- === CI / Security / Build Badges === --&gt; 
&lt;p&gt;&lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/python-package.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/python-package.yml/badge.svg?sanitize=true" alt="Build Python Package" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/codeql.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/codeql.yml/badge.svg?sanitize=true" alt="CodeQL" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/bandit.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/bandit.yml/badge.svg?sanitize=true" alt="Bandit Security" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/dependency-review.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/dependency-review.yml/badge.svg?sanitize=true" alt="Dependency Review" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/pytest.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/pytest.yml/badge.svg?sanitize=true" alt="Tests &amp;amp; Coverage" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/lint.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/lint.yml/badge.svg?sanitize=true" alt="Lint &amp;amp; Static Analysis" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- === Container Build &amp; Deploy === --&gt; 
&lt;p&gt;&lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/docker-image.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/docker-image.yml/badge.svg?sanitize=true" alt="Secure Docker Build" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/IBM/mcp-context-forge/actions/workflows/ibm-cloud-code-engine.yml"&gt;&lt;img src="https://github.com/IBM/mcp-context-forge/actions/workflows/ibm-cloud-code-engine.yml/badge.svg?sanitize=true" alt="Deploy to IBM Code Engine" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- === Package / Container === --&gt; 
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/asyncio.html"&gt;&lt;img src="https://img.shields.io/badge/async-await-green.svg?sanitize=true" alt="Async" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/ibm/mcp-context-forge" alt="License" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://pypi.org/project/mcp-contextforge-gateway/"&gt;&lt;img src="https://img.shields.io/pypi/v/mcp-contextforge-gateway" alt="PyPI" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/pkgs/container/mcp-context-forge"&gt;&lt;img src="https://img.shields.io/badge/docker-ghcr.io%2Fibm%2Fmcp--context--forge-blue" alt="Docker Image" /&gt;&lt;/a&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;ContextForge MCP Gateway is a feature-rich gateway, proxy and MCP Registry that federates MCP and REST services - unifying discovery, auth, rate-limiting, observability, virtual servers, multi-transport protocols, and an optional Admin UI into one clean endpoint for your AI clients. It runs as a fully compliant MCP server, deployable via PyPI or Docker, and scales to multi-cluster environments on Kubernetes with Redis-backed federation and caching.&lt;/p&gt; 
&lt;h2&gt;&lt;img src="https://ibm.github.io/mcp-context-forge/images/mcpgateway.gif" alt="MCP Gateway" /&gt;&lt;/h2&gt; 
&lt;!-- vscode-markdown-toc --&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-overview--goals"&gt;üöÄ Overview &amp;amp; Goals&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="3"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#quick-start---pypi"&gt;Quick Start - PyPI&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;3.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---install--run-copy-paste-friendly"&gt;1 - Install &amp;amp; run (copy-paste friendly)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="4"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#quick-start---containers"&gt;Quick Start - Containers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;4.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-docker"&gt;üê≥ Docker&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;4.1.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---minimum-viable-run"&gt;1 - Minimum viable run&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;4.1.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#2---persist-the-sqlite-database"&gt;2 - Persist the SQLite database&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;4.1.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#3---local-tool-discovery-host-network"&gt;3 - Local tool discovery (host network)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;4.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-podman-rootless-friendly"&gt;ü¶≠ Podman (rootless-friendly)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;4.2.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---basic-run"&gt;1 - Basic run&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;4.2.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#2---persist-sqlite"&gt;2 - Persist SQLite&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;4.2.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#3---host-networking-rootless"&gt;3 - Host networking (rootless)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="5"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#testing-mcpgatewaywrapper-by-hand"&gt;Testing &lt;code&gt;mcpgateway.wrapper&lt;/code&gt; by hand&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;5.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-running-from-an-mcp-client-mcpgatewaywrapper"&gt;üß© Running from an MCP Client (&lt;code&gt;mcpgateway.wrapper&lt;/code&gt;)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;5.1.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---install-uv-uvx-is-an-alias-it-provides"&gt;1 - Install &lt;code&gt;uv&lt;/code&gt; (&lt;code&gt;uvx&lt;/code&gt; is an alias it provides)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;5.1.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#2---create-an-on-the-spot-venv--run-the-wrapper"&gt;2 - Create an on-the-spot venv &amp;amp; run the wrapper&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;5.1.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#claude-desktop-json-runs-through-uvx"&gt;Claude Desktop JSON (runs through &lt;strong&gt;uvx&lt;/strong&gt;)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;5.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-using-with-claude-desktop-or-any-gui-mcp-client"&gt;üöÄ Using with Claude Desktop (or any GUI MCP client)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="6"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-quick-start-vs-code-dev-container"&gt;üöÄ Quick Start: VS Code Dev Container&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;6.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#1---clone--open"&gt;1 - Clone &amp;amp; Open&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;6.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#2---first-time-build-automatic"&gt;2 - First-Time Build (Automatic)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="7"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#quick-start-manual-install"&gt;Quick Start (manual install)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;7.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;7.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#one-liner-dev"&gt;One-liner (dev)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;7.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#containerized-self-signed-tls"&gt;Containerized (self-signed TLS)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;7.4. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#smoke-test-the-api"&gt;Smoke-test the API&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="8"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;8.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#via-make"&gt;Via Make&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;8.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#uv-alternative"&gt;UV (alternative)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;8.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#pip-alternative"&gt;pip (alternative)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;8.4. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#optional-postgresql-adapter"&gt;Optional (PostgreSQL adapter)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;8.4.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#quick-postgres-container"&gt;Quick Postgres container&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="9"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#configuration-env-or-env-vars"&gt;Configuration (&lt;code&gt;.env&lt;/code&gt; or env vars)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;9.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#basic"&gt;Basic&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#authentication"&gt;Authentication&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#ui-features"&gt;UI Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.4. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#security"&gt;Security&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.5. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#logging"&gt;Logging&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.6. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#transport"&gt;Transport&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.7. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#federation"&gt;Federation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.8. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.9. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.10. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#prompts"&gt;Prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.11. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#health-checks"&gt;Health Checks&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.12. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#database"&gt;Database&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.13. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#cache-backend"&gt;Cache Backend&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;9.14. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#development"&gt;Development&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="10"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#running"&gt;Running&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;10.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#makefile"&gt;Makefile&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#script-helper"&gt;Script helper&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;10.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#manual-uvicorn"&gt;Manual (Uvicorn)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="11"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#authentication-examples"&gt;Authentication examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="12"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#%EF%B8%8F-aws--azure--openshift"&gt;‚òÅÔ∏è AWS / Azure / OpenShift&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="13"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#%EF%B8%8F-ibm-cloud-code-engine-deployment"&gt;‚òÅÔ∏è IBM Cloud Code Engine Deployment&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;13.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-prerequisites-1"&gt;üîß Prerequisites&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;13.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-environment-variables"&gt;üì¶ Environment Variables&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;13.3. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-make-targets"&gt;üöÄ Make Targets&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;13.4. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-example-workflow"&gt;üìù Example Workflow&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="14"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#api-endpoints"&gt;API Endpoints&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="15"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#testing"&gt;Testing&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="16"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#project-structure"&gt;Project Structure&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="17"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#api-documentation"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="18"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#makefile-targets"&gt;Makefile targets&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="19"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#-troubleshooting"&gt;üîç Troubleshooting&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;19.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#diagnose-the-listener"&gt;Diagnose the listener&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;19.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#why-localhost-fails-on-windows"&gt;Why localhost fails on Windows&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;19.2.1. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#fix-podman-rootless"&gt;Fix (Podman rootless)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;19.2.2. &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#fix-docker-desktop--419"&gt;Fix (Docker Desktop &amp;gt; 4.19)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="20"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="21"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#changelog"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="22"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="23"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#core-authors-and-maintainers"&gt;Core Authors and Maintainers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;ol start="24"&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/#star-history-and-project-activity"&gt;Star History and Project Activity&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- vscode-markdown-toc-config
    numbering=true
    autoSave=true
    /vscode-markdown-toc-config --&gt; 
&lt;!-- /vscode-markdown-toc --&gt; 
&lt;h2&gt;üöÄ Overview &amp;amp; Goals&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ContextForge MCP Gateway&lt;/strong&gt; is a gateway, registry, and proxy that sits in front of any &lt;a href="https://modelcontextprotocol.io"&gt;Model Context Protocol&lt;/a&gt; (MCP) server or REST API-exposing a unified endpoint for all your AI clients.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Caution&lt;/strong&gt;: The current release (0.6.0) is considered alpha / early beta. It is not production-ready and should only be used for local development, testing, or experimentation. Features, APIs, and behaviors are subject to change without notice. &lt;strong&gt;Do not&lt;/strong&gt; deploy in production environments without thorough security review, validation and additional security mechanisms. Many of the features required for secure, large-scale, or multi-tenant production deployments are still on the &lt;a href="https://ibm.github.io/mcp-context-forge/architecture/roadmap/"&gt;project roadmap&lt;/a&gt; - which is itself evolving.&lt;/p&gt; 
&lt;p&gt;It currently supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Federation across multiple MCP and REST services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;A2A (Agent-to-Agent) integration&lt;/strong&gt; for external AI agents (OpenAI, Anthropic, custom)&lt;/li&gt; 
 &lt;li&gt;Virtualization of legacy APIs as MCP-compliant tools and servers&lt;/li&gt; 
 &lt;li&gt;Transport over HTTP, JSON-RPC, WebSocket, SSE (with configurable keepalive), stdio and streamable-HTTP&lt;/li&gt; 
 &lt;li&gt;An Admin UI for real-time management, configuration, and log monitoring&lt;/li&gt; 
 &lt;li&gt;Built-in auth, retries, and rate-limiting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenTelemetry observability&lt;/strong&gt; with Phoenix, Jaeger, Zipkin, and other OTLP backends&lt;/li&gt; 
 &lt;li&gt;Scalable deployments via Docker or PyPI, Redis-backed caching, and multi-cluster federation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://ibm.github.io/mcp-context-forge/images/mcpgateway.svg?sanitize=true" alt="MCP Gateway Architecture" /&gt;&lt;/p&gt; 
&lt;p&gt;For a list of upcoming features, check out the &lt;a href="https://ibm.github.io/mcp-context-forge/architecture/roadmap/"&gt;ContextForge MCP Gateway Roadmap&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important&lt;/strong&gt;: MCP Gateway is not a standalone product - it is an open source component with &lt;strong&gt;NO OFFICIAL SUPPORT&lt;/strong&gt; from IBM or its affiliates that can be integrated into your own solution architecture. If you choose to use it, you are responsible for evaluating its fit, securing the deployment, and managing its lifecycle. See &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/SECURITY.md"&gt;SECURITY.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîå Gateway Layer with Protocol Flexibility&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Sits in front of any MCP server or REST API&lt;/li&gt; 
  &lt;li&gt;Lets you choose your MCP protocol version (e.g., &lt;code&gt;2025-03-26&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;Exposes a single, unified interface for diverse backends&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üåê Federation of Peer Gateways (MCP Registry)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Auto-discovers or configures peer gateways (via mDNS or manual)&lt;/li&gt; 
  &lt;li&gt;Performs health checks and merges remote registries transparently&lt;/li&gt; 
  &lt;li&gt;Supports Redis-backed syncing and fail-over&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üß© Virtualization of REST/gRPC Services&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Wraps non-MCP services as virtual MCP servers&lt;/li&gt; 
  &lt;li&gt;Registers tools, prompts, and resources with minimal configuration&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîÅ REST-to-MCP Tool Adapter&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Adapts REST APIs into tools with:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Automatic JSON Schema extraction&lt;/li&gt; 
    &lt;li&gt;Support for headers, tokens, and custom auth&lt;/li&gt; 
    &lt;li&gt;Retry, timeout, and rate-limit policies&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üß† Unified Registries&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Prompts&lt;/strong&gt;: Jinja2 templates, multimodal support, rollback/versioning&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Resources&lt;/strong&gt;: URI-based access, MIME detection, caching, SSE updates&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Native or adapted, with input validation and concurrency controls&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìà Admin UI, Observability &amp;amp; Dev Experience&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Admin UI built with HTMX + Alpine.js&lt;/li&gt; 
  &lt;li&gt;Real-time log viewer with filtering, search, and export capabilities&lt;/li&gt; 
  &lt;li&gt;Auth: Basic, JWT, or custom schemes&lt;/li&gt; 
  &lt;li&gt;Structured logs, health endpoints, metrics&lt;/li&gt; 
  &lt;li&gt;400+ tests, Makefile targets, live reload, pre-commit hooks&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîç OpenTelemetry Observability&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Vendor-agnostic tracing&lt;/strong&gt; with OpenTelemetry (OTLP) protocol support&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Multiple backend support&lt;/strong&gt;: Phoenix (LLM-focused), Jaeger, Zipkin, Tempo, DataDog, New Relic&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Distributed tracing&lt;/strong&gt; across federated gateways and services&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Automatic instrumentation&lt;/strong&gt; of tools, prompts, resources, and gateway operations&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;LLM-specific metrics&lt;/strong&gt;: Token usage, costs, model performance&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Zero-overhead when disabled&lt;/strong&gt; with graceful degradation&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Easy configuration&lt;/strong&gt; via environment variables&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Quick start with Phoenix (LLM observability):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Start Phoenix
docker run -p 6006:6006 -p 4317:4317 arizephoenix/phoenix:latest

# Configure gateway
export OTEL_ENABLE_OBSERVABILITY=true
export OTEL_TRACES_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Run gateway - traces automatically sent to Phoenix
mcpgateway
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See &lt;a href="https://ibm.github.io/mcp-context-forge/manage/observability/"&gt;Observability Documentation&lt;/a&gt; for detailed setup with other backends.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start - PyPI&lt;/h2&gt; 
&lt;p&gt;MCP Gateway is published on &lt;a href="https://pypi.org/project/mcp-contextforge-gateway/"&gt;PyPI&lt;/a&gt; as &lt;code&gt;mcp-contextforge-gateway&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;TLDR;&lt;/strong&gt;: (single command using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;BASIC_AUTH_PASSWORD=pass \
MCPGATEWAY_UI_ENABLED=true \
MCPGATEWAY_ADMIN_API_ENABLED=true \
uvx --from mcp-contextforge-gateway mcpgateway --host 0.0.0.0 --port 4444
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Prerequisites&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Python ‚â• 3.10&lt;/strong&gt; (3.11 recommended)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;curl + jq&lt;/strong&gt; - only for the last smoke-test step&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;1 - Install &amp;amp; run (copy-paste friendly)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1Ô∏è‚É£  Isolated env + install from pypi
mkdir mcpgateway &amp;amp;&amp;amp; cd mcpgateway
python3 -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate
pip install --upgrade pip
pip install mcp-contextforge-gateway

# 2Ô∏è‚É£  Launch on all interfaces with custom creds &amp;amp; secret key
# Enable the Admin API endpoints (true/false) - disabled by default
export MCPGATEWAY_UI_ENABLED=true
export MCPGATEWAY_ADMIN_API_ENABLED=true

BASIC_AUTH_PASSWORD=pass JWT_SECRET_KEY=my-test-key \
  mcpgateway --host 0.0.0.0 --port 4444 &amp;amp;   # admin/pass

# 3Ô∏è‚É£  Generate a bearer token &amp;amp; smoke-test the API
export MCPGATEWAY_BEARER_TOKEN=$(python3 -m mcpgateway.utils.create_jwt_token \
    --username admin --exp 10080 --secret my-test-key)

curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://127.0.0.1:4444/version | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Windows (PowerShell) quick-start&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;# 1Ô∏è‚É£  Isolated env + install from PyPI
mkdir mcpgateway ; cd mcpgateway
python3 -m venv .venv ; .\.venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install mcp-contextforge-gateway

# 2Ô∏è‚É£  Environment variables (session-only)
$Env:MCPGATEWAY_UI_ENABLED        = "true"
$Env:MCPGATEWAY_ADMIN_API_ENABLED = "true"
$Env:BASIC_AUTH_PASSWORD          = "changeme"      # admin/changeme
$Env:JWT_SECRET_KEY               = "my-test-key"

# 3Ô∏è‚É£  Launch the gateway
mcpgateway.exe --host 0.0.0.0 --port 4444

#   Optional: background it
# Start-Process -FilePath "mcpgateway.exe" -ArgumentList "--host 0.0.0.0 --port 4444"

# 4Ô∏è‚É£  Bearer token and smoke-test
$Env:MCPGATEWAY_BEARER_TOKEN = python3 -m mcpgateway.utils.create_jwt_token `
    --username admin --exp 10080 --secret my-test-key

curl -s -H "Authorization: Bearer $Env:MCPGATEWAY_BEARER_TOKEN" `
     http://127.0.0.1:4444/version | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;More configuration&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Copy &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/.env.example"&gt;.env.example&lt;/a&gt; to &lt;code&gt;.env&lt;/code&gt; and tweak any of the settings (or use them as env variables).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üöÄ End-to-end demo (register a local MCP server)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 1Ô∏è‚É£  Spin up the sample GO MCP time server using mcpgateway.translate &amp;amp; docker
python3 -m mcpgateway.translate \
     --stdio "docker run --rm -i -p 8888:8080 ghcr.io/ibm/fast-time-server:latest -transport=stdio" \
     --expose-sse \
     --port 8003

# Or using the official mcp-server-git using uvx:
pip install uv # to install uvx, if not already installed
python3 -m mcpgateway.translate --stdio "uvx mcp-server-git" --expose-sse --port 9000

# Alternative: running the local binary
# cd mcp-servers/go/fast-time-server; make build
# python3 -m mcpgateway.translate --stdio "./dist/fast-time-server -transport=stdio" --expose-sse --port 8002

# NEW: Expose via multiple protocols simultaneously!
python3 -m mcpgateway.translate \
     --stdio "uvx mcp-server-git" \
     --expose-sse \
     --expose-streamable-http \
     --port 9000
# Now accessible via both /sse (SSE) and /mcp (streamable HTTP) endpoints

# 2Ô∏è‚É£  Register it with the gateway
curl -s -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"name":"fast_time","url":"http://localhost:9000/sse"}' \
     http://localhost:4444/gateways

# 3Ô∏è‚É£  Verify tool catalog
curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools | jq

# 4Ô∏è‚É£  Create a *virtual server* bundling those tools. Use the ID of tools from the tool catalog (Step #3) and pass them in the associatedTools list.
curl -s -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"name":"time_server","description":"Fast time tools","associatedTools":[&amp;lt;ID_OF_TOOLS&amp;gt;]}' \
     http://localhost:4444/servers | jq

# Example curl
curl -s -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN"
     -H "Content-Type: application/json"
     -d '{"name":"time_server","description":"Fast time tools","associatedTools":["6018ca46d32a4ac6b4c054c13a1726a2"]}' \
     http://localhost:4444/servers | jq

# 5Ô∏è‚É£  List servers (should now include the UUID of the newly created virtual server)
curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/servers | jq

# 6Ô∏è‚É£  Client SSE endpoint. Inspect it interactively with the MCP Inspector CLI (or use any MCP client)
npx -y @modelcontextprotocol/inspector
# Transport Type: SSE, URL: http://localhost:4444/servers/UUID_OF_SERVER_1/sse,  Header Name: "Authorization", Bearer Token
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üñß Using the stdio wrapper (mcpgateway-wrapper)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;export MCP_AUTH=$MCPGATEWAY_BEARER_TOKEN
export MCP_SERVER_URL=http://localhost:4444/servers/UUID_OF_SERVER_1/mcp
python3 -m mcpgateway.wrapper  # Ctrl-C to exit
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You can also run it with &lt;code&gt;uv&lt;/code&gt; or inside Docker/Podman - see the &lt;em&gt;Containers&lt;/em&gt; section above.&lt;/p&gt; 
 &lt;p&gt;In MCP Inspector, define &lt;code&gt;MCP_AUTH&lt;/code&gt; and &lt;code&gt;MCP_SERVER_URL&lt;/code&gt; env variables, and select &lt;code&gt;python3&lt;/code&gt; as the Command, and &lt;code&gt;-m mcpgateway.wrapper&lt;/code&gt; as Arguments.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;echo $PWD/.venv/bin/python3 # Using the Python3 full path ensures you have a working venv
export MCP_SERVER_URL='http://localhost:4444/servers/UUID_OF_SERVER_1/mcp'
export MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN}
npx -y @modelcontextprotocol/inspector
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;or&lt;/p&gt; 
 &lt;p&gt;Pass the url and auth as arguments (no need to set environment variables)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;npx -y @modelcontextprotocol/inspector
command as `python`
Arguments as `-m mcpgateway.wrapper --url "http://localhost:4444/servers/UUID_OF_SERVER_1/mcp" --auth "Bearer &amp;lt;your token&amp;gt;"`
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When using a MCP Client such as Claude with stdio:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcpgateway-wrapper": {
      "command": "python",
      "args": ["-m", "mcpgateway.wrapper"],
      "env": {
        "MCP_AUTH": "your-token-here",
        "MCP_SERVER_URL": "http://localhost:4444/servers/UUID_OF_SERVER_1",
        "MCP_TOOL_CALL_TIMEOUT": "120"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start - Containers&lt;/h2&gt; 
&lt;p&gt;Use the official OCI image from GHCR with &lt;strong&gt;Docker&lt;/strong&gt; &lt;em&gt;or&lt;/em&gt; &lt;strong&gt;Podman&lt;/strong&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üê≥ Docker&lt;/h3&gt; 
&lt;h4&gt;1 - Minimum viable run&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --name mcpgateway \
  -p 4444:4444 \
  -e MCPGATEWAY_UI_ENABLED=true \
  -e MCPGATEWAY_ADMIN_API_ENABLED=true \
  -e HOST=0.0.0.0 \
  -e JWT_SECRET_KEY=my-test-key \
  -e BASIC_AUTH_USER=admin \
  -e BASIC_AUTH_PASSWORD=changeme \
  -e AUTH_REQUIRED=true \
  -e DATABASE_URL=sqlite:///./mcp.db \
  ghcr.io/ibm/mcp-context-forge:0.6.0

# Tail logs (Ctrl+C to quit)
docker logs -f mcpgateway

# Generating an API key
docker run --rm -it ghcr.io/ibm/mcp-context-forge:0.6.0 \
  python3 -m mcpgateway.utils.create_jwt_token --username admin --exp 0 --secret my-test-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Browse to &lt;strong&gt;&lt;a href="http://localhost:4444/admin"&gt;http://localhost:4444/admin&lt;/a&gt;&lt;/strong&gt; (user &lt;code&gt;admin&lt;/code&gt; / pass &lt;code&gt;changeme&lt;/code&gt;).&lt;/p&gt; 
&lt;h4&gt;2 - Persist the SQLite database&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p $(pwd)/data

touch $(pwd)/data/mcp.db

sudo chown -R :docker $(pwd)/data

chmod 777 $(pwd)/data

docker run -d --name mcpgateway \
  --restart unless-stopped \
  -p 4444:4444 \
  -v $(pwd)/data:/data \
  -e MCPGATEWAY_UI_ENABLED=true \
  -e MCPGATEWAY_ADMIN_API_ENABLED=true \
  -e DATABASE_URL=sqlite:////data/mcp.db \
  -e HOST=0.0.0.0 \
  -e JWT_SECRET_KEY=my-test-key \
  -e BASIC_AUTH_USER=admin \
  -e BASIC_AUTH_PASSWORD=changeme \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;SQLite now lives on the host at &lt;code&gt;./data/mcp.db&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;3 - Local tool discovery (host network)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p $(pwd)/data

touch $(pwd)/data/mcp.db

sudo chown -R :docker $(pwd)/data

chmod 777 $(pwd)/data

docker run -d --name mcpgateway \
  --network=host \
  -e MCPGATEWAY_UI_ENABLED=true \
  -e MCPGATEWAY_ADMIN_API_ENABLED=true \
  -e HOST=0.0.0.0 \
  -e PORT=4444 \
  -e DATABASE_URL=sqlite:////data/mcp.db \
  -v $(pwd)/data:/data \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Using &lt;code&gt;--network=host&lt;/code&gt; allows Docker to access the local network, allowing you to add MCP servers running on your host. See &lt;a href="https://docs.docker.com/engine/network/drivers/host/"&gt;Docker Host network driver documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ü¶≠ Podman (rootless-friendly)&lt;/h3&gt; 
&lt;h4&gt;1 - Basic run&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;podman run -d --name mcpgateway \
  -p 4444:4444 \
  -e HOST=0.0.0.0 \
  -e DATABASE_URL=sqlite:///./mcp.db \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2 - Persist SQLite&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p $(pwd)/data

touch $(pwd)/data/mcp.db

sudo chown -R :docker $(pwd)/data

chmod 777 $(pwd)/data

podman run -d --name mcpgateway \
  --restart=on-failure \
  -p 4444:4444 \
  -v $(pwd)/data:/data \
  -e DATABASE_URL=sqlite:////data/mcp.db \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3 - Host networking (rootless)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p $(pwd)/data

touch $(pwd)/data/mcp.db

sudo chown -R :docker $(pwd)/data

chmod 777 $(pwd)/data

podman run -d --name mcpgateway \
  --network=host \
  -v $(pwd)/data:/data \
  -e DATABASE_URL=sqlite:////data/mcp.db \
  ghcr.io/ibm/mcp-context-forge:0.6.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;‚úèÔ∏è Docker/Podman tips&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;.env files&lt;/strong&gt; - Put all the &lt;code&gt;-e FOO=&lt;/code&gt; lines into a file and replace them with &lt;code&gt;--env-file .env&lt;/code&gt;. See the provided &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/.env.example"&gt;.env.example&lt;/a&gt; for reference.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pinned tags&lt;/strong&gt; - Use an explicit version (e.g. &lt;code&gt;v0.6.0&lt;/code&gt;) instead of &lt;code&gt;latest&lt;/code&gt; for reproducible builds.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;JWT tokens&lt;/strong&gt; - Generate one in the running container:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker exec mcpgateway python3 -m mcpgateway.utils.create_jwt_token -u admin -e 10080 --secret my-test-key
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgrades&lt;/strong&gt; - Stop, remove, and rerun with the same &lt;code&gt;-v $(pwd)/data:/data&lt;/code&gt; mount; your DB and config stay intact.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üöë Smoke-test the running container&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/health | jq
curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/tools | jq
curl -s -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/version | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üñß Running the MCP Gateway stdio wrapper&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;The &lt;code&gt;mcpgateway.wrapper&lt;/code&gt; lets you connect to the gateway over &lt;strong&gt;stdio&lt;/strong&gt; while keeping JWT authentication. You should run this from the MCP Client. The example below is just for testing.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Set environment variables
export MCPGATEWAY_BEARER_TOKEN=$(python3 -m mcpgateway.utils.create_jwt_token --username admin --exp 10080 --secret my-test-key)
export MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN}
export MCP_SERVER_URL='http://localhost:4444/servers/UUID_OF_SERVER_1/mcp'
export MCP_TOOL_CALL_TIMEOUT=120
export MCP_WRAPPER_LOG_LEVEL=DEBUG  # or OFF to disable logging

docker run --rm -i \
  -e MCP_AUTH=$MCPGATEWAY_BEARER_TOKEN \
  -e MCP_SERVER_URL=http://host.docker.internal:4444/servers/UUID_OF_SERVER_1/mcp \
  -e MCP_TOOL_CALL_TIMEOUT=120 \
  -e MCP_WRAPPER_LOG_LEVEL=DEBUG \
  ghcr.io/ibm/mcp-context-forge:0.6.0 \
  python3 -m mcpgateway.wrapper
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Testing &lt;code&gt;mcpgateway.wrapper&lt;/code&gt; by hand:&lt;/h2&gt; 
&lt;p&gt;Because the wrapper speaks JSON-RPC over stdin/stdout, you can interact with it using nothing more than a terminal or pipes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the MCP Gateway Wrapper
export MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN}
export MCP_SERVER_URL=http://localhost:4444/servers/YOUR_SERVER_UUID
python3 -m mcpgateway.wrapper
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Initialize the protocol&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;# Initialize the protocol
{"jsonrpc":"2.0","id":1,"method":"initialize","params":{"protocolVersion":"2025-03-26","capabilities":{},"clientInfo":{"name":"demo","version":"0.0.1"}}}

# Then after the reply:
{"jsonrpc":"2.0","method":"notifications/initialized","params":{}}

# Get prompts
{"jsonrpc":"2.0","id":4,"method":"prompts/list"}
{"jsonrpc":"2.0","id":5,"method":"prompts/get","params":{"name":"greeting","arguments":{"user":"Bob"}}}

# Get resources
{"jsonrpc":"2.0","id":6,"method":"resources/list"}
{"jsonrpc":"2.0","id":7,"method":"resources/read","params":{"uri":"https://example.com/some.txt"}}

# Get / call tools
{"jsonrpc":"2.0","id":2,"method":"tools/list"}
{"jsonrpc":"2.0","id":3,"method":"tools/call","params":{"name":"get_system_time","arguments":{"timezone":"Europe/Dublin"}}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Expected responses from mcpgateway.wrapper&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{"jsonrpc":"2.0","id":1,"result":{"protocolVersion":"2025-03-26","capabilities":{"experimental":{},"prompts":{"listChanged":false},"resources":{"subscribe":false,"listChanged":false},"tools":{"listChanged":false}},"serverInfo":{"name":"mcpgateway-wrapper","version":"0.6.0"}}}

# When there's no tools
{"jsonrpc":"2.0","id":2,"result":{"tools":[]}}

# After you add some tools and create a virtual server
{"jsonrpc":"2.0","id":2,"result":{"tools":[{"annotations":{"readOnlyHint":false,"destructiveHint":true,"idempotentHint":false,"openWorldHint":true},"description":"Convert time between different timezones","inputSchema":{"properties":{"source_timezone":{"description":"Source IANA timezone name","type":"string"},"target_timezone":{"description":"Target IANA timezone name","type":"string"},"time":{"description":"Time to convert in RFC3339 format or common formats like '2006-01-02 15:04:05'","type":"string"}},"required":["time","source_timezone","target_timezone"],"type":"object"},"name":"convert_time"},{"annotations":{"readOnlyHint":false,"destructiveHint":true,"idempotentHint":false,"openWorldHint":true},"description":"Get current system time in specified timezone","inputSchema":{"properties":{"timezone":{"description":"IANA timezone name (e.g., 'America/New_York', 'Europe/London'). Defaults to UTC","type":"string"}},"type":"object"},"name":"get_system_time"}]}}

# Running the time tool:
{"jsonrpc":"2.0","id":3,"result":{"content":[{"type":"text","text":"2025-07-09T00:09:45+01:00"}]}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üß© Running from an MCP Client (&lt;code&gt;mcpgateway.wrapper&lt;/code&gt;)&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;mcpgateway.wrapper&lt;/code&gt; exposes everything your Gateway knows about over &lt;strong&gt;stdio&lt;/strong&gt;, so any MCP client that &lt;em&gt;can't&lt;/em&gt; (or &lt;em&gt;shouldn't&lt;/em&gt;) open an authenticated SSE stream still gets full tool-calling power.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Remember&lt;/strong&gt; to substitute your real Gateway URL (and server ID) for &lt;code&gt;http://localhost:4444/servers/UUID_OF_SERVER_1/mcp&lt;/code&gt;. When inside Docker/Podman, that often becomes &lt;code&gt;http://host.docker.internal:4444/servers/UUID_OF_SERVER_1/mcp&lt;/code&gt; (macOS/Windows) or the gateway container's hostname (Linux).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üê≥ Docker / Podman&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -i --rm \
  --network=host \
  -e MCP_SERVER_URL=http://localhost:4444/servers/UUID_OF_SERVER_1/mcp \
  -e MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN} \
  -e MCP_TOOL_CALL_TIMEOUT=120 \
  ghcr.io/ibm/mcp-context-forge:0.6.0 \
  python3 -m mcpgateway.wrapper
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üì¶ pipx (one-liner install &amp;amp; run)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install gateway package in its own isolated venv
pipx install --include-deps mcp-contextforge-gateway

# Run the stdio wrapper
MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN} \
MCP_SERVER_URL=http://localhost:4444/servers/UUID_OF_SERVER_1/mcp \
python3 -m mcpgateway.wrapper
# Alternatively with uv
uv run --directory . -m mcpgateway.wrapper
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Claude Desktop JSON&lt;/strong&gt; (uses the host Python that pipx injected):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcpgateway-wrapper": {
      "command": "python3",
      "args": ["-m", "mcpgateway.wrapper"],
      "env": {
        "MCP_AUTH": "&amp;lt;your-token&amp;gt;",
        "MCP_SERVER_URL": "http://localhost:4444/servers/UUID_OF_SERVER_1/mcp",
        "MCP_TOOL_CALL_TIMEOUT": "120"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;‚ö° uv / uvx (light-speed venvs)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;1 - Install &lt;code&gt;uv&lt;/code&gt; (&lt;code&gt;uvx&lt;/code&gt; is an alias it provides)&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# (a) official one-liner
curl -Ls https://astral.sh/uv/install.sh | sh

# (b) or via pipx
pipx install uv
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;2 - Create an on-the-spot venv &amp;amp; run the wrapper&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Create venv in ~/.venv/mcpgateway (or current dir if you prefer)
uv venv ~/.venv/mcpgateway
source ~/.venv/mcpgateway/bin/activate

# Install the gateway package using uv
uv pip install mcp-contextforge-gateway

# Launch wrapper
MCP_AUTH=${MCPGATEWAY_BEARER_TOKEN} \
MCP_SERVER_URL=http://localhost:4444/servers/UUID_OF_SERVER_1/mcp \
uv run --directory . -m mcpgateway.wrapper # Use this just for testing, as the Client will run the uv command
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Claude Desktop JSON (runs through &lt;strong&gt;uvx&lt;/strong&gt;)&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "mcpgateway-wrapper": {
      "command": "uvx",
      "args": [
        "run",
        "--",
        "python",
        "-m",
        "mcpgateway.wrapper"
      ],
      "env": {
        "MCP_AUTH": "&amp;lt;your-token&amp;gt;",
        "MCP_SERVER_URL": "http://localhost:4444/servers/UUID_OF_SERVER_1/mcp"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üöÄ Using with Claude Desktop (or any GUI MCP client)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Edit Config&lt;/strong&gt; ‚Üí &lt;code&gt;File ‚ñ∏ Settings ‚ñ∏ Developer ‚ñ∏ Edit Config&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Paste one of the JSON blocks above (Docker / pipx / uvx).&lt;/li&gt; 
 &lt;li&gt;Restart the app so the new stdio server is spawned.&lt;/li&gt; 
 &lt;li&gt;Open logs in the same menu to verify &lt;code&gt;mcpgateway-wrapper&lt;/code&gt; started and listed your tools.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Need help? See:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Debugging Guide&lt;/strong&gt; - &lt;a href="https://modelcontextprotocol.io/docs/tools/debugging"&gt;https://modelcontextprotocol.io/docs/tools/debugging&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start: VS Code Dev Container&lt;/h2&gt; 
&lt;p&gt;Spin up a fully-loaded dev environment (Python 3.11, Docker/Podman CLI, all project dependencies) in just two clicks.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Prerequisites&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;VS Code&lt;/strong&gt; with the &lt;a href="https://code.visualstudio.com/docs/devcontainers/containers"&gt;Dev Containers extension&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt; or &lt;strong&gt;Podman&lt;/strong&gt; installed and running locally&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üß∞ Setup Instructions&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;1 - Clone &amp;amp; Open&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ibm/mcp-context-forge.git
cd mcp-context-forge
code .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;VS Code will detect the &lt;code&gt;.devcontainer&lt;/code&gt; and prompt: &lt;strong&gt;"Reopen in Container"&lt;/strong&gt; &lt;em&gt;or&lt;/em&gt; manually run: &lt;kbd&gt;Ctrl/Cmd ‚áß P&lt;/kbd&gt; ‚Üí &lt;strong&gt;Dev Containers: Reopen in Container&lt;/strong&gt;&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;2 - First-Time Build (Automatic)&lt;/h3&gt; 
 &lt;p&gt;The container build will:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Install system packages &amp;amp; Python 3.11&lt;/li&gt; 
  &lt;li&gt;Run &lt;code&gt;make install-dev&lt;/code&gt; to pull all dependencies&lt;/li&gt; 
  &lt;li&gt;Execute tests to verify the toolchain&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;You'll land in &lt;code&gt;/workspace&lt;/code&gt; ready to develop.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Daily Developer Workflow&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Common tasks inside the container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Start dev server (hot reload)
make dev            # http://localhost:4444

# Run tests &amp;amp; linters
make test
make lint
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Optional:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;make bash&lt;/code&gt; - drop into an interactive shell&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt; - clear build artefacts &amp;amp; caches&lt;/li&gt; 
  &lt;li&gt;Port forwarding is automatic (customize via &lt;code&gt;.devcontainer/devcontainer.json&lt;/code&gt;)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;‚òÅÔ∏è GitHub Codespaces: 1-Click Cloud IDE&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;No local Docker? Use Codespaces:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Go to the repo ‚Üí &lt;strong&gt;Code ‚ñ∏ Codespaces ‚ñ∏ Create codespace on main&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;Wait for the container image to build in the cloud&lt;/li&gt; 
  &lt;li&gt;Develop using the same workflow above&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start (manual install)&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python ‚â• 3.10&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GNU Make&lt;/strong&gt; (optional, but all common workflows are available as Make targets)&lt;/li&gt; 
 &lt;li&gt;Optional: &lt;strong&gt;Docker / Podman&lt;/strong&gt; for containerized runs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;One-liner (dev)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make venv install serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;What it does:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Creates / activates a &lt;code&gt;.venv&lt;/code&gt; in your home folder &lt;code&gt;~/.venv/mcpgateway&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Installs the gateway and necessary dependencies&lt;/li&gt; 
 &lt;li&gt;Launches &lt;strong&gt;Gunicorn&lt;/strong&gt; (Uvicorn workers) on &lt;a href="http://localhost:4444"&gt;http://localhost:4444&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For development, you can use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make install-dev # Install development dependencies, ex: linters and test harness
make lint          # optional: run style checks (ruff, mypy, etc.)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Containerized (self-signed TLS)&lt;/h3&gt; 
&lt;h2&gt;Container Runtime Support&lt;/h2&gt; 
&lt;p&gt;This project supports both Docker and Podman. The Makefile automatically detects which runtime is available and handles image naming differences.&lt;/p&gt; 
&lt;h3&gt;Auto-detection&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make container-build  # Uses podman if available, otherwise docker

&amp;gt; You can use docker or podman, ex:

```bash
make podman            # build production image
make podman-run-ssl    # run at https://localhost:4444
# or listen on port 4444 on your host directly, adds --network=host to podman
make podman-run-ssl-host
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Smoke-test the API&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -k -sX GET \
     -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     https://localhost:4444/tools | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should receive &lt;code&gt;[]&lt;/code&gt; until you register a tool.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Via Make&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make venv install          # create .venv + install deps
make serve                 # gunicorn on :4444
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;UV (alternative)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv &amp;amp;&amp;amp; source .venv/bin/activate
uv pip install -e '.[dev]' # IMPORTANT: in zsh, quote to disable glob expansion!
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;pip (alternative)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional (PostgreSQL adapter)&lt;/h3&gt; 
&lt;p&gt;You can configure the gateway with SQLite, PostgreSQL (or any other compatible database) in .env.&lt;/p&gt; 
&lt;p&gt;When using PostgreSQL, you need to install &lt;code&gt;psycopg2&lt;/code&gt; driver.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install psycopg2-binary   # dev convenience
# or
uv pip install psycopg2          # production build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Quick Postgres container&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --name mcp-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=mysecretpassword \
  -e POSTGRES_DB=mcp \
  -p 5432:5432 -d postgres
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A &lt;code&gt;make compose-up&lt;/code&gt; target is provided along with a &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; file to make this process simpler.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Configuration (&lt;code&gt;.env&lt;/code&gt; or env vars)&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è If any required &lt;code&gt;.env&lt;/code&gt; variable is missing or invalid, the gateway will fail fast at startup with a validation error via Pydantic.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can get started by copying the provided &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/.env.example"&gt;.env.example&lt;/a&gt; to &lt;code&gt;.env&lt;/code&gt; and making the necessary edits to fit your environment.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Environment Configuration Variables&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;Basic&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;APP_NAME&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Gateway / OpenAPI title&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;MCP Gateway&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HOST&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Bind address for the app&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;IPv4/IPv6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PORT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Port the server listens on&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;4444&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;1-65535&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DATABASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;SQLAlchemy connection URL&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;sqlite:///./mcp.db&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;any SQLAlchemy dialect&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;APP_ROOT_PATH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Subpath prefix for app (e.g. &lt;code&gt;/gateway&lt;/code&gt;)&lt;/td&gt; 
    &lt;td&gt;(empty)&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TEMPLATES_DIR&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Path to Jinja2 templates&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgateway/templates&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;path&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;STATIC_DIR&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Path to static files&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgateway/static&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;path&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;üí° Use &lt;code&gt;APP_ROOT_PATH=/foo&lt;/code&gt; if reverse-proxying under a subpath like &lt;code&gt;https://host.com/foo/&lt;/code&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Authentication&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;BASIC_AUTH_USER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Username for Admin UI login and HTTP Basic authentication&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;admin&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;BASIC_AUTH_PASSWORD&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Password for Admin UI login and HTTP Basic authentication&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;changeme&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;AUTH_REQUIRED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Require authentication for all API routes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JWT_SECRET_KEY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Secret key used to &lt;strong&gt;sign JWT tokens&lt;/strong&gt; for API access&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;my-test-key&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JWT_ALGORITHM&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Algorithm used to sign the JWTs (&lt;code&gt;HS256&lt;/code&gt; is default, HMAC-based)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;HS256&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;PyJWT algs&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TOKEN_EXPIRY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Expiry of generated JWTs in minutes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10080&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;AUTH_ENCRYPTION_SECRET&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Passphrase used to derive AES key for encrypting tool auth headers&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;my-test-salt&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;üîê &lt;code&gt;BASIC_AUTH_USER&lt;/code&gt;/&lt;code&gt;PASSWORD&lt;/code&gt; are used for:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Logging into the web-based Admin UI&lt;/li&gt; 
   &lt;li&gt;Accessing APIs via Basic Auth (&lt;code&gt;curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN"&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;üîë &lt;code&gt;JWT_SECRET_KEY&lt;/code&gt; is used to:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Sign JSON Web Tokens (&lt;code&gt;Authorization: Bearer &amp;lt;token&amp;gt;&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Generate tokens via:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export MCPGATEWAY_BEARER_TOKEN=$(python3 -m mcpgateway.utils.create_jwt_token --username admin --exp 0 --secret my-test-key)
echo $MCPGATEWAY_BEARER_TOKEN
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Tokens allow non-interactive API clients to authenticate securely.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;üß™ Set &lt;code&gt;AUTH_REQUIRED=false&lt;/code&gt; during development if you want to disable all authentication (e.g. for local testing or open APIs) or clients that don't support SSE authentication. In production, you should use the SSE to stdio &lt;code&gt;mcpgateway-wrapper&lt;/code&gt; for such tools that don't support authenticated SSE, while still ensuring the gateway uses authentication.&lt;/p&gt; 
  &lt;p&gt;üîê &lt;code&gt;AUTH_ENCRYPTION_SECRET&lt;/code&gt; is used to encrypt and decrypt tool authentication credentials (&lt;code&gt;auth_value&lt;/code&gt;). You must set the same value across environments to decode previously stored encrypted auth values. Recommended: use a long, random string.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;UI Features&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_UI_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable the interactive Admin dashboard&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_ADMIN_API_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable API endpoints for admin ops&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_BULK_IMPORT_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable bulk import endpoint for tools&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;üñ•Ô∏è Set both UI and Admin API to &lt;code&gt;false&lt;/code&gt; to disable management UI and APIs in production. üì• The bulk import endpoint allows importing up to 200 tools in a single request via &lt;code&gt;/admin/tools/import&lt;/code&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;A2A (Agent-to-Agent) Features&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable A2A agent features&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_MAX_AGENTS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of A2A agents allowed&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_DEFAULT_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Default timeout for A2A HTTP requests (seconds)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_MAX_RETRIES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum retry attempts for A2A calls&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MCPGATEWAY_A2A_METRICS_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable A2A agent metrics collection&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ü§ñ &lt;strong&gt;A2A Integration&lt;/strong&gt;: Register external AI agents (OpenAI, Anthropic, custom) and expose them as MCP tools üìä &lt;strong&gt;Metrics&lt;/strong&gt;: Track agent performance, success rates, and response times üîí &lt;strong&gt;Security&lt;/strong&gt;: Encrypted credential storage and configurable authentication üéõÔ∏è &lt;strong&gt;Admin UI&lt;/strong&gt;: Dedicated tab for agent management with test functionality&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;A2A Configuration Effects:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;MCPGATEWAY_A2A_ENABLED=false&lt;/code&gt;: Completely disables A2A features (API endpoints return 404, admin tab hidden)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;MCPGATEWAY_A2A_METRICS_ENABLED=false&lt;/code&gt;: Disables metrics collection while keeping functionality&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Security&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SKIP_SSL_VERIFY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Skip upstream TLS verification&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ENVIRONMENT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Deployment environment (affects security defaults)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;development&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;development&lt;/code&gt;/&lt;code&gt;production&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;APP_DOMAIN&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Domain for production CORS origins&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ALLOWED_ORIGINS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;CORS allow-list&lt;/td&gt; 
    &lt;td&gt;Auto-configured by environment&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;CORS_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable CORS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;CORS_ALLOW_CREDENTIALS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Allow credentials in CORS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SECURE_COOKIES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Force secure cookie flags&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;COOKIE_SAMESITE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Cookie SameSite attribute&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lax&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;strict&lt;/code&gt;/&lt;code&gt;lax&lt;/code&gt;/&lt;code&gt;none&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SECURITY_HEADERS_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable security headers middleware&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;X_FRAME_OPTIONS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;X-Frame-Options header value&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;DENY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;DENY&lt;/code&gt;/&lt;code&gt;SAMEORIGIN&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HSTS_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable HSTS header&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HSTS_MAX_AGE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;HSTS max age in seconds&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;31536000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REMOVE_SERVER_HEADERS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Remove server identification&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DOCS_ALLOW_BASIC_AUTH&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Allow Basic Auth for docs (in addition to JWT)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;CORS Configuration&lt;/strong&gt;: When &lt;code&gt;ENVIRONMENT=development&lt;/code&gt;, CORS origins are automatically configured for common development ports (3000, 8080, gateway port). In production, origins are constructed from &lt;code&gt;APP_DOMAIN&lt;/code&gt; (e.g., &lt;code&gt;https://yourdomain.com&lt;/code&gt;, &lt;code&gt;https://app.yourdomain.com&lt;/code&gt;). You can override this by explicitly setting &lt;code&gt;ALLOWED_ORIGINS&lt;/code&gt;.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Security Headers&lt;/strong&gt;: The gateway automatically adds configurable security headers to all responses including CSP, X-Frame-Options, X-Content-Type-Options, X-Download-Options, and HSTS (on HTTPS). All headers can be individually enabled/disabled. Sensitive server headers are removed.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;iframe Embedding&lt;/strong&gt;: By default, &lt;code&gt;X-Frame-Options: DENY&lt;/code&gt; prevents iframe embedding for security. To allow embedding, set &lt;code&gt;X_FRAME_OPTIONS=SAMEORIGIN&lt;/code&gt; (same domain) or disable with &lt;code&gt;X_FRAME_OPTIONS=""&lt;/code&gt;. Also update CSP &lt;code&gt;frame-ancestors&lt;/code&gt; directive if needed.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Cookie Security&lt;/strong&gt;: Authentication cookies are automatically configured with HttpOnly, Secure (in production), and SameSite attributes for CSRF protection.&lt;/p&gt; 
  &lt;p&gt;Note: do not quote the ALLOWED_ORIGINS values, this needs to be valid JSON, such as: ALLOWED_ORIGINS=["http://localhost", "http://localhost:4444"]&lt;/p&gt; 
  &lt;p&gt;Documentation endpoints (&lt;code&gt;/docs&lt;/code&gt;, &lt;code&gt;/redoc&lt;/code&gt;, &lt;code&gt;/openapi.json&lt;/code&gt;) are always protected by authentication. By default, they require Bearer token authentication. Setting &lt;code&gt;DOCS_ALLOW_BASIC_AUTH=true&lt;/code&gt; enables HTTP Basic Authentication as an additional method using the same credentials as &lt;code&gt;BASIC_AUTH_USER&lt;/code&gt; and &lt;code&gt;BASIC_AUTH_PASSWORD&lt;/code&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Logging&lt;/h3&gt; 
 &lt;p&gt;MCP Gateway provides flexible logging with &lt;strong&gt;stdout/stderr output by default&lt;/strong&gt; and &lt;strong&gt;optional file-based logging&lt;/strong&gt;. When file logging is enabled, it provides JSON formatting for structured logs and text formatting for console output.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_LEVEL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Minimum log level&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;INFO&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;DEBUG&lt;/code&gt;...&lt;code&gt;CRITICAL&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_FORMAT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Console log format&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;json&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;json&lt;/code&gt;, &lt;code&gt;text&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_TO_FILE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;Enable file logging&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;&lt;code&gt;false&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_FILE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Log filename (when enabled)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;null&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgateway.log&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_FOLDER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Directory for log files&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;null&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;logs&lt;/code&gt;, &lt;code&gt;/var/log/gateway&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_FILEMODE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;File write mode&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;a+&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;a+&lt;/code&gt; (append), &lt;code&gt;w&lt;/code&gt; (overwrite)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_ROTATION_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;Enable log file rotation&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;&lt;code&gt;false&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_MAX_SIZE_MB&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max file size before rotation (MB)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Any positive integer&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;LOG_BACKUP_COUNT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Number of backup files to keep&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Any non-negative integer&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Logging Behavior:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Default&lt;/strong&gt;: Logs only to &lt;strong&gt;stdout/stderr&lt;/strong&gt; with human-readable text format&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;File Logging&lt;/strong&gt;: When &lt;code&gt;LOG_TO_FILE=true&lt;/code&gt;, logs to &lt;strong&gt;both&lt;/strong&gt; file (JSON format) and console (text format)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Log Rotation&lt;/strong&gt;: When &lt;code&gt;LOG_ROTATION_ENABLED=true&lt;/code&gt;, files rotate at &lt;code&gt;LOG_MAX_SIZE_MB&lt;/code&gt; with &lt;code&gt;LOG_BACKUP_COUNT&lt;/code&gt; backup files (e.g., &lt;code&gt;.log.1&lt;/code&gt;, &lt;code&gt;.log.2&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Directory Creation&lt;/strong&gt;: Log folder is automatically created if it doesn't exist&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Centralized Service&lt;/strong&gt;: All modules use the unified &lt;code&gt;LoggingService&lt;/code&gt; for consistent formatting&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Example Configurations:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Default: stdout/stderr only (recommended for containers)
LOG_LEVEL=INFO
# No additional config needed - logs to stdout/stderr

# Optional: Enable file logging (no rotation)
LOG_TO_FILE=true
LOG_FOLDER=/var/log/mcpgateway
LOG_FILE=gateway.log
LOG_FILEMODE=a+

# Optional: Enable file logging with rotation
LOG_TO_FILE=true
LOG_ROTATION_ENABLED=true
LOG_MAX_SIZE_MB=10
LOG_BACKUP_COUNT=3
LOG_FOLDER=/var/log/mcpgateway
LOG_FILE=gateway.log
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Default Behavior:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Logs are written &lt;strong&gt;only to stdout/stderr&lt;/strong&gt; in human-readable text format&lt;/li&gt; 
  &lt;li&gt;File logging is &lt;strong&gt;disabled by default&lt;/strong&gt; (no files created)&lt;/li&gt; 
  &lt;li&gt;Set &lt;code&gt;LOG_TO_FILE=true&lt;/code&gt; to enable optional file logging with JSON format&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Observability (OpenTelemetry)&lt;/h3&gt; 
 &lt;p&gt;MCP Gateway includes &lt;strong&gt;vendor-agnostic OpenTelemetry support&lt;/strong&gt; for distributed tracing. Works with Phoenix, Jaeger, Zipkin, Tempo, DataDog, New Relic, and any OTLP-compatible backend.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_ENABLE_OBSERVABILITY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Master switch for observability&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_SERVICE_NAME&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Service identifier in traces&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcp-gateway&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_SERVICE_VERSION&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Service version in traces&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.6.0&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_DEPLOYMENT_ENVIRONMENT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Environment tag (dev/staging/prod)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;development&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_TRACES_EXPORTER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Trace exporter backend&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;otlp&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;otlp&lt;/code&gt;, &lt;code&gt;jaeger&lt;/code&gt;, &lt;code&gt;zipkin&lt;/code&gt;, &lt;code&gt;console&lt;/code&gt;, &lt;code&gt;none&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_RESOURCE_ATTRIBUTES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Custom resource attributes&lt;/td&gt; 
    &lt;td&gt;(empty)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;key=value,key2=value2&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;OTLP Configuration&lt;/strong&gt; (for Phoenix, Tempo, DataDog, etc.):&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_OTLP_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;OTLP collector endpoint&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:4317&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_OTLP_PROTOCOL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;OTLP protocol&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;grpc&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;grpc&lt;/code&gt;, &lt;code&gt;http/protobuf&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_OTLP_HEADERS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Authentication headers&lt;/td&gt; 
    &lt;td&gt;(empty)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;api-key=secret,x-auth=token&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_OTLP_INSECURE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Skip TLS verification&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Alternative Backends&lt;/strong&gt; (optional):&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_JAEGER_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Jaeger collector endpoint&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:14268/api/traces&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;URL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_EXPORTER_ZIPKIN_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Zipkin collector endpoint&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:9411/api/v2/spans&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;URL&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Performance Tuning&lt;/strong&gt;:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_TRACES_SAMPLER&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Sampling strategy&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;parentbased_traceidratio&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;always_on&lt;/code&gt;, &lt;code&gt;always_off&lt;/code&gt;, &lt;code&gt;traceidratio&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_TRACES_SAMPLER_ARG&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Sample rate (0.0-1.0)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0.1&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;float (0.1 = 10% sampling)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_BSP_MAX_QUEUE_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max queued spans&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;2048&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_BSP_MAX_EXPORT_BATCH_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max batch size for export&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;512&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;OTEL_BSP_SCHEDULE_DELAY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Export interval (ms)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Quick Start with Phoenix&lt;/strong&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Start Phoenix for LLM observability
docker run -p 6006:6006 -p 4317:4317 arizephoenix/phoenix:latest

# Configure gateway
export OTEL_ENABLE_OBSERVABILITY=true
export OTEL_TRACES_EXPORTER=otlp
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Run gateway - traces automatically sent to Phoenix
mcpgateway
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;üîç &lt;strong&gt;What Gets Traced&lt;/strong&gt;: Tool invocations, prompt rendering, resource fetching, gateway federation, health checks, plugin execution (if enabled)&lt;/p&gt; 
  &lt;p&gt;üöÄ &lt;strong&gt;Zero Overhead&lt;/strong&gt;: When &lt;code&gt;OTEL_ENABLE_OBSERVABILITY=false&lt;/code&gt;, all tracing is disabled with no performance impact&lt;/p&gt; 
  &lt;p&gt;üìä &lt;strong&gt;View Traces&lt;/strong&gt;: Phoenix UI at &lt;code&gt;http://localhost:6006&lt;/code&gt;, Jaeger at &lt;code&gt;http://localhost:16686&lt;/code&gt;, or your configured backend&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Transport&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TRANSPORT_TYPE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enabled transports&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http&lt;/code&gt;,&lt;code&gt;ws&lt;/code&gt;,&lt;code&gt;sse&lt;/code&gt;,&lt;code&gt;stdio&lt;/code&gt;,&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;WEBSOCKET_PING_INTERVAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;WebSocket ping (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSE_RETRY_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;SSE retry timeout (ms)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSE_KEEPALIVE_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable SSE keepalive events&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;SSE_KEEPALIVE_INTERVAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;SSE keepalive interval (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;USE_STATEFUL_SESSIONS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;streamable http config&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;JSON_RESPONSE_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;json/sse streams (streamable http)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üí° SSE Keepalive Events&lt;/strong&gt;: The gateway sends periodic keepalive events to prevent connection timeouts with proxies and load balancers. Disable with &lt;code&gt;SSE_KEEPALIVE_ENABLED=false&lt;/code&gt; if your client doesn't handle unknown event types. Common intervals: 30s (default), 60s (AWS ALB), 240s (Azure).&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Federation&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_ENABLED&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable federation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_DISCOVERY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Auto-discover peers&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_PEERS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Comma-sep peer URLs&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;[]&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Gateway timeout (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEDERATION_SYNC_INTERVAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Sync interval (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;300&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Resources&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RESOURCE_CACHE_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;LRU cache size&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RESOURCE_CACHE_TTL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Cache TTL (seconds)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3600&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MAX_RESOURCE_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max resource bytes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10485760&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;ALLOWED_MIME_TYPES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Acceptable MIME types&lt;/td&gt; 
    &lt;td&gt;see code&lt;/td&gt; 
    &lt;td&gt;JSON array&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Tools&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TOOL_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Tool invocation timeout (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;60&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MAX_TOOL_RETRIES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max retry attempts&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int ‚â• 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TOOL_RATE_LIMIT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Tool calls per minute&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TOOL_CONCURRENT_LIMIT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Concurrent tool invocations&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Prompts&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PROMPT_CACHE_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Cached prompt templates&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;MAX_PROMPT_SIZE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max prompt template size (bytes)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;102400&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;PROMPT_RENDER_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Jinja render timeout (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Health Checks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HEALTH_CHECK_INTERVAL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Health poll interval (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;60&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;HEALTH_CHECK_TIMEOUT&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Health request timeout (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;UNHEALTHY_THRESHOLD&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Fail-count before peer deactivation,&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Set to -1 if deactivation is not needed.&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Database&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_POOL_SIZE&lt;/code&gt; .&lt;/td&gt; 
    &lt;td&gt;SQLAlchemy connection pool size&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;200&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_MAX_OVERFLOW&lt;/code&gt;.&lt;/td&gt; 
    &lt;td&gt;Extra connections beyond pool&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int ‚â• 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_POOL_TIMEOUT&lt;/code&gt;.&lt;/td&gt; 
    &lt;td&gt;Wait for connection (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_POOL_RECYCLE&lt;/code&gt;.&lt;/td&gt; 
    &lt;td&gt;Recycle connections (secs)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3600&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_MAX_RETRIES&lt;/code&gt; .&lt;/td&gt; 
    &lt;td&gt;Max Retry Attempts&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DB_RETRY_INTERVAL_MS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Retry Interval (ms)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;2000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Cache Backend&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;CACHE_TYPE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Backend (&lt;code&gt;memory&lt;/code&gt;/&lt;code&gt;redis&lt;/code&gt;)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;memory&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;none&lt;/code&gt;, &lt;code&gt;memory&lt;/code&gt;,&lt;code&gt;redis&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REDIS_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Redis connection URL&lt;/td&gt; 
    &lt;td&gt;(none)&lt;/td&gt; 
    &lt;td&gt;string or empty&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;CACHE_PREFIX&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Key prefix&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;mcpgw:&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REDIS_MAX_RETRIES&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Max Retry Attempts&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;REDIS_RETRY_INTERVAL_MS&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Retry Interval (ms)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;2000&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int &amp;gt; 0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;üß† &lt;code&gt;none&lt;/code&gt; disables caching entirely. Use &lt;code&gt;memory&lt;/code&gt; for dev, &lt;code&gt;database&lt;/code&gt; for persistence, or &lt;code&gt;redis&lt;/code&gt; for distributed caching.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Database Management&lt;/h3&gt; 
 &lt;p&gt;MCP Gateway uses Alembic for database migrations. Common commands:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;make db-current&lt;/code&gt; - Show current database version&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make db-upgrade&lt;/code&gt; - Apply pending migrations&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make db-migrate&lt;/code&gt; - Create new migration&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make db-history&lt;/code&gt; - Show migration history&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;make db-status&lt;/code&gt; - Detailed migration status&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;Troubleshooting&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;Common Issues:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;"No 'script_location' key found"&lt;/strong&gt;: Ensure you're running from the project root directory.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;"Unknown SSE event: keepalive" warnings&lt;/strong&gt;: Some MCP clients don't recognize keepalive events. These warnings are harmless and don't affect functionality. To disable: &lt;code&gt;SSE_KEEPALIVE_ENABLED=false&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connection timeouts with proxies/load balancers&lt;/strong&gt;: If experiencing timeouts, adjust keepalive interval to match your infrastructure: &lt;code&gt;SSE_KEEPALIVE_INTERVAL=60&lt;/code&gt; (AWS ALB) or &lt;code&gt;240&lt;/code&gt; (Azure).&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Development&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Setting&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Default&lt;/th&gt; 
    &lt;th&gt;Options&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DEV_MODE&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Enable dev mode&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;RELOAD&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Auto-reload on changes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;DEBUG&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Debug logging&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running&lt;/h2&gt; 
&lt;h3&gt;Makefile&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt; make serve               # Run production Gunicorn server on
 make serve-ssl           # Run Gunicorn behind HTTPS on :4444 (uses ./certs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Script helper&lt;/h3&gt; 
&lt;p&gt;To run the development (uvicorn) server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make dev
# or
./run.sh --reload --log debug --workers 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;code&gt;run.sh&lt;/code&gt; is a wrapper around &lt;code&gt;uvicorn&lt;/code&gt; that loads &lt;code&gt;.env&lt;/code&gt;, supports reload, and passes arguments to the server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Key flags:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-e, --env FILE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;load env-file&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--env prod.env&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-H, --host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;bind address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--host 127.0.0.1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-p, --port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;listen port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--port 8080&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-w, --workers&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;gunicorn workers&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--workers 4&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-r, --reload&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;auto-reload&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;--reload&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Manual (Uvicorn)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvicorn mcpgateway.main:app --host 0.0.0.0 --port 4444 --workers 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Authentication examples&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Generate a JWT token using JWT_SECRET_KEY and export it as MCPGATEWAY_BEARER_TOKEN
# Note that the module needs to be installed. If running locally use:
export MCPGATEWAY_BEARER_TOKEN=$(JWT_SECRET_KEY=my-test-key python3 -m mcpgateway.utils.create_jwt_token)

# Use the JWT token in an API call
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚òÅÔ∏è AWS / Azure / OpenShift&lt;/h2&gt; 
&lt;p&gt;Deployment details can be found in the GitHub Pages.&lt;/p&gt; 
&lt;h2&gt;‚òÅÔ∏è IBM Cloud Code Engine Deployment&lt;/h2&gt; 
&lt;p&gt;This project supports deployment to &lt;a href="https://cloud.ibm.com/codeengine"&gt;IBM Cloud Code Engine&lt;/a&gt; using the &lt;strong&gt;ibmcloud&lt;/strong&gt; CLI and the IBM Container Registry.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;‚òÅÔ∏è IBM Cloud Code Engine Deployment&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;üîß Prerequisites&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Podman &lt;strong&gt;or&lt;/strong&gt; Docker installed locally&lt;/li&gt; 
  &lt;li&gt;IBM Cloud CLI (use &lt;code&gt;make ibmcloud-cli-install&lt;/code&gt; to install)&lt;/li&gt; 
  &lt;li&gt;An &lt;a href="https://cloud.ibm.com/iam/apikeys"&gt;IBM Cloud API key&lt;/a&gt; with access to Code Engine &amp;amp; Container Registry&lt;/li&gt; 
  &lt;li&gt;Code Engine and Container Registry services &lt;strong&gt;enabled&lt;/strong&gt; in your IBM Cloud account&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üì¶ Environment Variables&lt;/h3&gt; 
 &lt;p&gt;Create a &lt;strong&gt;&lt;code&gt;.env&lt;/code&gt;&lt;/strong&gt; file (or export the variables in your shell). The first block is &lt;strong&gt;required&lt;/strong&gt;; the second provides &lt;strong&gt;tunable defaults&lt;/strong&gt; you can override:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ‚îÄ‚îÄ Required ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
IBMCLOUD_REGION=us-south
IBMCLOUD_RESOURCE_GROUP=default
IBMCLOUD_PROJECT=my-codeengine-project
IBMCLOUD_CODE_ENGINE_APP=mcpgateway
IBMCLOUD_IMAGE_NAME=us.icr.io/myspace/mcpgateway:latest
IBMCLOUD_IMG_PROD=mcpgateway/mcpgateway
IBMCLOUD_API_KEY=your_api_key_here   # Optional - omit to use interactive `ibmcloud login --sso`

# ‚îÄ‚îÄ Optional overrides (sensible defaults provided) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
IBMCLOUD_CPU=1                       # vCPUs for the app
IBMCLOUD_MEMORY=4G                   # Memory allocation
IBMCLOUD_REGISTRY_SECRET=my-regcred  # Name of the Container Registry secret
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;‚úÖ &lt;strong&gt;Quick check:&lt;/strong&gt; &lt;code&gt;make ibmcloud-check-env&lt;/code&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üöÄ Make Targets&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Target&lt;/th&gt; 
    &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-cli-install&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Install IBM Cloud CLI and required plugins&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-login&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Log in to IBM Cloud (API key or SSO)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-ce-login&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Select the Code Engine project &amp;amp; region&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-tag&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Tag the local container image&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-push&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Push the image to IBM Container Registry&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-deploy&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;Create or update&lt;/strong&gt; the Code Engine application (uses CPU/memory/secret)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-ce-status&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Show current deployment status&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-ce-logs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Stream logs from the running app&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;make ibmcloud-ce-rm&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Delete the Code Engine application&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üìù Example Workflow&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;make ibmcloud-check-env
make ibmcloud-cli-install
make ibmcloud-login
make ibmcloud-ce-login
make ibmcloud-tag
make ibmcloud-push
make ibmcloud-deploy
make ibmcloud-ce-status
make ibmcloud-ce-logs
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;API Endpoints&lt;/h2&gt; 
&lt;p&gt;You can test the API endpoints through curl, or Swagger UI, and check detailed documentation on ReDoc:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Swagger UI&lt;/strong&gt; ‚Üí &lt;a href="http://localhost:4444/docs"&gt;http://localhost:4444/docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ReDoc&lt;/strong&gt; ‚Üí &lt;a href="http://localhost:4444/redoc"&gt;http://localhost:4444/redoc&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Generate an API Bearer token, and test the various API endpoints.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîê Authentication &amp;amp; Health Checks&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Generate a bearer token using the configured secret key (use the same as your .env)
export MCPGATEWAY_BEARER_TOKEN=$(python3 -m mcpgateway.utils.create_jwt_token -u admin --secret my-test-key)
echo ${MCPGATEWAY_BEARER_TOKEN}

# Quickly confirm that authentication works and the gateway is healthy
curl -s -k -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" https://localhost:4444/health
# {"status":"healthy"}

# Quickly confirm the gateway version &amp;amp; DB connectivity
curl -s -k -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" https://localhost:4444/version | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üß± Protocol APIs (MCP) /protocol&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Initialize MCP session
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "protocol_version":"2025-03-26",
           "capabilities":{},
           "client_info":{"name":"MyClient","version":"1.0.0"}
         }' \
     http://localhost:4444/protocol/initialize

# Ping (JSON-RPC style)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"jsonrpc":"2.0","id":1,"method":"ping"}' \
     http://localhost:4444/protocol/ping

# Completion for prompt/resource arguments (not implemented)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "ref":{"type":"ref/prompt","name":"example_prompt"},
           "argument":{"name":"topic","value":"py"}
         }' \
     http://localhost:4444/protocol/completion/complete

# Sampling (streaming) (not implemented)
curl -N -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "messages":[{"role":"user","content":{"type":"text","text":"Hello"}}],
           "maxTokens":16
         }' \
     http://localhost:4444/protocol/sampling/createMessage
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üß† JSON-RPC Utility /rpc&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Generic JSON-RPC calls (tools, gateways, roots, etc.)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"jsonrpc":"2.0","id":1,"method":"list_tools"}' \
     http://localhost:4444/rpc
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Handles any method name: &lt;code&gt;list_tools&lt;/code&gt;, &lt;code&gt;list_gateways&lt;/code&gt;, &lt;code&gt;prompts/get&lt;/code&gt;, or invokes a tool if method matches a registered tool name .&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Tool Management /tools&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Register a new tool
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "name":"clock_tool",
           "url":"http://localhost:9000/rpc",
           "description":"Returns current time",
           "input_schema":{
             "type":"object",
             "properties":{"timezone":{"type":"string"}},
             "required":[]
           }
         }' \
     http://localhost:4444/tools

# List tools
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools

# Get tool by ID
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools/1

# Update tool
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{ "description":"Updated desc" }' \
     http://localhost:4444/tools/1

# Toggle active status
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/tools/1/toggle?activate=false
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/tools/1/toggle?activate=true

# Delete tool
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/tools/1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ü§ñ A2A Agent Management /a2a&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Register a new A2A agent
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "name":"hello_world_agent",
           "endpoint_url":"http://localhost:9999/",
           "agent_type":"jsonrpc",
           "description":"External AI agent for hello world functionality",
           "auth_type":"api_key",
           "auth_value":"your-api-key",
           "tags":["ai", "hello-world"]
         }' \
     http://localhost:4444/a2a

# List A2A agents
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/a2a

# Get agent by ID
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/a2a/agent-id

# Update agent
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{ "description":"Updated description" }' \
     http://localhost:4444/a2a/agent-id

# Test agent (direct invocation)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "parameters": {
             "method": "message/send",
             "params": {
               "message": {
                 "messageId": "test-123",
                 "role": "user",
                 "parts": [{"type": "text", "text": "Hello!"}]
               }
             }
           },
           "interaction_type": "test"
         }' \
     http://localhost:4444/a2a/agent-name/invoke

# Toggle agent status
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/a2a/agent-id/toggle?activate=false

# Delete agent
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/a2a/agent-id

# Associate agent with virtual server (agents become available as MCP tools)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "name":"AI Assistant Server",
           "description":"Virtual server with AI agents",
           "associated_a2a_agents":["agent-id"]
         }' \
     http://localhost:4444/servers
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ü§ñ &lt;strong&gt;A2A Integration&lt;/strong&gt;: A2A agents are external AI agents that can be registered and exposed as MCP tools üîÑ &lt;strong&gt;Protocol Detection&lt;/strong&gt;: Gateway automatically detects JSONRPC vs custom A2A protocols üìä &lt;strong&gt;Testing&lt;/strong&gt;: Built-in test functionality via Admin UI or &lt;code&gt;/a2a/{agent_id}/test&lt;/code&gt; endpoint üéõÔ∏è &lt;strong&gt;Virtual Servers&lt;/strong&gt;: Associate agents with servers to expose them as standard MCP tools&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üåê Gateway Management /gateways&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Register an MCP server as a new gateway provider
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"name":"peer_gateway","url":"http://peer:4444"}' \
     http://localhost:4444/gateways

# List gateways
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/gateways

# Get gateway by ID
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/gateways/1

# Update gateway
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"description":"New description"}' \
     http://localhost:4444/gateways/1

# Toggle active status
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/gateways/1/toggle?activate=false

# Delete gateway
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/gateways/1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìÅ Resource Management /resources&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Register resource
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "uri":"config://app/settings",
           "name":"App Settings",
           "content":"key=value"
         }' \
     http://localhost:4444/resources

# List resources
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/resources

# Read a resource
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/resources/config://app/settings

# Update resource
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"content":"new=value"}' \
     http://localhost:4444/resources/config://app/settings

# Delete resource
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/resources/config://app/settings

# Subscribe to updates (SSE)
curl -N -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/resources/subscribe/config://app/settings
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìù Prompt Management /prompts&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Create prompt template
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
           "name":"greet",
           "template":"Hello, {{ user }}!",
           "argument_schema":{
             "type":"object",
             "properties":{"user":{"type":"string"}},
             "required":["user"]
           }
         }' \
     http://localhost:4444/prompts

# List prompts
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/prompts

# Get prompt (with args)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"user":"Alice"}' \
     http://localhost:4444/prompts/greet

# Get prompt (no args)
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/prompts/greet

# Update prompt
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"template":"Hi, {{ user }}!"}' \
     http://localhost:4444/prompts/greet

# Toggle active
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/prompts/5/toggle?activate=false

# Delete prompt
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/prompts/greet
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üå≤ Root Management /roots&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# List roots
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/roots

# Add root
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"uri":"/data","name":"Data Root"}' \
     http://localhost:4444/roots

# Remove root
curl -X DELETE -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/roots/%2Fdata

# Subscribe to root changes (SSE)
curl -N -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/roots/changes
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üñ•Ô∏è Server Management /servers&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# List servers
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/servers

# Get server
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/servers/UUID_OF_SERVER_1

# Create server
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"name":"db","description":"Database","associatedTools": ["1","2","3"]}' \
     http://localhost:4444/servers

# Update server
curl -X PUT -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"description":"Updated"}' \
     http://localhost:4444/servers/UUID_OF_SERVER_1

# Toggle active
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
     http://localhost:4444/servers/UUID_OF_SERVER_1/toggle?activate=false
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìä Metrics /metrics&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Get aggregated metrics
curl -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/metrics

# Reset metrics (all or per-entity)
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/metrics/reset
curl -X POST -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/metrics/reset?entity=tool&amp;amp;id=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üì° Events &amp;amp; Health&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# SSE: all events
curl -N -H "Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" http://localhost:4444/events

# WebSocket
wscat -c ws://localhost:4444/ws \
      -H "Authorization: Basic $(echo -n admin:changeme|base64)"

# Health check
curl http://localhost:4444/health
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Full Swagger UI at &lt;code&gt;/docs&lt;/code&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Sample Tool&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;uvicorn sample_tool.clock_tool:app --host 0.0.0.0 --port 9000
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST -H "Content-Type: application/json" \
     -d '{"jsonrpc":"2.0","id":1,"method":"get_time","params":{"timezone":"UTC"}}' \
     http://localhost:9000/rpc
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make test            # Run unit tests
make lint            # Run lint tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Doctest Coverage&lt;/h2&gt; 
&lt;p&gt;MCP Context Forge implements comprehensive doctest coverage to ensure all code examples in documentation are tested and verified:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make doctest         # Run all doctests
make doctest-verbose # Run with detailed output
make doctest-coverage # Generate coverage report
make doctest-check   # Check coverage percentage
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Coverage Status:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Transport Modules&lt;/strong&gt;: 100% (base, stdio, SSE, WebSocket, streamable HTTP)&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Utility Functions&lt;/strong&gt;: 100% (slug generation, JWT tokens, validation)&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Configuration&lt;/strong&gt;: 100% (settings, environment variables)&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Service Classes&lt;/strong&gt;: ~60% (in progress)&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Complex Classes&lt;/strong&gt;: ~40% (in progress)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All documented examples are automatically tested&lt;/li&gt; 
 &lt;li&gt;Documentation stays accurate and up-to-date&lt;/li&gt; 
 &lt;li&gt;Developers can run examples directly from docstrings&lt;/li&gt; 
 &lt;li&gt;Regression prevention through automated verification&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For detailed information, see the &lt;a href="https://ibm.github.io/mcp-context-forge/development/doctest-coverage/"&gt;Doctest Coverage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Project Structure&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìÅ Directory and file structure for mcpgateway&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CI / Quality &amp;amp; Meta-files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îú‚îÄ‚îÄ .bumpversion.cfg                # Automated semantic-version bumps
‚îú‚îÄ‚îÄ .coveragerc                     # Coverage.py settings
‚îú‚îÄ‚îÄ .darglint                       # Doc-string linter rules
‚îú‚îÄ‚îÄ .dockerignore                   # Context exclusions for image builds
‚îú‚îÄ‚îÄ .editorconfig                   # Consistent IDE / editor behaviour
‚îú‚îÄ‚îÄ .env                            # Local runtime variables (git-ignored)
‚îú‚îÄ‚îÄ .env.ce                         # IBM Code Engine runtime env (ignored)
‚îú‚îÄ‚îÄ .env.ce.example                 # Sample env for IBM Code Engine
‚îú‚îÄ‚îÄ .env.example                    # Generic sample env file
‚îú‚îÄ‚îÄ .env.gcr                        # Google Cloud Run runtime env (ignored)
‚îú‚îÄ‚îÄ .eslintrc.json                  # ESLint rules for JS / TS assets
‚îú‚îÄ‚îÄ .flake8                         # Flake-8 configuration
‚îú‚îÄ‚îÄ .gitattributes                  # Git attributes (e.g. EOL normalisation)
‚îú‚îÄ‚îÄ .github                         # GitHub settings, CI/CD workflows &amp;amp; templates
‚îÇ   ‚îú‚îÄ‚îÄ CODEOWNERS                  # Default reviewers
‚îÇ   ‚îî‚îÄ‚îÄ workflows/                  # Bandit, Docker, CodeQL, Python Package, Container Deployment, etc.
‚îú‚îÄ‚îÄ .gitignore                      # Git exclusion rules
‚îú‚îÄ‚îÄ .hadolint.yaml                  # Hadolint rules for Dockerfiles
‚îú‚îÄ‚îÄ .htmlhintrc                     # HTMLHint rules
‚îú‚îÄ‚îÄ .markdownlint.json              # Markdown-lint rules
‚îú‚îÄ‚îÄ .pre-commit-config.yaml         # Pre-commit hooks (ruff, black, mypy, ...)
‚îú‚îÄ‚îÄ .pycodestyle                    # PEP-8 checker settings
‚îú‚îÄ‚îÄ .pylintrc                       # Pylint configuration
‚îú‚îÄ‚îÄ .pyspelling.yml                 # Spell-checker dictionary &amp;amp; filters
‚îú‚îÄ‚îÄ .ruff.toml                      # Ruff linter / formatter settings
‚îú‚îÄ‚îÄ .spellcheck-en.txt              # Extra dictionary entries
‚îú‚îÄ‚îÄ .stylelintrc.json               # Stylelint rules for CSS
‚îú‚îÄ‚îÄ .travis.yml                     # Legacy Travis CI config (reference)
‚îú‚îÄ‚îÄ .whitesource                    # WhiteSource security-scanning config
‚îú‚îÄ‚îÄ .yamllint                       # yamllint ruleset

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Documentation &amp;amp; Guidance ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îú‚îÄ‚îÄ CHANGELOG.md                    # Version-by-version change log
‚îú‚îÄ‚îÄ CODE_OF_CONDUCT.md              # Community behaviour guidelines
‚îú‚îÄ‚îÄ CONTRIBUTING.md                 # How to file issues &amp;amp; send PRs
‚îú‚îÄ‚îÄ DEVELOPING.md                   # Contributor workflows &amp;amp; style guide
‚îú‚îÄ‚îÄ LICENSE                         # Apache License 2.0
‚îú‚îÄ‚îÄ README.md                       # Project overview &amp;amp; quick-start
‚îú‚îÄ‚îÄ SECURITY.md                     # Security policy &amp;amp; CVE disclosure process
‚îú‚îÄ‚îÄ TESTING.md                      # Testing strategy, fixtures &amp;amp; guidelines

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Containerisation &amp;amp; Runtime ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îú‚îÄ‚îÄ Containerfile                   # OCI image build (Docker / Podman)
‚îú‚îÄ‚îÄ Containerfile.lite              # FROM scratch UBI-Micro production build
‚îú‚îÄ‚îÄ docker-compose.yml              # Local multi-service stack
‚îú‚îÄ‚îÄ podman-compose-sonarqube.yaml   # One-liner SonarQube stack
‚îú‚îÄ‚îÄ run-gunicorn.sh                 # Opinionated Gunicorn startup script
‚îú‚îÄ‚îÄ run.sh                          # Uvicorn shortcut with arg parsing

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Build / Packaging / Tooling ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îú‚îÄ‚îÄ MANIFEST.in                     # sdist inclusion rules
‚îú‚îÄ‚îÄ Makefile                        # Dev &amp;amp; deployment targets
‚îú‚îÄ‚îÄ package-lock.json               # Deterministic npm lock-file
‚îú‚îÄ‚îÄ package.json                    # Front-end / docs tooling deps
‚îú‚îÄ‚îÄ pyproject.toml                  # Poetry / PDM config &amp;amp; lint rules
‚îú‚îÄ‚îÄ sonar-code.properties           # SonarQube analysis settings
‚îú‚îÄ‚îÄ uv.lock                         # UV resolver lock-file

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Kubernetes &amp;amp; Helm Assets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îú‚îÄ‚îÄ charts                          # Helm chart(s) for K8s / OpenShift
‚îÇ   ‚îú‚îÄ‚îÄ mcp-stack                   # Umbrella chart
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Chart.yaml              # Chart metadata
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ templates/...             # Manifest templates
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ values.yaml             # Default values
‚îÇ   ‚îî‚îÄ‚îÄ README.md                   # Install / upgrade guide
‚îú‚îÄ‚îÄ k8s                             # Raw (non-Helm) K8s manifests
‚îÇ   ‚îî‚îÄ‚îÄ *.yaml                      # Deployment, Service, PVC resources

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Documentation Source ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îú‚îÄ‚îÄ docs                            # MkDocs site source
‚îÇ   ‚îú‚îÄ‚îÄ base.yml                    # MkDocs "base" configuration snippet (do not modify)
‚îÇ   ‚îú‚îÄ‚îÄ mkdocs.yml                  # Site configuration (requires base.yml)
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies for the MkDocs site
‚îÇ   ‚îú‚îÄ‚îÄ Makefile                    # Make targets for building/serving the docs
‚îÇ   ‚îî‚îÄ‚îÄ theme                       # Custom MkDocs theme assets
‚îÇ       ‚îî‚îÄ‚îÄ logo.png                # Logo for the documentation theme
‚îÇ   ‚îî‚îÄ‚îÄ docs                        # Markdown documentation
‚îÇ       ‚îú‚îÄ‚îÄ architecture/           # ADRs for the project
‚îÇ       ‚îú‚îÄ‚îÄ articles/               # Long-form writeups
‚îÇ       ‚îú‚îÄ‚îÄ blog/                   # Blog posts
‚îÇ       ‚îú‚îÄ‚îÄ deployment/             # Deployment guides (AWS, Azure, etc.)
‚îÇ       ‚îú‚îÄ‚îÄ development/            # Development workflows &amp;amp; CI docs
‚îÇ       ‚îú‚îÄ‚îÄ images/                 # Diagrams &amp;amp; screenshots
‚îÇ       ‚îú‚îÄ‚îÄ index.md                # Top-level docs landing page
‚îÇ       ‚îú‚îÄ‚îÄ manage/                 # Management topics (backup, logging, tuning, upgrade)
‚îÇ       ‚îú‚îÄ‚îÄ overview/               # Feature overviews &amp;amp; UI documentation
‚îÇ       ‚îú‚îÄ‚îÄ security/               # Security guidance &amp;amp; policies
‚îÇ       ‚îú‚îÄ‚îÄ testing/                # Testing strategy &amp;amp; fixtures
‚îÇ       ‚îî‚îÄ‚îÄ using/                  # User-facing usage guides (agents, clients, etc.)
‚îÇ       ‚îú‚îÄ‚îÄ media/                  # Social media, press coverage, videos &amp;amp; testimonials
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ press/              # Press articles and blog posts
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ social/             # Tweets, LinkedIn posts, YouTube embeds
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ testimonials/       # Customer quotes &amp;amp; community feedback
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ kit/                # Media kit &amp;amp; logos for bloggers &amp;amp; press
‚îú‚îÄ‚îÄ dictionary.dic                  # Custom dictionary for spell-checker (make spellcheck)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Application &amp;amp; Libraries ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îú‚îÄ‚îÄ agent_runtimes                  # Configurable agentic frameworks converted to MCP Servers
‚îú‚îÄ‚îÄ mcpgateway                      # ‚Üê main application package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                 # Package metadata &amp;amp; version constant
‚îÇ   ‚îú‚îÄ‚îÄ admin.py                    # FastAPI routers for Admin UI
‚îÇ   ‚îú‚îÄ‚îÄ cache
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resource_cache.py       # LRU+TTL cache implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session_registry.py     # Session ‚Üî cache mapping
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # Pydantic settings loader
‚îÇ   ‚îú‚îÄ‚îÄ db.py                       # SQLAlchemy models &amp;amp; engine setup
‚îÇ   ‚îú‚îÄ‚îÄ federation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ discovery.py            # Peer-gateway discovery
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ forward.py              # RPC forwarding
‚îÇ   ‚îú‚îÄ‚îÄ handlers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sampling.py             # Streaming sampling handler
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # FastAPI app factory &amp;amp; startup events
‚îÇ   ‚îú‚îÄ‚îÄ mcp.db                      # SQLite fixture for tests
‚îÇ   ‚îú‚îÄ‚îÄ py.typed                    # PEP 561 marker (ships type hints)
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py                  # Shared Pydantic DTOs
‚îÇ   ‚îú‚îÄ‚îÄ services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ completion_service.py   # Prompt / argument completion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gateway_service.py      # Peer-gateway registry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging_service.py      # Central logging helpers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_service.py       # Prompt CRUD &amp;amp; rendering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resource_service.py     # Resource registration &amp;amp; retrieval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ root_service.py         # File-system root registry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server_service.py       # Server registry &amp;amp; monitoring
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tool_service.py         # Tool registry &amp;amp; invocation
‚îÇ   ‚îú‚îÄ‚îÄ static
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ admin.css               # Styles for Admin UI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ admin.js                # Behaviour for Admin UI
‚îÇ   ‚îú‚îÄ‚îÄ templates
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ admin.html              # HTMX/Alpine Admin UI template
‚îÇ   ‚îú‚îÄ‚îÄ transports
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                 # Abstract transport interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sse_transport.py        # Server-Sent Events transport
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stdio_transport.py      # stdio transport for embedding
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ websocket_transport.py  # WS transport with ping/pong
‚îÇ   ‚îú‚îÄ‚îÄ models.py                   # Core enums / type aliases
‚îÇ   ‚îú‚îÄ‚îÄ utils
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_jwt_token.py     # CLI &amp;amp; library for JWT generation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services_auth.py        # Service-to-service auth dependency
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ verify_credentials.py   # Basic / JWT auth helpers
‚îÇ   ‚îú‚îÄ‚îÄ validation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ jsonrpc.py              # JSON-RPC 2.0 validation
‚îÇ   ‚îî‚îÄ‚îÄ version.py                  # Library version helper
‚îú‚îÄ‚îÄ mcpgateway-wrapper              # Stdio client wrapper (PyPI)
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ src/mcpgateway_wrapper/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ server.py               # Wrapper entry-point
‚îú‚îÄ‚îÄ mcp-servers                     # Sample downstream MCP servers
‚îú‚îÄ‚îÄ mcp.db                          # Default SQLite DB (auto-created)
‚îú‚îÄ‚îÄ mcpgrid                         # Experimental grid client / PoC
‚îú‚îÄ‚îÄ os_deps.sh                      # Installs system-level deps for CI

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Tests &amp;amp; QA Assets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îú‚îÄ‚îÄ test_readme.py                  # Guard: README stays in sync
‚îú‚îÄ‚îÄ tests
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                 # Shared fixtures
‚îÇ   ‚îú‚îÄ‚îÄ e2e/...                       # End-to-end scenarios
‚îÇ   ‚îú‚îÄ‚îÄ hey/...                       # Load-test logs &amp;amp; helper script
‚îÇ   ‚îú‚îÄ‚îÄ integration/...               # API-level integration tests
‚îÇ   ‚îî‚îÄ‚îÄ unit/...                      # Pure unit tests for business logic
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;API Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Swagger UI&lt;/strong&gt; ‚Üí &lt;a href="http://localhost:4444/docs"&gt;http://localhost:4444/docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ReDoc&lt;/strong&gt; ‚Üí &lt;a href="http://localhost:4444/redoc"&gt;http://localhost:4444/redoc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Admin Panel&lt;/strong&gt; ‚Üí &lt;a href="http://localhost:4444/admin"&gt;http://localhost:4444/admin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Makefile targets&lt;/h2&gt; 
&lt;p&gt;This project offer the following Makefile targets. Type &lt;code&gt;make&lt;/code&gt; in the project root to show all targets.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Available Makefile targets&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;üêç MCP CONTEXT FORGE  (An enterprise-ready Model Context Protocol Gateway)
üîß SYSTEM-LEVEL DEPENDENCIES (DEV BUILD ONLY)
os-deps              - Install Graphviz, Pandoc, Trivy, SCC used for dev docs generation and security scan
üå± VIRTUAL ENVIRONMENT &amp;amp; INSTALLATION
venv                 - Create a fresh virtual environment with uv &amp;amp; friends
activate             - Activate the virtual environment in the current shell
install              - Install project into the venv
install-dev          - Install project (incl. dev deps) into the venv
install-db           - Install project (incl. postgres and redis) into venv
update               - Update all installed deps inside the venv
check-env            - Verify all required env vars in .env are present
‚ñ∂Ô∏è SERVE &amp;amp; TESTING
serve                - Run production Gunicorn server on :4444
certs                - Generate self-signed TLS cert &amp;amp; key in ./certs (won't overwrite)
serve-ssl            - Run Gunicorn behind HTTPS on :4444 (uses ./certs)
dev                  - Run fast-reload dev server (uvicorn)
run                  - Execute helper script ./run.sh
test                 - Run unit tests with pytest
test-curl            - Smoke-test API endpoints with curl script
pytest-examples      - Run README / examples through pytest-examples
clean                - Remove caches, build artefacts, virtualenv, docs, certs, coverage, SBOM, etc.
üìä COVERAGE &amp;amp; METRICS
coverage             - Run tests with coverage, emit md/HTML/XML + badge
pip-licenses         - Produce dependency license inventory (markdown)
scc                  - Quick LoC/complexity snapshot with scc
scc-report           - Generate HTML LoC &amp;amp; per-file metrics with scc
üìö DOCUMENTATION &amp;amp; SBOM
docs                 - Build docs (graphviz + handsdown + images + SBOM)
images               - Generate architecture &amp;amp; dependency diagrams
üîç LINTING &amp;amp; STATIC ANALYSIS
lint                 - Run the full linting suite (see targets below)
black                - Reformat code with black
autoflake            - Remove unused imports / variables with autoflake
isort                - Organise &amp;amp; sort imports with isort
flake8               - PEP-8 style &amp;amp; logical errors
pylint               - Pylint static analysis
markdownlint         - Lint Markdown files with markdownlint (requires markdownlint-cli)
mypy                 - Static type-checking with mypy
bandit               - Security scan with bandit
pydocstyle           - Docstring style checker
pycodestyle          - Simple PEP-8 checker
pre-commit           - Run all configured pre-commit hooks
ruff                 - Ruff linter + formatter
ty                   - Ty type checker from astral
pyright              - Static type-checking with Pyright
radon                - Code complexity &amp;amp; maintainability metrics
pyroma               - Validate packaging metadata
importchecker        - Detect orphaned imports
spellcheck           - Spell-check the codebase
fawltydeps           - Detect undeclared / unused deps
wily                 - Maintainability report
pyre                 - Static analysis with Facebook Pyre
depend               - List dependencies in ‚âàrequirements format
snakeviz             - Profile &amp;amp; visualise with snakeviz
pstats               - Generate PNG call-graph from cProfile stats
spellcheck-sort      - Sort local spellcheck dictionary
tox                  - Run tox across multi-Python versions
sbom                 - Produce a CycloneDX SBOM and vulnerability scan
pytype               - Flow-sensitive type checker
check-manifest       - Verify sdist/wheel completeness
yamllint            - Lint YAML files (uses .yamllint)
jsonlint            - Validate every *.json file with jq (--exit-status)
tomllint            - Validate *.toml files with tomlcheck
üï∏Ô∏è  WEBPAGE LINTERS &amp;amp; STATIC ANALYSIS (HTML/CSS/JS lint + security scans + formatting)
install-web-linters  - Install HTMLHint, Stylelint, ESLint, Retire.js &amp;amp; Prettier via npm
lint-web             - Run HTMLHint, Stylelint, ESLint, Retire.js and npm audit
format-web           - Format HTML, CSS &amp;amp; JS files with Prettier
osv-install          - Install/upgrade osv-scanner (Go)
osv-scan-source      - Scan source &amp;amp; lockfiles for CVEs
osv-scan-image       - Scan the built container image for CVEs
osv-scan             - Run all osv-scanner checks (source, image, licence)
üì° SONARQUBE ANALYSIS
sonar-deps-podman    - Install podman-compose + supporting tools
sonar-deps-docker    - Install docker-compose + supporting tools
sonar-up-podman      - Launch SonarQube with podman-compose
sonar-up-docker      - Launch SonarQube with docker-compose
sonar-submit-docker  - Run containerized Sonar Scanner CLI with Docker
sonar-submit-podman  - Run containerized Sonar Scanner CLI with Podman
pysonar-scanner      - Run scan with Python wrapper (pysonar-scanner)
sonar-info           - How to create a token &amp;amp; which env vars to export
üõ°Ô∏è SECURITY &amp;amp; PACKAGE SCANNING
trivy                - Scan container image for CVEs (HIGH/CRIT). Needs podman socket enabled
grype-scan           - Scan container for security audit and vulnerability scanning
dockle               - Lint the built container image via tarball (no daemon/socket needed)
hadolint             - Lint Containerfile/Dockerfile(s) with hadolint
pip-audit            - Audit Python dependencies for published CVEs
üì¶ DEPENDENCY MANAGEMENT
deps-update          - Run update-deps.py to update all dependencies in pyproject.toml and docs/requirements.txt
containerfile-update - Update base image in Containerfile to latest tag
üì¶ PACKAGING &amp;amp; PUBLISHING
dist                 - Clean-build wheel *and* sdist into ./dist
wheel                - Build wheel only
sdist                - Build source distribution only
verify               - Build + twine + check-manifest + pyroma (no upload)
publish              - Verify, then upload to PyPI (needs TWINE_* creds)
ü¶≠ PODMAN CONTAINER BUILD &amp;amp; RUN
podman-dev           - Build development container image
podman               - Build container image
podman-prod          - Build production container image (using ubi-micro ‚Üí scratch). Not supported on macOS.
podman-run           - Run the container on HTTP  (port 4444)
podman-run-shell     - Run the container on HTTP  (port 4444) and start a shell
podman-run-ssl       - Run the container on HTTPS (port 4444, self-signed)
podman-run-ssl-host  - Run the container on HTTPS with --network=host (port 4444, self-signed)
podman-stop          - Stop &amp;amp; remove the container
podman-test          - Quick curl smoke-test against the container
podman-logs          - Follow container logs (‚åÉC to quit)
podman-stats         - Show container resource stats (if supported)
podman-top           - Show live top-level process info in container
podman-shell         - Open an interactive shell inside the Podman container
üêã DOCKER BUILD &amp;amp; RUN
docker-dev           - Build development Docker image
docker               - Build production Docker image
docker-prod          - Build production container image (using ubi-micro ‚Üí scratch). Not supported on macOS.
docker-run           - Run the container on HTTP  (port 4444)
docker-run-ssl       - Run the container on HTTPS (port 4444, self-signed)
docker-stop          - Stop &amp;amp; remove the container
docker-test          - Quick curl smoke-test against the container
docker-logs          - Follow container logs (‚åÉC to quit)
docker-stats         - Show container resource usage stats (non-streaming)
docker-top           - Show top-level process info in Docker container
docker-shell         - Open an interactive shell inside the Docker container
üõ†Ô∏è COMPOSE STACK     - Build / start / stop the multi-service stack
compose-up           - Bring the whole stack up (detached)
compose-restart      - Recreate changed containers, pulling / building as needed
compose-build        - Build (or rebuild) images defined in the compose file
compose-pull         - Pull the latest images only
compose-logs         - Tail logs from all services (Ctrl-C to exit)
compose-ps           - Show container status table
compose-shell        - Open an interactive shell in the "gateway" container
compose-stop         - Gracefully stop the stack (keep containers)
compose-down         - Stop &amp;amp; remove containers (keep named volumes)
compose-rm           - Remove *stopped* containers
compose-clean        - ‚ú® Down **and** delete named volumes (data-loss ‚ö†)
‚òÅÔ∏è IBM CLOUD CODE ENGINE
ibmcloud-check-env          - Verify all required IBM Cloud env vars are set
ibmcloud-cli-install        - Auto-install IBM Cloud CLI + required plugins (OS auto-detected)
ibmcloud-login              - Login to IBM Cloud CLI using IBMCLOUD_API_KEY (--sso)
ibmcloud-ce-login           - Set Code Engine target project and region
ibmcloud-list-containers    - List deployed Code Engine apps
ibmcloud-tag                - Tag container image for IBM Container Registry
ibmcloud-push               - Push image to IBM Container Registry
ibmcloud-deploy             - Deploy (or update) container image in Code Engine
ibmcloud-ce-logs            - Stream logs for the deployed application
ibmcloud-ce-status          - Get deployment status
ibmcloud-ce-rm              - Delete the Code Engine application
üß™ MINIKUBE LOCAL CLUSTER
minikube-install      - Install Minikube (macOS, Linux, or Windows via choco)
helm-install          - Install Helm CLI (macOS, Linux, or Windows)
minikube-start        - Start local Minikube cluster with Ingress + DNS + metrics-server
minikube-stop         - Stop the Minikube cluster
minikube-delete       - Delete the Minikube cluster
minikube-image-load   - Build and load ghcr.io/ibm/mcp-context-forge:latest into Minikube
minikube-k8s-apply    - Apply Kubernetes manifests from deployment/k8s/
minikube-status       - Show status of Minikube and ingress pods
üõ†Ô∏è HELM CHART TASKS
helm-lint            - Lint the Helm chart (static analysis)
helm-package         - Package the chart into dist/ as mcp-stack-&amp;lt;ver&amp;gt;.tgz
helm-deploy          - Upgrade/Install chart into Minikube (profile mcpgw)
helm-delete          - Uninstall the chart release from Minikube
üè† LOCAL PYPI SERVER
local-pypi-install   - Install pypiserver for local testing
local-pypi-start     - Start local PyPI server on :8084 (no auth)
local-pypi-start-auth - Start local PyPI server with basic auth (admin/admin)
local-pypi-stop      - Stop local PyPI server
local-pypi-upload    - Upload existing package to local PyPI (no auth)
local-pypi-upload-auth - Upload existing package to local PyPI (with auth)
local-pypi-test      - Install package from local PyPI
local-pypi-clean     - Full cycle: build ‚Üí upload ‚Üí install locally
üè† LOCAL DEVPI SERVER
devpi-install        - Install devpi server and client
devpi-init           - Initialize devpi server (first time only)
devpi-start          - Start devpi server
devpi-stop           - Stop devpi server
devpi-setup-user     - Create user and dev index
devpi-upload         - Upload existing package to devpi
devpi-test           - Install package from devpi
devpi-clean          - Full cycle: build ‚Üí upload ‚Üí install locally
devpi-status         - Show devpi server status
devpi-web            - Open devpi web interface
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üîç Troubleshooting&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Port publishing on WSL2 (rootless Podman &amp;amp; Docker Desktop)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;Diagnose the listener&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Inside your WSL distro
ss -tlnp | grep 4444        # Use ss
netstat -anp | grep 4444    # or netstat
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;em&gt;Seeing &lt;code&gt;:::4444 LISTEN rootlessport&lt;/code&gt; is normal&lt;/em&gt; - the IPv6 wildcard socket (&lt;code&gt;::&lt;/code&gt;) also accepts IPv4 traffic &lt;strong&gt;when&lt;/strong&gt; &lt;code&gt;net.ipv6.bindv6only = 0&lt;/code&gt; (default on Linux).&lt;/p&gt; 
 &lt;h3&gt;Why localhost fails on Windows&lt;/h3&gt; 
 &lt;p&gt;WSL 2's NAT layer rewrites only the &lt;em&gt;IPv6&lt;/em&gt; side of the dual-stack listener. From Windows, &lt;code&gt;http://127.0.0.1:4444&lt;/code&gt; (or Docker Desktop's "localhost") therefore times-out.&lt;/p&gt; 
 &lt;h4&gt;Fix (Podman rootless)&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Inside the WSL distro
echo "wsl" | sudo tee /etc/containers/podman-machine
systemctl --user restart podman.socket
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;code&gt;ss&lt;/code&gt; should now show &lt;code&gt;0.0.0.0:4444&lt;/code&gt; instead of &lt;code&gt;:::4444&lt;/code&gt;, and the service becomes reachable from Windows &lt;em&gt;and&lt;/em&gt; the LAN.&lt;/p&gt; 
 &lt;h4&gt;Fix (Docker Desktop &amp;gt; 4.19)&lt;/h4&gt; 
 &lt;p&gt;Docker Desktop adds a "WSL integration" switch per-distro. Turn it &lt;strong&gt;on&lt;/strong&gt; for your distro, restart Docker Desktop, then restart the container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker restart mcpgateway
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Gateway starts but immediately exits ("Failed to read DATABASE_URL")&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Copy &lt;code&gt;.env.example&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; first:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Then edit &lt;code&gt;DATABASE_URL&lt;/code&gt;, &lt;code&gt;JWT_SECRET_KEY&lt;/code&gt;, &lt;code&gt;BASIC_AUTH_PASSWORD&lt;/code&gt;, etc. Missing or empty required vars cause a fast-fail at startup.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repo, create a feature branch.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make lint&lt;/code&gt; and fix any issues.&lt;/li&gt; 
 &lt;li&gt;Keep &lt;code&gt;make test&lt;/code&gt; green and 100% coverage.&lt;/li&gt; 
 &lt;li&gt;Open a PR - describe your changes clearly.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;See &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/h2&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;A complete changelog can be found here: &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under the &lt;strong&gt;Apache License 2.0&lt;/strong&gt; - see &lt;a href="https://raw.githubusercontent.com/IBM/mcp-context-forge/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core Authors and Maintainers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/in/crivetimihai"&gt;Mihai Criveti&lt;/a&gt; - Distinguished Engineer, Agentic AI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Special thanks to our contributors for helping us improve ContextForge MCP Gateway:&lt;/p&gt; 
&lt;a href="https://github.com/ibm/mcp-context-forge/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=ibm/mcp-context-forge&amp;amp;max=100&amp;amp;anon=0&amp;amp;columns=10" /&gt; &lt;/a&gt; 
&lt;h2&gt;Star History and Project Activity&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#ibm/mcp-context-forge&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=ibm/mcp-context-forge&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- === Usage Stats === --&gt; 
&lt;p&gt;&lt;a href="https://pepy.tech/project/mcp-contextforge-gateway"&gt;&lt;img src="https://static.pepy.tech/badge/mcp-contextforge-gateway/month" alt="PyPi Downloads" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/ibm/mcp-context-forge?style=social" alt="Stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/ibm/mcp-context-forge?style=social" alt="Forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/ibm/mcp-context-forge" alt="Contributors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/commits"&gt;&lt;img src="https://img.shields.io/github/last-commit/ibm/mcp-context-forge" alt="Last Commit" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/ibm/mcp-context-forge/issues"&gt;&lt;img src="https://img.shields.io/github/issues/ibm/mcp-context-forge" alt="Open Issues" /&gt;&lt;/a&gt;&amp;nbsp;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>