<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Thu, 28 Aug 2025 01:30:13 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>microsoft/terminal</title>
      <link>https://github.com/microsoft/terminal</link>
      <description>&lt;p&gt;The new Windows Terminal and the original Windows console host, all in the same place!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/microsoft/terminal/assets/91625426/333ddc76-8ab2-4eb4-a8c0-4d7b953b1179" alt="terminal-logos" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://dev.azure.com/shine-oss/terminal/_build/latest?definitionId=1&amp;amp;branchName=main"&gt;&lt;img src="https://dev.azure.com/shine-oss/terminal/_apis/build/status%2FTerminal%20CI?branchName=main" alt="Terminal Build Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Welcome to the Windows Terminal, Console and Command-Line repo&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#installing-and-running-windows-terminal"&gt;Installing and running Windows Terminal&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#microsoft-store-recommended"&gt;Microsoft Store [Recommended]&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#other-install-methods"&gt;Other install methods&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#via-github"&gt;Via GitHub&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#via-windows-package-manager-cli-aka-winget"&gt;Via Windows Package Manager CLI (aka winget)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#via-chocolatey-unofficial"&gt;Via Chocolatey (unofficial)&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#via-scoop-unofficial"&gt;Via Scoop (unofficial)&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#installing-windows-terminal-canary"&gt;Installing Windows Terminal Canary&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#windows-terminal-roadmap"&gt;Windows Terminal Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#terminal--console-overview"&gt;Terminal &amp;amp; Console Overview&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#windows-terminal"&gt;Windows Terminal&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#the-windows-console-host"&gt;The Windows Console Host&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#shared-components"&gt;Shared Components&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#creating-the-new-windows-terminal"&gt;Creating the new Windows Terminal&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#faq"&gt;FAQ&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#i-built-and-ran-the-new-terminal-but-it-looks-just-like-the-old-console"&gt;I built and ran the new Terminal, but it looks just like the old console&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#communicating-with-the-team"&gt;Communicating with the Team&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#developer-guidance"&gt;Developer Guidance&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#building-the-code"&gt;Building the Code&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#building-in-powershell"&gt;Building in PowerShell&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#building-in-cmd"&gt;Building in Cmd&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#running--debugging"&gt;Running &amp;amp; Debugging&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#coding-guidance"&gt;Coding Guidance&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/#code-of-conduct"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;p&gt;This repository contains the source code for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/terminal"&gt;Windows Terminal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/terminal-preview"&gt;Windows Terminal Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The Windows console host (&lt;code&gt;conhost.exe&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Components shared between the two projects&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/src/tools/ColorTool"&gt;ColorTool&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/samples"&gt;Sample projects&lt;/a&gt; that show how to consume the Windows Console APIs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Related repositories include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.microsoft.com/windows/terminal"&gt;Windows Terminal Documentation&lt;/a&gt; (&lt;a href="https://github.com/MicrosoftDocs/terminal"&gt;Repo: Contribute to the docs&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MicrosoftDocs/Console-Docs"&gt;Console API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Microsoft/Cascadia-Code"&gt;Cascadia Code Font&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installing and running Windows Terminal&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Windows Terminal requires Windows 10 2004 (build 19041) or later&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Microsoft Store [Recommended]&lt;/h3&gt; 
&lt;p&gt;Install the &lt;a href="https://aka.ms/terminal"&gt;Windows Terminal from the Microsoft Store&lt;/a&gt;. This allows you to always be on the latest version when we release new builds with automatic upgrades.&lt;/p&gt; 
&lt;p&gt;This is our preferred method.&lt;/p&gt; 
&lt;h3&gt;Other install methods&lt;/h3&gt; 
&lt;h4&gt;Via GitHub&lt;/h4&gt; 
&lt;p&gt;For users who are unable to install Windows Terminal from the Microsoft Store, released builds can be manually downloaded from this repository's &lt;a href="https://github.com/microsoft/terminal/releases"&gt;Releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download the &lt;code&gt;Microsoft.WindowsTerminal_&amp;lt;versionNumber&amp;gt;.msixbundle&lt;/code&gt; file from the &lt;strong&gt;Assets&lt;/strong&gt; section. To install the app, you can simply double-click on the &lt;code&gt;.msixbundle&lt;/code&gt; file, and the app installer should automatically run. If that fails for any reason, you can try the following command at a PowerShell prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;# NOTE: If you are using PowerShell 7+, please run
# Import-Module Appx -UseWindowsPowerShell
# before using Add-AppxPackage.

Add-AppxPackage Microsoft.WindowsTerminal_&amp;lt;versionNumber&amp;gt;.msixbundle
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you install Terminal manually:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You may need to install the &lt;a href="https://docs.microsoft.com/troubleshoot/cpp/c-runtime-packages-desktop-bridge#how-to-install-and-update-desktop-framework-packages"&gt;VC++ v14 Desktop Framework Package&lt;/a&gt;. This should only be necessary on older builds of Windows 10 and only if you get an error about missing framework packages.&lt;/li&gt; 
  &lt;li&gt;Terminal will not auto-update when new builds are released so you will need to regularly install the latest Terminal release to receive all the latest fixes and improvements!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Via Windows Package Manager CLI (aka winget)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/winget-cli"&gt;winget&lt;/a&gt; users can download and install the latest Terminal release by installing the &lt;code&gt;Microsoft.WindowsTerminal&lt;/code&gt; package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;winget install --id Microsoft.WindowsTerminal -e
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Dependency support is available in WinGet version &lt;a href="https://github.com/microsoft/winget-cli/releases"&gt;1.6.2631 or later&lt;/a&gt;. To install the Terminal stable release 1.18 or later, please make sure you have the updated version of the WinGet client.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Via Chocolatey (unofficial)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://chocolatey.org"&gt;Chocolatey&lt;/a&gt; users can download and install the latest Terminal release by installing the &lt;code&gt;microsoft-windows-terminal&lt;/code&gt; package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;choco install microsoft-windows-terminal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To upgrade Windows Terminal using Chocolatey, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;choco upgrade microsoft-windows-terminal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have any issues when installing/upgrading the package please go to the &lt;a href="https://chocolatey.org/packages/microsoft-windows-terminal"&gt;Windows Terminal package page&lt;/a&gt; and follow the &lt;a href="https://chocolatey.org/docs/package-triage-process"&gt;Chocolatey triage process&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Via Scoop (unofficial)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://scoop.sh"&gt;Scoop&lt;/a&gt; users can download and install the latest Terminal release by installing the &lt;code&gt;windows-terminal&lt;/code&gt; package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;scoop bucket add extras
scoop install windows-terminal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To update Windows Terminal using Scoop, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;scoop update windows-terminal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have any issues when installing/updating the package, please search for or report the same on the &lt;a href="https://github.com/lukesampson/scoop-extras/issues"&gt;issues page&lt;/a&gt; of Scoop Extras bucket repository.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installing Windows Terminal Canary&lt;/h2&gt; 
&lt;p&gt;Windows Terminal Canary is a nightly build of Windows Terminal. This build has the latest code from our &lt;code&gt;main&lt;/code&gt; branch, giving you an opportunity to try features before they make it to Windows Terminal Preview.&lt;/p&gt; 
&lt;p&gt;Windows Terminal Canary is our least stable offering, so you may discover bugs before we have had a chance to find them.&lt;/p&gt; 
&lt;p&gt;Windows Terminal Canary is available as an App Installer distribution and a Portable ZIP distribution.&lt;/p&gt; 
&lt;p&gt;The App Installer distribution supports automatic updates. Due to platform limitations, this installer only works on Windows 11.&lt;/p&gt; 
&lt;p&gt;The Portable ZIP distribution is a portable application. It will not automatically update and will not automatically check for updates. This portable ZIP distribution works on Windows 10 (19041+) and Windows 11.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Distribution&lt;/th&gt; 
   &lt;th align="center"&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;App Installer&lt;/td&gt; 
   &lt;td align="center"&gt;x64, arm64, x86&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/terminal-canary-installer"&gt;download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Portable ZIP&lt;/td&gt; 
   &lt;td align="center"&gt;x64&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/terminal-canary-zip-x64"&gt;download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Portable ZIP&lt;/td&gt; 
   &lt;td align="center"&gt;ARM64&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/terminal-canary-zip-arm64"&gt;download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Portable ZIP&lt;/td&gt; 
   &lt;td align="center"&gt;x86&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/terminal-canary-zip-x86"&gt;download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Learn more about the &lt;a href="https://learn.microsoft.com/windows/terminal/distributions"&gt;types of Windows Terminal distributions&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Windows Terminal Roadmap&lt;/h2&gt; 
&lt;p&gt;The plan for the Windows Terminal &lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/roadmap-2023.md"&gt;is described here&lt;/a&gt; and will be updated as the project proceeds.&lt;/p&gt; 
&lt;h2&gt;Terminal &amp;amp; Console Overview&lt;/h2&gt; 
&lt;p&gt;Please take a few minutes to review the overview below before diving into the code:&lt;/p&gt; 
&lt;h3&gt;Windows Terminal&lt;/h3&gt; 
&lt;p&gt;Windows Terminal is a new, modern, feature-rich, productive terminal application for command-line users. It includes many of the features most frequently requested by the Windows command-line community including support for tabs, rich text, globalization, configurability, theming &amp;amp; styling, and more.&lt;/p&gt; 
&lt;p&gt;The Terminal will also need to meet our goals and measures to ensure it remains fast and efficient, and doesn't consume vast amounts of memory or power.&lt;/p&gt; 
&lt;h3&gt;The Windows Console Host&lt;/h3&gt; 
&lt;p&gt;The Windows Console host, &lt;code&gt;conhost.exe&lt;/code&gt;, is Windows' original command-line user experience. It also hosts Windows' command-line infrastructure and the Windows Console API server, input engine, rendering engine, user preferences, etc. The console host code in this repository is the actual source from which the &lt;code&gt;conhost.exe&lt;/code&gt; in Windows itself is built.&lt;/p&gt; 
&lt;p&gt;Since taking ownership of the Windows command-line in 2014, the team added several new features to the Console, including background transparency, line-based selection, support for &lt;a href="https://en.wikipedia.org/wiki/ANSI_escape_code"&gt;ANSI / Virtual Terminal sequences&lt;/a&gt;, &lt;a href="https://devblogs.microsoft.com/commandline/24-bit-color-in-the-windows-console/"&gt;24-bit color&lt;/a&gt;, a &lt;a href="https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/"&gt;Pseudoconsole ("ConPTY")&lt;/a&gt;, and more.&lt;/p&gt; 
&lt;p&gt;However, because Windows Console's primary goal is to maintain backward compatibility, we have been unable to add many of the features the community (and the team) have been wanting for the last several years including tabs, unicode text, and emoji.&lt;/p&gt; 
&lt;p&gt;These limitations led us to create the new Windows Terminal.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can read more about the evolution of the command-line in general, and the Windows command-line specifically in &lt;a href="https://devblogs.microsoft.com/commandline/windows-command-line-backgrounder/"&gt;this accompanying series of blog posts&lt;/a&gt; on the Command-Line team's blog.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Shared Components&lt;/h3&gt; 
&lt;p&gt;While overhauling Windows Console, we modernized its codebase considerably, cleanly separating logical entities into modules and classes, introduced some key extensibility points, replaced several old, home-grown collections and containers with safer, more efficient &lt;a href="https://docs.microsoft.com/en-us/cpp/standard-library/stl-containers?view=vs-2022"&gt;STL containers&lt;/a&gt;, and made the code simpler and safer by using Microsoft's &lt;a href="https://github.com/Microsoft/wil"&gt;Windows Implementation Libraries - WIL&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This overhaul resulted in several of Console's key components being available for re-use in any terminal implementation on Windows. These components include a new DirectWrite-based text layout and rendering engine, a text buffer capable of storing both UTF-16 and UTF-8, a VT parser/emitter, and more.&lt;/p&gt; 
&lt;h3&gt;Creating the new Windows Terminal&lt;/h3&gt; 
&lt;p&gt;When we started planning the new Windows Terminal application, we explored and evaluated several approaches and technology stacks. We ultimately decided that our goals would be best met by continuing our investment in our C++ codebase, which would allow us to reuse several of the aforementioned modernized components in both the existing Console and the new Terminal. Further, we realized that this would allow us to build much of the Terminal's core itself as a reusable UI control that others can incorporate into their own applications.&lt;/p&gt; 
&lt;p&gt;The result of this work is contained within this repo and delivered as the Windows Terminal application you can download from the Microsoft Store, or &lt;a href="https://github.com/microsoft/terminal/releases"&gt;directly from this repo's releases&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;For more information about Windows Terminal, you may find some of these resources useful and interesting:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://devblogs.microsoft.com/commandline"&gt;Command-Line Blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://devblogs.microsoft.com/commandline/windows-command-line-backgrounder/"&gt;Command-Line Backgrounder Blog Series&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Windows Terminal Launch: &lt;a href="https://www.youtube.com/watch?v=8gw0rXPMMPE&amp;amp;list=PLEHMQNlPj-Jzh9DkNpqipDGCZZuOwrQwR&amp;amp;index=2&amp;amp;t=0s"&gt;Terminal "Sizzle Video"&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Windows Terminal Launch: &lt;a href="https://www.youtube.com/watch?v=KMudkRcwjCw"&gt;Build 2019 Session&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Run As Radio: &lt;a href="https://www.runasradio.com/Shows/Show/645"&gt;Show 645 - Windows Terminal with Richard Turner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Azure Devops Podcast: &lt;a href="http://azuredevopspodcast.clear-measure.com/kayla-cinnamon-and-rich-turner-on-devops-on-the-windows-terminal-team-episode-54"&gt;Episode 54 - Kayla Cinnamon and Rich Turner on DevOps on the Windows Terminal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Microsoft Ignite 2019 Session: &lt;a href="https://myignite.techcommunity.microsoft.com/sessions/81329?source=sessions"&gt;The Modern Windows Command Line: Windows Terminal - BRK3321&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;I built and ran the new Terminal, but it looks just like the old console&lt;/h3&gt; 
&lt;p&gt;Cause: You're launching the incorrect solution in Visual Studio.&lt;/p&gt; 
&lt;p&gt;Solution: Make sure you're building &amp;amp; deploying the &lt;code&gt;CascadiaPackage&lt;/code&gt; project in Visual Studio.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;code&gt;OpenConsole.exe&lt;/code&gt; is just a locally-built &lt;code&gt;conhost.exe&lt;/code&gt;, the classic Windows Console that hosts Windows' command-line infrastructure. OpenConsole is used by Windows Terminal to connect to and communicate with command-line applications (via &lt;a href="https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/"&gt;ConPty&lt;/a&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;All project documentation is located at &lt;a href="https://aka.ms/terminal-docs"&gt;aka.ms/terminal-docs&lt;/a&gt;. If you would like to contribute to the documentation, please submit a pull request on the &lt;a href="https://github.com/MicrosoftDocs/terminal"&gt;Windows Terminal Documentation repo&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are excited to work alongside you, our amazing community, to build and enhance Windows Terminal!&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;BEFORE you start work on a feature/fix&lt;/strong&gt;&lt;/em&gt;, please read &amp;amp; follow our &lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt; to help avoid any wasted or duplicate effort.&lt;/p&gt; 
&lt;h2&gt;Communicating with the Team&lt;/h2&gt; 
&lt;p&gt;The easiest way to communicate with the team is via GitHub issues.&lt;/p&gt; 
&lt;p&gt;Please file new issues, feature requests and suggestions, but &lt;strong&gt;DO search for similar open/closed preexisting issues before creating a new issue.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to ask a question that you feel doesn't warrant an issue (yet), please reach out to us via Twitter:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Christopher Nguyen, Product Manager: &lt;a href="https://twitter.com/nguyen_dows"&gt;@nguyen_dows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Dustin Howett, Engineering Lead: &lt;a href="https://twitter.com/DHowett"&gt;@dhowett&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mike Griese, Senior Developer: &lt;a href="https://mastodon.social/@zadjii"&gt;@zadjii@mastodon.social&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Carlos Zamora, Developer: &lt;a href="https://twitter.com/cazamor_msft"&gt;@cazamor_msft&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Pankaj Bhojwani, Developer&lt;/li&gt; 
 &lt;li&gt;Leonard Hecker, Developer: &lt;a href="https://twitter.com/LeonardHecker"&gt;@LeonardHecker&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Developer Guidance&lt;/h2&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;You can configure your environment to build Terminal in one of two ways:&lt;/p&gt; 
&lt;h3&gt;Using WinGet configuration file&lt;/h3&gt; 
&lt;p&gt;After cloning the repository, you can use a &lt;a href="https://learn.microsoft.com/en-us/windows/package-manager/configuration/#use-a-winget-configuration-file-to-configure-your-machine"&gt;WinGet configuration file&lt;/a&gt; to set up your environment. The &lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/.config/configuration.winget"&gt;default configuration file&lt;/a&gt; installs Visual Studio 2022 Community &amp;amp; rest of the required tools. There are two other variants of the configuration file available in the &lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/.config"&gt;.config&lt;/a&gt; directory for Enterprise &amp;amp; Professional editions of Visual Studio 2022. To run the default configuration file, you can either double-click the file from explorer or run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;winget configure .config\configuration.winget
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;You must be running Windows 10 2004 (build &amp;gt;= 10.0.19041.0) or later to run Windows Terminal&lt;/li&gt; 
 &lt;li&gt;You must &lt;a href="https://docs.microsoft.com/en-us/windows/uwp/get-started/enable-your-device-for-development"&gt;enable Developer Mode in the Windows Settings app&lt;/a&gt; to locally install and run Windows Terminal&lt;/li&gt; 
 &lt;li&gt;You must have &lt;a href="https://github.com/PowerShell/PowerShell/releases/latest"&gt;PowerShell 7 or later&lt;/a&gt; installed&lt;/li&gt; 
 &lt;li&gt;You must have the &lt;a href="https://developer.microsoft.com/en-us/windows/downloads/windows-sdk/"&gt;Windows 11 (10.0.22621.0) SDK&lt;/a&gt; installed&lt;/li&gt; 
 &lt;li&gt;You must have at least &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;VS 2022&lt;/a&gt; installed&lt;/li&gt; 
 &lt;li&gt;You must install the following Workloads via the VS Installer. Note: Opening the solution in VS 2022 will &lt;a href="https://devblogs.microsoft.com/setup/configure-visual-studio-across-your-organization-with-vsconfig/"&gt;prompt you to install missing components automatically&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Desktop Development with C++&lt;/li&gt; 
   &lt;li&gt;Universal Windows Platform Development&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The following Individual Components&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;C++ (v143) Universal Windows Platform Tools&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;You must install the &lt;a href="https://docs.microsoft.com/dotnet/framework/install/guide-for-developers#to-install-the-net-framework-developer-pack-or-targeting-pack"&gt;.NET Framework Targeting Pack&lt;/a&gt; to build test projects&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building the Code&lt;/h2&gt; 
&lt;p&gt;OpenConsole.sln may be built from within Visual Studio or from the command-line using a set of convenience scripts &amp;amp; tools in the &lt;strong&gt;/tools&lt;/strong&gt; directory:&lt;/p&gt; 
&lt;h3&gt;Building in PowerShell&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;Import-Module .\tools\OpenConsole.psm1
Set-MsBuildDevEnvironment
Invoke-OpenConsoleBuild
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Building in Cmd&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;.\tools\razzle.cmd
bcz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Running &amp;amp; Debugging&lt;/h2&gt; 
&lt;p&gt;To debug the Windows Terminal in VS, right click on &lt;code&gt;CascadiaPackage&lt;/code&gt; (in the Solution Explorer) and go to properties. In the Debug menu, change "Application process" and "Background task process" to "Native Only".&lt;/p&gt; 
&lt;p&gt;You should then be able to build &amp;amp; debug the Terminal project by hitting &lt;kbd&gt;F5&lt;/kbd&gt;. Make sure to select either the "x64" or the "x86" platform - the Terminal doesn't build for "Any Cpu" (because the Terminal is a C++ application, not a C# one).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;👉 You will &lt;em&gt;not&lt;/em&gt; be able to launch the Terminal directly by running the WindowsTerminal.exe. For more details on why, see &lt;a href="https://github.com/microsoft/terminal/issues/926"&gt;#926&lt;/a&gt;, &lt;a href="https://github.com/microsoft/terminal/issues/4043"&gt;#4043&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Coding Guidance&lt;/h3&gt; 
&lt;p&gt;Please review these brief docs below about our coding practices.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;👉 If you find something missing from these docs, feel free to contribute to any of our documentation files anywhere in the repository (or write some new ones!)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;This is a work in progress as we learn what we'll need to provide people in order to be effective contributors to our project.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/STYLE.md"&gt;Coding Style&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/ORGANIZATION.md"&gt;Code Organization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/EXCEPTIONS.md"&gt;Exceptions in our legacy codebase&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/terminal/main/doc/WIL.md"&gt;Helpful smart pointers and macros for interfacing with Windows in WIL&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QuentinFuxa/WhisperLiveKit</title>
      <link>https://github.com/QuentinFuxa/WhisperLiveKit</link>
      <description>&lt;p&gt;Python package for Real-time, Local Speech-to-Text and Speaker Diarization. FastAPI Server &amp; Web Interface&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;WhisperLiveKit&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png" alt="WhisperLiveKit Demo" width="730" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&lt;b&gt;Real-time, Fully Local Speech-to-Text with Speaker Identification&lt;/b&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="PyPI Version" src="https://img.shields.io/pypi/v/whisperlivekit?color=g" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/whisperlivekit"&gt;&lt;img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/whisperlivekit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=installations" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="Python Versions" src="https://img.shields.io/badge/python-3.9--3.13-dark_green" /&gt;&lt;/a&gt; &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/badge/License-MIT/Dual Licensed-dark_green" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Real-time speech transcription directly to your browser, with a ready-to-use backend+server and a simple frontend. ✨&lt;/p&gt; 
&lt;h4&gt;Powered by Leading Research:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/SimulStreaming"&gt;SimulStreaming&lt;/a&gt; (SOTA 2025) - Ultra-low latency transcription with AlignAtt policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;WhisperStreaming&lt;/a&gt; (SOTA 2023) - Low latency transcription with LocalAgreement policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2507.18446"&gt;Streaming Sortformer&lt;/a&gt; (SOTA 2025) - Advanced real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/juanmc2005/diart"&gt;Diart&lt;/a&gt; (SOTA 2021) - Real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; (2024) - Enterprise-grade Voice Activity Detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Why not just run a simple Whisper model on every audio batch?&lt;/strong&gt; Whisper is designed for complete utterances, not real-time chunks. Processing small segments loses context, cuts off words mid-syllable, and produces poor transcription. WhisperLiveKit uses state-of-the-art simultaneous speech research for intelligent buffering and incremental processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Architecture&lt;/h3&gt; 
&lt;img alt="Architecture" src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png" /&gt; 
&lt;p&gt;&lt;em&gt;The backend supports multiple concurrent users. Voice Activity Detection reduces overhead when no voice is detected.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Installation &amp;amp; Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install whisperlivekit
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;FFmpeg is required&lt;/strong&gt; and must be installed before using WhisperLiveKit&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;OS&lt;/th&gt; 
    &lt;th&gt;How to install&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Ubuntu/Debian&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;sudo apt install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MacOS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;brew install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Windows&lt;/td&gt; 
    &lt;td&gt;Download .exe from &lt;a href="https://ffmpeg.org/download.html"&gt;https://ffmpeg.org/download.html&lt;/a&gt; and add to PATH&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the transcription server:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;whisperlivekit-server --model base --language en
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open your browser&lt;/strong&gt; and navigate to &lt;code&gt;http://localhost:8000&lt;/code&gt;. Start speaking and watch your words appear in real-time!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;See &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/li&gt; 
  &lt;li&gt;For HTTPS requirements, see the &lt;strong&gt;Parameters&lt;/strong&gt; section for SSL configuration options.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;pip install&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speaker diarization with Sortformer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speaker diarization with Diart&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Original Whisper backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Improved timestamps backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper-timestamped&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Apple Silicon optimization backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mlx-whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI API backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;strong&gt;Parameters &amp;amp; Configuration&lt;/strong&gt; below on how to use them.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Command-line Interface&lt;/strong&gt;: Start the transcription server with various options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use better model than default (small)
whisperlivekit-server --model large-v3

# Advanced configuration with diarization and language
whisperlivekit-server --host 0.0.0.0 --port 8000 --model medium --diarization --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Python API Integration&lt;/strong&gt;: Check &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/basic_server.py"&gt;basic_server&lt;/a&gt; for a more complete example of how to use the functions and classes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from whisperlivekit import TranscriptionEngine, AudioProcessor, parse_args
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
import asyncio

transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    transcription_engine = TranscriptionEngine(model="medium", diarization=True, lan="en")
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket: WebSocket, results_generator):
    async for response in results_generator:
        await websocket.send_json(response)
    await websocket.send_json({"type": "ready_to_stop"})

@app.websocket("/asr")
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # Create a new AudioProcessor for each connection, passing the shared engine
    audio_processor = AudioProcessor(transcription_engine=transcription_engine)    
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(handle_websocket_results(websocket, results_generator))
    await websocket.accept()
    while True:
        message = await websocket.receive_bytes()
        await audio_processor.process_audio(message)        
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Frontend Implementation&lt;/strong&gt;: The package includes an HTML/JavaScript implementation &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/web/live_transcription.html"&gt;here&lt;/a&gt;. You can also import it using &lt;code&gt;from whisperlivekit import get_web_interface_html&lt;/code&gt; &amp;amp; &lt;code&gt;page = get_web_interface_html()&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Parameters &amp;amp; Configuration&lt;/h2&gt; 
&lt;p&gt;An important list of parameters can be changed. But what &lt;em&gt;should&lt;/em&gt; you change?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;--model&lt;/code&gt; size. List and recommandations &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/available_models.md"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--language&lt;/code&gt;. List &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;here&lt;/a&gt;. If you use &lt;code&gt;auto&lt;/code&gt;, the model attempts to detect the language automatically, but it tends to bias towards English.&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--backend&lt;/code&gt; ? you can switch to &lt;code&gt;--backend faster-whisper&lt;/code&gt; if &lt;code&gt;simulstreaming&lt;/code&gt; does not work correctly or if you prefer to avoid the dual-license requirements.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--warmup-file&lt;/code&gt;, if you have one&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;, &lt;code&gt;--port&lt;/code&gt;, &lt;code&gt;--ssl-certfile&lt;/code&gt;, &lt;code&gt;--ssl-keyfile&lt;/code&gt;, if you set up a server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--diarization&lt;/code&gt;, if you want to use it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The rest I don't recommend. But below are your options.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Parameter&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whisper model size.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source language code or &lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--task&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt; or &lt;code&gt;translate&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Processing backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;simulstreaming&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--min-chunk-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio chunk size (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vac&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Controller&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vad&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--warmup-file&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio file path for model warmup&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jfk.wav&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server host address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-certfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL certificate file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-keyfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL private key file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;WhisperStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--confidence-validation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use confidence scores for faster validation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--buffer_trimming&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Buffer trimming strategy (&lt;code&gt;sentence&lt;/code&gt; or &lt;code&gt;segment&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;segment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SimulStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--frame-threshold&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AlignAtt frame threshold (lower = faster, higher = more accurate)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--beams&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Number of beams for beam search (1 = greedy decoding)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--decoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Force decoder type (&lt;code&gt;beam&lt;/code&gt; or &lt;code&gt;greedy&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-max-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum audio buffer length (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;30.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-min-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio length to process (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--cif-ckpt-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to CIF model for word boundary detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--never-fire&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Never truncate incomplete words&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for the model&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--static-init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Static prompt that doesn't scroll&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--max-context-tokens&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum context tokens&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Direct path to .pt model file. Download it if not found&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;./base.pt&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--preloaded-model-count&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Optional. Number of models to preload in memory to speed up loading (set up to the expected number of concurrent users)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Diarization options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable speaker identification&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt; or &lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--segmentation-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart segmentation model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--embedding-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart embedding model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;speechbrain/spkrec-ecapa-voxceleb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For diarization using Diart, you need access to pyannote.audio models:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation-3.0"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/embedding"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/embedding&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;Login with HuggingFace: &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🚀 Deployment Guide&lt;/h3&gt; 
&lt;p&gt;To deploy WhisperLiveKit in production:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Server Setup&lt;/strong&gt;: Install production ASGI server &amp;amp; launch with multiple workers&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install uvicorn gunicorn
gunicorn -k uvicorn.workers.UvicornWorker -w 4 your_app:app
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Host your customized version of the &lt;code&gt;html&lt;/code&gt; example &amp;amp; ensure WebSocket connection points correctly&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nginx Configuration&lt;/strong&gt; (recommended for production):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-nginx"&gt;server {
   listen 80;
   server_name your-domain.com;
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
}}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTPS Support&lt;/strong&gt;: For secure deployments, use "wss://" instead of "ws://" in WebSocket URL&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🐋 Docker&lt;/h2&gt; 
&lt;p&gt;Deploy the application easily using Docker with GPU or CPU support.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker installed on your system&lt;/li&gt; 
 &lt;li&gt;For GPU support: NVIDIA Docker runtime installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;With GPU acceleration (recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t wlk .
docker run --gpus all -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CPU only:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f Dockerfile.cpu -t wlk .
docker run -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Custom configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with custom model and language
docker run --gpus all -p 8000:8000 --name wlk wlk --model large-v3 --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Memory Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Large models&lt;/strong&gt;: Ensure your Docker runtime has sufficient memory allocated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--build-arg&lt;/code&gt; Options: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;EXTRAS="whisper-timestamped"&lt;/code&gt; - Add extras to the image's installation (no spaces). Remember to set necessary container options!&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_PRECACHE_DIR="./.cache/"&lt;/code&gt; - Pre-load a model cache for faster first-time start&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_TKN_FILE="./token"&lt;/code&gt; - Add your Hugging Face Hub access token to download gated models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔮 Use Cases&lt;/h2&gt; 
&lt;p&gt;Capture discussions in real-time for meeting transcription, help hearing-impaired users follow conversations through accessibility tools, transcribe podcasts or videos automatically for content creation, transcribe support calls with speaker identification for customer service...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>twbs/bootstrap</title>
      <link>https://github.com/twbs/bootstrap</link>
      <description>&lt;p&gt;The most popular HTML, CSS, and JavaScript framework for developing responsive, mobile first projects on the web.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://getbootstrap.com/"&gt; &lt;img src="https://getbootstrap.com/docs/5.3/assets/brand/bootstrap-logo-shadow.png" alt="Bootstrap logo" width="200" height="165" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt;Bootstrap&lt;/h3&gt; 
&lt;p align="center"&gt; Sleek, intuitive, and powerful front-end framework for faster and easier web development. &lt;br /&gt; &lt;a href="https://getbootstrap.com/docs/5.3/"&gt;&lt;strong&gt;Explore Bootstrap docs »&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://github.com/twbs/bootstrap/issues/new?assignees=-&amp;amp;labels=bug&amp;amp;template=bug_report.yml"&gt;Report bug&lt;/a&gt; · &lt;a href="https://github.com/twbs/bootstrap/issues/new?assignees=&amp;amp;labels=feature&amp;amp;template=feature_request.yml"&gt;Request feature&lt;/a&gt; · &lt;a href="https://blog.getbootstrap.com/"&gt;Blog&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Bootstrap 5&lt;/h2&gt; 
&lt;p&gt;Our default branch is for development of our Bootstrap 5 release. Head to the &lt;a href="https://github.com/twbs/bootstrap/tree/v4-dev"&gt;&lt;code&gt;v4-dev&lt;/code&gt; branch&lt;/a&gt; to view the readme, documentation, and source code for Bootstrap 4.&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#quick-start"&gt;Quick start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#status"&gt;Status&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#whats-included"&gt;What’s included&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#bugs-and-feature-requests"&gt;Bugs and feature requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#community"&gt;Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#versioning"&gt;Versioning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#creators"&gt;Creators&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#thanks"&gt;Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#copyright-and-license"&gt;Copyright and license&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Several quick start options are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/twbs/bootstrap/archive/v5.3.8.zip"&gt;Download the latest release&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Clone the repo: &lt;code&gt;git clone https://github.com/twbs/bootstrap.git&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://www.npmjs.com/"&gt;npm&lt;/a&gt;: &lt;code&gt;npm install bootstrap@v5.3.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://yarnpkg.com/"&gt;yarn&lt;/a&gt;: &lt;code&gt;yarn add bootstrap@v5.3.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://bun.sh/"&gt;Bun&lt;/a&gt;: &lt;code&gt;bun add bootstrap@v5.3.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://getcomposer.org/"&gt;Composer&lt;/a&gt;: &lt;code&gt;composer require twbs/bootstrap:5.3.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install with &lt;a href="https://www.nuget.org/"&gt;NuGet&lt;/a&gt;: CSS: &lt;code&gt;Install-Package bootstrap&lt;/code&gt; Sass: &lt;code&gt;Install-Package bootstrap.sass&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read the &lt;a href="https://getbootstrap.com/docs/5.3/getting-started/introduction/"&gt;Getting started page&lt;/a&gt; for information on the framework contents, templates, examples, and more.&lt;/p&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/twbs/bootstrap/actions/workflows/js.yml?query=workflow%3AJS+branch%3Amain"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/twbs/bootstrap/js.yml?branch=main&amp;amp;label=JS%20Tests&amp;amp;logo=github" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/bootstrap"&gt;&lt;img src="https://img.shields.io/npm/v/bootstrap?logo=npm&amp;amp;logoColor=fff" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://rubygems.org/gems/bootstrap"&gt;&lt;img src="https://img.shields.io/gem/v/bootstrap?logo=rubygems&amp;amp;logoColor=fff" alt="Gem version" /&gt;&lt;/a&gt; &lt;a href="https://atmospherejs.com/twbs/bootstrap"&gt;&lt;img src="https://img.shields.io/badge/meteor-twbs%3Abootstrap-blue?logo=meteor&amp;amp;logoColor=fff" alt="Meteor Atmosphere" /&gt;&lt;/a&gt; &lt;a href="https://packagist.org/packages/twbs/bootstrap"&gt;&lt;img src="https://img.shields.io/packagist/vpre/twbs/bootstrap?logo=packagist&amp;amp;logoColor=fff" alt="Packagist Prerelease" /&gt;&lt;/a&gt; &lt;a href="https://www.nuget.org/packages/bootstrap/absoluteLatest"&gt;&lt;img src="https://img.shields.io/nuget/vpre/bootstrap?logo=nuget&amp;amp;logoColor=fff" alt="NuGet" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/twbs/bootstrap?branch=main"&gt;&lt;img src="https://img.shields.io/coveralls/github/twbs/bootstrap/main?logo=coveralls&amp;amp;logoColor=fff" alt="Coverage Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/twbs/bootstrap/raw/main/dist/css/bootstrap.min.css"&gt;&lt;img src="https://img.badgesize.io/twbs/bootstrap/main/dist/css/bootstrap.min.css?compression=gzip&amp;amp;label=CSS%20gzip%20size" alt="CSS gzip size" /&gt;&lt;/a&gt; &lt;a href="https://github.com/twbs/bootstrap/raw/main/dist/css/bootstrap.min.css"&gt;&lt;img src="https://img.badgesize.io/twbs/bootstrap/main/dist/css/bootstrap.min.css?compression=brotli&amp;amp;label=CSS%20Brotli%20size" alt="CSS Brotli size" /&gt;&lt;/a&gt; &lt;a href="https://github.com/twbs/bootstrap/raw/main/dist/js/bootstrap.min.js"&gt;&lt;img src="https://img.badgesize.io/twbs/bootstrap/main/dist/js/bootstrap.min.js?compression=gzip&amp;amp;label=JS%20gzip%20size" alt="JS gzip size" /&gt;&lt;/a&gt; &lt;a href="https://github.com/twbs/bootstrap/raw/main/dist/js/bootstrap.min.js"&gt;&lt;img src="https://img.badgesize.io/twbs/bootstrap/main/dist/js/bootstrap.min.js?compression=brotli&amp;amp;label=JS%20Brotli%20size" alt="JS Brotli size" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/ossf-scorecard/github.com/twbs/bootstrap" alt="Open Source Security Foundation Scorecard" /&gt; &lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#backers"&gt;&lt;img src="https://img.shields.io/opencollective/backers/bootstrap?logo=opencollective&amp;amp;logoColor=fff" alt="Backers on Open Collective" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/twbs/bootstrap/main/#sponsors"&gt;&lt;img src="https://img.shields.io/opencollective/sponsors/bootstrap?logo=opencollective&amp;amp;logoColor=fff" alt="Sponsors on Open Collective" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What’s included&lt;/h2&gt; 
&lt;p&gt;Within the download you’ll find the following directories and files, logically grouping common assets and providing both compiled and minified variations.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Download contents&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;bootstrap/
├── css/
│   ├── bootstrap-grid.css
│   ├── bootstrap-grid.css.map
│   ├── bootstrap-grid.min.css
│   ├── bootstrap-grid.min.css.map
│   ├── bootstrap-grid.rtl.css
│   ├── bootstrap-grid.rtl.css.map
│   ├── bootstrap-grid.rtl.min.css
│   ├── bootstrap-grid.rtl.min.css.map
│   ├── bootstrap-reboot.css
│   ├── bootstrap-reboot.css.map
│   ├── bootstrap-reboot.min.css
│   ├── bootstrap-reboot.min.css.map
│   ├── bootstrap-reboot.rtl.css
│   ├── bootstrap-reboot.rtl.css.map
│   ├── bootstrap-reboot.rtl.min.css
│   ├── bootstrap-reboot.rtl.min.css.map
│   ├── bootstrap-utilities.css
│   ├── bootstrap-utilities.css.map
│   ├── bootstrap-utilities.min.css
│   ├── bootstrap-utilities.min.css.map
│   ├── bootstrap-utilities.rtl.css
│   ├── bootstrap-utilities.rtl.css.map
│   ├── bootstrap-utilities.rtl.min.css
│   ├── bootstrap-utilities.rtl.min.css.map
│   ├── bootstrap.css
│   ├── bootstrap.css.map
│   ├── bootstrap.min.css
│   ├── bootstrap.min.css.map
│   ├── bootstrap.rtl.css
│   ├── bootstrap.rtl.css.map
│   ├── bootstrap.rtl.min.css
│   └── bootstrap.rtl.min.css.map
└── js/
    ├── bootstrap.bundle.js
    ├── bootstrap.bundle.js.map
    ├── bootstrap.bundle.min.js
    ├── bootstrap.bundle.min.js.map
    ├── bootstrap.esm.js
    ├── bootstrap.esm.js.map
    ├── bootstrap.esm.min.js
    ├── bootstrap.esm.min.js.map
    ├── bootstrap.js
    ├── bootstrap.js.map
    ├── bootstrap.min.js
    └── bootstrap.min.js.map
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;We provide compiled CSS and JS (&lt;code&gt;bootstrap.*&lt;/code&gt;), as well as compiled and minified CSS and JS (&lt;code&gt;bootstrap.min.*&lt;/code&gt;). &lt;a href="https://web.dev/articles/source-maps"&gt;Source maps&lt;/a&gt; (&lt;code&gt;bootstrap.*.map&lt;/code&gt;) are available for use with certain browsers’ developer tools. Bundled JS files (&lt;code&gt;bootstrap.bundle.js&lt;/code&gt; and minified &lt;code&gt;bootstrap.bundle.min.js&lt;/code&gt;) include &lt;a href="https://popper.js.org/docs/v2/"&gt;Popper&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Bugs and feature requests&lt;/h2&gt; 
&lt;p&gt;Have a bug or a feature request? Please first read the &lt;a href="https://github.com/twbs/bootstrap/raw/main/.github/CONTRIBUTING.md#using-the-issue-tracker"&gt;issue guidelines&lt;/a&gt; and search for existing and closed issues. If your problem or idea is not addressed yet, &lt;a href="https://github.com/twbs/bootstrap/issues/new/choose"&gt;please open a new issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Bootstrap’s documentation, included in this repo in the root directory, is built with &lt;a href="https://astro.build/"&gt;Astro&lt;/a&gt; and publicly hosted on GitHub Pages at &lt;a href="https://getbootstrap.com/"&gt;https://getbootstrap.com/&lt;/a&gt;. The docs may also be run locally.&lt;/p&gt; 
&lt;p&gt;Documentation search is powered by &lt;a href="https://docsearch.algolia.com/"&gt;Algolia's DocSearch&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running documentation locally&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run &lt;code&gt;npm install&lt;/code&gt; to install the Node.js dependencies, including Astro (the site builder).&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;npm run test&lt;/code&gt; (or a specific npm script) to rebuild distributed CSS and JavaScript files, as well as our docs assets.&lt;/li&gt; 
 &lt;li&gt;From the root &lt;code&gt;/bootstrap&lt;/code&gt; directory, run &lt;code&gt;npm run docs-serve&lt;/code&gt; in the command line.&lt;/li&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:9001"&gt;http://localhost:9001&lt;/a&gt; in your browser, and voilà.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Learn more about using Astro by reading its &lt;a href="https://docs.astro.build/en/getting-started/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Documentation for previous releases&lt;/h3&gt; 
&lt;p&gt;You can find all our previous releases docs on &lt;a href="https://getbootstrap.com/docs/versions/"&gt;https://getbootstrap.com/docs/versions/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/twbs/bootstrap/releases"&gt;Previous releases&lt;/a&gt; and their documentation are also available for download.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please read through our &lt;a href="https://github.com/twbs/bootstrap/raw/main/.github/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;. Included are directions for opening issues, coding standards, and notes on development.&lt;/p&gt; 
&lt;p&gt;Moreover, if your pull request contains JavaScript patches or features, you must include &lt;a href="https://github.com/twbs/bootstrap/tree/main/js/tests"&gt;relevant unit tests&lt;/a&gt;. All HTML and CSS should conform to the &lt;a href="https://github.com/mdo/code-guide"&gt;Code Guide&lt;/a&gt;, maintained by &lt;a href="https://github.com/mdo"&gt;Mark Otto&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Editor preferences are available in the &lt;a href="https://github.com/twbs/bootstrap/raw/main/.editorconfig"&gt;editor config&lt;/a&gt; for easy use in common text editors. Read more and download plugins at &lt;a href="https://editorconfig.org/"&gt;https://editorconfig.org/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Get updates on Bootstrap’s development and chat with the project maintainers and community members.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow &lt;a href="https://x.com/getbootstrap"&gt;@getbootstrap on X&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Read and subscribe to &lt;a href="https://blog.getbootstrap.com/"&gt;The Official Bootstrap Blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Ask questions and explore &lt;a href="https://github.com/twbs/bootstrap/discussions"&gt;our GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Discuss, ask questions, and more on &lt;a href="https://discord.gg/bZUvakRU3M"&gt;the community Discord&lt;/a&gt; or &lt;a href="https://www.reddit.com/r/bootstrap/"&gt;Bootstrap subreddit&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Chat with fellow Bootstrappers in IRC. On the &lt;code&gt;irc.libera.chat&lt;/code&gt; server, in the &lt;code&gt;#bootstrap&lt;/code&gt; channel.&lt;/li&gt; 
 &lt;li&gt;Implementation help may be found at Stack Overflow (tagged &lt;a href="https://stackoverflow.com/questions/tagged/bootstrap-5"&gt;&lt;code&gt;bootstrap-5&lt;/code&gt;&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Developers should use the keyword &lt;code&gt;bootstrap&lt;/code&gt; on packages which modify or add to the functionality of Bootstrap when distributing through &lt;a href="https://www.npmjs.com/browse/keyword/bootstrap"&gt;npm&lt;/a&gt; or similar delivery mechanisms for maximum discoverability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Versioning&lt;/h2&gt; 
&lt;p&gt;For transparency into our release cycle and in striving to maintain backward compatibility, Bootstrap is maintained under &lt;a href="https://semver.org/"&gt;the Semantic Versioning guidelines&lt;/a&gt;. Sometimes we screw up, but we adhere to those rules whenever possible.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/twbs/bootstrap/releases"&gt;the Releases section of our GitHub project&lt;/a&gt; for changelogs for each release version of Bootstrap. Release announcement posts on &lt;a href="https://blog.getbootstrap.com/"&gt;the official Bootstrap blog&lt;/a&gt; contain summaries of the most noteworthy changes made in each release.&lt;/p&gt; 
&lt;h2&gt;Creators&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Mark Otto&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://x.com/mdo"&gt;https://x.com/mdo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mdo"&gt;https://github.com/mdo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Jacob Thornton&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://x.com/fat"&gt;https://x.com/fat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fat"&gt;https://github.com/fat&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;a href="https://www.browserstack.com/"&gt; &lt;img src="https://live.browserstack.com/images/opensource/browserstack-logo.svg?sanitize=true" alt="BrowserStack" width="192" height="42" /&gt; &lt;/a&gt; 
&lt;p&gt;Thanks to &lt;a href="https://www.browserstack.com/"&gt;BrowserStack&lt;/a&gt; for providing the infrastructure that allows us to test in real browsers!&lt;/p&gt; 
&lt;a href="https://www.netlify.com/"&gt; &lt;img src="https://www.netlify.com/v3/img/components/full-logo-light.svg?sanitize=true" alt="Netlify" width="147" height="40" /&gt; &lt;/a&gt; 
&lt;p&gt;Thanks to &lt;a href="https://www.netlify.com/"&gt;Netlify&lt;/a&gt; for providing us with Deploy Previews!&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [&lt;a href="https://opencollective.com/bootstrap#sponsor"&gt;Become a sponsor&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/bootstrap/sponsor/0/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/0/avatar.svg?sanitize=true" alt="OC sponsor 0" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/1/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/1/avatar.svg?sanitize=true" alt="OC sponsor 1" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/2/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/2/avatar.svg?sanitize=true" alt="OC sponsor 2" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/3/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/3/avatar.svg?sanitize=true" alt="OC sponsor 3" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/4/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/4/avatar.svg?sanitize=true" alt="OC sponsor 4" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/5/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/5/avatar.svg?sanitize=true" alt="OC sponsor 5" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/6/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/6/avatar.svg?sanitize=true" alt="OC sponsor 6" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/7/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/7/avatar.svg?sanitize=true" alt="OC sponsor 7" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/8/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/8/avatar.svg?sanitize=true" alt="OC sponsor 8" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/bootstrap/sponsor/9/website"&gt;&lt;img src="https://opencollective.com/bootstrap/sponsor/9/avatar.svg?sanitize=true" alt="OC sponsor 9" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Backers&lt;/h2&gt; 
&lt;p&gt;Thank you to all our backers! 🙏 [&lt;a href="https://opencollective.com/bootstrap#backer"&gt;Become a backer&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/bootstrap#backers"&gt;&lt;img src="https://opencollective.com/bootstrap/backers.svg?width=890" alt="Backers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Copyright and license&lt;/h2&gt; 
&lt;p&gt;Code and documentation copyright 2011-2025 the &lt;a href="https://github.com/twbs/bootstrap/graphs/contributors"&gt;Bootstrap Authors&lt;/a&gt;. Code released under the &lt;a href="https://github.com/twbs/bootstrap/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;. Docs released under &lt;a href="https://creativecommons.org/licenses/by/3.0/"&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>santinic/audiblez</title>
      <link>https://github.com/santinic/audiblez</link>
      <description>&lt;p&gt;Generate audiobooks from e-books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Audiblez: Generate audiobooks from e-books&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/santinic/audiblez/actions/workflows/pip-install.yaml"&gt;&lt;img src="https://github.com/santinic/audiblez/actions/workflows/pip-install.yaml/badge.svg?sanitize=true" alt="Installing via pip and running" /&gt;&lt;/a&gt; &lt;a href="https://github.com/santinic/audiblez/actions/workflows/git-clone-and-run.yml"&gt;&lt;img src="https://github.com/santinic/audiblez/actions/workflows/git-clone-and-run.yml/badge.svg?sanitize=true" alt="Git clone and run" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/audiblez" alt="PyPI - Python Version" /&gt; &lt;img src="https://img.shields.io/pypi/v/audiblez" alt="PyPI - Version" /&gt;&lt;/p&gt; 
&lt;h3&gt;v4 Now with Graphical interface, CUDA support, and many languages!&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/santinic/audiblez/main/imgs/mac.png" alt="Audiblez GUI on MacOSX" /&gt;&lt;/p&gt; 
&lt;p&gt;Audiblez generates &lt;code&gt;.m4b&lt;/code&gt; audiobooks from regular &lt;code&gt;.epub&lt;/code&gt; e-books, using Kokoro's high-quality speech synthesis.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt; is a recently published text-to-speech model with just 82M params and very natural sounding output. It's released under Apache licence and it was trained on &amp;lt; 100 hours of audio. It currently supports these languages: 🇺🇸 🇬🇧 🇪🇸 🇫🇷 🇮🇳 🇮🇹 🇯🇵 🇧🇷 🇨🇳&lt;/p&gt; 
&lt;p&gt;On a Google Colab's T4 GPU via Cuda, &lt;strong&gt;it takes about 5 minutes to convert "Animal's Farm" by Orwell&lt;/strong&gt; (which is about 160,000 characters) to audiobook, at a rate of about 600 characters per second.&lt;/p&gt; 
&lt;p&gt;On my M2 MacBook Pro, on CPU, it takes about 1 hour, at a rate of about 60 characters per second.&lt;/p&gt; 
&lt;h2&gt;How to install the Command Line tool&lt;/h2&gt; 
&lt;p&gt;If you have Python 3 on your computer, you can install it with pip. You also need &lt;code&gt;espeak-ng&lt;/code&gt; and &lt;code&gt;ffmpeg&lt;/code&gt; installed on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install ffmpeg espeak-ng                   # on Ubuntu/Debian 🐧
pip install audiblez
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install ffmpeg espeak-ng                       # on Mac 🍏
pip install audiblez
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can convert an .epub directly with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;audiblez book.epub -v af_sky
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will first create a bunch of &lt;code&gt;book_chapter_1.wav&lt;/code&gt;, &lt;code&gt;book_chapter_2.wav&lt;/code&gt;, etc. files in the same directory, and at the end it will produce a &lt;code&gt;book.m4b&lt;/code&gt; file with the whole book you can listen with VLC or any audiobook player. It will only produce the &lt;code&gt;.m4b&lt;/code&gt; file if you have &lt;code&gt;ffmpeg&lt;/code&gt; installed on your machine.&lt;/p&gt; 
&lt;h2&gt;How to run the GUI&lt;/h2&gt; 
&lt;p&gt;The GUI is a simple graphical interface to use audiblez. You need some extra dependencies to run the GUI:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt install ffmpeg espeak-ng 
sudo apt install libgtk-3-dev        # just for Ubuntu/Debian 🐧, Windows/Mac don't need this
  
pip install audiblez pillow wxpython
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can run the GUI with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;audiblez-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to run on Windows&lt;/h2&gt; 
&lt;p&gt;After many trials, on Windows we recommend to install audiblez in a Python venv:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open a Windows terminal&lt;/li&gt; 
 &lt;li&gt;Create anew folder: &lt;code&gt;mkdir audiblez&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Enter the folder: &lt;code&gt;cd audiblez&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Create a venv: &lt;code&gt;python -m venv venv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Activate the venv: &lt;code&gt;.\venv\Scripts\Activate.ps1&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install the dependencies: &lt;code&gt;pip install audiblez pillow wxpython&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Now you can run &lt;code&gt;audiblez&lt;/code&gt; or &lt;code&gt;audiblez-ui&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For Cuda support, you need to install Pytorch accordingly: &lt;a href="https://pytorch.org/get-started/locally/"&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Speed&lt;/h2&gt; 
&lt;p&gt;By default the audio is generated using a normal speed, but you can make it up to twice slower or faster by specifying a speed argument between 0.5 to 2.0:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;audiblez book.epub -v af_sky -s 1.5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Voices&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;-v&lt;/code&gt; option to specify the voice to use. Available voices are listed here. The first letter is the language code and the second is the gender of the speaker e.g. &lt;code&gt;im_nicola&lt;/code&gt; is an italian male voice.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://claudio.uk/posts/audiblez-v4.html"&gt;For hearing samples of Kokoro-82M voices, go here&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Voices&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🇺🇸 American English&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;af_alloy&lt;/code&gt;, &lt;code&gt;af_aoede&lt;/code&gt;, &lt;code&gt;af_bella&lt;/code&gt;, &lt;code&gt;af_heart&lt;/code&gt;, &lt;code&gt;af_jessica&lt;/code&gt;, &lt;code&gt;af_kore&lt;/code&gt;, &lt;code&gt;af_nicole&lt;/code&gt;, &lt;code&gt;af_nova&lt;/code&gt;, &lt;code&gt;af_river&lt;/code&gt;, &lt;code&gt;af_sarah&lt;/code&gt;, &lt;code&gt;af_sky&lt;/code&gt;, &lt;code&gt;am_adam&lt;/code&gt;, &lt;code&gt;am_echo&lt;/code&gt;, &lt;code&gt;am_eric&lt;/code&gt;, &lt;code&gt;am_fenrir&lt;/code&gt;, &lt;code&gt;am_liam&lt;/code&gt;, &lt;code&gt;am_michael&lt;/code&gt;, &lt;code&gt;am_onyx&lt;/code&gt;, &lt;code&gt;am_puck&lt;/code&gt;, &lt;code&gt;am_santa&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🇬🇧 British English&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;bf_alice&lt;/code&gt;, &lt;code&gt;bf_emma&lt;/code&gt;, &lt;code&gt;bf_isabella&lt;/code&gt;, &lt;code&gt;bf_lily&lt;/code&gt;, &lt;code&gt;bm_daniel&lt;/code&gt;, &lt;code&gt;bm_fable&lt;/code&gt;, &lt;code&gt;bm_george&lt;/code&gt;, &lt;code&gt;bm_lewis&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🇪🇸 Spanish&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ef_dora&lt;/code&gt;, &lt;code&gt;em_alex&lt;/code&gt;, &lt;code&gt;em_santa&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🇫🇷 French&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ff_siwis&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🇮🇳 Hindi&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;hf_alpha&lt;/code&gt;, &lt;code&gt;hf_beta&lt;/code&gt;, &lt;code&gt;hm_omega&lt;/code&gt;, &lt;code&gt;hm_psi&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🇮🇹 Italian&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;if_sara&lt;/code&gt;, &lt;code&gt;im_nicola&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🇯🇵 Japanese&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jf_alpha&lt;/code&gt;, &lt;code&gt;jf_gongitsune&lt;/code&gt;, &lt;code&gt;jf_nezumi&lt;/code&gt;, &lt;code&gt;jf_tebukuro&lt;/code&gt;, &lt;code&gt;jm_kumo&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🇧🇷 Brazilian Portuguese&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pf_dora&lt;/code&gt;, &lt;code&gt;pm_alex&lt;/code&gt;, &lt;code&gt;pm_santa&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🇨🇳 Mandarin Chinese&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;zf_xiaobei&lt;/code&gt;, &lt;code&gt;zf_xiaoni&lt;/code&gt;, &lt;code&gt;zf_xiaoxiao&lt;/code&gt;, &lt;code&gt;zf_xiaoyi&lt;/code&gt;, &lt;code&gt;zm_yunjian&lt;/code&gt;, &lt;code&gt;zm_yunxi&lt;/code&gt;, &lt;code&gt;zm_yunxia&lt;/code&gt;, &lt;code&gt;zm_yunyang&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more detaila about voice quality, check this document: &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md"&gt;Kokoro-82M voices&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to run on GPU&lt;/h2&gt; 
&lt;p&gt;By default, audiblez runs on CPU. If you pass the option &lt;code&gt;--cuda&lt;/code&gt; it will try to use the Cuda device via Torch.&lt;/p&gt; 
&lt;p&gt;Check out this example: &lt;a href="https://colab.research.google.com/drive/164PQLowogprWQpRjKk33e-8IORAvqXKI?usp=sharing%5D"&gt;Audiblez running on a Google Colab Notebook with Cuda &lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We don't currently support Apple Silicon, as there is not yet a Kokoro implementation in MLX. As soon as it will be available, we will support it.&lt;/p&gt; 
&lt;h2&gt;Manually pick chapters to convert&lt;/h2&gt; 
&lt;p&gt;Sometimes you want to manually select which chapters/sections in the e-book to read out loud. To do so, you can use &lt;code&gt;--pick&lt;/code&gt; to interactively choose the chapters to convert (without running the GUI).&lt;/p&gt; 
&lt;h2&gt;Help page&lt;/h2&gt; 
&lt;p&gt;For all the options available, you can check the help page &lt;code&gt;audiblez --help&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: audiblez [-h] [-v VOICE] [-p] [-s SPEED] [-c] [-o FOLDER] epub_file_path

positional arguments:
  epub_file_path        Path to the epub file

options:
  -h, --help            show this help message and exit
  -v VOICE, --voice VOICE
                        Choose narrating voice: a, b, e, f, h, i, j, p, z
  -p, --pick            Interactively select which chapters to read in the audiobook
  -s SPEED, --speed SPEED
                        Set speed from 0.5 to 2.0
  -c, --cuda            Use GPU via Cuda in Torch if available
  -o FOLDER, --output FOLDER
                        Output folder for the audiobook and temporary files

example:
  audiblez book.epub -l en-us -v af_sky

to use the GUI, run:
  audiblez-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Author&lt;/h2&gt; 
&lt;p&gt;by &lt;a href="https://claudio.uk"&gt;Claudio Santini&lt;/a&gt; in 2025, distributed under MIT licence.&lt;/p&gt; 
&lt;p&gt;Related Article: &lt;a href="https://claudio.uk/posts/audiblez-v4.html"&gt;Audiblez v4: Generate Audiobooks from E-books&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>firecracker-microvm/firecracker</title>
      <link>https://github.com/firecracker-microvm/firecracker</link>
      <description>&lt;p&gt;Secure and fast microVMs for serverless computing.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="docs/images/fc_logo_full_transparent-bg_white-fg.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="docs/images/fc_logo_full_transparent-bg.png" /&gt; 
 &lt;img alt="Firecracker Logo Title" width="750" src="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/images/fc_logo_full_transparent-bg.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;Our mission is to enable secure, multi-tenant, minimal-overhead execution of container and function workloads.&lt;/p&gt; 
&lt;p&gt;Read more about the Firecracker Charter &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CHARTER.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;What is Firecracker?&lt;/h2&gt; 
&lt;p&gt;Firecracker is an open source virtualization technology that is purpose-built for creating and managing secure, multi-tenant container and function-based services that provide serverless operational models. Firecracker runs workloads in lightweight virtual machines, called microVMs, which combine the security and isolation properties provided by hardware virtualization technology with the speed and flexibility of containers.&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;The main component of Firecracker is a virtual machine monitor (VMM) that uses the Linux Kernel Virtual Machine (KVM) to create and run microVMs. Firecracker has a minimalist design. It excludes unnecessary devices and guest-facing functionality to reduce the memory footprint and attack surface area of each microVM. This improves security, decreases the startup time, and increases hardware utilization. Firecracker has also been integrated in container runtimes, for example &lt;a href="https://github.com/kata-containers/kata-containers"&gt;Kata Containers&lt;/a&gt; and &lt;a href="https://github.com/liquidmetal-dev/flintlock"&gt;Flintlock&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Firecracker was developed at Amazon Web Services to accelerate the speed and efficiency of services like &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; and &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt;. Firecracker is open sourced under &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/LICENSE"&gt;Apache version 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To read more about Firecracker, check out &lt;a href="https://firecracker-microvm.github.io"&gt;firecracker-microvm.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To get started with Firecracker, download the latest &lt;a href="https://github.com/firecracker-microvm/firecracker/releases"&gt;release&lt;/a&gt; binaries or build it from source.&lt;/p&gt; 
&lt;p&gt;You can build Firecracker on any Unix/Linux system that has Docker running (we use a development container) and &lt;code&gt;bash&lt;/code&gt; installed, as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/firecracker-microvm/firecracker
cd firecracker
tools/devtool build
toolchain="$(uname -m)-unknown-linux-musl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Firecracker binary will be placed at &lt;code&gt;build/cargo_target/${toolchain}/debug/firecracker&lt;/code&gt;. For more information on building, testing, and running Firecracker, go to the &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/getting-started.md"&gt;quickstart guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The overall security of Firecracker microVMs, including the ability to meet the criteria for safe multi-tenant computing, depends on a well configured Linux host operating system. A configuration that we believe meets this bar is included in &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/prod-host-setup.md"&gt;the production host setup document&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Firecracker is already running production workloads within AWS, but it's still Day 1 on the journey guided by our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CHARTER.md"&gt;mission&lt;/a&gt;. There's a lot more to build and we welcome all contributions.&lt;/p&gt; 
&lt;p&gt;To contribute to Firecracker, check out the development setup section in the &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/getting-started.md"&gt;getting started guide&lt;/a&gt; and then the Firecracker &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;p&gt;New Firecracker versions are released via the GitHub repository &lt;a href="https://github.com/firecracker-microvm/firecracker/releases"&gt;releases&lt;/a&gt; page, typically every two or three months. A history of changes is recorded in our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CHANGELOG.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The Firecracker release policy is detailed &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/RELEASE_POLICY.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Design&lt;/h2&gt; 
&lt;p&gt;Firecracker's overall architecture is described in &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/design.md"&gt;the design document&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features &amp;amp; Capabilities&lt;/h2&gt; 
&lt;p&gt;Firecracker consists of a single micro Virtual Machine Manager process that exposes an API endpoint to the host once started. The API is &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/src/firecracker/swagger/firecracker.yaml"&gt;specified in OpenAPI format&lt;/a&gt;. Read more about it in the &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/api_requests"&gt;API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;API endpoint&lt;/strong&gt; can be used to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Configure the microvm by: 
  &lt;ul&gt; 
   &lt;li&gt;Setting the number of vCPUs (the default is 1).&lt;/li&gt; 
   &lt;li&gt;Setting the memory size (the default is 128 MiB).&lt;/li&gt; 
   &lt;li&gt;Configuring a &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/cpu_templates/cpu-templates.md"&gt;CPU template&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Add one or more network interfaces to the microVM.&lt;/li&gt; 
 &lt;li&gt;Add one or more read-write or read-only disks to the microVM, each represented by a file-backed block device.&lt;/li&gt; 
 &lt;li&gt;Trigger a block device re-scan while the guest is running. This enables the guest OS to pick up size changes to the block device's backing file.&lt;/li&gt; 
 &lt;li&gt;Change the backing file for a block device, before or after the guest boots.&lt;/li&gt; 
 &lt;li&gt;Configure rate limiters for virtio devices which can limit the bandwidth, operations per second, or both.&lt;/li&gt; 
 &lt;li&gt;Configure the logging and metric system.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[BETA]&lt;/code&gt; Configure the data tree of the guest-facing metadata service. The service is only available to the guest if this resource is configured.&lt;/li&gt; 
 &lt;li&gt;Add a &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/vsock.md"&gt;vsock socket&lt;/a&gt; to the microVM.&lt;/li&gt; 
 &lt;li&gt;Add a &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/entropy.md"&gt;entropy device&lt;/a&gt; to the microVM.&lt;/li&gt; 
 &lt;li&gt;Start the microVM using a given kernel image, root file system, and boot arguments.&lt;/li&gt; 
 &lt;li&gt;[x86_64 only] Stop the microVM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Built-in Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Demand fault paging and CPU oversubscription enabled by default.&lt;/li&gt; 
 &lt;li&gt;Advanced, thread-specific seccomp filters for enhanced security.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/docs/jailer.md"&gt;Jailer&lt;/a&gt; process for starting Firecracker in production scenarios; applies a cgroup/namespace isolation barrier and then drops privileges.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tested platforms&lt;/h2&gt; 
&lt;p&gt;We test all combinations of:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Instance&lt;/th&gt; 
   &lt;th align="left"&gt;Host OS &amp;amp; Kernel&lt;/th&gt; 
   &lt;th align="left"&gt;Guest Rootfs&lt;/th&gt; 
   &lt;th align="left"&gt;Guest Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m5n.metal (Intel Cascade Lake)&lt;/td&gt; 
   &lt;td align="left"&gt;al2 linux_5.10&lt;/td&gt; 
   &lt;td align="left"&gt;ubuntu 24.04&lt;/td&gt; 
   &lt;td align="left"&gt;linux_5.10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m6i.metal (Intel Ice Lake)&lt;/td&gt; 
   &lt;td align="left"&gt;al2023 linux_6.1&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;linux_6.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m7i.metal-24xl (Intel Sapphire Rapids)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m7i.metal-48xl (Intel Sapphire Rapids)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m6a.metal (AMD Milan)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m7a.metal-48xl (AMD Genoa)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m6g.metal (Graviton 2)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m7g.metal (Graviton 3)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m8g.metal-24xl (Graviton 4)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m8g.metal-48xl (Graviton 4)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Known issues and Limitations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;pl031&lt;/code&gt; RTC device on aarch64 does not support interrupts, so guest programs which use an RTC alarm (e.g. &lt;code&gt;hwclock&lt;/code&gt;) will not work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Firecracker's performance characteristics are listed as part of the &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/SPECIFICATION.md"&gt;specification documentation&lt;/a&gt;. All specifications are a part of our commitment to supporting container and function workloads in serverless operational models, and are therefore enforced via continuous integration testing.&lt;/p&gt; 
&lt;h2&gt;Policy for Security Disclosures&lt;/h2&gt; 
&lt;p&gt;The security of Firecracker is our top priority. If you suspect you have uncovered a vulnerability, contact us privately, as outlined in our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/SECURITY.md"&gt;security policy document&lt;/a&gt;; we will immediately prioritize your disclosure.&lt;/p&gt; 
&lt;h2&gt;FAQ &amp;amp; Contact&lt;/h2&gt; 
&lt;p&gt;Frequently asked questions are collected in our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/FAQ.md"&gt;FAQ doc&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can get in touch with the Firecracker community in the following ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Security-related issues, see our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/SECURITY.md"&gt;security policy document&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Chat with us on our &lt;a href="https://join.slack.com/t/firecracker-microvm/shared_invite/zt-2tc0mfxpc-tU~HYAYSzLDl5XGGJU3YIg"&gt;Slack workspace&lt;/a&gt; &lt;em&gt;Note: most of the maintainers are on a European time zone.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Open a GitHub issue in this repository.&lt;/li&gt; 
 &lt;li&gt;Email the maintainers at &lt;a href="mailto:firecracker-maintainers@amazon.com"&gt;firecracker-maintainers@amazon.com&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When communicating within the Firecracker community, please mind our &lt;a href="https://raw.githubusercontent.com/firecracker-microvm/firecracker/main/CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>karpathy/nn-zero-to-hero</title>
      <link>https://github.com/karpathy/nn-zero-to-hero</link>
      <description>&lt;p&gt;Neural Networks: Zero to Hero&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Neural Networks: Zero to Hero&lt;/h2&gt; 
&lt;p&gt;A course on neural networks that starts all the way at the basics. The course is a series of YouTube videos where we code and train neural networks together. The Jupyter notebooks we build in the videos are then captured here inside the &lt;a href="https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/"&gt;lectures&lt;/a&gt; directory. Every lecture also has a set of exercises included in the video description. (This may grow into something more respectable).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Lecture 1: The spelled-out intro to neural networks and backpropagation: building micrograd&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Backpropagation and training of neural networks. Assumes basic knowledge of Python and a vague recollection of calculus from high school.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=VMj-3S1tku0"&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/micrograd"&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/micrograd"&gt;micrograd Github repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Lecture 2: The spelled-out intro to language modeling: building makemore&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We implement a bigram character-level language model, which we will further complexify in followup videos into a modern Transformer language model, like GPT. In this video, the focus is on (1) introducing torch.Tensor and its subtleties and use in efficiently evaluating neural networks and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss (e.g. the negative log likelihood for classification).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=PaCmpygFfXo"&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part1_bigrams.ipynb"&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/makemore"&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Lecture 3: Building makemore Part 2: MLP&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/TCH_1BHY58I"&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part2_mlp.ipynb"&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/makemore"&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Lecture 4: Building makemore Part 3: Activations &amp;amp; Gradients, BatchNorm&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you'd want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/P6sfmUTpUmc"&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part3_bn.ipynb"&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/makemore"&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Lecture 5: Building makemore Part 4: Becoming a Backprop Ninja&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(). That is, we backprop through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get an intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.&lt;/p&gt; 
&lt;p&gt;I recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched. The exercise is &lt;a href="https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing"&gt;here as a Google Colab&lt;/a&gt;. Good luck :)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/q8SA3rM6ckI"&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part4_backprop.ipynb"&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/makemore"&gt;makemore Github repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Lecture 6: Building makemore Part 5: Building WaveNet&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We take the 2-layer MLP from previous video and make it deeper with a tree-like structure, arriving at a convolutional neural network architecture similar to the WaveNet (2016) from DeepMind. In the WaveNet paper, the same hierarchical architecture is implemented more efficiently using causal dilated convolutions (not yet covered). Along the way we get a better sense of torch.nn and what it is and how it works under the hood, and what a typical deep learning development process looks like (a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ...).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/t3YJ5hKiMQ0"&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/karpathy/nn-zero-to-hero/master/lectures/makemore/makemore_part5_cnn1.ipynb"&gt;Jupyter notebook files&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Lecture 7: Let's build GPT: from scratch, in code, spelled out.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=kCc8FmEb1nY"&gt;YouTube video lecture&lt;/a&gt;. For all other links see the video description.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Lecture 8: Let's build the GPT Tokenizer&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=zduSFxRajkE"&gt;YouTube video lecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karpathy/minbpe"&gt;minBPE code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing"&gt;Google Colab&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;Ongoing...&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>spf13/cobra</title>
      <link>https://github.com/spf13/cobra</link>
      <description>&lt;p&gt;A Commander for modern Go CLI interactions&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://cobra.dev"&gt; &lt;img width="512" height="535" alt="cobra-logo" src="https://github.com/user-attachments/assets/c8bf9aad-b5ae-41d3-8899-d83baec10af8" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Cobra is a library for creating powerful modern CLI applications.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://cobra.dev"&gt;Visit Cobra.dev for extensive documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Cobra is used in many Go projects such as &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt;, &lt;a href="https://gohugo.io"&gt;Hugo&lt;/a&gt;, and &lt;a href="https://github.com/cli/cli"&gt;GitHub CLI&lt;/a&gt; to name a few. &lt;a href="https://raw.githubusercontent.com/spf13/cobra/main/site/content/projects_using_cobra.md"&gt;This list&lt;/a&gt; contains a more extensive list of projects using Cobra.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/spf13/cobra/actions?query=workflow%3ATest"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/spf13/cobra/test.yml?branch=main&amp;amp;longCache=true&amp;amp;label=Test&amp;amp;logo=github%20actions&amp;amp;logoColor=fff" alt="" /&gt;&lt;/a&gt; &lt;a href="https://pkg.go.dev/github.com/spf13/cobra"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/spf13/cobra.svg?sanitize=true" alt="Go Reference" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/spf13/cobra"&gt;&lt;img src="https://goreportcard.com/badge/github.com/spf13/cobra" alt="Go Report Card" /&gt;&lt;/a&gt; &lt;a href="https://gophers.slack.com/archives/CD3LP1199"&gt;&lt;img src="https://img.shields.io/badge/Slack-cobra-brightgreen" alt="Slack" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Supported by:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.warp.dev/cobra"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="https://www.warp.dev/cobra"&gt;Warp, the AI terminal for devs&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://www.warp.dev/cobra"&gt;Try Cobra in Warp today&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Overview&lt;/h1&gt; 
&lt;p&gt;Cobra is a library providing a simple interface to create powerful modern CLI interfaces similar to git &amp;amp; go tools.&lt;/p&gt; 
&lt;p&gt;Cobra provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Easy subcommand-based CLIs: &lt;code&gt;app server&lt;/code&gt;, &lt;code&gt;app fetch&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Fully POSIX-compliant flags (including short &amp;amp; long versions)&lt;/li&gt; 
 &lt;li&gt;Nested subcommands&lt;/li&gt; 
 &lt;li&gt;Global, local and cascading flags&lt;/li&gt; 
 &lt;li&gt;Intelligent suggestions (&lt;code&gt;app srver&lt;/code&gt;... did you mean &lt;code&gt;app server&lt;/code&gt;?)&lt;/li&gt; 
 &lt;li&gt;Automatic help generation for commands and flags&lt;/li&gt; 
 &lt;li&gt;Grouping help for subcommands&lt;/li&gt; 
 &lt;li&gt;Automatic help flag recognition of &lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Automatically generated shell autocomplete for your application (bash, zsh, fish, powershell)&lt;/li&gt; 
 &lt;li&gt;Automatically generated man pages for your application&lt;/li&gt; 
 &lt;li&gt;Command aliases so you can change things without breaking them&lt;/li&gt; 
 &lt;li&gt;The flexibility to define your own help, usage, etc.&lt;/li&gt; 
 &lt;li&gt;Optional seamless integration with &lt;a href="https://github.com/spf13/viper"&gt;viper&lt;/a&gt; for 12-factor apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Concepts&lt;/h1&gt; 
&lt;p&gt;Cobra is built on a structure of commands, arguments &amp;amp; flags.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Commands&lt;/strong&gt; represent actions, &lt;strong&gt;Args&lt;/strong&gt; are things and &lt;strong&gt;Flags&lt;/strong&gt; are modifiers for those actions.&lt;/p&gt; 
&lt;p&gt;The best applications read like sentences when used, and as a result, users intuitively know how to interact with them.&lt;/p&gt; 
&lt;p&gt;The pattern to follow is &lt;code&gt;APPNAME VERB NOUN --ADJECTIVE&lt;/code&gt; or &lt;code&gt;APPNAME COMMAND ARG --FLAG&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;A few good real world examples may better illustrate this point.&lt;/p&gt; 
&lt;p&gt;In the following example, 'server' is a command, and 'port' is a flag:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;hugo server --port=1313
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this command we are telling Git to clone the url bare.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone URL --bare
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Commands&lt;/h2&gt; 
&lt;p&gt;Command is the central point of the application. Each interaction that the application supports will be contained in a Command. A command can have children commands and optionally run an action.&lt;/p&gt; 
&lt;p&gt;In the example above, 'server' is the command.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pkg.go.dev/github.com/spf13/cobra#Command"&gt;More about cobra.Command&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Flags&lt;/h2&gt; 
&lt;p&gt;A flag is a way to modify the behavior of a command. Cobra supports fully POSIX-compliant flags as well as the Go &lt;a href="https://golang.org/pkg/flag/"&gt;flag package&lt;/a&gt;. A Cobra command can define flags that persist through to children commands and flags that are only available to that command.&lt;/p&gt; 
&lt;p&gt;In the example above, 'port' is the flag.&lt;/p&gt; 
&lt;p&gt;Flag functionality is provided by the &lt;a href="https://github.com/spf13/pflag"&gt;pflag library&lt;/a&gt;, a fork of the flag standard library which maintains the same interface while adding POSIX compliance.&lt;/p&gt; 
&lt;h1&gt;Installing&lt;/h1&gt; 
&lt;p&gt;Using Cobra is easy. First, use &lt;code&gt;go get&lt;/code&gt; to install the latest version of the library.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;go get -u github.com/spf13/cobra@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, include Cobra in your application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import "github.com/spf13/cobra"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;cobra-cli&lt;/code&gt; is a command line program to generate cobra applications and command files. It will bootstrap your application scaffolding to rapidly develop a Cobra-based application. It is the easiest way to incorporate Cobra into your application.&lt;/p&gt; 
&lt;p&gt;It can be installed by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;go install github.com/spf13/cobra-cli@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For complete details on using the Cobra-CLI generator, please read &lt;a href="https://github.com/spf13/cobra-cli/raw/main/README.md"&gt;The Cobra Generator README&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For complete details on using the Cobra library, please read &lt;a href="https://raw.githubusercontent.com/spf13/cobra/main/site/content/user_guide.md"&gt;The Cobra User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Cobra is released under the Apache 2.0 license. See &lt;a href="https://raw.githubusercontent.com/spf13/cobra/main/LICENSE.txt"&gt;LICENSE.txt&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenPipe/ART</title>
      <link>https://github.com/OpenPipe/ART</link>
      <description>&lt;p&gt;Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://art.openpipe.ai"&gt;
   &lt;picture&gt; 
    &lt;img alt="ART logo" src="https://github.com/openpipe/art/raw/main/assets/ART_logo.png" width="160px" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Train multi-step agents for real-world tasks using GRPO. &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/openpipe/art/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true" alt="PRs-Welcome" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/openpipe-art/"&gt;&lt;img src="https://img.shields.io/pypi/v/openpipe-art?color=364fc7" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Train Agent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/zbBHRUpwf4"&gt;&lt;img src="https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://art.openpipe.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-orange?style=plastic&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📏 RULER: Zero-Shot Agent Rewards&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;RULER&lt;/strong&gt; (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the rest—&lt;strong&gt;no labeled data, expert feedback, or reward engineering required&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;✨ &lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2-3x faster development&lt;/strong&gt; - Skip reward function engineering entirely&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;General-purpose&lt;/strong&gt; - Works across any task without modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strong performance&lt;/strong&gt; - Matches or exceeds hand-crafted rewards in 3/4 benchmarks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy integration&lt;/strong&gt; - Drop-in replacement for manual reward functions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, "openai/o3")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://art.openpipe.ai/fundamentals/ruler"&gt;📖 Learn more about RULER →&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ART Overview&lt;/h2&gt; 
&lt;p&gt;ART is an open-source RL framework that improves agent reliability by allowing LLMs to &lt;strong&gt;learn from experience&lt;/strong&gt;. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the &lt;a href="https://art.openpipe.ai"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;📒 Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Task&lt;/th&gt; 
   &lt;th&gt;Example Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Comparative Performance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ART•E LangGraph&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb"&gt;🏋️ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using LangGraph&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP•RL&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb"&gt;🏋️ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B masters the NWS MCP server&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ART•E [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb"&gt;🏋️ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using RULER&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/dev/art-e/art_e/evaluate/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2048&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;🏋️ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play 2048&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/2048/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Clue&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb"&gt;🏋️ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to solve Temporal Clue&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tic Tac Toe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb"&gt;🏋️ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Tic Tac Toe&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/tic_tac_toe/display-benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codenames&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb"&gt;🏋️ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Codenames&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png" height="72" /&gt; &lt;a href="https://github.com/OpenPipe/art-notebooks/raw/main/examples/codenames/Codenames_RL.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AutoRL [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb"&gt;🏋️ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Train Qwen 2.5 7B to master any task&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;📰 ART News&lt;/h2&gt; 
&lt;p&gt;Explore our latest research and updates on building SOTA agents.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🗞️ &lt;strong&gt;&lt;a href="https://art.openpipe.ai/integrations/langgraph-integration"&gt;ART now integrates seamlessly with LangGraph&lt;/a&gt;&lt;/strong&gt; - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.&lt;/li&gt; 
 &lt;li&gt;🗞️ &lt;strong&gt;&lt;a href="https://x.com/corbtt/status/1953171838382817625"&gt;MCP•RL: Teach Your Model to Master Any MCP Server&lt;/a&gt;&lt;/strong&gt; - Automatically train models to effectively use MCP server tools through reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;🗞️ &lt;strong&gt;&lt;a href="https://x.com/mattshumer_/status/1950572449025650733"&gt;AutoRL: Zero-Data Training for Any Task&lt;/a&gt;&lt;/strong&gt; - Train custom AI models without labeled data using automatic input generation and RULER evaluation.&lt;/li&gt; 
 &lt;li&gt;🗞️ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards"&gt;RULER: Easy Mode for RL Rewards&lt;/a&gt;&lt;/strong&gt; is now available for automatic reward generation in reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;🗞️ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ART·E: How We Built an Email Research Agent That Beats o3&lt;/a&gt;&lt;/strong&gt; demonstrates a Qwen 2.5 14B email agent outperforming OpenAI's o3.&lt;/li&gt; 
 &lt;li&gt;🗞️ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-trainer"&gt;ART Trainer: A New RL Trainer for Agents&lt;/a&gt;&lt;/strong&gt; enables easy training of LLM-based agents using GRPO.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://openpipe.ai/blog"&gt;📖 See all blog posts →&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why ART?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ART provides convenient wrappers for introducing RL training into &lt;strong&gt;existing applications&lt;/strong&gt;. We abstract the training server into a modular service that your code doesn't need to interface with.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train from anywhere.&lt;/strong&gt; Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.&lt;/li&gt; 
 &lt;li&gt;Integrations with hosted platforms like W&amp;amp;B, Langfuse, and OpenPipe provide flexible observability and &lt;strong&gt;simplify debugging&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ART is customizable with &lt;strong&gt;intelligent defaults&lt;/strong&gt;. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install openpipe-art
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🤖 ART•E Agent&lt;/h2&gt; 
&lt;p&gt;Curious about how to use ART for a real-world task? Check out the &lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ART•E Agent&lt;/a&gt; blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!&lt;/p&gt; 
&lt;img src="https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png" width="700" /&gt; 
&lt;h2&gt;🔁 Training Loop Overview&lt;/h2&gt; 
&lt;p&gt;ART's functionality is divided into a &lt;strong&gt;client&lt;/strong&gt; and a &lt;strong&gt;server&lt;/strong&gt;. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).&lt;/li&gt; 
   &lt;li&gt;Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.&lt;/li&gt; 
   &lt;li&gt;As the agent executes, each &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, and &lt;code&gt;assistant&lt;/code&gt; message is stored in a Trajectory.&lt;/li&gt; 
   &lt;li&gt;When a rollout finishes, your code assigns a &lt;code&gt;reward&lt;/code&gt; to its Trajectory, indicating the performance of the LLM.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.&lt;/li&gt; 
   &lt;li&gt;The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).&lt;/li&gt; 
   &lt;li&gt;The server saves the newly trained LoRA to a local directory and loads it into vLLM.&lt;/li&gt; 
   &lt;li&gt;Inference is unblocked and the loop resumes at step 1.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This training loop runs until a specified number of inference and training iterations have completed.&lt;/p&gt; 
&lt;h2&gt;🧩 Supported Models&lt;/h2&gt; 
&lt;p&gt;ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth&lt;/a&gt;. Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on &lt;a href="https://discord.gg/zbBHRUpwf4"&gt;Discord&lt;/a&gt; or open an issue on &lt;a href="https://github.com/openpipe/art/issues"&gt;GitHub&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;🤝 Contributing&lt;/h2&gt; 
&lt;p&gt;ART is in active development, and contributions are most welcome! Please see the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;📖 Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;⚖️ License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🙏 Credits&lt;/h2&gt; 
&lt;p&gt;ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/trl"&gt;trl&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchtune"&gt;torchtune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>plait-board/drawnix</title>
      <link>https://github.com/plait-board/drawnix</link>
      <description>&lt;p&gt;开源白板工具（SaaS），一体化白板，包含思维导图、流程图、自由画等。All in one open-source whiteboard tool with mind, flowchart, freehand and etc.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture style="width: 320px"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/plait-board/drawnix/blob/develop/apps/web/public/logo/logo_drawnix_h.svg?raw=true" /&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/plait-board/drawnix/blob/develop/apps/web/public/logo/logo_drawnix_h_dark.svg?raw=true" /&gt; 
  &lt;img src="https://github.com/plait-board/drawnix/raw/develop/apps/web/public/logo/logo_drawnix_h.svg?raw=true" width="360" alt="Drawnix logo and name" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt; 开源白板工具（SaaS），一体化白板，包含思维导图、流程图、自由画等 &lt;br /&gt; &lt;/h2&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;figure&gt; 
  &lt;a target="_blank" rel="noopener"&gt; &lt;img src="https://github.com/plait-board/drawnix/raw/develop/apps/web/public/product_showcase/case-2.png" alt="Product showcase" width="80%" /&gt; &lt;/a&gt; 
  &lt;figcaption&gt; 
   &lt;p align="center"&gt; All in one 白板，思维导图、流程图、自由画等 &lt;/p&gt; 
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
 &lt;a href="https://hellogithub.com/repository/plait-board/drawnix" target="_blank"&gt; 
  &lt;picture style="width: 250"&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=4dcea807fab7468a962c153b07ae4e4e&amp;amp;claim_uid=zmFSY5k8EuZri43&amp;amp;theme=neutral" /&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=4dcea807fab7468a962c153b07ae4e4e&amp;amp;claim_uid=zmFSY5k8EuZri43&amp;amp;theme=dark" /&gt; 
   &lt;img src="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=4dcea807fab7468a962c153b07ae4e4e&amp;amp;claim_uid=zmFSY5k8EuZri43&amp;amp;theme=neutral" alt="Featured｜HelloGitHub" style="width: 250px; height: 54px;" width="250" height="54" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13979" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13979" alt="plait-board%2Fdrawnix | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://github.com/plait-board/drawnix/raw/develop/README_en.md"&gt;&lt;em&gt;English README&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;特性&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;💯&amp;nbsp;免费 + 开源&lt;/li&gt; 
 &lt;li&gt;⚒️&amp;nbsp;思维导图、流程图&lt;/li&gt; 
 &lt;li&gt;🖌 画笔&lt;/li&gt; 
 &lt;li&gt;😀 插入图片&lt;/li&gt; 
 &lt;li&gt;🚀 基于插件机制&lt;/li&gt; 
 &lt;li&gt;🖼️ 📃 导出为 PNG, JSON(&lt;code&gt;.drawnix&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;💾 自动保存（浏览器缓存）&lt;/li&gt; 
 &lt;li&gt;⚡ 编辑特性：撤销、重做、复制、粘贴等&lt;/li&gt; 
 &lt;li&gt;🌌 无限画布：缩放、滚动&lt;/li&gt; 
 &lt;li&gt;🎨 主题模式&lt;/li&gt; 
 &lt;li&gt;📱 移动设备适配&lt;/li&gt; 
 &lt;li&gt;📈 支持 mermaid 语法转流程图&lt;/li&gt; 
 &lt;li&gt;✨ 支持 markdown 文本转思维导图（新支持 🔥🔥🔥）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;关于名称&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Drawnix&lt;/strong&gt;&lt;/em&gt; ，源于绘画( &lt;em&gt;&lt;strong&gt;Draw&lt;/strong&gt;&lt;/em&gt; )与凤凰( &lt;em&gt;&lt;strong&gt;Phoenix&lt;/strong&gt;&lt;/em&gt; )的灵感交织。&lt;/p&gt; 
&lt;p&gt;凤凰象征着生生不息的创造力，而 &lt;em&gt;Draw&lt;/em&gt; 代表着人类最原始的表达方式。在这里，每一次创作都是一次艺术的涅槃，每一笔绘画都是灵感的重生。&lt;/p&gt; 
&lt;p&gt;创意如同凤凰，浴火方能重生，而 &lt;em&gt;&lt;strong&gt;Drawnix&lt;/strong&gt;&lt;/em&gt; 要做技术与创意之火的守护者。&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Draw Beyond, Rise Above.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;与 Plait 画图框架&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Drawnix&lt;/em&gt; 的定位是一个开箱即用、开源、免费的工具产品，它的底层是 &lt;em&gt;Plait&lt;/em&gt; 框架，&lt;em&gt;Plait&lt;/em&gt; 是我司开源的一款画图框架，代表着公司在知识库产品(&lt;a href="https://pingcode.com/product/wiki?utm_source=drawnix"&gt;PingCode Wiki&lt;/a&gt;)上的重要技术沉淀。&lt;/p&gt; 
&lt;p&gt;Drawnix 是插件架构，与前面说到开源工具比技术架构更复杂一些，但是插件架构也有优势，比如能够支持多种 UI 框架（&lt;em&gt;Angular、React&lt;/em&gt;），能够集成不同富文本框架（当前仅支持 &lt;em&gt;Slate&lt;/em&gt; 框架），在开发上可以很好的实现业务的分层，开发各种细粒度的可复用插件，可以扩展更多的画板的应用场景。&lt;/p&gt; 
&lt;h2&gt;仓储结构&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;drawnix/
├── apps/
│   ├── web                   # drawnix.com
│   │    └── index.html       # HTML
├── dist/                     # 构建产物
├── packages/
│   └── drawnix/              # 白板应用
│   └── react-board/          # 白板 React 视图层
│   └── react-text/           # 文本渲染模块
├── package.json
├── ...
└── README.md
└── README_en.md

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;应用&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://drawnix.com"&gt;&lt;em&gt;https://drawnix.com&lt;/em&gt;&lt;/a&gt; 是 &lt;em&gt;drawnix&lt;/em&gt; 的最小化应用。&lt;/p&gt; 
&lt;p&gt;近期会高频迭代 drawnix.com，直到发布 &lt;em&gt;Dawn（破晓）&lt;/em&gt; 版本。&lt;/p&gt; 
&lt;h2&gt;开发&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;npm install

npm run start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;docker pull pubuzhixing/drawnix:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;依赖&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/worktile/plait"&gt;plait&lt;/a&gt; - 开源画图框架&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ianstormtaylor/slate"&gt;slate&lt;/a&gt; - 富文本编辑器框架&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/floating-ui/floating-ui"&gt;floating-ui&lt;/a&gt; - 一个超级好用的创建弹出层基础库&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;贡献&lt;/h2&gt; 
&lt;p&gt;欢迎任何形式的贡献：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;提 Bug&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;贡献代码&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;感谢支持&lt;/h2&gt; 
&lt;p&gt;特别感谢公司对开源项目的大力支持，也感谢为本项目贡献代码、提供建议的朋友。&lt;/p&gt; 
&lt;p align="left"&gt; &lt;a href="https://pingcode.com?utm_source=drawnix" target="_blank"&gt; &lt;img src="https://cdn-aliyun.pingcode.com/static/site/img/pingcode-logo.4267e7b.svg?sanitize=true" width="120" alt="PingCode" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/plait-board/drawnix/raw/master/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>willccbb/verifiers</title>
      <link>https://github.com/willccbb/verifiers</link>
      <description>&lt;p&gt;Verifiers for LLM Reinforcement Learning&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Verifiers&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Environments for LLM Reinforcement Learning &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Verifiers is a library of modular components for creating RL environments and training LLM agents. Verifiers includes an async GRPO implementation built around the &lt;code&gt;transformers&lt;/code&gt; Trainer, is supported by &lt;code&gt;prime-rl&lt;/code&gt; for large-scale FSDP training, and can easily be integrated into any RL framework which exposes an OpenAI-compatible inference client. In addition to RL training, Verifiers can be used directly for building LLM evaluations, creating synthetic data pipelines, and implementing agent harnesses.&lt;/p&gt; 
&lt;p&gt;Full documentation is available &lt;a href="https://verifiers.readthedocs.io/en/latest/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;code&gt;verifiers&lt;/code&gt; with along &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; for dependency management in your own project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
uv init # create a fresh project
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For local (CPU) development and evaluation with API models, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add verifiers # uv add 'verifiers[dev]' for Jupyter + testing support
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For training on GPUs with &lt;code&gt;vf.GRPOTrainer&lt;/code&gt;, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add 'verifiers[all]' &amp;amp;&amp;amp; uv pip install flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use the latest &lt;code&gt;main&lt;/code&gt; branch, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add verifiers @ git+https://github.com/willccbb/verifiers.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use with &lt;code&gt;prime-rl&lt;/code&gt;, see &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To install &lt;code&gt;verifiers&lt;/code&gt; from source for core library development, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/willccbb/verifiers.git
cd verifiers
uv sync --all-extras &amp;amp;&amp;amp; uv pip install flash-attn --no-build-isolation
uv run pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In general, we recommend that you build and train Environments &lt;em&gt;with&lt;/em&gt; &lt;code&gt;verifiers&lt;/code&gt;, not &lt;em&gt;in&lt;/em&gt; &lt;code&gt;verifiers&lt;/code&gt;. If you find yourself needing to clone and modify the core library in order to implement key functionality for your project, we'd love for you to open an issue so that we can try and streamline the development experience. Our aim is for &lt;code&gt;verifiers&lt;/code&gt; to be a reliable toolkit to build on top of, and to minimize the "fork proliferation" which often pervades the RL infrastructure ecosystem.&lt;/p&gt; 
&lt;h2&gt;Environments&lt;/h2&gt; 
&lt;p&gt;Environments in Verifiers are installable Python modules which can specify dependencies in a &lt;code&gt;pyproject.toml&lt;/code&gt;, and which expose a &lt;code&gt;load_environment&lt;/code&gt; function for instantiation by downstream applications (e.g. trainers). See &lt;code&gt;environments/&lt;/code&gt; for examples.&lt;/p&gt; 
&lt;p&gt;To initialize a blank Environment module template, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-init vf-environment-name # -p /path/to/environments (defaults to "./environments")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To an install an Environment module into your project, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-install vf-environment-name # -p /path/to/environments (defaults to "./environments") 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install an Environment module from this repo's &lt;code&gt;environments&lt;/code&gt; folder, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-install vf-math-python --from-repo # -b branch_or_commit (defaults to "main")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once an Environment module is installed, you can create an instance of the Environment using &lt;code&gt;load_environment&lt;/code&gt;, passing any necessary args:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.load_environment("vf-environment-name", **env_args)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run a quick evaluation of your Environment with an API-based model, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-eval vf-environment-name # vf-eval -h for config options; defaults to gpt-4.1-mini, 5 prompts, 3 rollouts for each
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The core elements of Environments in are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Datasets: a Hugging Face &lt;code&gt;Dataset&lt;/code&gt; with a &lt;code&gt;prompt&lt;/code&gt; column for inputs, and either &lt;code&gt;answer (str)&lt;/code&gt; or &lt;code&gt;info (dict)&lt;/code&gt; columns for evaluation&lt;/li&gt; 
 &lt;li&gt;Rollout logic: interactions between models and the environment (e.g. &lt;code&gt;env_response&lt;/code&gt; + &lt;code&gt;is_completed&lt;/code&gt; for any &lt;code&gt;MultiTurnEnv&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Rubrics: an encapsulation for one or more reward functions&lt;/li&gt; 
 &lt;li&gt;Parsers: optional; an encapsulation for reusable parsing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We support both &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style and &lt;code&gt;/v1/completions&lt;/code&gt;-style inference via OpenAI clients, though we generally recommend &lt;code&gt;/v1/chat/completions&lt;/code&gt;-style inference for the vast majority of applications. Both the included &lt;code&gt;GRPOTrainer&lt;/code&gt; as well as &lt;code&gt;prime-rl&lt;/code&gt; support the full set of &lt;a href="https://docs.vllm.ai/en/v0.6.0/dev/sampling_params.html"&gt;SamplingParams&lt;/a&gt; exposed by vLLM (via their OpenAI-compatible &lt;a href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html"&gt;server&lt;/a&gt; interface), and leveraging this will often be the appropriate way to implement rollout strategies requiring finer-grained control, such as interrupting and resuming generations for interleaved tool use, or enforcing reasoning budgets.&lt;/p&gt; 
&lt;p&gt;The primary constraint we impose on rollout logic is that token sequences must be &lt;em&gt;increasing&lt;/em&gt;, i.e. once a token has been added to a model's context in a rollout, it must remain as the rollout progresses. Note that this causes issues with some popular reasoning models such as the Qwen3 and DeepSeek-R1-Distill series; see &lt;a href="https://raw.githubusercontent.com/willccbb/verifiers/main/#footguns"&gt;Footguns&lt;/a&gt; for guidance on adapting these models to support multi-turn rollouts.&lt;/p&gt; 
&lt;h3&gt;SingleTurnEnv&lt;/h3&gt; 
&lt;p&gt;For tasks requiring only a single response from a model for each prompt, you can use &lt;code&gt;SingleTurnEnv&lt;/code&gt; directly by specifying a Dataset and a Rubric. Rubrics are sets of reward functions, which can be either sync or async.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from datasets import load_dataset
import verifiers as vf

dataset = load_dataset("my-account/my-dataset", split="train")

def reward_A(prompt, completion, info) -&amp;gt; float:
	# reward fn, e.g. correctness
	...

def reward_B(parser, completion) -&amp;gt; float:
	# auxiliary reward fn, e.g. format
	...

async def metric(completion) -&amp;gt; float:
	# non-reward metric, e.g. proper noun count
	...

rubric = vf.Rubric(funcs=[reward_A, reward_B, metric], weights=[1.0, 0.5, 0.0])

vf_env = SingleTurnEnv(
	dataset=dataset,
	rubric=rubric
)
results = vf_env.evaluate(client=OpenAI(), model="gpt-4.1-mini", num_examples=100, rollouts_per_example=1)
vf_env.make_dataset(results) # HF dataset format
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Datasets should be formatted with columns for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;'prompt' (List[ChatMessage])&lt;/code&gt; OR &lt;code&gt;'question' (str)&lt;/code&gt; fields 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ChatMessage&lt;/code&gt; = e.g. &lt;code&gt;{'role': 'user', 'content': '...'}&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;if &lt;code&gt;question&lt;/code&gt; is set instead of &lt;code&gt;prompt&lt;/code&gt;, you can also pass &lt;code&gt;system_prompt (str)&lt;/code&gt; and/or &lt;code&gt;few_shot (List[ChatMessage])&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer (str)&lt;/code&gt; AND/OR &lt;code&gt;info (dict)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task (str)&lt;/code&gt;: optional, used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt; for orchestrating composition of Environments and Rubrics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following named attributes available for use by reward functions in your Rubric:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: sequence of input messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;completion&lt;/code&gt;: sequence of messages generated during rollout by model and Environment&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;answer&lt;/code&gt;: primary answer column, optional if &lt;code&gt;info&lt;/code&gt; is used&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;state&lt;/code&gt;: can be modified during rollout to accumulate any metadata (&lt;code&gt;state['responses']&lt;/code&gt; includes full OpenAI response objects by default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;info&lt;/code&gt;: auxiliary info needed for reward computation (e.g. test cases), optional if &lt;code&gt;answer&lt;/code&gt; is used&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;task&lt;/code&gt;: tag for task type (used by &lt;code&gt;EnvGroup&lt;/code&gt; and &lt;code&gt;RubricGroup&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;parser&lt;/code&gt;: the parser object declared. Note: &lt;code&gt;vf.Parser().get_format_reward_func()&lt;/code&gt; is a no-op (always 1.0); use &lt;code&gt;vf.ThinkParser&lt;/code&gt; or a custom parser if you want a real format adherence reward.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For tasks involving LLM judges, you may wish to use &lt;code&gt;vf.JudgeRubric()&lt;/code&gt; for managing requests to auxiliary models.&lt;/p&gt; 
&lt;p&gt;Note on concurrency: environment APIs accept &lt;code&gt;max_concurrent&lt;/code&gt; to control parallel rollouts. The &lt;code&gt;vf-eval&lt;/code&gt; CLI currently exposes &lt;code&gt;--max-concurrent-requests&lt;/code&gt;; ensure this maps to your environment’s concurrency as expected.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;vf-eval&lt;/code&gt; also supports specifying &lt;code&gt;sampling_args&lt;/code&gt; as a JSON object, which is sent to the vLLM inference engine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vf-eval vf-environment-name --sampling-args '{"reasoning_effort": "low"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;vf-eval -s&lt;/code&gt; to save outputs as dataset-formatted JSON, and view all locally-saved eval results with &lt;code&gt;vf-tui&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;ToolEnv&lt;/h3&gt; 
&lt;p&gt;For many applications involving tool use, you can use &lt;code&gt;ToolEnv&lt;/code&gt; to leverage models' native tool/function-calling capabilities in an agentic loop. Tools can be specified as generic Python functions (with type hints and docstrings), which will then be passed in JSON schema form to each inference request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import verifiers as vf
vf_env = vf.ToolEnv(
	dataset= ... # HF Dataset with 'prompt'/'question' + 'answer'/'info' columns
	rubric= ... # Rubric object; vf.ToolRubric() can be optionally used for counting tool invocations in each rollout
	tools=[search_tool, read_article_tool, python_tool], # python functions with type hints + docstrings
	max_turns=10
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In cases where your tools require heavy computational resources, we recommend hosting your tools as standalone servers (e.g. MCP servers) and creating lightweight wrapper functions to pass to &lt;code&gt;ToolEnv&lt;/code&gt;. Parallel tool call support is enabled by default.&lt;/p&gt; 
&lt;p&gt;For training, or self-hosted endpoints, you'll want to enable auto tool choice in &lt;a href="https://docs.vllm.ai/en/stable/features/tool_calling.html#automatic-function-calling"&gt;vLLM&lt;/a&gt; with the appropriate parser. If your model does not support native tool calling, you may find the &lt;code&gt;XMLParser&lt;/code&gt; abstraction useful for rolling your own tool call parsing on top of &lt;code&gt;MultiTurnEnv&lt;/code&gt;; see &lt;code&gt;environments/xml_tool_env&lt;/code&gt; for an example.&lt;/p&gt; 
&lt;h3&gt;MultiTurnEnv&lt;/h3&gt; 
&lt;p&gt;Both &lt;code&gt;SingleTurnEnv&lt;/code&gt; and &lt;code&gt;ToolEnv&lt;/code&gt; are instances of &lt;code&gt;MultiTurnEnv&lt;/code&gt;, which exposes an interface for writing custom Environment interaction protocols. The two methods you must override are&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from typing import Tuple
import verifiers as vf
from verifiers.types import Messages, State
class YourMultiTurnEnv(vf.MultiTurnEnv):
    def __init__(self,
                 dataset: Dataset,
                 rubric: Rubric,
				 max_turns: int,
                 **kwargs):
	
  async def is_completed(self, messages: Messages, state: State, **kwargs) -&amp;gt; bool:
    # return whether or not a rollout is completed

  async def env_response(self, messages: Messages, state: State, **kwargs) -&amp;gt; Tuple[Messages, State]:
    # return new environment message(s) + updated state
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your application requires more fine-grained control than is allowed by &lt;code&gt;MultiTurnEnv&lt;/code&gt;, you may want to inherit from the base &lt;code&gt;Environment&lt;/code&gt; functionality directly and override the &lt;code&gt;rollout&lt;/code&gt; method.&lt;/p&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;h3&gt;GRPOTrainer&lt;/h3&gt; 
&lt;p&gt;The included trainer (&lt;code&gt;vf.GRPOTrainer&lt;/code&gt;) supports running GRPO-style RL training via Accelerate/DeepSpeed, and uses vLLM for inference. It supports both full-parameter finetuning, and is optimized for efficiently training dense transformer models on 2-16 GPUs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install environment
vf-install vf-wordle (-p /path/to/environments | --from-repo)

# quick eval
vf-eval vf-wordle -m (model_name in configs/endpoints.py) -n NUM_EXAMPLES -r ROLLOUTS_PER_EXAMPLE

# inference (shell 0)
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 vf-vllm --model willcb/Qwen3-1.7B-Wordle \
    --data-parallel-size 7 --enforce-eager --disable-log-requests

# training (shell 1)
CUDA_VISIBLE_DEVICES=6,7 accelerate launch --num-processes 2 \
    --config-file configs/zero3.yaml examples/grpo/train_wordle.py --size 1.7B
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can train environments with the external &lt;code&gt;prime-rl&lt;/code&gt; project (FSDP-first orchestration). See the &lt;code&gt;prime-rl&lt;/code&gt; README for installation and examples. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# orchestrator config (prime-rl)
[environment]
id = "vf-math-python"  # or your environment ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# run (prime-rl)
uv run rl \
  --trainer @ configs/your_exp/train.toml \
  --orchestrator @ configs/your_exp/orch.toml \
  --inference @ configs/your_exp/infer.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ensure your &lt;code&gt;wandb&lt;/code&gt; and &lt;code&gt;huggingface-cli&lt;/code&gt; logins are set up (or set &lt;code&gt;report_to=None&lt;/code&gt; in &lt;code&gt;training_args&lt;/code&gt;). You should also have something set as your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in your environment (can be a dummy key for vLLM).&lt;/li&gt; 
 &lt;li&gt;If using high max concurrency, increase the number of allowed open sockets (e.g. &lt;code&gt;ulimit -n 4096&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;On some setups, inter-GPU communication can &lt;a href="https://github.com/huggingface/trl/issues/2923"&gt;hang&lt;/a&gt; or crash during vLLM weight syncing. This can usually be alleviated by setting (or unsetting) &lt;code&gt;NCCL_P2P_DISABLE=1&lt;/code&gt; in your environment (or potentially &lt;code&gt;NCCL_CUMEM_ENABLE=1&lt;/code&gt;). Try this as your first step if you experience NCCL-related issues.&lt;/li&gt; 
 &lt;li&gt;If problems persist, please open an &lt;a href="https://github.com/willccbb/verifiers/issues"&gt;issue&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resource Requirements&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;GRPOTrainer&lt;/code&gt; is optimized for setups with at least 2 GPUs, scaling up to multiple nodes. 2-GPU setups with sufficient memory to enable small-scale experimentation can be &lt;a href="https://app.primeintellect.ai/dashboard/create-cluster?image=ubuntu_22_cuda_12"&gt;rented&lt;/a&gt; for &amp;lt;$1/hr.&lt;/p&gt; 
&lt;h3&gt;PRIME-RL&lt;/h3&gt; 
&lt;p&gt;If you do not require LoRA support, you may want to use the &lt;code&gt;prime-rl&lt;/code&gt; trainer, which natively supports Environments created using &lt;code&gt;verifiers&lt;/code&gt;, is more optimized for performance and scalability via FSDP, includes a broader set of configuration options and user experience features, and has more battle-tested defaults. Both trainers support asynchronous rollouts, and use a one-step off-policy delay by default for overlapping training and inference. See the &lt;code&gt;prime-rl&lt;/code&gt; &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;docs&lt;/a&gt; for usage instructions.&lt;/p&gt; 
&lt;h2&gt;Further Documentation&lt;/h2&gt; 
&lt;p&gt;See the full &lt;a href="https://verifiers.readthedocs.io/en/latest/"&gt;docs&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Verifiers warmly welcomes community contributions! Please open an issue or PR if you encounter bugs or other pain points during your development, or start a discussion for more open-ended questions.&lt;/p&gt; 
&lt;p&gt;Please note that the core &lt;code&gt;verifiers/&lt;/code&gt; library is intended to be a relatively lightweight set of reusable components rather than an exhaustive catalog of RL environments. For &lt;em&gt;applications&lt;/em&gt; of &lt;code&gt;verifiers&lt;/code&gt; (e.g. "an Environment for XYZ task"), you are welcome to submit a PR for a self-contained module that lives within &lt;code&gt;environments/&lt;/code&gt; if it serves as a canonical example of a new pattern. Stay tuned for more info shortly about our plans for supporting community Environment contributions 🙂&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use this code in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{brown_verifiers_2025,
  author       = {William&amp;nbsp;Brown},
  title        = {{Verifiers}: Reinforcement Learning with LLMs in Verifiable Environments},
  howpublished = {\url{https://github.com/willccbb/verifiers}},
  note         = {Commit abcdefg • accessed DD Mon YYYY},
  year         = {2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A community Environments hub for crowdsourcing, sharing, and discovering new RL environments built with &lt;code&gt;verifiers&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Default patterns for hosted resources such as code sandboxes, auxiliary models, and MCP servers&lt;/li&gt; 
 &lt;li&gt;Multimodal input support&lt;/li&gt; 
 &lt;li&gt;Non-increasing token sequences via REINFORCE&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>asgeirtj/system_prompts_leaks</title>
      <link>https://github.com/asgeirtj/system_prompts_leaks</link>
      <description>&lt;p&gt;Collection of extracted System Prompts from popular chatbots like ChatGPT, Claude &amp; Gemini&lt;/p&gt;&lt;hr&gt;&lt;p&gt;NEW: 23 Aug 2025 &lt;a href="https://github.com/asgeirtj/system_prompts_leaks/raw/main/OpenAI/gpt-5-thinking.md"&gt;OpenAI/gpt-5-thinking.md &lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;System Prompts Leaks&lt;/h1&gt; 
&lt;p&gt;Collection of system message instructions for various publicly deployed chatbots.&lt;/p&gt; 
&lt;p&gt;Feel free to do PR's.&lt;/p&gt; 
&lt;p&gt;Please use discussions tabs for discussions not the Issues tab.&lt;/p&gt; 
&lt;p&gt;Discord username: asgeirtj&lt;br /&gt; X profile: &lt;a href="https://x.com/asgeirtj"&gt;https://x.com/asgeirtj&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#asgeirtj/system_prompts_leaks&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=asgeirtj/system_prompts_leaks&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>googleapis/genai-toolbox</title>
      <link>https://github.com/googleapis/genai-toolbox</link>
      <description>&lt;p&gt;MCP Toolbox for Databases is an open source MCP server for databases.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/logo.png" alt="logo" /&gt;&lt;/p&gt; 
&lt;h1&gt;MCP Toolbox for Databases&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://googleapis.github.io/genai-toolbox/"&gt;&lt;img src="https://img.shields.io/badge/docs-MCP_Toolbox-blue" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Dmm69peqjh"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://medium.com/@mcp_toolbox"&gt;&lt;img src="https://img.shields.io/badge/Medium-12100E?style=flat&amp;amp;logo=medium&amp;amp;logoColor=white" alt="Medium" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/googleapis/genai-toolbox"&gt;&lt;img src="https://goreportcard.com/badge/github.com/googleapis/genai-toolbox" alt="Go Report Card" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] MCP Toolbox for Databases is currently in beta, and may see breaking changes until the first stable release (v1.0).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MCP Toolbox for Databases is an open source MCP server for databases. It enables you to develop tools easier, faster, and more securely by handling the complexities such as connection pooling, authentication, and more.&lt;/p&gt; 
&lt;p&gt;This README provides a brief overview. For comprehensive details, see the &lt;a href="https://googleapis.github.io/genai-toolbox/"&gt;full documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This solution was originally named “Gen AI Toolbox for Databases” as its initial development predated MCP, but was renamed to align with recently added MCP compatibility.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!-- TOC ignore:true --&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;!-- TOC --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#why-toolbox"&gt;Why Toolbox?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#general-architecture"&gt;General Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#getting-started"&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#installing-the-server"&gt;Installing the server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#running-the-server"&gt;Running the server&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#homebrew-users"&gt;Homebrew Users&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#integrating-your-application"&gt;Integrating your application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#configuration"&gt;Configuration&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#sources"&gt;Sources&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#toolsets"&gt;Toolsets&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#versioning"&gt;Versioning&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#pre-100-versioning"&gt;Pre-1.0.0 Versioning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#post-100-versioning"&gt;Post-1.0.0 Versioning&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#community"&gt;Community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- /TOC --&gt; 
&lt;h2&gt;Why Toolbox?&lt;/h2&gt; 
&lt;p&gt;Toolbox helps you build Gen AI tools that let your agents access data in your database. Toolbox provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Simplified development&lt;/strong&gt;: Integrate tools to your agent in less than 10 lines of code, reuse tools between multiple agents or frameworks, and deploy new versions of tools more easily.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better performance&lt;/strong&gt;: Best practices such as connection pooling, authentication, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced security&lt;/strong&gt;: Integrated auth for more secure access to your data&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end observability&lt;/strong&gt;: Out of the box metrics and tracing with built-in support for OpenTelemetry.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;⚡ Supercharge Your Workflow with an AI Database Assistant ⚡&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Stop context-switching and let your AI assistant become a true co-developer. By &lt;a href="https://googleapis.github.io/genai-toolbox/how-to/connect-ide/"&gt;connecting your IDE to your databases with MCP Toolbox&lt;/a&gt;, you can delegate complex and time-consuming database tasks, allowing you to build faster and focus on what matters. This isn't just about code completion; it's about giving your AI the context it needs to handle the entire development lifecycle.&lt;/p&gt; 
&lt;p&gt;Here’s how it will save you time:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Query in Plain English&lt;/strong&gt;: Interact with your data using natural language right from your IDE. Ask complex questions like, &lt;em&gt;"How many orders were delivered in 2024, and what items were in them?"&lt;/em&gt; without writing any SQL.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automate Database Management&lt;/strong&gt;: Simply describe your data needs, and let the AI assistant manage your database for you. It can handle generating queries, creating tables, adding indexes, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generate Context-Aware Code&lt;/strong&gt;: Empower your AI assistant to generate application code and tests with a deep understanding of your real-time database schema. This accelerates the development cycle by ensuring the generated code is directly usable.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Slash Development Overhead&lt;/strong&gt;: Radically reduce the time spent on manual setup and boilerplate. MCP Toolbox helps streamline lengthy database configurations, repetitive code, and error-prone schema migrations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn &lt;a href="https://googleapis.github.io/genai-toolbox/how-to/connect-ide/"&gt;how to connect your AI tools (IDEs) to Toolbox using MCP&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;General Architecture&lt;/h2&gt; 
&lt;p&gt;Toolbox sits between your application's orchestration framework and your database, providing a control plane that is used to modify, distribute, or invoke tools. It simplifies the management of your tools by providing you with a centralized location to store and update tools, allowing you to share tools between agents and applications and update those tools without necessarily redeploying your application.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/docs/en/getting-started/introduction/architecture.png" alt="architecture" /&gt;&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installing the server&lt;/h3&gt; 
&lt;p&gt;For the latest version, check the &lt;a href="https://github.com/googleapis/genai-toolbox/releases"&gt;releases page&lt;/a&gt; and use the following instructions for your OS and CPU architecture.&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Binary&lt;/summary&gt; 
 &lt;p&gt;To install Toolbox as a binary:&lt;/p&gt; 
 &lt;!-- {x-release-please-start-version} --&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# see releases page for other versions
export VERSION=0.13.0
curl -O https://storage.googleapis.com/genai-toolbox/v$VERSION/linux/amd64/toolbox
chmod +x toolbox
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Container image&lt;/summary&gt; You can also install Toolbox as a container: 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# see releases page for other versions
export VERSION=0.13.0
docker pull us-central1-docker.pkg.dev/database-toolbox/toolbox/toolbox:$VERSION
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Homebrew&lt;/summary&gt; 
 &lt;p&gt;To install Toolbox using Homebrew on macOS or Linux:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;brew install mcp-toolbox
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Compile from source&lt;/summary&gt; 
 &lt;p&gt;To install from source, ensure you have the latest version of &lt;a href="https://go.dev/doc/install"&gt;Go installed&lt;/a&gt;, and then run the following command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;go install github.com/googleapis/genai-toolbox@v0.13.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;!-- {x-release-please-end} --&gt; 
&lt;/details&gt; 
&lt;h3&gt;Running the server&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/#configuration"&gt;Configure&lt;/a&gt; a &lt;code&gt;tools.yaml&lt;/code&gt; to define your tools, and then execute &lt;code&gt;toolbox&lt;/code&gt; to start the server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;./toolbox --tools-file "tools.yaml"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Toolbox enables dynamic reloading by default. To disable, use the &lt;code&gt;--disable-reload&lt;/code&gt; flag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Homebrew Users&lt;/h4&gt; 
&lt;p&gt;If you installed Toolbox using Homebrew, the &lt;code&gt;toolbox&lt;/code&gt; binary is available in your system path. You can start the server with the same command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;toolbox --tools-file "tools.yaml"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use &lt;code&gt;toolbox help&lt;/code&gt; for a full list of flags! To stop the server, send a terminate signal (&lt;code&gt;ctrl+c&lt;/code&gt; on most platforms).&lt;/p&gt; 
&lt;p&gt;For more detailed documentation on deploying to different environments, check out the resources in the &lt;a href="https://googleapis.github.io/genai-toolbox/how-to/"&gt;How-to section&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Integrating your application&lt;/h3&gt; 
&lt;p&gt;Once your server is up and running, you can load the tools into your application. See below the list of Client SDKs for using various frameworks:&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Python (&lt;a href="https://github.com/googleapis/mcp-toolbox-sdk-python"&gt;Github&lt;/a&gt;)&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Core&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://pypi.org/project/toolbox-core/"&gt;Toolbox Core SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install toolbox-core
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from toolbox_core import ToolboxClient

# update the url to point to your server
async with ToolboxClient("http://127.0.0.1:5000") as client:

    # these tools can be passed to your application!
    tools = await client.load_toolset("toolset_name")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;For more detailed instructions on using the Toolbox Core SDK, see the &lt;a href="https://github.com/googleapis/mcp-toolbox-sdk-python/tree/main/packages/toolbox-core/README.md"&gt;project's README&lt;/a&gt;.&lt;/p&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;LangChain / LangGraph&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://pypi.org/project/toolbox-langchain/"&gt;Toolbox LangChain SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install toolbox-langchain
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from toolbox_langchain import ToolboxClient

# update the url to point to your server
async with ToolboxClient("http://127.0.0.1:5000") as client:

    # these tools can be passed to your application!
    tools = client.load_toolset()
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For more detailed instructions on using the Toolbox LangChain SDK, see the &lt;a href="https://github.com/googleapis/mcp-toolbox-sdk-python/raw/main/packages/toolbox-langchain/README.md"&gt;project's README&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;LlamaIndex&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://pypi.org/project/toolbox-llamaindex/"&gt;Toolbox Llamaindex SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install toolbox-llamaindex
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from toolbox_llamaindex import ToolboxClient

# update the url to point to your server
async with ToolboxClient("http://127.0.0.1:5000") as client:

    # these tools can be passed to your application!
    tools = client.load_toolset()
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For more detailed instructions on using the Toolbox Llamaindex SDK, see the &lt;a href="https://github.com/googleapis/genai-toolbox-llamaindex-python/raw/main/README.md"&gt;project's README&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
 &lt;/blockquote&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Javascript/Typescript (&lt;a href="https://github.com/googleapis/mcp-toolbox-sdk-js"&gt;Github&lt;/a&gt;)&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Core&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://www.npmjs.com/package/@toolbox-sdk/core"&gt;Toolbox Core SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm install @toolbox-sdk/core
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-javascript"&gt;import { ToolboxClient } from '@toolbox-sdk/core';

// update the url to point to your server
const URL = 'http://127.0.0.1:5000';
let client = new ToolboxClient(URL);

// these tools can be passed to your application!
const tools = await client.loadToolset('toolsetName');
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For more detailed instructions on using the Toolbox Core SDK, see the &lt;a href="https://github.com/googleapis/mcp-toolbox-sdk-js/raw/main/packages/toolbox-core/README.md"&gt;project's README&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;LangChain / LangGraph&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://www.npmjs.com/package/@toolbox-sdk/core"&gt;Toolbox Core SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm install @toolbox-sdk/core
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-javascript"&gt;import { ToolboxClient } from '@toolbox-sdk/core';

// update the url to point to your server
const URL = 'http://127.0.0.1:5000';
let client = new ToolboxClient(URL);

// these tools can be passed to your application!
const toolboxTools = await client.loadToolset('toolsetName');

// Define the basics of the tool: name, description, schema and core logic
const getTool = (toolboxTool) =&amp;gt; tool(currTool, {
    name: toolboxTool.getName(),
    description: toolboxTool.getDescription(),
    schema: toolboxTool.getParamSchema()
});

// Use these tools in your Langchain/Langraph applications
const tools = toolboxTools.map(getTool);
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Genkit&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://www.npmjs.com/package/@toolbox-sdk/core"&gt;Toolbox Core SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm install @toolbox-sdk/core
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-javascript"&gt;import { ToolboxClient } from '@toolbox-sdk/core';
import { genkit } from 'genkit';

// Initialise genkit
const ai = genkit({
    plugins: [
        googleAI({
            apiKey: process.env.GEMINI_API_KEY || process.env.GOOGLE_API_KEY
        })
    ],
    model: googleAI.model('gemini-2.0-flash'),
});

// update the url to point to your server
const URL = 'http://127.0.0.1:5000';
let client = new ToolboxClient(URL);

// these tools can be passed to your application!
const toolboxTools = await client.loadToolset('toolsetName');

// Define the basics of the tool: name, description, schema and core logic
const getTool = (toolboxTool) =&amp;gt; ai.defineTool({
    name: toolboxTool.getName(),
    description: toolboxTool.getDescription(),
    schema: toolboxTool.getParamSchema()
}, toolboxTool)

// Use these tools in your Genkit applications
const tools = toolboxTools.map(getTool);
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
 &lt;/blockquote&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Go (&lt;a href="https://github.com/googleapis/mcp-toolbox-sdk-go"&gt;Github&lt;/a&gt;)&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Core&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://pkg.go.dev/github.com/googleapis/mcp-toolbox-sdk-go/core"&gt;Toolbox Go SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;go get github.com/googleapis/mcp-toolbox-sdk-go
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-go"&gt;package main

import (
  "github.com/googleapis/mcp-toolbox-sdk-go/core"
  "context"
)

func main() {
  // Make sure to add the error checks
  // update the url to point to your server
  URL := "http://127.0.0.1:5000";
  ctx := context.Background()

  client, err := core.NewToolboxClient(URL)

  // Framework agnostic tools
  tools, err := client.LoadToolset("toolsetName", ctx)
}
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For more detailed instructions on using the Toolbox Go SDK, see the &lt;a href="https://github.com/googleapis/mcp-toolbox-sdk-go/raw/main/core/README.md"&gt;project's README&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;LangChain Go&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://pkg.go.dev/github.com/googleapis/mcp-toolbox-sdk-go/core"&gt;Toolbox Go SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;go get github.com/googleapis/mcp-toolbox-sdk-go
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-go"&gt;package main

import (
  "context"
  "encoding/json"

  "github.com/googleapis/mcp-toolbox-sdk-go/core"
  "github.com/tmc/langchaingo/llms"
)

func main() {
  // Make sure to add the error checks
  // update the url to point to your server
  URL := "http://127.0.0.1:5000"
  ctx := context.Background()

  client, err := core.NewToolboxClient(URL)

  // Framework agnostic tool
  tool, err := client.LoadTool("toolName", ctx)

  // Fetch the tool's input schema
  inputschema, err := tool.InputSchema()

  var paramsSchema map[string]any
  _ = json.Unmarshal(inputschema, &amp;amp;paramsSchema)

  // Use this tool with LangChainGo
  langChainTool := llms.Tool{
    Type: "function",
    Function: &amp;amp;llms.FunctionDefinition{
      Name:        tool.Name(),
      Description: tool.Description(),
      Parameters:  paramsSchema,
    },
  }
}

&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Genkit&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://pkg.go.dev/github.com/googleapis/mcp-toolbox-sdk-go/core"&gt;Toolbox Go SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;go get github.com/googleapis/mcp-toolbox-sdk-go
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-go"&gt;package main
import (
  "context"
  "encoding/json"

  "github.com/firebase/genkit/go/ai"
  "github.com/firebase/genkit/go/genkit"
  "github.com/googleapis/mcp-toolbox-sdk-go/core"
  "github.com/googleapis/mcp-toolbox-sdk-go/tbgenkit"
  "github.com/invopop/jsonschema"
)

func main() {
  // Make sure to add the error checks
  // Update the url to point to your server
  URL := "http://127.0.0.1:5000"
  ctx := context.Background()
  g, err := genkit.Init(ctx)

  client, err := core.NewToolboxClient(URL)

  // Framework agnostic tool
  tool, err := client.LoadTool("toolName", ctx)

  // Convert the tool using the tbgenkit package
  // Use this tool with Genkit Go
  genkitTool, err := tbgenkit.ToGenkitTool(tool, g)
  if err != nil {
    log.Fatalf("Failed to convert tool: %v\n", err)
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Go GenAI&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://pkg.go.dev/github.com/googleapis/mcp-toolbox-sdk-go/core"&gt;Toolbox Go SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;go get github.com/googleapis/mcp-toolbox-sdk-go
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-go"&gt;package main

import (
  "context"
  "encoding/json"

  "github.com/googleapis/mcp-toolbox-sdk-go/core"
  "google.golang.org/genai"
)

func main() {
  // Make sure to add the error checks
  // Update the url to point to your server
  URL := "http://127.0.0.1:5000"
  ctx := context.Background()

  client, err := core.NewToolboxClient(URL)

  // Framework agnostic tool
  tool, err := client.LoadTool("toolName", ctx)

  // Fetch the tool's input schema
  inputschema, err := tool.InputSchema()

  var schema *genai.Schema
  _ = json.Unmarshal(inputschema, &amp;amp;schema)

  funcDeclaration := &amp;amp;genai.FunctionDeclaration{
    Name:        tool.Name(),
    Description: tool.Description(),
    Parameters:  schema,
  }

  // Use this tool with Go GenAI
  genAITool := &amp;amp;genai.Tool{
    FunctionDeclarations: []*genai.FunctionDeclaration{funcDeclaration},
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt;OpenAI Go&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;Install &lt;a href="https://pkg.go.dev/github.com/googleapis/mcp-toolbox-sdk-go/core"&gt;Toolbox Go SDK&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;go get github.com/googleapis/mcp-toolbox-sdk-go
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Load tools:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-go"&gt;package main

import (
  "context"
  "encoding/json"

  "github.com/googleapis/mcp-toolbox-sdk-go/core"
  openai "github.com/openai/openai-go"
)

func main() {
  // Make sure to add the error checks
  // Update the url to point to your server
  URL := "http://127.0.0.1:5000"
  ctx := context.Background()

  client, err := core.NewToolboxClient(URL)

  // Framework agnostic tool
  tool, err := client.LoadTool("toolName", ctx)

  // Fetch the tool's input schema
  inputschema, err := tool.InputSchema()

  var paramsSchema openai.FunctionParameters
  _ = json.Unmarshal(inputschema, &amp;amp;paramsSchema)

  // Use this tool with OpenAI Go
  openAITool := openai.ChatCompletionToolParam{
    Function: openai.FunctionDefinitionParam{
      Name:        tool.Name(),
      Description: openai.String(tool.Description()),
      Parameters:  paramsSchema,
    },
  }

}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
 &lt;/blockquote&gt;
&lt;/details&gt;   
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;The primary way to configure Toolbox is through the &lt;code&gt;tools.yaml&lt;/code&gt; file. If you have multiple files, you can tell toolbox which to load with the &lt;code&gt;--tools-file tools.yaml&lt;/code&gt; flag.&lt;/p&gt; 
&lt;p&gt;You can find more detailed reference documentation to all resource types in the &lt;a href="https://googleapis.github.io/genai-toolbox/resources/"&gt;Resources&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Sources&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;sources&lt;/code&gt; section of your &lt;code&gt;tools.yaml&lt;/code&gt; defines what data sources your Toolbox should have access to. Most tools will have at least one source to execute against.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;sources:
  my-pg-source:
    kind: postgres
    host: 127.0.0.1
    port: 5432
    database: toolbox_db
    user: toolbox_user
    password: my-password
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more details on configuring different types of sources, see the &lt;a href="https://googleapis.github.io/genai-toolbox/resources/sources"&gt;Sources&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;tools&lt;/code&gt; section of a &lt;code&gt;tools.yaml&lt;/code&gt; define the actions an agent can take: what kind of tool it is, which source(s) it affects, what parameters it uses, etc.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;tools:
  search-hotels-by-name:
    kind: postgres-sql
    source: my-pg-source
    description: Search for hotels based on name.
    parameters:
      - name: name
        type: string
        description: The name of the hotel.
    statement: SELECT * FROM hotels WHERE name ILIKE '%' || $1 || '%';
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more details on configuring different types of tools, see the &lt;a href="https://googleapis.github.io/genai-toolbox/resources/tools"&gt;Tools&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Toolsets&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;toolsets&lt;/code&gt; section of your &lt;code&gt;tools.yaml&lt;/code&gt; allows you to define groups of tools that you want to be able to load together. This can be useful for defining different groups based on agent or application.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;toolsets:
    my_first_toolset:
        - my_first_tool
        - my_second_tool
    my_second_toolset:
        - my_second_tool
        - my_third_tool
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can load toolsets by name:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# This will load all tools
all_tools = client.load_toolset()

# This will only load the tools listed in 'my_second_toolset'
my_second_toolset = client.load_toolset("my_second_toolset")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Versioning&lt;/h2&gt; 
&lt;p&gt;This project uses &lt;a href="https://semver.org/"&gt;semantic versioning&lt;/a&gt; (&lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt;). Since the project is in a pre-release stage (version &lt;code&gt;0.x.y&lt;/code&gt;), we follow the standard conventions for initial development:&lt;/p&gt; 
&lt;h3&gt;Pre-1.0.0 Versioning&lt;/h3&gt; 
&lt;p&gt;While the major version is &lt;code&gt;0&lt;/code&gt;, the public API should be considered unstable. The version will be incremented as follows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;0.MINOR.PATCH&lt;/code&gt;&lt;/strong&gt;: The &lt;strong&gt;MINOR&lt;/strong&gt; version is incremented when we add new functionality or make breaking, incompatible API changes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;0.MINOR.PATCH&lt;/code&gt;&lt;/strong&gt;: The &lt;strong&gt;PATCH&lt;/strong&gt; version is incremented for backward-compatible bug fixes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Post-1.0.0 Versioning&lt;/h3&gt; 
&lt;p&gt;Once the project reaches a stable &lt;code&gt;1.0.0&lt;/code&gt; release, the versioning will follow the more common convention:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt;&lt;/strong&gt;: Incremented for incompatible API changes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt;&lt;/strong&gt;: Incremented for new, backward-compatible functionality.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt;&lt;/strong&gt;: Incremented for backward-compatible bug fixes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The public API that this applies to is the CLI associated with Toolbox, the interactions with official SDKs, and the definitions in the &lt;code&gt;tools.yaml&lt;/code&gt; file.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome. Please, see the &lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;Please note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms. See &lt;a href="https://raw.githubusercontent.com/googleapis/genai-toolbox/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/GQrFB3Ec3W"&gt;discord community&lt;/a&gt; to connect with our developers!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>eythaann/Seelen-UI</title>
      <link>https://github.com/eythaann/Seelen-UI</link>
      <description>&lt;p&gt;The Fully Customizable Desktop Environment for Windows 10/11.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/logo.svg?sanitize=true" width="44" align="top" alt="Seelen UI Logo" /&gt; Seelen UI &lt;/h1&gt; 
&lt;h2 align="center"&gt; Fully Customizable Desktop Environment for Windows &lt;br /&gt; Available in 70+ Languages &lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/eythaann/seelen-ui/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/eythaann/seelen-ui.svg?sanitize=true" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/eythaann/seelen-ui/commits/main"&gt;&lt;img src="https://img.shields.io/github/last-commit/eythaann/seelen-ui.svg?sanitize=true" alt="Last Commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/eythaann/seelen-ui/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/eythaann/seelen-ui.svg?sanitize=true" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/eythaann/seelen-ui/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/eythaann/seelen-ui/total.svg?sanitize=true" alt="Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/preview.png" width="100%" alt="Screenshot of Seelen UI desktop showing a customized desktop environment" /&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center" width="33%"&gt; &lt;a href="https://apps.microsoft.com/detail/Seelen%20UI/9p67c2d4t9fb?mode=full" target="_blank" rel="noopener noreferrer" aria-label="Download Seelen UI from Microsoft Store"&gt; &lt;img src="https://get.microsoft.com/images/en-us%20dark.svg?sanitize=true" width="100%" alt="Download Seelen UI from Microsoft Store" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; &lt;a href="https://discord.gg/ABfASx5ZAJ" target="_blank" rel="noopener noreferrer" aria-label="Join the Seelen UI Discord community"&gt; &lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/discord-alt.png" width="100%" alt="Join the Seelen UI Discord community" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; &lt;a href="https://www.digitalocean.com/?refcode=955c7335abf5&amp;amp;utm_campaign=Referral_Invite&amp;amp;utm_medium=Referral_Program&amp;amp;utm_source=badge" target="_blank" rel="noopener noreferrer" aria-label="DigitalOcean Referral Badge"&gt; &lt;img src="https://web-platforms.sfo2.cdn.digitaloceanspaces.com/WWW/Badge%201.svg?sanitize=true" width="100%" alt="DigitalOcean Referral Badge" /&gt; &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://seelen.io/apps/seelen-ui"&gt;Seelen UI&lt;/a&gt; is a tool designed to enhance your Windows desktop experience with a focus on customization and productivity. It integrates smoothly into your system, providing a range of features that allow you to personalize your desktop and optimize your workflow.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Be Creative&lt;/strong&gt;: Seelen UI lets you tailor your desktop to fit your style and needs. You can adjust menus, widgets, icons, and other elements to create a personalized and visually appealing desktop environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/theme_preview.png" alt="Seelen UI Custom Theme" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhance Your Productivity&lt;/strong&gt;: Seelen UI helps you organize your desktop efficiently. With a Tiling Windows Manager, windows automatically arrange themselves to support multitasking, making your work more streamlined.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/twm_preview.png" alt="Seelen UI Tiling Window Manager" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enjoy your music&lt;/strong&gt;: With an integrated media module that's compatible with most music players, Seelen UI allows you to enjoy your music seamlessly. You can pause, resume, and skip tracks at any time without the need to open additional windows.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/media_module_preview.png" alt="Seelen UI Media Module" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Be faster!&lt;/strong&gt;: With an app launcher inspired by Rofi, Seelen UI provides a simple and intuitive way to quickly access your applications and execute commands.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/app_launcher_preview.png" alt="Seelen UI App Launcher" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;User-Friendly Configuration&lt;/strong&gt;: Seelen UI offers an intuitive interface for easy customization. Adjust settings such as themes, taskbar layouts, icons, etc. With just a few clicks.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/images/settings_preview.png" alt="Seelen UI Settings" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] Seelen UI requires the WebView runtime to be installed. On Windows 11, it comes pre-installed with the system. However, on Windows 10, the WebView runtime is included with the &lt;code&gt;setup.exe&lt;/code&gt; installer. Additionally, Microsoft Edge is necessary to function correctly. Some users may have modified their system and removed Edge, so please ensure both Edge and the WebView runtime are installed on your system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] On fresh installations of Windows, the app might show a white or dark screen. You only need to update your Windows through Windows Update and restart your PC.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can choose from different installation options based on your preference:&lt;/p&gt; 
&lt;h3&gt;Microsoft Store &lt;em&gt;(recommended)&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Download the latest version from the &lt;a href="https://www.microsoft.com/store/productId/9P67C2D4T9FB?ocid=pdpshare"&gt;Store&lt;/a&gt; page. This is the recommended option because you will receive updates and a secure version of the program.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/em&gt;: It may take around 1 to 3 business days for changes to be reflected in the Microsoft Store, as updates are approved by real people in the store.&lt;/p&gt; 
&lt;h3&gt;Winget&lt;/h3&gt; 
&lt;p&gt;Install the latest version using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-pwsh"&gt;winget install --id Seelen.SeelenUI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This option also uses the signed &lt;code&gt;.msix&lt;/code&gt; package and ensures you have the latest secure version. Similar to the Microsoft Store, it may take around 1 to 3 business days for changes to be reflected in Winget, as updates are approved by real people in the &lt;code&gt;winget-pkg&lt;/code&gt; project.&lt;/p&gt; 
&lt;h3&gt;.msix Installer&lt;/h3&gt; 
&lt;p&gt;Download the &lt;code&gt;.msix&lt;/code&gt; installer from the &lt;a href="https://github.com/eythaann/seelen-ui/releases"&gt;Releases&lt;/a&gt; page. This package is signed, ensuring a secure installation. This is the same option as the Microsoft Store but is a portable installer.&lt;/p&gt; 
&lt;h3&gt;.exe Installer&lt;/h3&gt; 
&lt;p&gt;Download the latest version from the &lt;a href="https://github.com/eythaann/seelen-ui/releases"&gt;Releases&lt;/a&gt; page and run the &lt;code&gt;setup.exe&lt;/code&gt; installer. This option is less recommended as the installer is not signed, which may cause it to be flagged as a potential threat by some antivirus programs. The &lt;code&gt;setup.exe&lt;/code&gt; is updated more quickly than the Microsoft Store or Winget versions and also it receives notifications updates on new release.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Once installed or extracted, simply open the program. The easy-to-use and intuitive GUI will guide you through the configuration process. Customize your desktop environment effortlessly.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For in-depth details on various aspects of Seelen UI, explore the following documents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/languages.md"&gt;Languages&lt;/a&gt; - Information regarding translations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/toolbar.md"&gt;Toolbar&lt;/a&gt; - Details about customizing and using the toolbar.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://seelen.io/blog/seelen-ui-theme-tutorial"&gt;Themes&lt;/a&gt; - Guidance on creating and applying themes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/window_manager.md"&gt;Window Manager&lt;/a&gt; - Instructions on configuring the window manager.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/project.md"&gt;Project&lt;/a&gt; - General information about the project and its structure.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Upcoming Features&lt;/h2&gt; 
&lt;p&gt;I’m excited to share some upcoming features for Seelen UI! Here’s a glimpse of what’s planned for the future:&lt;/p&gt; 
&lt;h3&gt;&lt;del&gt;App Launcher&lt;/del&gt; ✅&lt;/h3&gt; 
&lt;p&gt;I’m planning to develop an app launcher inspired by &lt;a href="https://github.com/davatorium/rofi"&gt;Rofi&lt;/a&gt; on Linux. This feature will provide a sleek and highly customizable way to quickly access your applications.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/adi1090x/files/master/rofi/previews/colorful/main.gif" alt="App Launcher Preview" /&gt; &lt;em&gt;Image courtesy of &lt;a href="https://github.com/dctxmei/rofi-themes"&gt;rofi-themes&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Customizable Popup Widgets&lt;/h3&gt; 
&lt;p&gt;I aim to introduce a set of fully customizable popup widgets, similar to the features available in &lt;a href="https://github.com/elkowar/eww"&gt;EWW&lt;/a&gt;. These widgets will be highly configurable and adaptable to your needs, providing an enhanced and interactive way to manage your desktop environment.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/adi1090x/widgets/main/previews/dashboard.png" alt="Customizable Widgets Preview" /&gt; &lt;em&gt;Image courtesy of &lt;a href="https://github.com/adi1090x/widgets"&gt;adi1090x&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Custom Alt + Tab (Task Switching)&lt;/h3&gt; 
&lt;p&gt;An upgraded Alt + Tab system for task switching is on the horizon. This will offer a more visually appealing and functional experience, allowing for smoother transitions between open applications and windows.&lt;/p&gt; 
&lt;h3&gt;Custom Virtual Desktops Viewer and Animations&lt;/h3&gt; 
&lt;p&gt;I’m also working on a custom virtual desktops viewer and dynamic animations to improve navigation between different workspaces. This will provide a more intuitive and immersive multitasking experience.&lt;/p&gt; 
&lt;p&gt;Stay tuned for more updates as I develop these features. I appreciate your support and enthusiasm!&lt;/p&gt; 
&lt;p&gt;Happy customizing!&lt;/p&gt; 
&lt;p&gt;The Seelen UI Team&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read the &lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/CONTRIBUTING"&gt;Contribution Guidelines&lt;/a&gt; to get started with terms.&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/documentation/project.md"&gt;Project Documentation&lt;/a&gt; to understand the project structure and how to use it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/eythaann/Seelen-UI/master/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;For inquiries and support, please contact me on &lt;a href="https://discord.gg/ABfASx5ZAJ"&gt;Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;See you later&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;                   .      .&amp;amp;     _,x&amp;amp;"``
                    &amp;amp; .   &amp;amp;'  ;.&amp;amp;&amp;amp;'
              &amp;amp;.  . &amp;amp;.&amp;amp;     .0&amp;amp;&amp;amp;&amp;amp;;&amp;amp;""`
         .    '&amp;amp;  &amp;amp;.&amp;amp;&amp;amp;&amp;amp;  .&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;'
       .&amp;amp;         ;&amp;amp;&amp;amp;&amp;amp; &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;'
      &amp;amp;&amp;amp;          &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;     &amp;amp;&amp;amp;&amp;amp;
     0&amp;amp;    .     &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;""
    &amp;amp;&amp;amp;   .0     &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;
   0&amp;amp;&amp;amp; .&amp;amp;'     &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;
  :&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;    . &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp; 
  0&amp;amp;&amp;amp;&amp;amp;&amp;amp;    &amp;amp; &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;
  &amp;amp;&amp;amp;&amp;amp;&amp;amp;'   &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;               .&amp;amp;&amp;amp;&amp;amp;x&amp;amp;
  &amp;amp;&amp;amp;&amp;amp;&amp;amp;   :&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0.&amp;amp;'        , .&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;;.
  &amp;amp;&amp;amp;&amp;amp;&amp;amp;.  &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;        .&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;'               .
  0&amp;amp;&amp;amp;&amp;amp;&amp;amp;  &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;       ,&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;                &amp;amp;
  :&amp;amp;&amp;amp;&amp;amp;&amp;amp;; &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0       ,;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;             ;  .0
   0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0     ,;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;             &amp;amp;  &amp;amp;;
    0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0   :',;".&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;".&amp;amp;             &amp;amp;&amp;amp; &amp;amp;0
     0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0  ',;',&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;" ,&amp;amp;'             &amp;amp;&amp;amp;&amp;amp;&amp;amp;0
      0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0 ,x&amp;amp;&amp;amp;&amp;amp;&amp;amp;" .&amp;amp;&amp;amp;&amp;amp;              &amp;amp;&amp;amp;&amp;amp;&amp;amp;0
        0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp; .&amp;amp;&amp;amp;&amp;amp;&amp;amp;"'''"&amp;amp;&amp;amp;"&amp;amp;&amp;amp;            &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
         0&amp;amp;&amp;amp; .&amp;amp;&amp;amp;;``       `&amp;amp;: :&amp;amp;         &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
            &amp;amp;"' &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;   &amp;amp;"&amp;amp; &amp;amp;"&amp;amp;   &amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
              0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
                 0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0         Seelen
                      0&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;0
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;📌 &lt;strong&gt;Official Website&lt;/strong&gt;: &lt;a href="https://seelen.io"&gt;https://seelen.io&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Seelen Inc © 2025 - All rights reserved&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>