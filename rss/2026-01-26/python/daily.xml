<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 25 Jan 2026 01:39:44 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>kyutai-labs/moshi</title>
      <link>https://github.com/kyutai-labs/moshi</link>
      <description>&lt;p&gt;Moshi is a speech-text foundation model and full-duplex spoken dialogue framework. It uses Mimi, a state-of-the-art streaming neural audio codec.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Moshi: a speech-text foundation model for real time dialogue&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://github.com/kyutai-labs/moshi/workflows/precommit/badge.svg?sanitize=true" alt="precommit badge" /&gt; &lt;img src="https://github.com/kyutai-labs/moshi/workflows/Rust%20CI/badge.svg?sanitize=true" alt="rust ci badge" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2410.00037"&gt;[Read the paper]&lt;/a&gt; &lt;a href="https://moshi.chat"&gt;[Demo]&lt;/a&gt; &lt;a href="https://huggingface.co/collections/kyutai/moshi-v01-release-66eaeaf3302bef6bd9ad7acd"&gt;[Hugging Face]&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2410.00037"&gt;Moshi&lt;/a&gt; is a speech-text foundation model and &lt;strong&gt;full-duplex&lt;/strong&gt; spoken dialogue framework. It uses &lt;a href="https://arxiv.org/abs/2410.00037"&gt;Mimi&lt;/a&gt;, a state-of-the-art streaming neural audio codec. &lt;a href="https://moshi.chat"&gt;Talk to Moshi&lt;/a&gt; now in our live demo.&lt;/p&gt; 
&lt;h2&gt;Organisation of the repository&lt;/h2&gt; 
&lt;p&gt;There are three separate versions of the Moshi inference stack in this repo.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/kyutai-labs/moshi/main/#pytorch-implementation"&gt;PyTorch&lt;/a&gt;: for research and tinkering.&lt;/strong&gt; The code is in the &lt;a href="https://raw.githubusercontent.com/kyutai-labs/moshi/main/moshi/"&gt;&lt;code&gt;moshi/&lt;/code&gt;&lt;/a&gt; directory.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/kyutai-labs/moshi/main/#mlx-implementation-for-local-inference-on-macos"&gt;MLX&lt;/a&gt;: for on-device inference on iPhone and Mac.&lt;/strong&gt; The code is in the &lt;a href="https://raw.githubusercontent.com/kyutai-labs/moshi/main/moshi_mlx/"&gt;&lt;code&gt;moshi_mlx/&lt;/code&gt;&lt;/a&gt; directory.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/kyutai-labs/moshi/main/#rust-implementation"&gt;Rust&lt;/a&gt;: for production.&lt;/strong&gt; The code is in the &lt;a href="https://raw.githubusercontent.com/kyutai-labs/moshi/main/rust/"&gt;&lt;code&gt;rust/&lt;/code&gt;&lt;/a&gt; directory. This contains in particular a Mimi implementation in Rust, with Python bindings available as &lt;code&gt;rustymimi&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, the code for the web UI client used in the &lt;a href="https://moshi.chat"&gt;Moshi demo&lt;/a&gt; is provided in the &lt;a href="https://raw.githubusercontent.com/kyutai-labs/moshi/main/client/"&gt;&lt;code&gt;client/&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; 
&lt;p&gt;If you want to fine tune Moshi, head out to &lt;a href="https://github.com/kyutai-labs/moshi-finetune"&gt;kyutai-labs/moshi-finetune&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Other Kyutai models&lt;/h3&gt; 
&lt;p&gt;The Moshi codebase is also used to run related models from Kyutai that use a multi-stream architecture similar to Moshi:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hibiki: simultaneous speech translation.&lt;/strong&gt; Check out the &lt;a href="https://github.com/kyutai-labs/hibiki"&gt;Hibiki repo&lt;/a&gt; for more info.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Kyutai Text-To-Speech and Speech-To-Text.&lt;/strong&gt; Check out the &lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling"&gt;Delayed Streams Modeling repo&lt;/a&gt; for more info.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model architecture&lt;/h2&gt; 
&lt;p&gt;Moshi models &lt;strong&gt;two streams of audio&lt;/strong&gt;: one corresponds to Moshi speaking, and the other one to the user speaking. Along with these two audio streams, Moshi predicts text tokens corresponding to its own speech, its &lt;strong&gt;inner monologue&lt;/strong&gt;, which greatly improves the quality of its generation. A small Depth Transformer models inter-codebook dependencies for a given time step, while a large, 7B-parameter Temporal Transformer models the temporal dependencies. Moshi achieves a theoretical latency of 160ms (80ms for the frame size of Mimi + 80ms of acoustic delay), with a practical overall latency as low as 200ms on an L4 GPU.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/kyutai-labs/moshi/main/moshi.png" alt="Schema representing the structure of Moshi. Moshi models two streams of audio:
    one corresponds to Moshi, and the other one to the user. At inference, the audio stream of the user is taken from the audio input, and the audio stream for Moshi is sampled from the model's output. Along that, Moshi predicts text tokens corresponding to its own speech for improved accuracy. A small Depth Transformer models inter codebook dependencies for a given step." width="650px" /&gt;&lt;/p&gt; 
&lt;h3&gt;Mimi&lt;/h3&gt; 
&lt;p&gt;Mimi is a neural audio codec that processes 24 kHz audio, down to a 12.5 Hz representation with a bandwidth of 1.1 kbps, in a fully streaming manner (latency of 80ms, the frame size), yet performs better than existing, non-streaming, codecs like &lt;a href="https://github.com/ZhangXInFD/SpeechTokenizer"&gt;SpeechTokenizer&lt;/a&gt; (50 Hz, 4kbps), or &lt;a href="https://github.com/haoheliu/SemantiCodec-inference"&gt;SemantiCodec&lt;/a&gt; (50 Hz, 1.3kbps).&lt;/p&gt; 
&lt;p&gt;Mimi builds on previous neural audio codecs such as &lt;a href="https://arxiv.org/abs/2107.03312"&gt;SoundStream&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/encodec"&gt;EnCodec&lt;/a&gt;, adding a Transformer both in the encoder and decoder, and adapting the strides to match an overall frame rate of 12.5 Hz. This allows Mimi to get closer to the average frame rate of text tokens (~3-4 Hz), and limit the number of autoregressive steps in Moshi. Similarly to SpeechTokenizer, Mimi uses a distillation loss so that the first codebook tokens match a self-supervised representation from &lt;a href="https://arxiv.org/abs/2110.13900"&gt;WavLM&lt;/a&gt;, which allows modeling semantic and acoustic information with a single model. Finally, and similarly to &lt;a href="https://arxiv.org/pdf/2210.14090"&gt;EBEN&lt;/a&gt;, Mimi uses &lt;strong&gt;only an adversarial training loss&lt;/strong&gt;, along with feature matching, showing strong improvements in terms of subjective quality despite its low bitrate.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/kyutai-labs/moshi/main/mimi.png" alt="Schema representing the structure of Mimi, our proposed neural codec. Mimi contains a Transformer
in both its encoder and decoder, and achieves a frame rate closer to that of text tokens. This allows us to reduce
the number of auto-regressive steps taken by Moshi, thus reducing the latency of the model." width="800px" /&gt;&lt;/p&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;p&gt;We release three models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Moshi fine-tuned on a male synthetic voice (Moshiko),&lt;/li&gt; 
 &lt;li&gt;Moshi fine-tuned on a female synthetic voice (Moshika),&lt;/li&gt; 
 &lt;li&gt;Mimi, our speech codec.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Depending on the backend, the file format and quantization available will vary. Here is the list of the HuggingFace repo with each model. Mimi is bundled in each of those, and always use the same checkpoint format.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Moshika for PyTorch (bf16, int8): &lt;a href="https://huggingface.co/kyutai/moshika-pytorch-bf16"&gt;kyutai/moshika-pytorch-bf16&lt;/a&gt;, &lt;a href="https://huggingface.co/kyutai/moshika-pytorch-q8"&gt;kyutai/moshika-pytorch-q8&lt;/a&gt; (experimental).&lt;/li&gt; 
 &lt;li&gt;Moshiko for PyTorch (bf16, int8): &lt;a href="https://huggingface.co/kyutai/moshiko-pytorch-bf16"&gt;kyutai/moshiko-pytorch-bf16&lt;/a&gt;, &lt;a href="https://huggingface.co/kyutai/moshiko-pytorch-q8"&gt;kyutai/moshiko-pytorch-q8&lt;/a&gt; (experimental).&lt;/li&gt; 
 &lt;li&gt;Moshika for MLX (int4, int8, bf16): &lt;a href="https://huggingface.co/kyutai/moshika-mlx-q4"&gt;kyutai/moshika-mlx-q4&lt;/a&gt;, &lt;a href="https://huggingface.co/kyutai/moshika-mlx-q8"&gt;kyutai/moshika-mlx-q8&lt;/a&gt;, &lt;a href="https://huggingface.co/kyutai/moshika-mlx-bf16"&gt;kyutai/moshika-mlx-bf16&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Moshiko for MLX (int4, int8, bf16): &lt;a href="https://huggingface.co/kyutai/moshiko-mlx-q4"&gt;kyutai/moshiko-mlx-q4&lt;/a&gt;, &lt;a href="https://huggingface.co/kyutai/moshiko-mlx-q8"&gt;kyutai/moshiko-mlx-q8&lt;/a&gt;, &lt;a href="https://huggingface.co/kyutai/moshiko-mlx-bf16"&gt;kyutai/moshiko-mlx-bf16&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Moshika for Rust/Candle (int8, bf16): &lt;a href="https://huggingface.co/kyutai/moshika-candle-q8"&gt;kyutai/moshika-candle-q8&lt;/a&gt;, &lt;a href="https://huggingface.co/kyutai/moshika-candle-bf16"&gt;kyutai/moshika-mlx-bf16&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Moshiko for Rust/Candle (int8, bf16): &lt;a href="https://huggingface.co/kyutai/moshiko-candle-q8"&gt;kyutai/moshiko-candle-q8&lt;/a&gt;, &lt;a href="https://huggingface.co/kyutai/moshiko-candle-bf16"&gt;kyutai/moshiko-mlx-bf16&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All models are released under the CC-BY 4.0 license.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;You will need at least Python 3.10, with 3.12 recommended. For specific requirements, please check the individual backends directories. You can install the PyTorch and MLX clients with the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U moshi      # moshi PyTorch, from PyPI
pip install -U moshi_mlx  # moshi MLX, from PyPI, best with Python 3.12.
# Or the bleeding edge versions for Moshi and Moshi-MLX.
pip install -U -e "git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi&amp;amp;subdirectory=moshi"
pip install -U -e "git+https://git@github.com/kyutai-labs/moshi.git#egg=moshi_mlx&amp;amp;subdirectory=moshi_mlx"

pip install rustymimi  # mimi, rust implementation with Python bindings from PyPI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are not using Python 3.12, you might get an error when installing &lt;code&gt;moshi_mlx&lt;/code&gt; or &lt;code&gt;rustymimi&lt;/code&gt; (which &lt;code&gt;moshi_mlx&lt;/code&gt; depends on). Then, you will need to install the &lt;a href="https://rustup.rs/"&gt;Rust toolchain&lt;/a&gt;, or switch to Python 3.12.&lt;/p&gt; 
&lt;p&gt;While we hope that the present codebase will work on Windows, we do not provide official support for it. We have tested the MLX version on a MacBook Pro M3. At the moment, we do not support quantization for the PyTorch version, so you will need a GPU with a significant amount of memory (24GB).&lt;/p&gt; 
&lt;p&gt;For using the Rust backend, you will need a recent version of the &lt;a href="https://rustup.rs/"&gt;Rust toolchain&lt;/a&gt;. To compile GPU support, you will also need the &lt;a href="https://developer.nvidia.com/cuda-toolkit"&gt;CUDA&lt;/a&gt; properly installed for your GPU, in particular with &lt;code&gt;nvcc&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;PyTorch implementation&lt;/h2&gt; 
&lt;p&gt;The PyTorch based API can be found in the &lt;code&gt;moshi&lt;/code&gt; directory. It provides a streaming version of the audio tokenizer (mimi) and the language model (moshi).&lt;/p&gt; 
&lt;p&gt;In order to run in interactive mode, you need to start a server which will run the model, you can then use either the web UI or a command line client.&lt;/p&gt; 
&lt;p&gt;Start the server with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m moshi.server [--gradio-tunnel] [--hf-repo kyutai/moshika-pytorch-bf16]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then access the web UI on &lt;a href="http://localhost:8998"&gt;localhost:8998&lt;/a&gt;. If your GPU is on a distant machine this will not work because for security reasons, websites using HTTP are not allowed to use the microphone. There are two ways to get around this:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Forward the remote 8998 port to your localhost using ssh &lt;code&gt;-L&lt;/code&gt; flag. Then connects to &lt;a href="http://localhost:8998"&gt;localhost:8998&lt;/a&gt; as mentioned previously.&lt;/li&gt; 
 &lt;li&gt;Use the &lt;code&gt;--gradio-tunnel&lt;/code&gt; argument, setting up a tunnel with a URL accessible from anywhere. Keep in mind that this tunnel goes through the US and can add significant latency (up to 500ms from Europe). You can use &lt;code&gt;--gradio-tunnel-token&lt;/code&gt; to set a fixed secret token and reuse the same address over time.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can use &lt;code&gt;--hf-repo&lt;/code&gt; to select a different pretrained model, by setting the proper Hugging Face repository.&lt;/p&gt; 
&lt;p&gt;Accessing a server that is not localhost via http may cause issues with using the microphone in the web UI (in some browsers this is only allowed using https).&lt;/p&gt; 
&lt;p&gt;A command-line client is also available, as&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m moshi.client [--url URL_TO_GRADIO]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However note that, unlike the web browser, this client is barebones: it does not perform any echo cancellation, nor does it try to compensate for a growing lag by skipping frames.&lt;/p&gt; 
&lt;p&gt;For more information, in particular on how to use the API directly, please checkout &lt;a href="https://raw.githubusercontent.com/kyutai-labs/moshi/main/moshi/README.md"&gt;moshi/README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;MLX implementation for local inference on macOS&lt;/h2&gt; 
&lt;p&gt;Once you have installed &lt;code&gt;moshi_mlx&lt;/code&gt;, you can run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m moshi_mlx.local -q 4   # weights quantized to 4 bits
python -m moshi_mlx.local -q 8   # weights quantized to 8 bits
# And using a different pretrained model:
python -m moshi_mlx.local -q 4 --hf-repo kyutai/moshika-mlx-q4
python -m moshi_mlx.local -q 8 --hf-repo kyutai/moshika-mlx-q8
# be careful to always match the `-q` and `--hf-repo` flag.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command line interface is also barebone. It does not perform any echo cancellation, nor does it try to compensate for a growing lag by skipping frames.&lt;/p&gt; 
&lt;p&gt;Alternatively you can run &lt;code&gt;python -m moshi_mlx.local_web&lt;/code&gt; to use the web UI, the connection is via http and will be at &lt;a href="http://localhost:8998"&gt;localhost:8998&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Rust implementation&lt;/h2&gt; 
&lt;p&gt;In order to run the Rust inference server, use the following command from within the &lt;code&gt;rust&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo run --features cuda --bin moshi-backend -r -- --config moshi-backend/config.json standalone
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using macOS, you can replace &lt;code&gt;--features cuda&lt;/code&gt; with &lt;code&gt;--features metal&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Alternatively you can use &lt;code&gt;config-q8.json&lt;/code&gt; rather than &lt;code&gt;config.json&lt;/code&gt; to use the quantized q8 model. You can select a different pretrained model, e.g. Moshika, by changing the &lt;code&gt;"hf_repo"&lt;/code&gt; key in either file.&lt;/p&gt; 
&lt;p&gt;Once the server has printed 'standalone worker listening', you can use the web UI. By default the Rust server uses https so it will be at &lt;a href="https://localhost:8998"&gt;localhost:8998&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You will get warnings about the site being unsafe. When using chrome you can bypass these by selecting "Details" or "Advanced", then "Visit this unsafe site" or "Proceed to localhost (unsafe)".&lt;/p&gt; 
&lt;h2&gt;Clients&lt;/h2&gt; 
&lt;p&gt;We recommend using the web UI as it provides additional echo cancellation that helps the overall model quality. Note that most commands will directly serve this UI in the provided URL, and there is in general nothing more to do.&lt;/p&gt; 
&lt;p&gt;Alternatively, we provide command line interfaces for the Rust and Python versions, the protocol is the same as with the web UI so there is nothing to change on the server side.&lt;/p&gt; 
&lt;p&gt;For reference, here is the list of clients for Moshi.&lt;/p&gt; 
&lt;h3&gt;Web UI&lt;/h3&gt; 
&lt;p&gt;The web UI can be built from this repo via the following steps (these will require &lt;code&gt;npm&lt;/code&gt; being installed).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd client
npm install
npm run build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The web UI can then be found in the &lt;code&gt;client/dist&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Rust Command Line&lt;/h3&gt; 
&lt;p&gt;From within the &lt;code&gt;rust&lt;/code&gt; directory, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo run --bin moshi-cli -r -- tui --host localhost
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python with PyTorch&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m moshi.client
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Gradio Demo&lt;/h3&gt; 
&lt;p&gt;You can launch a Gradio demo locally with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m moshi.client_gradio --url &amp;lt;moshi-server-url&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Prior to running the Gradio demo, please install &lt;code&gt;gradio-webrtc&amp;gt;=0.0.18&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker Compose (CUDA only)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Requires &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"&gt;NVIDIA Container Toolkit&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;If you wish to install from a clone of this repository, maybe to further develop Moshi, you can do the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# From the root of the clone of the repo
pip install -e 'moshi[dev]'
pip install -e 'moshi_mlx[dev]'
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you wish to build locally &lt;code&gt;rustymimi&lt;/code&gt; (assuming you have Rust properly installed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install maturin
maturin dev -r -m rust/mimi-pyo3/Cargo.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;Checkout the &lt;a href="https://raw.githubusercontent.com/kyutai-labs/moshi/main/FAQ.md"&gt;Frequently Asked Questions&lt;/a&gt; section before opening an issue.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The present code is provided under the MIT license for the Python parts, and Apache license for the Rust backend. The web client code is provided under the MIT license. Note that parts of this code is based on &lt;a href="https://github.com/facebookresearch/audiocraft"&gt;AudioCraft&lt;/a&gt;, released under the MIT license.&lt;/p&gt; 
&lt;p&gt;The weights for the models are released under the CC-BY 4.0 license.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use either Mimi or Moshi, please cite the following paper,&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@techreport{kyutai2024moshi,
      title={Moshi: a speech-text foundation model for real-time dialogue},
      author={Alexandre D\'efossez and Laurent Mazar\'e and Manu Orsini and
      Am\'elie Royer and Patrick P\'erez and Herv\'e J\'egou and Edouard Grave and Neil Zeghidour},
      year={2024},
      eprint={2410.00037},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2410.00037},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>anthropics/skills</title>
      <link>https://github.com/anthropics/skills</link>
      <description>&lt;p&gt;Public repository for Agent Skills&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This repository contains Anthropic's implementation of skills for Claude. For information about the Agent Skills standard, see &lt;a href="http://agentskills.io"&gt;agentskills.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Skills&lt;/h1&gt; 
&lt;p&gt;Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that's creating documents with your company's brand guidelines, analyzing data using your organization's specific workflows, or automating personal tasks.&lt;/p&gt; 
&lt;p&gt;For more information, check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512176-what-are-skills"&gt;What are skills?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude"&gt;Using skills in Claude&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills"&gt;Equipping agents for the real world with Agent Skills&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;About This Repository&lt;/h1&gt; 
&lt;p&gt;This repository contains skills that demonstrate what's possible with Claude's skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).&lt;/p&gt; 
&lt;p&gt;Each skill is self-contained in its own folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.&lt;/p&gt; 
&lt;p&gt;Many skills in this repo are open source (Apache 2.0). We've also included the document creation &amp;amp; editing skills that power &lt;a href="https://www.anthropic.com/news/create-files"&gt;Claude's document capabilities&lt;/a&gt; under the hood in the &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/docx"&gt;&lt;code&gt;skills/docx&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pdf"&gt;&lt;code&gt;skills/pdf&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pptx"&gt;&lt;code&gt;skills/pptx&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/xlsx"&gt;&lt;code&gt;skills/xlsx&lt;/code&gt;&lt;/a&gt; subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;These skills are provided for demonstration and educational purposes only.&lt;/strong&gt; While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.&lt;/p&gt; 
&lt;h1&gt;Skill Sets&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills"&gt;./skills&lt;/a&gt;: Skill examples for Creative &amp;amp; Design, Development &amp;amp; Technical, Enterprise &amp;amp; Communication, and Document Skills&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/spec"&gt;./spec&lt;/a&gt;: The Agent Skills specification&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/template"&gt;./template&lt;/a&gt;: Skill template&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Try in Claude Code, Claude.ai, and the API&lt;/h1&gt; 
&lt;h2&gt;Claude Code&lt;/h2&gt; 
&lt;p&gt;You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin marketplace add anthropics/skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, to install a specific set of skills:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Select &lt;code&gt;Browse and install plugins&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;anthropic-agent-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;document-skills&lt;/code&gt; or &lt;code&gt;example-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;Install now&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Alternatively, directly install either Plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the &lt;code&gt;document-skills&lt;/code&gt; plugin from the marketplace, you can ask Claude Code to do something like: "Use the PDF skill to extract the form fields from &lt;code&gt;path/to/some-file.pdf&lt;/code&gt;"&lt;/p&gt; 
&lt;h2&gt;Claude.ai&lt;/h2&gt; 
&lt;p&gt;These example skills are all already available to paid plans in Claude.ai.&lt;/p&gt; 
&lt;p&gt;To use any skill from this repository or upload custom skills, follow the instructions in &lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b"&gt;Using skills in Claude&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Claude API&lt;/h2&gt; 
&lt;p&gt;You can use Anthropic's pre-built skills, and upload custom skills, via the Claude API. See the &lt;a href="https://docs.claude.com/en/api/skills-guide#creating-a-skill"&gt;Skills API Quickstart&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h1&gt;Creating a Basic Skill&lt;/h1&gt; 
&lt;p&gt;Skills are simple to create - just a folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing YAML frontmatter and instructions. You can use the &lt;strong&gt;template-skill&lt;/strong&gt; in this repository as a starting point:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontmatter requires only two fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt; - A unique identifier for your skill (lowercase, hyphens for spaces)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;description&lt;/code&gt; - A complete description of what the skill does and when to use it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see &lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Partner Skills&lt;/h1&gt; 
&lt;p&gt;Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Notion&lt;/strong&gt; - &lt;a href="https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0"&gt;Notion Skills for Claude&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>business-science/ai-data-science-team</title>
      <link>https://github.com/business-science/ai-data-science-team</link>
      <description>&lt;p&gt;An AI-powered data science team of agents to help you perform common data science tasks 10X faster.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt; 
  &lt;picture&gt; 
   &lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/ai_data_science_logo.png" alt="AI Data Science Team" width="360" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;em&gt;AI Data Science Team + AI Pipeline Studio&lt;/em&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.python.org/pypi/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/v/ai-data-science-team.svg?style=for-the-badge" alt="PyPI" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ai-data-science-team.svg?style=for-the-badge" alt="versions" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/business-science/ai-data-science-team.svg?style=for-the-badge" alt="license" /&gt;&lt;/a&gt; 
 &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/business-science/ai-data-science-team?style=for-the-badge" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;AI Data Science Team&lt;/h1&gt; 
&lt;p&gt;AI Data Science Team is a Python library of specialized agents for common data science workflows, plus a flagship app: &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt;. The Studio turns your work into a visual, reproducible pipeline, while the AI team handles data loading, cleaning, visualization, and modeling.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Beta. Breaking changes may occur until 0.1.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;strong&gt;Please ‚≠ê us on GitHub (it takes 2 seconds and means a lot).&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;AI Pipeline Studio (Flagship App)&lt;/h2&gt; 
&lt;p&gt;AI Pipeline Studio is the main example of the AI Data Science Team in action.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/apps/ai_pipeline_studio_app.jpg" alt="AI Pipeline Studio" /&gt;&lt;/p&gt; 
&lt;p&gt;Highlights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pipeline-first workspace: Visual Editor, Table, Chart, EDA, Code, Model, Predictions, MLflow&lt;/li&gt; 
 &lt;li&gt;Manual + AI steps with lineage and reproducible scripts&lt;/li&gt; 
 &lt;li&gt;Multi-dataset handling and merge workflows&lt;/li&gt; 
 &lt;li&gt;Project saves: metadata-only or full-data&lt;/li&gt; 
 &lt;li&gt;Storage footprint controls and rehydrate workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Full app docs: &lt;code&gt;apps/ai-pipeline-studio-app/README.md&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;OpenAI API key (or Ollama for local models)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install the app and library&lt;/h3&gt; 
&lt;p&gt;Clone the repo and install in editable mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run the AI Pipeline Studio app&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Library Overview&lt;/h2&gt; 
&lt;p&gt;The repository includes both the &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt; app and the underlying &lt;strong&gt;AI Data Science Team&lt;/strong&gt; library. The library provides agent building blocks and multi-agent workflows for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data loading and inspection&lt;/li&gt; 
 &lt;li&gt;Cleaning, wrangling, and feature engineering&lt;/li&gt; 
 &lt;li&gt;Visualization and EDA&lt;/li&gt; 
 &lt;li&gt;Modeling and evaluation (H2O + MLflow tools)&lt;/li&gt; 
 &lt;li&gt;SQL database interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Agents (Snapshot)&lt;/h3&gt; 
&lt;p&gt;Agent examples live in &lt;code&gt;examples/&lt;/code&gt;. Notable agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data Loader Tools Agent&lt;/li&gt; 
 &lt;li&gt;Data Wrangling Agent&lt;/li&gt; 
 &lt;li&gt;Data Cleaning Agent&lt;/li&gt; 
 &lt;li&gt;Data Visualization Agent&lt;/li&gt; 
 &lt;li&gt;EDA Tools Agent&lt;/li&gt; 
 &lt;li&gt;Feature Engineering Agent&lt;/li&gt; 
 &lt;li&gt;SQL Database Agent&lt;/li&gt; 
 &lt;li&gt;H2O ML Agent&lt;/li&gt; 
 &lt;li&gt;MLflow Tools Agent&lt;/li&gt; 
 &lt;li&gt;Multi-agent workflows (e.g., Pandas Data Analyst, SQL Data Analyst)&lt;/li&gt; 
 &lt;li&gt;Supervisor Agent (oversees other agents)&lt;/li&gt; 
 &lt;li&gt;Custom tools for data science tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Apps&lt;/h2&gt; 
&lt;p&gt;See all apps in &lt;code&gt;apps/&lt;/code&gt;. Notable apps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Pipeline Studio: &lt;code&gt;apps/ai-pipeline-studio-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;EDA Explorer App: &lt;code&gt;apps/exploratory-copilot-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Pandas Data Analyst App: &lt;code&gt;apps/pandas-data-analyst-app/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use OpenAI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    model_name="gpt-4.1-mini",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Ollama (Local LLM)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama serve
ollama pull llama3.1:8b
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.1:8b",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Next-Gen AI Agentic Workshop&lt;/h2&gt; 
&lt;p&gt;Want to learn how to build AI agents and AI apps for real data science workflows? Join my next‚Äëgen AI workshop: &lt;a href="https://learn.business-science.io/ai-register"&gt;https://learn.business-science.io/ai-register&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/UltraRAG</title>
      <link>https://github.com/OpenBMB/UltraRAG</link>
      <description>&lt;p&gt;UltraRAG v3: A Low-Code MCP Framework for Building Complex and Innovative RAG Pipelines&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="./docs/ultrarag_dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="./docs/ultrarag.svg" /&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/ultrarag.svg?sanitize=true" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Less Code, Lower Barrier, Faster Deployment &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://modelscope.cn/datasets/UltraRAG/UltraRAG_Benchmark"&gt;&lt;b&gt;Dataset&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;&lt;b&gt;Paper Daily&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/README_zh.md"&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; | &lt;b&gt;English&lt;/b&gt; | &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; üî•&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2026.01.23] üéâ UltraRAG 3.0 Released: Say no to "black box" development‚Äîmake every line of reasoning logic clearly visible üëâ|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/ultrarag3_0.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
 &lt;li&gt;[2026.01.20] üéâ AgentCPM-Report Model Released! DeepResearch is finally localized: 8B on-device writing agent AgentCPM-Report is open-sourced üëâ |&lt;a href="https://huggingface.co/openbmb/AgentCPM-Report"&gt;ü§ó Model&lt;/a&gt;|&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025.11.11] üéâ UltraRAG 2.1 Released: Enhanced knowledge ingestion &amp;amp; multimodal support, with a more complete unified evaluation system!&lt;/li&gt; 
  &lt;li&gt;[2025.09.23] New daily RAG paper digest, updated every day üëâ |&lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;üìñ Papers&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.09] Released a Lightweight DeepResearch Pipeline local setup tutorial üëâ |&lt;a href="https://www.bilibili.com/video/BV1p8JfziEwM"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/01_build_light_deepresearch.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.01] Released a step-by-step UltraRAG installation and full RAG walkthrough video üëâ |&lt;a href="https://www.bilibili.com/video/BV1B9apz4E7K/?share_source=copy_web&amp;amp;vd_source=7035ae721e76c8149fb74ea7a2432710"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/00_Installing_and_Running_RAG.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.08.28] üéâ UltraRAG 2.0 Released! UltraRAG 2.0 is fully upgraded: build a high-performance RAG with just a few dozen lines of code, empowering researchers to focus on ideas and innovation! We have preserved the UltraRAG v2 code, which can be viewed at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v2"&gt;v2&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025.01.23] UltraRAG Released! Enabling large models to better comprehend and utilize knowledge bases. The UltraRAG 1.0 code is still available at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v1"&gt;v1&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About UltraRAG&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/18747" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/18747" alt="OpenBMB%2FUltraRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;UltraRAG is the first lightweight RAG development framework based on the &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;Model Context Protocol (MCP)&lt;/a&gt; architecture design, jointly launched by &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt; at Tsinghua University, &lt;a href="https://neuir.github.io"&gt;NEUIR&lt;/a&gt; at Northeastern University, &lt;a href="https://www.openbmb.cn/home"&gt;OpenBMB&lt;/a&gt;, and &lt;a href="https://github.com/AI9Stars"&gt;AI9stars&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Designed for research exploration and industrial prototyping, UltraRAG standardizes core RAG components (Retriever, Generation, etc.) as independent &lt;strong&gt;MCP Servers&lt;/strong&gt;, combined with the powerful workflow orchestration capabilities of the &lt;strong&gt;MCP Client&lt;/strong&gt;. Developers can achieve precise orchestration of complex control structures such as conditional branches and loops simply through YAML configuration.&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/architecture.png" width="90%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3&gt;UltraRAG UI&lt;/h3&gt; 
&lt;p&gt;UltraRAG UI transcends the boundaries of traditional chat interfaces, evolving into a visual RAG Integrated Development Environment (IDE) that combines orchestration, debugging, and demonstration.&lt;/p&gt; 
&lt;p&gt;The system features a powerful built-in Pipeline Builder that supports bidirectional real-time synchronization between "Canvas Construction" and "Code Editing," allowing for granular online adjustments of pipeline parameters and prompts. Furthermore, it introduces an Intelligent AI Assistant to empower the entire development lifecycle, from pipeline structural design to parameter tuning and prompt generation. Once constructed, logic flows can be converted into interactive dialogue systems with a single click. The system seamlessly integrates Knowledge Base Management components, enabling users to build custom knowledge bases for document Q&amp;amp;A. This truly realizes a one-stop closed loop, spanning from underlying logic construction and data governance to final application deployment.&lt;/p&gt; 
&lt;!-- &lt;p align="center"&gt;
  &lt;picture&gt;
    &lt;img alt="UltraRAG_UI" src="./docs/chat_menu.png" width=80%&gt;
  &lt;/picture&gt;
&lt;/p&gt; --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9"&gt;https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Key Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Low-Code Orchestration of Complex Workflows&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Inference Orchestration&lt;/strong&gt;: Natively supports control structures such as sequential, loop, and conditional branches. Developers only need to write YAML configuration files to implement complex iterative RAG logic in dozens of lines of code.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ö° &lt;strong&gt;Modular Extension and Reproduction&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Atomic Servers&lt;/strong&gt;: Based on the MCP architecture, functions are decoupled into independent Servers. New features only need to be registered as function-level Tools to seamlessly integrate into workflows, achieving extremely high reusability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìä &lt;strong&gt;Unified Evaluation and Benchmark Comparison&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Research Efficiency&lt;/strong&gt;: Built-in standardized evaluation workflows, ready-to-use mainstream research benchmarks. Through unified metric management and baseline integration, significantly improves experiment reproducibility and comparison efficiency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ú® &lt;strong&gt;Rapid Interactive Prototype Generation&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;One-Click Delivery&lt;/strong&gt;: Say goodbye to tedious UI development. With just one command, Pipeline logic can be instantly converted into an interactive conversational Web UI, shortening the distance from algorithm to demonstration.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We provide two installation methods: local source code installation (recommended using &lt;code&gt;uv&lt;/code&gt; for package management) and Docker container deployment&lt;/p&gt; 
&lt;h3&gt;Method 1: Source Code Installation&lt;/h3&gt; 
&lt;p&gt;We strongly recommend using &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; to manage Python environments and dependencies, as it can greatly improve installation speed.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Prepare Environment&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you haven't installed uv yet, please execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;## Direct installation
pip install uv
## Download
curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Download Source Code&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Choose one of the following modes to install dependencies based on your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A: Create a New Environment&lt;/strong&gt; Use &lt;code&gt;uv sync&lt;/code&gt; to automatically create a virtual environment and synchronize dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Core dependencies: If you only need to run basic core functions, such as only using UltraRAG UI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Full installation: If you want to fully experience UltraRAG's retrieval, generation, corpus processing, and evaluation functions, please run:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;On-demand installation: If you only need to run specific modules, keep the corresponding &lt;code&gt;--extra&lt;/code&gt; as needed, for example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --extra retriever   # Retrieval module only
uv sync --extra generation  # Generation module only
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once installed, activate the virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Windows CMD
.venv\Scripts\activate.bat

# Windows Powershell
.venv\Scripts\Activate.ps1

# macOS / Linux
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;B: Install into an Existing Environment&lt;/strong&gt; To install UltraRAG into your currently active Python environment, use &lt;code&gt;uv pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Core dependencies
uv pip install -e .

# Full installation
uv pip install -e ".[all]"

# On-demand installation
uv pip install -e ".[retriever]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Method 2: Docker Container Deployment&lt;/h3&gt; 
&lt;p&gt;If you prefer not to configure a local Python environment, you can deploy using Docker.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Get Code and Images&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# 1. Clone the repository
git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG

# 2. Prepare the image (choose one)
# Option A: Pull from Docker Hub
docker pull hdxin2002/ultrarag:v0.3.0-base-cpu # Base version (CPU)
docker pull hdxin2002/ultrarag:v0.3.0-base-gpu # Base version (GPU)
docker pull hdxin2002/ultrarag:v0.3.0          # Full version (GPU)

# Option B: Build locally
docker build -t ultrarag:v0.3.0 .

# 3. Start container (port 5050 is automatically mapped)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Start the Container&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Start the container (Port 5050 is mapped by default)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: After the container starts, UltraRAG UI will run automatically. You can directly access &lt;code&gt;http://localhost:5050&lt;/code&gt; in your browser to use it.&lt;/p&gt; 
&lt;h3&gt;Verify Installation&lt;/h3&gt; 
&lt;p&gt;After installation, run the following example command to check if the environment is normal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ultrarag run examples/sayhello.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you see the following output, the installation is successful:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Hello, UltraRAG v3!
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;We provide complete tutorial examples from beginner to advanced. Whether you are conducting academic research or building industrial applications, you can find guidance here. Welcome to visit the &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;Documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Research Experiments&lt;/h3&gt; 
&lt;p&gt;Designed for researchers, providing data, experimental workflows, and visualization analysis tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/quick_start"&gt;Getting Started&lt;/a&gt;: Learn how to quickly run standard RAG experimental workflows based on UltraRAG.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/dataset"&gt;Evaluation Data&lt;/a&gt;: Download the most commonly used public evaluation datasets in the RAG field and large-scale retrieval corpora, directly for research benchmark testing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/case_study"&gt;Case Analysis&lt;/a&gt;: Provides a visual Case Study interface to deeply track each intermediate output of the workflow, assisting in analysis and error attribution.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/code_integration"&gt;Code Integration&lt;/a&gt;: Learn how to directly call UltraRAG components in Python code to achieve more flexible customized development.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Demo Systems&lt;/h3&gt; 
&lt;p&gt;Designed for developers and end users, providing complete UI interaction and complex application cases.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/start"&gt;Quick Start&lt;/a&gt;: Learn how to start UltraRAG UI and familiarize yourself with various advanced configurations in administrator mode.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/prepare"&gt;Deployment Guide&lt;/a&gt;: Detailed production environment deployment tutorials, covering the setup of Retriever, Generation models (LLM), and Milvus vector database.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/demo/deepresearch"&gt;Deep Research&lt;/a&gt;: Flagship case, deploy a Deep Research Pipeline. Combined with the AgentCPM-Report model, it can automatically perform multi-step retrieval and integration to generate tens of thousands of words of survey reports.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thanks to the following contributors for their code submissions and testing. We also welcome new members to join us in collectively building a comprehensive RAG ecosystem!&lt;/p&gt; 
&lt;p&gt;You can contribute by following the standard process: &lt;strong&gt;Fork this repository ‚Üí Submit Issues ‚Üí Create Pull Requests (PRs)&lt;/strong&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBMB/UltraRAG/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=OpenBMB/UltraRAG&amp;amp;nocache=true" /&gt; &lt;/a&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;If you find this repository helpful for your research, please consider giving us a ‚≠ê to show your support.&lt;/p&gt; 
&lt;a href="https://star-history.com/#OpenBMB/UltraRAG&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical issues and feature requests, please use &lt;a href="https://github.com/OpenBMB/UltraRAG/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For questions about usage, feedback, or any discussions related to RAG technologies, you are welcome to join our &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/wechat_qr.png"&gt;WeChat group&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/feishu_qr.png"&gt;Feishu group&lt;/a&gt;, and &lt;a href="https://discord.gg/yRFFjjJnnS"&gt;Discord&lt;/a&gt; to exchange ideas with us.&lt;/li&gt; 
 &lt;li&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:yanyk.thu@gmail.com"&gt;yanyk.thu@gmail.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/wechat_qr.png" alt="WeChat Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;WeChat Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/feishu_qr.png" alt="Feishu Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;Feishu Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://discord.gg/yRFFjjJnnS"&gt; &lt;img src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt; &lt;/a&gt;&lt;br /&gt; &lt;b&gt;Discord&lt;/b&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from &lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; , &lt;img src="https://cdn.simpleicons.org/anthropic" alt="anthropic logo" width="25" height="15" /&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/googlegemini" alt="google logo" width="25" height="18" /&gt;&lt;strong&gt;Google&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/x" alt="X logo" width="25" height="15" /&gt;&lt;strong&gt;xAI&lt;/strong&gt; and open-source models like &lt;img src="https://cdn.simpleicons.org/alibabacloud" alt="alibaba logo" width="25" height="15" /&gt;&lt;strong&gt;Qwen&lt;/strong&gt; or &lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt;&lt;strong&gt;Llama&lt;/strong&gt; that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Thanks to our sponsors&lt;/h2&gt; 
&lt;table align="center" cellpadding="16" cellspacing="12"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" title="Tiger Data"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/tigerdata.png" alt="Tiger Data" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Tiger Data MCP &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" title="Speechmatics"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/speechmatics.png" alt="Speechmatics" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Speechmatics &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" title="Okara"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/okara.png" alt="Okara" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Okara AI &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sponsorunwindai.com/" title="Become a Sponsor"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsor_awesome_llm_apps.png" alt="Become a Sponsor" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sponsorunwindai.com/" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Become a Sponsor &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scraping AI Agent (Local &amp;amp; Cloud SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent"&gt;üèöÔ∏è üçå AI Home Renovation Agent with Nano Banana Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team"&gt;üìä AI VC Due Diligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api"&gt;üî¨ AI Research Planner &amp;amp; Executor (Google Interactions API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team"&gt;üë®üèª‚Äçüíº AI Sales Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/accomplish-ai/openwork"&gt;üåê Openwork - Open Browser Automation Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/"&gt;üé® üçå Multimodal UI/UX Feedback Agent Team with Nano Banana&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akshayaggarwal99/jarvis-ai-assistant"&gt;üéôÔ∏è OpenSource Voice Dictation Agent (like Wispr Flow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/modelcontextprotocol" alt="mcp logo" width="25" height="20" /&gt; MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_embedding_gemma"&gt;üî• Agentic RAG with Embedding Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/contextualai_rag_agent/"&gt;üîÑ Contextual AI RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéØ LLM Optimization Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/"&gt;üéØ Toonify Token Optimization&lt;/a&gt; - Reduce LLM API costs by 30-60% using TOON format&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="20" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/"&gt;Gemma 3 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üßë‚Äçüè´ AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; model‚Äëagnostic (OpenAI, Claude)&lt;/li&gt; 
 &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty, MCP tools&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
 &lt;li&gt;Simple multi‚Äëagent; Multi‚Äëagent patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/openai_sdk_crash_course/"&gt;OpenAI Agents SDK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; function calling; structured outputs&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty integrations&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; evaluation&lt;/li&gt; 
 &lt;li&gt;Multi‚Äëagent patterns; agent handoffs&lt;/li&gt; 
 &lt;li&gt;Swarm orchestration; routing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/github" alt="github logo" width="25" height="20" /&gt; Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Blaizzy/mlx-audio</title>
      <link>https://github.com/Blaizzy/mlx-audio</link>
      <description>&lt;p&gt;A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX-Audio&lt;/h1&gt; 
&lt;p&gt;The best audio processing library built on Apple's MLX framework, providing fast and efficient text-to-speech (TTS), speech-to-text (STT), and speech-to-speech (STS) on Apple Silicon.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fast inference optimized for Apple Silicon (M series chips)&lt;/li&gt; 
 &lt;li&gt;Multiple model architectures for TTS, STT, and STS&lt;/li&gt; 
 &lt;li&gt;Multilingual support across models&lt;/li&gt; 
 &lt;li&gt;Voice customization and cloning capabilities&lt;/li&gt; 
 &lt;li&gt;Adjustable speech speed control&lt;/li&gt; 
 &lt;li&gt;Interactive web interface with 3D audio visualization&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible REST API&lt;/li&gt; 
 &lt;li&gt;Quantization support (3-bit, 4-bit, 6-bit, 8-bit, and more) for optimized performance&lt;/li&gt; 
 &lt;li&gt;Swift package for iOS/macOS integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Using pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mlx-audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using uv to install only the command line tools&lt;/h3&gt; 
&lt;p&gt;Latest release from pypi:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force mlx-audio --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Latest code from github:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force git+https://github.com/Blaizzy/mlx-audio.git --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For development or web interface:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Blaizzy/mlx-audio.git
cd mlx-audio
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic TTS generation
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello, world!" --lang_code a

# With voice selection and speed adjustment
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello!" --voice af_heart --speed 1.2 --lang_code a

# Play audio immediately
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello!" --play  --lang_code a

# Save to a specific directory
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello!" --output_path ./my_audio  --lang_code a
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

# Load model
model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate speech
for result in model.generate("Hello from MLX-Audio!", voice="af_heart"):
    print(f"Generated {result.audio.shape[0]} samples")
    # result.audio contains the waveform as mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;h3&gt;Text-to-Speech (TTS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Kokoro&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fast, high-quality multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, JA, ZH, FR, ES, IT, PT, HI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Kokoro-82M-bf16"&gt;mlx-community/Kokoro-82M-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alibaba's multilingual TTS with voice design&lt;/td&gt; 
   &lt;td&gt;ZH, EN, JA, KO, + more&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16"&gt;mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CSM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational Speech Model with voice cloning&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/csm-1b"&gt;mlx-community/csm-1b&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Dia&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dialogue-focused TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Dia-1.6B-bf16"&gt;mlx-community/Dia-1.6B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OuteTTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient TTS model&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/OuteTTS-0.2-500M"&gt;mlx-community/OuteTTS-0.2-500M&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;SparkTTS model&lt;/td&gt; 
   &lt;td&gt;EN, ZH&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/SparkTTS-0.5B-bf16"&gt;mlx-community/SparkTTS-0.5B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chatterbox&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Expressive multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, ES, FR, DE, IT, PT, PL, TR, RU, NL, CS, AR, ZH, JA, HU, KO&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Chatterbox-bf16"&gt;mlx-community/Chatterbox-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Soprano&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;High-quality TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Soprano-bf16"&gt;mlx-community/Soprano-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Text (STT)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI's robust STT model&lt;/td&gt; 
   &lt;td&gt;99+ languages&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/whisper-large-v3-turbo-asr-fp16"&gt;mlx-community/whisper-large-v3-turbo-asr-fp16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Parakeet&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NVIDIA's accurate STT&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2"&gt;mlx-community/parakeet-tdt-0.6b-v2&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voxtral&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Mistral's speech model&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16"&gt;mlx-community/Voxtral-Mini-3B-2507-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Microsoft's 9B ASR with diarization &amp;amp; timestamps&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/VibeVoice-ASR-bf16"&gt;mlx-community/VibeVoice-ASR-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Speech (STS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SAM-Audio&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text-guided source separation&lt;/td&gt; 
   &lt;td&gt;Extract specific sounds&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/sam-audio-large"&gt;mlx-community/sam-audio-large&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Liquid2.5-Audio&lt;/strong&gt;*&lt;/td&gt; 
   &lt;td&gt;Speech-to-Speech, Text-to-Speech and Speech-to-Text&lt;/td&gt; 
   &lt;td&gt;Speech interactions&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-8bit"&gt;mlx-community/LFM2.5-Audio-1.5B-8bit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MossFormer2 SE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Speech enhancement&lt;/td&gt; 
   &lt;td&gt;Noise removal&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/starkdmi/MossFormer2_SE_48K_MLX"&gt;starkdmi/MossFormer2_SE_48K_MLX&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Examples&lt;/h2&gt; 
&lt;h3&gt;Kokoro TTS&lt;/h3&gt; 
&lt;p&gt;Kokoro is a fast, multilingual TTS model with 54 voice presets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate with different voices
for result in model.generate(
    text="Welcome to MLX-Audio!",
    voice="af_heart",  # American female
    speed=1.0,
    lang_code="a"  # American English
):
    audio = result.audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Voices:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;American English: &lt;code&gt;af_heart&lt;/code&gt;, &lt;code&gt;af_bella&lt;/code&gt;, &lt;code&gt;af_nova&lt;/code&gt;, &lt;code&gt;af_sky&lt;/code&gt;, &lt;code&gt;am_adam&lt;/code&gt;, &lt;code&gt;am_echo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;British English: &lt;code&gt;bf_alice&lt;/code&gt;, &lt;code&gt;bf_emma&lt;/code&gt;, &lt;code&gt;bm_daniel&lt;/code&gt;, &lt;code&gt;bm_george&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Japanese: &lt;code&gt;jf_alpha&lt;/code&gt;, &lt;code&gt;jm_kumo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Chinese: &lt;code&gt;zf_xiaobei&lt;/code&gt;, &lt;code&gt;zm_yunxi&lt;/code&gt;, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Language Codes:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Code&lt;/th&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;a&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;American English&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;b&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;British English&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;j&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Japanese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[ja]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;z&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mandarin Chinese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[zh]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Spanish&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;f&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;French&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Qwen3-TTS&lt;/h3&gt; 
&lt;p&gt;Alibaba's state-of-the-art multilingual TTS with voice cloning, emotion control, and voice design capabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16")
results = list(model.generate(
    text="Hello, welcome to MLX-Audio!",
    voice="Chelsie",
    language="English",
))

audio = results[0].audio  # mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/mlx_audio/tts/models/qwen3_tts/README.md"&gt;Qwen3-TTS README&lt;/a&gt; for voice cloning, CustomVoice, VoiceDesign, and all available models.&lt;/p&gt; 
&lt;h3&gt;CSM (Voice Cloning)&lt;/h3&gt; 
&lt;p&gt;Clone any voice using a reference audio sample:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_audio.tts.generate \
    --model mlx-community/csm-1b \
    --text "Hello from Sesame." \
    --ref_audio ./reference_voice.wav \
    --play
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Whisper STT&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.utils import load_model, transcribe

model = load_model("mlx-community/whisper-large-v3-turbo-asr-fp16")
result = transcribe("audio.wav", model=model)
print(result["text"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;VibeVoice-ASR&lt;/h3&gt; 
&lt;p&gt;Microsoft's 9B parameter speech-to-text model with speaker diarization and timestamps. Supports long-form audio (up to 60 minutes) and outputs structured JSON.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.utils import load

model = load("mlx-community/VibeVoice-ASR-bf16")

# Basic transcription
result = model.generate(audio="meeting.wav", max_tokens=8192, temperature=0.0)
print(result.text)
# [{"Start":0,"End":5.2,"Speaker":0,"Content":"Hello everyone, let's begin."},
#  {"Start":5.5,"End":9.8,"Speaker":1,"Content":"Thanks for joining today."}]

# Access parsed segments
for seg in result.segments:
    print(f"[{seg['start_time']:.1f}-{seg['end_time']:.1f}] Speaker {seg['speaker_id']}: {seg['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Streaming transcription:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Stream tokens as they are generated
for text in model.stream_transcribe(audio="speech.wav", max_tokens=4096):
    print(text, end="", flush=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With context (hotwords/metadata):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = model.generate(
    audio="technical_talk.wav",
    context="MLX, Apple Silicon, PyTorch, Transformer",
    max_tokens=8192,
    temperature=0.0,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CLI usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic transcription
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio meeting.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --verbose

# With context/hotwords
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio technical_talk.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --context "MLX, Apple Silicon, PyTorch, Transformer" \
    --verbose
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;SAM-Audio (Source Separation)&lt;/h3&gt; 
&lt;p&gt;Separate specific sounds from audio using text prompts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import SAMAudio, SAMAudioProcessor, save_audio

model = SAMAudio.from_pretrained("mlx-community/sam-audio-large")
processor = SAMAudioProcessor.from_pretrained("mlx-community/sam-audio-large")

batch = processor(
    descriptions=["A person speaking"],
    audios=["mixed_audio.wav"],
)

result = model.separate_long(
    batch.audios,
    descriptions=batch.descriptions,
    anchors=batch.anchor_ids,
    chunk_seconds=10.0,
    overlap_seconds=3.0,
    ode_opt={"method": "midpoint", "step_size": 2/32},
)

save_audio(result.target[0], "voice.wav")
save_audio(result.residual[0], "background.wav")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MossFormer2 (Speech Enhancement)&lt;/h3&gt; 
&lt;p&gt;Remove noise from speech recordings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import MossFormer2SEModel, save_audio

model = MossFormer2SEModel.from_pretrained("starkdmi/MossFormer2_SE_48K_MLX")
enhanced = model.enhance("noisy_speech.wav")
save_audio(enhanced, "clean.wav", 48000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Web Interface &amp;amp; API Server&lt;/h2&gt; 
&lt;p&gt;MLX-Audio includes a modern web interface and OpenAI-compatible API.&lt;/p&gt; 
&lt;h3&gt;Starting the Server&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start API server
mlx_audio.server --host 0.0.0.0 --port 8000

# Start web UI (in another terminal)
cd mlx_audio/ui
npm install &amp;amp;&amp;amp; npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;API Endpoints&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; (OpenAI-compatible):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{"model": "mlx-community/Kokoro-82M-bf16", "input": "Hello!", "voice": "af_heart"}' \
  --output speech.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.wav" \
  -F "model=mlx-community/whisper-large-v3-turbo-asr-fp16"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quantization&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;MLX&lt;/li&gt; 
 &lt;li&gt;Python 3.8+&lt;/li&gt; 
 &lt;li&gt;Apple Silicon Mac (for optimal performance)&lt;/li&gt; 
 &lt;li&gt;For the web interface and API: 
  &lt;ul&gt; 
   &lt;li&gt;FastAPI&lt;/li&gt; 
   &lt;li&gt;Uvicorn&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Swift&lt;/h2&gt; 
&lt;p&gt;Looking for Swift/iOS support? Check out &lt;a href="https://github.com/Blaizzy/mlx-audio-swift"&gt;mlx-audio-swift&lt;/a&gt; for on-device TTS using MLX on macOS and iOS. Reduce model size and improve performance with quantization using the convert script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert and quantize to 4-bit
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-4bit \
    --quantize \
    --q-bits 4 \
    --upload-repo username/Kokoro-82M-4bit (optional: if you want to upload the model to Hugging Face)

# Convert with specific dtype (bfloat16)
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-bf16 \
    --dtype bfloat16 \
    --upload-repo username/Kokoro-82M-bf16 (optional: if you want to upload the model to Hugging Face)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--hf-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source Hugging Face model or local path&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--mlx-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Output directory for converted model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-q, --quantize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable quantization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-bits&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bits per weight (4, 6, or 8)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-group-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group size for quantization (default: 64)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--dtype&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Weight dtype: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;bfloat16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--upload-repo&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Upload converted model to HF Hub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Apple Silicon Mac (M1/M2/M3/M4)&lt;/li&gt; 
 &lt;li&gt;MLX framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ffmpeg&lt;/strong&gt; (required for MP3/FLAC audio encoding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing ffmpeg&lt;/h3&gt; 
&lt;p&gt;ffmpeg is required for saving audio in MP3 or FLAC format. Install it using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS (using Homebrew)
brew install ffmpeg

# Ubuntu/Debian
sudo apt install ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;WAV format works without ffmpeg.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mlx-audio,
  author = {Canuma, Prince},
  title = {MLX Audio},
  year = {2025},
  howpublished = {\url{https://github.com/Blaizzy/mlx-audio}},
  note = {Audio processing library for Apple Silicon with TTS, STT, and STS capabilities.}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx"&gt;Apple MLX Team&lt;/a&gt; for the MLX framework&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>marimo-team/marimo</title>
      <link>https://github.com/marimo-team/marimo</link>
      <description>&lt;p&gt;A reactive notebook for Python ‚Äî run reproducible experiments, query with SQL, execute as a script, deploy as an app, and version with git. Stored as pure Python. All in a modern, AI-native editor.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;A reactive Python notebook that's reproducible, git-friendly, and deployable as scripts or apps.&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://docs.marimo.io" target="_blank"&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href="https://marimo.io/discord?ref=readme" target="_blank"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href="https://docs.marimo.io/examples/" target="_blank"&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href="https://marimo.io/gallery/" target="_blank"&gt;&lt;strong&gt;Gallery&lt;/strong&gt;&lt;/a&gt; ¬∑ &lt;a href="https://www.youtube.com/@marimo-team/" target="_blank"&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;b&gt;English&lt;/b&gt; &lt;b&gt; | &lt;/b&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/README_Traditional_Chinese.md" target="_blank"&gt;&lt;b&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; &lt;b&gt; | &lt;/b&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/README_Chinese.md" target="_blank"&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; &lt;b&gt; | &lt;/b&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/README_Japanese.md" target="_blank"&gt;&lt;b&gt;Êó•Êú¨Ë™û&lt;/b&gt;&lt;/a&gt; &lt;b&gt; | &lt;/b&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/README_Spanish.md" target="_blank"&gt;&lt;b&gt;Espa√±ol&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/marimo/"&gt;&lt;img src="https://img.shields.io/pypi/v/marimo?color=%2334D058&amp;amp;label=pypi" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/marimo"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/marimo.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://marimo.io/discord?ref=readme"&gt;&lt;img src="https://shields.io/discord/1059888774789730424" alt="discord" /&gt;&lt;/a&gt; &lt;img alt="Pepy Total Downloads" src="https://img.shields.io/pepy/dt/marimo?label=pypi%20%7C%20downloads" /&gt; &lt;img alt="Conda Downloads" src="https://img.shields.io/conda/d/conda-forge/marimo" /&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/marimo" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;marimo&lt;/strong&gt; is a reactive Python notebook: run a cell or interact with a UI element, and marimo automatically runs dependent cells (or &lt;a href="https://raw.githubusercontent.com/marimo-team/marimo/main/#expensive-notebooks"&gt;marks them as stale&lt;/a&gt;), keeping code and outputs consistent. marimo notebooks are stored as pure Python (with first-class SQL support), executable as scripts, and deployable as apps.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;batteries-included:&lt;/strong&gt; replaces &lt;code&gt;jupyter&lt;/code&gt;, &lt;code&gt;streamlit&lt;/code&gt;, &lt;code&gt;jupytext&lt;/code&gt;, &lt;code&gt;ipywidgets&lt;/code&gt;, &lt;code&gt;papermill&lt;/code&gt;, and more&lt;/li&gt; 
 &lt;li&gt;‚ö°Ô∏è &lt;strong&gt;reactive&lt;/strong&gt;: run a cell, and marimo reactively &lt;a href="https://docs.marimo.io/guides/reactivity.html"&gt;runs all dependent cells&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/marimo-team/marimo/main/#expensive-notebooks"&gt;marks them as stale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üñêÔ∏è &lt;strong&gt;interactive:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/interactivity.html"&gt;bind sliders, tables, plots, and more&lt;/a&gt; to Python ‚Äî no callbacks required&lt;/li&gt; 
 &lt;li&gt;üêç &lt;strong&gt;git-friendly:&lt;/strong&gt; stored as &lt;code&gt;.py&lt;/code&gt; files&lt;/li&gt; 
 &lt;li&gt;üõ¢Ô∏è &lt;strong&gt;designed for data&lt;/strong&gt;: query dataframes, databases, warehouses, or lakehouses &lt;a href="https://docs.marimo.io/guides/working_with_data/sql.html"&gt;with SQL&lt;/a&gt;, filter and search &lt;a href="https://docs.marimo.io/guides/working_with_data/dataframes.html"&gt;dataframes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;AI-native&lt;/strong&gt;: &lt;a href="https://docs.marimo.io/guides/generate_with_ai/"&gt;generate cells with AI&lt;/a&gt; tailored for data work&lt;/li&gt; 
 &lt;li&gt;üî¨ &lt;strong&gt;reproducible:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/reactivity.html#no-hidden-state"&gt;no hidden state&lt;/a&gt;, deterministic execution, &lt;a href="https://docs.marimo.io/guides/package_management/"&gt;built-in package management&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üèÉ &lt;strong&gt;executable:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/scripts.html"&gt;execute as a Python script&lt;/a&gt;, parameterized by CLI args&lt;/li&gt; 
 &lt;li&gt;üõú &lt;strong&gt;shareable&lt;/strong&gt;: &lt;a href="https://docs.marimo.io/guides/apps.html"&gt;deploy as an interactive web app&lt;/a&gt; or &lt;a href="https://docs.marimo.io/guides/apps.html#slides-layout"&gt;slides&lt;/a&gt;, &lt;a href="https://docs.marimo.io/guides/wasm.html"&gt;run in the browser via WASM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üß© &lt;strong&gt;reusable:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/reusing_functions/"&gt;import functions and classes&lt;/a&gt; from one notebook to another&lt;/li&gt; 
 &lt;li&gt;üß™ &lt;strong&gt;testable:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/testing/"&gt;run pytest&lt;/a&gt; on notebooks&lt;/li&gt; 
 &lt;li&gt;‚å®Ô∏è &lt;strong&gt;a modern editor&lt;/strong&gt;: &lt;a href="https://docs.marimo.io/guides/editor_features/ai_completion.html#github-copilot"&gt;GitHub Copilot&lt;/a&gt;, &lt;a href="https://docs.marimo.io/guides/editor_features/ai_completion.html"&gt;AI assistants&lt;/a&gt;, vim keybindings, variable explorer, and &lt;a href="https://docs.marimo.io/guides/editor_features/index.html"&gt;more&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üßë‚Äçüíª &lt;strong&gt;use your favorite editor&lt;/strong&gt;: run in &lt;a href="https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo"&gt;VS Code or Cursor&lt;/a&gt;, or edit in neovim, Zed, &lt;a href="https://docs.marimo.io/guides/editor_features/watching/"&gt;or any other text editor&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pip install marimo &amp;amp;&amp;amp; marimo tutorial intro
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Try marimo at &lt;a href="https://marimo.app/l/c7h6pz"&gt;our online playground&lt;/a&gt;, which runs entirely in the browser!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Jump to the &lt;a href="https://raw.githubusercontent.com/marimo-team/marimo/main/#quickstart"&gt;quickstart&lt;/a&gt; for a primer on our CLI.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;A reactive programming environment&lt;/h2&gt; 
&lt;p&gt;marimo guarantees your notebook code, outputs, and program state are consistent. This &lt;a href="https://docs.marimo.io/faq.html#faq-problems"&gt;solves many problems&lt;/a&gt; associated with traditional notebooks like Jupyter.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A reactive programming environment.&lt;/strong&gt; Run a cell and marimo &lt;em&gt;reacts&lt;/em&gt; by automatically running the cells that reference its variables, eliminating the error-prone task of manually re-running cells. Delete a cell and marimo scrubs its variables from program memory, eliminating hidden state.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/reactive.gif" width="700px" /&gt; 
&lt;p&gt;&lt;a name="expensive-notebooks"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Compatible with expensive notebooks.&lt;/strong&gt; marimo lets you &lt;a href="https://docs.marimo.io/guides/configuration/runtime_configuration.html"&gt;configure the runtime to be lazy&lt;/a&gt;, marking affected cells as stale instead of automatically running them. This gives you guarantees on program state while preventing accidental execution of expensive cells.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Synchronized UI elements.&lt;/strong&gt; Interact with &lt;a href="https://docs.marimo.io/guides/interactivity.html"&gt;UI elements&lt;/a&gt; like &lt;a href="https://docs.marimo.io/api/inputs/slider.html#slider"&gt;sliders&lt;/a&gt;, &lt;a href="https://docs.marimo.io/api/inputs/dropdown.html"&gt;dropdowns&lt;/a&gt;, &lt;a href="https://docs.marimo.io/api/inputs/dataframe.html"&gt;dataframe transformers&lt;/a&gt;, and &lt;a href="https://docs.marimo.io/api/inputs/chat.html"&gt;chat interfaces&lt;/a&gt;, and the cells that use them are automatically re-run with their latest values.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif" width="700px" /&gt; 
&lt;p&gt;&lt;strong&gt;Interactive dataframes.&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/working_with_data/dataframes.html"&gt;Page through, search, filter, and sort&lt;/a&gt; millions of rows blazingly fast, no code required.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-df.gif" width="700px" /&gt; 
&lt;p&gt;&lt;strong&gt;Generate cells with data-aware AI.&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/editor_features/ai_completion/"&gt;Generate code with an AI assistant&lt;/a&gt; that is highly specialized for working with data, with context about your variables in memory; &lt;a href="https://docs.marimo.io/guides/generate_with_ai/text_to_notebook/"&gt;zero-shot entire notebooks&lt;/a&gt;. Customize the system prompt, bring your own API keys, or use local models.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-generate-with-ai.gif" width="700px" /&gt; 
&lt;p&gt;&lt;strong&gt;Query data with SQL.&lt;/strong&gt; Build &lt;a href="https://docs.marimo.io/guides/working_with_data/sql.html"&gt;SQL&lt;/a&gt; queries that depend on Python values and execute them against dataframes, databases, lakehouses, CSVs, Google Sheets, or anything else using our built-in SQL engine, which returns the result as a Python dataframe.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-sql-cell.png" width="700px" /&gt; 
&lt;p&gt;Your notebooks are still pure Python, even if they use SQL.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Dynamic markdown.&lt;/strong&gt; Use markdown parametrized by Python variables to tell dynamic stories that depend on Python data.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Built-in package management.&lt;/strong&gt; marimo has built-in support for all major package managers, letting you &lt;a href="https://docs.marimo.io/guides/editor_features/package_management.html"&gt;install packages on import&lt;/a&gt;. marimo can even &lt;a href="https://docs.marimo.io/guides/package_management/inlining_dependencies/"&gt;serialize package requirements&lt;/a&gt; in notebook files, and auto install them in isolated venv sandboxes.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deterministic execution order.&lt;/strong&gt; Notebooks are executed in a deterministic order, based on variable references instead of cells' positions on the page. Organize your notebooks to best fit the stories you'd like to tell.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Performant runtime.&lt;/strong&gt; marimo runs only those cells that need to be run by statically analyzing your code.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Batteries-included.&lt;/strong&gt; marimo comes with GitHub Copilot, AI assistants, Ruff code formatting, HTML export, fast code completion, a &lt;a href="https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo"&gt;VS Code extension&lt;/a&gt;, an interactive dataframe viewer, and &lt;a href="https://docs.marimo.io/guides/editor_features/index.html"&gt;many more&lt;/a&gt; quality-of-life features.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;The &lt;a href="https://www.youtube.com/watch?v=3N6lInzq5MI&amp;amp;list=PLNJXGo8e1XT9jP7gPbRdm1XwloZVFvLEq"&gt;marimo concepts playlist&lt;/a&gt; on our &lt;a href="https://www.youtube.com/@marimo-team"&gt;YouTube channel&lt;/a&gt; gives an overview of many features.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Installation.&lt;/strong&gt; In a terminal, run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install marimo  # or conda install -c conda-forge marimo
marimo tutorial intro
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install with additional dependencies that unlock SQL cells, AI completion, and more, run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "marimo[recommended]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Create notebooks.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Create or edit notebooks with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;marimo edit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Run apps.&lt;/strong&gt; Run your notebook as a web app, with Python code hidden and uneditable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;marimo run your_notebook.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-model-comparison.gif" style="border-radius: 8px" width="450px" /&gt; 
&lt;p&gt;&lt;strong&gt;Execute as scripts.&lt;/strong&gt; Execute a notebook as a script at the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python your_notebook.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Automatically convert Jupyter notebooks.&lt;/strong&gt; Automatically convert Jupyter notebooks to marimo notebooks with the CLI&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;marimo convert your_notebook.ipynb &amp;gt; your_notebook.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or use our &lt;a href="https://marimo.io/convert"&gt;web interface&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tutorials.&lt;/strong&gt; List all tutorials:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;marimo tutorial --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Share cloud-based notebooks.&lt;/strong&gt; Use &lt;a href="https://molab.marimo.io/notebooks"&gt;molab&lt;/a&gt;, a cloud-based marimo notebook service similar to Google Colab, to create and share notebook links.&lt;/p&gt; 
&lt;h2&gt;Questions?&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://docs.marimo.io/faq.html"&gt;FAQ&lt;/a&gt; at our docs.&lt;/p&gt; 
&lt;h2&gt;Learn more&lt;/h2&gt; 
&lt;p&gt;marimo is easy to get started with, with lots of room for power users. For example, here's an embedding visualizer made in marimo (&lt;a href="https://marimo.io/videos/landing/full.mp4"&gt;video&lt;/a&gt;):&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/embedding.gif" width="700px" /&gt; 
&lt;p&gt;Check out our &lt;a href="https://docs.marimo.io"&gt;docs&lt;/a&gt;, &lt;a href="https://docs.marimo.io/examples/"&gt;usage examples&lt;/a&gt;, and our &lt;a href="https://marimo.io/gallery"&gt;gallery&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;table border="0"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/getting_started/key_concepts.html"&gt; &lt;img src="https://docs.marimo.io/_static/reactive.gif" style="max-height: 150px; width: auto; display: block" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/api/inputs/index.html"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif" style="max-height: 150px; width: auto; display: block" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/guides/working_with_data/plotting.html"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-intro.gif" style="max-height: 150px; width: auto; display: block" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/api/layouts/index.html"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/outputs.gif" style="max-height: 150px; width: auto; display: block" /&gt; &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/getting_started/key_concepts.html"&gt; Tutorial &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/api/inputs/index.html"&gt; Inputs &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/guides/working_with_data/plotting.html"&gt; Plots &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/api/layouts/index.html"&gt; Layout &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://marimo.app/l/c7h6pz"&gt; &lt;img src="https://marimo.io/shield.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://marimo.app/l/0ue871"&gt; &lt;img src="https://marimo.io/shield.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://marimo.app/l/lxp1jk"&gt; &lt;img src="https://marimo.io/shield.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://marimo.app/l/14ovyr"&gt; &lt;img src="https://marimo.io/shield.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We appreciate all contributions! You don't need to be an expert to help out. Please see &lt;a href="https://github.com/marimo-team/marimo/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more details on how to get started.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Questions? Reach out to us &lt;a href="https://marimo.io/discord?ref=readme"&gt;on Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;We're building a community. Come hang out with us!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üåü &lt;a href="https://github.com/marimo-team/marimo"&gt;Star us on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://marimo.io/discord?ref=readme"&gt;Chat with us on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìß &lt;a href="https://marimo.io/newsletter"&gt;Subscribe to our Newsletter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚òÅÔ∏è &lt;a href="https://marimo.io/cloud"&gt;Join our Cloud Waitlist&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úèÔ∏è &lt;a href="https://github.com/marimo-team/marimo/discussions"&gt;Start a GitHub Discussion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ü¶ã &lt;a href="https://bsky.app/profile/marimo.io"&gt;Follow us on Bluesky&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üê¶ &lt;a href="https://twitter.com/marimo_io"&gt;Follow us on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üé• &lt;a href="https://www.youtube.com/@marimo-team"&gt;Subscribe on YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üï¥Ô∏è &lt;a href="https://www.linkedin.com/company/marimo-io"&gt;Follow us on LinkedIn&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;A NumFOCUS affiliated project.&lt;/strong&gt; marimo is a core part of the broader Python ecosystem and is a member of the NumFOCUS community, which includes projects such as NumPy, SciPy, and Matplotlib.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/numfocus_affiliated_project.png" height="40px" /&gt; 
&lt;h2&gt;Inspiration ‚ú®&lt;/h2&gt; 
&lt;p&gt;marimo is a &lt;strong&gt;reinvention&lt;/strong&gt; of the Python notebook as a reproducible, interactive, and shareable Python program, instead of an error-prone JSON scratchpad.&lt;/p&gt; 
&lt;p&gt;We believe that the tools we use shape the way we think ‚Äî better tools, for better minds. With marimo, we hope to provide the Python community with a better programming environment to do research and communicate it; to experiment with code and share it; to learn computational science and teach it.&lt;/p&gt; 
&lt;p&gt;Our inspiration comes from many places and projects, especially &lt;a href="https://github.com/fonsp/Pluto.jl"&gt;Pluto.jl&lt;/a&gt;, &lt;a href="https://observablehq.com/tutorials"&gt;ObservableHQ&lt;/a&gt;, and &lt;a href="http://worrydream.com/"&gt;Bret Victor's essays&lt;/a&gt;. marimo is part of a greater movement toward reactive dataflow programming. From &lt;a href="https://github.com/ipyflow/ipyflow"&gt;IPyflow&lt;/a&gt;, &lt;a href="https://github.com/streamlit/streamlit"&gt;streamlit&lt;/a&gt;, &lt;a href="https://github.com/tensorflow/tensorflow"&gt;TensorFlow&lt;/a&gt;, &lt;a href="https://github.com/pytorch/pytorch/tree/main"&gt;PyTorch&lt;/a&gt;, &lt;a href="https://github.com/google/jax"&gt;JAX&lt;/a&gt;, and &lt;a href="https://github.com/facebook/react"&gt;React&lt;/a&gt;, the ideas of functional, declarative, and reactive programming are transforming a broad range of tools for the better.&lt;/p&gt; 
&lt;p align="right"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-horizontal.png" height="200px" /&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AI4Finance-Foundation/FinRL-Trading</title>
      <link>https://github.com/AI4Finance-Foundation/FinRL-Trading</link>
      <description>&lt;p&gt;For trading. Please star.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img align="center" width="30%" alt="image" src="https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;FinRL Trading Platform v2.0&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/80fe89bb-fb09-4267-b29a-76030512f8cf" width="500" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pepy.tech/project/finrl-trading"&gt;&lt;img src="https://static.pepy.tech/badge/finrl-trading" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/finrl-trading"&gt;&lt;img src="https://static.pepy.tech/badge/finrl-trading/week" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/trsr8SXpW5"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-blue" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.11-blue.svg?sanitize=true" alt="Python 3.11" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/finrl-trading/"&gt;&lt;img src="https://img.shields.io/pypi/v/finrl-trading.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/license/AI4Finance-Foundation/FinRL-Trading.svg?color=brightgreen" alt="License" /&gt; &lt;img src="https://img.shields.io/github/issues-raw/AI4Finance-Foundation/FinRL-Trading?label=Issues" alt="" /&gt; &lt;img src="https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/FinRL-Trading?label=Closed+Issues" alt="" /&gt; &lt;img src="https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/FinRL-Trading?label=Open+PRs" alt="" /&gt; &lt;img src="https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/FinRL-Trading?label=Closed+PRs" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&amp;amp;repo=FinRL-Trading&amp;amp;countColor=%23B17A" alt="Visitors" /&gt; &lt;a href="https://discord.gg/trsr8SXpW5"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/trsr8SXpW5?cb=1" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A modern, modular quantitative trading platform built with Python, featuring machine learning strategies, professional backtesting, and live trading capabilities.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Strategy Framework&lt;/strong&gt;: Multiple quantitative strategies including ML-based stock selection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìà Risk Management&lt;/strong&gt;: Comprehensive risk controls and position limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí∞ Live Trading&lt;/strong&gt;: Alpaca integration with paper and live trading support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Modular Design&lt;/strong&gt;: Clean, extensible architecture following best practices&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üèóÔ∏è Project Architecture&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;finrl-trading/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config/           # Centralized configuration management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py   # Pydantic-based settings with environment variables
‚îÇ   ‚îú‚îÄ‚îÄ data/            # Data acquisition and processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_fetcher.py     # Multi-source data integration (Yahoo/FMP/WRDS)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_processor.py   # Data cleaning and feature engineering
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_store.py       # SQLite-based data persistence
‚îÇ   ‚îú‚îÄ‚îÄ backtest/      # Backtesting system
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backtest_engine.py  # Professional backtesting engine powered by bt library
‚îÇ   ‚îú‚îÄ‚îÄ strategies/      # Trading strategies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_strategy.py    # Abstract strategy framework
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ml_strategy.py      # ML-based stock selection
‚îÇ   ‚îú‚îÄ‚îÄ trading/         # Live trading execution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alpaca_manager.py     # Alpaca API integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trade_executor.py     # Order execution and risk management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ performance_analyzer.py  # Performance analysis
‚îÇ   ‚îî‚îÄ‚îÄ main.py         # CLI entry point
‚îú‚îÄ‚îÄ examples/           # Examples and tutorials
‚îÇ   ‚îú‚îÄ‚îÄ FinRL_Full_Workflow.ipynb  # Complete workflow tutorial (recommended)
‚îÇ   ‚îî‚îÄ‚îÄ README.md       # Examples documentation
‚îú‚îÄ‚îÄ data/               # Runtime data storage (gitignored)
‚îú‚îÄ‚îÄ logs/               # Application logs (gitignored)
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îî‚îÄ‚îÄ setup.py           # Package installation
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üõ†Ô∏è Installation &amp;amp; Setup&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.11+&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Alpaca Account&lt;/strong&gt; (for live trading)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Source APIs&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;FMP API Key&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/your-username/FinRL-Trading.git
cd FinRL-Trading
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure environment variables&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Copy configuration template
cp .env.example .env

# Edit .env file with your API keys
# Windows: notepad .env
# Linux/Mac: nano .env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run example tutorial&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Launch Jupyter Notebook (recommended starting point)
jupyter notebook examples/FinRL_Full_Workflow.ipynb
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Complete Example Tutorial&lt;/h3&gt; 
&lt;p&gt;The project includes a comprehensive interactive tutorial covering the entire workflow from data acquisition to live trading:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# View examples documentation
cat examples/README.md

# Run complete workflow tutorial (recommended)
jupyter notebook examples/FinRL_Full_Workflow.ipynb
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Tutorial Contents:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ S&amp;amp;P 500 components data acquisition&lt;/li&gt; 
 &lt;li&gt;‚úÖ Fundamental and historical price data fetching&lt;/li&gt; 
 &lt;li&gt;‚úÖ Machine learning stock selection strategy implementation&lt;/li&gt; 
 &lt;li&gt;‚úÖ Professional backtesting (with VOO/QQQ benchmark comparison)&lt;/li&gt; 
 &lt;li&gt;‚úÖ Alpaca Paper Trading execution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìñ Usage Examples&lt;/h2&gt; 
&lt;h3&gt;Data Acquisition&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from src.data.data_fetcher import get_data_manager

# Initialize data manager (automatically selects best available source)
manager = get_data_manager()

# Check current data source
info = manager.get_source_info()
print(f"Current data source: {info['current_source']}")
print(f"Available sources: {info['available_sources']}")

# Get S&amp;amp;P 500 components
components = manager.get_sp500_components()

# Fetch fundamental data
tickers = ['AAPL', 'MSFT', 'GOOGL']
fundamentals = manager.get_fundamental_data(
    tickers, '2020-01-01', '2023-12-31'
)

# Fetch historical price data
prices = manager.get_price_data(
    tickers, '2020-01-01', '2023-12-31'
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Strategy Development&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from src.strategies.ml_strategy import MLStockSelectorStrategy
from src.strategies.base_strategy import StrategyConfig

# Create ML-based stock selection strategy
config = StrategyConfig(
    name="ML Stock Selector",
    parameters={
        'model_type': 'random_forest',
        'top_n': 30,
        'sector_neutral': True
    },
    risk_limits={'max_weight': 0.1}
)

strategy = MLStockSelectorStrategy(config)

# Generate portfolio weights
data = {
    'fundamentals': fundamentals,
    'prices': prices
}
result = strategy.generate_weights(data)
print(result.weights.head())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Strategy Backtesting&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from src.backtest.backtest_engine import BacktestEngine, BacktestConfig

# Configure backtest parameters
backtest_config = BacktestConfig(
    start_date='2020-01-01',
    end_date='2023-12-31',
    initial_capital=1000000,
    rebalance_freq='Q',  # Quarterly rebalancing
    transaction_cost=0.001,  # 0.1% transaction cost
    benchmark_tickers=['VOO', 'QQQ']  # Benchmark comparison
)

# Run backtest
engine = BacktestEngine(backtest_config)
result = engine.run_backtest(
    strategy_name="ML Stock Selector",
    weight_signals=ml_weights,
    price_data=prices
)

# View backtest results
print(f"Total Return: {result.metrics['total_return']:.2%}")
print(f"Annualized Return: {result.annualized_return:.2%}")
print(f"Sharpe Ratio: {result.metrics['sharpe_ratio']:.2f}")
print(f"Max Drawdown: {result.metrics['max_drawdown']:.2%}")

# Generate visualization report
engine.plot_results(result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Live Trading&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from src.trading.alpaca_manager import create_alpaca_account_from_env, AlpacaManager
from src.trading.trade_executor import TradeExecutor, ExecutionConfig

# Connect to Alpaca
account = create_alpaca_account_from_env()
alpaca_manager = AlpacaManager([account])

# Configure execution settings
exec_config = ExecutionConfig(
    max_order_value=100000,
    risk_checks_enabled=True
)

executor = TradeExecutor(alpaca_manager, exec_config)

# Execute portfolio rebalance
target_weights = {'AAPL': 0.3, 'MSFT': 0.3, 'GOOGL': 0.4}
result = executor.execute_portfolio_rebalance(target_weights)

print(f"Orders placed: {len(result.orders_placed)}")
print(f"Execution success: {result.success}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üéØ Core Components&lt;/h2&gt; 
&lt;h3&gt;Data Layer (&lt;code&gt;src/data/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Source Data Manager&lt;/strong&gt; (&lt;code&gt;data_fetcher.py&lt;/code&gt;): Intelligent data source selection and management 
  &lt;ul&gt; 
   &lt;li&gt;Yahoo Finance: Free financial data (default)&lt;/li&gt; 
   &lt;li&gt;FMP (Financial Modeling Prep): High-quality paid data (requires API Key)&lt;/li&gt; 
   &lt;li&gt;WRDS: Academic database (requires credentials)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Processor&lt;/strong&gt; (&lt;code&gt;data_processor.py&lt;/code&gt;): Feature engineering, data cleaning, and quality checks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Storage&lt;/strong&gt; (&lt;code&gt;data_store.py&lt;/code&gt;): SQLite-based data persistence with caching and version control&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Strategy Framework (&lt;code&gt;src/strategies/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Base Strategy&lt;/strong&gt; (&lt;code&gt;base_strategy.py&lt;/code&gt;): Abstract framework for custom strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ML Strategy&lt;/strong&gt; (&lt;code&gt;ml_strategy.py&lt;/code&gt;): Random Forest-based stock selection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Implemented Strategies:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Equal Weight Strategy&lt;/li&gt; 
 &lt;li&gt;Market Cap Weighted Strategy&lt;/li&gt; 
 &lt;li&gt;ML-based Stock Selection&lt;/li&gt; 
 &lt;li&gt;Sector Neutral ML Strategy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Backtesting System (&lt;code&gt;src/backtest/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Professional Backtesting Engine&lt;/strong&gt; (&lt;code&gt;backtest_engine.py&lt;/code&gt;): Powered by &lt;code&gt;bt&lt;/code&gt; library 
  &lt;ul&gt; 
   &lt;li&gt;Comprehensive performance and risk analysis&lt;/li&gt; 
   &lt;li&gt;Multiple benchmark comparison (SPY, VOO, QQQ, etc.)&lt;/li&gt; 
   &lt;li&gt;Transaction cost simulation&lt;/li&gt; 
   &lt;li&gt;Visualization report generation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Trading System (&lt;code&gt;src/trading/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Alpaca Integration&lt;/strong&gt; (&lt;code&gt;alpaca_manager.py&lt;/code&gt;): Alpaca API client with multi-account support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Trade Executor&lt;/strong&gt; (&lt;code&gt;trade_executor.py&lt;/code&gt;): Order management and risk controls&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Analyzer&lt;/strong&gt; (&lt;code&gt;performance_analyzer.py&lt;/code&gt;): Real-time position tracking and P&amp;amp;L calculation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Configuration System (&lt;code&gt;src/config/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pydantic Settings&lt;/strong&gt; (&lt;code&gt;settings.py&lt;/code&gt;): Type-safe configuration with environment variables&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-environment Support&lt;/strong&gt;: Development, testing, production configurations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Centralized Management&lt;/strong&gt;: All settings in one place&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîß Configuration&lt;/h2&gt; 
&lt;p&gt;The platform uses &lt;strong&gt;Pydantic-based settings&lt;/strong&gt; with environment variable support:&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file and configure the following variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Application
ENVIRONMENT=development
APP_NAME="FinRL Trading"

# Alpaca API (Required for live trading)
APCA_API_KEY=your_alpaca_key
APCA_API_SECRET=your_alpaca_secret
APCA_BASE_URL=https://paper-api.alpaca.markets  # Paper Trading

# Data Sources (Optional, prioritized: FMP &amp;gt; WRDS &amp;gt; Yahoo)
FMP_API_KEY=your_fmp_api_key           # Financial Modeling Prep


# Risk Management
TRADING_MAX_ORDER_VALUE=100000         # Maximum order value
TRADING_MAX_PORTFOLIO_TURNOVER=0.5     # Maximum portfolio turnover
STRATEGY_MAX_WEIGHT_PER_STOCK=0.1      # Maximum weight per stock

# Data Management
DATA_CACHE_TTL_HOURS=24                # Cache TTL in hours
DATA_MAX_CACHE_SIZE_MB=1000            # Maximum cache size in MB
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuration Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from src.config.settings import get_config

config = get_config()
print(f"Environment: {config.environment}")
print(f"Database: {config.database.url}")
print(f"Risk Limits: {config.trading.max_order_value}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìä Performance Metrics&lt;/h2&gt; 
&lt;p&gt;The backtesting engine provides comprehensive quantitative analysis:&lt;/p&gt; 
&lt;h3&gt;Return Metrics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Total Return&lt;/strong&gt;: Cumulative portfolio performance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Annualized Return&lt;/strong&gt;: Time-weighted annual performance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Alpha&lt;/strong&gt;: Excess return over benchmark&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Risk Metrics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Volatility&lt;/strong&gt;: Standard deviation of returns&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sharpe Ratio&lt;/strong&gt;: Risk-adjusted returns (Return √∑ Volatility)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sortino Ratio&lt;/strong&gt;: Downside risk-adjusted returns&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Maximum Drawdown&lt;/strong&gt;: Peak-to-trough decline&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calmar Ratio&lt;/strong&gt;: Return √∑ Maximum Drawdown&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tail Risk Measures&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Skewness &amp;amp; Kurtosis&lt;/strong&gt;: Return distribution characteristics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Benchmarking&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Information Ratio&lt;/strong&gt;: Active return √∑ Tracking error&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Beta&lt;/strong&gt;: Portfolio sensitivity to market&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tracking Error&lt;/strong&gt;: Standard deviation of active returns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! Please follow these guidelines:&lt;/p&gt; 
&lt;h3&gt;Development Workflow&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fork the repository&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create a feature branch&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git checkout -b feature/your-feature-name
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Install development dependencies&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
pip install pytest black flake8 mypy
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Make your changes&lt;/strong&gt; with proper testing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Commit and push&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git commit -m "Add: your feature description"
git push origin feature/your-feature-name
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open a Pull Request&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Code Standards&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Type Hints&lt;/strong&gt;: Use modern Python typing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Add docstrings to all public functions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing&lt;/strong&gt;: Write tests for new features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Style&lt;/strong&gt;: Follow PEP 8 with Black formatting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Adding New Strategies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from src.strategies.base_strategy import BaseStrategy, StrategyConfig, StrategyResult

class MyCustomStrategy(BaseStrategy):
    def generate_weights(self, data, **kwargs) -&amp;gt; StrategyResult:
        # Your strategy logic here
        pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìã Roadmap&lt;/h2&gt; 
&lt;h3&gt;Completed Features ‚úÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ Modular strategy framework&lt;/li&gt; 
 &lt;li&gt;‚úÖ ML-based stock selection strategies&lt;/li&gt; 
 &lt;li&gt;‚úÖ Professional backtesting system (powered by bt library)&lt;/li&gt; 
 &lt;li&gt;‚úÖ Alpaca live trading integration&lt;/li&gt; 
 &lt;li&gt;‚úÖ Multi-source data support (Yahoo/FMP/WRDS)&lt;/li&gt; 
 &lt;li&gt;‚úÖ Comprehensive risk management system&lt;/li&gt; 
 &lt;li&gt;‚úÖ Performance analysis and reporting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Planned Enhancements üöß&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîÑ Deep reinforcement learning strategies&lt;/li&gt; 
 &lt;li&gt;üîÑ Alternative data integration&lt;/li&gt; 
 &lt;li&gt;üîÑ Multi-asset support (crypto, futures)&lt;/li&gt; 
 &lt;li&gt;üîÑ Advanced portfolio optimization algorithms&lt;/li&gt; 
 &lt;li&gt;üîÑ Real-time alerting system&lt;/li&gt; 
 &lt;li&gt;üîÑ Web visualization interface&lt;/li&gt; 
 &lt;li&gt;üîÑ Docker containerization&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìù License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;strong&gt;Apache License 2.0&lt;/strong&gt; - see the &lt;a href="https://raw.githubusercontent.com/AI4Finance-Foundation/FinRL-Trading/master/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;‚ö†Ô∏è Important Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è NOT FINANCIAL ADVICE&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This software is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Always consult with qualified financial professionals before making investment decisions. Past performance does not guarantee future results.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üìö References &amp;amp; Acknowledgments&lt;/h2&gt; 
&lt;h3&gt;Academic Papers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3302088"&gt;Machine Learning for Stock Recommendation&lt;/a&gt; - Machine learning approaches to stock selection&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2011.09607"&gt;FinRL: Deep Reinforcement Learning Framework&lt;/a&gt; - Deep RL framework for quantitative trading&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3690996"&gt;Portfolio Allocation with Deep Reinforcement Learning&lt;/a&gt; - Portfolio optimization research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Open Source Projects&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AI4Finance-Foundation/FinRL"&gt;FinRL&lt;/a&gt; - Deep reinforcement learning framework for quantitative trading&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/alpacahq/alpaca-py"&gt;Alpaca-py&lt;/a&gt; - Alpaca trading API&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pmorissette/bt"&gt;bt&lt;/a&gt; - Flexible backtesting framework for Python&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Data Sources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://finance.yahoo.com/"&gt;Yahoo Finance&lt;/a&gt; - Free financial data&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://financialmodelingprep.com/"&gt;Financial Modeling Prep&lt;/a&gt; - Professional financial data API&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://wrds.wharton.upenn.edu/"&gt;WRDS (Wharton Research Data Services)&lt;/a&gt; - Academic financial database&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://alpaca.markets/"&gt;Alpaca Markets&lt;/a&gt; - Brokerage API and market data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Built with ‚ù§Ô∏è for the quantitative finance community&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=githubpages" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/TTS-Report-red?logo=arxiv" alt="TTS Report" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb"&gt;&lt;img src="https://img.shields.io/badge/StreamingTTS-Colab-green?logo=googlecolab" alt="Colab" /&gt;&lt;/a&gt; &lt;a href="https://aka.ms/vibevoice-asr"&gt;&lt;img src="https://img.shields.io/badge/ASR-Playground-6F42C1?logo=gradio" alt="ASR Playground" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;2026-01-21: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/a&gt;, a unified speech-to-text model designed to handle 60-minute long-form audio in a single pass, generating structured transcriptions containing Who (Speaker), When (Timestamps), and What (Content), with support for User-Customized Context. Try it in &lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‚≠êÔ∏è VibeVoice-ASR is natively multilingual, supporting over 100 languages ‚Äî check the &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md#language-distribution"&gt;supported languages&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;li&gt;üî• The VibeVoice-ASR &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;finetuning code&lt;/a&gt; is now available!&lt;/li&gt; 
  &lt;li&gt;‚ö°Ô∏è &lt;strong&gt;vLLM inference&lt;/strong&gt; is now supported for faster inference; see &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-vllm-asr.md"&gt;vllm-asr&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;2025-12-16: üì£ We added experimental speakers to &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt; for exploration, including multilingual voices in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) and 11 distinct English style voices. &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices"&gt;Try it&lt;/a&gt;. More speaker types will be added over time.&lt;/p&gt; 
 &lt;p&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have removed the VibeVoice-TTS code from this repository.&lt;/p&gt; 
 &lt;p&gt;2025-08-25: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;&lt;strong&gt;VibeVoice-TTS&lt;/strong&gt;&lt;/a&gt;, a long-form multi-speaker text-to-speech model that can synthesize speech up to 90 minutes long with up to 4 distinct speakers.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;VibeVoice is a &lt;strong&gt;family of open-source frontier voice AI models&lt;/strong&gt; that includes both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) models.&lt;/p&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of &lt;strong&gt;7.5 Hz&lt;/strong&gt;. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p&gt;For more information, demos, and examples, please visit our &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Weight&lt;/th&gt; 
    &lt;th&gt;Quick Try&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-ASR-7B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-TTS-1.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Disabled&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-Realtime-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;h3&gt;1. üìñ &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;VibeVoice-ASR&lt;/a&gt; - Long-form Speech Recognition&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt; is a unified speech-to-text model designed to handle &lt;strong&gt;60-minute long-form audio&lt;/strong&gt; in a single pass, generating structured transcriptions containing &lt;strong&gt;Who (Speaker), When (Timestamps), and What (Content)&lt;/strong&gt;, with support for &lt;strong&gt;Customized Hotwords&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üïí 60-minute Single-Pass Processing&lt;/strong&gt;: Unlike conventional ASR models that slice audio into short chunks (often losing global context), VibeVoice ASR accepts up to &lt;strong&gt;60 minutes&lt;/strong&gt; of continuous audio input within 64K token length. This ensures consistent speaker tracking and semantic coherence across the entire hour.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë§ Customized Hotwords&lt;/strong&gt;: Users can provide customized hotwords (e.g., specific names, technical terms, or background info) to guide the recognition process, significantly improving accuracy on domain-specific content.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Rich Transcription (Who, When, What)&lt;/strong&gt;: The model jointly performs ASR, diarization, and timestamping, producing a structured output that indicates &lt;em&gt;who&lt;/em&gt; said &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;when&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://aka.ms/vibevoice-asr"&gt;üéÆ Playground&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;üõ†Ô∏è Finetuning&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/DER.jpg" alt="DER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/cpWER.jpg" alt="cpWER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/tcpWER.jpg" alt="tcpWER" width="50%" /&gt; &lt;/p&gt; 
&lt;div align="center" id="vibevoice-asr"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa"&gt;https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;2. üéôÔ∏è &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;VibeVoice-TTS&lt;/a&gt; - Long-form Multi-speaker TTS&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Best for&lt;/strong&gt;: Long-form conversational audio, podcasts, multi-speaker dialogues&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚è±Ô∏è 90-minute Long-form Generation&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; in a single pass, maintaining speaker consistency and semantic coherence throughout.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë• Multi-speaker Support&lt;/strong&gt;: Supports up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt; in a single conversation, with natural turn-taking and speaker consistency across long dialogues.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üé≠ Expressive Speech&lt;/strong&gt;: Generates expressive, natural-sounding speech that captures conversational dynamics and emotional nuances.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Multi-lingual Support&lt;/strong&gt;: Supports English, Chinese and other languages.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;üìä Paper&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice-TTS-results.jpg" alt="VibeVoice Results" width="80%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;3. ‚ö° &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;VibeVoice-Streaming&lt;/a&gt; - Real-time Streaming TTS&lt;/h3&gt; 
&lt;p&gt;VibeVoice-Realtime is a &lt;strong&gt;lightweight real‚Äëtime&lt;/strong&gt; text-to-speech model supporting &lt;strong&gt;streaming text input&lt;/strong&gt; and &lt;strong&gt;robust long-form speech generation&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Parameter size: 0.5B (deployment-friendly)&lt;/li&gt; 
 &lt;li&gt;Real-time TTS (~300 milliseconds first audible latency)&lt;/li&gt; 
 &lt;li&gt;Streaming text input&lt;/li&gt; 
 &lt;li&gt;Robust long-form speech generation (~10 minutes)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;üöÄ Colab&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center" id="generated-example-audio-vibevoice-realtime"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;‚ö†Ô∏è Risks and Limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>roboflow/rf-detr</title>
      <link>https://github.com/roboflow/rf-detr</link>
      <description>&lt;p&gt;RF-DETR is a real-time object detection and segmentation model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RF-DETR: Real-Time SOTA Detection and Segmentation&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/py/rfdetr"&gt;&lt;img src="https://badge.fury.io/py/rfdetr.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/rfdetr"&gt;&lt;img src="https://img.shields.io/pypi/dm/rfdetr" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2511.09554"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2511.09554-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/rfdetr"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/rfdetr" alt="python-version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/roboflow/rfdetr/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/SkalskiP/RF-DETR"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="hf space" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="colab" /&gt;&lt;/a&gt; &lt;a href="https://blog.roboflow.com/rf-detr"&gt;&lt;img src="https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true" alt="roboflow" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/GbfgXGJ8Bk"&gt;&lt;img src="https://img.shields.io/discord/1159501506232451173?logo=discord&amp;amp;label=discord&amp;amp;labelColor=fff&amp;amp;color=5865f2&amp;amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk" alt="discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;RF-DETR is a real-time transformer architecture for object detection and instance segmentation developed by Roboflow. Built on a DINOv2 vision transformer backbone, RF-DETR delivers state-of-the-art accuracy and latency trade-offs on &lt;a href="https://cocodataset.org/#home"&gt;Microsoft COCO&lt;/a&gt; and &lt;a href="https://github.com/roboflow/rf100-vl"&gt;RF100-VL&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;RF-DETR uses a DINOv2 vision transformer backbone and supports both detection and instance segmentation in a single, consistent API. All core models and code are released under the Apache 2.0 license.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/add23fd1-266f-4538-8809-d7dd5767e8e6"&gt;https://github.com/user-attachments/assets/add23fd1-266f-4538-8809-d7dd5767e8e6&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;To install RF-DETR, install the &lt;code&gt;rfdetr&lt;/code&gt; package in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.10&lt;/strong&gt;&lt;/a&gt; environment with &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install rfdetr
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Install from source&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;By installing RF-DETR from source, you can explore the most recent features and enhancements that have not yet been officially released. &lt;strong&gt;Please note that these updates are still in development and may not be as stable as the latest published release.&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install https://github.com/roboflow/rf-detr/archive/refs/heads/develop.zip
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;RF-DETR achieves state-of-the-art results in both object detection and instance segmentation, with benchmarks reported on Microsoft COCO and RF100-VL. The charts and tables below compare RF-DETR against other top real-time models across accuracy and latency for detection and segmentation. All latency numbers were measured on an NVIDIA T4 using TensorRT, FP16, and batch size 1. For full benchmarking methodology and reproducibility details, see &lt;a href="https://github.com/roboflow/single_artifact_benchmarking"&gt;roboflow/sab&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Detection&lt;/h3&gt; 
&lt;img alt="rf_detr_1-4_latency_accuracy_object_detection" src="https://storage.googleapis.com/com-roboflow-marketing/rf-detr/rf_detr_1-4_latency_accuracy_object_detection.png" /&gt; 
&lt;details&gt; 
 &lt;summary&gt;See object detection benchmark numbers&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Architecture&lt;/th&gt; 
    &lt;th align="center"&gt;COCO AP&lt;sub&gt;50&lt;/sub&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;COCO AP&lt;sub&gt;50:95&lt;/sub&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;RF100VL AP&lt;sub&gt;50&lt;/sub&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;RF100VL AP&lt;sub&gt;50:95&lt;/sub&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;Latency (ms)&lt;/th&gt; 
    &lt;th align="center"&gt;Params (M)&lt;/th&gt; 
    &lt;th align="center"&gt;Resolution&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-N&lt;/td&gt; 
    &lt;td align="center"&gt;67.6&lt;/td&gt; 
    &lt;td align="center"&gt;48.4&lt;/td&gt; 
    &lt;td align="center"&gt;85.0&lt;/td&gt; 
    &lt;td align="center"&gt;57.7&lt;/td&gt; 
    &lt;td align="center"&gt;2.3&lt;/td&gt; 
    &lt;td align="center"&gt;30.5&lt;/td&gt; 
    &lt;td align="center"&gt;384x384&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-S&lt;/td&gt; 
    &lt;td align="center"&gt;72.1&lt;/td&gt; 
    &lt;td align="center"&gt;53.0&lt;/td&gt; 
    &lt;td align="center"&gt;86.7&lt;/td&gt; 
    &lt;td align="center"&gt;60.2&lt;/td&gt; 
    &lt;td align="center"&gt;3.5&lt;/td&gt; 
    &lt;td align="center"&gt;32.1&lt;/td&gt; 
    &lt;td align="center"&gt;512x512&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-M&lt;/td&gt; 
    &lt;td align="center"&gt;73.6&lt;/td&gt; 
    &lt;td align="center"&gt;54.7&lt;/td&gt; 
    &lt;td align="center"&gt;87.4&lt;/td&gt; 
    &lt;td align="center"&gt;61.2&lt;/td&gt; 
    &lt;td align="center"&gt;4.4&lt;/td&gt; 
    &lt;td align="center"&gt;33.7&lt;/td&gt; 
    &lt;td align="center"&gt;576x576&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-L&lt;/td&gt; 
    &lt;td align="center"&gt;75.1&lt;/td&gt; 
    &lt;td align="center"&gt;56.5&lt;/td&gt; 
    &lt;td align="center"&gt;88.2&lt;/td&gt; 
    &lt;td align="center"&gt;62.2&lt;/td&gt; 
    &lt;td align="center"&gt;6.8&lt;/td&gt; 
    &lt;td align="center"&gt;33.9&lt;/td&gt; 
    &lt;td align="center"&gt;704x704&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-XL&lt;/td&gt; 
    &lt;td align="center"&gt;77.4&lt;/td&gt; 
    &lt;td align="center"&gt;58.6&lt;/td&gt; 
    &lt;td align="center"&gt;88.5&lt;/td&gt; 
    &lt;td align="center"&gt;62.9&lt;/td&gt; 
    &lt;td align="center"&gt;11.5&lt;/td&gt; 
    &lt;td align="center"&gt;126.4&lt;/td&gt; 
    &lt;td align="center"&gt;700x700&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-2XL&lt;/td&gt; 
    &lt;td align="center"&gt;78.5&lt;/td&gt; 
    &lt;td align="center"&gt;60.1&lt;/td&gt; 
    &lt;td align="center"&gt;89.0&lt;/td&gt; 
    &lt;td align="center"&gt;63.2&lt;/td&gt; 
    &lt;td align="center"&gt;17.2&lt;/td&gt; 
    &lt;td align="center"&gt;126.9&lt;/td&gt; 
    &lt;td align="center"&gt;880x880&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO11-N&lt;/td&gt; 
    &lt;td align="center"&gt;52.0&lt;/td&gt; 
    &lt;td align="center"&gt;37.4&lt;/td&gt; 
    &lt;td align="center"&gt;81.4&lt;/td&gt; 
    &lt;td align="center"&gt;55.3&lt;/td&gt; 
    &lt;td align="center"&gt;2.5&lt;/td&gt; 
    &lt;td align="center"&gt;2.6&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO11-S&lt;/td&gt; 
    &lt;td align="center"&gt;59.7&lt;/td&gt; 
    &lt;td align="center"&gt;44.4&lt;/td&gt; 
    &lt;td align="center"&gt;82.3&lt;/td&gt; 
    &lt;td align="center"&gt;56.2&lt;/td&gt; 
    &lt;td align="center"&gt;3.2&lt;/td&gt; 
    &lt;td align="center"&gt;9.4&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO11-M&lt;/td&gt; 
    &lt;td align="center"&gt;64.1&lt;/td&gt; 
    &lt;td align="center"&gt;48.6&lt;/td&gt; 
    &lt;td align="center"&gt;82.5&lt;/td&gt; 
    &lt;td align="center"&gt;56.5&lt;/td&gt; 
    &lt;td align="center"&gt;5.1&lt;/td&gt; 
    &lt;td align="center"&gt;20.1&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO11-L&lt;/td&gt; 
    &lt;td align="center"&gt;64.9&lt;/td&gt; 
    &lt;td align="center"&gt;49.9&lt;/td&gt; 
    &lt;td align="center"&gt;82.2&lt;/td&gt; 
    &lt;td align="center"&gt;56.5&lt;/td&gt; 
    &lt;td align="center"&gt;6.5&lt;/td&gt; 
    &lt;td align="center"&gt;25.3&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO11-X&lt;/td&gt; 
    &lt;td align="center"&gt;66.1&lt;/td&gt; 
    &lt;td align="center"&gt;50.9&lt;/td&gt; 
    &lt;td align="center"&gt;81.7&lt;/td&gt; 
    &lt;td align="center"&gt;56.2&lt;/td&gt; 
    &lt;td align="center"&gt;10.5&lt;/td&gt; 
    &lt;td align="center"&gt;56.9&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-N&lt;/td&gt; 
    &lt;td align="center"&gt;55.8&lt;/td&gt; 
    &lt;td align="center"&gt;40.3&lt;/td&gt; 
    &lt;td align="center"&gt;76.7&lt;/td&gt; 
    &lt;td align="center"&gt;52.0&lt;/td&gt; 
    &lt;td align="center"&gt;1.7&lt;/td&gt; 
    &lt;td align="center"&gt;2.6&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-S&lt;/td&gt; 
    &lt;td align="center"&gt;64.3&lt;/td&gt; 
    &lt;td align="center"&gt;47.7&lt;/td&gt; 
    &lt;td align="center"&gt;82.7&lt;/td&gt; 
    &lt;td align="center"&gt;57.0&lt;/td&gt; 
    &lt;td align="center"&gt;2.6&lt;/td&gt; 
    &lt;td align="center"&gt;9.4&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-M&lt;/td&gt; 
    &lt;td align="center"&gt;69.7&lt;/td&gt; 
    &lt;td align="center"&gt;52.5&lt;/td&gt; 
    &lt;td align="center"&gt;84.4&lt;/td&gt; 
    &lt;td align="center"&gt;58.7&lt;/td&gt; 
    &lt;td align="center"&gt;4.4&lt;/td&gt; 
    &lt;td align="center"&gt;20.1&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-L&lt;/td&gt; 
    &lt;td align="center"&gt;71.1&lt;/td&gt; 
    &lt;td align="center"&gt;54.1&lt;/td&gt; 
    &lt;td align="center"&gt;85.0&lt;/td&gt; 
    &lt;td align="center"&gt;59.3&lt;/td&gt; 
    &lt;td align="center"&gt;5.7&lt;/td&gt; 
    &lt;td align="center"&gt;25.3&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-X&lt;/td&gt; 
    &lt;td align="center"&gt;74.0&lt;/td&gt; 
    &lt;td align="center"&gt;56.9&lt;/td&gt; 
    &lt;td align="center"&gt;85.6&lt;/td&gt; 
    &lt;td align="center"&gt;60.0&lt;/td&gt; 
    &lt;td align="center"&gt;9.6&lt;/td&gt; 
    &lt;td align="center"&gt;56.9&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;LW-DETR-T&lt;/td&gt; 
    &lt;td align="center"&gt;60.7&lt;/td&gt; 
    &lt;td align="center"&gt;42.9&lt;/td&gt; 
    &lt;td align="center"&gt;84.7&lt;/td&gt; 
    &lt;td align="center"&gt;57.1&lt;/td&gt; 
    &lt;td align="center"&gt;1.9&lt;/td&gt; 
    &lt;td align="center"&gt;12.1&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;LW-DETR-S&lt;/td&gt; 
    &lt;td align="center"&gt;66.8&lt;/td&gt; 
    &lt;td align="center"&gt;48.0&lt;/td&gt; 
    &lt;td align="center"&gt;85.0&lt;/td&gt; 
    &lt;td align="center"&gt;57.4&lt;/td&gt; 
    &lt;td align="center"&gt;2.6&lt;/td&gt; 
    &lt;td align="center"&gt;14.6&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;LW-DETR-M&lt;/td&gt; 
    &lt;td align="center"&gt;72.0&lt;/td&gt; 
    &lt;td align="center"&gt;52.6&lt;/td&gt; 
    &lt;td align="center"&gt;86.8&lt;/td&gt; 
    &lt;td align="center"&gt;59.8&lt;/td&gt; 
    &lt;td align="center"&gt;4.4&lt;/td&gt; 
    &lt;td align="center"&gt;28.2&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;LW-DETR-L&lt;/td&gt; 
    &lt;td align="center"&gt;74.6&lt;/td&gt; 
    &lt;td align="center"&gt;56.1&lt;/td&gt; 
    &lt;td align="center"&gt;87.4&lt;/td&gt; 
    &lt;td align="center"&gt;61.5&lt;/td&gt; 
    &lt;td align="center"&gt;6.9&lt;/td&gt; 
    &lt;td align="center"&gt;46.8&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;LW-DETR-X&lt;/td&gt; 
    &lt;td align="center"&gt;76.9&lt;/td&gt; 
    &lt;td align="center"&gt;58.3&lt;/td&gt; 
    &lt;td align="center"&gt;87.9&lt;/td&gt; 
    &lt;td align="center"&gt;62.1&lt;/td&gt; 
    &lt;td align="center"&gt;13.0&lt;/td&gt; 
    &lt;td align="center"&gt;118.0&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;D-FINE-N&lt;/td&gt; 
    &lt;td align="center"&gt;60.2&lt;/td&gt; 
    &lt;td align="center"&gt;42.7&lt;/td&gt; 
    &lt;td align="center"&gt;84.4&lt;/td&gt; 
    &lt;td align="center"&gt;58.2&lt;/td&gt; 
    &lt;td align="center"&gt;2.1&lt;/td&gt; 
    &lt;td align="center"&gt;3.8&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;D-FINE-S&lt;/td&gt; 
    &lt;td align="center"&gt;67.6&lt;/td&gt; 
    &lt;td align="center"&gt;50.6&lt;/td&gt; 
    &lt;td align="center"&gt;85.3&lt;/td&gt; 
    &lt;td align="center"&gt;60.3&lt;/td&gt; 
    &lt;td align="center"&gt;3.5&lt;/td&gt; 
    &lt;td align="center"&gt;10.2&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;D-FINE-M&lt;/td&gt; 
    &lt;td align="center"&gt;72.6&lt;/td&gt; 
    &lt;td align="center"&gt;55.0&lt;/td&gt; 
    &lt;td align="center"&gt;85.5&lt;/td&gt; 
    &lt;td align="center"&gt;60.6&lt;/td&gt; 
    &lt;td align="center"&gt;5.4&lt;/td&gt; 
    &lt;td align="center"&gt;19.2&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;D-FINE-L&lt;/td&gt; 
    &lt;td align="center"&gt;74.9&lt;/td&gt; 
    &lt;td align="center"&gt;57.2&lt;/td&gt; 
    &lt;td align="center"&gt;86.4&lt;/td&gt; 
    &lt;td align="center"&gt;61.6&lt;/td&gt; 
    &lt;td align="center"&gt;7.5&lt;/td&gt; 
    &lt;td align="center"&gt;31.0&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;D-FINE-X&lt;/td&gt; 
    &lt;td align="center"&gt;76.8&lt;/td&gt; 
    &lt;td align="center"&gt;59.3&lt;/td&gt; 
    &lt;td align="center"&gt;86.9&lt;/td&gt; 
    &lt;td align="center"&gt;62.2&lt;/td&gt; 
    &lt;td align="center"&gt;11.5&lt;/td&gt; 
    &lt;td align="center"&gt;62.0&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Segmentation&lt;/h3&gt; 
&lt;img alt="rf_detr_1-4_latency_accuracy_instance_segmentation" src="https://storage.googleapis.com/com-roboflow-marketing/rf-detr/rf_detr_1-4_latency_accuracy_instance_segmentation.png" /&gt; 
&lt;details&gt; 
 &lt;summary&gt;See instance segmentation benchmark numbers&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Architecture&lt;/th&gt; 
    &lt;th align="center"&gt;COCO AP&lt;sub&gt;50&lt;/sub&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;COCO AP&lt;sub&gt;50:95&lt;/sub&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;Latency (ms)&lt;/th&gt; 
    &lt;th align="center"&gt;Params (M)&lt;/th&gt; 
    &lt;th align="center"&gt;Resolution&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-Seg-N&lt;/td&gt; 
    &lt;td align="center"&gt;63.0&lt;/td&gt; 
    &lt;td align="center"&gt;40.3&lt;/td&gt; 
    &lt;td align="center"&gt;3.4&lt;/td&gt; 
    &lt;td align="center"&gt;33.6&lt;/td&gt; 
    &lt;td align="center"&gt;312x312&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-Seg-S&lt;/td&gt; 
    &lt;td align="center"&gt;66.2&lt;/td&gt; 
    &lt;td align="center"&gt;43.1&lt;/td&gt; 
    &lt;td align="center"&gt;4.4&lt;/td&gt; 
    &lt;td align="center"&gt;33.7&lt;/td&gt; 
    &lt;td align="center"&gt;384x384&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-Seg-M&lt;/td&gt; 
    &lt;td align="center"&gt;68.4&lt;/td&gt; 
    &lt;td align="center"&gt;45.3&lt;/td&gt; 
    &lt;td align="center"&gt;5.9&lt;/td&gt; 
    &lt;td align="center"&gt;35.7&lt;/td&gt; 
    &lt;td align="center"&gt;432x432&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-Seg-L&lt;/td&gt; 
    &lt;td align="center"&gt;70.5&lt;/td&gt; 
    &lt;td align="center"&gt;47.1&lt;/td&gt; 
    &lt;td align="center"&gt;8.8&lt;/td&gt; 
    &lt;td align="center"&gt;36.2&lt;/td&gt; 
    &lt;td align="center"&gt;504x504&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-Seg-XL&lt;/td&gt; 
    &lt;td align="center"&gt;72.2&lt;/td&gt; 
    &lt;td align="center"&gt;48.8&lt;/td&gt; 
    &lt;td align="center"&gt;13.5&lt;/td&gt; 
    &lt;td align="center"&gt;38.1&lt;/td&gt; 
    &lt;td align="center"&gt;624x624&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;RF-DETR-Seg-2XL&lt;/td&gt; 
    &lt;td align="center"&gt;73.1&lt;/td&gt; 
    &lt;td align="center"&gt;49.9&lt;/td&gt; 
    &lt;td align="center"&gt;21.8&lt;/td&gt; 
    &lt;td align="center"&gt;38.6&lt;/td&gt; 
    &lt;td align="center"&gt;768x768&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv8-N-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;45.6&lt;/td&gt; 
    &lt;td align="center"&gt;28.3&lt;/td&gt; 
    &lt;td align="center"&gt;3.5&lt;/td&gt; 
    &lt;td align="center"&gt;3.4&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv8-S-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;53.8&lt;/td&gt; 
    &lt;td align="center"&gt;34.0&lt;/td&gt; 
    &lt;td align="center"&gt;4.2&lt;/td&gt; 
    &lt;td align="center"&gt;11.8&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv8-M-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;58.2&lt;/td&gt; 
    &lt;td align="center"&gt;37.3&lt;/td&gt; 
    &lt;td align="center"&gt;7.0&lt;/td&gt; 
    &lt;td align="center"&gt;27.3&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv8-L-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;60.5&lt;/td&gt; 
    &lt;td align="center"&gt;39.0&lt;/td&gt; 
    &lt;td align="center"&gt;9.7&lt;/td&gt; 
    &lt;td align="center"&gt;46.0&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv8-XL-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;61.3&lt;/td&gt; 
    &lt;td align="center"&gt;39.5&lt;/td&gt; 
    &lt;td align="center"&gt;14.0&lt;/td&gt; 
    &lt;td align="center"&gt;71.8&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv11-N-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;47.8&lt;/td&gt; 
    &lt;td align="center"&gt;30.0&lt;/td&gt; 
    &lt;td align="center"&gt;3.6&lt;/td&gt; 
    &lt;td align="center"&gt;2.9&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv11-S-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;55.4&lt;/td&gt; 
    &lt;td align="center"&gt;35.0&lt;/td&gt; 
    &lt;td align="center"&gt;4.6&lt;/td&gt; 
    &lt;td align="center"&gt;10.1&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv11-M-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;60.0&lt;/td&gt; 
    &lt;td align="center"&gt;38.5&lt;/td&gt; 
    &lt;td align="center"&gt;6.9&lt;/td&gt; 
    &lt;td align="center"&gt;22.4&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv11-L-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;61.5&lt;/td&gt; 
    &lt;td align="center"&gt;39.5&lt;/td&gt; 
    &lt;td align="center"&gt;8.3&lt;/td&gt; 
    &lt;td align="center"&gt;27.6&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLOv11-XL-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;62.4&lt;/td&gt; 
    &lt;td align="center"&gt;40.1&lt;/td&gt; 
    &lt;td align="center"&gt;13.7&lt;/td&gt; 
    &lt;td align="center"&gt;62.1&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-N-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;54.3&lt;/td&gt; 
    &lt;td align="center"&gt;34.7&lt;/td&gt; 
    &lt;td align="center"&gt;2.31&lt;/td&gt; 
    &lt;td align="center"&gt;2.7&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-S-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;62.4&lt;/td&gt; 
    &lt;td align="center"&gt;40.2&lt;/td&gt; 
    &lt;td align="center"&gt;3.47&lt;/td&gt; 
    &lt;td align="center"&gt;10.4&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-M-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;67.8&lt;/td&gt; 
    &lt;td align="center"&gt;44.0&lt;/td&gt; 
    &lt;td align="center"&gt;6.32&lt;/td&gt; 
    &lt;td align="center"&gt;23.6&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-L-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;69.8&lt;/td&gt; 
    &lt;td align="center"&gt;45.5&lt;/td&gt; 
    &lt;td align="center"&gt;7.58&lt;/td&gt; 
    &lt;td align="center"&gt;28.0&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;YOLO26-X-Seg&lt;/td&gt; 
    &lt;td align="center"&gt;71.6&lt;/td&gt; 
    &lt;td align="center"&gt;46.8&lt;/td&gt; 
    &lt;td align="center"&gt;12.92&lt;/td&gt; 
    &lt;td align="center"&gt;62.8&lt;/td&gt; 
    &lt;td align="center"&gt;640x640&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;Run Models&lt;/h2&gt; 
&lt;h3&gt;Detection&lt;/h3&gt; 
&lt;p&gt;RF-DETR provides multiple model sizes, ranging from Nano to 2XLarge. To use a different model size, replace the class name in the code snippet below with another class from the table.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRMedium
from rfdetr.util.coco_classes import COCO_CLASSES

model = RFDETRMedium()

image = Image.open(requests.get('https://media.roboflow.com/dog.jpg', stream=True).raw)
detections = model.predict(image, threshold=0.5)

labels = [
    f"{COCO_CLASSES[class_id]}"
    for class_id
    in detections.class_id
]

annotated_image = sv.BoxAnnotator().annotate(image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Run RF-DETR with Inference&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;You can also run RF-DETR models using the Inference library. To switch model size, select the appropriate inference package alias from the table below.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import requests
import supervision as sv
from PIL import Image
from inference import get_model

model = get_model("rfdetr-medium")

image = Image.open(requests.get('https://media.roboflow.com/dog.jpg', stream=True).raw)
predictions = model.infer(image, confidence=0.5)[0]
detections = sv.Detections.from_inference(predictions)

annotated_image = sv.BoxAnnotator().annotate(image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Size&lt;/th&gt; 
   &lt;th align="center"&gt;RF-DETR package class&lt;/th&gt; 
   &lt;th align="left"&gt;Inference package alias&lt;/th&gt; 
   &lt;th align="center"&gt;COCO AP&lt;sub&gt;50&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;COCO AP&lt;sub&gt;50:95&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;Latency (ms)&lt;/th&gt; 
   &lt;th align="center"&gt;Params (M)&lt;/th&gt; 
   &lt;th align="center"&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;N&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRNano&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-nano&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;67.6&lt;/td&gt; 
   &lt;td align="center"&gt;48.4&lt;/td&gt; 
   &lt;td align="center"&gt;2.3&lt;/td&gt; 
   &lt;td align="center"&gt;30.5&lt;/td&gt; 
   &lt;td align="center"&gt;384x384&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;S&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRSmall&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-small&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;72.1&lt;/td&gt; 
   &lt;td align="center"&gt;53.0&lt;/td&gt; 
   &lt;td align="center"&gt;3.5&lt;/td&gt; 
   &lt;td align="center"&gt;32.1&lt;/td&gt; 
   &lt;td align="center"&gt;512x512&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;M&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRMedium&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-medium&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;73.6&lt;/td&gt; 
   &lt;td align="center"&gt;54.7&lt;/td&gt; 
   &lt;td align="center"&gt;4.4&lt;/td&gt; 
   &lt;td align="center"&gt;33.7&lt;/td&gt; 
   &lt;td align="center"&gt;576x576&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;L&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRLarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-large&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;75.1&lt;/td&gt; 
   &lt;td align="center"&gt;56.5&lt;/td&gt; 
   &lt;td align="center"&gt;6.8&lt;/td&gt; 
   &lt;td align="center"&gt;33.9&lt;/td&gt; 
   &lt;td align="center"&gt;704x704&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;XL&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRXLarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-xlarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;77.4&lt;/td&gt; 
   &lt;td align="center"&gt;58.6&lt;/td&gt; 
   &lt;td align="center"&gt;11.5&lt;/td&gt; 
   &lt;td align="center"&gt;126.4&lt;/td&gt; 
   &lt;td align="center"&gt;700x700&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;2XL&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETR2XLarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-2xlarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;78.5&lt;/td&gt; 
   &lt;td align="center"&gt;60.1&lt;/td&gt; 
   &lt;td align="center"&gt;17.2&lt;/td&gt; 
   &lt;td align="center"&gt;126.9&lt;/td&gt; 
   &lt;td align="center"&gt;880x880&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Segmentation&lt;/h3&gt; 
&lt;p&gt;RF-DETR supports instance segmentation with model sizes from Nano to 2XLarge. To use a different model size, replace the class name in the code snippet below with another class from the table.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRSegMedium
from rfdetr.util.coco_classes import COCO_CLASSES

model = RFDETRSegMedium()

image = Image.open(requests.get('https://media.roboflow.com/dog.jpg', stream=True).raw)
detections = model.predict(image, threshold=0.5)

labels = [
    f"{COCO_CLASSES[class_id]}"
    for class_id
    in detections.class_id
]

annotated_image = sv.MaskAnnotator().annotate(image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Run RF-DETR-Seg with Inference&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;You can also run RF-DETR-Seg models using the Inference library. To switch model size, select the appropriate inference package alias from the table below.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import requests
import supervision as sv
from PIL import Image
from inference import get_model

model = get_model("rfdetr-seg-medium")

image = Image.open(requests.get('https://media.roboflow.com/dog.jpg', stream=True).raw)
predictions = model.infer(image, confidence=0.5)[0]
detections = sv.Detections.from_inference(predictions)

annotated_image = sv.MaskAnnotator().annotate(image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Size&lt;/th&gt; 
   &lt;th align="center"&gt;RF-DETR package class&lt;/th&gt; 
   &lt;th align="left"&gt;Inference package alias&lt;/th&gt; 
   &lt;th align="center"&gt;COCO AP&lt;sub&gt;50&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;COCO AP&lt;sub&gt;50:95&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;Latency (ms)&lt;/th&gt; 
   &lt;th align="center"&gt;Params (M)&lt;/th&gt; 
   &lt;th align="center"&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;N&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRSegNano&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-seg-nano&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;63.0&lt;/td&gt; 
   &lt;td align="center"&gt;40.3&lt;/td&gt; 
   &lt;td align="center"&gt;3.4&lt;/td&gt; 
   &lt;td align="center"&gt;33.6&lt;/td&gt; 
   &lt;td align="center"&gt;312x312&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;S&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRSegSmall&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-seg-small&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;66.2&lt;/td&gt; 
   &lt;td align="center"&gt;43.1&lt;/td&gt; 
   &lt;td align="center"&gt;4.4&lt;/td&gt; 
   &lt;td align="center"&gt;33.7&lt;/td&gt; 
   &lt;td align="center"&gt;384x384&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;M&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRSegMedium&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-seg-medium&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;68.4&lt;/td&gt; 
   &lt;td align="center"&gt;45.3&lt;/td&gt; 
   &lt;td align="center"&gt;5.9&lt;/td&gt; 
   &lt;td align="center"&gt;35.7&lt;/td&gt; 
   &lt;td align="center"&gt;432x432&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;L&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRSegLarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-seg-large&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;70.5&lt;/td&gt; 
   &lt;td align="center"&gt;47.1&lt;/td&gt; 
   &lt;td align="center"&gt;8.8&lt;/td&gt; 
   &lt;td align="center"&gt;36.2&lt;/td&gt; 
   &lt;td align="center"&gt;504x504&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;XL&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRSegXLarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-seg-xlarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;72.2&lt;/td&gt; 
   &lt;td align="center"&gt;48.8&lt;/td&gt; 
   &lt;td align="center"&gt;13.5&lt;/td&gt; 
   &lt;td align="center"&gt;38.1&lt;/td&gt; 
   &lt;td align="center"&gt;624x624&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;2XL&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;code&gt;RFDETRSeg2XLarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;rfdetr-seg-2xlarge&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;73.1&lt;/td&gt; 
   &lt;td align="center"&gt;49.9&lt;/td&gt; 
   &lt;td align="center"&gt;21.8&lt;/td&gt; 
   &lt;td align="center"&gt;38.6&lt;/td&gt; 
   &lt;td align="center"&gt;768x768&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Train Models&lt;/h3&gt; 
&lt;p&gt;RF-DETR supports training for both object detection and instance segmentation. You can train models in &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb"&gt;Google Colab&lt;/a&gt; or directly on the Roboflow platform. Below you will find a step-by-step video fine-tuning tutorial.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/-OvpdLAElFA"&gt;&lt;img src="https://github.com/user-attachments/assets/555a45c3-96e8-4d8a-ad29-f23403c8edfd" alt="rf-detr-tutorial-banner" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://rfdetr.roboflow.com"&gt;documentation website&lt;/a&gt; to learn more about how to use RF-DETR.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;All source code and model weights are licensed under the Apache License 2.0. See &lt;a href="https://raw.githubusercontent.com/roboflow/rf-detr/develop/LICENSE.core"&gt;&lt;code&gt;LICENSE.core&lt;/code&gt;&lt;/a&gt; for details. RF-DETR XLarge and 2XLarge code and checkpoints are licensed under the Platform Model License 1.0. See &lt;a href="https://raw.githubusercontent.com/roboflow/rf-detr/develop/LICENSE.platform"&gt;&lt;code&gt;LICENSE.platform&lt;/code&gt;&lt;/a&gt; for details. These models require a Roboflow account to run and fine-tune.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Our work is built upon &lt;a href="https://arxiv.org/pdf/2406.03459"&gt;LW-DETR&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2304.07193"&gt;DINOv2&lt;/a&gt;, and &lt;a href="https://arxiv.org/pdf/2010.04159"&gt;Deformable DETR&lt;/a&gt;. Thanks to their authors for their excellent work!&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{rf-detr,
    title={RF-DETR: Neural Architecture Search for Real-Time Detection Transformers},
    author={Isaac Robinson and Peter Robicheaux and Matvei Popov and Deva Ramanan and Neehar Peri},
    year={2025},
    eprint={2511.09554},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2511.09554},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome and appreciate all contributions! If you notice any issues or bugs, have questions, or would like to suggest new features, please &lt;a href="https://github.com/roboflow/rf-detr/issues/new"&gt;open an issue&lt;/a&gt; or pull request. By sharing your ideas and improvements, you help make RF-DETR better for everyone.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://youtube.com/roboflow"&gt;&lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652" width="3%" /&gt;&lt;/a&gt; &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; &lt;a href="https://roboflow.com"&gt;&lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649" width="3%" /&gt;&lt;/a&gt; &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; &lt;a href="https://www.linkedin.com/company/roboflow-ai/"&gt;&lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691" width="3%" /&gt;&lt;/a&gt; &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; &lt;a href="https://docs.roboflow.com"&gt;&lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511" width="3%" /&gt;&lt;/a&gt; &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; &lt;a href="https://discuss.roboflow.com"&gt;&lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584" width="3%" /&gt;&lt;/a&gt; &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; &lt;a href="https://blog.roboflow.com"&gt;&lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605" width="3%" /&gt;&lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; ‚ó¶ &amp;nbsp;No Vector DB&amp;nbsp; ‚ó¶ &amp;nbsp;No Chunking&amp;nbsp; ‚ó¶ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;üñ•Ô∏è Chat Platform&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;üîå MCP&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;üìö Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h2&gt;üì¢ Latest Updates&lt;/h2&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;üî• Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework ‚Äî an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;üß™ Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìë Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity ‚â† relevance&lt;/strong&gt; ‚Äî what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; ‚Äî a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a ‚ÄúTable-of-Contents‚Äù &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;üìç Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host ‚Äî run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service ‚Äî try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; ‚Äî private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß™ Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook ‚Äî a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; ‚Äî no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üå≤ PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚öôÔ∏è Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# ‚òÅÔ∏è Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìà Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üß≠ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß™ &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;üîå &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚≠ê Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star üåü if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/PageIndexAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;¬© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>github/spec-kit</title>
      <link>https://github.com/github/spec-kit</link>
      <description>&lt;p&gt;üí´ Toolkit to help you get started with Spec-Driven Development&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/github/spec-kit/main/media/logo_large.webp" alt="Spec Kit Logo" width="200" height="200" /&gt; 
 &lt;h1&gt;üå± Spec Kit&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Build high-quality software faster.&lt;/em&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;strong&gt;An open source toolkit that allows you to focus on product scenarios and predictable outcomes instead of vibe coding every piece from scratch.&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/github/spec-kit/actions/workflows/release.yml"&gt;&lt;img src="https://github.com/github/spec-kit/actions/workflows/release.yml/badge.svg?sanitize=true" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/github/spec-kit/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/github/spec-kit?style=social" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/github/spec-kit/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/github/spec-kit" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.github.io/spec-kit/"&gt;&lt;img src="https://img.shields.io/badge/docs-GitHub_Pages-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-what-is-spec-driven-development"&gt;ü§î What is Spec-Driven Development?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-get-started"&gt;‚ö° Get Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#%EF%B8%8F-video-overview"&gt;üìΩÔ∏è Video Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-supported-ai-agents"&gt;ü§ñ Supported AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-specify-cli-reference"&gt;üîß Specify CLI Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-core-philosophy"&gt;üìö Core Philosophy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-development-phases"&gt;üåü Development Phases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-experimental-goals"&gt;üéØ Experimental Goals&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-prerequisites"&gt;üîß Prerequisites&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-learn-more"&gt;üìñ Learn More&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-detailed-process"&gt;üìã Detailed Process&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-troubleshooting"&gt;üîç Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-maintainers"&gt;üë• Maintainers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-support"&gt;üí¨ Support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-acknowledgements"&gt;üôè Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-license"&gt;üìÑ License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§î What is Spec-Driven Development?&lt;/h2&gt; 
&lt;p&gt;Spec-Driven Development &lt;strong&gt;flips the script&lt;/strong&gt; on traditional software development. For decades, code has been king ‚Äî specifications were just scaffolding we built and discarded once the "real work" of coding began. Spec-Driven Development changes this: &lt;strong&gt;specifications become executable&lt;/strong&gt;, directly generating working implementations rather than just guiding them.&lt;/p&gt; 
&lt;h2&gt;‚ö° Get Started&lt;/h2&gt; 
&lt;h3&gt;1. Install Specify CLI&lt;/h3&gt; 
&lt;p&gt;Choose your preferred installation method:&lt;/p&gt; 
&lt;h4&gt;Option 1: Persistent Installation (Recommended)&lt;/h4&gt; 
&lt;p&gt;Install once and use everywhere:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install specify-cli --from git+https://github.com/github/spec-kit.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then use the tool directly:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create new project
specify init &amp;lt;PROJECT_NAME&amp;gt;

# Or initialize in existing project
specify init . --ai claude
# or
specify init --here --ai claude

# Check installed tools
specify check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To upgrade Specify, see the &lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/docs/upgrade.md"&gt;Upgrade Guide&lt;/a&gt; for detailed instructions. Quick upgrade:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install specify-cli --force --from git+https://github.com/github/spec-kit.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: One-time Usage&lt;/h4&gt; 
&lt;p&gt;Run directly without installing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx --from git+https://github.com/github/spec-kit.git specify init &amp;lt;PROJECT_NAME&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Benefits of persistent installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Tool stays installed and available in PATH&lt;/li&gt; 
 &lt;li&gt;No need to create shell aliases&lt;/li&gt; 
 &lt;li&gt;Better tool management with &lt;code&gt;uv tool list&lt;/code&gt;, &lt;code&gt;uv tool upgrade&lt;/code&gt;, &lt;code&gt;uv tool uninstall&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Cleaner shell configuration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. Establish project principles&lt;/h3&gt; 
&lt;p&gt;Launch your AI assistant in the project directory. The &lt;code&gt;/speckit.*&lt;/code&gt; commands are available in the assistant.&lt;/p&gt; 
&lt;p&gt;Use the &lt;strong&gt;&lt;code&gt;/speckit.constitution&lt;/code&gt;&lt;/strong&gt; command to create your project's governing principles and development guidelines that will guide all subsequent development.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/speckit.constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Create the spec&lt;/h3&gt; 
&lt;p&gt;Use the &lt;strong&gt;&lt;code&gt;/speckit.specify&lt;/code&gt;&lt;/strong&gt; command to describe what you want to build. Focus on the &lt;strong&gt;what&lt;/strong&gt; and &lt;strong&gt;why&lt;/strong&gt;, not the tech stack.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/speckit.specify Build an application that can help me organize my photos in separate photo albums. Albums are grouped by date and can be re-organized by dragging and dropping on the main page. Albums are never in other nested albums. Within each album, photos are previewed in a tile-like interface.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Create a technical implementation plan&lt;/h3&gt; 
&lt;p&gt;Use the &lt;strong&gt;&lt;code&gt;/speckit.plan&lt;/code&gt;&lt;/strong&gt; command to provide your tech stack and architecture choices.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/speckit.plan The application uses Vite with minimal number of libraries. Use vanilla HTML, CSS, and JavaScript as much as possible. Images are not uploaded anywhere and metadata is stored in a local SQLite database.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;5. Break down into tasks&lt;/h3&gt; 
&lt;p&gt;Use &lt;strong&gt;&lt;code&gt;/speckit.tasks&lt;/code&gt;&lt;/strong&gt; to create an actionable task list from your implementation plan.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/speckit.tasks
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6. Execute implementation&lt;/h3&gt; 
&lt;p&gt;Use &lt;strong&gt;&lt;code&gt;/speckit.implement&lt;/code&gt;&lt;/strong&gt; to execute all tasks and build your feature according to the plan.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/speckit.implement
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed step-by-step instructions, see our &lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/spec-driven.md"&gt;comprehensive guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìΩÔ∏è Video Overview&lt;/h2&gt; 
&lt;p&gt;Want to see Spec Kit in action? Watch our &lt;a href="https://www.youtube.com/watch?v=a9eR1xsfvHg&amp;amp;pp=0gcJCckJAYcqIYzv"&gt;video overview&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=a9eR1xsfvHg&amp;amp;pp=0gcJCckJAYcqIYzv"&gt;&lt;img src="https://raw.githubusercontent.com/github/spec-kit/main/media/spec-kit-video-header.jpg" alt="Spec Kit video header" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ü§ñ Supported AI Agents&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent&lt;/th&gt; 
   &lt;th&gt;Support&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://qoder.com/cli"&gt;Qoder CLI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aws.amazon.com/developer/learning/q-developer-cli/"&gt;Amazon Q Developer CLI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚ö†Ô∏è&lt;/td&gt; 
   &lt;td&gt;Amazon Q Developer CLI &lt;a href="https://github.com/aws/amazon-q-developer-cli/issues/3064"&gt;does not support&lt;/a&gt; custom arguments for slash commands.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ampcode.com/"&gt;Amp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.augmentcode.com/cli/overview"&gt;Auggie CLI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.codebuddy.ai/cli"&gt;CodeBuddy CLI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/openai/codex"&gt;Codex CLI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://cursor.sh/"&gt;Cursor&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://code.visualstudio.com/"&gt;GitHub Copilot&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.ibm.com/products/bob"&gt;IBM Bob&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;IDE-based agent with slash command support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://jules.google.com/"&gt;Jules&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Kilo-Org/kilocode"&gt;Kilo Code&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://opencode.ai/"&gt;opencode&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/qwen-code"&gt;Qwen Code&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://roocode.com/"&gt;Roo Code&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ovh/shai"&gt;SHAI (OVHcloud)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://windsurf.com/"&gt;Windsurf&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üîß Specify CLI Reference&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;specify&lt;/code&gt; command supports the following options:&lt;/p&gt; 
&lt;h3&gt;Commands&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;init&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initialize a new Specify project from the latest template&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;check&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Check for installed tools (&lt;code&gt;git&lt;/code&gt;, &lt;code&gt;claude&lt;/code&gt;, &lt;code&gt;gemini&lt;/code&gt;, &lt;code&gt;code&lt;/code&gt;/&lt;code&gt;code-insiders&lt;/code&gt;, &lt;code&gt;cursor-agent&lt;/code&gt;, &lt;code&gt;windsurf&lt;/code&gt;, &lt;code&gt;qwen&lt;/code&gt;, &lt;code&gt;opencode&lt;/code&gt;, &lt;code&gt;codex&lt;/code&gt;, &lt;code&gt;shai&lt;/code&gt;, &lt;code&gt;qoder&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;code&gt;specify init&lt;/code&gt; Arguments &amp;amp; Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Argument/Option&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;&amp;lt;project-name&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Argument&lt;/td&gt; 
   &lt;td&gt;Name for your new project directory (optional if using &lt;code&gt;--here&lt;/code&gt;, or use &lt;code&gt;.&lt;/code&gt; for current directory)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ai&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Option&lt;/td&gt; 
   &lt;td&gt;AI assistant to use: &lt;code&gt;claude&lt;/code&gt;, &lt;code&gt;gemini&lt;/code&gt;, &lt;code&gt;copilot&lt;/code&gt;, &lt;code&gt;cursor-agent&lt;/code&gt;, &lt;code&gt;qwen&lt;/code&gt;, &lt;code&gt;opencode&lt;/code&gt;, &lt;code&gt;codex&lt;/code&gt;, &lt;code&gt;windsurf&lt;/code&gt;, &lt;code&gt;kilocode&lt;/code&gt;, &lt;code&gt;auggie&lt;/code&gt;, &lt;code&gt;roo&lt;/code&gt;, &lt;code&gt;codebuddy&lt;/code&gt;, &lt;code&gt;amp&lt;/code&gt;, &lt;code&gt;shai&lt;/code&gt;, &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;bob&lt;/code&gt;, or &lt;code&gt;qoder&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--script&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Option&lt;/td&gt; 
   &lt;td&gt;Script variant to use: &lt;code&gt;sh&lt;/code&gt; (bash/zsh) or &lt;code&gt;ps&lt;/code&gt; (PowerShell)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ignore-agent-tools&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Flag&lt;/td&gt; 
   &lt;td&gt;Skip checks for AI agent tools like Claude Code&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-git&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Flag&lt;/td&gt; 
   &lt;td&gt;Skip git repository initialization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--here&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Flag&lt;/td&gt; 
   &lt;td&gt;Initialize project in the current directory instead of creating a new one&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--force&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Flag&lt;/td&gt; 
   &lt;td&gt;Force merge/overwrite when initializing in current directory (skip confirmation)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--skip-tls&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Flag&lt;/td&gt; 
   &lt;td&gt;Skip SSL/TLS verification (not recommended)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--debug&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Flag&lt;/td&gt; 
   &lt;td&gt;Enable detailed debug output for troubleshooting&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--github-token&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Option&lt;/td&gt; 
   &lt;td&gt;GitHub token for API requests (or set GH_TOKEN/GITHUB_TOKEN env variable)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic project initialization
specify init my-project

# Initialize with specific AI assistant
specify init my-project --ai claude

# Initialize with Cursor support
specify init my-project --ai cursor-agent

# Initialize with Qoder support
specify init my-project --ai qoder

# Initialize with Windsurf support
specify init my-project --ai windsurf

# Initialize with Amp support
specify init my-project --ai amp

# Initialize with SHAI support
specify init my-project --ai shai

# Initialize with IBM Bob support
specify init my-project --ai bob

# Initialize with PowerShell scripts (Windows/cross-platform)
specify init my-project --ai copilot --script ps

# Initialize in current directory
specify init . --ai copilot
# or use the --here flag
specify init --here --ai copilot

# Force merge into current (non-empty) directory without confirmation
specify init . --force --ai copilot
# or
specify init --here --force --ai copilot

# Skip git initialization
specify init my-project --ai gemini --no-git

# Enable debug output for troubleshooting
specify init my-project --ai claude --debug

# Use GitHub token for API requests (helpful for corporate environments)
specify init my-project --ai claude --github-token ghp_your_token_here

# Check system requirements
specify check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Available Slash Commands&lt;/h3&gt; 
&lt;p&gt;After running &lt;code&gt;specify init&lt;/code&gt;, your AI coding agent will have access to these slash commands for structured development:&lt;/p&gt; 
&lt;h4&gt;Core Commands&lt;/h4&gt; 
&lt;p&gt;Essential commands for the Spec-Driven Development workflow:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/speckit.constitution&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Create or update project governing principles and development guidelines&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/speckit.specify&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Define what you want to build (requirements and user stories)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/speckit.plan&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Create technical implementation plans with your chosen tech stack&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/speckit.tasks&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Generate actionable task lists for implementation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/speckit.implement&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Execute all tasks to build the feature according to the plan&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Optional Commands&lt;/h4&gt; 
&lt;p&gt;Additional commands for enhanced quality and validation:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/speckit.clarify&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Clarify underspecified areas (recommended before &lt;code&gt;/speckit.plan&lt;/code&gt;; formerly &lt;code&gt;/quizme&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/speckit.analyze&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cross-artifact consistency &amp;amp; coverage analysis (run after &lt;code&gt;/speckit.tasks&lt;/code&gt;, before &lt;code&gt;/speckit.implement&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;/speckit.checklist&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Generate custom quality checklists that validate requirements completeness, clarity, and consistency (like "unit tests for English")&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SPECIFY_FEATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override feature detection for non-Git repositories. Set to the feature directory name (e.g., &lt;code&gt;001-photo-albums&lt;/code&gt;) to work on a specific feature when not using Git branches.&lt;br /&gt;**Must be set in the context of the agent you're working with prior to using &lt;code&gt;/speckit.plan&lt;/code&gt; or follow-up commands.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üìö Core Philosophy&lt;/h2&gt; 
&lt;p&gt;Spec-Driven Development is a structured process that emphasizes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intent-driven development&lt;/strong&gt; where specifications define the "&lt;em&gt;what&lt;/em&gt;" before the "&lt;em&gt;how&lt;/em&gt;"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich specification creation&lt;/strong&gt; using guardrails and organizational principles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-step refinement&lt;/strong&gt; rather than one-shot code generation from prompts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Heavy reliance&lt;/strong&gt; on advanced AI model capabilities for specification interpretation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåü Development Phases&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Phase&lt;/th&gt; 
   &lt;th&gt;Focus&lt;/th&gt; 
   &lt;th&gt;Key Activities&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;0-to-1 Development&lt;/strong&gt; ("Greenfield")&lt;/td&gt; 
   &lt;td&gt;Generate from scratch&lt;/td&gt; 
   &lt;td&gt;
    &lt;ul&gt;
     &lt;li&gt;Start with high-level requirements&lt;/li&gt;
     &lt;li&gt;Generate specifications&lt;/li&gt;
     &lt;li&gt;Plan implementation steps&lt;/li&gt;
     &lt;li&gt;Build production-ready applications&lt;/li&gt;
    &lt;/ul&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Creative Exploration&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Parallel implementations&lt;/td&gt; 
   &lt;td&gt;
    &lt;ul&gt;
     &lt;li&gt;Explore diverse solutions&lt;/li&gt;
     &lt;li&gt;Support multiple technology stacks &amp;amp; architectures&lt;/li&gt;
     &lt;li&gt;Experiment with UX patterns&lt;/li&gt;
    &lt;/ul&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Iterative Enhancement&lt;/strong&gt; ("Brownfield")&lt;/td&gt; 
   &lt;td&gt;Brownfield modernization&lt;/td&gt; 
   &lt;td&gt;
    &lt;ul&gt;
     &lt;li&gt;Add features iteratively&lt;/li&gt;
     &lt;li&gt;Modernize legacy systems&lt;/li&gt;
     &lt;li&gt;Adapt processes&lt;/li&gt;
    &lt;/ul&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üéØ Experimental Goals&lt;/h2&gt; 
&lt;p&gt;Our research and experimentation focus on:&lt;/p&gt; 
&lt;h3&gt;Technology independence&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create applications using diverse technology stacks&lt;/li&gt; 
 &lt;li&gt;Validate the hypothesis that Spec-Driven Development is a process not tied to specific technologies, programming languages, or frameworks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise constraints&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Demonstrate mission-critical application development&lt;/li&gt; 
 &lt;li&gt;Incorporate organizational constraints (cloud providers, tech stacks, engineering practices)&lt;/li&gt; 
 &lt;li&gt;Support enterprise design systems and compliance requirements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User-centric development&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build applications for different user cohorts and preferences&lt;/li&gt; 
 &lt;li&gt;Support various development approaches (from vibe-coding to AI-native development)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Creative &amp;amp; iterative processes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Validate the concept of parallel implementation exploration&lt;/li&gt; 
 &lt;li&gt;Provide robust iterative feature development workflows&lt;/li&gt; 
 &lt;li&gt;Extend processes to handle upgrades and modernization tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîß Prerequisites&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/macOS/Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-supported-ai-agents"&gt;Supported&lt;/a&gt; AI coding agent.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; for package management&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.11+&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://git-scm.com/downloads"&gt;Git&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you encounter issues with an agent, please open an issue so we can refine the integration.&lt;/p&gt; 
&lt;h2&gt;üìñ Learn More&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/spec-driven.md"&gt;Complete Spec-Driven Development Methodology&lt;/a&gt;&lt;/strong&gt; - Deep dive into the full process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/#-detailed-process"&gt;Detailed Walkthrough&lt;/a&gt;&lt;/strong&gt; - Step-by-step implementation guide&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìã Detailed Process&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to expand the detailed step-by-step walkthrough&lt;/summary&gt; 
 &lt;p&gt;You can use the Specify CLI to bootstrap your project, which will bring in the required artifacts in your environment. Run:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;specify init &amp;lt;project_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Or initialize in the current directory:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;specify init .
# or use the --here flag
specify init --here
# Skip confirmation when the directory already has files
specify init . --force
# or
specify init --here --force
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/github/spec-kit/main/media/specify_cli.gif" alt="Specify CLI bootstrapping a new project in the terminal" /&gt;&lt;/p&gt; 
 &lt;p&gt;You will be prompted to select the AI agent you are using. You can also proactively specify it directly in the terminal:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;specify init &amp;lt;project_name&amp;gt; --ai claude
specify init &amp;lt;project_name&amp;gt; --ai gemini
specify init &amp;lt;project_name&amp;gt; --ai copilot

# Or in current directory:
specify init . --ai claude
specify init . --ai codex

# or use --here flag
specify init --here --ai claude
specify init --here --ai codex

# Force merge into a non-empty current directory
specify init . --force --ai claude

# or
specify init --here --force --ai claude
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The CLI will check if you have Claude Code, Gemini CLI, Cursor CLI, Qwen CLI, opencode, Codex CLI, Qoder CLI, or Amazon Q Developer CLI installed. If you do not, or you prefer to get the templates without checking for the right tools, use &lt;code&gt;--ignore-agent-tools&lt;/code&gt; with your command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;specify init &amp;lt;project_name&amp;gt; --ai claude --ignore-agent-tools
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;STEP 1:&lt;/strong&gt; Establish project principles&lt;/h3&gt; 
 &lt;p&gt;Go to the project folder and run your AI agent. In our example, we're using &lt;code&gt;claude&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/github/spec-kit/main/media/bootstrap-claude-code.gif" alt="Bootstrapping Claude Code environment" /&gt;&lt;/p&gt; 
 &lt;p&gt;You will know that things are configured correctly if you see the &lt;code&gt;/speckit.constitution&lt;/code&gt;, &lt;code&gt;/speckit.specify&lt;/code&gt;, &lt;code&gt;/speckit.plan&lt;/code&gt;, &lt;code&gt;/speckit.tasks&lt;/code&gt;, and &lt;code&gt;/speckit.implement&lt;/code&gt; commands available.&lt;/p&gt; 
 &lt;p&gt;The first step should be establishing your project's governing principles using the &lt;code&gt;/speckit.constitution&lt;/code&gt; command. This helps ensure consistent decision-making throughout all subsequent development phases:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;/speckit.constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements. Include governance for how these principles should guide technical decisions and implementation choices.
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This step creates or updates the &lt;code&gt;.specify/memory/constitution.md&lt;/code&gt; file with your project's foundational guidelines that the AI agent will reference during specification, planning, and implementation phases.&lt;/p&gt; 
 &lt;h3&gt;&lt;strong&gt;STEP 2:&lt;/strong&gt; Create project specifications&lt;/h3&gt; 
 &lt;p&gt;With your project principles established, you can now create the functional specifications. Use the &lt;code&gt;/speckit.specify&lt;/code&gt; command and then provide the concrete requirements for the project you want to develop.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!IMPORTANT] Be as explicit as possible about &lt;em&gt;what&lt;/em&gt; you are trying to build and &lt;em&gt;why&lt;/em&gt;. &lt;strong&gt;Do not focus on the tech stack at this point&lt;/strong&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;An example prompt:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;Develop Taskify, a team productivity platform. It should allow users to create projects, add team members,
assign tasks, comment and move tasks between boards in Kanban style. In this initial phase for this feature,
let's call it "Create Taskify," let's have multiple users but the users will be declared ahead of time, predefined.
I want five users in two different categories, one product manager and four engineers. Let's create three
different sample projects. Let's have the standard Kanban columns for the status of each task, such as "To Do,"
"In Progress," "In Review," and "Done." There will be no login for this application as this is just the very
first testing thing to ensure that our basic features are set up. For each task in the UI for a task card,
you should be able to change the current status of the task between the different columns in the Kanban work board.
You should be able to leave an unlimited number of comments for a particular card. You should be able to, from that task
card, assign one of the valid users. When you first launch Taskify, it's going to give you a list of the five users to pick
from. There will be no password required. When you click on a user, you go into the main view, which displays the list of
projects. When you click on a project, you open the Kanban board for that project. You're going to see the columns.
You'll be able to drag and drop cards back and forth between different columns. You will see any cards that are
assigned to you, the currently logged in user, in a different color from all the other ones, so you can quickly
see yours. You can edit any comments that you make, but you can't edit comments that other people made. You can
delete any comments that you made, but you can't delete comments anybody else made.
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;After this prompt is entered, you should see Claude Code kick off the planning and spec drafting process. Claude Code will also trigger some of the built-in scripts to set up the repository.&lt;/p&gt; 
 &lt;p&gt;Once this step is completed, you should have a new branch created (e.g., &lt;code&gt;001-create-taskify&lt;/code&gt;), as well as a new specification in the &lt;code&gt;specs/001-create-taskify&lt;/code&gt; directory.&lt;/p&gt; 
 &lt;p&gt;The produced specification should contain a set of user stories and functional requirements, as defined in the template.&lt;/p&gt; 
 &lt;p&gt;At this stage, your project folder contents should resemble the following:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;‚îî‚îÄ‚îÄ .specify
    ‚îú‚îÄ‚îÄ memory
    ‚îÇ  ‚îî‚îÄ‚îÄ constitution.md
    ‚îú‚îÄ‚îÄ scripts
    ‚îÇ  ‚îú‚îÄ‚îÄ check-prerequisites.sh
    ‚îÇ  ‚îú‚îÄ‚îÄ common.sh
    ‚îÇ  ‚îú‚îÄ‚îÄ create-new-feature.sh
    ‚îÇ  ‚îú‚îÄ‚îÄ setup-plan.sh
    ‚îÇ  ‚îî‚îÄ‚îÄ update-claude-md.sh
    ‚îú‚îÄ‚îÄ specs
    ‚îÇ  ‚îî‚îÄ‚îÄ 001-create-taskify
    ‚îÇ      ‚îî‚îÄ‚îÄ spec.md
    ‚îî‚îÄ‚îÄ templates
        ‚îú‚îÄ‚îÄ plan-template.md
        ‚îú‚îÄ‚îÄ spec-template.md
        ‚îî‚îÄ‚îÄ tasks-template.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;STEP 3:&lt;/strong&gt; Functional specification clarification (required before planning)&lt;/h3&gt; 
 &lt;p&gt;With the baseline specification created, you can go ahead and clarify any of the requirements that were not captured properly within the first shot attempt.&lt;/p&gt; 
 &lt;p&gt;You should run the structured clarification workflow &lt;strong&gt;before&lt;/strong&gt; creating a technical plan to reduce rework downstream.&lt;/p&gt; 
 &lt;p&gt;Preferred order:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Use &lt;code&gt;/speckit.clarify&lt;/code&gt; (structured) ‚Äì sequential, coverage-based questioning that records answers in a Clarifications section.&lt;/li&gt; 
  &lt;li&gt;Optionally follow up with ad-hoc free-form refinement if something still feels vague.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;If you intentionally want to skip clarification (e.g., spike or exploratory prototype), explicitly state that so the agent doesn't block on missing clarifications.&lt;/p&gt; 
 &lt;p&gt;Example free-form refinement prompt (after &lt;code&gt;/speckit.clarify&lt;/code&gt; if still needed):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;For each sample project or project that you create there should be a variable number of tasks between 5 and 15
tasks for each one randomly distributed into different states of completion. Make sure that there's at least
one task in each stage of completion.
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You should also ask Claude Code to validate the &lt;strong&gt;Review &amp;amp; Acceptance Checklist&lt;/strong&gt;, checking off the things that are validated/pass the requirements, and leave the ones that are not unchecked. The following prompt can be used:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;Read the review and acceptance checklist, and check off each item in the checklist if the feature spec meets the criteria. Leave it empty if it does not.
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;It's important to use the interaction with Claude Code as an opportunity to clarify and ask questions around the specification - &lt;strong&gt;do not treat its first attempt as final&lt;/strong&gt;.&lt;/p&gt; 
 &lt;h3&gt;&lt;strong&gt;STEP 4:&lt;/strong&gt; Generate a plan&lt;/h3&gt; 
 &lt;p&gt;You can now be specific about the tech stack and other technical requirements. You can use the &lt;code&gt;/speckit.plan&lt;/code&gt; command that is built into the project template with a prompt like this:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;We are going to generate this using .NET Aspire, using Postgres as the database. The frontend should use
Blazor server with drag-and-drop task boards, real-time updates. There should be a REST API created with a projects API,
tasks API, and a notifications API.
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The output of this step will include a number of implementation detail documents, with your directory tree resembling this:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;.
‚îú‚îÄ‚îÄ CLAUDE.md
‚îú‚îÄ‚îÄ memory
‚îÇ  ‚îî‚îÄ‚îÄ constitution.md
‚îú‚îÄ‚îÄ scripts
‚îÇ  ‚îú‚îÄ‚îÄ check-prerequisites.sh
‚îÇ  ‚îú‚îÄ‚îÄ common.sh
‚îÇ  ‚îú‚îÄ‚îÄ create-new-feature.sh
‚îÇ  ‚îú‚îÄ‚îÄ setup-plan.sh
‚îÇ  ‚îî‚îÄ‚îÄ update-claude-md.sh
‚îú‚îÄ‚îÄ specs
‚îÇ  ‚îî‚îÄ‚îÄ 001-create-taskify
‚îÇ      ‚îú‚îÄ‚îÄ contracts
‚îÇ      ‚îÇ  ‚îú‚îÄ‚îÄ api-spec.json
‚îÇ      ‚îÇ  ‚îî‚îÄ‚îÄ signalr-spec.md
‚îÇ      ‚îú‚îÄ‚îÄ data-model.md
‚îÇ      ‚îú‚îÄ‚îÄ plan.md
‚îÇ      ‚îú‚îÄ‚îÄ quickstart.md
‚îÇ      ‚îú‚îÄ‚îÄ research.md
‚îÇ      ‚îî‚îÄ‚îÄ spec.md
‚îî‚îÄ‚îÄ templates
    ‚îú‚îÄ‚îÄ CLAUDE-template.md
    ‚îú‚îÄ‚îÄ plan-template.md
    ‚îú‚îÄ‚îÄ spec-template.md
    ‚îî‚îÄ‚îÄ tasks-template.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Check the &lt;code&gt;research.md&lt;/code&gt; document to ensure that the right tech stack is used, based on your instructions. You can ask Claude Code to refine it if any of the components stand out, or even have it check the locally-installed version of the platform/framework you want to use (e.g., .NET).&lt;/p&gt; 
 &lt;p&gt;Additionally, you might want to ask Claude Code to research details about the chosen tech stack if it's something that is rapidly changing (e.g., .NET Aspire, JS frameworks), with a prompt like this:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;I want you to go through the implementation plan and implementation details, looking for areas that could
benefit from additional research as .NET Aspire is a rapidly changing library. For those areas that you identify that
require further research, I want you to update the research document with additional details about the specific
versions that we are going to be using in this Taskify application and spawn parallel research tasks to clarify
any details using research from the web.
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;During this process, you might find that Claude Code gets stuck researching the wrong thing - you can help nudge it in the right direction with a prompt like this:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;I think we need to break this down into a series of steps. First, identify a list of tasks
that you would need to do during implementation that you're not sure of or would benefit
from further research. Write down a list of those tasks. And then for each one of these tasks,
I want you to spin up a separate research task so that the net results is we are researching
all of those very specific tasks in parallel. What I saw you doing was it looks like you were
researching .NET Aspire in general and I don't think that's gonna do much for us in this case.
That's way too untargeted research. The research needs to help you solve a specific targeted question.
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] Claude Code might be over-eager and add components that you did not ask for. Ask it to clarify the rationale and the source of the change.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;&lt;strong&gt;STEP 5:&lt;/strong&gt; Have Claude Code validate the plan&lt;/h3&gt; 
 &lt;p&gt;With the plan in place, you should have Claude Code run through it to make sure that there are no missing pieces. You can use a prompt like this:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;Now I want you to go and audit the implementation plan and the implementation detail files.
Read through it with an eye on determining whether or not there is a sequence of tasks that you need
to be doing that are obvious from reading this. Because I don't know if there's enough here. For example,
when I look at the core implementation, it would be useful to reference the appropriate places in the implementation
details where it can find the information as it walks through each step in the core implementation or in the refinement.
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This helps refine the implementation plan and helps you avoid potential blind spots that Claude Code missed in its planning cycle. Once the initial refinement pass is complete, ask Claude Code to go through the checklist once more before you can get to the implementation.&lt;/p&gt; 
 &lt;p&gt;You can also ask Claude Code (if you have the &lt;a href="https://docs.github.com/en/github-cli/github-cli"&gt;GitHub CLI&lt;/a&gt; installed) to go ahead and create a pull request from your current branch to &lt;code&gt;main&lt;/code&gt; with a detailed description, to make sure that the effort is properly tracked.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] Before you have the agent implement it, it's also worth prompting Claude Code to cross-check the details to see if there are any over-engineered pieces (remember - it can be over-eager). If over-engineered components or decisions exist, you can ask Claude Code to resolve them. Ensure that Claude Code follows the &lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/base/memory/constitution.md"&gt;constitution&lt;/a&gt; as the foundational piece that it must adhere to when establishing the plan.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;&lt;strong&gt;STEP 6:&lt;/strong&gt; Generate task breakdown with /speckit.tasks&lt;/h3&gt; 
 &lt;p&gt;With the implementation plan validated, you can now break down the plan into specific, actionable tasks that can be executed in the correct order. Use the &lt;code&gt;/speckit.tasks&lt;/code&gt; command to automatically generate a detailed task breakdown from your implementation plan:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;/speckit.tasks
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This step creates a &lt;code&gt;tasks.md&lt;/code&gt; file in your feature specification directory that contains:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Task breakdown organized by user story&lt;/strong&gt; - Each user story becomes a separate implementation phase with its own set of tasks&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Dependency management&lt;/strong&gt; - Tasks are ordered to respect dependencies between components (e.g., models before services, services before endpoints)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Parallel execution markers&lt;/strong&gt; - Tasks that can run in parallel are marked with &lt;code&gt;[P]&lt;/code&gt; to optimize development workflow&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;File path specifications&lt;/strong&gt; - Each task includes the exact file paths where implementation should occur&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Test-driven development structure&lt;/strong&gt; - If tests are requested, test tasks are included and ordered to be written before implementation&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Checkpoint validation&lt;/strong&gt; - Each user story phase includes checkpoints to validate independent functionality&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;The generated tasks.md provides a clear roadmap for the &lt;code&gt;/speckit.implement&lt;/code&gt; command, ensuring systematic implementation that maintains code quality and allows for incremental delivery of user stories.&lt;/p&gt; 
 &lt;h3&gt;&lt;strong&gt;STEP 7:&lt;/strong&gt; Implementation&lt;/h3&gt; 
 &lt;p&gt;Once ready, use the &lt;code&gt;/speckit.implement&lt;/code&gt; command to execute your implementation plan:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;/speckit.implement
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;/speckit.implement&lt;/code&gt; command will:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Validate that all prerequisites are in place (constitution, spec, plan, and tasks)&lt;/li&gt; 
  &lt;li&gt;Parse the task breakdown from &lt;code&gt;tasks.md&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Execute tasks in the correct order, respecting dependencies and parallel execution markers&lt;/li&gt; 
  &lt;li&gt;Follow the TDD approach defined in your task plan&lt;/li&gt; 
  &lt;li&gt;Provide progress updates and handle errors appropriately&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!IMPORTANT] The AI agent will execute local CLI commands (such as &lt;code&gt;dotnet&lt;/code&gt;, &lt;code&gt;npm&lt;/code&gt;, etc.) - make sure you have the required tools installed on your machine.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;Once the implementation is complete, test the application and resolve any runtime errors that may not be visible in CLI logs (e.g., browser console errors). You can copy and paste such errors back to your AI agent for resolution.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîç Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Git Credential Manager on Linux&lt;/h3&gt; 
&lt;p&gt;If you're having issues with Git authentication on Linux, you can install Git Credential Manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#!/usr/bin/env bash
set -e
echo "Downloading Git Credential Manager v2.6.1..."
wget https://github.com/git-ecosystem/git-credential-manager/releases/download/v2.6.1/gcm-linux_amd64.2.6.1.deb
echo "Installing Git Credential Manager..."
sudo dpkg -i gcm-linux_amd64.2.6.1.deb
echo "Configuring Git to use GCM..."
git config --global credential.helper manager
echo "Cleaning up..."
rm gcm-linux_amd64.2.6.1.deb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üë• Maintainers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Den Delimarsky (&lt;a href="https://github.com/localden"&gt;@localden&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;John Lam (&lt;a href="https://github.com/jflam"&gt;@jflam&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üí¨ Support&lt;/h2&gt; 
&lt;p&gt;For support, please open a &lt;a href="https://github.com/github/spec-kit/issues/new"&gt;GitHub issue&lt;/a&gt;. We welcome bug reports, feature requests, and questions about using Spec-Driven Development.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is heavily influenced by and based on the work and research of &lt;a href="https://github.com/jflam"&gt;John Lam&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the terms of the MIT open source license. Please refer to the &lt;a href="https://raw.githubusercontent.com/github/spec-kit/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for the full terms.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Polymarket/agents</title>
      <link>https://github.com/Polymarket/agents</link>
      <description>&lt;p&gt;Trade autonomously on Polymarket using AI Agents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/polymarket/agents/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/polymarket/agents?style=for-the-badge" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/polymarket/agents/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/polymarket/agents?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/polymarket/agents/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/polymarket/agents?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/polymarket/agents/issues"&gt;&lt;img src="https://img.shields.io/github/issues/polymarket/agents?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/polymarket/agents/raw/master/LICENSE.md"&gt;&lt;img src="https://img.shields.io/github/license/polymarket/agents?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/polymarket/agents"&gt; &lt;img src="https://raw.githubusercontent.com/Polymarket/agents/main/docs/images/cli.png" alt="Logo" width="466" height="262" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Polymarket Agents&lt;/h3&gt; 
 &lt;p align="center"&gt; Trade autonomously on Polymarket using AI Agents &lt;br /&gt; &lt;a href="https://github.com/polymarket/agents"&gt;&lt;strong&gt;Explore the docs ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://github.com/polymarket/agents"&gt;View Demo&lt;/a&gt; ¬∑ &lt;a href="https://github.com/polymarket/agents/issues/new?labels=bug&amp;amp;template=bug-report---.md"&gt;Report Bug&lt;/a&gt; ¬∑ &lt;a href="https://github.com/polymarket/agents/issues/new?labels=enhancement&amp;amp;template=feature-request---.md"&gt;Request Feature&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- CONTENT --&gt; 
&lt;h1&gt;Polymarket Agents&lt;/h1&gt; 
&lt;p&gt;Polymarket Agents is a developer framework and set of utilities for building AI agents for Polymarket.&lt;/p&gt; 
&lt;p&gt;This code is free and publicly available under MIT License open source license (&lt;a href="https://raw.githubusercontent.com/Polymarket/agents/main/#terms-of-service"&gt;terms of service&lt;/a&gt;)!&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integration with Polymarket API&lt;/li&gt; 
 &lt;li&gt;AI agent utilities for prediction markets&lt;/li&gt; 
 &lt;li&gt;Local and remote RAG (Retrieval-Augmented Generation) support&lt;/li&gt; 
 &lt;li&gt;Data sourcing from betting services, news providers, and web search&lt;/li&gt; 
 &lt;li&gt;Comphrehensive LLM tools for prompt engineering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Getting started&lt;/h1&gt; 
&lt;p&gt;This repo is inteded for use with Python 3.9&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/{username}/polymarket-agents.git
cd polymarket-agents
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create the virtual environment&lt;/p&gt; &lt;pre&gt;&lt;code&gt;virtualenv --python=python3.9 .venv
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Activate the virtual environment&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;On Windows:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;.venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;On macOS and Linux:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the required dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Set up your environment variables:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the project root directory&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Add the following environment variables:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;POLYGON_WALLET_PRIVATE_KEY=""
OPENAI_API_KEY=""
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Load your wallet with USDC.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Try the command line interface...&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/python/cli.py
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or just go trade!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python agents/application/trade.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Note: If running the command outside of docker, please set the following env var:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export PYTHONPATH="."
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If running with docker is preferred, we provide the following scripts:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./scripts/bash/build-docker.sh
./scripts/bash/run-docker-dev.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;The Polymarket Agents architecture features modular components that can be maintained and extended by individual community members.&lt;/p&gt; 
&lt;h3&gt;APIs&lt;/h3&gt; 
&lt;p&gt;Polymarket Agents connectors standardize data sources and order types.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Chroma.py&lt;/code&gt;: chroma DB for vectorizing news sources and other API data. Developers are able to add their own vector database implementations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Gamma.py&lt;/code&gt;: defines &lt;code&gt;GammaMarketClient&lt;/code&gt; class, which interfaces with the Polymarket Gamma API to fetch and parse market and event metadata. Methods to retrieve current and tradable markets, as well as defined information on specific markets and events.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Polymarket.py&lt;/code&gt;: defines a Polymarket class that interacts with the Polymarket API to retrieve and manage market and event data, and to execute orders on the Polymarket DEX. It includes methods for API key initialization, market and event data retrieval, and trade execution. The file also provides utility functions for building and signing orders, as well as examples for testing API interactions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Objects.py&lt;/code&gt;: data models using Pydantic; representations for trades, markets, events, and related entities.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Scripts&lt;/h3&gt; 
&lt;p&gt;Files for managing your local environment, server set-up to run the application remotely, and cli for end-user commands.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;cli.py&lt;/code&gt; is the primary user interface for the repo. Users can run various commands to interact with the Polymarket API, retrieve relevant news articles, query local data, send data/prompts to LLMs, and execute trades in Polymarkets.&lt;/p&gt; 
&lt;p&gt;Commands should follow this format:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python scripts/python/cli.py command_name [attribute value] [attribute value]&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;get-all-markets&lt;/code&gt; Retrieve and display a list of markets from Polymarket, sorted by volume.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python scripts/python/cli.py get-all-markets --limit &amp;lt;LIMIT&amp;gt; --sort-by &amp;lt;SORT_BY&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;limit: The number of markets to retrieve (default: 5).&lt;/li&gt; 
 &lt;li&gt;sort_by: The sorting criterion, either volume (default) or another valid attribute.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;If you would like to contribute to this project, please follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch.&lt;/li&gt; 
 &lt;li&gt;Make your changes.&lt;/li&gt; 
 &lt;li&gt;Submit a pull request.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please run pre-commit hooks before making contributions. To initialize them:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Related Repos&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Polymarket/py-clob-client"&gt;py-clob-client&lt;/a&gt;: Python client for the Polymarket CLOB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Polymarket/python-order-utils"&gt;python-order-utils&lt;/a&gt;: Python utilities to generate and sign orders from Polymarket's CLOB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Polymarket/clob-client"&gt;Polymarket CLOB client&lt;/a&gt;: Typescript client for Polymarket CLOB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langchain-ai/langchain"&gt;Langchain&lt;/a&gt;: Utility for building context-aware reasoning applications&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.trychroma.com/getting-started"&gt;Chroma&lt;/a&gt;: Chroma is an AI-native open-source vector database&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Prediction markets reading&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Prediction Markets: Bottlenecks and the Next Major Unlocks, Mikey 0x: &lt;a href="https://mirror.xyz/1kx.eth/jnQhA56Kx9p3RODKiGzqzHGGEODpbskivUUNdd7hwh0"&gt;https://mirror.xyz/1kx.eth/jnQhA56Kx9p3RODKiGzqzHGGEODpbskivUUNdd7hwh0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The promise and challenges of crypto + AI applications, Vitalik Buterin: &lt;a href="https://vitalik.eth.limo/general/2024/01/30/cryptoai.html"&gt;https://vitalik.eth.limo/general/2024/01/30/cryptoai.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Superforecasting: How to Upgrade Your Company's Judgement, Schoemaker and Tetlock: &lt;a href="https://hbr.org/2016/05/superforecasting-how-to-upgrade-your-companys-judgment"&gt;https://hbr.org/2016/05/superforecasting-how-to-upgrade-your-companys-judgment&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://github.com/Polymarket/agents/raw/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h1&gt;Contact&lt;/h1&gt; 
&lt;p&gt;For any questions or inquiries, please contact &lt;a href="mailto:liam@polymarket.com"&gt;liam@polymarket.com&lt;/a&gt; or reach out at &lt;a href="http://www.greenestreet.xyz"&gt;www.greenestreet.xyz&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Enjoy using the CLI application! If you encounter any issues, feel free to open an issue on the repository.&lt;/p&gt; 
&lt;h1&gt;Terms of Service&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://polymarket.com/tos"&gt;Terms of Service&lt;/a&gt; prohibit US persons and persons from certain other jurisdictions from trading on Polymarket (via UI &amp;amp; API and including agents developed by persons in restricted jurisdictions), although data and information is viewable globally.&lt;/p&gt; 
&lt;!-- LINKS --&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3</title>
      <link>https://github.com/QwenLM/Qwen3</link>
      <description>&lt;p&gt;Qwen3 is the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; üíú &lt;a href="https://chat.qwen.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/Qwen"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/organization/qwen"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://arxiv.org/abs/2505.09388"&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ÔΩú &amp;nbsp;&amp;nbsp;üìñ &lt;a href="https://qwen.readthedocs.io/"&gt;Documentation&lt;/a&gt; &lt;br /&gt; üñ•Ô∏è &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Demo"&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-&lt;/code&gt; or visit the &lt;a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f"&gt;Qwen3 collection&lt;/a&gt;, and you will find all you need! Enjoy!&lt;/p&gt; 
&lt;p&gt;To learn more about Qwen3, feel free to read our documentation [&lt;a href="https://qwen.readthedocs.io/en/latest/"&gt;EN&lt;/a&gt;|&lt;a href="https://qwen.readthedocs.io/zh-cn/latest/"&gt;ZH&lt;/a&gt;]. Our documentation consists of the following sections:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Quickstart: the basic usages and demonstrations;&lt;/li&gt; 
 &lt;li&gt;Inference: the guidance for the inference with Transformers, including batch inference, streaming, etc.;&lt;/li&gt; 
 &lt;li&gt;Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like llama.cpp, Ollama, and LM Studio;&lt;/li&gt; 
 &lt;li&gt;Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like SGLang, vLLM, TGI, etc.;&lt;/li&gt; 
 &lt;li&gt;Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;&lt;/li&gt; 
 &lt;li&gt;Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.&lt;/li&gt; 
 &lt;li&gt;Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;h3&gt;Qwen3-2507&lt;/h3&gt; 
&lt;p&gt;Over the past three months, we continued to explore the potential of the Qwen3 families and we are excited to introduce the updated &lt;strong&gt;Qwen3-2507&lt;/strong&gt; in two variants, Qwen3-Instruct-2507 and Qwen3-Thinking-2507, and three sizes, 235B-A22B, 30B-A3B, and 4B.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Qwen3-Instruct-2507&lt;/strong&gt; is the updated version of the previous Qwen3 non-thinking mode, featuring the following key enhancements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Significant improvements&lt;/strong&gt; in general capabilities, including &lt;strong&gt;instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Substantial gains&lt;/strong&gt; in long-tail knowledge coverage across &lt;strong&gt;multiple languages&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Markedly better alignment&lt;/strong&gt; with user preferences in &lt;strong&gt;subjective and open-ended tasks&lt;/strong&gt;, enabling more helpful responses and higher-quality text generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced capabilities&lt;/strong&gt; in &lt;strong&gt;256K-token long-context understanding&lt;/strong&gt;, extendable up to &lt;strong&gt;1 million tokens&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Qwen3-Thinking-2507&lt;/strong&gt; is the continuation of Qwen3 thinking model, with improved quality and depth of reasoning, featuring the following key enhancements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise ‚Äî achieving &lt;strong&gt;state-of-the-art results among open-weight thinking models&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities, extendable up to &lt;strong&gt;1 million tokens&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Previous Qwen3 Release&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;Qwen3 (aka Qwen3-2504)&lt;/h3&gt; 
 &lt;p&gt; We are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. These models represent our most advanced and intelligent systems to date, improving from our experience in building QwQ and Qwen2.5. We are making the weights of Qwen3 available to the public, including both dense and Mixture-of-Expert (MoE) models. &lt;br /&gt;&lt;br /&gt; The highlights from Qwen3 include: &lt;/p&gt;
 &lt;ul&gt; 
  &lt;li&gt;&lt;b&gt;Dense and Mixture-of-Experts (MoE) models of various sizes&lt;/b&gt;, available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Seamless switching between thinking mode&lt;/b&gt; (for complex logical reasoning, math, and coding) and &lt;b&gt;non-thinking mode&lt;/b&gt; (for efficient, general-purpose chat), ensuring optimal performance across various scenarios.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Significantly enhancement in reasoning capabilities&lt;/b&gt;, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Superior human preference alignment&lt;/b&gt;, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Expertise in agent capabilities&lt;/b&gt;, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.&lt;/li&gt; 
  &lt;li&gt;&lt;b&gt;Support of 100+ languages and dialects&lt;/b&gt; with strong capabilities for &lt;b&gt;multilingual instruction following&lt;/b&gt; and &lt;b&gt;translation&lt;/b&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.08.08: You can now use Qwen3-2507 to handle ultra-long inputs of &lt;strong&gt;1 million tokens&lt;/strong&gt;! See the update modelcards (&lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;235B-A22B-Instruct-2507&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;235B-A22B-Thinking-2507&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;A30B-A3B-Instruct-2507&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;A30B-A3B-Thinking-2507&lt;/a&gt;) for how to enable this feature.&lt;/li&gt; 
 &lt;li&gt;2025.08.06: The final open release of Qwen3-2507, &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507"&gt;Qwen3-4B-Instruct-2507&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;Qwen3-4B-Thinking-2507&lt;/a&gt;, is out!&lt;/li&gt; 
 &lt;li&gt;2025.07.31: Qwen3-30B-A3B-Thinking-2507 is released. Check out the &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.07.30: Qwen3-30B-A3B-Instruct-2507 is released. Check out the &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.07.25: We released the updated version of Qwen3-235B-A22B thinking mode, named Qwen3-235B-A22B-Thinking-2507. Check out the &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.07.21: We released the updated version of Qwen3-235B-A22B non-thinking mode, named Qwen3-235B-A22B-Instruct-2507, featuring significant enhancements over the previous version and supporting 256K-token long-context understanding. Check our &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507"&gt;modelcard&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2025.04.29: We released the Qwen3 series. Check our &lt;a href="https://qwenlm.github.io/blog/qwen3"&gt;blog&lt;/a&gt; for more details!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5"&gt;blog&lt;/a&gt; for more!&lt;/li&gt; 
 &lt;li&gt;2024.06.06: We released the Qwen2 series. Check our &lt;a href="https://qwenlm.github.io/blog/qwen2/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our &lt;a href="https://qwenlm.github.io/blog/qwen-moe/"&gt;blog&lt;/a&gt; for more information!&lt;/li&gt; 
 &lt;li&gt;2024.02.05: We released the Qwen1.5 series.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Detailed evaluation results are reported in this &lt;a href="https://qwenlm.github.io/blog/qwen3/"&gt;üìë blog (Qwen3-2504)&lt;/a&gt; and this &lt;a href=""&gt;üìë blog (Qwen3-2507) [coming soon]&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For requirements on GPU memory and the respective throughput, see results &lt;a href="https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Run Qwen3&lt;/h2&gt; 
&lt;h3&gt;ü§ó Transformers&lt;/h3&gt; 
&lt;p&gt;Transformers is a library of pretrained natural language processing for inference and training. The latest version of &lt;code&gt;transformers&lt;/code&gt; is recommended and &lt;code&gt;transformers&amp;gt;=4.51.0&lt;/code&gt; is required.&lt;/p&gt; 
&lt;h4&gt;Qwen3-Instruct-2507&lt;/h4&gt; 
&lt;p&gt;The following contains a code snippet illustrating how to use Qwen3-30B-A3B-Instruct-2507 to generate content based on given inputs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-30B-A3B-Instruct-2507"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=16384
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

content = tokenizer.decode(output_ids, skip_special_tokens=True)

print("content:", content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Qwen3-Instruct-2507 supports only non-thinking mode and does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks in its output. Meanwhile, specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Qwen3-Thinking-2507&lt;/h4&gt; 
&lt;p&gt;The following contains a code snippet illustrating how to use Qwen3-30B-A3B-Thinking-2507 to generate content based on given inputs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-30B-A3B-Thinking-2507"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# parsing thinking content
try:
    # rindex finding 151668 (&amp;lt;/think&amp;gt;)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print("thinking content:", thinking_content)  # no opening &amp;lt;think&amp;gt; tag
print("content:", content)

&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Qwen3-Thinking-2507 supports only thinking mode. Additionally, to enforce model thinking, the default chat template automatically includes &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;. Therefore, it is normal for the model's output to contain only &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; without an explicit opening &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tag.&lt;/p&gt; 
 &lt;p&gt;Qwen3-Thinking-2507 also features an increased thinking length. We strongly recommend its use in highly complex reasoning tasks with adequate maximum generation length.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Switching Thinking/Non-thinking Modes for Previous Qwen3 Models&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt; By default, Qwen3 models will think before response. This could be controlled by &lt;/p&gt;
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;enable_thinking=False&lt;/code&gt;: Passing &lt;code&gt;enable_thinking=False&lt;/code&gt; to `tokenizer.apply_chat_template` will strictly prevent the model from generating thinking content.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;/think&lt;/code&gt; and &lt;code&gt;/no_think&lt;/code&gt; instructions: Use those words in the system or user message to signify whether Qwen3 should think. In multi-turn conversations, the latest instruction is followed.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. ModelScope adopts a Python API similar to Transformers. The CLI tool &lt;code&gt;modelscope download&lt;/code&gt; can help you solve issues concerning downloading checkpoints. For vLLM and SGLang, the environment variable &lt;code&gt;VLLM_USE_MODELSCOPE=true&lt;/code&gt; and &lt;code&gt;SGLANG_USE_MODELSCOPE=true&lt;/code&gt; can be used respectively.&lt;/p&gt; 
&lt;h3&gt;llama.cpp&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp"&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; enables LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware. &lt;code&gt;llama.cpp&amp;gt;=b5401&lt;/code&gt; is recommended for the full support of Qwen3.&lt;/p&gt; 
&lt;p&gt;To use the CLI, run the following in a terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --color -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift
# CTRL+C to exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use the API server, run the following in a terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;./llama-server -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --reasoning-format deepseek -ngl 99 -fa -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -c 40960 -n 32768 --no-context-shift --port 8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A simple web front end will be at &lt;code&gt;http://localhost:8080&lt;/code&gt; and an OpenAI-compatible API will be at &lt;code&gt;http://localhost:8080/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For additional guides, please refer to &lt;a href="https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html"&gt;our documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] llama.cpp adopts "rotating context management" and infinite generation is made possible by evicting earlier tokens. It could configured by parameters and the commands above effectively disable it. For more details, please refer to &lt;a href="https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli"&gt;our documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Ollama&lt;/h3&gt; 
&lt;p&gt;After &lt;a href="https://ollama.com/"&gt;installing Ollama&lt;/a&gt;, you can initiate the Ollama service with the following command (Ollama v0.9.0 or higher is recommended):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama serve
# You need to keep this service running whenever you are using ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To pull a model checkpoint and run the model, use the &lt;code&gt;ollama run&lt;/code&gt; command. You can specify a model size by adding a suffix to &lt;code&gt;qwen3&lt;/code&gt;, such as &lt;code&gt;:8b&lt;/code&gt; or &lt;code&gt;:30b-a3b&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama run qwen3:8b
# Setting parameters, type "/set parameter num_ctx 40960" and "/set parameter num_predict 32768"
# To exit, type "/bye" and press ENTER
# For Qwen3-2504 models,
# - To enable thinking, which is the default, type "/set think"
# - To disable thinking, type "/set nothink"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also access the Ollama service via its OpenAI-compatible API. Please note that you need to (1) keep &lt;code&gt;ollama serve&lt;/code&gt; running while using the API, and (2) execute &lt;code&gt;ollama run qwen3:8b&lt;/code&gt; before utilizing this API to ensure that the model checkpoint is prepared. The API is at &lt;code&gt;http://localhost:11434/v1/&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;For additional details, please visit &lt;a href="https://ollama.com/"&gt;ollama.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Ollama's naming may not be consistent with the Qwen's original naming. For example, &lt;code&gt;qwen3:30b-a3b&lt;/code&gt; in Ollama points to &lt;code&gt;qwen3:30b-a3b-thinking-2507-q4_K_M&lt;/code&gt; as of August 2025. Please check &lt;a href="https://ollama.com/library/qwen3/tags"&gt;https://ollama.com/library/qwen3/tags&lt;/a&gt; before use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Ollama adopts the same "rotating context management" with llama.cpp. However, its default settings (&lt;code&gt;num_ctx&lt;/code&gt; 2048 and &lt;code&gt;num_predict&lt;/code&gt; -1), suggesting infinite generation with a 2048-token context, could lead to trouble for Qwen3 models. We recommend setting &lt;code&gt;num_ctx&lt;/code&gt; and &lt;code&gt;num_predict&lt;/code&gt; properly.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;LMStudio&lt;/h3&gt; 
&lt;p&gt;Qwen3 has already been supported by &lt;a href="https://lmstudio.ai/"&gt;lmstudio.ai&lt;/a&gt;. You can directly use LMStudio with our GGUF files.&lt;/p&gt; 
&lt;h3&gt;ExecuTorch&lt;/h3&gt; 
&lt;p&gt;To export and run on ExecuTorch (iOS, Android, Mac, Linux, and more), please follow this &lt;a href="https://github.com/pytorch/executorch/raw/main/examples/models/qwen3/README.md"&gt;example&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MNN&lt;/h3&gt; 
&lt;p&gt;To export and run on MNN, which supports Qwen3 on mobile devices, please visit &lt;a href="https://github.com/alibaba/MNN"&gt;Alibaba MNN&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MLX LM&lt;/h3&gt; 
&lt;p&gt;If you are running on Apple Silicon, &lt;a href="https://github.com/ml-explore/mlx-lm"&gt;&lt;code&gt;mlx-lm&lt;/code&gt;&lt;/a&gt; also supports Qwen3 (&lt;code&gt;mlx-lm&amp;gt;=0.24.0&lt;/code&gt;). Look for models ending with MLX on Hugging Face Hub.&lt;/p&gt; 
&lt;h3&gt;OpenVINO&lt;/h3&gt; 
&lt;p&gt;If you are running on Intel CPU or GPU, &lt;a href="https://github.com/openvinotoolkit"&gt;OpenVINO toolkit&lt;/a&gt; supports Qwen3. You can follow this &lt;a href="https://github.com/openvinotoolkit/openvino_notebooks/raw/latest/notebooks/llm-chatbot/llm-chatbot.ipynb"&gt;chatbot example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Deploy Qwen3&lt;/h2&gt; 
&lt;p&gt;Qwen3 is supported by multiple inference frameworks. Here we demonstrate the usage of &lt;code&gt;SGLang&lt;/code&gt;, &lt;code&gt;vLLM&lt;/code&gt; and &lt;code&gt;TensorRT-LLM&lt;/code&gt;. You can also find Qwen3 models from various inference providers, e.g., &lt;a href="https://www.alibabacloud.com/en/product/modelstudio"&gt;Alibaba Cloud Model Studio&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;SGLang&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; is a fast serving framework for large language models and vision language models. SGLang could be used to launch a server with OpenAI-compatible API service. &lt;code&gt;sglang&amp;gt;=0.4.6.post1&lt;/code&gt; is required.&lt;/p&gt; 
&lt;p&gt;For Qwen3-Instruct-2507,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Instruct-2507 --port 30000 --context-length 262144
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Qwen3-Thinking-2507,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Thinking-2507 --port 30000 --context-length 262144 --reasoning-parser deepseek-r1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Qwen3, it is&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python -m sglang.launch_server --model-path Qwen/Qwen3-8B --port 30000 --context-length 131072 --reasoning-parser qwen3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:30000/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Due to the preprocessing of API requests in SGLang, which drops all &lt;code&gt;reasoning_content&lt;/code&gt; fields, the quality of &lt;strong&gt;multi-step tool use with Qwen3 thinking models&lt;/strong&gt; may be suboptimal, which requires the existence of the related thinking content. While the fixes are being worked on, as a workdaround, we recommend passing the content as it is, without extracting thinking content, and the chat template will correctly handle the processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;vLLM&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; is a high-throughput and memory-efficient inference and serving engine for LLMs. &lt;code&gt;vllm&amp;gt;=0.9.0&lt;/code&gt; is recommended.&lt;/p&gt; 
&lt;p&gt;For Qwen3-Instruct-2507,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;vllm serve Qwen/Qwen3-30B-A3B-Instruct-2507 --port 8000 --max-model-len 262144
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Qwen3-Thinking-2507,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;vllm serve Qwen/Qwen3-30B-A3B-Thinking-2507 --port 8000 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Qwen3, it is&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;vllm serve Qwen/Qwen3-8B --port 8000 --max-model-len 131072 --enable-reasoning --reasoning-parser qwen3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Due to the preprocessing of API requests in vLLM, which drops all &lt;code&gt;reasoning_content&lt;/code&gt; fields, the quality of &lt;strong&gt;multi-step tool use with Qwen3 thinking models&lt;/strong&gt; may be suboptimal, which requires the existence of the related thinking content. While the fixes are being worked on, as a workdaround, we recommend passing the content as it is, without extracting thinking content, and the chat template will correctly handle the processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;TensorRT-LLM&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt; is an open-source LLM inference engine from NVIDIA, which provides optimizations including custom attention kernels, quantization and more on NVIDIA GPUs. Qwen3 is supported in its re-architected &lt;a href="https://nvidia.github.io/TensorRT-LLM/torch.html"&gt;PyTorch backend&lt;/a&gt;. &lt;code&gt;tensorrt_llm&amp;gt;=0.20.0rc3&lt;/code&gt; is recommended. Please refer to the &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/raw/main/examples/models/core/qwen/README.md#qwen3"&gt;README&lt;/a&gt; page for more details.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;trtllm-serve Qwen/Qwen3-8B --host localhost --port 8000 --backend pytorch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An OpenAI-compatible API will be available at &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;MindIE&lt;/h3&gt; 
&lt;p&gt;For deployment on Ascend NPUs, please visit &lt;a href="https://modelers.cn/"&gt;Modelers&lt;/a&gt; and search for Qwen3.&lt;/p&gt; 
&lt;!-- 
### OpenLLM

[OpenLLM](https://github.com/bentoml/OpenLLM) allows you to easily run¬†Qwen2.5 as OpenAI-compatible APIs. You can start a model server using `openllm serve`. For example:

```bash
openllm serve qwen2.5:7b
```

The server is active at `http://localhost:3000/`, providing OpenAI-compatible APIs. You can create an OpenAI client to call its chat API. For more information, refer to [our documentation](https://qwen.readthedocs.io/en/latest/deployment/openllm.html). --&gt; 
&lt;h2&gt;Build with Qwen3&lt;/h2&gt; 
&lt;h3&gt;Tool Use&lt;/h3&gt; 
&lt;p&gt;For tool use capabilities, we recommend taking a look at &lt;a href="https://github.com/QwenLM/Qwen-Agent"&gt;Qwen-Agent&lt;/a&gt;, which provides a wrapper around these APIs to support tool use or function calling with MCP support. Tool use with Qwen3 can also be conducted with SGLang, vLLM, Transformers, llama.cpp, Ollama, etc. Follow guides in our documentation to see how to enable the support.&lt;/p&gt; 
&lt;h3&gt;Finetuning&lt;/h3&gt; 
&lt;p&gt;We advise you to use training frameworks, including &lt;a href="https://github.com/OpenAccess-AI-Collective/axolotl"&gt;Axolotl&lt;/a&gt;, &lt;a href="https://github.com/unslothai/unsloth"&gt;UnSloth&lt;/a&gt;, &lt;a href="https://github.com/modelscope/swift"&gt;Swift&lt;/a&gt;, &lt;a href="https://github.com/hiyouga/LLaMA-Factory"&gt;Llama-Factory&lt;/a&gt;, etc., to finetune your models with SFT, DPO, GRPO, etc.&lt;/p&gt; 
&lt;h2&gt;License Agreement&lt;/h2&gt; 
&lt;p&gt;All our open-weight models are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{qwen3,
    title={Qwen3 Technical Report}, 
    author={An Yang and Anfeng Li and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Gao and Chengen Huang and Chenxu Lv and Chujie Zheng and Dayiheng Liu and Fan Zhou and Fei Huang and Feng Hu and Hao Ge and Haoran Wei and Huan Lin and Jialong Tang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jing Zhou and Jingren Zhou and Junyang Lin and Kai Dang and Keqin Bao and Kexin Yang and Le Yu and Lianghao Deng and Mei Li and Mingfeng Xue and Mingze Li and Pei Zhang and Peng Wang and Qin Zhu and Rui Men and Ruize Gao and Shixuan Liu and Shuang Luo and Tianhao Li and Tianyi Tang and Wenbiao Yin and Xingzhang Ren and Xinyu Wang and Xinyu Zhang and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yinger Zhang and Yu Wan and Yuqiong Liu and Zekun Wang and Zeyu Cui and Zhenru Zhang and Zhipeng Zhou and Zihan Qiu},
    journal = {arXiv preprint arXiv:2505.09388},
    year={2025}
}

@article{qwen2.5,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    journal = {arXiv preprint arXiv:2412.15115},
    year    = {2024}
}

@article{qwen2,
    title   = {Qwen2 Technical Report}, 
    author  = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},
    journal = {arXiv preprint arXiv:2407.10671},
    year    = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;üåê Make websites accessible for AI agents. Automate tasks online with ease.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" " /&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/774a46d5-27a0-490c-b7d0-e65fcbbfa358" /&gt; 
 &lt;img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" width="full" /&gt; 
&lt;/picture&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" " /&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/6797d09b-8ac3-4cb9-ba07-b289e080765a" /&gt; 
  &lt;img alt="The AI browser agent." src="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" width="400" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/package" height="48" alt="Browser-Use Package Download Statistics" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/#demos"&gt;&lt;img src="https://media.browser-use.tools/badges/demos" alt="Demos" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://docs.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/docs" alt="Docs" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browser-use.com/posts"&gt;&lt;img src="https://media.browser-use.tools/badges/blog" alt="Blog" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browsermerch.com"&gt;&lt;img src="https://media.browser-use.tools/badges/merch" alt="Merch" /&gt;&lt;/a&gt; 
 &lt;img width="100" height="1" alt="" /&gt; 
 &lt;a href="https://github.com/browser-use/browser-use"&gt;&lt;img src="https://media.browser-use.tools/badges/github" alt="Github Stars" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://x.com/intent/user?screen_name=browser_use"&gt;&lt;img src="https://media.browser-use.tools/badges/twitter" alt="Twitter" /&gt;&lt;/a&gt; 
 &lt;img width="4 height=" 1" alt="" /&gt; 
 &lt;a href="https://link.browser-use.com/discord"&gt;&lt;img src="https://media.browser-use.tools/badges/discord" alt="Discord" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/cloud" height="48" alt="Browser-Use Cloud" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;üå§Ô∏è Want to skip the setup? Use our &lt;b&gt;&lt;a href="https://cloud.browser-use.com"&gt;cloud&lt;/a&gt;&lt;/b&gt; for faster, scalable, stealth-enabled browser automation!&lt;/p&gt; 
&lt;h1&gt;ü§ñ LLM Quickstart&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;Direct your favorite coding agent (Cursor, Claude Code, etc) to &lt;a href="https://docs.browser-use.com/llms-full.txt"&gt;Agents.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Prompt away!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;h1&gt;üëã Human Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;1. Create environment with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; (Python&amp;gt;=3.11):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. Install Browser-Use package:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#  We ship every day - use the latest version!
uv add browser-use
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Get your API key from &lt;a href="https://cloud.browser-use.com/new-api-key"&gt;Browser Use Cloud&lt;/a&gt; and add it to your &lt;code&gt;.env&lt;/code&gt; file (new signups get $10 free credits):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# .env
BROWSER_USE_API_KEY=your-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. Install Chromium browser:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. Run your first agent:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Agent, Browser, ChatBrowserUse
import asyncio

async def example():
    browser = Browser(
        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud
    )

    llm = ChatBrowserUse()

    agent = Agent(
        task="Find the number of stars of the browser-use repo",
        llm=llm,
        browser=browser,
    )

    history = await agent.run()
    return history

if __name__ == "__main__":
    history = asyncio.run(example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://docs.browser-use.com"&gt;library docs&lt;/a&gt; and the &lt;a href="https://docs.cloud.browser-use.com"&gt;cloud docs&lt;/a&gt; for more!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üî• Deploy on Sandboxes&lt;/h1&gt; 
&lt;p&gt;We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Browser, sandbox, ChatBrowserUse
from browser_use.agent.service import Agent
import asyncio

@sandbox()
async def my_task(browser: Browser):
    agent = Agent(task="Find the top HN post", browser=browser, llm=ChatBrowserUse())
    await agent.run()

# Just call it like any async function
asyncio.run(my_task())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://docs.browser-use.com/production"&gt;Going to Production&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üöÄ Template Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Want to get started even faster?&lt;/strong&gt; Generate a ready-to-run template:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;browser_use_default.py&lt;/code&gt; file with a working example. Available templates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;default&lt;/code&gt; - Minimal setup to get started quickly&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;advanced&lt;/code&gt; - All configuration options with detailed comments&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tools&lt;/code&gt; - Examples of custom tools and extending the agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also specify a custom output path:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default --output my_agent.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;üíª CLI&lt;/h1&gt; 
&lt;p&gt;Fast, persistent browser automation from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;browser-use open https://example.com    # Navigate to URL
browser-use state                       # See clickable elements
browser-use click 5                     # Click element by index
browser-use type "Hello"                # Type text
browser-use screenshot page.png         # Take screenshot
browser-use close                       # Close browser
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The CLI keeps the browser running between commands for fast iteration. See &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/browser_use/skill_cli/README.md"&gt;CLI docs&lt;/a&gt; for all commands.&lt;/p&gt; 
&lt;h3&gt;Claude Code Skill&lt;/h3&gt; 
&lt;p&gt;For &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;, install the skill to enable AI-assisted browser automation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/.claude/skills/browser-use
curl -o ~/.claude/skills/browser-use/SKILL.md \
  https://raw.githubusercontent.com/browser-use/browser-use/main/skills/browser-use/SKILL.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;h3&gt;üìã Form-Filling&lt;/h3&gt; 
&lt;h4&gt;Task = "Fill in this job application with my resume and information."&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/57865ee6-6004-49d5-b2c2-6dff39ec2ba9" alt="Job Application Demo" /&gt; &lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/apply_to_job.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üçé Grocery-Shopping&lt;/h3&gt; 
&lt;h4&gt;Task = "Put this list of items into my instacart."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850"&gt;https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/buy_groceries.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üíª Personal-Assistant.&lt;/h3&gt; 
&lt;h4&gt;Task = "Help me find parts for a custom PC."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06"&gt;https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/pcpartpicker.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üí°See &lt;a href="https://docs.browser-use.com/examples"&gt;more examples here ‚Üó&lt;/a&gt; and give us a star!&lt;/h3&gt; 
&lt;br /&gt; 
&lt;h2&gt;Integrations, hosting, custom tools, MCP, and more on our &lt;a href="https://docs.browser-use.com"&gt;Docs ‚Üó&lt;/a&gt;&lt;/h2&gt; 
&lt;br /&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;What's the best model to use?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;We optimized &lt;strong&gt;ChatBrowserUse()&lt;/strong&gt; specifically for browser automation tasks. On avg it completes tasks 3-5x faster than other models with SOTA accuracy.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Pricing (per 1M tokens):&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Input tokens: $0.20&lt;/li&gt; 
  &lt;li&gt;Cached input tokens: $0.02&lt;/li&gt; 
  &lt;li&gt;Output tokens: $2.00&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For other LLM providers, see our &lt;a href="https://docs.browser-use.com/supported-models"&gt;supported models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use custom tools with the agent?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! You can add custom tools to extend the agent's capabilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Tools

tools = Tools()

@tools.action(description='Description of what this tool does.')
def custom_tool(param: str) -&amp;gt; str:
    return f"Result: {param}"

agent = Agent(
    task="Your task",
    llm=llm,
    browser=browser,
    tools=tools,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use this for free?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! Browser-Use is open source and free to use. You only need to choose an LLM provider (like OpenAI, Google, ChatBrowserUse, or run local models with Ollama).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I handle authentication?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Check out our authentication examples:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py"&gt;Using real browser profiles&lt;/a&gt; - Reuse your existing Chrome profile with saved logins&lt;/li&gt; 
  &lt;li&gt;If you want to use temporary accounts with inbox, choose AgentMail&lt;/li&gt; 
  &lt;li&gt;To sync your auth profile with the remote browser, run &lt;code&gt;curl -fsSL https://browser-use.com/profile.sh | BROWSER_USE_API_KEY=XXXX sh&lt;/code&gt; (replace XXXX with your API key)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These examples show how to maintain sessions and handle authentication seamlessly.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I solve CAPTCHAs?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;For CAPTCHA handling, you need better browser fingerprinting and proxies. Use &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud&lt;/a&gt; which provides stealth browsers designed to avoid detection and CAPTCHA challenges.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I go into production?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Chrome can consume a lot of memory, and running many agents in parallel can be tricky to manage.&lt;/p&gt; 
 &lt;p&gt;For production use cases, use our &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud API&lt;/a&gt; which handles:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Scalable browser infrastructure&lt;/li&gt; 
  &lt;li&gt;Memory management&lt;/li&gt; 
  &lt;li&gt;Proxy rotation&lt;/li&gt; 
  &lt;li&gt;Stealth browser fingerprinting&lt;/li&gt; 
  &lt;li&gt;High-performance parallel execution&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Tell your computer what to do, and it gets it done.&lt;/strong&gt;&lt;/p&gt; 
 &lt;img src="https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f" width="400" /&gt; 
 &lt;p&gt;&lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; ‚ÄÉ‚ÄÉ‚ÄÉ &lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  Made with ‚ù§Ô∏è in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>