<rss version="2.0">
  <channel>
    <title>GitHub All Languages Monthly Trending</title>
    <description>Monthly Trending of All Languages in GitHub</description>
    <pubDate>Tue, 04 Nov 2025 01:48:51 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GitHub Workflow &amp;amp; Templates&lt;/h1&gt; 
&lt;p&gt;This folder contains GitHub-specific configuration for Open Notebook's contribution workflow.&lt;/p&gt; 
&lt;h2&gt;ğŸ“‹ Issue Templates&lt;/h2&gt; 
&lt;p&gt;We have three issue templates to guide contributors:&lt;/p&gt; 
&lt;h3&gt;ğŸ› Bug Report (&lt;code&gt;ISSUE_TEMPLATE/bug_report.yml&lt;/code&gt;)&lt;/h3&gt; 
&lt;p&gt;For reporting bugs or unexpected behavior when the app is running but misbehaving.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Structured format for bug details&lt;/li&gt; 
 &lt;li&gt;Environment and version information&lt;/li&gt; 
 &lt;li&gt;Checkbox for contributors who want to fix the issue&lt;/li&gt; 
 &lt;li&gt;Automatic &lt;code&gt;bug&lt;/code&gt; and &lt;code&gt;needs-triage&lt;/code&gt; labels&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;When to Use:&lt;/strong&gt; App is installed and running, but something doesn't work as expected.&lt;/p&gt; 
&lt;h3&gt;âœ¨ Feature Request (&lt;code&gt;ISSUE_TEMPLATE/feature_request.yml&lt;/code&gt;)&lt;/h3&gt; 
&lt;p&gt;For suggesting new features or improvements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Description of the feature&lt;/li&gt; 
 &lt;li&gt;Explanation of why it would be helpful&lt;/li&gt; 
 &lt;li&gt;Space for proposed solution&lt;/li&gt; 
 &lt;li&gt;Checkbox for contributors who want to implement it&lt;/li&gt; 
 &lt;li&gt;Automatic &lt;code&gt;enhancement&lt;/code&gt; and &lt;code&gt;needs-triage&lt;/code&gt; labels&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;When to Use:&lt;/strong&gt; You have an idea for a new feature or improvement.&lt;/p&gt; 
&lt;h3&gt;ğŸ”§ Installation Issue (&lt;code&gt;ISSUE_TEMPLATE/installation_issue.yml&lt;/code&gt;)&lt;/h3&gt; 
&lt;p&gt;For problems with installation or setup.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Deployment type selection&lt;/li&gt; 
 &lt;li&gt;Environment details&lt;/li&gt; 
 &lt;li&gt;Error message collection&lt;/li&gt; 
 &lt;li&gt;Automatic &lt;code&gt;installation&lt;/code&gt; label&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;When to Use:&lt;/strong&gt; Having trouble getting Open Notebook running.&lt;/p&gt; 
&lt;h2&gt;ğŸ”„ Pull Request Template&lt;/h2&gt; 
&lt;p&gt;The PR template (&lt;code&gt;pull_request_template.md&lt;/code&gt;) ensures contributors provide all necessary information:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Sections:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Description and related issue&lt;/li&gt; 
 &lt;li&gt;Type of change&lt;/li&gt; 
 &lt;li&gt;Testing details&lt;/li&gt; 
 &lt;li&gt;Design alignment with project principles&lt;/li&gt; 
 &lt;li&gt;Comprehensive checklist for code quality, testing, documentation&lt;/li&gt; 
 &lt;li&gt;Screenshots for UI changes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Key Checkpoints:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;References an approved issue&lt;/li&gt; 
 &lt;li&gt;Aligns with &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/DESIGN_PRINCIPLES.md"&gt;design principles&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Includes tests and documentation&lt;/li&gt; 
 &lt;li&gt;Follows code style guidelines&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ GitHub Actions&lt;/h2&gt; 
&lt;p&gt;Our CI/CD workflows in &lt;code&gt;.github/workflows/&lt;/code&gt;:&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;build-and-release.yml&lt;/code&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Builds Docker images on releases&lt;/li&gt; 
 &lt;li&gt;Publishes to Docker Hub and GitHub Container Registry&lt;/li&gt; 
 &lt;li&gt;Supports multi-platform builds (amd64, arm64)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;code&gt;build-dev.yml&lt;/code&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Builds dev images on pushes to main&lt;/li&gt; 
 &lt;li&gt;Tags with commit SHA for testing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;code&gt;claude-code-review.yml&lt;/code&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automated code review using Claude Code&lt;/li&gt; 
 &lt;li&gt;Runs on pull requests&lt;/li&gt; 
 &lt;li&gt;Provides AI-powered suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“– How the Contribution Flow Works&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Contributor identifies a bug or has a feature idea   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Creates an issue using appropriate template          â”‚
â”‚    - Describes the problem/feature                      â”‚
â”‚    - Checks "I am a developer..." if willing to work    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Maintainer reviews issue (within 48 hours)           â”‚
â”‚    - Assesses alignment with design principles          â”‚
â”‚    - Labels appropriately                               â”‚
â”‚    - Asks for clarification if needed                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. If approved: Contributor proposes solution approach  â”‚
â”‚    - Discusses implementation strategy                  â”‚
â”‚    - Maintainer provides feedback                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Issue is assigned to contributor                     â”‚
â”‚    - Contributor forks repo                             â”‚
â”‚    - Creates feature branch from main                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Contributor develops solution                        â”‚
â”‚    - Reads DESIGN_PRINCIPLES.md                         â”‚
â”‚    - Reads docs/development/architecture.md             â”‚
â”‚    - Writes code, tests, documentation                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Creates Pull Request                                 â”‚
â”‚    - Uses PR template                                   â”‚
â”‚    - References issue number                            â”‚
â”‚    - Fills out all checklist items                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Maintainer reviews PR                                â”‚
â”‚    - Checks code quality                                â”‚
â”‚    - Verifies tests pass                                â”‚
â”‚    - Ensures alignment with architecture                â”‚
â”‚    - Provides feedback or approves                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. If approved: PR is merged! ğŸ‰                        â”‚
â”‚    - Contributor is thanked                             â”‚
â”‚    - Issue is closed                                    â”‚
â”‚    - Changes included in next release                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¯ Why This Process?&lt;/h2&gt; 
&lt;h3&gt;Prevents Wasted Effort&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Contributors don't spend time on code that won't be merged&lt;/li&gt; 
 &lt;li&gt;Ensures alignment with project vision before coding starts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Maintains Quality&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;All code reviewed against design principles&lt;/li&gt; 
 &lt;li&gt;Consistent architecture across contributions&lt;/li&gt; 
 &lt;li&gt;Proper testing and documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Respects Everyone's Time&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Clear expectations upfront&lt;/li&gt; 
 &lt;li&gt;Structured feedback process&lt;/li&gt; 
 &lt;li&gt;Efficient review process&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Protects Project Vision&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Maintainers can guide direction&lt;/li&gt; 
 &lt;li&gt;Features align with long-term goals&lt;/li&gt; 
 &lt;li&gt;Technical debt is minimized&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“š Related Documentation&lt;/h2&gt; 
&lt;p&gt;For contributors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; - Contribution guidelines&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/DESIGN_PRINCIPLES.md"&gt;DESIGN_PRINCIPLES.md&lt;/a&gt; - Project vision and principles&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/docs/development/architecture.md"&gt;docs/development/architecture.md&lt;/a&gt; - Technical architecture&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For maintainers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/MAINTAINER_GUIDE.md"&gt;MAINTAINER_GUIDE.md&lt;/a&gt; - How to review and manage contributions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ Questions?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Open a &lt;a href="https://github.com/lfnovo/open-notebook/discussions"&gt;Discussion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/docs/troubleshooting/faq.md"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Thank you for contributing to Open Notebook!&lt;/strong&gt; Your contributions help make this the best open-source research tool available.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>cjpais/Handy</title>
      <link>https://github.com/cjpais/Handy</link>
      <description>&lt;p&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Handy&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://discord.com/invite/WVBeWsNXK4"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Handy is a cross-platform desktop application built with Tauri (Rust + React/TypeScript) that provides simple, privacy-focused speech transcription. Press a shortcut, speak, and have your words appear in any text fieldâ€”all without sending your voice to the cloud.&lt;/p&gt; 
&lt;h2&gt;Why Handy?&lt;/h2&gt; 
&lt;p&gt;Handy was created to fill the gap for a truly open source, extensible speech-to-text tool. As stated on &lt;a href="https://handy.computer"&gt;handy.computer&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt;: Accessibility tooling belongs in everyone's hands, not behind a paywall&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Together we can build further. Extend Handy for yourself and contribute to something bigger&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: Your voice stays on your computer. Get transcriptions without sending audio to the cloud&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: One tool, one job. Transcribe what you say and put it into a text box&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Handy isn't trying to be the best speech-to-text appâ€”it's trying to be the most forkable one.&lt;/p&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Press&lt;/strong&gt; a configurable keyboard shortcut to start/stop recording (or use push-to-talk mode)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speak&lt;/strong&gt; your words while the shortcut is active&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Release&lt;/strong&gt; and Handy processes your speech using Whisper&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Get&lt;/strong&gt; your transcribed text pasted directly into whatever app you're using&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The process is entirely local:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Silence is filtered using VAD (Voice Activity Detection) with Silero&lt;/li&gt; 
 &lt;li&gt;Transcription uses your choice of models: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Whisper models&lt;/strong&gt; (Small/Medium/Turbo/Large) with GPU acceleration when available&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parakeet V3&lt;/strong&gt; - CPU-optimized model with excellent performance and automatic language detection&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Works on Windows, macOS, and Linux&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the latest release from the &lt;a href="https://github.com/cjpais/Handy/releases"&gt;releases page&lt;/a&gt; or the &lt;a href="https://handy.computer"&gt;website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install the application following platform-specific instructions&lt;/li&gt; 
 &lt;li&gt;Launch Handy and grant necessary system permissions (microphone, accessibility)&lt;/li&gt; 
 &lt;li&gt;Configure your preferred keyboard shortcuts in Settings&lt;/li&gt; 
 &lt;li&gt;Start transcribing!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Development Setup&lt;/h3&gt; 
&lt;p&gt;For detailed build instructions including platform-specific requirements, see &lt;a href="https://raw.githubusercontent.com/cjpais/Handy/main/BUILD.md"&gt;BUILD.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Handy is built as a Tauri application combining:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + TypeScript with Tailwind CSS for the settings UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Rust for system integration, audio processing, and ML inference&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core Libraries&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;whisper-rs&lt;/code&gt;: Local speech recognition with Whisper models&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;transcription-rs&lt;/code&gt;: CPU-optimized speech recognition with Parakeet models&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cpal&lt;/code&gt;: Cross-platform audio I/O&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;vad-rs&lt;/code&gt;: Voice Activity Detection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;rdev&lt;/code&gt;: Global keyboard shortcuts and system events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;rubato&lt;/code&gt;: Audio resampling&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Debug Mode&lt;/h3&gt; 
&lt;p&gt;Handy includes an advanced debug mode for development and troubleshooting. Access it by pressing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;Cmd+Shift+D&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows/Linux&lt;/strong&gt;: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Manual Model Installation (For Proxy Users or Network Restrictions)&lt;/h3&gt; 
&lt;p&gt;If you're behind a proxy, firewall, or in a restricted network environment where Handy cannot download models automatically, you can manually download and install them. The URLs are publicly accessible from any browser.&lt;/p&gt; 
&lt;h4&gt;Step 1: Find Your App Data Directory&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open Handy settings&lt;/li&gt; 
 &lt;li&gt;Navigate to the &lt;strong&gt;About&lt;/strong&gt; section&lt;/li&gt; 
 &lt;li&gt;Copy the "App Data Directory" path shown there, or use the shortcuts: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;Cmd+Shift+D&lt;/code&gt; to open debug menu&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Windows/Linux&lt;/strong&gt;: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt; to open debug menu&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The typical paths are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;~/Library/Application Support/com.pais.handy/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;code&gt;C:\Users\{username}\AppData\Roaming\com.pais.handy\&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: &lt;code&gt;~/.config/com.pais.handy/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Step 2: Create Models Directory&lt;/h4&gt; 
&lt;p&gt;Inside your app data directory, create a &lt;code&gt;models&lt;/code&gt; folder if it doesn't already exist:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS/Linux
mkdir -p ~/Library/Application\ Support/com.pais.handy/models

# Windows (PowerShell)
New-Item -ItemType Directory -Force -Path "$env:APPDATA\com.pais.handy\models"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 3: Download Model Files&lt;/h4&gt; 
&lt;p&gt;Download the models you want from &lt;a href="https://blob.handy.computer/"&gt;https://blob.handy.computer/&lt;/a&gt; using your web browser or download tool. All files are publicly accessible.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Whisper Models (single .bin files):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Small (487 MB): &lt;code&gt;https://blob.handy.computer/ggml-small.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Medium (492 MB): &lt;code&gt;https://blob.handy.computer/whisper-medium-q4_1.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Turbo (1600 MB): &lt;code&gt;https://blob.handy.computer/ggml-large-v3-turbo.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Large (1100 MB): &lt;code&gt;https://blob.handy.computer/ggml-large-v3-q5_0.bin&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Parakeet Models (compressed archives):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;V2 (473 MB): &lt;code&gt;https://blob.handy.computer/parakeet-v2-int8.tar.gz&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;V3 (478 MB): &lt;code&gt;https://blob.handy.computer/parakeet-v3-int8.tar.gz&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Step 4: Install Models&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;For Whisper Models (.bin files):&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Simply place the &lt;code&gt;.bin&lt;/code&gt; file directly into the &lt;code&gt;models&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{app_data_dir}/models/
â”œâ”€â”€ ggml-small.bin
â”œâ”€â”€ whisper-medium-q4_1.bin
â”œâ”€â”€ ggml-large-v3-turbo.bin
â””â”€â”€ ggml-large-v3-q5_0.bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;For Parakeet Models (.tar.gz archives):&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Extract the &lt;code&gt;.tar.gz&lt;/code&gt; file&lt;/li&gt; 
 &lt;li&gt;Place the &lt;strong&gt;extracted directory&lt;/strong&gt; into the &lt;code&gt;models&lt;/code&gt; folder&lt;/li&gt; 
 &lt;li&gt;The directory must be named exactly as follows: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Parakeet V2&lt;/strong&gt;: &lt;code&gt;parakeet-tdt-0.6b-v2-int8&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parakeet V3&lt;/strong&gt;: &lt;code&gt;parakeet-tdt-0.6b-v3-int8&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Final structure should look like:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{app_data_dir}/models/
â”œâ”€â”€ parakeet-tdt-0.6b-v2-int8/     (directory with model files inside)
â”‚   â”œâ”€â”€ (model files)
â”‚   â””â”€â”€ (config files)
â””â”€â”€ parakeet-tdt-0.6b-v3-int8/     (directory with model files inside)
    â”œâ”€â”€ (model files)
    â””â”€â”€ (config files)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For Parakeet models, the extracted directory name &lt;strong&gt;must&lt;/strong&gt; match exactly as shown above&lt;/li&gt; 
 &lt;li&gt;Do not rename the &lt;code&gt;.bin&lt;/code&gt; files for Whisper modelsâ€”use the exact filenames from the download URLs&lt;/li&gt; 
 &lt;li&gt;After placing the files, restart Handy to detect the new models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Step 5: Verify Installation&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Restart Handy&lt;/li&gt; 
 &lt;li&gt;Open Settings â†’ Models&lt;/li&gt; 
 &lt;li&gt;Your manually installed models should now appear as "Downloaded"&lt;/li&gt; 
 &lt;li&gt;Select the model you want to use and test transcription&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Known Issues &amp;amp; Current Limitations&lt;/h2&gt; 
&lt;p&gt;This project is actively being developed and has some &lt;a href="https://github.com/cjpais/Handy/issues"&gt;known issues&lt;/a&gt;. We believe in transparency about the current state:&lt;/p&gt; 
&lt;h3&gt;Platform Support&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS (both Intel and Apple Silicon)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;x64 Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;x64 Linux&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Requirements/Recommendations&lt;/h3&gt; 
&lt;p&gt;The following are recommendations for running Handy on your own machine. If you don't meet the system requirements, the performance of the application may be degraded. We are working on improving the performance across all kinds of computers and hardware.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For Whisper Models:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: M series Mac, Intel Mac&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Intel, AMD, or NVIDIA GPU&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Intel, AMD, or NVIDIA GPU 
  &lt;ul&gt; 
   &lt;li&gt;Ubuntu 22.04, 24.04&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;For Parakeet V3 Model:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CPU-only operation&lt;/strong&gt; - runs on a wide variety of hardware&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Minimum&lt;/strong&gt;: Intel Skylake (6th gen) or equivalent AMD processors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: ~5x real-time speed on mid-range hardware (tested on i5)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic language detection&lt;/strong&gt; - no manual language selection required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Check existing issues&lt;/strong&gt; at &lt;a href="https://github.com/cjpais/Handy/issues"&gt;github.com/cjpais/Handy/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fork the repository&lt;/strong&gt; and create a feature branch&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test thoroughly&lt;/strong&gt; on your target platform&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Submit a pull request&lt;/strong&gt; with clear description of changes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Join the discussion&lt;/strong&gt; - reach out at &lt;a href="mailto:contact@handy.computer"&gt;contact@handy.computer&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The goal is to create both a useful tool and a foundation for others to build uponâ€”a well-patterned, simple codebase that serves the community.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;div align="center"&gt;
  We're grateful for the support of our sponsors who help make Handy possible: 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://wordcab.com"&gt; &lt;img src="https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/wordcab.png" alt="Wordcab" width="120" height="120" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
 &lt;a href="https://github.com/epicenter-so/epicenter"&gt; &lt;img src="https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/epicenter.png" alt="Epicenter" width="120" height="120" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/cjpais/handy-cli"&gt;Handy CLI&lt;/a&gt;&lt;/strong&gt; - The original Python command-line version&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://handy.computer"&gt;handy.computer&lt;/a&gt;&lt;/strong&gt; - Project website with demos and documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/cjpais/Handy/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Whisper&lt;/strong&gt; by OpenAI for the speech recognition model&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;whisper.cpp and ggml&lt;/strong&gt; for amazing cross-platform whisper inference/acceleration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Silero&lt;/strong&gt; for great lightweight VAD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tauri&lt;/strong&gt; team for the excellent Rust-based app framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community contributors&lt;/strong&gt; helping make Handy better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Your search for the right speech-to-text tool can end hereâ€”not because Handy is perfect, but because you can make it perfect for you."&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>coinbase/x402</title>
      <link>https://github.com/coinbase/x402</link>
      <description>&lt;p&gt;A payments protocol for the internet. Built on HTTP.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;x402 payments protocol&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"1 line of code to accept digital dollars. No fee, 2 second settlement, $0.001 minimum payment."&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;app.use(
  // How much you want to charge, and where you want the funds to land
  paymentMiddleware("0xYourAddress", { "/your-endpoint": "$0.01" })
);
// That's it! See examples/typescript/servers/express.ts for a complete example. Instruction below for running on base-sepolia.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Philosophy&lt;/h2&gt; 
&lt;p&gt;Payments on the internet are fundamentally flawed. Credit Cards are high friction, hard to accept, have minimum payments that are far too high, and don't fit into the programmatic nature of the internet. It's time for an open, internet-native form of payments. A payment rail that doesn't have high minimums + % based fee. Payments that are amazing for humans and AI agents.&lt;/p&gt; 
&lt;h2&gt;Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Open standard:&lt;/strong&gt; the x402 protocol will never force reliance on a single party&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP Native:&lt;/strong&gt; x402 is meant to seamlessly complement the existing HTTP request made by traditional web services, it should not mandate additional requests outside the scope of a typical client / server flow.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chain and token agnostic:&lt;/strong&gt; we welcome contributions that add support for new chains, signing standards, or schemes, so long as they meet our acceptance criteria laid out in &lt;a href="https://github.com/coinbase/x402/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Trust minimizing:&lt;/strong&gt; all payment schemes must not allow for the facilitator or resource server to move funds, other than in accordance with client intentions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy to use:&lt;/strong&gt; x402 needs to be 10x better than existing ways to pay on the internet. This means abstracting as many details of crypto as possible away from the client and resource server, and into the facilitator. This means the client/server should not need to think about gas, rpc, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Ecosystem&lt;/h2&gt; 
&lt;p&gt;The x402 ecosystem is growing! Check out our &lt;a href="https://x402.org/ecosystem"&gt;ecosystem page&lt;/a&gt; to see projects building with x402, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Client-side integrations&lt;/li&gt; 
 &lt;li&gt;Services and endpoints&lt;/li&gt; 
 &lt;li&gt;Ecosystem infrastructure and tooling&lt;/li&gt; 
 &lt;li&gt;Learning and community resources&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want to add your project to the ecosystem? See our &lt;a href="https://github.com/coinbase/x402/tree/main/typescript/site#adding-your-project-to-the-ecosystem"&gt;demo site README&lt;/a&gt; for detailed instructions on how to submit your project.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Roadmap:&lt;/strong&gt; see &lt;a href="https://github.com/coinbase/x402/raw/main/ROADMAP.md"&gt;ROADMAP.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Terms:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;resource&lt;/code&gt;: Something on the internet. This could be a webpage, file server, RPC service, API, any resource on the internet that accepts HTTP / HTTPS requests.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;client&lt;/code&gt;: An entity wanting to pay for a resource.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;facilitator server&lt;/code&gt;: A server that facilitates verification and execution of on-chain payments.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resource server&lt;/code&gt;: An HTTP server that provides an API or other resource for a client.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Technical Goals:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Permissionless and secure for clients and servers&lt;/li&gt; 
 &lt;li&gt;Gasless for client and resource servers&lt;/li&gt; 
 &lt;li&gt;Minimal integration for the resource server and client (1 line for the server, 1 function for the client)&lt;/li&gt; 
 &lt;li&gt;Ability to trade off speed of response for guarantee of payment&lt;/li&gt; 
 &lt;li&gt;Extensible to different payment flows and chains&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;V1 Protocol&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;x402&lt;/code&gt; protocol is a chain agnostic standard for payments on top of HTTP, leverage the existing &lt;code&gt;402 Payment Required&lt;/code&gt; HTTP status code to indicate that a payment is required for access to the resource.&lt;/p&gt; 
&lt;p&gt;It specifies:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A schema for how servers can respond to clients to facilitate payment for a resource (&lt;code&gt;PaymentRequirements&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A standard header &lt;code&gt;X-PAYMENT&lt;/code&gt; that is set by clients paying for resources&lt;/li&gt; 
 &lt;li&gt;A standard schema and encoding method for data in the &lt;code&gt;X-PAYMENT&lt;/code&gt; header&lt;/li&gt; 
 &lt;li&gt;A recommended flow for how payments should be verified and settled by a resource server&lt;/li&gt; 
 &lt;li&gt;A REST specification for how a resource server can perform verification and settlement against a remote 3rd party server (&lt;code&gt;facilitator&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A specification for a &lt;code&gt;X-PAYMENT-RESPONSE&lt;/code&gt; header that can be used by resource servers to communicate blockchain transactions details to the client in their HTTP response&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;V1 Protocol Sequencing&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/coinbase/x402/main/static/x402-protocol-flow.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;The following outlines the flow of a payment using the &lt;code&gt;x402&lt;/code&gt; protocol. Note that steps (1) and (2) are optional if the client already knows the payment details accepted for a resource.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; makes an HTTP request to a &lt;code&gt;resource server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; responds with a &lt;code&gt;402 Payment Required&lt;/code&gt; status and a &lt;code&gt;Payment Required Response&lt;/code&gt; JSON object in the response body.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; selects one of the &lt;code&gt;paymentRequirements&lt;/code&gt; returned by the server response and creates a &lt;code&gt;Payment Payload&lt;/code&gt; based on the &lt;code&gt;scheme&lt;/code&gt; of the &lt;code&gt;paymentRequirements&lt;/code&gt; they have selected.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; sends the HTTP request with the &lt;code&gt;X-PAYMENT&lt;/code&gt; header containing the &lt;code&gt;Payment Payload&lt;/code&gt; to the resource server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; verifies the &lt;code&gt;Payment Payload&lt;/code&gt; is valid either via local verification or by POSTing the &lt;code&gt;Payment Payload&lt;/code&gt; and &lt;code&gt;Payment Requirements&lt;/code&gt; to the &lt;code&gt;/verify&lt;/code&gt; endpoint of a &lt;code&gt;facilitator server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; performs verification of the object based on the &lt;code&gt;scheme&lt;/code&gt; and &lt;code&gt;network&lt;/code&gt; of the &lt;code&gt;Payment Payload&lt;/code&gt; and returns a &lt;code&gt;Verification Response&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If the &lt;code&gt;Verification Response&lt;/code&gt; is valid, the resource server performs the work to fulfill the request. If the &lt;code&gt;Verification Response&lt;/code&gt; is invalid, the resource server returns a &lt;code&gt;402 Payment Required&lt;/code&gt; status and a &lt;code&gt;Payment Required Response&lt;/code&gt; JSON object in the response body.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; either settles the payment by interacting with a blockchain directly, or by POSTing the &lt;code&gt;Payment Payload&lt;/code&gt; and &lt;code&gt;Payment PaymentRequirements&lt;/code&gt; to the &lt;code&gt;/settle&lt;/code&gt; endpoint of a &lt;code&gt;facilitator server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; submits the payment to the blockchain based on the &lt;code&gt;scheme&lt;/code&gt; and &lt;code&gt;network&lt;/code&gt; of the &lt;code&gt;Payment Payload&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; waits for the payment to be confirmed on the blockchain.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; returns a &lt;code&gt;Payment Execution Response&lt;/code&gt; to the resource server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; returns a &lt;code&gt;200 OK&lt;/code&gt; response to the &lt;code&gt;Client&lt;/code&gt; with the resource they requested as the body of the HTTP response, and a &lt;code&gt;X-PAYMENT-RESPONSE&lt;/code&gt; header containing the &lt;code&gt;Settlement Response&lt;/code&gt; as Base64 encoded JSON if the payment was executed successfully.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Type Specifications&lt;/h3&gt; 
&lt;h4&gt;Data types&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Payment Required Response&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Version of the x402 payment protocol
  x402Version: int,

  // List of payment requirements that the resource server accepts. A resource server may accept on multiple chains, or in multiple currencies.
  accepts: [paymentRequirements]

  // Message from the resource server to the client to communicate errors in processing payment
  error: string
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;paymentRequirements&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Scheme of the payment protocol to use
  scheme: string;

  // Network of the blockchain to send payment on
  network: string;

  // Maximum amount required to pay for the resource in atomic units of the asset
  maxAmountRequired: uint256 as string;

  // URL of resource to pay for
  resource: string;

  // Description of the resource
  description: string;

  // MIME type of the resource response
  mimeType: string;

  // Output schema of the resource response
  outputSchema?: object | null;

  // Address to pay value to
  payTo: string;

  // Maximum time in seconds for the resource server to respond
  maxTimeoutSeconds: number;

  // Address of the EIP-3009 compliant ERC20 contract
  asset: string;

  // Extra information about the payment details specific to the scheme
  // For `exact` scheme on a EVM network, expects extra to contain the records `name` and `version` pertaining to asset
  extra: object | null;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;code&gt;Payment Payload&lt;/code&gt;&lt;/strong&gt; (included as the &lt;code&gt;X-PAYMENT&lt;/code&gt; header in base64 encoded json)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Version of the x402 payment protocol
  x402Version: number;

  // scheme is the scheme value of the accepted `paymentRequirements` the client is using to pay
  scheme: string;

  // network is the network id of the accepted `paymentRequirements` the client is using to pay
  network: string;

  // payload is scheme dependent
  payload: &amp;lt;scheme dependent&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Facilitator Types &amp;amp; Interface&lt;/h4&gt; 
&lt;p&gt;A &lt;code&gt;facilitator server&lt;/code&gt; is a 3rd party service that can be used by a &lt;code&gt;resource server&lt;/code&gt; to verify and settle payments, without the &lt;code&gt;resource server&lt;/code&gt; needing to have access to a blockchain node or wallet.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;POST /verify&lt;/strong&gt;. Verify a payment with a supported scheme and network:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Request body JSON: &lt;pre&gt;&lt;code class="language-json5"&gt;{
  x402Version: number;
  paymentHeader: string;
  paymentRequirements: paymentRequirements;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Response: &lt;pre&gt;&lt;code class="language-json5"&gt;{
  isValid: boolean;
  invalidReason: string | null;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;POST /settle&lt;/strong&gt;. Settle a payment with a supported scheme and network:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Request body JSON:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json5"&gt;{
  x402Version: number;
  paymentHeader: string;
  paymentRequirements: paymentRequirements;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Whether the payment was successful
  success: boolean;

  // Error message from the facilitator server
  error: string | null;

  // Transaction hash of the settled payment
  txHash: string | null;

  // Network id of the blockchain the payment was settled on
  networkId: string | null;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;GET /supported&lt;/strong&gt;. Get supported payment schemes and networks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Response: &lt;pre&gt;&lt;code class="language-json5"&gt;{
  kinds: [
    {
      "scheme": string,
      "network": string,
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Schemes&lt;/h3&gt; 
&lt;p&gt;A scheme is a logical way of moving money.&lt;/p&gt; 
&lt;p&gt;Blockchains allow for a large number of flexible ways to move money. To help facilitate an expanding number of payment use cases, the &lt;code&gt;x402&lt;/code&gt; protocol is extensible to different ways of settling payments via its &lt;code&gt;scheme&lt;/code&gt; field.&lt;/p&gt; 
&lt;p&gt;Each payment scheme may have different operational functionality depending on what actions are necessary to fulfill the payment. For example &lt;code&gt;exact&lt;/code&gt;, the first scheme shipping as part of the protocol, would have different behavior than &lt;code&gt;upto&lt;/code&gt;. &lt;code&gt;exact&lt;/code&gt; transfers a specific amount (ex: pay $1 to read an article), while a theoretical &lt;code&gt;upto&lt;/code&gt; would transfer up to an amount, based on the resources consumed during a request (ex: generating tokens from an LLM).&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;specs/schemes&lt;/code&gt; for more details on schemes, and see &lt;code&gt;specs/schemes/exact/scheme_exact_evm.md&lt;/code&gt; to see the first proposed scheme for exact payment on EVM chains.&lt;/p&gt; 
&lt;h3&gt;Schemes vs Networks&lt;/h3&gt; 
&lt;p&gt;Because a scheme is a logical way of moving money, the way a scheme is implemented can be different for different blockchains. (ex: the way you need to implement &lt;code&gt;exact&lt;/code&gt; on Ethereum is very different from the way you need to implement &lt;code&gt;exact&lt;/code&gt; on Solana).&lt;/p&gt; 
&lt;p&gt;Clients and facilitators must explicitly support different &lt;code&gt;(scheme, network)&lt;/code&gt; pairs in order to be able to create proper payloads and verify / settle payments.&lt;/p&gt; 
&lt;h2&gt;Running example&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Node.js v24 or higher&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;From &lt;code&gt;examples/typescript&lt;/code&gt; run &lt;code&gt;pnpm install&lt;/code&gt; and &lt;code&gt;pnpm build&lt;/code&gt; to ensure all dependent packages and examples are setup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select a server, i.e. express, and &lt;code&gt;cd&lt;/code&gt; into that example. Add your server's ethereum address to get paid to into the &lt;code&gt;.env&lt;/code&gt; file, and then run &lt;code&gt;pnpm dev&lt;/code&gt; in that directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select a client, i.e. axios, and &lt;code&gt;cd&lt;/code&gt; into that example. Add your private key for the account making payments into the &lt;code&gt;.env&lt;/code&gt; file, and then run &lt;code&gt;pnpm dev&lt;/code&gt; in that directory.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You should see activities in the client terminal, which will display a weather report.&lt;/p&gt; 
&lt;h2&gt;Running tests&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the typescript directory: &lt;code&gt;cd typescript&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies: &lt;code&gt;pnpm install&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run the unit tests: &lt;code&gt;pnpm test&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will run the unit tests for the x402 packages.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>toeverything/AFFiNE</title>
      <link>https://github.com/toeverything/AFFiNE</link>
      <description>&lt;p&gt;There can be more than Notion and Miro. AFFiNE(pronounced [É™â€˜fain]) is a next-gen knowledge base that brings planning, sorting and creating all together. Privacy first, open-source, customizable and ready to use.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1 style="border-bottom: none"&gt; &lt;b&gt;&lt;a href="https://affine.pro"&gt;AFFiNE.Pro&lt;/a&gt;&lt;/b&gt;&lt;br /&gt; Write, Draw and Plan All at Once &lt;br /&gt; &lt;/h1&gt; 
 &lt;a href="https://affine.pro/download"&gt; &lt;img alt="affine logo" src="https://cdn.affine.pro/Github_hero_image2.png" style="width: 100%" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; A privacy-focused, local-first, open-source, and ready-to-use alternative for Notion &amp;amp; Miro. &lt;br /&gt; One hyper-fused platform for wildly creative minds. &lt;/p&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.producthunt.com/posts/affine-3?utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-affine-3" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=440671&amp;amp;theme=light" alt="AFFiNE - One app for all - Where Notion meets Miro | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;div align="left" valign="middle"&gt; 
  &lt;a href="https://runblaze.dev"&gt; 
   &lt;picture&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://www.runblaze.dev/logo_dark.png" /&gt; 
    &lt;img align="right" src="https://www.runblaze.dev/logo_light.png" height="102px" /&gt; 
   &lt;/picture&gt; &lt;/a&gt; 
  &lt;br style="display: none;" /&gt; 
  &lt;p&gt;&lt;em&gt;Special thanks to &lt;a href="https://runblaze.dev"&gt;Blaze&lt;/a&gt; for their support of this project. They provide high-performance Apple Silicon macOS and Linux (AMD64 &amp;amp; ARM64) runners for GitHub Actions, greatly reducing our automated build times.&lt;/em&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://affine.pro"&gt;Home Page&lt;/a&gt; | 
  &lt;a href="https://affine.pro/redirect/discord"&gt;Discord&lt;/a&gt; | 
  &lt;a href="https://app.affine.pro"&gt;Live Demo&lt;/a&gt; | 
  &lt;a href="https://affine.pro/blog/"&gt;Blog&lt;/a&gt; | 
  &lt;a href="https://docs.affine.pro/"&gt;Documentation&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/toeverything/AFFiNE/releases/latest"&gt;&lt;img src="https://img.shields.io/github/downloads/toeverything/AFFiNE/total" alt="Releases" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/#contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/toeverything/AFFiNE" alt="All Contributors" /&gt;&lt;/a&gt; &lt;a href="https://www.typescriptlang.org/"&gt;&lt;img src="https://img.shields.io/github/package-json/dependency-version/toeverything/affine/dev/typescript" alt="TypeScript-version-icon" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;em&gt;Docs, canvas and tables are hyper-merged with AFFiNE - just like the word affine (É™ËˆfÊŒÉªn | a-fine).&lt;/em&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://github.com/toeverything/AFFiNE/assets/79301703/49a426bb-8d2b-4216-891a-fa5993642253" style="width: 100%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Getting started &amp;amp; staying tuned with us.&lt;/h2&gt; 
&lt;p&gt;Star us, and you will receive all release notifications from GitHub without any delay!&lt;/p&gt; 
&lt;img src="https://user-images.githubusercontent.com/79301703/230891830-0110681e-8c7e-483b-b6d9-9e42b291b9ef.gif" style="width: 100%" /&gt; 
&lt;h2&gt;What is AFFiNE&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://affine.pro"&gt;AFFiNE&lt;/a&gt; is an open-source, all-in-one workspace and an operating system for all the building blocks that assemble your knowledge base and much more -- wiki, knowledge management, presentation and digital assets. It's a better alternative to Notion and Miro.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;A true canvas for blocks in any form. Docs and whiteboard are now fully merged.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Many editor apps claim to be a canvas for productivity, but AFFiNE is one of the very few which allows you to put any building block on an edgeless canvas -- rich text, sticky notes, any embedded web pages, multi-view databases, linked pages, shapes and even slides. We have it all.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Multimodal AI partner ready to kick in any work&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Write up professional work report? Turn an outline into expressive and presentable slides? Summary an article into a well-structured mindmap? Sorting your job plan and backlog for tasks? Or... draw and code prototype apps and web pages directly all with one prompt? With you, &lt;a href="https://affine.pro/ai"&gt;AFFiNE AI&lt;/a&gt; pushes your creativity to the edge of your imagination, just like &lt;a href="https://affine.pro/blog/best-canvas-ai"&gt;Canvas AI&lt;/a&gt; to generate mind map for brainstorming.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Local-first &amp;amp; Real-time collaborative&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We love the idea of local-first that you always own your data on your disk, in spite of the cloud. Furthermore, AFFiNE supports real-time sync and collaborations on web and cross-platform clients.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Self-host &amp;amp; Shape your own AFFiNE&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You have the freedom to manage, self-host, fork and build your own AFFiNE. Plugin community and third-party blocks are coming soon. More tractions on &lt;a href="https://blocksuite.io"&gt;Blocksuite&lt;/a&gt;. Check there to learn how to &lt;a href="https://docs.affine.pro/self-host-affine"&gt;self-host AFFiNE&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;â€œWe shape our tools and thereafter our tools shape usâ€. A lot of pioneers have inspired us along the way, e.g.:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Quip &amp;amp; Notion with their great concept of â€œeverything is a blockâ€&lt;/li&gt; 
 &lt;li&gt;Trello with their Kanban&lt;/li&gt; 
 &lt;li&gt;Airtable &amp;amp; Miro with their no-code programmable datasheets&lt;/li&gt; 
 &lt;li&gt;Miro &amp;amp; Whimiscal with their edgeless visual whiteboard&lt;/li&gt; 
 &lt;li&gt;Remote &amp;amp; Capacities with their object-based tag system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;There is a large overlap of their atomic â€œbuilding blocksâ€ between these apps. They are not open source, nor do they have a plugin system like Vscode for contributors to customize. We want to have something that contains all the features we love and also goes one step even further.&lt;/p&gt; 
&lt;p&gt;Thanks for checking us out, we appreciate your interest and sincerely hope that AFFiNE resonates with you! ğŸµ Checking &lt;a href="https://affine.pro/"&gt;https://affine.pro/&lt;/a&gt; for more details ions.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Bug Reports&lt;/th&gt; 
   &lt;th&gt;Feature Requests&lt;/th&gt; 
   &lt;th&gt;Questions/Discussions&lt;/th&gt; 
   &lt;th&gt;AFFiNE Community&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/toeverything/AFFiNE/issues/new?assignees=&amp;amp;labels=bug%2Cproduct-review&amp;amp;template=BUG-REPORT.yml&amp;amp;title=TITLE"&gt;Create a bug report&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/toeverything/AFFiNE/issues/new?assignees=&amp;amp;labels=feat%2Cproduct-review&amp;amp;template=FEATURE-REQUEST.yml&amp;amp;title=TITLE"&gt;Submit a feature request&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/toeverything/AFFiNE/discussions"&gt;Check GitHub Discussion&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://community.affine.pro"&gt;Vist the AFFiNE Community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Something isn't working as expected&lt;/td&gt; 
   &lt;td&gt;An idea for a new feature, or improvements&lt;/td&gt; 
   &lt;td&gt;Discuss and ask questions&lt;/td&gt; 
   &lt;td&gt;A place to ask, learn and engage with others&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Calling all developers, testers, tech writers and more! Contributions of all types are more than welcome, you can read more in &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/types-of-contributions.md"&gt;docs/types-of-contributions.md&lt;/a&gt;. If you are interested in contributing code, read our &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/CONTRIBUTING.md"&gt;docs/CONTRIBUTING.md&lt;/a&gt; and feel free to check out our GitHub issues to get stuck in to show us what youâ€™re made of.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Before you start contributing, please make sure you have read and accepted our &lt;a href="https://github.com/toeverything/affine/edit/canary/.github/CLA.md"&gt;Contributor License Agreement&lt;/a&gt;. To indicate your agreement, simply edit this file and submit a pull request.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For &lt;strong&gt;bug reports&lt;/strong&gt;, &lt;strong&gt;feature requests&lt;/strong&gt; and other &lt;strong&gt;suggestions&lt;/strong&gt; you can also &lt;a href="https://github.com/toeverything/AFFiNE/issues/new/choose"&gt;create a new issue&lt;/a&gt; and choose the most appropriate template for your feedback.&lt;/p&gt; 
&lt;p&gt;For &lt;strong&gt;translation&lt;/strong&gt; and &lt;strong&gt;language support&lt;/strong&gt; you can visit our &lt;a href="https://community.affine.pro/c/i18n-general"&gt;i18n General Space&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Looking for &lt;strong&gt;other ways to contribute&lt;/strong&gt; and wondering where to start? Check out the &lt;a href="https://community.affine.pro/c/start-here/affine-ambassador"&gt;AFFiNE Ambassador program&lt;/a&gt;, we work closely with passionate community members and provide them with a wide range of support and resources.&lt;/p&gt; 
&lt;p&gt;If you have questions, you are welcome to contact us. One of the best places to get more info and learn more is in the &lt;a href="https://community.affine.pro"&gt;AFFiNE Community&lt;/a&gt; where you can engage with other like-minded individuals.&lt;/p&gt; 
&lt;h2&gt;Templates&lt;/h2&gt; 
&lt;p&gt;AFFiNE now provides pre-built &lt;a href="https://affine.pro/templates"&gt;templates&lt;/a&gt; from our team. Following are the Top 10 most popular templates among AFFiNE users,if you want to contribute, you can contribute your own template so other people can use it too.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/category-vision-board-template"&gt;vision board template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/category-one-pager-template-free"&gt;one pager template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/sample-lesson-plan-math-template"&gt;sample lesson plan math template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/grr-lesson-plan-template-free"&gt;grr lesson plan template free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/free-editable-lesson-plan-template-for-pre-k"&gt;free editable lesson plan template for pre k&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/high-note-collection-planners"&gt;high note collection planners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/category-digital-planner"&gt;digital planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/adhd-planner"&gt;ADHD Planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/reading-log"&gt;Reading Log&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/category-cornell-notes-template"&gt;Cornell Notes Template&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Blog&lt;/h2&gt; 
&lt;p&gt;Welcome to the AFFiNE blog section! Here, youâ€™ll find the latest insights, tips, and guides on how to maximize your experience with AFFiNE and AFFiNE AI, the leading Canvas AI tool for flexible note-taking and creative organization.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/8-free-printable-vision-board-templates-examples-2023"&gt;vision board template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/ai-homework-helper"&gt;ai homework helper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/vision-board-maker"&gt;vision board maker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/free-customized-travel-itinerary-planner-templates"&gt;itinerary template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/top-12-one-pager-examples-how-to-create-your-own"&gt;one pager template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/the-cornell-notes-template-and-system-learning-tips"&gt;cornell notes template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/top-10-free-editable-swot-analysis-template-examples"&gt;swot chart template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/apps-like-luna-task"&gt;apps like luna task&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/dynamic-AI-notes"&gt;note taking ai from rough notes to mind map&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/best-canvas-ai"&gt;canvas ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/top-12-one-pager-examples-how-to-create-your-own"&gt;one pager&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/how-to-write-sop-step-by-step-guide-5-best-free-tools-templates"&gt;SOP Template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/10-best-free-chore-chart-templates-kids-adults"&gt;Chore Chart&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/packages/frontend/component"&gt;@affine/component&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AFFiNE Component Resources&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/codecov/c/github/toeverything/affine?style=flat-square" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/packages/common/theme"&gt;@toeverything/theme&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AFFiNE theme&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.npmjs.com/package/@toeverything/theme"&gt;&lt;img src="https://img.shields.io/npm/dm/@toeverything/theme?style=flat-square&amp;amp;color=eee" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Upstreams&lt;/h2&gt; 
&lt;p&gt;We would also like to give thanks to open-source projects that make AFFiNE possible:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/toeverything/BlockSuite"&gt;Blocksuite&lt;/a&gt; - ğŸ’  BlockSuite is the open-source collaborative editor project behind AFFiNE.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/toeverything/OctoBase"&gt;OctoBase&lt;/a&gt; - ğŸ™ OctoBase is the open-source database behind AFFiNE, local-first, yet collaborative. A light-weight, scalable, data engine written in Rust.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yjs/yjs"&gt;yjs&lt;/a&gt; - Fundamental support of CRDTs for our implementation on state management and data sync.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/electron/electron"&gt;electron&lt;/a&gt; - Build cross-platform desktop apps with JavaScript, HTML, and CSS.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebook/react"&gt;React&lt;/a&gt; - The library for web and native user interfaces.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/napi-rs/napi-rs"&gt;napi-rs&lt;/a&gt; - A framework for building compiled Node.js add-ons in Rust via Node-API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pmndrs/jotai"&gt;Jotai&lt;/a&gt; - Primitive and flexible state management for React.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Jack-Works/async-call-rpc"&gt;async-call-rpc&lt;/a&gt; - A lightweight JSON RPC client &amp;amp; server.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vitejs/vite"&gt;Vite&lt;/a&gt; - Next generation frontend tooling.&lt;/li&gt; 
 &lt;li&gt;Other upstream &lt;a href="https://github.com/toeverything/AFFiNE/network/dependencies"&gt;dependencies&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Thanks a lot to the community for providing such powerful and simple libraries, so that we can focus more on the implementation of the product logic, and we hope that in the future our projects will also provide a more easy-to-use knowledge base for everyone.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;We would like to express our gratitude to all the individuals who have already contributed to AFFiNE! If you have any AFFiNE-related project, documentation, tool or template, please feel free to contribute it by submitting a pull request to our curated list on GitHub: &lt;a href="https://github.com/toeverything/awesome-affine"&gt;awesome-affine&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/toeverything/affine/graphs/contributors"&gt; &lt;img alt="contributors" src="https://opencollective.com/affine/contributors.svg?width=890&amp;amp;button=false" /&gt; &lt;/a&gt; 
&lt;h2&gt;Self-Host&lt;/h2&gt; 
&lt;p&gt;Begin with Docker to deploy your own feature-rich, unrestricted version of AFFiNE. Our team is diligently updating to the latest version. For more information on how to self-host AFFiNE, please refer to our &lt;a href="https://docs.affine.pro/self-host-affine"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://template.run.claw.cloud/?openapp=system-fastdeploy%3FtemplateName%3Daffine"&gt;&lt;img src="https://raw.githubusercontent.com/ClawCloud/Run-Template/refs/heads/main/Run-on-ClawCloud.svg?sanitize=true" alt="Run on ClawCloud" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Hiring&lt;/h2&gt; 
&lt;p&gt;Some amazing companies, including AFFiNE, are looking for developers! Are you interested in joining AFFiNE or its partners? Check out our &lt;a href="https://affine.pro/redirect/discord"&gt;Discord channel&lt;/a&gt; for some of the latest jobs available.&lt;/p&gt; 
&lt;h2&gt;Feature Request&lt;/h2&gt; 
&lt;p&gt;For feature requests, please see &lt;a href="https://community.affine.pro/c/feature-requests/"&gt;community.affine.pro&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;h3&gt;Codespaces&lt;/h3&gt; 
&lt;p&gt;From the GitHub repo main page, click the green "Code" button and select "Create codespace on master". This will open a new Codespace with the (supposedly auto-forked AFFiNE repo cloned, built, and ready to go.&lt;/p&gt; 
&lt;h3&gt;Local&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/BUILDING.md"&gt;BUILDING.md&lt;/a&gt; for instructions on how to build AFFiNE from source code.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from everyone. See &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/contributing/tutorial.md"&gt;docs/contributing/tutorial.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.chromatic.com/"&gt;&lt;img src="https://user-images.githubusercontent.com/321738/84662277-e3db4f80-af1b-11ea-88f5-91d67a5e59f6.png" width="153" height="30" alt="Chromatic" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thanks to &lt;a href="https://www.chromatic.com/"&gt;Chromatic&lt;/a&gt; for providing the visual testing platform that helps us review UI changes and catch visual regressions.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;h3&gt;Editions&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;AFFiNE Community Edition (CE) is the current available version, it's free for self-host under the MIT license.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;AFFiNE Enterprise Edition (EE) is yet to be published, it will have more advanced features and enterprise-oriented offerings, including but not exclusive to rebranding and SSO, advanced admin and audit, etc., you may refer to &lt;a href="https://affine.pro/pricing"&gt;https://affine.pro/pricing&lt;/a&gt; for more information&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>github/copilot-cli</title>
      <link>https://github.com/github/copilot-cli</link>
      <description>&lt;p&gt;GitHub Copilot CLI brings the power of Copilot coding agent directly to your terminal.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GitHub Copilot CLI (Public Preview)&lt;/h1&gt; 
&lt;p&gt;The power of GitHub Copilot, now in your terminal.&lt;/p&gt; 
&lt;p&gt;GitHub Copilot CLI brings AI-powered coding assistance directly to your command line, enabling you to build, debug, and understand code through natural language conversations. Powered by the same agentic harness as GitHub's Copilot coding agent, it provides intelligent assistance while staying deeply integrated with your GitHub workflow.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://docs.github.com/copilot/concepts/agents/about-copilot-cli"&gt;our official documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/51ac25d2-c074-467a-9c88-38a8d76690e3" alt="Image of the splash screen for the Copilot CLI" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Introduction and Overview&lt;/h2&gt; 
&lt;p&gt;We're bringing the power of GitHub Copilot coding agent directly to your terminal. With GitHub Copilot CLI, you can work locally and synchronously with an AI agent that understands your code and GitHub context.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Terminal-native development:&lt;/strong&gt; Work with Copilot coding agent directly in your command line â€” no context switching required.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub integration out of the box:&lt;/strong&gt; Access your repositories, issues, and pull requests using natural language, all authenticated with your existing GitHub account.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agentic capabilities:&lt;/strong&gt; Build, edit, debug, and refactor code with an AI collaborator that can plan and execute complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP-powered extensibility:&lt;/strong&gt; Take advantage of the fact that the coding agent ships with GitHub's MCP server by default and supports custom MCP servers to extend capabilities.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full control:&lt;/strong&gt; Preview every action before execution â€” nothing happens without your explicit approval.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We're still early in our journey, but with your feedback, we're rapidly iterating to make the GitHub Copilot CLI the best possible companion in your terminal.&lt;/p&gt; 
&lt;h2&gt;ğŸ“¦ Getting Started&lt;/h2&gt; 
&lt;h3&gt;Supported Platforms&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Node.js&lt;/strong&gt; v22 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;npm&lt;/strong&gt; v10 or higher&lt;/li&gt; 
 &lt;li&gt;(On Windows) &lt;strong&gt;PowerShell&lt;/strong&gt; v6 or higher&lt;/li&gt; 
 &lt;li&gt;An &lt;strong&gt;active Copilot subscription&lt;/strong&gt;. See &lt;a href="https://github.com/features/copilot/plans?ref_cta=Copilot+plans+signup&amp;amp;ref_loc=install-copilot-cli&amp;amp;ref_page=docs"&gt;Copilot plans&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you have access to GitHub Copilot via your organization of enterprise, you cannot use GitHub Copilot CLI if your organization owner or enterprise administrator has disabled it in the organization or enterprise settings. See &lt;a href="http://docs.github.com/copilot/managing-copilot/managing-github-copilot-in-your-organization/managing-github-copilot-features-in-your-organization/managing-policies-for-copilot-in-your-organization"&gt;Managing policies and features for GitHub Copilot in your organization&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Install globally with npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @github/copilot
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launching the CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;copilot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On first launch, you'll be greeted with our adorable animated banner! If you'd like to see this banner again, launch &lt;code&gt;copilot&lt;/code&gt; with the &lt;code&gt;--banner&lt;/code&gt; flag.&lt;/p&gt; 
&lt;p&gt;If you're not currently logged in to GitHub, you'll be prompted to use the &lt;code&gt;/login&lt;/code&gt; slash command. Enter this command and follow the on-screen instructions to authenticate.&lt;/p&gt; 
&lt;h4&gt;Authenticate with a Personal Access Token (PAT)&lt;/h4&gt; 
&lt;p&gt;You can also authenticate using a fine-grained PAT with the "Copilot Requests" permission enabled.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Visit &lt;a href="https://github.com/settings/personal-access-tokens/new"&gt;https://github.com/settings/personal-access-tokens/new&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Under "Permissions," click "add permissions" and select "Copilot Requests"&lt;/li&gt; 
 &lt;li&gt;Generate your token&lt;/li&gt; 
 &lt;li&gt;Add the token to your environment via the environment variable &lt;code&gt;GH_TOKEN&lt;/code&gt; or &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; (in order of precedence)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Using the CLI&lt;/h3&gt; 
&lt;p&gt;Launch &lt;code&gt;copilot&lt;/code&gt; in a folder that contains code you want to work with.&lt;/p&gt; 
&lt;p&gt;By default, &lt;code&gt;copilot&lt;/code&gt; utilizes Claude Sonnet 4.5. Run the &lt;code&gt;/model&lt;/code&gt; slash command to choose from other available models, including Claude Sonnet 4 and GPT-5&lt;/p&gt; 
&lt;p&gt;Each time you submit a prompt to GitHub Copilot CLI, your monthly quota of premium requests is reduced by one. For information about premium requests, see &lt;a href="https://docs.github.com/copilot/managing-copilot/monitoring-usage-and-entitlements/about-premium-requests"&gt;About premium requests&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more information about how to use the GitHub Copilot CLI, see &lt;a href="https://docs.github.com/copilot/concepts/agents/about-copilot-cli"&gt;our official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“¢ Feedback and Participation&lt;/h2&gt; 
&lt;p&gt;We're excited to have you join us early in the Copilot CLI journey.&lt;/p&gt; 
&lt;p&gt;This is an early-stage preview, and we're building quickly. Expect frequent updates--please keep your client up to date for the latest features and fixes!&lt;/p&gt; 
&lt;p&gt;Your insights are invaluable! Open issue in this repo, join Discussions, and run &lt;code&gt;/feedback&lt;/code&gt; from the CLI to submit a confidential feedback survey!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Stremio/stremio-web</title>
      <link>https://github.com/Stremio/stremio-web</link>
      <description>&lt;p&gt;Stremio - Freedom to Stream&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stremio - Freedom to Stream&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Stremio/stremio-web/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/Stremio/stremio-web/actions/workflows/build.yml/badge.svg?sanitize=true" alt="Build" /&gt;&lt;/a&gt; &lt;a href="https://stremio.github.io/stremio-web/development"&gt;&lt;img src="https://img.shields.io/website?label=Page&amp;amp;logo=github&amp;amp;up_message=online&amp;amp;down_message=offline&amp;amp;url=https%3A%2F%2Fstremio.github.io%2Fstremio-web%2F" alt="Github Page" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Stremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.&lt;/p&gt; 
&lt;h2&gt;Build&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js 12 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pnpm.io/installation"&gt;pnpm&lt;/a&gt; 10 or higher&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Start development server&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Production build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm run build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run with Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t stremio-web .
docker run -p 8080:8080 stremio-web
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;h3&gt;Board&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Stremio/stremio-web/development/screenshots/board.png" alt="Board" /&gt;&lt;/p&gt; 
&lt;h3&gt;Discover&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Stremio/stremio-web/development/screenshots/discover.png" alt="Discover" /&gt;&lt;/p&gt; 
&lt;h3&gt;Meta Details&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Stremio/stremio-web/development/screenshots/metadetails.png" alt="Meta Details" /&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Stremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the &lt;a href="https://raw.githubusercontent.com/Stremio/stremio-web/development/LICENSE.md"&gt;LICENSE&lt;/a&gt; file in the project for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-cookbooks</title>
      <link>https://github.com/anthropics/claude-cookbooks</link>
      <description>&lt;p&gt;A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Cookbooks&lt;/h1&gt; 
&lt;p&gt;The Claude Cookbooks provide code and guides designed to help developers build with Claude, offering copy-able code snippets that you can easily integrate into your own projects.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;To make the most of the examples in this cookbook, you'll need an Claude API key (sign up for free &lt;a href="https://www.anthropic.com"&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;While the code examples are primarily written in Python, the concepts can be adapted to any programming language that supports interaction with the Claude API.&lt;/p&gt; 
&lt;p&gt;If you're new to working with the Claude API, we recommend starting with our &lt;a href="https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals"&gt;Claude API Fundamentals course&lt;/a&gt; to get a solid foundation.&lt;/p&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;Looking for more resources to enhance your experience with Claude and AI assistants? Check out these helpful links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.claude.com/claude/docs/guide-to-anthropics-prompt-engineering-resources"&gt;Anthropic developer documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.anthropic.com"&gt;Anthropic support docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.anthropic.com/discord"&gt;Anthropic Discord community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Claude Cookbooks thrives on the contributions of the developer community. We value your input, whether it's submitting an idea, fixing a typo, adding a new guide, or improving an existing one. By contributing, you help make this resource even more valuable for everyone.&lt;/p&gt; 
&lt;p&gt;To avoid duplication of efforts, please review the existing issues and pull requests before contributing.&lt;/p&gt; 
&lt;p&gt;If you have ideas for new examples or guides, share them on the &lt;a href="https://github.com/anthropics/anthropic-cookbook/issues"&gt;issues page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of recipes&lt;/h2&gt; 
&lt;h3&gt;Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/capabilities/classification"&gt;Classification&lt;/a&gt;: Explore techniques for text and data classification using Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/capabilities/retrieval_augmented_generation"&gt;Retrieval Augmented Generation&lt;/a&gt;: Learn how to enhance Claude's responses with external knowledge.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/capabilities/summarization"&gt;Summarization&lt;/a&gt;: Discover techniques for effective text summarization with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tool Use and Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/tool_use"&gt;Tool use&lt;/a&gt;: Learn how to integrate Claude with external tools and functions to extend its capabilities. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/customer_service_agent.ipynb"&gt;Customer service agent&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/calculator_tool.ipynb"&gt;Calculator integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_make_sql_queries.ipynb"&gt;SQL queries&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Third-Party Integrations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/third_party"&gt;Retrieval augmented generation&lt;/a&gt;: Supplement Claude's knowledge with external data sources. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Pinecone/rag_using_pinecone.ipynb"&gt;Vector databases (Pinecone)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Wikipedia/wikipedia-search-cookbook.ipynb/"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/read_web_pages_with_haiku.ipynb"&gt;Web pages&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/VoyageAI/how_to_create_embeddings.md"&gt;Embeddings with Voyage AI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal"&gt;Vision with Claude&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/getting_started_with_vision.ipynb"&gt;Getting started with images&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/best_practices_for_vision.ipynb"&gt;Best practices for vision&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/reading_charts_graphs_powerpoints.ipynb"&gt;Interpreting charts and graphs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/how_to_transcribe_text.ipynb"&gt;Extracting content from forms&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/illustrated_responses.ipynb"&gt;Generate images with Claude&lt;/a&gt;: Use Claude with Stable Diffusion for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Techniques&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/using_sub_agents.ipynb"&gt;Sub-agents&lt;/a&gt;: Learn how to use Haiku as a sub-agent in combination with Opus.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/pdf_upload_summarization.ipynb"&gt;Upload PDFs to Claude&lt;/a&gt;: Parse and pass PDFs as text to Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_evals.ipynb"&gt;Automated evaluations&lt;/a&gt;: Use Claude to automate the prompt evaluation process.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_enable_json_mode.ipynb"&gt;Enable JSON mode&lt;/a&gt;: Ensure consistent JSON output from Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_moderation_filter.ipynb"&gt;Create a moderation filter&lt;/a&gt;: Use Claude to create a content moderation filter for your application.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/prompt_caching.ipynb"&gt;Prompt caching&lt;/a&gt;: Learn techniques for efficient prompt caching with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/anthropic-on-aws"&gt;Anthropic on AWS&lt;/a&gt;: Explore examples and solutions for using Claude on AWS infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/"&gt;AWS Samples&lt;/a&gt;: A collection of code samples from AWS which can be adapted for use with Claude. Note that some samples may require modification to work optimally with Claude.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>TibixDev/winboat</title>
      <link>https://github.com/TibixDev/winboat</link>
      <description>&lt;p&gt;Run Windows apps on ğŸ§ Linux with âœ¨ seamless integration&lt;/p&gt;&lt;hr&gt;&lt;div align="left"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/icons/winboat_logo.svg?sanitize=true" alt="WinBoat Logo" width="150" /&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;h1 style="color: #7C86FF; margin: 0; font-size: 32px;"&gt;WinBoat&lt;/h1&gt; &lt;p style="color: oklch(90% 0 0); font-size: 14px; margin: 5px 0;"&gt;Windows for Penguins.&lt;br /&gt; Run Windows apps on ğŸ§ Linux with âœ¨ seamless integration&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_dash.png" alt="WinBoat Dashboard" width="45%" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_apps.png" alt="WinBoat Apps" width="45%" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_native.png" alt="Native Windows" width="45%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;âš ï¸ Work in Progress âš ï¸&lt;/h2&gt; 
&lt;p&gt;WinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;h4&gt;ğŸ§Š Feature Freeze&lt;/h4&gt; 
 &lt;p&gt;We're preparing for a 0.9.0 release, which already includes tons of changes. From this point forward, we'll be focusing on stabilizing the feature/fix set we have already merged or we're actively working on. Handling most pull requests or feature requests will be postponed up until we have a stable release and we can confidently start merging and reviewing again.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¨ Elegant Interface&lt;/strong&gt;: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“¦ Automated Installs&lt;/strong&gt;: Simple installation process through our interface - pick your preferences &amp;amp; specs and let us handle the rest&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸš€ Run Any App&lt;/strong&gt;: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ–¥ï¸ Full Windows Desktop&lt;/strong&gt;: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ Filesystem Integration&lt;/strong&gt;: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;âœ¨ And many more&lt;/strong&gt;: Smartcard passthrough, resource monitoring, and more features being added regularly&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How Does It Work?&lt;/h2&gt; 
&lt;p&gt;WinBoat is an Electron app which allows you to run Windows apps on Linux using a containerized approach. Windows runs as a VM inside a Docker container, we communicate with it using the &lt;a href="https://github.com/TibixDev/winboat/tree/main/guest_server"&gt;WinBoat Guest Server&lt;/a&gt; to retrieve data we need from Windows. For compositing applications as native OS-level windows, we use FreeRDP together with Windows's RemoteApp protocol.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before running WinBoat, ensure your system meets the following requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: At least 4 GB of RAM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: At least 2 CPU threads&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: At least 32 GB free space on the drive your selected install folder corresponds to&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Virtualization&lt;/strong&gt;: KVM enabled in BIOS/UEFI 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://duckduckgo.com/?t=h_&amp;amp;q=how+to+enable+virtualization+in+%3Cmotherboard+brand%3E+bios&amp;amp;ia=web"&gt;How to enable virtualization&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Required for containerization 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/engine/install/"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;âš ï¸ NOTE:&lt;/strong&gt; Docker Desktop is &lt;strong&gt;not&lt;/strong&gt; supported, you will run into issues if you use it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Compose v2&lt;/strong&gt;: Required for compatibility with docker-compose.yml files 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/compose/install/#plugin-linux-only"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker User Group&lt;/strong&gt;: Add your user to the &lt;code&gt;docker&lt;/code&gt; group 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user"&gt;Setup Instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FreeRDP&lt;/strong&gt;: Required for remote desktop connection (Please make sure you have &lt;strong&gt;Version 3.x.x&lt;/strong&gt; with sound support included) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/FreeRDP/FreeRDP/wiki/PreBuilds"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;[OPTIONAL] &lt;strong&gt;Kernel Modules&lt;/strong&gt;: The &lt;code&gt;iptables&lt;/code&gt; / &lt;code&gt;nftables&lt;/code&gt; and &lt;code&gt;iptable_nat&lt;/code&gt; kernel modules can be loaded for network autodiscovery and better shared filesystem performance, but this is not obligatory in newer versions of WinBoat 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://rentry.org/rmfq2e5e"&gt;Module loading instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Downloading&lt;/h2&gt; 
&lt;p&gt;You can download the latest Linux builds under the &lt;a href="https://github.com/TibixDev/winboat/releases"&gt;Releases&lt;/a&gt; tab. We currently offer four variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AppImage:&lt;/strong&gt; A popular &amp;amp; portable app format which should run fine on most distributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unpacked:&lt;/strong&gt; The raw unpacked files, simply run the executable (&lt;code&gt;linux-unpacked/winboat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;.deb:&lt;/strong&gt; The intended format for Debian based distributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;.rpm:&lt;/strong&gt; The intended format for Fedora based distributions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Known Issues About Container Runtimes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Podman is &lt;strong&gt;unsupported&lt;/strong&gt; for now&lt;/li&gt; 
 &lt;li&gt;Docker Desktop is &lt;strong&gt;unsupported&lt;/strong&gt; for now&lt;/li&gt; 
 &lt;li&gt;Distros that emulate Docker through a Podman socket are &lt;strong&gt;unsupported&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Any rootless containerization solution is currently &lt;strong&gt;unsupported&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building WinBoat&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For building you need to have NodeJS and Go installed on your system&lt;/li&gt; 
 &lt;li&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Build the app and the guest server using &lt;code&gt;npm run build:linux-gs&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;You can now find the built app under &lt;code&gt;dist&lt;/code&gt; with an AppImage and an Unpacked variant&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running WinBoat in development mode&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make sure you meet the &lt;a href="https://raw.githubusercontent.com/TibixDev/winboat/main/#prerequisites"&gt;prerequisites&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additionally, for development you need to have NodeJS and Go installed on your system&lt;/li&gt; 
 &lt;li&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Build the guest server (&lt;code&gt;npm run build-guest-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Run the app (&lt;code&gt;npm run dev&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! ğŸš€&lt;/p&gt; 
&lt;p&gt;Feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report bugs and issues&lt;/li&gt; 
 &lt;li&gt;Submit feature requests&lt;/li&gt; 
 &lt;li&gt;Contribute code improvements&lt;/li&gt; 
 &lt;li&gt;Help with documentation&lt;/li&gt; 
 &lt;li&gt;Share feedback and suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;WinBoat is licensed under the &lt;a href="https://github.com/TibixDev/winboat/raw/main/LICENSE"&gt;MIT&lt;/a&gt; license&lt;/p&gt; 
&lt;h2&gt;Inspiration / Alternatives&lt;/h2&gt; 
&lt;p&gt;These past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.&lt;br /&gt; They're awesome and you should check them out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/winapps-org/winapps"&gt;WinApps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/casualsnek/cassowary"&gt;Cassowary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dockur/windows"&gt;dockur/windows&lt;/a&gt; (ğŸŒŸ Also used in WinBoat)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Socials &amp;amp; Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.winboat.app/"&gt;&lt;img src="https://img.shields.io/badge/Website-winboat.app-blue?style=flat&amp;amp;logo=googlechrome&amp;amp;logoColor=white" alt="Website" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/winboat_app"&gt;&lt;img src="https://img.shields.io/badge/Twitter-@winboat__app-1DA1F2?style=flat&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fosstodon.org/@winboat"&gt;&lt;img src="https://img.shields.io/badge/Mastodon-@winboat-6364FF?style=flat&amp;amp;logo=mastodon&amp;amp;logoColor=white" alt="Mastodon" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://bsky.app/profile/winboat.app"&gt;&lt;img src="https://img.shields.io/badge/Bluesky-winboat.app-00A8E8?style=flat&amp;amp;logo=bluesky&amp;amp;logoColor=white" alt="Bluesky" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://discord.gg/MEwmpWm4tN"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join_Community-5865F2?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="mailto:staff@winboat.app"&gt;&lt;img src="https://img.shields.io/badge/Email-staff@winboat.app-D14836?style=flat&amp;amp;logo=gmail&amp;amp;logoColor=white" alt="Email" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deepwiki.com/TibixDev/winboat"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#tibixdev/winboat&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=tibixdev/winboat&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=tibixdev/winboat&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=tibixdev/winboat&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>evershopcommerce/evershop</title>
      <link>https://github.com/evershopcommerce/evershop</link>
      <description>&lt;p&gt;ğŸ›ï¸ Typescript E-commerce Platform&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="60" height="68" alt="EverShop Logo" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/logo-green.png" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;h1 align="center"&gt;EverShop&lt;/h1&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://evershop.io/docs/development/getting-started/introduction"&gt;Documentation&lt;/a&gt; | &lt;a href="https://demo.evershop.io/"&gt;Demo&lt;/a&gt; &lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/evershopcommerce/evershop/actions/workflows/build_test.yml/badge.svg?sanitize=true" alt="Github Action" /&gt; &lt;a href="https://twitter.com/evershopjs"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/evershopjs?style=social" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/GSzt7dt7RM"&gt; &lt;img src="https://img.shields.io/discord/757179260417867879?label=discord" alt="Discord" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/GPL-3.0"&gt; &lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img alt="EverShop" width="950" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/banner.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;EverShop is a modern, TypeScript-first eCommerce platform built with GraphQL and React. Designed for developers, it offers essential commerce features in a modular, fully customizable architectureâ€”perfect for building tailored shopping experiences with confidence and speed.&lt;/p&gt; 
&lt;h2&gt;Installation Using Docker&lt;/h2&gt; 
&lt;p&gt;You can get started with EverShop in minutes by using the Docker image. The Docker image is a great way to get started with EverShop without having to worry about installing dependencies or configuring your environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://raw.githubusercontent.com/evershopcommerce/evershop/main/docker-compose.yml &amp;gt; docker-compose.yml
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the full installation guide, please refer to our &lt;a href="https://evershop.io/docs/development/getting-started/installation-guide"&gt;Installation guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://evershop.io/docs/development/getting-started/installation-guide"&gt;Installation guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://evershop.io/docs/development/module/create-your-first-extension"&gt;Extension development&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://evershop.io/docs/development/theme/theme-overview"&gt;Theme development&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;Explore our demo store.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;a href="https://demo.evershop.io/admin" target="_blank"&gt; &lt;img alt="EverShop Admin Demo" height="35" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/evershop-demo-back.png" /&gt; &lt;/a&gt; &lt;a href="https://demo.evershop.io/" target="_blank"&gt; &lt;img alt="EverShop Store Demo" height="35" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/evershop-demo-front.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;b&gt;Demo user:&lt;/b&gt; 
&lt;p&gt;Email: &lt;a href="mailto:demo@evershop.io"&gt;demo@evershop.io&lt;/a&gt;&lt;br /&gt; Password: 123456&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;If you like my work, feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;â­ this repository. It helps.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fgithub.com%2Fevershopcommerce%2Fevershop&amp;amp;text=Awesome%20React%20Ecommerce%20Project&amp;amp;hashtags=react,ecommerce,expressjs,graphql"&gt;&lt;img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt; about EverShop. Thank you!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;EverShop is an open-source project. We are committed to a fully transparent development process and appreciate highly any contributions. Whether you are helping us fix bugs, proposing new features, improving our documentation or spreading the word - we would love to have you as part of the EverShop community.&lt;/p&gt; 
&lt;h3&gt;Ask a question about EverShop&lt;/h3&gt; 
&lt;p&gt;You can ask questions, and participate in discussions about EverShop-related topics in the EverShop Discord channel.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/GSzt7dt7RM"&gt;&lt;img src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/discord_banner_github.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Create a bug report&lt;/h3&gt; 
&lt;p&gt;If you see an error message or run into an issue, please &lt;a href="https://github.com/evershopcommerce/evershop/issues/new"&gt;create bug report&lt;/a&gt;. This effort is valued and it will help all EverShop users.&lt;/p&gt; 
&lt;h3&gt;Submit a feature request&lt;/h3&gt; 
&lt;p&gt;If you have an idea, or you're missing a capability that would make development easier and more robust, please &lt;a href="https://github.com/evershopcommerce/evershop/issues/new"&gt;Submit feature request&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If a similar feature request already exists, don't forget to leave a "+1". If you add some more information such as your thoughts and vision about the feature, your comments will be embraced warmly :)&lt;/p&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/evershopcommerce/evershop/raw/main/LICENSE"&gt;GPL-3.0 License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/prompt-eng-interactive-tutorial</title>
      <link>https://github.com/anthropics/prompt-eng-interactive-tutorial</link>
      <description>&lt;p&gt;Anthropic's Interactive Prompt Engineering Tutorial&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to Anthropic's Prompt Engineering Interactive Tutorial&lt;/h1&gt; 
&lt;h2&gt;Course introduction and goals&lt;/h2&gt; 
&lt;p&gt;This course is intended to provide you with a comprehensive step-by-step understanding of how to engineer optimal prompts within Claude.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;After completing this course, you will be able to&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Master the basic structure of a good prompt&lt;/li&gt; 
 &lt;li&gt;Recognize common failure modes and learn the '80/20' techniques to address them&lt;/li&gt; 
 &lt;li&gt;Understand Claude's strengths and weaknesses&lt;/li&gt; 
 &lt;li&gt;Build strong prompts from scratch for common use cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Course structure and content&lt;/h2&gt; 
&lt;p&gt;This course is structured to allow you many chances to practice writing and troubleshooting prompts yourself. The course is broken up into &lt;strong&gt;9 chapters with accompanying exercises&lt;/strong&gt;, as well as an appendix of even more advanced methods. It is intended for you to &lt;strong&gt;work through the course in chapter order&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Each lesson has an "Example Playground" area&lt;/strong&gt; at the bottom where you are free to experiment with the examples in the lesson and see for yourself how changing prompts can change Claude's responses. There is also an &lt;a href="https://docs.google.com/spreadsheets/d/1jIxjzUWG-6xBVIa2ay6yDpLyeuOh_hR_ZB75a47KX_E/edit?usp=sharing"&gt;answer key&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: This tutorial uses our smallest, fastest, and cheapest model, Claude 3 Haiku. Anthropic has &lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;two other models&lt;/a&gt;, Claude 3 Sonnet and Claude 3 Opus, which are more intelligent than Haiku, with Opus being the most intelligent.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This tutorial also exists on &lt;a href="https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit?usp=sharing"&gt;Google Sheets using Anthropic's Claude for Sheets extension&lt;/a&gt;. We recommend using that version as it is more user friendly.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;When you are ready to begin, go to &lt;code&gt;01_Basic Prompt Structure&lt;/code&gt; to proceed.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;p&gt;Each chapter consists of a lesson and a set of exercises.&lt;/p&gt; 
&lt;h3&gt;Beginner&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 1:&lt;/strong&gt; Basic Prompt Structure&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 2:&lt;/strong&gt; Being Clear and Direct&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 3:&lt;/strong&gt; Assigning Roles&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Intermediate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 4:&lt;/strong&gt; Separating Data from Instructions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 5:&lt;/strong&gt; Formatting Output &amp;amp; Speaking for Claude&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 6:&lt;/strong&gt; Precognition (Thinking Step by Step)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 7:&lt;/strong&gt; Using Examples&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 8:&lt;/strong&gt; Avoiding Hallucinations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 9:&lt;/strong&gt; Building Complex Prompts (Industry Use Cases)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Complex Prompts from Scratch - Chatbot&lt;/li&gt; 
   &lt;li&gt;Complex Prompts for Legal Services&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Complex Prompts for Financial Services&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Complex Prompts for Coding&lt;/li&gt; 
   &lt;li&gt;Congratulations &amp;amp; Next Steps&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Appendix:&lt;/strong&gt; Beyond Standard Prompting&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Chaining Prompts&lt;/li&gt; 
   &lt;li&gt;Tool Use&lt;/li&gt; 
   &lt;li&gt;Search &amp;amp; Retrieval&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>davila7/claude-code-templates</title>
      <link>https://github.com/davila7/claude-code-templates</link>
      <description>&lt;p&gt;CLI tool for configuring and monitoring Claude Code&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/v/claude-code-templates.svg?sanitize=true" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/dt/claude-code-templates.svg?sanitize=true" alt="npm downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://github.com/davila7/claude-code-templates"&gt;&lt;img src="https://img.shields.io/github/stars/davila7/claude-code-templates.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/15113" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15113" alt="davila7%2Fclaude-code-templates | Trendshift" style="width: 200px; height: 40px;" width="125" height="40" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Claude Code Templates (&lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;)&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Ready-to-use configurations for Anthropic's Claude Code.&lt;/strong&gt; A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.&lt;/p&gt; 
&lt;h2&gt;Browse &amp;amp; Install Components and Templates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse All Templates&lt;/a&gt;&lt;/strong&gt; - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.&lt;/p&gt; 
&lt;img width="1049" height="855" alt="Screenshot 2025-08-19 at 08 09 24" src="https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737" /&gt; 
&lt;h2&gt;ğŸš€ Quick Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install a complete development stack
npx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration --yes

# Browse and install interactively
npx claude-code-templates@latest

# Install specific components
npx claude-code-templates@latest --agent development-tools/code-reviewer --yes
npx claude-code-templates@latest --command performance/optimize-bundle --yes
npx claude-code-templates@latest --setting performance/mcp-timeouts --yes
npx claude-code-templates@latest --hook git/pre-commit-validation --yes
npx claude-code-templates@latest --mcp database/postgresql-integration --yes
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What You Get&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ¤– Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AI specialists for specific domains&lt;/td&gt; 
   &lt;td&gt;Security auditor, React performance optimizer, database architect&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âš¡ Commands&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom slash commands&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/generate-tests&lt;/code&gt;, &lt;code&gt;/optimize-bundle&lt;/code&gt;, &lt;code&gt;/check-security&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ”Œ MCPs&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;External service integrations&lt;/td&gt; 
   &lt;td&gt;GitHub, PostgreSQL, Stripe, AWS, OpenAI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âš™ï¸ Settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Claude Code configurations&lt;/td&gt; 
   &lt;td&gt;Timeouts, memory settings, output styles&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸª Hooks&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automation triggers&lt;/td&gt; 
   &lt;td&gt;Pre-commit validation, post-completion actions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ¨ Skills&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reusable capabilities with progressive disclosure&lt;/td&gt; 
   &lt;td&gt;PDF processing, Excel automation, custom workflows&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ› ï¸ Additional Tools&lt;/h2&gt; 
&lt;p&gt;Beyond the template catalog, Claude Code Templates includes powerful development tools:&lt;/p&gt; 
&lt;h3&gt;ğŸ“Š Claude Code Analytics&lt;/h3&gt; 
&lt;p&gt;Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --analytics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ’¬ Conversation Monitor&lt;/h3&gt; 
&lt;p&gt;Mobile-optimized interface to view Claude responses in real-time with secure remote access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Local access
npx claude-code-templates@latest --chats

# Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ” Health Check&lt;/h3&gt; 
&lt;p&gt;Comprehensive diagnostics to ensure your Claude Code installation is optimized.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --health-check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”Œ Plugin Dashboard&lt;/h3&gt; 
&lt;p&gt;View marketplaces, installed plugins, and manage permissions from a unified interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ“– Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.aitmpl.com/"&gt;ğŸ“š docs.aitmpl.com&lt;/a&gt;&lt;/strong&gt; - Complete guides, examples, and API reference for all components and tools.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! &lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse existing templates&lt;/a&gt;&lt;/strong&gt; to see what's available, then check our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to add your own agents, commands, MCPs, settings, or hooks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please read our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; before contributing.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Attribution&lt;/h2&gt; 
&lt;p&gt;This collection includes components from multiple sources:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agents Collection:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;wshobson/agents Collection&lt;/strong&gt; by &lt;a href="https://github.com/wshobson/agents"&gt;wshobson&lt;/a&gt; - Licensed under MIT License (48 agents)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands Collection:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;awesome-claude-code Commands&lt;/strong&gt; by &lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;hesreallyhim&lt;/a&gt; - Licensed under CC0 1.0 Universal (21 commands)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;ğŸ”— Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸŒ Browse Templates&lt;/strong&gt;: &lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“š Documentation&lt;/strong&gt;: &lt;a href="https://docs.aitmpl.com"&gt;docs.aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ’¬ Community&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ› Issues&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Stargazers over time&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/davila7/claude-code-templates"&gt;&lt;img src="https://starchart.cc/davila7/claude-code-templates.svg?variant=adaptive" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;â­ Found this useful? Give us a star to support the project!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.buymeacoffee.com/button-api/?text=Buy%20me%20a%20coffee&amp;amp;slug=daniavila&amp;amp;button_colour=FFDD00&amp;amp;font_colour=000000&amp;amp;font_family=Cookie&amp;amp;outline_colour=000000&amp;amp;coffee_colour=ffffff" alt="Buy Me A Coffee" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ“š ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;Docker&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;GPU options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-pre-built-docker-container"&gt;Docker Run&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Docker Build&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide"&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations"&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“š Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;ğŸ™ï¸ High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; (and more).&lt;/li&gt; 
 &lt;li&gt;ğŸ—£ï¸ Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;ğŸŒ Supports +1110 languages (English by default). &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ–¥ï¸ Designed to run on 4GB RAM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1100 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4gb RAM minimum, 8GB recommended&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU) *available very soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd  # Run launch script or double click on it
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual Python Install&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# (for experts only!)
REQUIRED_PROGRAMS=("calibre" "ffmpeg" "nodejs" "mecab" "espeak-ng" "rust" "sox")
REQUIRED_PYTHON_VERSION="3.12"
pip install -r requirements.txt  # Install Python Requirements
python app.py  # Run Ebook2Audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;python app.py --share&lt;/code&gt; (all OS) &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; \
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt;
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file'
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file'
    
Tip: to add of silence (1.4 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] equals to 1.4 seconds&lt;/p&gt; 
&lt;h4&gt;Docker GPU Options&lt;/h4&gt; 
&lt;p&gt;Available pre-build tags: &lt;code&gt;latest&lt;/code&gt; (CUDA 11.8)&lt;/p&gt; 
&lt;h4&gt;Edit: IF GPU isn't detected then you'll have to build the image -&amp;gt; &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Building the Docker Container&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Running the pre-built Docker Container&lt;/h4&gt; 
&lt;p&gt;-Run with CPU only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;-Run with GPU Speedup (NVIDIA compatible only)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more options add the parameter &lt;code&gt;--help&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Building the Docker Container&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can build the docker image with the command:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker build -t athomasson2/ebook2audiobook .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Avalible Docker Build Arguments&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;--build-arg TORCH_VERSION=cuda118&lt;/code&gt; Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu]&lt;/p&gt; 
&lt;p&gt;All CUDA version numbers should work, Ex: CUDA 11.6-&amp;gt; cuda116&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--build-arg SKIP_XTTS_TEST=true&lt;/code&gt; (Saves space by not baking XTTSv2 model into docker image)&lt;/p&gt; 
&lt;h2&gt;Docker container file locations&lt;/h2&gt; 
&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/app/audiobooks&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Docker headless guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Before you do run this you need to create a dir named "input-folder" in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm athomasson2/ebook2audiobook --help

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That will output this &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Steps to Run&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven't already): &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start the service:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Docker
docker-compose up -d # To update add --build

# Podman
podman compose -f podman-compose.yml up -d # To update add --build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href="http://localhost:7860"&gt;http://localhost:7860&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common Docker Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;python: can't open file '/home/user/app/app.py': [Errno 2] No such file or directory&lt;/code&gt; (Just remove all post arguments as I replaced the &lt;code&gt;CMD&lt;/code&gt; with &lt;code&gt;ENTRYPOINT&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Example: &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker&lt;/code&gt; - &amp;gt; corrected - &amp;gt; &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arguments can be easily added like this now &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook --share&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/191"&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creates a &lt;code&gt;['m4b', 'm4a', 'mp4', 'webm', 'mov', 'mp3', 'flac', 'wav', 'ogg', 'aac']&lt;/code&gt; (set in ./lib/conf.py) file with metadata and chapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Updating to Latest Version&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git pull # Locally/Compose

docker pull athomasson2/ebook2audiobook:latest # For Pre-build docker images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7

athomasson2/ebook2audiobook:VERSION_NUM # For Pre-build docker images -&amp;gt; Example: athomasson2/ebook2audiobook:v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while NVIDIA GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.ğŸ˜Š&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! ğŸ™Œ&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Do you need to rent a GPU to boost service from us?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A poll is open here &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/889"&gt;https://github.com/DrewThomasson/ebook2audiobook/discussions/889&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-VL</title>
      <link>https://github.com/QwenLM/Qwen3-VL</link>
      <description>&lt;p&gt;Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3-VL&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vllogo.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; ğŸ’œ &lt;a href="https://chat.qwenlm.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤— &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤– &lt;a href="https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ“‘ &lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ“š &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ“‘ Paper is coming&amp;nbsp;&amp;nbsp; &lt;br /&gt; ğŸ–¥ï¸ &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-VL-Demo"&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ’¬ &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (å¾®ä¿¡)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ«¨ &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ“‘ &lt;a href="https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api"&gt;API&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ–¥ï¸ &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl"&gt;PAI-DSW&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Meet Qwen3-VL â€” the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; 
&lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; 
&lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoningâ€‘enhanced Thinking editions for flexible, onâ€‘demand deployment.&lt;/p&gt; 
&lt;h4&gt;Key Enhancements:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIsâ€”recognizes elements, understands functions, invokes tools, completes tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates Draw.io/HTML/CSS/JS from images/videos.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Mathâ€”causal analysis and logical, evidence-based answers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to â€œrecognize everythingâ€â€”celebrities, anime, products, landmarks, flora/fauna, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 10); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless textâ€“vision fusion for lossless, unified comprehension.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Architecture Updates:&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg" width="80%" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interleaved-MRoPE&lt;/strong&gt;: Fullâ€‘frequency allocation over time, width, and height via robust positional embeddings, enhancing longâ€‘horizon video reasoning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepStack&lt;/strong&gt;: Fuses multiâ€‘level ViT features to capture fineâ€‘grained details and sharpen imageâ€“text alignment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Textâ€“Timestamp Alignment:&lt;/strong&gt; Moves beyond Tâ€‘RoPE to precise, timestampâ€‘grounded event localization for stronger video temporal modeling.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.10.21: We have released the &lt;strong&gt;Qwen3-VL-2B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking"&gt;Thinking&lt;/a&gt;) and &lt;strong&gt;Qwen3-VL-32B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking"&gt;Thinking&lt;/a&gt;). Enjoy it!&lt;/li&gt; 
 &lt;li&gt;2025.10.15: We have released the &lt;strong&gt;Qwen3-VL-4B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking"&gt;Thinking&lt;/a&gt;) and &lt;strong&gt;Qwen3-VL-8B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking"&gt;Thinking&lt;/a&gt;). Enjoy it!&lt;/li&gt; 
 &lt;li&gt;2025.10.4: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;Qwen3-VL-30B-A3B-Instruct&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"&gt;Qwen3-VL-30B-A3B-Thinking&lt;/a&gt;. We have also released the FP8 version of the Qwen3-VL models â€” available in our &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;HuggingFace collection&lt;/a&gt; and &lt;a href="https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b"&gt;ModelScope collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2025.09.23: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;Qwen3-VL-235B-A22B-Instruct&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;. For more details, please check our &lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.04.08: We provide the &lt;a href="https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune"&gt;code&lt;/a&gt; for fine-tuning Qwen2-VL and Qwen2.5-VL.&lt;/li&gt; 
 &lt;li&gt;2025.03.25: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct"&gt;Qwen2.5-VL-32B&lt;/a&gt;. It is smarter and its responses align more closely with human preferences. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5-vl-32b/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.02.20: we have released the &lt;a href="https://arxiv.org/abs/2502.13923"&gt;Qwen2.5-VL Technical Report&lt;/a&gt;. Alongside the report, we have also released AWQ-quantized models for Qwen2.5-VL in three different sizes: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ"&gt;3B&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ"&gt;7B&lt;/a&gt; , and &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ"&gt;72B&lt;/a&gt; parameters.&lt;/li&gt; 
 &lt;li&gt;2025.01.28: We have released the &lt;a href="https://huggingface.co/Qwen"&gt;Qwen2.5-VL series&lt;/a&gt;. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5-vl/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.12.25: We have released the &lt;a href="https://huggingface.co/Qwen/QVQ-72B-Preview"&gt;QvQ-72B-Preview&lt;/a&gt;. QvQ-72B-Preview is an experimental research model, focusing on enhancing visual reasoning capabilities. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qvq-72b-preview/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: The instruction-tuned &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct"&gt;Qwen2-VL-72B model&lt;/a&gt; and its quantized version [&lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ"&gt;AWQ&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4"&gt;GPTQ-Int4&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8"&gt;GPTQ-Int8&lt;/a&gt;] are now available. We have also released the &lt;a href="https://arxiv.org/pdf/2409.12191"&gt;Qwen2-VL paper&lt;/a&gt; simultaneously.&lt;/li&gt; 
 &lt;li&gt;2024.08.30: We have released the &lt;a href="https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d"&gt;Qwen2-VL series&lt;/a&gt;. The 2B and 7B models are now available, and the 72B model for open source is coming soon. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2-vl/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;h3&gt;Visual Tasks&lt;/h3&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl.jpg" width="24%" /&gt; 
 &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-VL/table_thinking_vl_.jpg" width="24%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl-30a3.jpg" width="26%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_vl_30A3.jpg" width="22.5%" /&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_2b_32b_vl_instruct.jpg" width="30%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_2b_32b_vl_thinking.jpg" width="24%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Text-Centric Tasks&lt;/h3&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text.jpg" width="30%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_text.jpg" width="32%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text-30a3.jpg" width="30%" /&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_instruct.jpg" width="33%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_thinking.jpg" width="28%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Cookbooks&lt;/h2&gt; 
&lt;p&gt;We are preparing &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Cookbook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Open&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/omni_recognition.ipynb"&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/document_parsing.ipynb"&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/2d_grounding.ipynb"&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/ocr.ipynb"&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/video_understanding.ipynb"&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mobile_agent.ipynb"&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for mobile phone control.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/computer_use.ipynb"&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for controlling computers and Web.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/3d_grounding.ipynb"&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/think_with_images.ipynb"&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Utilize image_zoom_in_tool and search_tool to facilitate the modelâ€™s precise comprehension of fine-grained visual details within images.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mmcode.ipynb"&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/long_document_understanding.ipynb"&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/spatial_understanding.ipynb"&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;See, understand and reason about the spatial information&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Below, we provide simple examples to show how to use Qwen3-VL with ğŸ¤– ModelScope and ğŸ¤— Transformers.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# The Qwen3-VL model requires transformers &amp;gt;= 4.57.0
pip install "transformers&amp;gt;=4.57.0"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ¤– ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; 
&lt;h3&gt;Using ğŸ¤— Transformers to Chat&lt;/h3&gt; 
&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor

# default: Load the model on the available device(s)
model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# model = AutoModelForImageTextToText.from_pretrained(
#     "Qwen/Qwen3-VL-235B-A22B-Instruct",
#     dtype=torch.bfloat16,
#     attn_implementation="flash_attention_2",
#     device_map="auto",
# )

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- &lt;details&gt;
&lt;summary&gt;Minimum VRAM requirements&lt;/summary&gt;

| Precision | Qwen2.5-VL-3B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|-----------|------------| --------- | -------- |
| FP32      | 11.5 GB    | 26.34 GB  | 266.21 GB |
| BF16      | 5.75 GB    | 13.17 GB  | 133.11 GB |
| INT8      | 2.87 GB    | 6.59 GB   | 66.5 GB |
| INT4      | 1.44 GB    | 3.29 GB   | 33.28 GB |

Note: The table above presents the theoretical minimum video memory requirements for inference with `transformers`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).
&lt;/details&gt; --&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi image inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing multiple images and a text query
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/image1.jpg"},
            {"type": "image", "image": "file:///path/to/image2.jpg"},
            {"type": "text", "text": "Identify the similarities between these images."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing a video url(or a local path) and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Batch inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# for batch generation, padding_side should be set to left!
processor.tokenizer.padding_side = 'left'

# Sample messages for batch inference
messages1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/image1.jpg"},
            {"type": "image", "image": "file:///path/to/image2.jpg"},
            {"type": "text", "text": "What are the common elements in these pictures?"},
        ],
    }
]
messages2 = [
    {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
    {"role": "user", "content": [{"type": "text", "text": "Who are you?"}]},
]
# Combine messages for batch processing
messages = [messages1, messages2]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
    padding=True # padding should be set for batch generation!
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Pixel Control via Official Processor&lt;/summary&gt; 
 &lt;p&gt;Using the official HF processor, we can conveniently control the budget of visual tokens. Since the Qwen3-VL processor separates image and video processing, we can independently configure the pixel budget for each modality.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the image processor&lt;/strong&gt;:&lt;br /&gt; The parameter &lt;code&gt;size['longest_edge']&lt;/code&gt; originally corresponds to &lt;code&gt;max_pixels&lt;/code&gt;, which defines the maximum number of pixels allowed for an image (i.e., for an image of height H and width W, H Ã— W must not exceed &lt;code&gt;max_pixels&lt;/code&gt;; image channels are ignored for simplicity).&lt;br /&gt; Similarly, &lt;code&gt;size['shortest_edge']&lt;/code&gt; corresponds to &lt;code&gt;min_pixels&lt;/code&gt;, specifying the minimum allowable pixel count for an image.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the video processor&lt;/strong&gt;:&lt;br /&gt; The interpretation differs slightly. &lt;code&gt;size['longest_edge']&lt;/code&gt; represents the maximum total number of pixels across all frames in a video â€” for a video of shape TÃ—HÃ—W, the product TÃ—HÃ—W must not exceed &lt;code&gt;size['longest_edge']&lt;/code&gt;.&lt;br /&gt; Similarly, &lt;code&gt;size['shortest_edge']&lt;/code&gt; sets the minimum total pixel budget for the video.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# budget for image processor, since the compression ratio is 32 for Qwen3-VL, we can set the number of visual tokens of a single image to 256-1280
processor.image_processor.size = {"longest_edge": 1280*32*32, "shortest_edge": 256*32*32}

# budget for video processor, we can set the number of visual tokens of a single video to 256-16384
processor.video_processor.size = {"longest_edge": 16384*32*32, "shortest_edge": 256*32*32}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You can further control the &lt;strong&gt;sample fps&lt;/strong&gt; or &lt;strong&gt;sample frames&lt;/strong&gt; of video, as shown below.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# for video input, we can further control the fps or num_frames. \
# defaultly, fps is set to 2

# set fps = 4
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
    fps=4
)
inputs = inputs.to(model.device)

# set num_frames = 128 and overwrite the fps to None!
# inputs = processor.apply_chat_template(
#     messages,
#     tokenize=True,
#     add_generation_prompt=True,
#     return_dict=True,
#     return_tensors="pt",
#     num_frames=128,
#     fps=None,
# )
# inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;New &lt;code&gt;qwen-vl-utils&lt;/code&gt; Usage&lt;/h3&gt; 
&lt;p&gt;With the latest &lt;code&gt;qwen-vl-utils&lt;/code&gt; toolkit (backward compatible with Qwen2.5-VL), you can control pixel constraints per visual input.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install qwen-vl-utils==0.0.14
# It's highly recommended to use `[decord]` feature for faster video loading.
# pip install qwen-vl-utils[decord]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compared to previous version, the new &lt;code&gt;qwen-vl-utils&lt;/code&gt; introduces:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;"image_patch_size": &lt;code&gt;14&lt;/code&gt; for Qwen2.5-VL and &lt;code&gt;16&lt;/code&gt; for Qwen3-VL. Default set to &lt;code&gt;14&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;"return_video_metadata"(Qwen3-VL only): Due to the new video processor, if True, each video returns as (video_tensor, video_metadata). Default set to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# for Qwen2.5VL, you can simply call 
images, videos, video_kwargs = process_vision_info(messages, return_video_kwargs=True)

# For Qwen3VL series, you should call 
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ“Œ Note: Since &lt;code&gt;qwen-vl-utils&lt;/code&gt; already resizes images/videos, pass &lt;code&gt;do_resize=False&lt;/code&gt; to the processor to avoid duplicate resizing.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Images&lt;/summary&gt; 
 &lt;p&gt;For input images, we support local files, base64, and URLs.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.
## Local file path
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/your/image.jpg"},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
## Image URL
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "http://path/to/your/image.jpg"},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
## Base64 encoded image
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "data:image;base64,/9j/..."},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We provide two methods for fine-grained control over the image size input to the model:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Specify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 32 (32 for Qwen3VL, 28 for Qwen2.5VL).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# resized_height and resized_width
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                "resized_height": 280,
                "resized_width": 420,
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# min_pixels and max_pixels
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                "min_pixels": 50176,
                "max_pixels": 50176,

            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference with qwen-vl-utils
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos = process_vision_info(messages, image_patch_size=16)

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, do_resize=False, return_tensors="pt")
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Videos&lt;/summary&gt; 
 &lt;p&gt;For input videos, we support images lists, local path and url.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing a images list as a video and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": [
                    "file:///path/to/frame1.jpg",
                    "file:///path/to/frame2.jpg",
                    "file:///path/to/frame3.jpg",
                    "file:///path/to/frame4.jpg",
                ],
                'sample_fps':'1', # sample_fps: frame sampling rate (frames per second), used to determine timestamps for each frame
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Messages containing a local video path and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "file:///path/to/video1.mp4",
                "max_pixels": 360 * 420,
                "fps": 1.0,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Messages containing a video url and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
                "min_pixels": 4 * 32 * 32,
                "max_pixels": 256 * 32 * 32,
                "total_pixels": 20480 * 32 * 32,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We recommend setting appropriate values for the &lt;code&gt;min_pixels&lt;/code&gt; and &lt;code&gt;max_pixels&lt;/code&gt; parameters based on available GPU memory and the specific application scenario to restrict the resolution of individual frames in the video.&lt;/p&gt; 
 &lt;p&gt;Alternatively, you can use the &lt;code&gt;total_pixels&lt;/code&gt; parameter to limit the total number of tokens in the video (it is recommended to set this value below 24576 * 32 * 32 to avoid excessively long input sequences). For more details on parameter usage and processing logic, please refer to the &lt;code&gt;fetch_video&lt;/code&gt; function in &lt;code&gt;qwen_vl_utils/vision_process.py&lt;/code&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
                "min_pixels": 4 * 32 * 32,
                "max_pixels": 256 * 32 * 32,
                "total_pixels": 20480 * 32 * 32,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)

# split the videos and according metadatas
if videos is not None:
    videos, video_metadatas = zip(*videos)
    videos, video_metadatas = list(videos), list(video_metadatas)
else:
    video_metadatas = None

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors="pt", do_resize=False, **video_kwargs)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Backends and URL Compatibility&lt;/summary&gt; 
 &lt;p&gt;Currently, &lt;code&gt;qwen-vl-utils&lt;/code&gt; supports three video decoding backends: &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, and &lt;code&gt;torchcodec&lt;/code&gt;. While &lt;code&gt;decord&lt;/code&gt; and &lt;code&gt;torchcodec&lt;/code&gt; generally offer significantly faster decoding speeds compared to &lt;code&gt;torchvision&lt;/code&gt;, we recommend using &lt;code&gt;torchcodec&lt;/code&gt;. This is because &lt;code&gt;decord&lt;/code&gt; has known issues, such as decoding hangs, and its project is no longer actively maintained.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;For &lt;code&gt;decord&lt;/code&gt;, if you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-vl-utils&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href="https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source"&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;To use &lt;code&gt;torchcodec&lt;/code&gt; as the backend for video decoding, follow the installation instructions provided in the official &lt;a href="https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec"&gt;torchcodec repository&lt;/a&gt; and install it manually. Note that &lt;code&gt;torchcodec&lt;/code&gt; depends on FFmpeg for decoding functionality.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Video URL compatibility is primarily determined by the version of the third-party library being used. For more details, refer to the table below. If you prefer not to use the default backend, you can switch it by setting &lt;code&gt;FORCE_QWENVL_VIDEO_READER&lt;/code&gt; to &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, or &lt;code&gt;torchcodec&lt;/code&gt;.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;HTTP&lt;/th&gt; 
    &lt;th&gt;HTTPS&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; 
    &lt;td&gt;âœ…&lt;/td&gt; 
    &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; 
    &lt;td&gt;âŒ&lt;/td&gt; 
    &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;decord&lt;/td&gt; 
    &lt;td&gt;âœ…&lt;/td&gt; 
    &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchcodec&lt;/td&gt; 
    &lt;td&gt;âœ…&lt;/td&gt; 
    &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;More Usage Tips&lt;/h3&gt; 
&lt;h4&gt;Add ids for Multiple Visual Inputs&lt;/h4&gt; 
&lt;p&gt;By default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Add vision ids&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;conversation = [
    {
        "role": "user",
        "content": [{"type": "image"}, {"type": "text", "text": "Hello, how are you?"}],
    },
    {
        "role": "assistant",
        "content": "I'm doing well, thank you for asking. How can I assist you today?",
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "Can you describe these images and video?"},
            {"type": "image"},
            {"type": "image"},
            {"type": "video"},
            {"type": "text", "text": "These are from my vacation."},
        ],
    },
    {
        "role": "assistant",
        "content": "I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?",
    },
    {
        "role": "user",
        "content": "It was a trip to the mountains. Can you see the details in the images and video?",
    },
]

# default:
prompt_without_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True
)
# Excepted output: '&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'm doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n'


# add ids
prompt_with_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True, add_vision_id=True
)
# Excepted output: '&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nPicture 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'm doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?Picture 2: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Picture 3: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Video 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; 
&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To load and run a model using Flash Attention-2, simply add &lt;code&gt;attn_implementation="flash_attention_2"&lt;/code&gt; when loading the model as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from transformers import AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", 
    torch_dtype=torch.bfloat16, 
    attn_implementation="flash_attention_2",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Processing Long Texts&lt;/h4&gt; 
&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 256K tokens. To handle extensive inputs exceeding 256K tokens, we utilize &lt;a href="https://arxiv.org/abs/2309.00071"&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; 
&lt;p&gt;For supported frameworks (currently transformers and vLLM), you could modify &lt;code&gt;max_position_embeddings&lt;/code&gt; and &lt;code&gt;rope_scaling&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    "max_position_embeddings": 1000000,
	...,
    "rope_scaling": {
        "rope_type": "yarn",
        "mrope_section": [
            24,
            20,
            20
        ],
        "mrope_interleaved": true,
        "factor": 3.0,
        "original_max_position_embeddings": 262144
    },
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using vLLM for serving, you can also enable YaRN by adding the additional arguments &lt;code&gt;--rope-scaling&lt;/code&gt; and &lt;code&gt;--max-model-len&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct --rope-scaling '{"rope_type":"yarn","factor":3.0,"original_max_position_embeddings": 262144,"mrope_section":[24,20,20],"mrope_interleaved": true}' --max-model-len 1000000
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Because Interleaved-MRoPEâ€™s position IDs grow more slowly than vanilla RoPE, use a &lt;strong&gt;smaller scaling factor&lt;/strong&gt;. For example, to support 1M context with 256K context length, set factor=2 or 3 â€” not 4.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Try Qwen3-VL-235B-A22 with API!&lt;/h3&gt; 
&lt;p&gt;To explore Qwen3-VL-235B-A22, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let's start the exciting journey right now!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# set your DASHSCOPE_API_KEY here
DASHSCOPE_API_KEY = ""

client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

completion = client.chat.completions.create(
    model="qwen3-vl-235b-a22b-instruct",
    messages=[{"role": "user", "content": [
        {"type": "image_url",
         "image_url": {"url": "https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg"}},
        {"type": "text", "text": "è¿™æ˜¯ä»€ä¹ˆ"},
    ]}]
)
print(completion.model_dump_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more usage, please refer to the tutorial at &lt;a href="https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api"&gt;aliyun&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Web UI Example&lt;/h3&gt; 
&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.&lt;/p&gt; 
&lt;p&gt;Install the required dependencies by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements_web_demo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Launch a browser-based UI to interact with the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_demo_mm.py -c /your/path/to/qwen3vl/weight
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the command, youâ€™ll see a link generated in the terminal similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open the link in your browser to interact with the model â€” try text, images, or other features. For a quick start, you can also use our pre-built Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd docker &amp;amp;&amp;amp; bash run_web_demo.sh -c /your/path/to/qwen3vl/weight --port 8881
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;p&gt;We recommend using vLLM for fast Qwen3-VL deployment and inference. You need to install &lt;code&gt;vllm&amp;gt;=0.11.0&lt;/code&gt; to enable Qwen3-VL support. You can also use our &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-VL/main/#-docker"&gt;official docker image&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please check &lt;a href="https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html"&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference for multimodal models.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install accelerate
pip install qwen-vl-utils==0.0.14
# Install the latest version of vLLM 'vllm&amp;gt;=0.11.0'
uv pip install -U vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Online Serving&lt;/h3&gt; 
&lt;p&gt;You can start either a vLLM or SGLang server to serve LLMs efficiently, and then access it using an OpenAI-style API.&lt;/p&gt; 
&lt;p&gt;The following launch command is applicable to H100/H200; for more efficient deployment or deployment on other GPUs, please refer to the &lt;a href="https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html"&gt;vLLM community guide&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Efficient inference with FP8 checkpoint
# Requires NVIDIA H100+ and CUDA 12+
vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct-FP8 \
  --tensor-parallel-size 8 \
  --mm-encoder-tp-mode data \
  --enable-expert-parallel \
  --async-scheduling \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --host 0.0.0.0 \
  --port 22002
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server \
   --model-path Qwen/Qwen3-VL-235B-A22B-Instruct \
   --host 0.0.0.0 \
   --port 22002 \
   --tp 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key="EMPTY",
    base_url="http://127.0.0.1:22002/v1",
    timeout=3600
)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png"
                }
            },
            {
                "type": "text",
                "text": "Read all the text in the image."
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-235B-A22B-Instruct-FP8",
    messages=messages,
    max_tokens=2048
)
print(f"Response costs: {time.time() - start:.2f}s")
print(f"Generated text: {response.choices[0].message.content}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key="EMPTY",
    base_url="http://127.0.0.1:22002/v1",
    timeout=3600
)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video_url",
                "video_url": {
                    "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4"
                }
            },
            {
                "type": "text",
                "text": "How long is this video?"
            }
        ]
    }
]

start = time.time()

# When vLLM is launched with `--media-io-kwargs '{"video": {"num_frames": -1}}'`,
# video frame sampling can be configured via `extra_body` (e.g., by setting `fps`).
# This feature is currently supported only in vLLM.
#
# By default, `fps=2` and `do_sample_frames=True`.
# With `do_sample_frames=True`, you can customize the `fps` value to set your desired video sampling rate.
response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-235B-A22B-Instruct-FP8",
    messages=messages,
    max_tokens=2048,
    extra_body={"mm_processor_kwargs": {"fps": 2, "do_sample_frames": True}}
)

print(f"Response costs: {time.time() - start:.2f}s")
print(f"Generated text: {response.choices[0].message.content}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Offline Inference&lt;/h3&gt; 
&lt;p&gt;You can also use vLLM or SGLang to inference Qwen3-VL locally:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# -*- coding: utf-8 -*-
import torch
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor
from vllm import LLM, SamplingParams

import os
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'

def prepare_inputs_for_vllm(messages, processor):
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    # qwen_vl_utils 0.0.14+ reqired
    image_inputs, video_inputs, video_kwargs = process_vision_info(
        messages,
        image_patch_size=processor.image_processor.patch_size,
        return_video_kwargs=True,
        return_video_metadata=True
    )
    print(f"video_kwargs: {video_kwargs}")

    mm_data = {}
    if image_inputs is not None:
        mm_data['image'] = image_inputs
    if video_inputs is not None:
        mm_data['video'] = video_inputs

    return {
        'prompt': text,
        'multi_modal_data': mm_data,
        'mm_processor_kwargs': video_kwargs
    }


if __name__ == '__main__':
    # messages = [
    #     {
    #         "role": "user",
    #         "content": [
    #             {
    #                 "type": "video",
    #                 "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
    #             },
    #             {"type": "text", "text": "è¿™æ®µè§†é¢‘æœ‰å¤šé•¿"},
    #         ],
    #     }
    # ]

    messages = [
        {
            "role": "user",
            "content": [
              {
                  "type": "image",
                  "image": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png",
              },
              {"type": "text", "text": "Read all the text in the image."},
            ],
        }
    ]

    # TODO: change to your own checkpoint path
    checkpoint_path = "Qwen/Qwen3-VL-235B-A22B-Instruct-FP8"
    processor = AutoProcessor.from_pretrained(checkpoint_path)
    inputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]

    llm = LLM(
        model=checkpoint_path,
        mm_encoder_tp_mode="data",
        enable_expert_parallel=True,
        tensor_parallel_size=torch.cuda.device_count(),
        seed=0
    )

    sampling_params = SamplingParams(
        temperature=0,
        max_tokens=1024,
        top_k=-1,
        stop_token_ids=[],
    )

    for i, input_ in enumerate(inputs):
        print()
        print('=' * 40)
        print(f"Inputs[{i}]: {input_['prompt']=!r}")
    print('\n' + '&amp;gt;' * 40)

    outputs = llm.generate(inputs, sampling_params=sampling_params)
    for i, output in enumerate(outputs):
        generated_text = output.outputs[0].text
        print()
        print('=' * 40)
        print(f"Generated text: {generated_text!r}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from PIL import Image
from sglang import Engine
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor, AutoConfig


if __name__ == "__main__":
    # TODO: change to your own checkpoint path
    checkpoint_path = "Qwen/Qwen3-VL-235B-A22B-Instruct"
    processor = AutoProcessor.from_pretrained(checkpoint_path)

    messages = [
        {
            "role": "user",
            "content": [
              {
                  "type": "image",
                  "image": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png",
              },
              {"type": "text", "text": "Read all the text in the image."},
            ],
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    image_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)

    llm = Engine(
        model_path=checkpoint_path,
        enable_multimodal=True,
        mem_fraction_static=0.8,
        tp_size=4,
        attention_backend="fa3",
        context_length=10240,
        disable_cuda_graph=True,
    )

    start = time.time()
    sampling_params = {"max_new_tokens": 1024}
    response = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)
    print(f"Response costs: {time.time() - start:.2f}s")
    print(f"Generated text: {response['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Evaluation Reproduction&lt;/h2&gt; 
&lt;p&gt;To facilitate faithful reproduction of our reported results, we summarize our official evaluation settings below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inference runtime: &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Evaluation frameworks: &lt;a href="https://github.com/open-compass/VLMEvalKit"&gt;VLMEvalKit&lt;/a&gt;, &lt;a href="https://github.com/EvolvingLMMs-Lab/lmms-eval"&gt;lmms-eval&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Notes: 
  &lt;ul&gt; 
   &lt;li&gt;For a few benchmarks, we slightly modified the evaluation prompts; detailed changes will be documented in the upcoming technical report.&lt;/li&gt; 
   &lt;li&gt;A small number of benchmarks are internally constructed; we plan to release the code and reproduction assets afterwards.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Generation Hyperparameters&lt;/h3&gt; 
&lt;h4&gt;Instruct models&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export greedy='false'
export seed=3407
export top_p=0.8
export top_k=20
export temperature=0.7
export repetition_penalty=1.0
export presence_penalty=1.5
export out_seq_length=32768
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Thinking models&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export greedy='false'
export seed=1234
export top_p=0.95
export top_k=20
export repetition_penalty=1.0
export presence_penalty=0.0
export temperature=0.6
export out_seq_length=40960
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ³ Docker&lt;/h2&gt; 
&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href="https://hub.docker.com/r/qwenllm/qwenvl"&gt;qwenllm/qwenvl&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen3vl -it qwenllm/qwenvl:qwen3vl-cu128 bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;â­&lt;/span&gt; and citation &lt;span&gt;ğŸ“&lt;/span&gt; :)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;
@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{Qwen2-VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;ç¹é«”ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;FranÃ§ais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- icon --&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/forks/PaddlePaddle/PaddleOCR.svg?sanitize=true" alt="forks" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR_3.0-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.14528"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pepy.tech/projectsproject/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-6k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/paddleocr/"&gt;&lt;img src="https://img.shields.io/pypi/v/paddleocr" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://www.paddleocr.com"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR-_Offiical_Website-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-100+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR-VL Technical Report is now available. See details at &lt;a href="https://arxiv.org/abs/2510.14528"&gt;PaddleOCR-VL Technical Report&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The Beta version of the PaddleOCR official website is now live, offering a more convenient online experience and large-scale PDF file parsing, as well as free API and MCP services. For more details, please visit the &lt;a href="https://www.paddleocr.com"&gt;PaddleOCR official website&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;â€”powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;60,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, pathway and cherry-studio&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_HuggingFace-yellow?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/application/detail/98365"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/PaddlePaddle/PaddleOCR-VL_Online_Demo"&gt;&lt;img src="https://img.shields.io/badge/PaddleOCR--VL-_Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--OCRv5-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--StructureV3-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP--ChatOCRv4-Demo_on_AI_Studio-1927BA?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PaddleOCR-VL - Multilingual Document Parsing via a 0.9B VLM&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;The SOTA and resource-efficient model tailored for document parsing&lt;/strong&gt;, that supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 â€” Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 â€” Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 â€” Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;ğŸ“£ Recent updates&lt;/h2&gt; 
&lt;h3&gt;ğŸ”¥ğŸ”¥ 2025.10.16: PaddleOCR 3.3.0 released, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Released PaddleOCR-VL:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Introduction&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;PaddleOCR-VL&lt;/strong&gt; is a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. &lt;strong&gt;This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption&lt;/strong&gt;. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. The model has been released on &lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;HuggingFace&lt;/a&gt;. Everyone is welcome to download and use it! More introduction infomation can be found in &lt;a href="https://www.paddleocr.ai/latest/version3.x/algorithm/PaddleOCR-VL/PaddleOCR-VL.html"&gt;PaddleOCR-VL&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Compact yet Powerful VLM Architecture&lt;/strong&gt;: We present a novel vision-language model that is specifically designed for resource-efficient inference, achieving outstanding performance in element recognition. By integrating a NaViT-style dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model, we significantly enhance the modelâ€™s recognition capabilities and decoding efficiency. This integration maintains high accuracy while reducing computational demands, making it well-suited for efficient and practical document processing applications.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;SOTA Performance on Document Parsing&lt;/strong&gt;: PaddleOCR-VL achieves state-of-the-art performance in both page-level document parsing and element-level recognition. It significantly outperforms existing pipeline-based solutions and exhibiting strong competitiveness against leading vision-language models (VLMs) in document parsing. Moreover, it excels in recognizing complex document elements, such as text, tables, formulas, and charts, making it suitable for a wide range of challenging content types, including handwritten text and historical documents. This makes it highly versatile and suitable for a wide range of document types and scenarios.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multilingual Support&lt;/strong&gt;: PaddleOCR-VL Supports 109 languages, covering major global languages, including but not limited to Chinese, English, Japanese, Latin, and Korean, as well as languages with different scripts and structures, such as Russian (Cyrillic script), Arabic, Hindi (Devanagari script), and Thai. This broad language coverage substantially enhances the applicability of our system to multilingual and globalized document processing scenarios.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Released PP-OCRv5 Multilingual Recognition Model:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Improved the accuracy and coverage of Latin script recognition; added support for Cyrillic, Arabic, Devanagari, Telugu, Tamil, and other language systems, covering recognition of 109 languages. The model has only 2M parameters, and the accuracy of some models has increased by over 40% compared to the previous generation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.21: Release of PaddleOCR 3.2.0&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
    &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
    &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
    &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
    &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languagesâ€”C++, Java, Go, C#, Node.js, and PHPâ€”for pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ğŸ”¥ğŸ”¥2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸŒ Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;âœï¸ Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;ğŸ¯ &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing â€“ Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸ§® &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;ğŸ§  Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding â€“ Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸ”¥ &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;ğŸ’» Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;ğŸ¤ Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;âš¡ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3, PaddleOCR-VL&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k é©¾é©¶å®¤å‡†ä¹˜äººæ•° --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Run PaddleOCR-VL inference
paddleocr doc_parser -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["é©¾é©¶å®¤å‡†ä¹˜äººæ•°"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["é©¾é©¶å®¤å‡†ä¹˜äººæ•°"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.4 PaddleOCR-VL Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PaddleOCRVL

pipeline = PaddleOCRVL()
output = pipeline.predict("https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/paddleocr_vl_demo.png")
for res in output:
    res.print()
    res.save_to_json(save_path="output")
    res.save_to_markdown(save_path="output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ§© More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â›°ï¸ Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PaddleOCR-VL.html"&gt;PaddleOCR-VL Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”„ Quick Overview of Execution Results&lt;/h2&gt; 
&lt;h3&gt;PP-OCRv5&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/PP-OCRv5_demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;PP-StructureV3&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/PP-StructureV3_demo.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;PaddleOCR-VL&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/PaddleOCR-VL_demo.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;âœ¨ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;â­ &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; â­&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ˜ƒ Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! ğŸ’— A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR â€” whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/pathwaycom/pathway"&gt;pathway&lt;/a&gt; &lt;a href="https://github.com/pathwaycom/pathway"&gt;&lt;img src="https://img.shields.io/github/stars/pathwaycom/pathway" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;cherry-studio&lt;/a&gt; &lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;&lt;img src="https://img.shields.io/github/stars/CherryHQ/cherry-studio" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A desktop client that supports for multiple LLM providers.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}

@misc{cui2025paddleocrvlboostingmultilingualdocument,
      title={PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model}, 
      author={Cheng Cui and Ting Sun and Suyin Liang and Tingquan Gao and Zelun Zhang and Jiaxuan Liu and Xueqing Wang and Changda Zhou and Hongen Liu and Manhui Lin and Yue Zhang and Yubo Zhang and Handong Zheng and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2510.14528},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.14528}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>jingyaogong/minimind</title>
      <link>https://github.com/jingyaogong/minimind</link>
      <description>&lt;p&gt;ğŸš€ğŸš€ ã€Œå¤§æ¨¡å‹ã€2å°æ—¶å®Œå…¨ä»0è®­ç»ƒ26Mçš„å°å‚æ•°GPTï¼ğŸŒ Train a 26M-parameter GPT from scratch in just 2h!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/logo.png" alt="logo" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind" alt="visitors" /&gt; &lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/jingyaogong/minimind?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/jingyaogong/minimind" alt="GitHub Code License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/commits/master"&gt;&lt;img src="https://img.shields.io/github/last-commit/jingyaogong/minimind" alt="GitHub last commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/pulls"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue" alt="GitHub pull request" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-MiniMind%20%20Collection-blue" alt="Collection" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://trendshift.io/api/badge/repositories/12586" alt="GitHub Trend" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;"å¤§é“è‡³ç®€"&lt;/h3&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;ä¸­æ–‡ | &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ­¤å¼€æºé¡¹ç›®æ—¨åœ¨å®Œå…¨ä»0å¼€å§‹ï¼Œä»…ç”¨3å—é’±æˆæœ¬ + 2å°æ—¶ï¼å³å¯è®­ç»ƒå‡ºä»…ä¸º25.8Mçš„è¶…å°è¯­è¨€æ¨¡å‹&lt;strong&gt;MiniMind&lt;/strong&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;ç³»åˆ—æå…¶è½»é‡ï¼Œæœ€å°ç‰ˆæœ¬ä½“ç§¯æ˜¯ GPT-3 çš„ $\frac{1}{7000}$ï¼ŒåŠ›æ±‚åšåˆ°æœ€æ™®é€šçš„ä¸ªäººGPUä¹Ÿå¯å¿«é€Ÿè®­ç»ƒã€‚&lt;/li&gt; 
 &lt;li&gt;é¡¹ç›®åŒæ—¶å¼€æºäº†å¤§æ¨¡å‹çš„æç®€ç»“æ„-åŒ…å«æ‹“å±•å…±äº«æ··åˆä¸“å®¶(MoE)ã€æ•°æ®é›†æ¸…æ´—ã€é¢„è®­ç»ƒ(Pretrain)ã€ç›‘ç£å¾®è°ƒ(SFT)ã€LoRAå¾®è°ƒã€ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ã€å¼ºåŒ–å­¦ä¹ è®­ç»ƒ(RLAIF: PPO/GRPOç­‰)ã€æ¨¡å‹è’¸é¦ç­‰å…¨è¿‡ç¨‹ä»£ç ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;åŒæ—¶æ‹“å±•äº†è§†è§‰å¤šæ¨¡æ€çš„VLM: &lt;a href="https://github.com/jingyaogong/minimind-v"&gt;MiniMind-V&lt;/a&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;é¡¹ç›®æ‰€æœ‰æ ¸å¿ƒç®—æ³•ä»£ç å‡ä»0ä½¿ç”¨PyTorchåŸç”Ÿé‡æ„ï¼ä¸ä¾èµ–ç¬¬ä¸‰æ–¹åº“æä¾›çš„æŠ½è±¡æ¥å£ã€‚&lt;/li&gt; 
 &lt;li&gt;è¿™ä¸ä»…æ˜¯å¤§è¯­è¨€æ¨¡å‹çš„å…¨é˜¶æ®µå¼€æºå¤ç°ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå…¥é—¨LLMçš„æ•™ç¨‹ã€‚&lt;/li&gt; 
 &lt;li&gt;å¸Œæœ›æ­¤é¡¹ç›®èƒ½ä¸ºæ‰€æœ‰äººæä¾›ä¸€ä¸ªæŠ›ç –å¼•ç‰çš„ç¤ºä¾‹ï¼Œä¸€èµ·æ„Ÿå—åˆ›é€ çš„ä¹è¶£ï¼æ¨åŠ¨æ›´å¹¿æ³›AIç¤¾åŒºçš„è¿›æ­¥ï¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸ºé˜²æ­¢è¯¯è§£ï¼Œâ€œ2å°æ—¶â€ åŸºäºNVIDIA 3090ç¡¬ä»¶è®¾å¤‡ï¼ˆå•å¡ï¼‰æµ‹è¯•ï¼Œâ€œ3å—é’±â€æŒ‡GPUæœåŠ¡å™¨ç§Ÿç”¨æˆæœ¬ï¼Œå…·ä½“è§„æ ¼è¯¦æƒ…è§ä¸‹æ–‡ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/minimind2.gif" alt="minimind2" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning"&gt;ğŸ”—ğŸ“æ¨ç†æ¨¡å‹&lt;/a&gt; | &lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind"&gt;ğŸ”—ğŸ¤–å¸¸è§„æ¨¡å‹&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8"&gt;ğŸ”—ğŸï¸è§†é¢‘ä»‹ç»&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;tbody&gt;
    &lt;tr&gt; 
     &lt;td align="center"&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_huggingface.png" alt="Hugging Face Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
     &lt;td align="center"&gt; &lt;a href="https://www.modelscope.cn/profile/gongjy" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_modelscope.png" alt="ModelScope Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt;
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h1&gt;ğŸ“Œ Introduction&lt;/h1&gt; 
&lt;p&gt;å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Model, LLMï¼‰çš„å‡ºç°å¼•å‘äº†å…¨ä¸–ç•Œå¯¹AIçš„ç©ºå‰å…³æ³¨ã€‚ æ— è®ºæ˜¯ChatGPTã€DeepSeekè¿˜æ˜¯Qwenï¼Œéƒ½ä»¥å…¶æƒŠè‰³çš„æ•ˆæœä»¤äººå¹ä¸ºè§‚æ­¢ã€‚ ç„¶è€Œï¼ŒåŠ¨è¾„æ•°ç™¾äº¿å‚æ•°çš„åºå¤§è§„æ¨¡ï¼Œä½¿å¾—å®ƒä»¬å¯¹ä¸ªäººè®¾å¤‡è€Œè¨€ä¸ä»…éš¾ä»¥è®­ç»ƒï¼Œç”šè‡³è¿éƒ¨ç½²éƒ½æ˜¾å¾—é¥ä¸å¯åŠã€‚ æ‰“å¼€å¤§æ¨¡å‹çš„â€œé»‘ç›’å­â€ï¼Œæ¢ç´¢å…¶å†…éƒ¨è¿ä½œæœºåˆ¶ï¼Œå¤šä¹ˆä»¤äººå¿ƒæ½®æ¾æ¹ƒï¼ é—æ†¾çš„æ˜¯ï¼Œ99%çš„æ¢ç´¢åªèƒ½æ­¢æ­¥äºä½¿ç”¨LoRAç­‰æŠ€æœ¯å¯¹ç°æœ‰å¤§æ¨¡å‹è¿›è¡Œå°‘é‡å¾®è°ƒï¼Œå­¦ä¹ ä¸€äº›æ–°æŒ‡ä»¤æˆ–ä»»åŠ¡ã€‚ è¿™å°±å¥½æ¯”æ•™ç‰›é¡¿å¦‚ä½•ä½¿ç”¨21ä¸–çºªçš„æ™ºèƒ½æ‰‹æœºâ€”â€”è™½ç„¶æœ‰è¶£ï¼Œå´å®Œå…¨åç¦»äº†ç†è§£ç‰©ç†æœ¬è´¨çš„åˆè¡·ã€‚ ä¸æ­¤åŒæ—¶ï¼Œç¬¬ä¸‰æ–¹çš„å¤§æ¨¡å‹æ¡†æ¶å’Œå·¥å…·åº“ï¼Œå¦‚transformers+trlï¼Œå‡ ä¹åªæš´éœ²äº†é«˜åº¦æŠ½è±¡çš„æ¥å£ã€‚ é€šè¿‡çŸ­çŸ­10è¡Œä»£ç ï¼Œå°±èƒ½å®Œæˆâ€œåŠ è½½æ¨¡å‹+åŠ è½½æ•°æ®é›†+æ¨ç†+å¼ºåŒ–å­¦ä¹ â€çš„å…¨æµç¨‹è®­ç»ƒã€‚ è¿™ç§é«˜æ•ˆçš„å°è£…å›ºç„¶ä¾¿åˆ©ï¼Œä½†ä¹Ÿåƒä¸€æ¶é«˜é€Ÿé£èˆ¹ï¼Œå°†å¼€å‘è€…ä¸åº•å±‚å®ç°éš”ç¦»å¼€æ¥ï¼Œé˜»ç¢äº†æ·±å…¥æ¢ç©¶LLMæ ¸å¿ƒä»£ç çš„æœºä¼šã€‚ ç„¶è€Œï¼Œâ€œç”¨ä¹é«˜æ‹¼å‡ºä¸€æ¶é£æœºï¼Œè¿œæ¯”ååœ¨å¤´ç­‰èˆ±é‡Œé£è¡Œæ›´è®©äººå…´å¥‹ï¼â€ã€‚ æ›´ç³Ÿç³•çš„æ˜¯ï¼Œäº’è”ç½‘ä¸Šå……æ–¥ç€å¤§é‡ä»˜è´¹è¯¾ç¨‹å’Œè¥é”€å·ï¼Œä»¥æ¼æ´ç™¾å‡ºã€ä¸€çŸ¥åŠè§£çš„å†…å®¹æ¨é”€AIæ•™ç¨‹ã€‚ æ­£å› å¦‚æ­¤ï¼Œæœ¬é¡¹ç›®åˆè¡·æ˜¯æ‹‰ä½LLMçš„å­¦ä¹ é—¨æ§›ï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½ä»ç†è§£æ¯ä¸€è¡Œä»£ç å¼€å§‹ï¼Œ ä»é›¶å¼€å§‹äº²æ‰‹è®­ç»ƒä¸€ä¸ªæå°çš„è¯­è¨€æ¨¡å‹ã€‚æ˜¯çš„ï¼Œä»&lt;strong&gt;é›¶å¼€å§‹è®­ç»ƒ&lt;/strong&gt;ï¼Œè€Œä¸æ˜¯ä»…ä»…è¿›è¡Œ&lt;strong&gt;æ¨ç†&lt;/strong&gt;ï¼ æœ€ä½åªéœ€3å—é’±ä¸åˆ°çš„æœåŠ¡å™¨æˆæœ¬ï¼Œå°±èƒ½äº²èº«ä½“éªŒä»0åˆ°1æ„å»ºä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å…¨è¿‡ç¨‹ã€‚ ä¸€èµ·æ„Ÿå—åˆ›é€ çš„ä¹è¶£å§ï¼&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] ï¼ˆæˆªè‡³2025-10ï¼‰MiniMindç³»åˆ—å·²å®Œæˆå¤šä¸ªå‹å·æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œæœ€å°ä»…éœ€25.8Mï¼ˆ0.02Bï¼‰ï¼Œå³å¯å…·å¤‡æµç•…å¯¹è¯èƒ½åŠ›ï¼&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Models List&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;æ¨¡å‹ (å¤§å°)&lt;/th&gt; 
    &lt;th&gt;æ¨ç†å ç”¨ (çº¦)&lt;/th&gt; 
    &lt;th&gt;Release&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE (145M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2 (104M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2024.08.28&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe (4Ã—26M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.17&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1 (108M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.01&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;é¡¹ç›®åŒ…å«&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MiniMind-LLMç»“æ„çš„å…¨éƒ¨ä»£ç ï¼ˆDense+MoEæ¨¡å‹ï¼‰ã€‚&lt;/li&gt; 
 &lt;li&gt;åŒ…å«Tokenizeråˆ†è¯å™¨è¯¦ç»†è®­ç»ƒä»£ç ã€‚&lt;/li&gt; 
 &lt;li&gt;åŒ…å«Pretrainã€SFTã€LoRAã€RLHF-DPOã€RLAIF(PPO/GRPO/SPO)ã€æ¨¡å‹è’¸é¦çš„å…¨è¿‡ç¨‹è®­ç»ƒä»£ç ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¶é›†ã€è’¸é¦ã€æ•´ç†å¹¶æ¸…æ´—å»é‡æ‰€æœ‰é˜¶æ®µçš„é«˜è´¨é‡æ•°æ®é›†ï¼Œä¸”å…¨éƒ¨å¼€æºã€‚&lt;/li&gt; 
 &lt;li&gt;ä»0å®ç°é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒã€LoRAã€DPO/PPO/GRPO/SPOå¼ºåŒ–å­¦ä¹ ï¼Œç™½ç›’æ¨¡å‹è’¸é¦ã€‚å…³é”®ç®—æ³•å‡ ä¹ä¸ä¾èµ–ç¬¬ä¸‰æ–¹å°è£…çš„æ¡†æ¶ï¼Œä¸”å…¨éƒ¨å¼€æºã€‚&lt;/li&gt; 
 &lt;li&gt;åŒæ—¶å…¼å®¹&lt;code&gt;transformers&lt;/code&gt;ã€&lt;code&gt;trl&lt;/code&gt;ã€&lt;code&gt;peft&lt;/code&gt;ç­‰ç¬¬ä¸‰æ–¹ä¸»æµæ¡†æ¶ã€‚&lt;/li&gt; 
 &lt;li&gt;è®­ç»ƒæ”¯æŒå•æœºå•å¡ã€å•æœºå¤šå¡(DDPã€DeepSpeed)è®­ç»ƒï¼Œæ”¯æŒwandb/swanlabå¯è§†åŒ–è®­ç»ƒæµç¨‹ã€‚æ”¯æŒåŠ¨æ€å¯åœè®­ç»ƒã€‚&lt;/li&gt; 
 &lt;li&gt;åœ¨ç¬¬ä¸‰æ–¹æµ‹è¯„æ¦œï¼ˆC-Evalã€C-MMLUã€OpenBookQAç­‰ï¼‰è¿›è¡Œæ¨¡å‹æµ‹è¯•ï¼Œæ”¯æŒYaRNç®—æ³•æ‰§è¡ŒRoPEé•¿æ–‡æœ¬å¤–æ¨ã€‚&lt;/li&gt; 
 &lt;li&gt;å®ç°Openai-Apiåè®®çš„æç®€æœåŠ¡ç«¯ï¼Œä¾¿äºé›†æˆåˆ°ç¬¬ä¸‰æ–¹ChatUIä½¿ç”¨ï¼ˆFastGPTã€Open-WebUIç­‰ï¼‰ã€‚&lt;/li&gt; 
 &lt;li&gt;åŸºäºstreamlitå®ç°æœ€ç®€èŠå¤©WebUIå‰ç«¯ã€‚&lt;/li&gt; 
 &lt;li&gt;å…¨é¢å…¼å®¹ç¤¾åŒºçƒ­é—¨&lt;code&gt;llama.cpp&lt;/code&gt;ã€&lt;code&gt;vllm&lt;/code&gt;ã€&lt;code&gt;ollama&lt;/code&gt;æ¨ç†å¼•æ“æˆ–&lt;code&gt;Llama-Factory&lt;/code&gt;è®­ç»ƒæ¡†æ¶ã€‚&lt;/li&gt; 
 &lt;li&gt;å¤ç°(è’¸é¦/RL)å¤§å‹æ¨ç†æ¨¡å‹DeepSeek-R1çš„MiniMind-Reasonæ¨¡å‹ï¼Œ&lt;strong&gt;æ•°æ®+æ¨¡å‹&lt;/strong&gt;å…¨éƒ¨å¼€æºï¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¸Œæœ›æ­¤å¼€æºé¡¹ç›®å¯ä»¥å¸®åŠ©LLMåˆå­¦è€…å¿«é€Ÿå…¥é—¨ï¼&lt;/p&gt; 
&lt;h3&gt;ğŸ‘‰&lt;strong&gt;æ›´æ–°æ—¥å¿—&lt;/strong&gt;&lt;/h3&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-10-24&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ”¥ æ–°å¢RLAIFè®­ç»ƒç®—æ³•ï¼šPPOã€GRPOã€SPOï¼ˆä»0åŸç”Ÿå®ç°ï¼‰&lt;/li&gt; 
  &lt;li&gt;æ–°å¢æ–­ç‚¹ç»­è®­åŠŸèƒ½ï¼šæ”¯æŒè®­ç»ƒè‡ªåŠ¨æ¢å¤ã€è·¨GPUæ•°é‡æ¢å¤ã€wandbè®°å½•è¿ç»­æ€§&lt;/li&gt; 
  &lt;li&gt;æ–°å¢RLAIFæ•°æ®é›†ï¼šrlaif-mini.jsonlï¼ˆä»SFTæ•°æ®éšæœºé‡‡æ ·1ä¸‡æ¡ï¼‰ï¼›ç®€åŒ–DPOæ•°æ®é›†ï¼ŒåŠ å…¥ä¸­æ–‡æ•°æ®&lt;/li&gt; 
  &lt;li&gt;æ–°å¢YaRNç®—æ³•ï¼šæ”¯æŒRoPEé•¿æ–‡æœ¬å¤–æ¨ï¼Œæå‡é•¿åºåˆ—å¤„ç†èƒ½åŠ›&lt;/li&gt; 
  &lt;li&gt;Adaptive Thinkingï¼šReasonæ¨¡å‹å¯é€‰æ˜¯å¦å¯ç”¨æ€è€ƒé“¾&lt;/li&gt; 
  &lt;li&gt;chat_templateå…¨é¢æ”¯æŒTool Callingå’ŒReasoningæ ‡ç­¾ï¼ˆ&lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt;ã€&lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;ç­‰ï¼‰&lt;/li&gt; 
  &lt;li&gt;æ–°å¢RLAIFå®Œæ•´ç« èŠ‚ã€è®­ç»ƒæ›²çº¿å¯¹æ¯”ã€ç®—æ³•åŸç†æŠ˜å è¯´æ˜&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://swanlab.cn/"&gt;SwanLab&lt;/a&gt;æ›¿ä»£WandBï¼ˆå›½å†…è®¿é—®å‹å¥½ï¼ŒAPIå®Œå…¨å…¼å®¹ï¼‰&lt;/li&gt; 
  &lt;li&gt;è§„èŒƒåŒ–æ‰€æœ‰ä»£ç  &amp;amp; ä¿®å¤ä¸€äº›å·²çŸ¥bugs&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-04-26&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;é‡è¦æ›´æ–°&lt;/li&gt; 
  &lt;li&gt;å¦‚æœ‰å…¼å®¹æ€§éœ€è¦ï¼Œå¯è®¿é—®&lt;a href="https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a"&gt;ğŸ”—æ—§ä»“åº“å†…å®¹ğŸ”—&lt;/a&gt;ã€‚&lt;/li&gt; 
  &lt;li&gt;MiniMindæ¨¡å‹å‚æ•°å®Œå…¨æ”¹åï¼Œå¯¹é½Transformersåº“æ¨¡å‹ï¼ˆç»Ÿä¸€å‘½åï¼‰ã€‚&lt;/li&gt; 
  &lt;li&gt;generateæ–¹å¼é‡æ„ï¼Œç»§æ‰¿è‡ªGenerationMixinç±»ã€‚&lt;/li&gt; 
  &lt;li&gt;ğŸ”¥æ”¯æŒllama.cppã€vllmã€ollamaç­‰çƒ­é—¨ä¸‰æ–¹ç”Ÿæ€ã€‚&lt;/li&gt; 
  &lt;li&gt;è§„èŒƒä»£ç å’Œç›®å½•ç»“æ„ã€‚&lt;/li&gt; 
  &lt;li&gt;æ”¹åŠ¨è¯è¡¨&lt;code&gt;&amp;lt;s&amp;gt;&amp;lt;/s&amp;gt;&lt;/code&gt;-&amp;gt;&lt;code&gt;&amp;lt;|im_start|&amp;gt;&amp;lt;|im_end|&amp;gt;&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;ä¸ºå…¼å®¹ç¬¬ä¸‰æ–¹æ¨ç†æ¡†æ¶llama.cppã€vllmï¼Œæœ¬æ¬¡æ›´æ–°éœ€ä»˜å‡ºä¸€äº›å¯è§‚ä»£ä»·ã€‚
æœ¬æ¬¡æ›´æ–°ä¸å†æ”¯æŒã€Œç›´æ¥ã€åŠ è½½25-04-26ä»¥å‰çš„æ—§æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
ç”±äºLlamaä½ç½®ç¼–ç æ–¹å¼ä¸minimindå­˜åœ¨åŒºåˆ«ï¼Œå¯¼è‡´æ˜ å°„Llamaæ¨¡å‹åQKå€¼å­˜åœ¨å·®å¼‚
MiniMind2ç³»åˆ—æ—§æ¨¡å‹å‡ç»è¿‡æƒé‡æ˜ å°„+ï¼ˆå¾®è°ƒè®­ç»ƒï¼‰QKVOçº¿æ€§å±‚æ ¡å‡†æ¢å¤è€Œæ¥ã€‚
æœ¬æ¬¡æ›´æ–°åå°†æ”¾å¼ƒå¯¹`minimind-v1`å…¨ç³»åˆ—çš„ç»´æŠ¤ï¼Œå¹¶åœ¨ä»“åº“ä¸­ä¸‹çº¿ã€‚
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;è¿æ¥å‘å¸ƒä»¥æ¥é‡å¤§æ›´æ–°ï¼ŒRelease MiniMind2 Seriesã€‚&lt;/li&gt; 
  &lt;li&gt;ä»£ç å‡ ä¹å…¨éƒ¨é‡æ„ï¼Œä½¿ç”¨æ›´ç®€æ´æ˜äº†çš„ç»Ÿä¸€ç»“æ„ã€‚ å¦‚æœ‰æ—§ä»£ç çš„å…¼å®¹æ€§éœ€è¦ï¼Œå¯è®¿é—®&lt;a href="https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb"&gt;ğŸ”—æ—§ä»“åº“å†…å®¹ğŸ”—&lt;/a&gt;ã€‚&lt;/li&gt; 
  &lt;li&gt;å…å»æ•°æ®é¢„å¤„ç†æ­¥éª¤ã€‚ç»Ÿä¸€æ•°æ®é›†æ ¼å¼ï¼Œæ›´æ¢ä¸º&lt;code&gt;jsonl&lt;/code&gt;æ ¼å¼æœç»æ•°æ®é›†ä¸‹è½½æ··ä¹±çš„é—®é¢˜ã€‚&lt;/li&gt; 
  &lt;li&gt;MiniMind2ç³»åˆ—æ•ˆæœç›¸æ¯”MiniMind-V1æ˜¾è‘—æå‡ã€‚&lt;/li&gt; 
  &lt;li&gt;å°é—®é¢˜ï¼š{kv-cacheå†™æ³•æ›´æ ‡å‡†ã€MoEçš„è´Ÿè½½å‡è¡¡lossè¢«è€ƒè™‘ç­‰ç­‰}&lt;/li&gt; 
  &lt;li&gt;æä¾›æ¨¡å‹è¿ç§»åˆ°ç§æœ‰æ•°æ®é›†çš„è®­ç»ƒæ–¹æ¡ˆï¼ˆåŒ»ç–—æ¨¡å‹ã€è‡ªæˆ‘è®¤çŸ¥æ ·ä¾‹ï¼‰ã€‚&lt;/li&gt; 
  &lt;li&gt;ç²¾ç®€é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å¤§å¹…æå‡é¢„è®­ç»ƒæ•°æ®è´¨é‡ï¼Œå¤§å¹…ç¼©çŸ­ä¸ªäººå¿«é€Ÿè®­ç»ƒæ‰€éœ€æ—¶é—´ï¼Œå•å¡3090å³å¯2å°æ—¶å¤ç°ï¼&lt;/li&gt; 
  &lt;li&gt;æ›´æ–°ï¼šLoRAå¾®è°ƒè„±ç¦»peftåŒ…è£…ï¼Œä»0å®ç°LoRAè¿‡ç¨‹ï¼›DPOç®—æ³•ä»0ä½¿ç”¨PyTorchåŸç”Ÿå®ç°ï¼›æ¨¡å‹ç™½ç›’è’¸é¦åŸç”Ÿå®ç°ã€‚&lt;/li&gt; 
  &lt;li&gt;MiniMind2-DeepSeek-R1ç³»åˆ—è’¸é¦æ¨¡å‹è¯ç”Ÿï¼&lt;/li&gt; 
  &lt;li&gt;MiniMind2å…·å¤‡ä¸€å®šçš„è‹±æ–‡èƒ½åŠ›ï¼&lt;/li&gt; 
  &lt;li&gt;æ›´æ–°MiniMind2ä¸ç¬¬ä¸‰æ–¹æ¨¡å‹çš„åŸºäºæ›´å¤šå¤§æ¨¡å‹æ¦œå•æµ‹è¯•æ€§èƒ½çš„ç»“æœã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;More...&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;2024-10-05&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ä¸ºMiniMindæ‹“å±•äº†å¤šæ¨¡æ€èƒ½åŠ›ä¹‹---è§†è§‰&lt;/li&gt; 
  &lt;li&gt;ç§»æ­¥å­ªç”Ÿé¡¹ç›®&lt;a href="https://github.com/jingyaogong/minimind-v"&gt;minimind-v&lt;/a&gt;æŸ¥çœ‹è¯¦æƒ…ï¼&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;2024-09-27&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;09-27æ›´æ–°pretrainæ•°æ®é›†çš„é¢„å¤„ç†æ–¹å¼ï¼Œä¸ºäº†ä¿è¯æ–‡æœ¬å®Œæ•´æ€§ï¼Œæ”¾å¼ƒé¢„å¤„ç†æˆ.binè®­ç»ƒçš„å½¢å¼ï¼ˆè½»å¾®ç‰ºç‰²è®­ç»ƒé€Ÿåº¦ï¼‰ã€‚&lt;/li&gt; 
  &lt;li&gt;ç›®å‰pretrainé¢„å¤„ç†åçš„æ–‡ä»¶å‘½åä¸ºï¼špretrain_data.csvã€‚&lt;/li&gt; 
  &lt;li&gt;åˆ é™¤äº†ä¸€äº›å†—ä½™çš„ä»£ç ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;2024-09-17&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ›´æ–°minimind-v1-moeæ¨¡å‹&lt;/li&gt; 
  &lt;li&gt;ä¸ºäº†é˜²æ­¢æ­§ä¹‰ï¼Œä¸å†ä½¿ç”¨mistral_tokenizeråˆ†è¯ï¼Œå…¨éƒ¨é‡‡ç”¨è‡ªå®šä¹‰çš„minimind_tokenizerä½œä¸ºåˆ†è¯å™¨ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;2024-09-01&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ›´æ–°minimind-v1 (108M)æ¨¡å‹ï¼Œé‡‡ç”¨minimind_tokenizerï¼Œé¢„è®­ç»ƒè½®æ¬¡3 + SFTè½®æ¬¡10ï¼Œæ›´å……åˆ†è®­ç»ƒï¼Œæ€§èƒ½æ›´å¼ºã€‚&lt;/li&gt; 
  &lt;li&gt;é¡¹ç›®å·²éƒ¨ç½²è‡³ModelScopeåˆ›ç©ºé—´ï¼Œå¯ä»¥åœ¨æ­¤ç½‘ç«™ä¸Šä½“éªŒï¼š&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/minimind"&gt;ğŸ”—ModelScopeåœ¨çº¿ä½“éªŒğŸ”—&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;2024-08-27&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;é¡¹ç›®é¦–æ¬¡å¼€æº&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;ğŸ“Œ å¿«é€Ÿå¼€å§‹&lt;/h1&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;åˆ†äº«æœ¬äººçš„è½¯ç¡¬ä»¶é…ç½®ï¼ˆä»…ä¾›å‚è€ƒï¼‰&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz&lt;/li&gt; 
  &lt;li&gt;RAM: 128 GB&lt;/li&gt; 
  &lt;li&gt;GPU: NVIDIA GeForce RTX 3090(24GB) * 8&lt;/li&gt; 
  &lt;li&gt;Ubuntu==20.04&lt;/li&gt; 
  &lt;li&gt;CUDA==12.2&lt;/li&gt; 
  &lt;li&gt;Python==3.10.16&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ç¬¬0æ­¥&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/jingyaogong/minimind.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…  æµ‹è¯•å·²æœ‰æ¨¡å‹æ•ˆæœ&lt;/h2&gt; 
&lt;h3&gt;1.ç¯å¢ƒå‡†å¤‡&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.ä¸‹è½½æ¨¡å‹&lt;/h3&gt; 
&lt;p&gt;åˆ°é¡¹ç›®æ ¹ç›®å½•&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://huggingface.co/jingyaogong/MiniMind2 # or https://www.modelscope.cn/models/gongjy/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ï¼ˆå¯é€‰ï¼‰å‘½ä»¤è¡Œé—®ç­”&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ä½¿ç”¨transformersæ ¼å¼æ¨¡å‹
python eval_llm.py --load_from ./MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ï¼ˆå¯é€‰ï¼‰å¯åŠ¨WebUI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å¯èƒ½éœ€è¦`python&amp;gt;=3.10` å®‰è£… `pip install streamlit`
# cd scripts
streamlit run web_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ï¼ˆå¯é€‰ï¼‰ç¬¬ä¸‰æ–¹æ¨ç†æ¡†æ¶&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name "minimind"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…¡ ä»0å¼€å§‹è‡ªå·±è®­ç»ƒ&lt;/h2&gt; 
&lt;h3&gt;1.ç¯å¢ƒå‡†å¤‡&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šæå‰æµ‹è¯•Torchæ˜¯å¦å¯ç”¨cuda&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;import torch
print(torch.cuda.is_available())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;å¦‚æœä¸å¯ç”¨ï¼Œè¯·è‡ªè¡Œå»&lt;a href="https://download.pytorch.org/whl/torch_stable.html"&gt;torch_stable&lt;/a&gt; ä¸‹è½½whlæ–‡ä»¶å®‰è£…ã€‚å‚è€ƒ&lt;a href="https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;amp;request_id=&amp;amp;biz_id=102&amp;amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;amp;spm=1018.2226.3001.4187"&gt;é“¾æ¥&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;2.æ•°æ®ä¸‹è½½&lt;/h3&gt; 
&lt;p&gt;ä»ä¸‹æ–‡æä¾›çš„&lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;æ•°æ®é›†ä¸‹è½½é“¾æ¥&lt;/a&gt; ä¸‹è½½éœ€è¦çš„æ•°æ®æ–‡ä»¶ï¼ˆåˆ›å»º&lt;code&gt;./dataset&lt;/code&gt;ç›®å½•ï¼‰å¹¶æ”¾åˆ°&lt;code&gt;./dataset&lt;/code&gt;ä¸‹&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šæ•°æ®é›†é¡»çŸ¥&lt;/summary&gt; 
 &lt;p&gt;é»˜è®¤æ¨èä¸‹è½½&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;æœ€å¿«é€Ÿåº¦å¤ç°ZeroèŠå¤©æ¨¡å‹ã€‚&lt;/p&gt; 
 &lt;p&gt;æ•°æ®æ–‡ä»¶å¯è‡ªç”±é€‰æ‹©ï¼Œä¸‹æ–‡æä¾›äº†å¤šç§æ­é…æ–¹æ¡ˆï¼Œå¯æ ¹æ®è‡ªå·±æ‰‹å¤´çš„è®­ç»ƒéœ€æ±‚å’ŒGPUèµ„æºè¿›è¡Œé€‚å½“ç»„åˆã€‚&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;3.å¼€å§‹è®­ç»ƒ&lt;/h3&gt; 
&lt;p&gt;ç›®å½•ä½äº&lt;code&gt;trainer&lt;/code&gt;&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;ğŸ’¡ æ£€æŸ¥ç‚¹æš‚åœç»­è®­&lt;/summary&gt; 
 &lt;p&gt;æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œåªéœ€æ·»åŠ  &lt;code&gt;--from_resume 1&lt;/code&gt; å‚æ•°å³å¯è‡ªåŠ¨æ£€æµ‹åŠ è½½&amp;amp;æ¢å¤è®­ç»ƒï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain.py --from_resume 1
python train_full_sft.py --from_resume 1
...
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;æ–­ç‚¹ç»­è®­æœºåˆ¶è¯´æ˜ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;è®­ç»ƒè¿‡ç¨‹è‡ªåŠ¨åœ¨ &lt;code&gt;./checkpoints/&lt;/code&gt; ç›®å½•ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹ï¼ˆæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è®­ç»ƒè¿›åº¦ç­‰ï¼‰&lt;/li&gt; 
  &lt;li&gt;æ£€æŸ¥ç‚¹æ–‡ä»¶å‘½åï¼š&lt;code&gt;&amp;lt;æƒé‡å&amp;gt;_&amp;lt;ç»´åº¦&amp;gt;_resume.pth&lt;/code&gt;ï¼ˆå¦‚ï¼š&lt;code&gt;full_sft_512_resume.pth&lt;/code&gt;ï¼‰&lt;/li&gt; 
  &lt;li&gt;æ”¯æŒè·¨ä¸åŒGPUæ•°é‡æ¢å¤ï¼ˆè‡ªåŠ¨è°ƒæ•´stepï¼‰&lt;/li&gt; 
  &lt;li&gt;æ”¯æŒwandbè®­ç»ƒè®°å½•è¿ç»­æ€§ï¼ˆè‡ªåŠ¨æ¢å¤åŒä¸€ä¸ªrunï¼‰&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;é€‚åˆé•¿æ—¶é—´è®­ç»ƒæˆ–ä¸ç¨³å®šç¯å¢ƒï¼Œæ— éœ€æ‹…å¿ƒè®­ç»ƒä¸­æ–­å¯¼è‡´è¿›åº¦ä¸¢å¤±&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;3.1 é¢„è®­ç»ƒï¼ˆå­¦çŸ¥è¯†ï¼‰&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ‰§è¡Œé¢„è®­ç»ƒï¼Œå¾—åˆ° &lt;code&gt;pretrain_*.pth&lt;/code&gt; ä½œä¸ºé¢„è®­ç»ƒçš„è¾“å‡ºæƒé‡ï¼ˆå…¶ä¸­*ä¸ºæ¨¡å‹çš„dimensionï¼Œé»˜è®¤ä¸º512ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;3.2 ç›‘ç£å¾®è°ƒï¼ˆå­¦å¯¹è¯æ–¹å¼ï¼‰&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ‰§è¡Œç›‘ç£å¾®è°ƒï¼Œå¾—åˆ° &lt;code&gt;full_sft_*.pth&lt;/code&gt; ä½œä¸ºæŒ‡ä»¤å¾®è°ƒçš„è¾“å‡ºæƒé‡ï¼ˆå…¶ä¸­&lt;code&gt;full&lt;/code&gt;å³ä¸ºå…¨å‚æ•°å¾®è°ƒï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šè®­ç»ƒé¡»çŸ¥&lt;/summary&gt; 
 &lt;p&gt;æ‰€æœ‰è®­ç»ƒè¿‡ç¨‹é»˜è®¤æ¯éš”100æ­¥ä¿å­˜1æ¬¡å‚æ•°åˆ°æ–‡ä»¶&lt;code&gt;./out/***.pth&lt;/code&gt;ï¼ˆæ¯æ¬¡ä¼šè¦†ç›–æ‰æ—§æƒé‡æ–‡ä»¶ï¼‰ã€‚&lt;/p&gt; 
 &lt;p&gt;ç®€å•èµ·è§ï¼Œæ­¤å¤„åªå†™æ˜ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚å¦‚éœ€å…¶å®ƒè®­ç»ƒ (LoRA, è’¸é¦, å¼ºåŒ–å­¦ä¹ , å¾®è°ƒæ¨ç†ç­‰) å¯å‚è€ƒä¸‹æ–‡ã€å®éªŒã€‘å°èŠ‚çš„è¯¦ç»†è¯´æ˜ã€‚&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;4.æµ‹è¯•è‡ªå·±è®­ç»ƒçš„æ¨¡å‹æ•ˆæœ&lt;/h3&gt; 
&lt;p&gt;ç¡®ä¿éœ€è¦æµ‹è¯•çš„æ¨¡å‹&lt;code&gt;*.pth&lt;/code&gt;æ–‡ä»¶ä½äº&lt;code&gt;./out/&lt;/code&gt;ç›®å½•ä¸‹ã€‚ ä¹Ÿå¯ä»¥ç›´æ¥å»&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files"&gt;æ­¤å¤„&lt;/a&gt;ä¸‹è½½ä½¿ç”¨æˆ‘è®­ç»ƒçš„&lt;code&gt;*.pth&lt;/code&gt;æ–‡ä»¶ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_llm.py --weight full_sft # æˆ– pretrain/dpo/ppo/grpo...
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šæµ‹è¯•é¡»çŸ¥&lt;/summary&gt; 
 &lt;p&gt;&lt;code&gt;--weight&lt;/code&gt; å‚æ•°æŒ‡å®šæƒé‡åç§°å‰ç¼€ï¼Œå¯é€‰ï¼š&lt;code&gt;pretrain&lt;/code&gt;, &lt;code&gt;full_sft&lt;/code&gt;, &lt;code&gt;dpo&lt;/code&gt;, &lt;code&gt;reason&lt;/code&gt;, &lt;code&gt;ppo_actor&lt;/code&gt;, &lt;code&gt;grpo&lt;/code&gt;, &lt;code&gt;spo&lt;/code&gt; ç­‰&lt;/p&gt; 
 &lt;p&gt;å…¶ä»–å¸¸ç”¨å‚æ•°ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;--load_from&lt;/code&gt;: æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆ&lt;code&gt;model&lt;/code&gt;=åŸç”Ÿtorchæƒé‡ï¼Œå…¶ä»–è·¯å¾„=transformersæ ¼å¼ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--save_dir&lt;/code&gt;: æ¨¡å‹æƒé‡ç›®å½•ï¼ˆé»˜è®¤&lt;code&gt;out&lt;/code&gt;ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--lora_weight&lt;/code&gt;: LoRAæƒé‡åç§°ï¼ˆ&lt;code&gt;None&lt;/code&gt;è¡¨ç¤ºä¸ä½¿ç”¨ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--historys&lt;/code&gt;: æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--max_new_tokens&lt;/code&gt;: æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆé»˜è®¤8192ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--temperature&lt;/code&gt;: ç”Ÿæˆæ¸©åº¦ï¼ˆé»˜è®¤0.85ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--top_p&lt;/code&gt;: nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆé»˜è®¤0.85ï¼‰&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ä½¿ç”¨æ–¹å¼ç›´æ¥æŸ¥çœ‹&lt;code&gt;eval_llm.py&lt;/code&gt;ä»£ç å³å¯ã€‚&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ä¸ºPytorchåŸç”Ÿæ¡†æ¶ï¼Œå‡æ”¯æŒå¤šå¡åŠ é€Ÿï¼Œå‡è®¾ä½ çš„è®¾å¤‡æœ‰N (Nï¼1) å¼ æ˜¾å¡ï¼š&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;å•æœºNå¡å¯åŠ¨è®­ç»ƒæ–¹å¼ (DDP, æ”¯æŒå¤šæœºå¤šå¡é›†ç¾¤)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šå…¶å®ƒé¡»çŸ¥&lt;/summary&gt; 
 &lt;del&gt; å•æœºNå¡å¯åŠ¨è®­ç»ƒ (DeepSpeed) &lt;pre&gt;&lt;code class="language-bash"&gt;deepspeed --master_port 29500 --num_gpus=N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; &lt;/del&gt; 
 &lt;p&gt;å¯æ ¹æ®éœ€è¦å¼€å¯wandbè®°å½•è®­ç»ƒè¿‡ç¨‹ï¼ˆéœ€å¯ç›´è¿ï¼‰&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# éœ€è¦ç™»å½•: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;é€šè¿‡æ·»åŠ &lt;code&gt;--use_wandb&lt;/code&gt;å‚æ•°ï¼Œå¯ä»¥è®°å½•è®­ç»ƒè¿‡ç¨‹ï¼Œè®­ç»ƒå®Œæˆåï¼Œå¯ä»¥åœ¨wandbç½‘ç«™ä¸ŠæŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡ä¿®æ”¹&lt;code&gt;wandb_project&lt;/code&gt; å’Œ&lt;code&gt;wandb_run_name&lt;/code&gt;å‚æ•°ï¼Œå¯ä»¥æŒ‡å®šé¡¹ç›®åç§°å’Œè¿è¡Œåç§°ã€‚&lt;/p&gt; 
 &lt;p&gt;ã€æ³¨ã€‘ï¼š25å¹´6æœˆåï¼Œå›½å†…ç½‘ç»œç¯å¢ƒæ— æ³•ç›´è¿WandBï¼ŒMiniMindé¡¹ç›®é»˜è®¤è½¬ä¸ºä½¿ç”¨&lt;a href="https://swanlab.cn/"&gt;SwanLab&lt;/a&gt;ä½œä¸ºè®­ç»ƒå¯è§†åŒ–å·¥å…·ï¼ˆå®Œå…¨å…¼å®¹WandB APIï¼‰ï¼Œå³&lt;code&gt;import wandb&lt;/code&gt;æ”¹ä¸º&lt;code&gt;import swanlab as wandb&lt;/code&gt;å³å¯ï¼Œå…¶ä»–å‡æ— éœ€æ”¹åŠ¨ã€‚&lt;/p&gt; 
&lt;/details&gt; 
&lt;h1&gt;ğŸ“Œ æ•°æ®ä»‹ç»&lt;/h1&gt; 
&lt;h2&gt;â…  Tokenizer&lt;/h2&gt; 
&lt;p&gt;åˆ†è¯å™¨å°†å•è¯ä»è‡ªç„¶è¯­è¨€é€šè¿‡â€œè¯å…¸â€æ˜ å°„åˆ°&lt;code&gt;0, 1, 36&lt;/code&gt;è¿™æ ·çš„æ•°å­—ï¼Œå¯ä»¥ç†è§£ä¸ºæ•°å­—å°±ä»£è¡¨äº†å•è¯åœ¨â€œè¯å…¸â€ä¸­çš„é¡µç ã€‚ å¯ä»¥é€‰æ‹©è‡ªå·±æ„é€ è¯è¡¨è®­ç»ƒä¸€ä¸ªâ€œè¯å…¸â€ï¼Œä»£ç å¯è§&lt;code&gt;./scripts/train_tokenizer.py&lt;/code&gt;ï¼ˆä»…ä¾›å­¦ä¹ å‚è€ƒï¼Œè‹¥éå¿…è¦æ— éœ€å†è‡ªè¡Œè®­ç»ƒï¼ŒMiniMindå·²è‡ªå¸¦tokenizerï¼‰ã€‚ æˆ–è€…é€‰æ‹©æ¯”è¾ƒå‡ºåçš„å¼€æºå¤§æ¨¡å‹åˆ†è¯å™¨ï¼Œ æ­£å¦‚åŒç›´æ¥ç”¨æ–°å/ç‰›æ´¥è¯å…¸çš„ä¼˜ç‚¹æ˜¯tokenç¼–ç å‹ç¼©ç‡å¾ˆå¥½ï¼Œç¼ºç‚¹æ˜¯é¡µæ•°å¤ªå¤šï¼ŒåŠ¨è¾„æ•°åä¸‡ä¸ªè¯æ±‡çŸ­è¯­ï¼› è‡ªå·±è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œä¼˜ç‚¹æ˜¯è¯è¡¨é•¿åº¦å’Œå†…å®¹éšæ„æ§åˆ¶ï¼Œç¼ºç‚¹æ˜¯å‹ç¼©ç‡å¾ˆä½ï¼ˆä¾‹å¦‚"hello"ä¹Ÿè®¸ä¼šè¢«æ‹†åˆ†ä¸º"h e l l o" äº”ä¸ªç‹¬ç«‹çš„tokenï¼‰ï¼Œä¸”ç”Ÿåƒ»è¯éš¾ä»¥è¦†ç›–ã€‚ â€œè¯å…¸â€çš„é€‰æ‹©å›ºç„¶å¾ˆé‡è¦ï¼ŒLLMçš„è¾“å‡ºæœ¬è´¨ä¸Šæ˜¯SoftMaxåˆ°è¯å…¸Nä¸ªè¯çš„å¤šåˆ†ç±»é—®é¢˜ï¼Œç„¶åé€šè¿‡â€œè¯å…¸â€è§£ç åˆ°è‡ªç„¶è¯­è¨€ã€‚ å› ä¸ºMiniMindä½“ç§¯éœ€è¦ä¸¥æ ¼æ§åˆ¶ï¼Œä¸ºäº†é¿å…æ¨¡å‹å¤´é‡è„šè½»ï¼ˆè¯åµŒå…¥embeddingå±‚å‚æ•°åœ¨LLMå æ¯”å¤ªé«˜ï¼‰ï¼Œæ‰€ä»¥è¯è¡¨é•¿åº¦çŸ­çŸ­ç›Šå–„ã€‚&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Tokenizerä»‹ç»&lt;/summary&gt; 
 &lt;p&gt;ç¬¬ä¸‰æ–¹å¼ºå¤§çš„å¼€æºæ¨¡å‹ä¾‹å¦‚Yiã€qwenã€chatglmã€mistralã€Llama3çš„tokenizerè¯è¡¨é•¿åº¦å¦‚ä¸‹ï¼š&lt;/p&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;th&gt;Tokenizeræ¨¡å‹&lt;/th&gt;
    &lt;th&gt;è¯è¡¨å¤§å°&lt;/th&gt;
    &lt;th&gt;æ¥æº&lt;/th&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;yi tokenizer&lt;/td&gt;
    &lt;td&gt;64,000&lt;/td&gt;
    &lt;td&gt;01ä¸‡ç‰©ï¼ˆä¸­å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;qwen2 tokenizer&lt;/td&gt;
    &lt;td&gt;151,643&lt;/td&gt;
    &lt;td&gt;é˜¿é‡Œäº‘ï¼ˆä¸­å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;glm tokenizer&lt;/td&gt;
    &lt;td&gt;151,329&lt;/td&gt;
    &lt;td&gt;æ™ºè°±AIï¼ˆä¸­å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;mistral tokenizer&lt;/td&gt;
    &lt;td&gt;32,000&lt;/td&gt;
    &lt;td&gt;Mistral AIï¼ˆæ³•å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;llama3 tokenizer&lt;/td&gt;
    &lt;td&gt;128,000&lt;/td&gt;
    &lt;td&gt;Metaï¼ˆç¾å›½ï¼‰&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;minimind tokenizer&lt;/td&gt;
    &lt;td&gt;6,400&lt;/td&gt;
    &lt;td&gt;è‡ªå®šä¹‰&lt;/td&gt;
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸ‘‰2024-09-17æ›´æ–°ï¼šä¸ºäº†é˜²æ­¢è¿‡å»çš„ç‰ˆæœ¬æ­§ä¹‰&amp;amp;æ§åˆ¶ä½“ç§¯ï¼Œminimindæ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨minimind_tokenizeråˆ†è¯ï¼ŒåºŸå¼ƒæ‰€æœ‰mistral_tokenizerç‰ˆæœ¬ã€‚&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code&gt;# ä¸€äº›è‡ªè¨€è‡ªè¯­
&amp;gt; å°½ç®¡minimind_tokenizeré•¿åº¦å¾ˆå°ï¼Œç¼–è§£ç æ•ˆç‡å¼±äºqwen2ã€glmç­‰ä¸­æ–‡å‹å¥½å‹åˆ†è¯å™¨ã€‚
&amp;gt; ä½†minimindæ¨¡å‹é€‰æ‹©äº†è‡ªå·±è®­ç»ƒçš„minimind_tokenizerä½œä¸ºåˆ†è¯å™¨ï¼Œä»¥ä¿æŒæ•´ä½“å‚æ•°è½»é‡ï¼Œé¿å…ç¼–ç å±‚å’Œè®¡ç®—å±‚å æ¯”å¤±è¡¡ï¼Œå¤´é‡è„šè½»ï¼Œå› ä¸ºminimindçš„è¯è¡¨å¤§å°åªæœ‰6400ã€‚
&amp;gt; ä¸”minimindåœ¨å®é™…æµ‹è¯•ä¸­æ²¡æœ‰å‡ºç°è¿‡ç”Ÿåƒ»è¯æ±‡è§£ç å¤±è´¥çš„æƒ…å†µï¼Œæ•ˆæœè‰¯å¥½ã€‚
&amp;gt; ç”±äºè‡ªå®šä¹‰è¯è¡¨å‹ç¼©é•¿åº¦åˆ°6400ï¼Œä½¿å¾—LLMæ€»å‚æ•°é‡æœ€ä½åªæœ‰25.8Mã€‚
&amp;gt; è®­ç»ƒæ•°æ®`pretrain_hq.jsonl`å‡æ¥è‡ªäº`åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†`ï¼Œè¿™éƒ¨åˆ†æ•°æ®ç›¸å¯¹æ¬¡è¦ï¼Œå¦‚éœ€è®­ç»ƒå¯ä»¥è‡ªç”±é€‰æ‹©ã€‚
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;â…¡ Pretrainæ•°æ®&lt;/h2&gt; 
&lt;p&gt;ç»å†äº†MiniMind-V1çš„ä½è´¨é‡é¢„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹èƒ¡è¨€ä¹±è¯­çš„æ•™è®­ï¼Œ&lt;code&gt;2025-02-05&lt;/code&gt; ä¹‹åå†³å®šä¸å†é‡‡ç”¨å¤§è§„æ¨¡æ— ç›‘ç£çš„æ•°æ®é›†åšé¢„è®­ç»ƒã€‚ è¿›è€Œå°è¯•æŠŠ&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†&lt;/a&gt;çš„ä¸­æ–‡éƒ¨åˆ†æå–å‡ºæ¥ï¼Œ æ¸…æ´—å‡ºå­—ç¬¦&lt;code&gt;&amp;lt;512&lt;/code&gt;é•¿åº¦çš„å¤§çº¦1.6GBçš„è¯­æ–™ç›´æ¥æ‹¼æ¥æˆé¢„è®­ç»ƒæ•°æ® &lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;ï¼Œhqå³ä¸ºhigh qualityï¼ˆå½“ç„¶ä¹Ÿè¿˜ä¸ç®—highï¼Œæå‡æ•°æ®è´¨é‡æ— æ­¢å°½ï¼‰ã€‚&lt;/p&gt; 
&lt;p&gt;æ–‡ä»¶&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; æ•°æ®æ ¼å¼ä¸º&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{"text": "å¦‚ä½•æ‰èƒ½æ‘†è„±æ‹–å»¶ç—‡ï¼Ÿ æ²»æ„ˆæ‹–å»¶ç—‡å¹¶ä¸å®¹æ˜“ï¼Œä½†ä»¥ä¸‹å»ºè®®å¯èƒ½æœ‰æ‰€å¸®åŠ©..."}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…¢ SFTæ•°æ®&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;åŒ æ•°å¤§æ¨¡å‹SFTæ•°æ®é›†&lt;/a&gt; â€œæ˜¯ä¸€ä¸ªå®Œæ•´ã€æ ¼å¼ç»Ÿä¸€ã€å®‰å…¨çš„å¤§æ¨¡å‹è®­ç»ƒå’Œç ”ç©¶èµ„æºã€‚ ä»ç½‘ç»œä¸Šçš„å…¬å¼€æ•°æ®æºæ”¶é›†å¹¶æ•´ç†äº†å¤§é‡å¼€æºæ•°æ®é›†ï¼Œå¯¹å…¶è¿›è¡Œäº†æ ¼å¼ç»Ÿä¸€ï¼Œæ•°æ®æ¸…æ´—ï¼Œ åŒ…å«10Mæ¡æ•°æ®çš„ä¸­æ–‡æ•°æ®é›†å’ŒåŒ…å«2Mæ¡æ•°æ®çš„è‹±æ–‡æ•°æ®é›†ã€‚â€ ä»¥ä¸Šæ˜¯å®˜æ–¹ä»‹ç»ï¼Œä¸‹è½½æ–‡ä»¶åçš„æ•°æ®æ€»é‡å¤§çº¦åœ¨4B tokensï¼Œè‚¯å®šæ˜¯é€‚åˆä½œä¸ºä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹çš„SFTæ•°æ®çš„ã€‚ ä½†æ˜¯å®˜æ–¹æä¾›çš„æ•°æ®æ ¼å¼å¾ˆä¹±ï¼Œå…¨éƒ¨ç”¨æ¥sftä»£ä»·å¤ªå¤§ã€‚ æˆ‘å°†æŠŠå®˜æ–¹æ•°æ®é›†è¿›è¡Œäº†äºŒæ¬¡æ¸…æ´—ï¼ŒæŠŠå«æœ‰ç¬¦å·æ±¡æŸ“å’Œå™ªå£°çš„æ¡ç›®å»é™¤ï¼›å¦å¤–ä¾ç„¶åªä¿ç•™äº†æ€»é•¿åº¦&lt;code&gt;&amp;lt;512&lt;/code&gt; çš„å†…å®¹ï¼Œæ­¤é˜¶æ®µå¸Œæœ›é€šè¿‡å¤§é‡å¯¹è¯è¡¥å……é¢„è®­ç»ƒé˜¶æ®µæ¬ ç¼ºçš„çŸ¥è¯†ã€‚ å¯¼å‡ºæ–‡ä»¶ä¸º&lt;code&gt;sft_512.jsonl&lt;/code&gt;(~7.5GB)ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/organization/Magpie-Align"&gt;Magpie-SFTæ•°æ®é›†&lt;/a&gt; æ”¶é›†äº†~1Mæ¡æ¥è‡ªQwen2/2.5çš„é«˜è´¨é‡å¯¹è¯ï¼Œæˆ‘å°†è¿™éƒ¨åˆ†æ•°æ®è¿›ä¸€æ­¥æ¸…æ´—ï¼ŒæŠŠæ€»é•¿åº¦&lt;code&gt;&amp;lt;2048&lt;/code&gt;çš„éƒ¨åˆ†å¯¼å‡ºä¸º&lt;code&gt;sft_2048.jsonl&lt;/code&gt;(~9GB)ã€‚ é•¿åº¦&lt;code&gt;&amp;lt;1024&lt;/code&gt;çš„éƒ¨åˆ†å¯¼å‡ºä¸º&lt;code&gt;sft_1024.jsonl&lt;/code&gt;(~5.5GB)ï¼Œç”¨å¤§æ¨¡å‹å¯¹è¯æ•°æ®ç›´æ¥è¿›è¡Œsftå°±å±äºâ€œé»‘ç›’è’¸é¦â€çš„èŒƒç•´ã€‚&lt;/p&gt; 
&lt;p&gt;è¿›ä¸€æ­¥æ¸…æ´—å‰ä¸¤æ­¥sftçš„æ•°æ®ï¼ˆåªä¿ç•™ä¸­æ–‡å­—ç¬¦å æ¯”é«˜çš„å†…å®¹ï¼‰ï¼Œç­›é€‰é•¿åº¦&lt;code&gt;&amp;lt;512&lt;/code&gt;çš„å¯¹è¯ï¼Œå¾—åˆ°&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;(~1.2GB)ã€‚&lt;/p&gt; 
&lt;p&gt;æ‰€æœ‰sftæ–‡ä»¶ &lt;code&gt;sft_X.jsonl&lt;/code&gt; æ•°æ®æ ¼å¼å‡ä¸º&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
    "conversations": [
        {"role": "user", "content": "ä½ å¥½"},
        {"role": "assistant", "content": "ä½ å¥½ï¼"},
        {"role": "user", "content": "å†è§"},
        {"role": "assistant", "content": "å†è§ï¼"}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…£ RLHFæ•°æ®&lt;/h2&gt; 
&lt;p&gt;æ¥è‡ª&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1"&gt;Magpie-DPOæ•°æ®é›†&lt;/a&gt; å¤§çº¦200kæ¡åå¥½æ•°æ®ï¼ˆå‡æ˜¯è‹±æ–‡ï¼‰ç”Ÿæˆè‡ªLlama3.1-70B/8Bï¼Œå¯ä»¥ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä¼˜åŒ–æ¨¡å‹å›å¤è´¨é‡ï¼Œä½¿å…¶æ›´åŠ ç¬¦åˆäººç±»åå¥½ã€‚ è¿™é‡Œå°†æ•°æ®æ€»é•¿åº¦&lt;code&gt;&amp;lt;3000&lt;/code&gt;çš„å†…å®¹é‡ç»„ä¸º&lt;code&gt;dpo.jsonl&lt;/code&gt;(~0.9GB)ï¼ŒåŒ…å«&lt;code&gt;chosen&lt;/code&gt;å’Œ&lt;code&gt;rejected&lt;/code&gt;ä¸¤ä¸ªå­—æ®µï¼Œ&lt;code&gt;chosen&lt;/code&gt; ä¸ºåå¥½çš„å›å¤ï¼Œ&lt;code&gt;rejected&lt;/code&gt;ä¸ºæ‹’ç»çš„å›å¤ã€‚&lt;/p&gt; 
&lt;p&gt;æ–‡ä»¶ &lt;code&gt;dpo.jsonl&lt;/code&gt; æ•°æ®æ ¼å¼ä¸º&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
  "chosen": [
    {"content": "Q", "role": "user"}, 
    {"content": "good answer", "role": "assistant"}
  ], 
  "rejected": [
    {"content": "Q", "role": "user"}, 
    {"content": "bad answer", "role": "assistant"}
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…¤ Reasonæ•°æ®é›†ï¼š&lt;/h2&gt; 
&lt;p&gt;ä¸å¾—ä¸è¯´2025å¹´2æœˆè°èƒ½ç«çš„è¿‡DeepSeek... ä¹Ÿæ¿€å‘äº†æˆ‘å¯¹RLå¼•å¯¼çš„æ¨ç†æ¨¡å‹çš„æµ“åšå…´è¶£ï¼Œç›®å‰å·²ç»ç”¨Qwen2.5å¤ç°äº†R1-Zeroã€‚ å¦‚æœæœ‰æ—¶é—´+æ•ˆæœworkï¼ˆä½†99%åŸºæ¨¡èƒ½åŠ›ä¸è¶³ï¼‰æˆ‘ä¼šåœ¨ä¹‹åæ›´æ–°MiniMindåŸºäºRLè®­ç»ƒçš„æ¨ç†æ¨¡å‹è€Œä¸æ˜¯è’¸é¦æ¨¡å‹ã€‚ æ—¶é—´æœ‰é™ï¼Œæœ€å¿«çš„ä½æˆæœ¬æ–¹æ¡ˆä¾ç„¶æ˜¯ç›´æ¥è’¸é¦ï¼ˆé»‘ç›’æ–¹å¼ï¼‰ã€‚ è€ä¸ä½R1å¤ªç«ï¼ŒçŸ­çŸ­å‡ å¤©å°±å·²ç»å­˜åœ¨ä¸€äº›R1çš„è’¸é¦æ•°æ®é›†&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B"&gt;R1-Llama-70B&lt;/a&gt;ã€&lt;a href="https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT"&gt;R1-Distill-SFT&lt;/a&gt;ã€ &lt;a href="https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH"&gt;Alpaca-Distill-R1&lt;/a&gt;ã€ &lt;a href="https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh"&gt;deepseek_r1_zh&lt;/a&gt;ç­‰ç­‰ï¼Œçº¯ä¸­æ–‡çš„æ•°æ®å¯èƒ½æ¯”è¾ƒå°‘ã€‚ æœ€ç»ˆæ•´åˆå®ƒä»¬ï¼Œå¯¼å‡ºæ–‡ä»¶ä¸º&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;ï¼Œæ•°æ®æ ¼å¼å’Œ&lt;code&gt;sft_X.jsonl&lt;/code&gt;ä¸€è‡´ã€‚&lt;/p&gt; 
&lt;h2&gt;â…¥ æ›´å¤šæ•°æ®é›†&lt;/h2&gt; 
&lt;p&gt;ç›®å‰å·²ç»æœ‰&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt; åœ¨æ”¶é›†å’Œæ¢³ç†ä¸­æ–‡LLMç›¸å…³çš„å¼€æºæ¨¡å‹ã€åº”ç”¨ã€æ•°æ®é›†åŠæ•™ç¨‹ç­‰èµ„æ–™ï¼Œå¹¶æŒç»­æ›´æ–°è¿™æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚å…¨é¢ä¸”ä¸“ä¸šï¼ŒRespectï¼&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â…§ MiniMindè®­ç»ƒæ•°æ®é›†&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 2025-02-05åï¼Œå¼€æºMiniMindæœ€ç»ˆè®­ç»ƒæ‰€ç”¨çš„æ‰€æœ‰æ•°æ®é›†ï¼Œå› æ­¤æ— éœ€å†è‡ªè¡Œé¢„å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé¿å…é‡å¤æ€§çš„æ•°æ®å¤„ç†å·¥ä½œã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MiniMindè®­ç»ƒæ•°æ®é›†ä¸‹è½½åœ°å€ï¼š &lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ— éœ€å…¨éƒ¨cloneï¼Œå¯å•ç‹¬ä¸‹è½½æ‰€éœ€çš„æ–‡ä»¶&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;å°†ä¸‹è½½çš„æ•°æ®é›†æ–‡ä»¶æ”¾åˆ°&lt;code&gt;./dataset/&lt;/code&gt;ç›®å½•ä¸‹ï¼ˆâœ¨ä¸ºæ¨èçš„å¿…é¡»é¡¹ï¼‰&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./dataset/
â”œâ”€â”€ dpo.jsonl (55MB, âœ¨)
â”œâ”€â”€ lora_identity.jsonl (22.8KB)
â”œâ”€â”€ lora_medical.jsonl (34MB)
â”œâ”€â”€ pretrain_hq.jsonl (1.6GB, âœ¨)
â”œâ”€â”€ r1_mix_1024.jsonl (340MB)
â”œâ”€â”€ rlaif-mini.jsonl (1MB)
â”œâ”€â”€ sft_1024.jsonl (5.6GB)
â”œâ”€â”€ sft_2048.jsonl (9GB)
â”œâ”€â”€ sft_512.jsonl (7.5GB)
â””â”€â”€ sft_mini_512.jsonl (1.2GB, âœ¨)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æ³¨ï¼šå„æ•°æ®é›†ç®€ä»‹&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;dpo.jsonl&lt;/code&gt;âœ¨ --RLHFé˜¶æ®µæ•°æ®é›†ï¼ˆå·²ç²¾ç®€ä¼˜åŒ–ï¼Œé€‚åˆå¿«é€Ÿè®­ç»ƒï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_identity.jsonl&lt;/code&gt; --è‡ªæˆ‘è®¤çŸ¥æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼šä½ æ˜¯è°ï¼Ÿæˆ‘æ˜¯minimind...ï¼‰ï¼Œæ¨èç”¨äºloraè®­ç»ƒï¼ˆäº¦å¯ç”¨äºå…¨å‚SFTï¼Œå‹¿è¢«åå­—å±€é™ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_medical.jsonl&lt;/code&gt; --åŒ»ç–—é—®ç­”æ•°æ®é›†ï¼Œæ¨èç”¨äºloraè®­ç»ƒï¼ˆäº¦å¯ç”¨äºå…¨å‚SFTï¼Œå‹¿è¢«åå­—å±€é™ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;âœ¨ --é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ•´åˆè‡ªåŒ æ•°ç§‘æŠ€&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt; --DeepSeek-R1-1.5Bè’¸é¦æ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º1024ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=1024ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;rlaif-mini.jsonl&lt;/code&gt; --RLAIFè®­ç»ƒæ•°æ®é›†ï¼Œä»SFTæ•°æ®é›†ä¸­éšæœºé‡‡æ ·1ä¸‡æ¡é«˜è´¨é‡å¯¹è¯ï¼Œç”¨äºPPO/GRPO/SPOç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•è®­ç»ƒ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_1024.jsonl&lt;/code&gt; --æ•´åˆè‡ªQwen2.5è’¸é¦æ•°æ®ï¼ˆæ˜¯sft_2048çš„å­é›†ï¼‰ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º1024ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=1024ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_2048.jsonl&lt;/code&gt; --æ•´åˆè‡ªQwen2.5è’¸é¦æ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º2048ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=2048ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_512.jsonl&lt;/code&gt; --æ•´åˆè‡ªåŒ æ•°ç§‘æŠ€SFTæ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º512ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=512ï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;âœ¨ --æç®€æ•´åˆè‡ªåŒ æ•°ç§‘æŠ€SFTæ•°æ®+Qwen2.5è’¸é¦æ•°æ®ï¼ˆç”¨äºå¿«é€Ÿè®­ç»ƒZeroæ¨¡å‹ï¼‰ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º512ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=512ï¼‰&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/dataset.jpg" alt="dataset" /&gt;&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;è¯´æ˜ &amp;amp; æ¨èè®­ç»ƒæ–¹æ¡ˆ&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;MiniMind2 Serieså‡ç»è¿‡å…±çº¦20GBè¯­æ–™è®­ç»ƒï¼Œå¤§çº¦4B tokensï¼Œå³å¯¹åº”ä¸Šé¢çš„æ•°æ®ç»„åˆè®­ç»ƒç»“æœï¼ˆå¼€é”€ï¼šğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Šï¼‰&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;æƒ³è¦æœ€å¿«é€Ÿåº¦ä»0å®ç°Zeroæ¨¡å‹ï¼Œæ¨èä½¿ç”¨&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt; çš„æ•°æ®ç»„åˆï¼Œå…·ä½“èŠ±é”€å’Œæ•ˆæœå¯æŸ¥çœ‹ä¸‹æ–‡è¡¨æ ¼ï¼ˆå¼€é”€ï¼šğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜Šï¼‰&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;æ¨èå…·å¤‡ä¸€å®šç®—åŠ›èµ„æºæˆ–æ›´åœ¨æ„æ•ˆæœçš„æœ‹å‹å¯ä»¥è€ƒè™‘å‰è€…å®Œæ•´å¤ç°MiniMind2ï¼›ä»…æœ‰å•å¡GPUæˆ–åœ¨ä¹çŸ­æ—¶é—´å¿«é€Ÿå¤ç°çš„æœ‹å‹å¼ºçƒˆæ¨èåè€…ï¼›&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;ã€æŠ˜ä¸­æ–¹æ¡ˆã€‘äº¦å¯é€‰æ‹©ä¾‹å¦‚&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;ã€&lt;code&gt;sft_1024.jsonl&lt;/code&gt;ä¸­ç­‰è§„æ¨¡æ•°æ®è¿›è¡Œè‡ªç”±ç»„åˆè®­ç»ƒï¼ˆå¼€é”€ï¼šğŸ’°ğŸ’°ğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Šï¼‰ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;ğŸ“Œ Model&lt;/h1&gt; 
&lt;h2&gt;Structure&lt;/h2&gt; 
&lt;p&gt;MiniMind-Denseï¼ˆå’Œ&lt;a href="https://ai.meta.com/blog/meta-llama-3-1/"&gt;Llama3.1&lt;/a&gt;ä¸€æ ·ï¼‰ä½¿ç”¨äº†Transformerçš„Decoder-Onlyç»“æ„ï¼Œè·ŸGPT-3çš„åŒºåˆ«åœ¨äºï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;é‡‡ç”¨äº†GPT-3çš„é¢„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯åœ¨æ¯ä¸ªTransformerå­å±‚çš„è¾“å…¥ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯åœ¨è¾“å‡ºä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨çš„æ˜¯RMSNormå½’ä¸€åŒ–å‡½æ•°ã€‚&lt;/li&gt; 
 &lt;li&gt;ç”¨SwiGLUæ¿€æ´»å‡½æ•°æ›¿ä»£äº†ReLUï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†æé«˜æ€§èƒ½ã€‚&lt;/li&gt; 
 &lt;li&gt;åƒGPT-Neoä¸€æ ·ï¼Œå»æ‰äº†ç»å¯¹ä½ç½®åµŒå…¥ï¼Œæ”¹ç”¨äº†æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ï¼Œè¿™æ ·åœ¨å¤„ç†è¶…å‡ºè®­ç»ƒé•¿åº¦çš„æ¨ç†æ—¶æ•ˆæœæ›´å¥½ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMind-MoEæ¨¡å‹ï¼Œå®ƒçš„ç»“æ„åŸºäºLlama3å’Œ&lt;a href="https://arxiv.org/pdf/2405.04434"&gt;Deepseek-V2/3&lt;/a&gt;ä¸­çš„MixFFNæ··åˆä¸“å®¶æ¨¡å—ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-V2åœ¨å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰æ–¹é¢ï¼Œé‡‡ç”¨äº†æ›´ç»†ç²’åº¦çš„ä¸“å®¶åˆ†å‰²å’Œå…±äº«çš„ä¸“å®¶éš”ç¦»æŠ€æœ¯ï¼Œä»¥æé«˜Expertsçš„æ•ˆæœã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMindçš„æ•´ä½“ç»“æ„ä¸€è‡´ï¼Œåªæ˜¯åœ¨RoPEè®¡ç®—ã€æ¨ç†å‡½æ•°å’ŒFFNå±‚çš„ä»£ç ä¸Šåšäº†ä¸€äº›å°è°ƒæ•´ã€‚ å…¶ç»“æ„å¦‚ä¸‹å›¾ï¼ˆé‡ç»˜ç‰ˆï¼‰ï¼š&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure.png" alt="structure" /&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure-moe.png" alt="structure-moe" /&gt;&lt;/p&gt; 
&lt;p&gt;ä¿®æ”¹æ¨¡å‹é…ç½®è§&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/model/model_minimind.py"&gt;./model/model_minimind.py&lt;/a&gt;ã€‚ å‚è€ƒæ¨¡å‹å‚æ•°ç‰ˆæœ¬è§ä¸‹è¡¨ï¼š&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;len_vocab&lt;/th&gt; 
   &lt;th&gt;rope_theta&lt;/th&gt; 
   &lt;th&gt;n_layers&lt;/th&gt; 
   &lt;th&gt;d_model&lt;/th&gt; 
   &lt;th&gt;kv_heads&lt;/th&gt; 
   &lt;th&gt;q_heads&lt;/th&gt; 
   &lt;th&gt;share+route&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
   &lt;td&gt;4Ã—26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1&lt;/td&gt; 
   &lt;td&gt;108M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Configuration&lt;/h2&gt; 
&lt;p&gt;ğŸ“‹å…³äºLLMçš„å‚æ•°é…ç½®ï¼Œæœ‰ä¸€ç¯‡å¾ˆæœ‰æ„æ€çš„è®ºæ–‡&lt;a href="https://arxiv.org/pdf/2402.14905"&gt;MobileLLM&lt;/a&gt;åšäº†è¯¦ç»†çš„ç ”ç©¶å’Œå®éªŒã€‚ Scaling Lawåœ¨å°æ¨¡å‹ä¸­æœ‰è‡ªå·±ç‹¬ç‰¹çš„è§„å¾‹ã€‚ å¼•èµ·Transformerå‚æ•°æˆè§„æ¨¡å˜åŒ–çš„å‚æ•°å‡ ä¹åªå–å†³äº&lt;code&gt;d_model&lt;/code&gt;å’Œ&lt;code&gt;n_layers&lt;/code&gt;ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;â†‘ + &lt;code&gt;n_layers&lt;/code&gt;â†“ -&amp;gt; çŸ®èƒ–å­&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;â†“ + &lt;code&gt;n_layers&lt;/code&gt;â†‘ -&amp;gt; ç˜¦é«˜ä¸ª&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;2020å¹´æå‡ºScaling Lawçš„è®ºæ–‡è®¤ä¸ºï¼Œè®­ç»ƒæ•°æ®é‡ã€å‚æ•°é‡ä»¥åŠè®­ç»ƒè¿­ä»£æ¬¡æ•°æ‰æ˜¯å†³å®šæ€§èƒ½çš„å…³é”®å› ç´ ï¼Œè€Œæ¨¡å‹æ¶æ„çš„å½±å“å‡ ä¹å¯ä»¥å¿½è§†ã€‚ ç„¶è€Œä¼¼ä¹è¿™ä¸ªå®šå¾‹å¯¹å°æ¨¡å‹å¹¶ä¸å®Œå…¨é€‚ç”¨ã€‚ MobileLLMæå‡ºæ¶æ„çš„æ·±åº¦æ¯”å®½åº¦æ›´é‡è¦ï¼Œã€Œæ·±è€Œçª„ã€çš„ã€Œç˜¦é•¿ã€æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°æ¯”ã€Œå®½è€Œæµ…ã€æ¨¡å‹æ›´å¤šçš„æŠ½è±¡æ¦‚å¿µã€‚ ä¾‹å¦‚å½“æ¨¡å‹å‚æ•°å›ºå®šåœ¨125Mæˆ–è€…350Mæ—¶ï¼Œ30ï½42å±‚çš„ã€Œç‹­é•¿ã€æ¨¡å‹æ˜æ˜¾æ¯”12å±‚å·¦å³çš„ã€ŒçŸ®èƒ–ã€æ¨¡å‹æœ‰æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œ åœ¨å¸¸è¯†æ¨ç†ã€é—®ç­”ã€é˜…è¯»ç†è§£ç­‰8ä¸ªåŸºå‡†æµ‹è¯•ä¸Šéƒ½æœ‰ç±»ä¼¼çš„è¶‹åŠ¿ã€‚ è¿™å…¶å®æ˜¯éå¸¸æœ‰è¶£çš„å‘ç°ï¼Œå› ä¸ºä»¥å¾€ä¸º100Må·¦å³é‡çº§çš„å°æ¨¡å‹è®¾è®¡æ¶æ„æ—¶ï¼Œå‡ ä¹æ²¡äººå°è¯•è¿‡å åŠ è¶…è¿‡12å±‚ã€‚ è¿™ä¸MiniMindåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å‚æ•°é‡åœ¨&lt;code&gt;d_model&lt;/code&gt;å’Œ&lt;code&gt;n_layers&lt;/code&gt;ä¹‹é—´è¿›è¡Œè°ƒæ•´å®éªŒè§‚å¯Ÿåˆ°çš„æ•ˆæœæ˜¯ä¸€è‡´çš„ã€‚ ç„¶è€Œã€Œæ·±è€Œçª„ã€çš„ã€Œçª„ã€ä¹Ÿæ˜¯æœ‰ç»´åº¦æé™çš„ï¼Œå½“d_model&amp;lt;512æ—¶ï¼Œè¯åµŒå…¥ç»´åº¦åå¡Œçš„åŠ£åŠ¿éå¸¸æ˜æ˜¾ï¼Œ å¢åŠ çš„layerså¹¶ä¸èƒ½å¼¥è¡¥è¯åµŒå…¥åœ¨å›ºå®šq_headå¸¦æ¥d_headä¸è¶³çš„åŠ£åŠ¿ã€‚ å½“d_model&amp;gt;1536æ—¶ï¼Œlayersçš„å¢åŠ ä¼¼ä¹æ¯”d_modelçš„ä¼˜å…ˆçº§æ›´é«˜ï¼Œæ›´èƒ½å¸¦æ¥å…·æœ‰"æ€§ä»·æ¯”"çš„å‚æ•°-&amp;gt;æ•ˆæœå¢ç›Šã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å› æ­¤MiniMindè®¾å®šsmallæ¨¡å‹dim=512ï¼Œn_layers=8æ¥è·å–çš„ã€Œæå°ä½“ç§¯&amp;lt;-&amp;gt;æ›´å¥½æ•ˆæœã€çš„å¹³è¡¡ã€‚&lt;/li&gt; 
 &lt;li&gt;è®¾å®šdim=768ï¼Œn_layers=16æ¥è·å–æ•ˆæœçš„æ›´å¤§æ”¶ç›Šï¼Œæ›´åŠ ç¬¦åˆå°æ¨¡å‹Scaling-Lawçš„å˜åŒ–æ›²çº¿ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ä½œä¸ºå‚è€ƒï¼ŒGPT3çš„å‚æ•°è®¾å®šè§ä¸‹è¡¨ï¼š &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/gpt3_config.png" alt="gpt3_config.png" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸ“Œ Experiment&lt;/h1&gt; 
&lt;h2&gt;â…  è®­ç»ƒå¼€é”€&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ—¶é—´å•ä½&lt;/strong&gt;ï¼šå°æ—¶ (h)ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æˆæœ¬å•ä½&lt;/strong&gt;ï¼šäººæ°‘å¸ (ï¿¥)ï¼›7ï¿¥ â‰ˆ 1ç¾å…ƒã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3090 ç§Ÿå¡å•ä»·&lt;/strong&gt;ï¼šâ‰ˆ1.3ï¿¥/hï¼ˆå¯è‡ªè¡Œå‚è€ƒå®æ—¶å¸‚ä»·ï¼‰ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å‚è€ƒæ ‡å‡†&lt;/strong&gt;ï¼šè¡¨æ ¼ä»…å®æµ‹ &lt;code&gt;pretrain&lt;/code&gt; å’Œ &lt;code&gt;sft_mini_512&lt;/code&gt; ä¸¤ä¸ªæ•°æ®é›†çš„è®­ç»ƒæ—¶é—´ï¼Œå…¶å®ƒè€—æ—¶æ ¹æ®æ•°æ®é›†å¤§å°ä¼°ç®—ï¼ˆå¯èƒ½å­˜åœ¨äº›è®¸å‡ºå…¥ï¼‰ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;åŸºäº 3090 ï¼ˆå•å¡ï¼‰æˆæœ¬è®¡ç®—&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;pretrain&lt;/th&gt; 
   &lt;th&gt;sft_mini_512&lt;/th&gt; 
   &lt;th&gt;sft_512&lt;/th&gt; 
   &lt;th&gt;sft_1024&lt;/th&gt; 
   &lt;th&gt;sft_2048&lt;/th&gt; 
   &lt;th&gt;RLHF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;â‰ˆ1.1h&lt;br /&gt;â‰ˆ1.43ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ1h&lt;br /&gt;â‰ˆ1.3ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ6h&lt;br /&gt;â‰ˆ7.8ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ4.58h&lt;br /&gt;â‰ˆ5.95ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ7.5h&lt;br /&gt;â‰ˆ9.75ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ1h&lt;br /&gt;â‰ˆ1.3ï¿¥&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;â‰ˆ3.9h&lt;br /&gt;â‰ˆ5.07ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ3.3h&lt;br /&gt;â‰ˆ4.29ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ20h&lt;br /&gt;â‰ˆ26ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ15h&lt;br /&gt;â‰ˆ19.5ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ25h&lt;br /&gt;â‰ˆ32.5ï¿¥&lt;/td&gt; 
   &lt;td&gt;â‰ˆ3h&lt;br /&gt;â‰ˆ3.9ï¿¥&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;è®­ç»ƒå¼€é”€æ€»ç»“&amp;amp;é¢„æµ‹&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Smallå‚æ•°&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_mini_512&lt;/code&gt;æ•°æ®é›† &lt;br /&gt;å•å¡3090 (1 epoch) + 2.1å°æ—¶ + èŠ±è´¹2.73å…ƒäººæ°‘å¸ &lt;br /&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind-Zero-0.025Bæ¨¡å‹!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Smallå‚æ•°&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;æ•°æ®é›† &lt;br /&gt;å•å¡3090 (2 epochs) + å¤§çº¦38.16å°æ—¶ + èŠ±è´¹49.61å…ƒäººæ°‘å¸ &lt;br /&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind2-Small-0.025Bæ¨¡å‹!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2å‚æ•°&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;æ•°æ®é›† &lt;br /&gt;å•å¡3090 (2 epochs) + å¤§çº¦122å°æ—¶ + èŠ±è´¹158.6å…ƒäººæ°‘å¸ &lt;br /&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind2-0.1Bæ¨¡å‹!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;âœ¨åŸºäºå•å¡NVIDIA 3090çš„&lt;code&gt;MiniMind-Zero&lt;/code&gt;ä»0è®­ç»ƒä»…éœ€&lt;code&gt;2å°æ—¶&lt;/code&gt; + &lt;code&gt;3å—é’±&lt;/code&gt;ï¼Œå®ç°ChatBotæ•ˆæœï¼&lt;/p&gt; 
&lt;p&gt;âœ¨PSï¼šè‹¥é‡‡ç”¨8å¡4090è®­ç»ƒï¼Œæ€»ç”¨æ—¶ç”šè‡³å¯ä»¥å‹ç¼©åˆ°10åˆ†é’Ÿä»¥å†…ï¼ï¼ˆç”±äºæ—¶é—´æ›´çŸ­ï¼ŒèŠ±è´¹åŒæ ·3å…ƒå·¦å³ï¼Œä¸å•å¡æˆæœ¬ç›¸å½“ï¼‰&lt;/p&gt; 
&lt;p&gt;âœ¨ä»¥æä½æä½çš„é—¨æ§›ï¼Œå®ç°äººäººå¯ç©çš„å¤§æ¨¡å‹è‡ªç”±ï¼è¿™æ­£æ˜¯MiniMindç³»åˆ—çš„è¯ç”Ÿåˆè¡·ï¼&lt;/p&gt; 
&lt;p&gt;âœ¨ä»…ä»·å€¼&lt;code&gt;3å—é’±&lt;/code&gt;æˆæœ¬çš„&lt;code&gt;MiniMind-Zero&lt;/code&gt;å¹¶ä¸æ˜¯å™±å¤´ï¼Chatæµ‹è¯•ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;ğŸ‘¶: è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚
ğŸ¤–ï¸: ä½œä¸ºäººå·¥æ™ºèƒ½ï¼Œæˆ‘æ²¡æœ‰å®é™…çš„ç”Ÿæ´»ï¼Œä¹Ÿæ²¡æœ‰è‡ªæˆ‘æ„è¯†ï¼Œæ‰€ä»¥æ²¡æœ‰è‡ªå·±çš„ç”Ÿæ´»ã€‚æˆ‘è¢«è®¾è®¡æˆèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€è¿›è¡Œå¯¹è¯ç­‰ã€‚æˆ‘çš„è®¾è®¡å’ŒåŠŸèƒ½æ˜¯ç”±è®¡ç®—æœºç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€äººå·¥æ™ºèƒ½æŠ€æœ¯å’Œç®—æ³•æ‰€æ„å»ºçš„ï¼Œæˆ‘é€šè¿‡ç¼–ç¨‹å’Œç®—æ³•å®ç°ã€‚

ğŸ‘¶: æ¨èä¸€äº›æ­å·çš„ç‰¹è‰²ç¾é£Ÿå§ã€‚
ğŸ¤–ï¸: æ­å·æœ‰å¾ˆå¤šç¾å‘³çš„ç‰¹è‰²ç¾é£Ÿï¼Œæ¯”å¦‚é¸¡è‚‰ä¸²ã€é³—é±¼å¤´ã€æ°´ç…®é±¼ã€è±†è…è„‘ç­‰ã€‚è¿™äº›ç¾é£Ÿéƒ½éå¸¸ç¾å‘³ï¼Œæœ‰ç€ç‹¬ç‰¹çš„é£å‘³å’Œå£æ„Ÿï¼Œç»å¯¹æ˜¯å€¼å¾—ä¸€è¯•çš„ç¾é£Ÿã€‚

ğŸ‘¶: è¯·ä¸ºæˆ‘è®²è§£â€œå¤§è¯­è¨€æ¨¡å‹â€è¿™ä¸ªæ¦‚å¿µã€‚
ğŸ¤–ï¸: å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨å¤§é‡çš„æ–‡æœ¬æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨ä¸æ–­çš„è¿­ä»£ä¸­ä¸æ–­ä¼˜åŒ–æ¨¡å‹ã€‚å®ƒçš„ç›®æ ‡æ˜¯æ ¹æ®è¾“å…¥çš„æ•°æ®æ¥ç”Ÿæˆç¬¦åˆç‰¹å®šéœ€æ±‚çš„æ–‡æœ¬ï¼Œè¿™äº›æ–‡æœ¬å¯ä»¥æ˜¯ä»»ä½•ä½ æ„Ÿå…´è¶£çš„ä¸»é¢˜ã€‚

ğŸ‘¶: Introduce the history of the United States, please.
ğŸ¤–ï¸: æ‚¨æåˆ°çš„â€œIntrook's the believeations of theument." è¿™ä¸ªåå­—æ¥æºäºä¸­å›½å¤ä»£çš„"groty of of the change."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æé€Ÿä¸”åˆå…·æ•ˆæœï¼Œç”šè‡³ä»ç„¶å¯ä»¥è¿›ä¸€æ­¥å‹ç¼©è·å–æ›´å°æ›´ä¼˜è´¨çš„è®­ç»ƒæ•°æ®ã€‚ Zeroæ¨¡å‹æƒé‡ä¿å­˜ä¸º &lt;code&gt;full_sft_512_zero.pth&lt;/code&gt;ï¼ˆè§ä¸‹æ–‡MiniMindæ¨¡å‹æ–‡ä»¶é“¾æ¥ï¼‰ï¼Œå¦‚æœ‰å…´è¶£å¯ä¸‹è½½æ£€éªŒæ­¤æ¨¡å‹æ•ˆæœã€‚&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â…¡ ä¸»è¦è®­ç»ƒï¼ˆå¿…é¡»ï¼‰&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ &lt;code&gt;cd ./trainer&lt;/code&gt; ç›®å½•æ‰§è¡Œ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;1. é¢„è®­ç»ƒ (Pretrain)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;LLMé¦–å…ˆè¦å­¦ä¹ çš„å¹¶éç›´æ¥ä¸äººäº¤æµï¼Œè€Œæ˜¯è®©ç½‘ç»œå‚æ•°ä¸­å……æ»¡çŸ¥è¯†çš„å¢¨æ°´ï¼Œâ€œå¢¨æ°´â€ ç†è®ºä¸Šå–çš„è¶Šé¥±è¶Šå¥½ï¼Œäº§ç”Ÿå¤§é‡çš„å¯¹ä¸–ç•Œçš„çŸ¥è¯†ç§¯ç´¯ã€‚ é¢„è®­ç»ƒå°±æ˜¯è®©Modelå…ˆåŸ‹å¤´è‹¦å­¦å¤§é‡åŸºæœ¬çš„çŸ¥è¯†ï¼Œä¾‹å¦‚ä»Wikiç™¾ç§‘ã€æ–°é—»ã€ä¹¦ç±æ•´ç†å¤§è§„æ¨¡çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚ è¿™ä¸ªè¿‡ç¨‹æ˜¯â€œæ— ç›‘ç£â€çš„ï¼Œå³äººç±»ä¸éœ€è¦åœ¨è¿‡ç¨‹ä¸­åšä»»ä½•â€œæœ‰ç›‘ç£â€çš„æ ¡æ­£ï¼Œè€Œæ˜¯ç”±æ¨¡å‹è‡ªå·±ä»å¤§é‡æ–‡æœ¬ä¸­æ€»ç»“è§„å¾‹å­¦ä¹ çŸ¥è¯†ç‚¹ã€‚ æ¨¡å‹æ­¤é˜¶æ®µç›®çš„åªæœ‰ä¸€ä¸ªï¼š&lt;strong&gt;å­¦ä¼šè¯è¯­æ¥é¾™&lt;/strong&gt;ã€‚ä¾‹å¦‚è¾“å…¥"ç§¦å§‹çš‡"å››ä¸ªå­—ï¼Œå®ƒå¯ä»¥æ¥é¾™"æ˜¯ä¸­å›½çš„ç¬¬ä¸€ä½çš‡å¸"ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_pretrain.py # 1å³ä¸ºå•å¡è®­ç»ƒï¼Œå¯æ ¹æ®ç¡¬ä»¶æƒ…å†µè‡ªè¡Œè°ƒæ•´ (è®¾ç½®&amp;gt;=2)
# or
python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;pretrain_*.pth&lt;/code&gt;ï¼ˆ* ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_512_loss.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_768_loss.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;2. æœ‰ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;ç»è¿‡é¢„è®­ç»ƒï¼ŒLLMæ­¤æ—¶å·²ç»æŒæ¡äº†å¤§é‡çŸ¥è¯†ï¼Œç„¶è€Œæ­¤æ—¶å®ƒåªä¼šæ— è„‘åœ°è¯è¯­æ¥é¾™ï¼Œè¿˜ä¸ä¼šä¸äººèŠå¤©ã€‚ SFTé˜¶æ®µå°±éœ€è¦æŠŠåŠæˆå“LLMæ–½åŠ ä¸€ä¸ªè‡ªå®šä¹‰çš„èŠå¤©æ¨¡æ¿è¿›è¡Œå¾®è°ƒã€‚ ä¾‹å¦‚æ¨¡å‹é‡åˆ°è¿™æ ·çš„æ¨¡æ¿ã€é—®é¢˜-&amp;gt;å›ç­”ï¼Œé—®é¢˜-&amp;gt;å›ç­”ã€‘åä¸å†æ— è„‘æ¥é¾™ï¼Œè€Œæ˜¯æ„è¯†åˆ°è¿™æ˜¯ä¸€æ®µå®Œæ•´çš„å¯¹è¯ç»“æŸã€‚ ç§°è¿™ä¸ªè¿‡ç¨‹ä¸ºæŒ‡ä»¤å¾®è°ƒï¼Œå°±å¦‚åŒè®©å·²ç»å­¦å¯Œäº”è½¦çš„ã€Œç‰›é¡¿ã€å…ˆç”Ÿé€‚åº”21ä¸–çºªæ™ºèƒ½æ‰‹æœºçš„èŠå¤©ä¹ æƒ¯ï¼Œå­¦ä¹ å±å¹•å·¦ä¾§æ˜¯å¯¹æ–¹æ¶ˆæ¯ï¼Œå³ä¾§æ˜¯æœ¬äººæ¶ˆæ¯è¿™ä¸ªè§„å¾‹ã€‚ åœ¨è®­ç»ƒæ—¶ï¼ŒMiniMindçš„æŒ‡ä»¤å’Œå›ç­”é•¿åº¦è¢«æˆªæ–­åœ¨512ï¼Œæ˜¯ä¸ºäº†èŠ‚çœæ˜¾å­˜ç©ºé—´ã€‚å°±åƒå­¦ä¹ å†™ä½œæ—¶ï¼Œä¼šå…ˆä»çŸ­çš„æ–‡ç« å¼€å§‹ï¼Œå½“å­¦ä¼šå†™ä½œ200å­—ä½œæ–‡åï¼Œ800å­—æ–‡ç« ä¹Ÿå¯ä»¥æ‰‹åˆ°æ“’æ¥ã€‚ åœ¨éœ€è¦é•¿åº¦æ‹“å±•æ—¶ï¼Œåªéœ€è¦å‡†å¤‡å°‘é‡çš„2k/4k/8ké•¿åº¦å¯¹è¯æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒå³å¯ï¼ˆæ­¤æ—¶æœ€å¥½é…åˆRoPE-NTKçš„åŸºå‡†å·®å€¼ï¼‰ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;åœ¨æ¨ç†æ—¶é€šè¿‡è°ƒæ•´RoPEçº¿æ€§å·®å€¼ï¼Œå®ç°å…è®­ç»ƒé•¿åº¦å¤–æ¨åˆ°2048åŠä»¥ä¸Šå°†ä¼šå¾ˆæ–¹ä¾¿ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;full_sft_*.pth&lt;/code&gt;ï¼ˆ* ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_512_loss.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_768_loss.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;â…¢ å…¶å®ƒè®­ç»ƒé˜¶æ®µï¼ˆå¯é€‰ï¼‰&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ &lt;code&gt;cd ./trainer&lt;/code&gt; ç›®å½•æ‰§è¡Œ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;3. çŸ¥è¯†è’¸é¦ (Knowledge Distillation, KD)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;åœ¨å‰é¢çš„æ‰€æœ‰è®­ç»ƒæ­¥éª¤ä¸­ï¼Œæ¨¡å‹å·²ç»å®Œå…¨å…·å¤‡äº†åŸºæœ¬èƒ½åŠ›ï¼Œé€šå¸¸å¯ä»¥å­¦æˆå‡ºå¸ˆäº†ã€‚ è€ŒçŸ¥è¯†è’¸é¦å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œæ‰€è°“çŸ¥è¯†è’¸é¦ï¼Œå³å­¦ç”Ÿæ¨¡å‹é¢å‘æ•™å¸ˆæ¨¡å‹å­¦ä¹ ã€‚ æ•™å¸ˆæ¨¡å‹é€šå¸¸æ˜¯ç»è¿‡å……åˆ†è®­ç»ƒçš„å¤§æ¨¡å‹ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ å­¦ç”Ÿæ¨¡å‹æ˜¯ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼Œç›®æ ‡æ˜¯å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„è¡Œä¸ºï¼Œè€Œä¸æ˜¯ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­å­¦ä¹ ã€‚ åœ¨SFTå­¦ä¹ ä¸­ï¼Œæ¨¡å‹çš„ç›®æ ‡æ˜¯æ‹Ÿåˆè¯Tokenåˆ†ç±»ç¡¬æ ‡ç­¾ï¼ˆhard labelsï¼‰ï¼Œå³çœŸå®çš„ç±»åˆ«æ ‡ç­¾ï¼ˆå¦‚ 0 æˆ– 6400ï¼‰ã€‚ åœ¨çŸ¥è¯†è’¸é¦ä¸­ï¼Œæ•™å¸ˆæ¨¡å‹çš„softmaxæ¦‚ç‡åˆ†å¸ƒè¢«ç”¨ä½œè½¯æ ‡ç­¾ï¼ˆsoft labelsï¼‰ã€‚å°æ¨¡å‹ä»…å­¦ä¹ è½¯æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨KL-Lossæ¥ä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ã€‚ é€šä¿—åœ°è¯´ï¼ŒSFTç›´æ¥å­¦ä¹ è€å¸ˆç»™çš„è§£é¢˜ç­”æ¡ˆã€‚è€ŒKDè¿‡ç¨‹ç›¸å½“äºâ€œæ‰“å¼€â€è€å¸ˆèªæ˜çš„å¤§è„‘ï¼Œå°½å¯èƒ½åœ°æ¨¡ä»¿è€å¸ˆâ€œå¤§è„‘â€æ€è€ƒé—®é¢˜çš„ç¥ç»å…ƒçŠ¶æ€ã€‚ ä¾‹å¦‚ï¼Œå½“è€å¸ˆæ¨¡å‹è®¡ç®—&lt;code&gt;1+1=2&lt;/code&gt;è¿™ä¸ªé—®é¢˜çš„æ—¶å€™ï¼Œæœ€åä¸€å±‚ç¥ç»å…ƒaçŠ¶æ€ä¸º0ï¼Œç¥ç»å…ƒbçŠ¶æ€ä¸º100ï¼Œç¥ç»å…ƒcçŠ¶æ€ä¸º-99... å­¦ç”Ÿæ¨¡å‹é€šè¿‡å¤§é‡æ•°æ®ï¼Œå­¦ä¹ æ•™å¸ˆæ¨¡å‹å¤§è„‘å†…éƒ¨çš„è¿è½¬è§„å¾‹ã€‚è¿™ä¸ªè¿‡ç¨‹å³ç§°ä¹‹ä¸ºï¼šçŸ¥è¯†è’¸é¦ã€‚ çŸ¥è¯†è’¸é¦çš„ç›®çš„åªæœ‰ä¸€ä¸ªï¼šè®©å°æ¨¡å‹ä½“ç§¯æ›´å°çš„åŒæ—¶æ•ˆæœæ›´å¥½ã€‚ ç„¶è€Œéšç€LLMè¯ç”Ÿå’Œå‘å±•ï¼Œæ¨¡å‹è’¸é¦ä¸€è¯è¢«å¹¿æ³›æ»¥ç”¨ï¼Œä»è€Œäº§ç”Ÿäº†â€œç™½ç›’/é»‘ç›’â€çŸ¥è¯†è’¸é¦ä¸¤ä¸ªæ´¾åˆ«ã€‚ GPT-4è¿™ç§é—­æºæ¨¡å‹ï¼Œç”±äºæ— æ³•è·å–å…¶å†…éƒ¨ç»“æ„ï¼Œå› æ­¤åªèƒ½é¢å‘å®ƒæ‰€è¾“å‡ºçš„æ•°æ®å­¦ä¹ ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¹‹ä¸ºé»‘ç›’è’¸é¦ï¼Œä¹Ÿæ˜¯å¤§æ¨¡å‹æ—¶ä»£æœ€æ™®éçš„åšæ³•ã€‚ é»‘ç›’è’¸é¦ä¸SFTè¿‡ç¨‹å®Œå…¨ä¸€è‡´ï¼Œåªä¸è¿‡æ•°æ®æ˜¯ä»å¤§æ¨¡å‹çš„è¾“å‡ºæ”¶é›†ï¼Œå› æ­¤åªéœ€è¦å‡†å¤‡æ•°æ®å¹¶ä¸”è¿›ä¸€æ­¥FTå³å¯ã€‚ æ³¨æ„æ›´æ”¹è¢«åŠ è½½çš„åŸºç¡€æ¨¡å‹ä¸º&lt;code&gt;full_sft_*.pth&lt;/code&gt;ï¼Œå³åŸºäºå¾®è°ƒæ¨¡å‹åšè¿›ä¸€æ­¥çš„è’¸é¦å­¦ä¹ ã€‚ &lt;code&gt;./dataset/sft_1024.jsonl&lt;/code&gt;ä¸&lt;code&gt;./dataset/sft_2048.jsonl&lt;/code&gt; å‡æ”¶é›†è‡ªqwen2.5-7/72B-Instructå¤§æ¨¡å‹ï¼Œå¯ç›´æ¥ç”¨äºSFTä»¥è·å–Qwençš„éƒ¨åˆ†è¡Œä¸ºã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æ³¨æ„éœ€è¦æ›´æ”¹train_full_sft.pyæ•°æ®é›†è·¯å¾„ï¼Œä»¥åŠmax_seq_len  
torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;åŒæ ·ä¿å­˜ä¸º: &lt;code&gt;full_sft_*.pth&lt;/code&gt;ï¼ˆ*ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;æ­¤å¤„åº”å½“ç€é‡ä»‹ç»MiniMindå®ç°çš„ç™½ç›’è’¸é¦ä»£ç &lt;code&gt;train_distillation.py&lt;/code&gt;ï¼Œç”±äºMiniMindåŒç³»åˆ—æœ¬èº«å¹¶ä¸å­˜åœ¨å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ï¼Œå› æ­¤ç™½ç›’è’¸é¦ä»£ç ä»…ä½œä¸ºå­¦ä¹ å‚è€ƒã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distillation.py
# or
python train_distillation.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;4. LoRA (Low-Rank Adaptation)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;LoRAæ˜¯ä¸€ç§é«˜æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-Efficient Fine-Tuning, PEFTï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä½ç§©åˆ†è§£çš„æ–¹å¼å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ ç›¸æ¯”äºå…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰ï¼ŒLoRA åªéœ€è¦æ›´æ–°å°‘é‡çš„å‚æ•°ã€‚ LoRA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåœ¨æ¨¡å‹çš„æƒé‡çŸ©é˜µä¸­å¼•å…¥ä½ç§©åˆ†è§£ï¼Œä»…å¯¹ä½ç§©éƒ¨åˆ†è¿›è¡Œæ›´æ–°ï¼Œè€Œä¿æŒåŸå§‹é¢„è®­ç»ƒæƒé‡ä¸å˜ã€‚ ä»£ç å¯è§&lt;code&gt;./model/model_lora.py&lt;/code&gt;å’Œ&lt;code&gt;train_lora.py&lt;/code&gt;ï¼Œå®Œå…¨ä»0å®ç°LoRAæµç¨‹ï¼Œä¸ä¾èµ–ç¬¬ä¸‰æ–¹åº“çš„å°è£…ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_lora.py
# or
python train_lora.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;lora_xxx_*.pth&lt;/code&gt;ï¼ˆ* ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;éå¸¸å¤šçš„äººå›°æƒ‘ï¼Œå¦‚ä½•ä½¿æ¨¡å‹å­¦ä¼šè‡ªå·±ç§æœ‰é¢†åŸŸçš„çŸ¥è¯†ï¼Ÿå¦‚ä½•å‡†å¤‡æ•°æ®é›†ï¼Ÿå¦‚ä½•è¿ç§»é€šç”¨é¢†åŸŸæ¨¡å‹æ‰“é€ å‚åŸŸæ¨¡å‹ï¼Ÿ è¿™é‡Œä¸¾å‡ ä¸ªä¾‹å­ï¼Œå¯¹äºé€šç”¨æ¨¡å‹ï¼ŒåŒ»å­¦é¢†åŸŸçŸ¥è¯†æ¬ ç¼ºï¼Œå¯ä»¥å°è¯•åœ¨åŸæœ‰æ¨¡å‹åŸºç¡€ä¸ŠåŠ å…¥é¢†åŸŸçŸ¥è¯†ï¼Œä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ åŒæ—¶ï¼Œé€šå¸¸ä¸å¸Œæœ›å­¦ä¼šé¢†åŸŸçŸ¥è¯†çš„åŒæ—¶æŸå¤±åŸæœ‰åŸºç¡€æ¨¡å‹çš„å…¶å®ƒèƒ½åŠ›ï¼Œæ­¤æ—¶LoRAå¯ä»¥å¾ˆå¥½çš„æ”¹å–„è¿™ä¸ªé—®é¢˜ã€‚ åªéœ€è¦å‡†å¤‡å¦‚ä¸‹æ ¼å¼çš„å¯¹è¯æ•°æ®é›†æ”¾ç½®åˆ°&lt;code&gt;./dataset/lora_xxx.jsonl&lt;/code&gt;ï¼Œå¯åŠ¨ &lt;code&gt;python train_lora.py&lt;/code&gt; è®­ç»ƒå³å¯å¾—åˆ°&lt;code&gt;./out/lora/lora_xxx.pth&lt;/code&gt;æ–°æ¨¡å‹æƒé‡ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;åŒ»ç–—åœºæ™¯&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "è¯·é—®é¢ˆæ¤ç—…çš„äººæ•å¤´å¤šé«˜æ‰æœ€å¥½ï¼Ÿ"}, {"role": "assistant", "content": "é¢ˆæ¤ç—…æ‚£è€…é€‰æ‹©æ•å¤´çš„é«˜åº¦åº”è¯¥æ ¹æ®..."}]}
 {"conversations": [{"role": "user", "content": "è¯·é—®xxx"}, {"role": "assistant", "content": "xxx..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;è‡ªæˆ‘è®¤çŸ¥åœºæ™¯&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ"}, {"role": "assistant", "content": "æˆ‘å«minimind..."}]}
 {"conversations": [{"role": "user", "content": "ä½ æ˜¯è°"}, {"role": "assistant", "content": "æˆ‘æ˜¯..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æ­¤æ—¶ã€åŸºç¡€æ¨¡å‹+LoRAæ¨¡å‹ã€‘å³å¯è·å¾—åŒ»ç–—åœºæ™¯æ¨¡å‹å¢å¼ºçš„èƒ½åŠ›ï¼Œç›¸å½“äºä¸ºåŸºç¡€æ¨¡å‹å¢åŠ äº†LoRAå¤–æŒ‚ï¼Œè¿™ä¸ªè¿‡ç¨‹å¹¶ä¸æŸå¤±åŸºç¡€æ¨¡å‹çš„æœ¬èº«èƒ½åŠ›ã€‚ å¯ä»¥é€šè¿‡&lt;code&gt;eval_llm.py&lt;/code&gt;è¿›è¡Œæ¨¡å‹è¯„ä¼°æµ‹è¯•ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æ³¨æ„ï¼šweightå‚æ•°æŒ‡å®šåŸºç¡€æ¨¡å‹ç±»å‹ï¼Œéœ€ä¸train_loraè®­ç»ƒæ—¶ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹ä¿æŒä¸€è‡´
python eval_llm.py  --weight dpo --lora_weight lora_medical
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;å°æµ‹è¯•&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ğŸ‘¶: æˆ‘æœ€è¿‘ç»å¸¸æ„Ÿåˆ°å¤´æ™•ï¼Œå¯èƒ½æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ
ğŸ¤–ï¸: å¤´æ™•å¯èƒ½æ˜¯ç”±äºå¤šç§åŸå› ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šå¿ƒè„ç–¾ç—…ã€çœ¼ç›æŸä¼¤ã€è¿‡æ•ååº”ã€çš®è‚¤é—®é¢˜ã€è¿‡æ•ååº”ã€å‹åŠ›ã€ç„¦è™‘ã€ç–²åŠ³ã€è¯ç‰©å‰¯ä½œç”¨ã€æ€§åŠŸèƒ½éšœç¢æˆ–è€…å…¶ä»–...

---

ğŸ‘¶: ä½ å’Œopenaiæ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ
ğŸ¤–ï¸: æˆ‘æ˜¯ MiniMindï¼Œä¸€ä¸ªç”± Jingyao Gong å¼€å‘çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘é€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†å’Œç®—æ³•è®­ç»ƒæ¥ä¸ç”¨æˆ·è¿›è¡Œäº¤äº’ã€‚
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PSï¼šåªè¦æœ‰æ‰€éœ€è¦çš„æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥full_sftå…¨å‚å¾®è°ƒï¼ˆéœ€è¦è¿›è¡Œé€šç”¨çŸ¥è¯†çš„æ··åˆé…æ¯”ï¼Œå¦åˆ™è¿‡æ‹Ÿåˆé¢†åŸŸæ•°æ®ä¼šè®©æ¨¡å‹å˜å‚»ï¼ŒæŸå¤±é€šç”¨æ€§ï¼‰&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;5. è®­ç»ƒæ¨ç†æ¨¡å‹ (Reasoning Model)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;DeepSeek-R1å®åœ¨å¤ªç«äº†ï¼Œå‡ ä¹é‡æ–°æŒ‡æ˜äº†æœªæ¥LLMçš„æ–°èŒƒå¼ã€‚ è®ºæ–‡æŒ‡å‡º&lt;code&gt;&amp;gt;3B&lt;/code&gt;çš„æ¨¡å‹ç»å†å¤šæ¬¡åå¤çš„å†·å¯åŠ¨å’ŒRLå¥–åŠ±è®­ç»ƒæ‰èƒ½è·å¾—è‚‰çœ¼å¯è§çš„æ¨ç†èƒ½åŠ›æå‡ã€‚ æœ€å¿«æœ€ç¨³å¦¥æœ€ç»æµçš„åšæ³•ï¼Œä»¥åŠæœ€è¿‘çˆ†å‘çš„å„ç§å„æ ·æ‰€è°“çš„æ¨ç†æ¨¡å‹å‡ ä¹éƒ½æ˜¯ç›´æ¥é¢å‘æ•°æ®è¿›è¡Œè’¸é¦è®­ç»ƒï¼Œ ä½†ç”±äºç¼ºä¹æŠ€æœ¯å«é‡ï¼Œè’¸é¦æ´¾è¢«RLæ´¾ç§ä¸èµ·ï¼ˆhhhhï¼‰ã€‚ æœ¬äººè¿…é€Ÿå·²ç»åœ¨Qwenç³»åˆ—1.5Bå°æ¨¡å‹ä¸Šè¿›è¡Œäº†å°è¯•ï¼Œå¾ˆå¿«å¤ç°äº†Zeroè¿‡ç¨‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ ç„¶è€Œä¸€ä¸ªé—æ†¾çš„å…±è¯†æ˜¯ï¼šå‚æ•°å¤ªå°çš„æ¨¡å‹ç›´æ¥é€šè¿‡å†·å¯åŠ¨SFT+GRPOå‡ ä¹ä¸å¯èƒ½è·å¾—ä»»ä½•æ¨ç†æ•ˆæœã€‚ &lt;del&gt; MiniMind2ç¬¬ä¸€æ—¶é—´åªèƒ½åšå®šä¸ç§»çš„é€‰æ‹©åšè’¸é¦æ´¾ï¼Œæ—¥ååŸºäº0.1Bæ¨¡å‹çš„RLå¦‚æœåŒæ ·å–å¾—å°å°è¿›å±•ä¼šæ›´æ–°æ­¤éƒ¨åˆ†çš„è®­ç»ƒæ–¹æ¡ˆã€‚ &lt;/del&gt;&lt;/p&gt; 
&lt;p&gt;åšè’¸é¦éœ€è¦å‡†å¤‡çš„ä¾ç„¶æ˜¯å’ŒSFTé˜¶æ®µåŒæ ·æ ¼å¼çš„æ•°æ®å³å¯ï¼Œæ•°æ®é›†æ¥æºå·²å¦‚ä¸Šæ–‡ä»‹ç»ã€‚æ•°æ®æ ¼å¼ä¾‹å¦‚ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "conversations": [
    {
      "role": "user",
      "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯å°èŠ³ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ã€‚"
    },
    {
      "role": "assistant",
      "content": "&amp;lt;think&amp;gt;\nä½ å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…ç‹¬ç«‹å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹MiniMind-R1-Lite-Previewï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æä¾›æœåŠ¡ï¼\n&amp;lt;/think&amp;gt;\n&amp;lt;answer&amp;gt;\nä½ å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…ç‹¬ç«‹å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹MiniMind-R1-Lite-Previewï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æä¾›æœåŠ¡ï¼\n&amp;lt;/answer&amp;gt;"
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æ¨ç†æ¨¡å‹R1çš„å›å¤æ¨¡æ¿æ˜¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;lt;think&amp;gt;\næ€è€ƒè¿‡ç¨‹\n&amp;lt;/think&amp;gt;\n
&amp;lt;answer&amp;gt;\næœ€ç»ˆå›ç­”\n&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;è¿™åœ¨GRPOä¸­é€šè¿‡è®¾ç½®è§„åˆ™å¥–åŠ±å‡½æ•°çº¦æŸæ¨¡å‹ç¬¦åˆæ€è€ƒæ ‡ç­¾å’Œå›å¤æ ‡ç­¾ï¼ˆåœ¨å†·å¯åŠ¨é å‰çš„é˜¶æ®µå¥–åŠ±å€¼è®¾ç½®åº”è¯¥æé«˜ä¸€äº›ï¼‰&lt;/p&gt; 
&lt;p&gt;å¦ä¸€ä¸ªé—®é¢˜æ˜¯è’¸é¦è¿‡ç¨‹è™½ç„¶å’ŒSFTä¸€æ ·ï¼Œä½†å®éªŒç»“æœæ˜¯æ¨¡å‹éš¾ä»¥æ¯æ¬¡éƒ½ç¬¦åˆæ¨¡æ¿è§„èŒƒçš„å›å¤ï¼Œå³è„±ç¦»æ€è€ƒå’Œå›å¤æ ‡ç­¾çº¦æŸã€‚ è¿™é‡Œçš„å°æŠ€å·§æ˜¯å¢åŠ æ ‡è®°ä½ç½®tokençš„æŸå¤±æƒ©ç½šï¼Œè¯¦è§&lt;code&gt;train_distill_reason.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# åœ¨ sp_ids å¯¹åº”çš„ä½ç½®å¢åŠ é¢å¤–çš„æƒ©ç½š
...
loss_mask[sp_ids] = 10 # æƒ©ç½šç³»æ•°
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;å¦å¦ä¸€ä¸ªtipsæ˜¯ç”±äºæ¨ç†æ•°æ®ç”±äºåªç­›é€‰äº†&lt;code&gt;&amp;lt;1024&lt;/code&gt;é•¿åº¦çš„æ•°æ®ï¼Œå…¶ä¸­å¤šè½®å¯¹è¯å’Œè‹±æ–‡æ•°æ®åå°‘ï¼Œ å› æ­¤&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;è¿›è¡Œäº†å¤§çº¦10kæ¡å¤šè½®å¯¹è¯+è‹±æ–‡æ•°æ®çš„æ··åˆï¼Œé˜²æ­¢æ¨¡å‹é—å¿˜ä¸¥é‡ã€‚&lt;/p&gt; 
&lt;p&gt;è„šæœ¬é»˜è®¤åŸºäºrlhfåçš„åŸºæ¨¡å‹åšæ¨ç†èƒ½åŠ›çš„è’¸é¦å¾®è°ƒï¼Œä¸‹é¢ç›´æ¥å¯åŠ¨è®­ç»ƒå³å¯ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distill_reason.py
# or
python train_distill_reason.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;reason_*.pth&lt;/code&gt;ï¼ˆ*ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;æµ‹è¯•ä¸€ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;ğŸ‘¶: è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚
ğŸ¤–ï¸: &amp;lt;think&amp;gt;
å—¯ï¼Œç”¨æˆ·è®©æˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚æˆ‘ç°åœ¨éœ€è¦è§£é‡Šä¸€ä¸‹è‡ªå·±ã€‚é¦–å…ˆï¼Œæˆ‘åº”è¯¥æ˜ç¡®ç”¨æˆ·æ˜¯è°ï¼Œæ˜¯å­¦ç”Ÿã€èŒåœºäººå£«è¿˜æ˜¯æ™®é€šäººã€‚ç”¨æˆ·å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿæˆ–è€…èŒåœºäººå£«ï¼Œæ‰€ä»¥ä¿¡æ¯ä¼ é€’åº”è¯¥æ˜¯å¸®åŠ©å­¦ç”Ÿå’ŒèŒåœºäººå£«çš„ä¿¡æ¯ã€‚
ç„¶åï¼Œæˆ‘éœ€è¦å›åº”ç”¨æˆ·ï¼Œè¿™å¯èƒ½åŒ…æ‹¬ä»‹ç»è‡ªå·±çš„ä½ç½®ã€å…´è¶£çˆ±å¥½ã€èŒä¸šç»å†ç­‰ä¿¡æ¯ã€‚ç”¨æˆ·è¿˜å¯èƒ½æƒ³äº†è§£è‡ªå·±åœ¨å“ªé‡Œï¼Œæ¯”å¦‚èŒä¸šå‘å±•ã€å·¥ä½œç»éªŒç­‰ã€‚
æˆ‘åº”è¯¥æ³¨æ„ç”¨è¯è¦æ­£å¼ï¼Œä¸è¦å¤ªæ­£å¼ï¼ŒåŒæ—¶è¦ä¿æŒç®€æ´æ˜äº†ã€‚è¿™æ ·ç”¨æˆ·å®¹æ˜“ç†è§£ï¼Œä¹Ÿæ–¹ä¾¿ä»–ä»¬æŸ¥é˜…ç›¸å…³èµ„æ–™ã€‚
æœ€åï¼Œæˆ‘åº”è¯¥æ€»ç»“ä¸€ä¸‹ï¼Œè®©ç”¨æˆ·çŸ¥é“æˆ‘æ˜¯å¦‚ä½•å¸®åŠ©ä»–ä»¬å®Œæˆä»–ä»¬çš„ä»»åŠ¡çš„ï¼Œè¿™æ ·ä»–ä»¬æœ‰æ›´å¤šå…³äºæˆ‘çš„ä¿¡æ¯ã€‚
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
æˆ‘æ˜¯**â€œä½ çš„åå­—â€ï¼Œä½ å«ä»€ä¹ˆåå­—ï¼Ÿ
&amp;lt;/answer&amp;gt;

ğŸ‘¶: ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ
ğŸ¤–ï¸: &amp;lt;think&amp;gt;
æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹MiniMind-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹MiniMind-R1ã€‚å¦‚æ‚¨æœ‰ä»»ä½•ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šå°½æˆ‘æ‰€èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚
&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;IV å¼ºåŒ–å­¦ä¹ åè®­ç»ƒ&lt;/h2&gt; 
&lt;p&gt;LLMé‡Œçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¯åˆ†ä¸¤ç±»ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;é€šè¿‡&lt;strong&gt;äººç±»&lt;/strong&gt;å¯¹æ¨¡å‹è¾“å‡ºçš„åå¥½è¿›è¡Œè¯„ä»·æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆæ›´ç¬¦åˆäººç±»ä»·å€¼è§‚å’Œåå¥½çš„å†…å®¹ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;åŸºäºAIåé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from AI Feedback, RLAIF)&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä½¿ç”¨&lt;strong&gt;AIæ¨¡å‹&lt;/strong&gt;ï¼ˆé€šå¸¸æ˜¯é¢„è®­ç»ƒçš„è¯­è¨€å¥–åŠ±æ¨¡å‹ï¼‰æ¥æä¾›åé¦ˆï¼Œè€Œä¸ç›´æ¥ä¾èµ–äººç±»çš„äººå·¥æ ‡æ³¨ã€‚&lt;/li&gt; 
 &lt;li&gt;è¿™é‡Œçš„â€œAIâ€ä¹Ÿå¯ä»¥æ˜¯æŸäº›è§„åˆ™å¥–åŠ±ï¼Œä¾‹å¦‚æ•°å­¦ç­”æ¡ˆ/ä»£ç è§£é‡Šå™¨...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç±»å‹&lt;/th&gt; 
   &lt;th&gt;è£åˆ¤&lt;/th&gt; 
   &lt;th&gt;ä¼˜ç‚¹&lt;/th&gt; 
   &lt;th&gt;ç¼ºç‚¹&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RLHF&lt;/td&gt; 
   &lt;td&gt;äººç±»&lt;/td&gt; 
   &lt;td&gt;æ›´è´´è¿‘çœŸå®äººç±»åå¥½&lt;/td&gt; 
   &lt;td&gt;æˆæœ¬é«˜ã€æ•ˆç‡ä½&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RLAIF&lt;/td&gt; 
   &lt;td&gt;æ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•æ€§å¼º&lt;/td&gt; 
   &lt;td&gt;å¯èƒ½åç¦»äººç±»çœŸå®åå¥½&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;äºŒè€…æœ¬è´¨ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯é€šè¿‡&lt;strong&gt;å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼&lt;/strong&gt;ï¼Œåˆ©ç”¨æŸç§å½¢å¼çš„"&lt;strong&gt;åé¦ˆ&lt;/strong&gt;"æ¥ä¼˜åŒ–æ¨¡å‹çš„è¡Œä¸ºã€‚&lt;/p&gt; 
&lt;p&gt;é™¤äº†&lt;strong&gt;åé¦ˆ&lt;/strong&gt;çš„æ¥æºä¸åŒï¼Œå…¶ä»–å¹¶æ— ä»»ä½•åŒºåˆ«ã€‚&lt;/p&gt; 
&lt;h3&gt;ğŸ‘€ POç®—æ³•çš„ç»Ÿä¸€è§†è§’&lt;/h3&gt; 
&lt;p&gt;åœ¨ä»‹ç»å®ç°å…·ä½“ç®—æ³•ä¹‹å‰ï¼Œæˆ‘å…ˆä»¥ä¸ªäººç†è§£çš„æç®€è§†è§’ï¼Œé˜è¿°æ‰€æœ‰Policy Optimization (PO)ç®—æ³•çš„ç»Ÿä¸€å…±æ€§ã€‚&lt;/p&gt; 
&lt;p&gt;æ‰€æœ‰RLç®—æ³•çš„æœ¬è´¨éƒ½åªæ˜¯åœ¨ä¼˜åŒ–ä¸€ä¸ªæœŸæœ›ï¼š&lt;/p&gt; 
&lt;p&gt;$$\mathcal{J}&lt;em&gt;{PO} = \mathbb{E}&lt;/em&gt;{q \sim P(Q), o \sim \pi(O|q)} \left[ \underbrace{f(r_t)}&lt;em&gt;{\text{ç­–ç•¥é¡¹}} \cdot \underbrace{g(A_t)}&lt;/em&gt;{\text{ä¼˜åŠ¿é¡¹}} - \underbrace{h(\text{KL}&lt;em&gt;t)}&lt;/em&gt;{\text{æ­£åˆ™é¡¹}} \right]$$&lt;/p&gt; 
&lt;p&gt;è®­ç»ƒæ—¶ï¼Œåªéœ€&lt;strong&gt;æœ€å°åŒ–è´Ÿç›®æ ‡å‡½æ•°&lt;/strong&gt;ï¼Œå³: $\mathcal{L_{PO}}=-\mathcal{J_{PO}}$&lt;/p&gt; 
&lt;p&gt;è¿™ä¸ªæ¡†æ¶åªåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt; $f(r_t)$: å¦‚ä½•ä½¿ç”¨æ¦‚ç‡æ¯” $r_t$? å³å‘Šè¯‰æ¨¡å‹æ–°æ—§ç­–ç•¥åå·®æœ‰å¤šå¤§ï¼Œæ˜¯å¦æ¢ç´¢åˆ°äº†æ›´å¥½çš„token&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt; $g(A_t)$: å¦‚ä½•è®¡ç®—ä¼˜åŠ¿ $A_t$, è¿™å¾ˆé‡è¦ï¼å¤§æ¨¡å‹ç®—å¯¹å®šç§¯åˆ†ä¹Ÿä¸è¶³ä¸ºå¥‡ï¼Œå°æ¨¡å‹å›ç­”å¯¹åŠ å‡æ³•ä¼˜åŠ¿é€šå¸¸éƒ½æ˜¯æ­£çš„&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt; $h(\text{KL}_t)$: å¦‚ä½•çº¦æŸå˜åŒ–å¹…åº¦ $\text{KL}_t$, æ—¢é˜²æ­¢è·‘ååˆé˜²æ­¢ç®¡çš„å¤ªæ­»&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;ï¼ˆå±•å¼€ï¼‰ç¬¦å·è¯´æ˜&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;ç¬¦å·&lt;/th&gt; 
    &lt;th&gt;å«ä¹‰&lt;/th&gt; 
    &lt;th&gt;è¯´æ˜&lt;/th&gt; 
    &lt;th&gt;å€¼åŸŸ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$q$&lt;/td&gt; 
    &lt;td&gt;é—®é¢˜/æç¤ºè¯&lt;/td&gt; 
    &lt;td&gt;ä»æ•°æ®é›† $P(Q)$ ä¸­é‡‡æ ·&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$o$&lt;/td&gt; 
    &lt;td&gt;æ¨¡å‹è¾“å‡ºåºåˆ—&lt;/td&gt; 
    &lt;td&gt;ç”±ç­–ç•¥ $\pi$ ç”Ÿæˆ&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$r_t$&lt;/td&gt; 
    &lt;td&gt;æ¦‚ç‡æ¯”&lt;/td&gt; 
    &lt;td&gt;$r_t = \frac{\pi_\theta(o_t|q, o_{&amp;lt;t})}{\pi_{ref}(o_t|q, o_{&amp;lt;t})}$&lt;/td&gt; 
    &lt;td&gt;$(0, +\infty)$&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$A_t$&lt;/td&gt; 
    &lt;td&gt;ä¼˜åŠ¿å‡½æ•°&lt;/td&gt; 
    &lt;td&gt;è¡¡é‡æŸä¸ªåŠ¨ä½œç›¸æ¯”åŸºçº¿æœ‰å¤šå¥½&lt;/td&gt; 
    &lt;td&gt;$(-\infty, +\infty)$&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$\text{KL}_t$&lt;/td&gt; 
    &lt;td&gt;KLæ•£åº¦&lt;/td&gt; 
    &lt;td&gt;é˜²æ­¢ç­–ç•¥åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ&lt;/td&gt; 
    &lt;td&gt;$[0, +\infty)$&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;ä¸åŒçš„&lt;strong&gt;xxPOç®—æ³•&lt;/strong&gt;æœ¬è´¨ä¸Šåªæ˜¯å¯¹è¿™ä¸‰ä¸ªç»„ä»¶çš„ä¸åŒè®¾è®¡çš„å®ä¾‹åŒ–ï¼&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;6. åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;åœ¨å‰é¢çš„è®­ç»ƒæ­¥éª¤ä¸­ï¼Œæ¨¡å‹å·²ç»å…·å¤‡äº†åŸºæœ¬çš„å¯¹è¯èƒ½åŠ›ï¼Œä½†æ˜¯è¿™æ ·çš„èƒ½åŠ›å®Œå…¨åŸºäºå•è¯æ¥é¾™ï¼Œç¼ºå°‘æ­£åæ ·ä¾‹çš„æ¿€åŠ±ã€‚ æ¨¡å‹æ­¤æ—¶å°šæœªçŸ¥ä»€ä¹ˆå›ç­”æ˜¯å¥½çš„ï¼Œä»€ä¹ˆæ˜¯å·®çš„ã€‚å¸Œæœ›å®ƒèƒ½å¤Ÿæ›´ç¬¦åˆäººçš„åå¥½ï¼Œé™ä½è®©äººç±»ä¸æ»¡æ„ç­”æ¡ˆçš„äº§ç”Ÿæ¦‚ç‡ã€‚ è¿™ä¸ªè¿‡ç¨‹å°±åƒæ˜¯è®©æ¨¡å‹å‚åŠ æ–°çš„åŸ¹è®­ï¼Œä»ä¼˜ç§€å‘˜å·¥çš„ä½œä¸ºä¾‹å­ï¼Œæ¶ˆæå‘˜å·¥ä½œä¸ºåä¾‹ï¼Œå­¦ä¹ å¦‚ä½•æ›´å¥½åœ°å›å¤ã€‚&lt;/p&gt; 
&lt;h4&gt;6.1 Direct Preference Optimization&lt;/h4&gt; 
&lt;p&gt;ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç®—æ³•ï¼ŒæŸå¤±ä¸ºï¼š&lt;/p&gt; 
&lt;p&gt;$$\mathcal{L}&lt;em&gt;{DPO} = -\mathbb{E}\left[\log \sigma\left(\beta \left[\log \frac{\pi&lt;/em&gt;\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right]\right)\right]$$&lt;/p&gt; 
&lt;p&gt;å…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt;: $f(r_t) = \log r_w - \log r_l$ (å¯¹æ¯”chosen vs rejectedçš„æ¦‚ç‡æ¯”)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt;: $g(A_t)$ = / (é€šè¿‡åå¥½å¯¹æ¯”ï¼Œæ— éœ€æ˜¾å¼è®¡ç®—ä¼˜åŠ¿)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt;: $h(\text{KL}_t)$ = éšå«åœ¨ $\beta$ ä¸­ (æ§åˆ¶åç¦»å‚è€ƒæ¨¡å‹ç¨‹åº¦)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ç‰¹åˆ«åœ°ï¼Œ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DPOä»PPOå¸¦KLçº¦æŸçš„ç›®æ ‡æ¨å¯¼å‡ºå¯¹åå¥½å¯¹çš„è§£æè®­ç»ƒç›®æ ‡ï¼Œç›´æ¥æœ€å¤§åŒ–"chosenä¼˜äºrejected"çš„å¯¹æ•°å‡ ç‡ï¼›æ— éœ€åŒæ­¥è®­ç»ƒReward/Valueæ¨¡å‹ã€‚DPOåªéœ€è·‘&lt;code&gt;actor&lt;/code&gt;ä¸&lt;code&gt;ref&lt;/code&gt;ä¸¤ä¸ªæ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨ä½ã€æ”¶æ•›ç¨³å®šã€å®ç°ç®€å•ã€‚&lt;/li&gt; 
 &lt;li&gt;è®­ç»ƒèŒƒå¼ï¼šoffâ€‘policyï¼Œä½¿ç”¨é™æ€åå¥½æ•°æ®é›†ï¼Œå¯åå¤å¤šè½®epochï¼›Refæ¨¡å‹å›ºå®šï¼ˆé¢„å…ˆç¼“å­˜è¾“å‡ºï¼‰ã€‚&lt;/li&gt; 
 &lt;li&gt;DPOçš„å±€é™åœ¨äºä¸åšåœ¨çº¿æ¢ç´¢ï¼Œæ›´å¤šç”¨äº"åå¥½/å®‰å…¨"çš„äººç±»ä»·å€¼å¯¹é½ï¼›å¯¹"èƒ½ä¸èƒ½åšå¯¹é¢˜"çš„æ™ºåŠ›èƒ½åŠ›æå‡æœ‰é™ï¼ˆå½“ç„¶è¿™ä¹Ÿå–å†³äºæ•°æ®é›†ï¼Œå¤§è§„æ¨¡æ”¶é›†æ­£åæ ·æœ¬å¹¶äººç±»è¯„ä¼°å¾ˆå›°éš¾ï¼‰ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_dpo.py
# or
python train_dpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;dpo_*.pth&lt;/code&gt;ï¼ˆ*ä¸ºæ¨¡å‹å…·ä½“dimensionï¼Œæ¯æ¬¡ä¿å­˜æ—¶æ–°æ–‡ä»¶ä¼šè¦†ç›–æ—§æ–‡ä»¶ï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;7. åŸºäºAIåé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from AI Feedback, RLAIF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;ç›¸æ¯”RLHFä¾èµ–äººç±»æ ‡æ³¨chosen/rejectedåå¥½å¯¹ï¼ŒRLAIFåˆ™å®Œå…¨ç”±AIæ¥å……å½“"è£åˆ¤"ã€‚ æ‰€è°“AI"è£åˆ¤"å¯ä»¥æ˜¯model-baseçš„å¥–åŠ±å¤§æ¨¡å‹(Reward Model)ï¼Œä¹Ÿå¯ä»¥æ˜¯R1ä¸€æ ·è®¾ç½®è§„åˆ™å‡½æ•°è¿›è¡Œæ ¡éªŒï¼Œä¹Ÿå¯ä»¥æ˜¯ä¾‹å¦‚å·¥å…·è°ƒç”¨çš„ç¯å¢ƒåé¦ˆã€‚ ä¾‹å¦‚ï¼šæ•°å­¦é¢˜ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€å·¥å…·è°ƒç”¨æ‰§è¡Œä»£ç èƒ½å¦é€šè¿‡æµ‹è¯•ç”¨ä¾‹ã€æ¨ç†è¿‡ç¨‹æ˜¯å¦ç¬¦åˆæ ¼å¼...éƒ½å¯ä»¥è‡ªåŠ¨åŒ–åˆ¤æ–­ã€‚ RLAIFçš„æœ€å¤§ä¼˜åŠ¿åœ¨äº&lt;strong&gt;å¯æ‰©å±•æ€§&lt;/strong&gt;å’Œ&lt;strong&gt;On-Policy&lt;/strong&gt;çš„ç‰¹ç‚¹â€”â€”ä¸éœ€è¦æ˜‚è´µçš„äººå·¥æ ‡æ³¨ï¼Œå¯ä»¥ç”Ÿæˆæµ·é‡çš„è®­ç»ƒæ ·æœ¬ï¼Œè®©æ¨¡å‹åœ¨åœ¨çº¿å¤§é‡è¯•é”™ä¸­å¿«é€Ÿè¿›åŒ–ã€‚&lt;/p&gt; 
&lt;p&gt;MiniMind ç€æ‰‹å®ç°&lt;strong&gt;2+N&lt;/strong&gt;ç§åŸºæœ¬+å‰æ²¿çš„RLAIFæ–¹æ³•ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PPO&lt;/strong&gt;ã€&lt;strong&gt;GRPO&lt;/strong&gt; è¢«å¤§è§„æ¨¡éªŒè¯çš„ç»å…¸RLç®—æ³•ï¼›&lt;/li&gt; 
 &lt;li&gt;Nç§å‰æ²¿RLç®—æ³•ï¼ˆä¸å®šæœŸä»¥Expæ€§è´¨æ›´æ–°ï¼‰ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;1ï¸âƒ£ æ•°æ®é›†å‡†å¤‡ (éœ€è¦)&lt;/h4&gt; 
&lt;p&gt;ä¸ºäº†å¿«é€ŸéªŒè¯RLAIFçš„æ•ˆæœï¼Œè¿™é‡Œä»SFTæ•°æ®é›†ä¸­éšæœºé‡‡æ ·äº†1ä¸‡æ¡é«˜è´¨é‡å¯¹è¯ï¼Œæ„å»ºçº¦1MBå¤§å°çš„&lt;code&gt;rlaif-mini.jsonl&lt;/code&gt;(&lt;a href="https://huggingface.co/datasets/jingyaogong/minimind_dataset/blob/main/rlaif-mini.jsonl"&gt;Huggingface&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;æ•°æ®æ ¼å¼ä¸SFTä¸€è‡´ï¼Œä½†assistantå¹¶ä¸éœ€è¦å†…å®¹ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­å®Œå…¨ç”± $\Pi$ ç­–ç•¥æ¨¡å‹å®æ—¶é‡‡æ ·ç”Ÿæˆã€‚å› æ­¤å½¢å¦‚ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "conversations": [
        {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨ï¼Ÿ"},
        {"role": "assistant", "content": "æ— "}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RLAIFçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šåŸºäºuserçš„é—®é¢˜ç”Ÿæˆ1æˆ–å¤šä¸ªå€™é€‰å›ç­”ï¼Œç„¶åç”±å¥–åŠ±å‡½æ•°/æ¨¡å‹å¯¹å›ç­”æ‰“åˆ†ï¼Œ åˆ†æ•°é«˜çš„å›ç­”ä¼šè¢«é¼“åŠ±ï¼ˆå¢åŠ  $\Pi$ ç­–ç•¥æ¦‚ç‡ï¼‰ï¼Œåˆ†æ•°ä½çš„å›ç­”ä¼šè¢«æŠ‘åˆ¶ï¼ˆé™ä½ $\Pi$ ç­–ç•¥æ¦‚ç‡ï¼‰ã€‚è¿™ä¸ª"æ‰“åˆ†-&amp;gt;è°ƒæ•´"çš„å¾ªç¯å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒã€‚&lt;/p&gt; 
&lt;h4&gt;2ï¸âƒ£ å¥–åŠ±æ¨¡å‹å‡†å¤‡ (éœ€è¦)&lt;/h4&gt; 
&lt;p&gt;å·²çŸ¥RLAIFè®­ç»ƒéœ€è¦â€œå¥–åŠ±æ¨¡å‹ (Reward Model)â€å¯¹ç”Ÿæˆçš„å›ç­”è¿›è¡Œæ‰“åˆ†ã€‚&lt;/p&gt; 
&lt;p&gt;æ­¤å¤„é€‰å–å°å‹ä¸”é«˜è´¨é‡çš„InternLM2-1.8B-Reward (&lt;a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b-reward"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/internlm/internlm2-1_8b-reward"&gt;HuggingFace&lt;/a&gt;) ä½œä¸ºåŸºç¡€å¥–åŠ±æ¨¡å‹ã€‚&lt;/p&gt; 
&lt;p&gt;ä¸‹è½½å¥–åŠ±æ¨¡å‹åéœ€è¦æ”¾ç½®åœ¨minimindé¡¹ç›®çš„&lt;strong&gt;åŒçº§ç›®å½•&lt;/strong&gt;ä¸‹ï¼Œæ¨èç»“æ„å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;project/
â”œâ”€â”€ minimind/                    # MiniMindé¡¹ç›®
â”‚   â”œâ”€â”€ model/
â”‚   â””â”€â”€ ...
â””â”€â”€ internlm2-1_8b-reward/       # å¥–åŠ±æ¨¡å‹ï¼ˆä¸minimindåŒçº§ï¼‰
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â””â”€â”€ ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;å¥–åŠ±æœºåˆ¶é€‰æ‹©ä¸MiniMindé™åˆ¶è¯´æ˜ï¼ˆç‚¹å‡»å±•å¼€ï¼‰&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;1. å¥–åŠ±æœºåˆ¶çš„å¤šæ ·æ€§&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;RLAIFä¸­çš„"å¥–åŠ±ä¿¡å·"æ¥æºå¯ä»¥éå¸¸çµæ´»ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-basedå¥–åŠ±&lt;/strong&gt;ï¼šå¯ä½¿ç”¨ä¸“é—¨çš„Reward Modelï¼ˆå¦‚InternLM2-Rewardï¼‰ï¼Œä¹Ÿå¯ä½¿ç”¨é€šç”¨LLM+æç¤ºè¯è¿›è¡Œæ‰“åˆ†ï¼ˆå¦‚Qwen3-as-a-Judgeï¼‰ã€‚å¥–åŠ±æ¨¡å‹è§„æ¨¡å’Œæ¶æ„å‡å¯è‡ªç”±é€‰æ‹©ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rule-basedå¥–åŠ±&lt;/strong&gt;ï¼šå¯ä»¥åŸºäºè§„åˆ™å‡½æ•°æ„é€ å¥–åŠ±ä¿¡å·ï¼Œä¾‹å¦‚ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;æ•°å­¦é¢˜ç­”æ¡ˆæ­£ç¡®æ€§éªŒè¯ï¼ˆGround Truthå¯¹æ¯”ï¼‰&lt;/li&gt; 
    &lt;li&gt;SQLæ‰§è¡ŒæˆåŠŸç‡ä¸ç»“æœå‡†ç¡®æ€§&lt;/li&gt; 
    &lt;li&gt;ä»£ç è§£é‡Šå™¨è¿è¡Œç»“æœï¼ˆpass@kï¼‰&lt;/li&gt; 
    &lt;li&gt;å·¥å…·è°ƒç”¨è¿”å›çŠ¶æ€ï¼ˆAPIæˆåŠŸ/å¤±è´¥ï¼‰&lt;/li&gt; 
    &lt;li&gt;æ ¼å¼åˆè§„æ€§æ£€æŸ¥ï¼ˆJSON/XMLè§£æï¼‰&lt;/li&gt; 
    &lt;li&gt;æ¨ç†é“¾å®Œæ•´æ€§è¯„ä¼°ï¼ˆCoTæ­¥éª¤æ•°ï¼‰&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment-basedå¥–åŠ±&lt;/strong&gt;ï¼šåœ¨Agentåœºæ™¯ä¸­ï¼Œç¯å¢ƒåé¦ˆæœ¬èº«å³ä¸ºå¤©ç„¶å¥–åŠ±ï¼ˆå¦‚æ¸¸æˆå¾—åˆ†ã€Researchå®Œæ•´åº¦ã€ä»»åŠ¡å®Œæˆåº¦ï¼‰ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ä»»ä½•èƒ½å¤Ÿé‡åŒ–"å›ç­”è´¨é‡"çš„æœºåˆ¶éƒ½å¯ä½œä¸ºRLçš„å¥–åŠ±æ¥æºã€‚DeepSeek R1å°±æ˜¯å…¸å‹æ¡ˆä¾‹ï¼šä½¿ç”¨è§„åˆ™å‡½æ•°éªŒè¯æ•°å­¦ç­”æ¡ˆæ­£ç¡®æ€§ä½œä¸ºå¥–åŠ±ï¼Œæ— éœ€é¢å¤–çš„Reward Modelã€‚&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2. MiniMindé™åˆ¶ï¼šå¥–åŠ±ç¨€ç–é—®é¢˜&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;RLAIFè®­ç»ƒæ—¢å¯ä»¥é’ˆå¯¹æ¨ç†æ¨¡å‹ä¹Ÿå¯ä»¥é’ˆå¯¹éæ¨ç†æ¨¡å‹ï¼ŒåŒºåˆ«ä»…åœ¨äºæ ¼å¼ã€‚&lt;/p&gt; 
 &lt;p&gt;ç„¶è€Œå¯¹äºMiniMindè¿™ç§0.1Bå‚æ•°é‡æå°èƒ½åŠ›å¼±çš„æ¨¡å‹ï¼Œåœ¨é€šç”¨ä»»åŠ¡ï¼ˆå¦‚R1é£æ ¼çš„æ•°å­¦æ•°æ®é›†ï¼‰ä¸Šä¼šé‡åˆ°ä¸¥é‡çš„å¥–åŠ±ç¨€ç–(Reward Sparsity)é—®é¢˜ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ç°è±¡&lt;/strong&gt;ï¼šæ¨¡å‹ç”Ÿæˆçš„å€™é€‰å›ç­”å‡ ä¹å…¨éƒ¨é”™è¯¯ï¼Œå¯¼è‡´æ‰€æœ‰å¥–åŠ±åˆ†æ•° $r(x,y) \approx 0$&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;åæœ&lt;/strong&gt;ï¼šä¼˜åŠ¿å‡½æ•° $A(x,y) = r(x,y) - b(x) \approx 0$ï¼Œç­–ç•¥æ¢¯åº¦ä¿¡å·æ¶ˆå¤±ï¼Œæ— æ³•æœ‰æ•ˆæ›´æ–°å‚æ•° $\theta$&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;å¦‚åŒè®©å°å­¦ç”Ÿåšé«˜è€ƒæ•°å­¦é¢˜ï¼Œæ— è®ºå°è¯•å¤šå°‘æ¬¡éƒ½å¾—é›¶åˆ†ï¼Œæ— æ³•é€šè¿‡åˆ†æ•°å·®å¼‚å­¦ä¹ æ”¹è¿›ç­–ç•¥ã€‚å› æ­¤è¿™æ˜¯RLç®—æ³•çš„æ ¹æœ¬åŸç†é™åˆ¶çš„ã€‚&lt;/p&gt; 
 &lt;p&gt;ä¸ºç¼“è§£æ­¤é—®é¢˜ï¼ŒMiniMindçš„å®ç°é€‰æ‹©äº†&lt;strong&gt;model-basedçš„è¿ç»­æ€§å¥–åŠ±ä¿¡å·&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Reward Modelè¾“å‡ºè¿ç»­åˆ†æ•°ï¼ˆå¦‚-2.5åˆ°+3.0ï¼‰ï¼Œè€ŒéäºŒå…ƒçš„0/1&lt;/li&gt; 
  &lt;li&gt;å³ä½¿å›ç­”è´¨é‡éƒ½å·®ï¼Œä¹Ÿä»èƒ½åŒºåˆ†"æ›´æ›´å·®"(-3.0)å’Œ"æ›´å·®"(-2.8)çš„ç»†å¾®å·®å¼‚ã€‚æ‰€ä»¥è¿™ç§&lt;strong&gt;ç¨ å¯†ä¸”è¿ç»­&lt;/strong&gt;çš„å¥–åŠ±ä¿¡å·èƒ½å¤Ÿä¸ºä¼˜åŠ¿å‡½æ•° $A(x,y)$ æä¾›éé›¶æ¢¯åº¦ï¼Œä½¿å¾—ç­–ç•¥ç½‘ç»œå¾—ä»¥æ¸è¿›å¼ä¼˜åŒ–&lt;/li&gt; 
  &lt;li&gt;ä¹Ÿå¯ä»¥æ··åˆå¤šç§å¥–åŠ±æº: $r_{\text{total}} = \alpha \cdot r_{\text{model}} + \beta \cdot r_{\text{rule}}$ (ä¾‹å¦‚æ—¢å¯ä»¥æ£€æµ‹thinkæ ‡ç­¾æ ¼å¼rewardï¼Œåˆå¯ä»¥ç»¼åˆå›ç­”æœ¬èº«è´¨é‡çš„rewardåˆ†æ•°)&lt;/li&gt; 
  &lt;li&gt;minimindå®è·µä¸­é¿å…ç›´æ¥ä½¿ç”¨rule-basedäºŒå…ƒå¥–åŠ± + è¶…çº²éš¾åº¦æ•°æ®ï¼ˆå¦‚MATH500ï¼‰ï¼Œæ˜“å¯¼è‡´å¥–åŠ±å…¨é›¶ï¼›&lt;/li&gt; 
  &lt;li&gt;ç›‘æ§è®­ç»ƒæ—¶è§‚å¯Ÿå¥–åŠ±åˆ†æ•°çš„æ–¹å·® $\text{Var}(r)$ï¼Œè‹¥æŒç»­æ¥è¿‘0åˆ™éœ€è°ƒæ•´æ•°æ®æˆ–å¥–åŠ±æœºåˆ¶&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;å¯¹äºç”Ÿäº§çº§å¤§æ¨¡å‹çš„Agentic RLåœºæ™¯&lt;/strong&gt;ï¼š&lt;/p&gt; 
 &lt;p&gt;åœ¨çœŸå®Agentç³»ç»Ÿï¼ˆä»£ç ç”Ÿæˆã€å·¥å…·è°ƒç”¨ã€æ£€ç´¢-è§„åˆ’-æ‰§è¡Œçš„å¤šè½®é“¾è·¯ï¼‰ä¸­ï¼Œå¥–åŠ±æ˜¯â€œå»¶è¿Ÿæ•´è½®ç»“ç®—â€çš„ä¸åŒèŒƒå¼ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;LLMéœ€è¦é€tokenç”Ÿæˆå·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼ˆtool_callï¼‰ï¼Œç»å†è§£æï¼ˆtool_parseï¼‰ã€å·¥å…·æ‰§è¡Œï¼ˆtool_execï¼‰ï¼Œå†æŠŠç»“æœæ‹¼æ¥å›ä¸Šä¸‹æ–‡ç»§ç»­ä¸‹ä¸€æ­¥ï¼›å¾ªç¯å¾€å¤ç›´åˆ°å®Œæˆã€‚&lt;/li&gt; 
  &lt;li&gt;ä¸€æ¬¡å®Œæ•´çš„ä»»åŠ¡é“¾è·¯åŒ…å«å¤šæ¬¡è°ƒç”¨+æ€è€ƒï¼Œç›´åˆ°ç»ˆæ­¢æ¡ä»¶æ»¡è¶³æ—¶è®¡ç®—ä¸€æ¬¡æ€»rewardï¼ˆå¦‚ä»»åŠ¡æ˜¯å¦å®Œæˆã€æµ‹è¯•æ˜¯å¦é€šè¿‡ã€ç›®æ ‡æ˜¯å¦å‘½ä¸­ï¼‰ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;å› æ­¤ï¼ŒAgentic RLæ›´æ¥è¿‘ç¨€ç–/å»¶è¿Ÿå¥–åŠ±è®¾å®šï¼šæ¢¯åº¦å›ä¼ åœ¨â€œæ•´è½®ç»“æŸåâ€æ‰å‘ç”Ÿï¼Œå’ŒéAgentic RLä»»åŠ¡åœ¨å¯¹è¯å•è½®ä¸Šâ€œå³æ—¶è¯„åˆ†å³æ—¶æ›´æ–°â€æœ‰å¾ˆå¤§ä¸åŒã€‚ è¿™ä¹Ÿè§£é‡Šäº†Agentä»»åŠ¡ä¸Šæ›´åå‘ç¯å¢ƒåé¦ˆï¼ˆenvironment-based rewardï¼‰ï¼Œè€Œéå‡­Reward Modelè¿›è¡Œé™æ€æ‰“åˆ†ã€‚&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ç¯å¢ƒäº¤äº’åé¦ˆ&lt;/strong&gt;ï¼šæœ€ç»ˆä»¥æ‰§è¡Œç»“æœä¸ºå‡†ï¼ˆä»£ç æ˜¯å¦è·‘é€šã€APIæ˜¯å¦è¿”å›æˆåŠŸã€å­ç›®æ ‡æ˜¯å¦å®Œæˆï¼‰ï¼›&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Model-basedå¥–åŠ±å±€é™&lt;/strong&gt;ï¼šå¯¹é•¿é“¾è·¯ã€å¯æ‰§è¡Œè¯­ä¹‰çš„å…¨è²Œæ•æ‰æœ‰é™ï¼Œä¸”å¤§æ¦‚ç‡å’ŒçœŸå®ç¯å¢ƒåé¦ˆä¸ä¸€è‡´ï¼ˆreward hackingï¼‰ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h4&gt;7.1 &lt;a href="https://arxiv.org/abs/1707.06347"&gt;Proximal Policy Optimization&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;PPO æ˜¯2017å¹´OpenAIæå‡ºçš„éå¸¸ç»å…¸å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¹Ÿæ˜¯LLM RLé€šç”¨çš„åŸºçº¿æ–¹æ³•ï¼Œç”šè‡³ä¸éœ€è¦åŠ ä¹‹ä¸€ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;PPOæŸå¤±&lt;/strong&gt;ï¼š $$\mathcal{L}_{PPO} = -\mathbb{E}\left[\min(r_t \cdot A_t, \text{clip}(r_t, 1-\varepsilon, 1+\varepsilon) \cdot A_t)\right] + \beta \cdot \mathbb{E}[\text{KL}]$$&lt;/p&gt; 
&lt;p&gt;å…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt;: $f(r_t) = \min(r_t, \text{clip}(r_t, 1-\varepsilon, 1+\varepsilon))$ (è£å‰ªæ¦‚ç‡æ¯”é˜²æ­¢æ›´æ–°è¿‡æ¿€)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt;: $g(A_t) = R - V(s)$ (é€šè¿‡Criticç½‘ç»œä¼°è®¡ä»·å€¼å‡½æ•°)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt;: $h(\text{KL}_t) = \beta \cdot \mathbb{E}[\text{KL}]$ (å…¨å±€KLæ•£åº¦çº¦æŸ)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¯¹æ¯”DPOè€Œè¨€ï¼Œ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DPO (Off-Policy)ï¼šè®­ç»ƒæ•°æ®æ˜¯é™æ€çš„åå¥½æ•°æ®é›†ï¼ˆchosen vs rejectedï¼‰ï¼Œå¯ä»¥åå¤ä½¿ç”¨åŒä¸€æ‰¹æ•°æ®è®­ç»ƒå¤šä¸ªepochï¼Œå°±åƒä¼ ç»Ÿç›‘ç£å­¦ä¹ ä¸€æ ·ã€‚æ•°æ®æ•ˆç‡é«˜ï¼Œè®­ç»ƒæˆæœ¬ä½ã€‚å®ƒç›´æ¥ä¼˜åŒ–åå¥½å¯¹çš„å¯¹æ•°ä¼¼ç„¶ï¼Œæ— éœ€Reward Modelã€‚&lt;/li&gt; 
 &lt;li&gt;PPO (On-Policy)ï¼šå¿…é¡»ç”¨å½“å‰ç­–ç•¥å®æ—¶é‡‡æ ·ç”Ÿæˆæ–°æ•°æ®ï¼Œæ—§ç­–ç•¥é‡‡é›†çš„æ•°æ®ä¸èƒ½ç”¨ï¼ˆä¼šæœ‰distribution shifté—®é¢˜ï¼‰ã€‚è™½ç„¶é€šè¿‡importance samplingå’Œclipæœºåˆ¶å…è®¸è½»å¾®çš„åˆ†å¸ƒåç§»ï¼Œä½†æœ¬è´¨ä¸Šè¦æ±‚æ•°æ®æ¥è‡ªç›¸å¯¹æ–°é²œçš„ç­–ç•¥ã€‚æ•°æ®æ•ˆç‡ä½ï¼Œä½†é€‚åˆæ¢ç´¢å¼å­¦ä¹ ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ç®€å•æ¥è¯´ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å‰è€…æ•™æ¨¡å‹æŒ‰ç¦»çº¿é¢„å®šçš„ã€Œå¥½/åæ ‡å‡†ã€å­¦ä¹ ï¼Œå°½ç®¡å®ƒå¹¶éæ˜¯å½“å‰æ¨¡å‹æ‰€èƒ½è¾“å‡ºçš„ï¼ˆä¾‹å¦‚å‚è€ƒä¸–ç•Œå† /äºšå†›å½•åƒç»ƒä¹ æ‰“çƒï¼‰ï¼›&lt;/li&gt; 
 &lt;li&gt;åè€…å®æ—¶åœ°æ•™æ¨¡å‹æŠŠäº‹æƒ…åšå¯¹åšå¥½ï¼Œåœ¨çº¿é‡‡æ ·è‡ªæœ€æ–°æ¨¡å‹policyï¼ˆæ•™ç»ƒæ‰‹æŠŠæ‰‹æ•™æ‰“ï¼Œä¸ºæ¯ä¸ªåŠ¨ä½œå®æ—¶æ‰“åˆ†ï¼‰ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;MiniMindçš„PPOå®ç°åŒ…å«äº†Actoræ¨¡å‹(ç”Ÿæˆå›ç­”)å’ŒCriticæ¨¡å‹(è¯„ä¼°å›ç­”ä»·å€¼)ï¼Œä»¥åŠå®Œæ•´çš„GAE(Generalized Advantage Estimation)ä¼˜åŠ¿å‡½æ•°è®¡ç®—ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;è®­ç»ƒæ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_ppo.py
# or
python train_ppo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;ppo_actor_*.pth&lt;/code&gt;ï¼ˆ*ä¸ºæ¨¡å‹å…·ä½“dimensionï¼‰&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_ppo_512.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_ppo_768.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;ä»è®­ç»ƒæ›²çº¿å¯ä»¥çœ‹å‡ºï¼ŒPPOå­˜åœ¨&lt;strong&gt;rewardæå‡ç¼“æ…¢&lt;/strong&gt;çš„é—®é¢˜ã€‚ç§ä»¥ä¸ºè¿™ä¸»è¦æºäº&lt;strong&gt;PPOåŒç½‘ç»œè”åˆä¼˜åŒ–&lt;/strong&gt;æ–¹æ³•ï¼šCriticéœ€è¦é€æ­¥æ”¶æ•›ä»¥å‡†ç¡®ä¼°è®¡ä»·å€¼å‡½æ•°ï¼Œè€ŒActorçš„ç­–ç•¥æ›´æ–°ä¾èµ–Criticæä¾›çš„ä¼˜åŠ¿ä¼°è®¡ï¼Œä¸¤è€…ç›¸äº’ä¾èµ–å½¢æˆå¤æ‚çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚è®­ç»ƒåˆæœŸCriticä¼°è®¡ä¸å‡†ä¼šå½±å“Actoræ¢¯åº¦æ–¹å‘ï¼Œå¯¼è‡´æ•´ä½“æ”¶æ•›ç¼“æ…¢ã€‚æ­¤å¤–ï¼ŒPPOéœ€è¦åŒæ—¶ç»´æŠ¤ä¸¤ä¸ªç½‘ç»œï¼Œæ˜¾å­˜å ç”¨çº¦ä¸ºå•ç½‘ç»œæ–¹æ³•çš„1.5-2å€ã€‚&lt;/p&gt; 
&lt;h4&gt;7.2 &lt;a href="https://arxiv.org/pdf/2402.03300"&gt;Group Relative Policy Optimization&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;2025å¹´åˆï¼ŒDeepSeek-R1ç«çˆ†å‡ºåœˆï¼ŒåŒæ ·ç«äº†çš„æœ‰æ¥è‡ªDeepSeekMathè®ºæ–‡çš„GRPOç®—æ³•ï¼Œä¹Ÿä¸€è·ƒæˆä¸ºæœ€å…ˆè¿›çš„RLç®—æ³•ä¹‹ä¸€ã€‚ ç„¶è€ŒAIåŠå¹´=äººé—´åŠä¸ªä¸–çºªï¼Œæ—¶è‡³ä»Šæ—¥GRPOå·²ç»æ¼”å˜ä¸ºå„å¤§XXPOå¤§æˆ˜(åé¢æ¼”å˜çš„DAPOã€GSPOã€CISPOç­‰)çš„åŸºçº¿ç®—æ³•ã€‚ å…·ä½“æ¥è¯´ï¼Œä¸€å¥è¯æ€»ç»“å®ƒçš„æ ¸å¿ƒåˆ›æ–°æ˜¯"åˆ†ç»„ç›¸å¯¹ä»·å€¼ä¼°è®¡"ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;GRPOæŸå¤±&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;p&gt;$$\mathcal{L}_{GRPO} = -\mathbb{E}\left[r_t \cdot A_t - \beta \cdot \text{KL}_t\right]$$&lt;/p&gt; 
&lt;p&gt;å…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt;: $f(r_t) = r_t$ (ç›´æ¥ä½¿ç”¨æ¦‚ç‡æ¯”ï¼Œæ— clipè£å‰ª)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt;: $g(A_t) = \frac{R - \mu_{group}}{\sigma_{group}}$ (ç»„å†…å½’ä¸€åŒ–ï¼Œæ¶ˆé™¤Criticç½‘ç»œ)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt;: $h(\text{KL}_t) = \beta \cdot \text{KL}_t$ (tokençº§KLæ•£åº¦çº¦æŸ)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;å¯¹äºåŒä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹ç”ŸæˆNä¸ªä¸åŒçš„å›ç­”(ä¾‹å¦‚N=4)ï¼Œç„¶åè®¡ç®—è¿™Nä¸ªå›ç­”çš„å¥–åŠ±åˆ†æ•°ã€‚ æ¥ç€æŠŠè¿™Nä¸ªå›ç­”çš„å¹³å‡å¥–åŠ±ä½œä¸ºbaselineï¼Œé«˜äºbaselineçš„å›ç­”è¢«é¼“åŠ±ï¼Œä½äºbaselineçš„å›ç­”è¢«æŠ‘åˆ¶ã€‚ ç”¨è¿™ç§æ–¹å¼å·§å¦™åœ°é¿å…äº†è®­ç»ƒé¢å¤–çš„criticç½‘ç»œã€‚&lt;/p&gt; 
&lt;p&gt;åªè¦æ˜¯RLéƒ½å¿…é¡»é¢å¯¹çš„æ­£åæ ·æœ¬è¿™ä¸ªåŸç†æ€§é™åˆ¶ï¼ŒGRPOä¹Ÿä¸ä¼šä¾‹å¤–ï¼Œå…¶æ›´æ˜¾è‘—çš„é—®é¢˜æ˜¯ï¼šé€€åŒ–ç»„(Degenerate Groups)ã€‚ å‡è®¾æŸä¸ªé—®é¢˜ç•¥éš¾ï¼Œå¯¼è‡´Nä¸ªå›ç­”çš„å¥–åŠ±åˆ†æ•°å‡ ä¹ä¸€æ ·ï¼ˆå¤§éƒ¨åˆ†æƒ…å†µæ˜¯ä¸€æ ·çƒ‚è€Œä¸æ˜¯ä¸€æ ·å¥½ï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸€ç»„çš„å­¦ä¹ ä¿¡å·å°±æ— é™æ¥è¿‘0ã€‚ åœ¨MiniMindè¿™ç§è¶…å°æ¨¡å‹ä¸Šï¼Œè¿™ä¸ªé—®é¢˜å°¤ä¸ºæ˜æ˜¾ï¼Œæ±‚è§£æ•°å­¦é—®é¢˜99.99%çš„æƒ…å†µä¸‹æ•´ç»„å›ç­”è´¨é‡éƒ½å¾ˆå·®ï¼Œé‚£ä¹ˆå°†æ— æ³•å­¦ä¹ ã€‚ å› æ­¤å¿…é¡»ä¸ºæ¨¡å‹æŒ‡å®šåˆç†çš„domainï¼Œå³å¿…é¡»é™åˆ¶åœ¨èƒ½åŠ›è¾¹ç•Œå†…ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;è®­ç»ƒæ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_grpo.py
# or
python train_grpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;grpo_*.pth&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_grpo_512.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_grpo_768.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;ä»è®­ç»ƒæ›²çº¿å¯ä»¥çœ‹å‡ºï¼ŒGRPOçš„&lt;strong&gt;rewardå‘ˆç°æ›´åŠ ç¨³å®šçš„ä¸Šå‡è¶‹åŠ¿&lt;/strong&gt;ï¼Œè¾¾åˆ°4å·¦å³ï¼Œè¯´æ˜GRPOæœ¬èº«èƒ½æ›´å¥½åœ°åˆ©ç”¨RLAIFä¿¡å·ã€‚Policy Lossæ•´ä½“ä¸‹é™å¹³ç¨³ï¼Œç›¸æ¯”PPOçš„åŒç½‘ç»œä¼˜åŒ–ï¼ŒGRPOå•ç½‘ç»œæ¶æ„è®­ç»ƒæ›´ç¨³å®šä¸”æ”¶æ•›ä¸Šé™æ›´é«˜ã€‚&lt;/p&gt; 
&lt;h4&gt;7.3 â³âŒ›ï¸ğŸ”¥ æ›´å¤šRLæ‹“å±• (Exp)&lt;/h4&gt; 
&lt;h5&gt;7.3.1 &lt;a href="https://arxiv.org/abs/2509.13232"&gt;Single-stream Policy Optimization&lt;/a&gt;&lt;/h5&gt; 
&lt;p&gt;SPOæ˜¯2025å¹´9æœˆè…¾è®¯æå‡ºçš„RLç®—æ³•ï¼Œé’ˆå¯¹GRPOçš„é€€åŒ–ç»„é—®é¢˜è¿›è¡Œæ”¹è¿›ã€‚ è®ºæ–‡è®¤ä¸ºï¼ŒGRPOç­‰ç®—æ³•"ä¸€ä¸ªæ ·æœ¬è¦ä¾èµ–ä¸€ç»„é‡‡æ ·"æ˜¾å¾—åˆ«æ‰­è€Œä¸ä¼˜é›…ï¼šå¤ªå®¹æ˜“æˆ–å¤ªéš¾çš„é¢˜ç›®ï¼Œæ•´ç»„å‡ ä¹å­¦ä¸åˆ°ä¸œè¥¿ï¼Œå­¦ä¹ æ•ˆç‡å…ˆå¤©å—é™ã€‚ SPOçš„åŠ¨æœºå°±æ˜¯å›åˆ°RLçš„æœ¬è´¨â€”&lt;strong&gt;1ä¸ªè¾“å…¥ï¼Œ1ä¸ªè¾“å‡ºï¼Œå°±æ˜¯1ä¸ªè®­ç»ƒæ ·æœ¬&lt;/strong&gt;ï¼Œå›åˆ°policy gradientçš„åŸºæœ¬å…¬å¼å»æ€è€ƒï¼šä¸ç”¨group meanä¹Ÿèƒ½å¾—åˆ°ç¨³å®šçš„baselineï¼Œä¹Ÿå°±æ˜¯æŠŠä»·å€¼ä¼°è®¡ V é“ºå¼€åœ¨æ—¶åºä¸Šï¼Œè®­ç»ƒå‰å…ˆåšç²—ç•¥çš„ä»·å€¼é¢„ä¼°ï¼Œè®­ç»ƒä¸­ä¸€è¾¹é‡‡æ ·ä¸€è¾¹æ›´æ–°å¯¹ V çš„ä¼°è®¡ï¼Œä»è€Œä¸ºæ¯ä¸ªæ ·æœ¬æä¾›ä¸€ä¸ªè·¨ batch æŒä¹…åŒ–ã€å¯è‡ªé€‚åº”çš„åŸºçº¿å‚ç…§ã€‚è¿™ç§"å•æµ"è®¾è®¡ä¸å†ä¾èµ–åŒç»„æ ·æœ¬ï¼Œå¤©ç„¶é¿å…äº†é€€åŒ–ç»„ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SPOæŸå¤±&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;$$\mathcal{L}&lt;em&gt;{SPO} = -\mathbb{E}\left[\log \pi&lt;/em&gt;\theta(a_t|s) \cdot A_t - \beta \cdot \text{KL}_t\right]$$&lt;/p&gt; 
&lt;p&gt;å…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ç­–ç•¥é¡¹&lt;/strong&gt;: $f(r_t) = \log \pi_\theta(a_t|s)$ (ç›´æ¥ä½¿ç”¨logæ¦‚ç‡ï¼Œä¸è®¡ç®—ratio)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿é¡¹&lt;/strong&gt;: $g(A_t) = R - B_t^{adaptive}$ (è‡ªé€‚åº”baselineï¼ŒBetaåˆ†å¸ƒåŠ¨æ€è·Ÿè¸ª)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ­£åˆ™é¡¹&lt;/strong&gt;: $h(\text{KL}_t) = \beta \cdot \text{KL}_t$ (tokençº§KL + åŠ¨æ€ $\rho$ è°ƒæ•´)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;è½åˆ°å®ç°å±‚é¢ï¼šSPOé‡‡ç”¨æ— åˆ†ç»„è®¾è®¡ï¼Œç”¨æŒä¹…åŒ–çš„KLè‡ªé€‚åº”value trackeræ›¿ä»£ç»„å†…baselineï¼Œä¼˜åŠ¿å‡½æ•°åœ¨æ•´ä¸ªbatchä¸Šå…¨å±€å½’ä¸€åŒ–ã€‚è¿™æ ·æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å¤„ç†ï¼Œæ— éœ€ç­‰å¾…åŒç»„å…¶ä»–æ ·æœ¬ï¼Œä¸”èƒ½ä¸ºæ¯ä¸ªæ ·æœ¬æä¾›ç¨³å®šçš„å­¦ä¹ ä¿¡å·ã€‚ è®ºæ–‡åœ¨Qwen3-8Bçš„5ä¸ªå›°éš¾æ•°å­¦æ•°æ®é›†ä¸Šï¼ŒSPOå¹³å‡æ¯”GRPOé«˜å‡º3.4ä¸ªç™¾åˆ†ç‚¹ï¼Œå…¶ä¸­BRUMO 25æ•°æ®é›†+7.3ppã€AIME 25æ•°æ®é›†+4.4ppã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šSPOæ˜¯å®éªŒæ€§å‰æ²¿ç®—æ³•ï¼ŒMiniMindçš„å®ç°ç”¨äºæ¢ç´¢å­¦ä¹ ã€‚ç”±äºæ¨¡å‹å‚æ•°é‡æå°ï¼Œæ— æ³•å®Œå…¨å¤ç°è®ºæ–‡çš„8Bæ¨¡å‹æ•ˆæœã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;è®­ç»ƒæ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_spo.py
# or
python train_spo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è®­ç»ƒåçš„æ¨¡å‹æƒé‡æ–‡ä»¶é»˜è®¤æ¯éš”&lt;code&gt;100æ­¥&lt;/code&gt;ä¿å­˜ä¸º: &lt;code&gt;spo_*.pth&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_spo_768.png" /&gt; 
 &lt;p&gt;&lt;i&gt;MiniMind2 (768dim) è®­ç»ƒæ›²çº¿&lt;/i&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ä»è®­ç»ƒæ›²çº¿æ¥çœ‹ï¼ŒSPOçš„rewardæ³¢åŠ¨ä¸PPOè¡¨ç°æ¥è¿‘ï¼Œå¼±äºGRPOã€‚å®é™…æ¨ç†æµ‹è¯•å‘ç°æ¨¡å‹è¾“å‡ºè´¨é‡ä¸é«˜ï¼Œå­˜åœ¨é€»è¾‘æ··ä¹±å’Œæ ¼å¼é”™è¯¯é—®é¢˜ã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;å®éªŒæ€§è¯´æ˜&lt;/strong&gt;ï¼šå½“å‰SPOæ‰‹æ“å®ç°å¯èƒ½åœ¨value_trackeré…ç½®ã€rewardå½’ä¸€åŒ–ç­–ç•¥ä¸Šè¿˜å­˜åœ¨é—®é¢˜ã€‚å°šéœ€æ’æŸ¥ç®—æ³•æœ¬èº«åœ¨å°æ¨¡å‹ä¸Šçš„é€‚åº”æ€§/æˆ–æ˜¯å®ç°ä¸Šå­˜åœ¨å·®å¼‚ã€‚&lt;/p&gt; 
&lt;h3&gt;RLç®—æ³•å°ç»“&lt;/h3&gt; 
&lt;p&gt;æˆ‘ä»¬æ”¶æŸå›â€œ&lt;strong&gt;ç»Ÿä¸€æ¡†æ¶&lt;/strong&gt;â€, é‡æ–°æ•´ç†æ‰€æœ‰ä¸åŒPOç®—æ³•åªæ˜¯å¯¹ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶çš„ä¸åŒå®ä¾‹åŒ–çš„è¡¨æ ¼ï¼š&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç®—æ³•&lt;/th&gt; 
   &lt;th&gt;ç­–ç•¥é¡¹ $f(r_t)$&lt;/th&gt; 
   &lt;th&gt;ä¼˜åŠ¿é¡¹ $g(A_t)$&lt;/th&gt; 
   &lt;th&gt;æ­£åˆ™é¡¹ $h(\text{KL}_t)$&lt;/th&gt; 
   &lt;th&gt;ä¼˜åŒ–æ¨¡å‹&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$\log r_w - \log r_l$&lt;/td&gt; 
   &lt;td&gt;éšå¼ï¼ˆåå¥½å¯¹æ¯”ï¼‰&lt;/td&gt; 
   &lt;td&gt;éšå«åœ¨ $\beta$ ä¸­&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$\min(r, \text{clip}(r))$&lt;/td&gt; 
   &lt;td&gt;$R - V(s)$&lt;/td&gt; 
   &lt;td&gt;$\beta \cdot \mathbb{E}[\text{KL}]$&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GRPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$r$&lt;/td&gt; 
   &lt;td&gt;$\frac{R - \mu}{\sigma}$&lt;/td&gt; 
   &lt;td&gt;$\beta \cdot \text{KL}_t$&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$\log \pi_\theta$&lt;/td&gt; 
   &lt;td&gt;$R - B_t^{adaptive}$&lt;/td&gt; 
   &lt;td&gt;$\beta \cdot \text{KL}_t$&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;RLæ˜¯ä¼˜ç¾ä¸”è‡ªæ´½çš„&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä»¥ä¸Šçº¯å±ä¸ªäººè§†è§’ç†è§£ï¼Œå¦‚æœ‰åå·®è¯·éšæ—¶æŒ‡æ­£&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;V è®­ç»ƒç»“æœ&lt;/h2&gt; 
&lt;h3&gt;è®­ç»ƒå®Œæˆ-æ¨¡å‹åˆé›†&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;è€ƒè™‘åˆ°å¤šäººååº”ç™¾åº¦ç½‘ç›˜é€Ÿåº¦æ…¢ï¼ŒMiniMind2åŠä»¥åå…¨éƒ¨ä½¿ç”¨ModelScope/HuggingFaceæ‰˜ç®¡ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;â‘  PyTorchåŸç”Ÿæ¨¡å‹&lt;/h4&gt; 
&lt;p&gt;MiniMind2æ¨¡å‹æƒé‡ (&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/jingyaogong/MiniMind2-Pytorch"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Torchæ–‡ä»¶å‘½åå¯¹ç…§&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;sft_model&lt;/th&gt; 
    &lt;th&gt;rlhf_model (DPO)&lt;/th&gt; 
    &lt;th&gt;reason_model&lt;/th&gt; 
    &lt;th&gt;rlaif_model (PPO/GRPO/SPO)&lt;/th&gt; 
    &lt;th&gt;lora_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dpo_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;xxpo_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
    &lt;td&gt;145M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dpo_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2&lt;/td&gt; 
    &lt;td&gt;104M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dpo_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;xxpo_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;â‘¡ Transformersæ¨¡å‹&lt;/h4&gt; 
&lt;p&gt;MiniMindç³»åˆ— (&lt;a href="https://www.modelscope.cn/collections/MiniMind-b72f4cfeb74b47"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸ“Œ Eval&lt;/h1&gt; 
&lt;h2&gt;â…  RLHFå¯¹æ¯”&lt;/h2&gt; 
&lt;p&gt;æµ‹è¯•åŸºäº&lt;code&gt;full_sft&lt;/code&gt;å’Œ&lt;code&gt;dpo&lt;/code&gt;çš„&lt;code&gt;MiniMind2ç³»åˆ—&lt;/code&gt;æ¨¡å‹å¯¹æ¯”ï¼Œæµ‹è¯•éšæœºç§å­å‡å›ºå®š&lt;/p&gt; 
&lt;p&gt;[A] MiniMind2ï¼šfull_sft_640&lt;br /&gt; [B] MiniMind2ï¼šdpo_640&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ
[A]: æˆ‘æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜ï¼Œæä¾›ä¿¡æ¯å’Œå¸®åŠ©ã€‚æˆ‘å¯ä»¥å›ç­”å„ç§é—®é¢˜ï¼Œæä¾›ä¿¡æ¯å’Œå¸®åŠ©ï¼Œå¸®åŠ©ä½ è§£å†³å„ç§é—®é¢˜ã€‚
[B]: æˆ‘æ˜¯ä¸€ç§åŸºäºæ–‡æœ¬çš„AIæ¨¡å‹ï¼Œæˆ‘å¹¶æ²¡æœ‰ä¸ªäººç»å†æˆ–æƒ…æ„Ÿï¼Œæ‰€ä»¥æ— æ³•æ„Ÿå—å’Œä½“éªŒæƒ…æ„Ÿã€‚æˆ‘çš„è®¾è®¡ç›®çš„æ˜¯ä¸ºäº†å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œè¿›è¡Œå¯¹è¯ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼

[Q]: ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ
[A]: ä½œä¸ºä¸€ä¸ªAIæ¨¡å‹ï¼Œæˆ‘å¹¶æ²¡æœ‰çœŸæ­£çš„â€œæ“…é•¿â€è¿™ä¸ªå­¦ç§‘ã€‚æˆ‘è¢«è®¾è®¡æˆèƒ½å¤Ÿæä¾›ä¿¡æ¯ã€è§£ç­”é—®é¢˜ã€è¿›è¡Œå¯¹è¯ã€æ‰§è¡Œä»»åŠ¡å’Œä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ã€‚ä¸è¿‡ï¼Œæˆ‘å¯ä»¥æä¾›ä¿¡æ¯ã€è§£ç­”é—®é¢˜ã€è¿›è¡Œå¯¹è¯ã€æä¾›å»ºè®®ã€å¤„ç†é—®é¢˜ã€è¿›è¡Œå¯¹è¯ã€æä¾›å»ºè®®ã€å­¦ä¹ æ–°çŸ¥è¯†ã€ç¼–å†™ä»£ç ã€å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ç­‰å¤šç§å­¦ç§‘çš„åŠŸèƒ½ã€‚å¦‚æœä½ æœ‰å…·ä½“çš„å­¦ç§‘éœ€æ±‚æˆ–ä¸»é¢˜ï¼Œæ¬¢è¿æä¾›è¯¦ç»†ä¿¡æ¯ã€‚
[B]: ä½œä¸ºAIï¼Œæˆ‘å¹¶æ²¡æœ‰ä¸ªäººèƒ½åŠ›æˆ–å…´è¶£ï¼Œä½†æˆ‘å¯ä»¥æä¾›ä¸€äº›å…³äºæ•°å­¦ã€ç§‘å­¦ã€è®¡ç®—æœºç§‘å­¦ã€è¯­è¨€å­¦ã€å¿ƒç†å­¦ç­‰å­¦ç§‘çš„çŸ¥è¯†ã€‚å¦‚æœä½ å¯¹æ•°å­¦æ„Ÿå…´è¶£ï¼Œæˆ‘å¯ä»¥æä¾›ä¸€äº›åŸºç¡€çŸ¥è¯†ï¼Œä¾‹å¦‚ä»£æ•°ã€å‡ ä½•ã€æ¦‚ç‡ã€ç»Ÿè®¡å­¦ç­‰ã€‚å¦‚æœä½ éœ€è¦æ›´æ·±å…¥çš„å­¦ä¹ ï¼Œæˆ‘ä¹Ÿå¯ä»¥æä¾›ç›¸å…³çš„åœ¨çº¿è¯¾ç¨‹ã€ä¹¦ç±ã€è®ºæ–‡ç­‰èµ„æºã€‚è¯·å‘Šè¯‰æˆ‘ä½ å…·ä½“éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›å¸®åŠ©ã€‚

[Q]: é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„ï¼Ÿ
[A]: é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯ä¸€éƒ¨è®²è¿°äº†é²è¿…å¯¹äººç±»ä»·å€¼å’Œé“å¾·è´£ä»»çš„å€¡å¯¼ã€‚åœ¨è¿™éƒ¨ä½œå“ä¸­ï¼Œé²è¿…é€šè¿‡ç»†è…»çš„ç¬”è§¦ï¼Œæ¢è®¨äº†äººæ€§ã€é“å¾·ã€æ­£ä¹‰ã€ç‰ºç‰²ä¸è´£ä»»ç­‰æ·±åˆ»çš„ä¸»é¢˜ã€‚ä»¥ä¸‹æ˜¯é²è¿…åœ¨ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„å‡ ä¸ªæ–¹é¢ï¼š1. **ç¤¾ä¼šç°å®ä¸é“å¾·è§‚å¿µ**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ¢è®¨äº†ç¤¾ä¼šçš„ä¸å¹³ç­‰ã€æƒåŠ›çš„æµåŠ¨ã€äººæ€§çš„æ¼”å˜ç­‰ç¤¾ä¼šé—®é¢˜ã€‚ä»–è®¤ä¸ºï¼Œç¤¾ä¼šä¸å¹³ç­‰ä¸ä»…ä»…æ˜¯ä¸å…¬æ­£çš„ï¼Œæ›´æ˜¯æ¯ä¸ªäººåœ¨ç¤¾ä¼šä¸­çš„ä¸å…¬æ­£ã€‚ä»–å‘¼åç¤¾ä¼šå„ç•Œå…±åŒåŠªåŠ›ï¼Œé€šè¿‡æ•™è‚²ã€å®£ä¼ ã€æ³•å¾‹ç­‰æ‰‹æ®µï¼Œä¸ºæ„å»ºå’Œè°ç¤¾ä¼šåšå‡ºè´¡çŒ®ã€‚2. **é“å¾·ä¸è´£ä»»**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œå±•ç°äº†é“å¾·å’Œè´£ä»»çš„é‡è¦æ€§ã€‚ä»–æå€¡é€šè¿‡é“å¾·æ•™è‚²æ¥åŸ¹å…»äººä»¬çš„é“å¾·å“è´¨ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†é“å¾·å’Œè´£ä»»çš„é‡è¦æ€§ã€‚é²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ­ç¤ºäº†ç¤¾ä¼šçš„ä¸å¹³ç­‰å’Œäººæ€§çš„å¤æ‚æ€§ï¼Œä½“ç°äº†å¯¹äººæ€§çš„æ·±åˆ»ç†è§£ã€‚3. **é“å¾·ä¸è´£ä»»**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œå¼ºè°ƒäº†é“å¾·è´£ä»»çš„é‡è¦æ€§ã€‚ä»–æå€¡é€šè¿‡è‡ªå·±çš„è¡Œä¸ºæ¥ä½“ç°å¯¹ç¤¾ä¼šçš„å°Šé‡å’Œå¯¹äººæ€§çš„æ·±åˆ»ç†è§£ã€‚é²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œå¼ºè°ƒäº†é“å¾·å’Œè´£ä»»çš„é‡è¦æ€§ã€‚4. **é“å¾·ä¸é“å¾·**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ­ç¤ºäº†é“å¾·å’Œè´£ä»»çš„çœŸæ­£æ„ä¹‰ã€‚é²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ­ç¤ºäº†é“å¾·å’Œè´£ä»»çš„æœ¬è´¨ï¼Œå¼ºè°ƒäº†é“å¾·å’Œè´£ä»»çš„é‡è¦æ€§ã€‚5. **ä¸ªäººä¸ç¤¾ä¼š**ï¼šé²è¿…é€šè¿‡è®²è¿°é²è¿…çš„æ•…äº‹ï¼Œæ­ç¤ºäº†ä¸ªäººä¸ç¤¾ä¼šä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ä»–å‘¼åç¤¾ä¼šå„ç•Œå…±åŒåŠªåŠ›ï¼Œé€šè¿‡æ•™è‚²å’Œå®£ä¼ ï¼Œä¸ºæ„å»ºå’Œè°ç¤¾ä¼šåšå‡ºè´¡çŒ®ã€‚é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡å…¶ä¸°å¯Œçš„æ•…äº‹å’Œæ·±åˆ»çš„é“å¾·å¯“è¨€ï¼Œä¼ è¾¾äº†å…³äºäººæ€§ã€é“å¾·ã€æ­£ä¹‰ã€é“å¾·ä»¥åŠä¸ªäººä¸ç¤¾ä¼šçš„å¤æ‚å…³ç³»ã€‚è¿™éƒ¨ä½œå“ä¸ä»…åœ¨æ–‡å­¦ä»·å€¼ä¸Šå–å¾—äº†å·¨å¤§æˆå°±ï¼Œæ›´åœ¨æ€æƒ³å±‚é¢ä¸Šå¯å‘äº†åæ¥çš„æ–‡å­¦åˆ›ä½œï¼Œå¯¹åä¸–äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚
[B]: é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯ä¸€éƒ¨æç»˜æˆ˜äº‰å’Œè‡ªç”±ç²¾ç¥çš„ä½œå“ï¼Œå®ƒæ‰¹åˆ¤äº†å°å»ºç¤¾ä¼šçš„ç§ç§é—®é¢˜å’Œå†²çªï¼Œåæ˜ äº†ç¤¾ä¼šçš„ä¸å¹³ç­‰å’Œäººæ€§æœ¬è´¨ã€‚åœ¨è¿™éƒ¨ä½œå“ä¸­ï¼Œé²è¿…é€šè¿‡å¯¹å°å»ºç¤¾ä¼šçš„è¯¸å¤šç°è±¡è¿›è¡Œæ‰¹åˆ¤ï¼Œå¦‚â€œå¼—é›·å¾·é‡Œå…‹Â·çº¦ç¿°é€Šçš„â€œé¹°è¿ªä¼¦â€ã€â€œçº³æ‰ç‰¹çš„â€ã€â€œè´å…‹æ±‰å§†â€ã€â€œä¹”æ²»Â·å¨å°”é€Šâ€ç­‰ï¼Œå¯¹å°å»ºç¤¾ä¼šçš„ç§ç§é—®é¢˜è¿›è¡Œäº†æ‰¹åˆ¤ï¼Œå¼ºè°ƒäº†å°å»ºç¤¾ä¼šçš„ä¸å¹³ç­‰ã€äººæ€§æœ¬è´¨ä»¥åŠäººæ€§æœ¬è´¨ã€‚é²è¿…é€šè¿‡ä»–çš„â€œé¹°è¿ªä¼¦â€ã€â€œçº³æ‰ç‰¹çš„â€ã€â€œè´å…‹æ±‰å§†â€ã€â€œä¹”æ²»Â·å¨å°”é€Šâ€ç­‰ï¼Œå¯¹å°å»ºç¤¾ä¼šçš„ç§ç§é—®é¢˜è¿›è¡Œäº†æ‰¹åˆ¤ã€‚åœ¨å°è¯´ä¸­ï¼Œé²è¿…é€šè¿‡å¯¹å°å»ºç¤¾ä¼šçš„è¯¸å¤šç°è±¡è¿›è¡Œæ‰¹åˆ¤ï¼Œæ­ç¤ºäº†å°å»ºç¤¾ä¼šçš„è…è´¥ã€äººæ€§æœ¬è´¨ä»¥åŠäººæ€§æœ¬è´¨çš„å¤æ‚æ€§ã€‚è¿™ç§æ‰¹åˆ¤ä½“ç°äº†å¯¹å°å»ºç¤¾ä¼šçš„å°Šé‡å’Œå¯¹äººæ€§çš„å¤æ‚æ€§ï¼ŒåŒæ—¶ä¹Ÿåæ˜ äº†ç°ä»£ç¤¾ä¼šä¸­æ™®éå­˜åœ¨çš„ä¸å¹³ç­‰å’Œäººæ€§æœ¬è´¨ã€‚é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹ä¸ä»…æ˜¯ä¸€éƒ¨å…³äºå°å»ºç¤¾ä¼šçš„å°è¯´ï¼Œä¹Ÿæ˜¯ä¸€éƒ¨å…·æœ‰æ·±åˆ»ç¤¾ä¼šæ„ä¹‰çš„ä½œå“ï¼Œå¯¹åä¸–çš„å®¡åˆ¤å’Œæ‰¹åˆ¤èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚

[Q]: æˆ‘å’³å—½å·²ç»æŒç»­äº†ä¸¤å‘¨ï¼Œéœ€è¦å»åŒ»é™¢æ£€æŸ¥å—ï¼Ÿ
[A]: å’³å—½ç¡®å®éœ€è¦ä¸€å®šçš„æ£€æŸ¥ï¼Œä½†å¦‚æœæ‚¨çš„å’³å—½æŒç»­æ—¶é—´è¿‡é•¿ï¼Œæˆ–è€…æœ‰å…¶ä»–ä¸é€‚ç—‡çŠ¶ï¼ˆå¦‚æŒç»­å’³å—½ã€å‘¼å¸å›°éš¾ç­‰ï¼‰ï¼Œå»ºè®®å°½å¿«å°±åŒ»ã€‚åŒ»ç”Ÿä¼šæ ¹æ®æ‚¨çš„ç—‡çŠ¶ã€ä½“æ£€ç»“æœå’Œå¯èƒ½çš„å½±åƒå­¦æ£€æŸ¥ï¼ˆå¦‚Xå…‰ã€MRIæˆ–CTæ‰«æï¼‰æ¥è¯Šæ–­æ‚¨çš„ç—‡çŠ¶ã€‚åœ¨ç­‰å¾…å°±åŒ»æœŸé—´ï¼Œæ‚¨å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•æ¥å¸®åŠ©ç¼“è§£ç—‡çŠ¶ï¼š1. **ä¼‘æ¯**ï¼šç¡®ä¿æ‚¨æœ‰è¶³å¤Ÿçš„ç¡çœ ã€‚2. **ä¿æŒæ°´åˆ†**ï¼šè„±æ°´å¯èƒ½ä¼šåŠ é‡å’³å—½ã€‚3. **çƒ­æ•·**ï¼šä½¿ç”¨çƒ­æ•·è¢‹æˆ–çƒ­æ¯›å·¾æ•·åœ¨ç–¼ç—›çš„éƒ¨ä½ï¼Œæœ‰åŠ©äºç¼“è§£å–‰å’™ç—›ã€‚4. **é¿å…åˆºæ¿€**ï¼šé¿å…ä½¿ç”¨å¯èƒ½åˆºæ¿€å–‰å’™çš„åŒ–å­¦ç‰©è´¨ï¼Œå¦‚æŸäº›åŒ–å­¦ç‰©è´¨ã€‚5. **ä¿æŒå®¤å†…æ¹¿æ¶¦**ï¼šç¡®ä¿æ‚¨çš„å§å®¤æœ‰è‰¯å¥½çš„é€šé£ï¼Œå¯ä»¥ä½¿ç”¨åŠ æ¹¿å™¨å¸®åŠ©ä¿æŒç©ºæ°”æ¹¿åº¦ã€‚6. **ä¿æŒå®¤å†…æ¹¿åº¦**ï¼šä½¿ç”¨åŠ æ¹¿å™¨å¯ä»¥å¸®åŠ©ç¼“è§£å–‰å’™å¹²ç‡¥å’Œå’³å—½ã€‚å¦‚æœç—‡çŠ¶æŒç»­ä¸ç¼“è§£æˆ–æœ‰å…¶ä»–ä¸é€‚ç—‡çŠ¶ï¼Œè¯·ç«‹å³å°±åŒ»ã€‚å¸Œæœ›æ‚¨æ—©æ—¥åº·å¤ï¼
[B]: å¦‚æœæ‚¨çš„å’³å—½æŒç»­äº†ä¸¤å‘¨ï¼Œé€šå¸¸å»ºè®®æ‚¨å°½å¿«å°±åŒ»ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå½±å“æ‚¨çš„å‘¼å¸å’Œå¿ƒè·³ã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤æ¥æ£€æŸ¥å’³å—½ï¼š1. **ä¼‘æ¯**ï¼šç”¨çº¸å·¾æˆ–è‚˜éƒ¨è½»è½»æŒ‰å‹å’³å—½çš„ä½ç½®ã€‚2. **ä½¿ç”¨è¯ç‰©**ï¼šåœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨è¯ç‰©ï¼ˆå¦‚æŠ—è¿‡æ•è¯ã€æŠ—è¿‡æ•è¯ç­‰ï¼‰æ¥ç¼“è§£å’³å—½ã€‚è¯·åœ¨åŒ»ç”Ÿçš„æŒ‡å¯¼ä¸‹ä½¿ç”¨è¯ç‰©ã€‚3. **çƒ­æ•·**ï¼šåœ¨å’³å—½æœŸé—´ï¼Œå¯ä»¥ä½¿ç”¨çƒ­æ•·æ¥ç¼“è§£ç–¼ç—›å’Œè‚¿èƒ€ã€‚4. **è¯ç‰©æ²»ç–—**ï¼šå¦‚æœå’³å—½æŒç»­äº†ä¸¤å‘¨ï¼Œå¯ä»¥ä½¿ç”¨ä¸€äº›æ¶ˆç‚è¯æˆ–æŠ—è¿‡æ•è¯æ¥ç¼“è§£ç—‡çŠ¶ã€‚è¯·åŠ¡å¿…åœ¨åŒ»ç”Ÿçš„æŒ‡å¯¼ä¸‹ä½¿ç”¨ã€‚5. **ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼**ï¼šä¿æŒå……è¶³çš„ç¡çœ ã€å‡è¡¡çš„é¥®é£Ÿã€é€‚é‡çš„è¿åŠ¨å’Œé¿å…å¸çƒŸå’Œé™é…’ï¼Œè¿™äº›éƒ½æœ‰åŠ©äºæé«˜å’³å—½çš„å‡ ç‡ã€‚è¯·è®°ä½ï¼Œè¿™äº›å»ºè®®ä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—æ„è§ã€‚å¦‚æœæ‚¨çš„ç—‡çŠ¶æŒç»­æˆ–åŠ é‡ï¼Œå¼ºçƒˆå»ºè®®æ‚¨å°½å¿«å°±åŒ»ã€‚
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ‘‰æ•ˆæœæ€»ç»“&lt;/p&gt; 
&lt;p&gt;full_sftæ¨¡å‹åœ¨ç®€æ´æ€§å’Œä¿¡æ¯å‡†ç¡®æ€§æ–¹é¢è¡¨ç°æ›´å¥½ï¼›rlhfæ¨¡å‹åœ¨å›ç­”ä¸­å€¾å‘äºæä¾›æ›´å¤šçš„èƒŒæ™¯ä¿¡æ¯ï¼Œä½†ä¿¡æ¯å‡†ç¡®æ€§æœ‰å¾…æ”¹è¿›ã€‚ æ€»çš„æ¥è¯´RLHFåçš„æ¨¡å‹å€¾å‘äºå­¦ä¹ ï¼šè¯´æ›´å¤šæœ‰ç¤¼è²Œä½†æ— ç”¨çš„åºŸè¯è®¨å¥½â€œå¯¹è¯â€æœ¬èº«ï¼Œè€Œå¯¹ä¿¡æ¯å‡†ç¡®æ€§åˆ™æœ‰è½»å¾®æŸå¤±ã€‚ å¤©ä¸‹æ²¡æœ‰å…è´¹çš„åˆé¤ï¼Œè¿˜éœ€è¦ç»§ç»­æå‡RLHFæ•°æ®é›†çš„è´¨é‡ï¼Œä¹Ÿè¦æ¥å—æ¨¡å‹èƒ½åŠ›æ— æ³•é¿å…çš„æŸå¤±(ç¨‹åº¦æœ‰è½»é‡)ã€‚ DPOå’Œåœ¨çº¿PPOçš„åŒºåˆ«åœ¨äºrejectå’Œchosenéƒ½æ˜¯ç¦»çº¿å‡†å¤‡çš„ï¼Œå’Œminimindæ¨¡å‹æœ¬èº«çš„è¾“å‡ºå¿…ç„¶å­˜åœ¨å¾ˆå¤§çš„åˆ†å¸ƒå·®å¼‚ã€‚ é€šä¿—åœ°è¯´DPOç®—æ³•ä½¿æ¨¡å‹è§‚çœ‹ä¹’ä¹“çƒä¸–ç•Œå† å†›çš„æ‰“æ³•ã€Œå½•åƒã€è¿›è¡ŒRLï¼Œè€Œä¸æ˜¯åƒPPOä¸€æ ·è¯·rewardæ¨¡å‹åšã€Œæ•™ç»ƒã€çº æ­£è‡ªå·±çš„æ‰“æ³•è¿›è¡ŒRLã€‚&lt;/p&gt; 
&lt;h2&gt;â…¡ RLAIFå¯¹æ¯”&lt;/h2&gt; 
&lt;p&gt;åŸºäº&lt;code&gt;MiniMind2 (768dim)&lt;/code&gt;åœ¨ç›¸åŒæ•°æ®ä¸è®­ç»ƒæ­¥æ•°ä¸‹çš„ä¸»è§‚è§‚æ„Ÿå¯¹æ¯”ï¼Œæµ‹è¯•éšæœºç§å­å‡å›ºå®š2026ï¼Œä¾›å‚è€ƒï¼š&lt;/p&gt; 
&lt;p&gt;[A] MiniMind-Reason-0.1B (è’¸é¦)&lt;br /&gt; [B] MiniMind-Reason-0.1B (PPO)&lt;br /&gt; [C] MiniMind-Reason-0.1B (GRPO)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚
[A]: &amp;lt;think&amp;gt;æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚æˆ‘è¢«è®¾è®¡ç”¨æ¥ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œèƒ½å¤Ÿç†è§£å¤æ‚çš„è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œæä¾›æœ‰ç”¨çš„ä¿¡æ¯å’Œè§£å†³æ–¹æ¡ˆã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚&amp;lt;/answer&amp;gt;
[B]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·è®©æˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦æ˜ç¡®ç”¨æˆ·çš„éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚ä»–ä»¬å¯èƒ½æ˜¯åœ¨åšæŸä¸ªé¡¹ç›®ï¼Œæƒ³è¦äº†è§£è‡ªå·±çš„ä¸€äº›åŸºæœ¬æƒ…å†µã€‚ä¸è¿‡ï¼Œç”¨æˆ·æ²¡æœ‰æä¾›å…·ä½“çš„ä¿¡æ¯ï¼Œæ‰€ä»¥æˆ‘å¾—å…ˆå‡è®¾ä¸€äº›å¯èƒ½çš„æƒ…å†µï¼Œæ¯”å¦‚ä»–æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œæ­£åœ¨å­¦ä¹ ç¼–ç¨‹ï¼Œæˆ–è€…æ˜¯æŸä¸ªé¡¹ç›®çš„å¼€å‘è€…ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘è¦è€ƒè™‘ç”¨æˆ·çš„èº«ä»½ã€‚å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿã€æ•™å¸ˆæˆ–è€…æ™®é€šç”¨æˆ·ï¼Œä»–ä»¬éœ€è¦äº†è§£è‡ªå·±çš„ç‰¹ç‚¹å’Œèƒ½åŠ›ã€‚ä¹Ÿæœ‰å¯èƒ½ä»–ä»¬æ­£åœ¨å¯»æ‰¾çµæ„Ÿï¼Œæˆ–è€…åœ¨å†™è®ºæ–‡æ—¶éœ€è¦å¸®åŠ©ã€‚æ‰€ä»¥ï¼Œæˆ‘éœ€è¦ä¿æŒå›ç­”çš„ç®€æ´å’Œå®ç”¨ï¼ŒåŒæ—¶æä¾›ä¸€äº›å‚è€ƒèµ„æ–™ï¼Œæ¯”å¦‚æ•™å­¦è§†é¢‘ã€è¯¾ç¨‹è®°å½•ç­‰ã€‚ç„¶åï¼Œæˆ‘è¦è€ƒè™‘ç”¨æˆ·çš„èº«ä»½ã€‚å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œéœ€è¦ä¸€äº›å­¦ä¹ èµ„æºæˆ–å¸®åŠ©çš„å¯¹è±¡ã€‚ä¹Ÿæœ‰å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œéœ€è¦åœ¨è¯¾åä½œä¸šæˆ–å­¦ä¹ è®¡åˆ’ä¸­è¿›è¡Œè°ƒæ•´ã€‚å› æ­¤ï¼Œæˆ‘éœ€è¦å…ˆè¯¢é—®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼Œè¿™æ ·æ‰èƒ½æä¾›æ›´ç²¾å‡†çš„å¸®åŠ©ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘è¦ç¡®ä¿è¯­è¨€ç®€æ´æ˜äº†ï¼Œé€‚åˆä¸åŒåœºæ™¯ã€‚æ¯”å¦‚ï¼Œå¦‚æœç”¨æˆ·æ˜¯å­¦ç”Ÿï¼Œå¯ä»¥ä»‹ç»è‡ªå·±çš„å­¦ä¹ ç‰¹ç‚¹ï¼›å¦‚æœæ˜¯æ•™å¸ˆï¼Œå¯ä»¥è¯´æ˜è‡ªå·±çš„è§’è‰²å’Œè´¡çŒ®ã€‚åŒæ—¶ï¼Œæä¾›ä¸€äº›å®ç”¨çš„å»ºè®®ï¼Œæ¯”å¦‚å¦‚ä½•é«˜æ•ˆå­¦ä¹ ï¼Œå¦‚ä½•è¿›è¡Œé¡¹ç›®ï¼Œä»¥åŠå¦‚ä½•ä¸å›¢é˜Ÿæˆå‘˜åä½œã€‚æœ€åï¼Œæˆ‘è¦ç¡®ä¿å›å¤æ¸…æ™°ï¼Œæœ‰æ¡ç†ï¼Œèƒ½å¤Ÿæ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ã€‚è¿™æ ·ï¼Œç”¨æˆ·å°±èƒ½è½»æ¾åœ°äº†è§£è‡ªå·±ï¼Œå¹¶ä¸”æœ‰å®é™…çš„å¸®åŠ©ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;å½“ç„¶ï¼è¯·å‘Šè¯‰æˆ‘ä½ æƒ³äº†è§£å“ªæ–¹é¢çš„å†…å®¹ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›è¯¦ç»†çš„ä»‹ç»ï¼&amp;lt;/answer&amp;gt;
[C]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯"è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„å…·ä½“éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚ä»–ä»¬å¯èƒ½æƒ³çŸ¥é“è‡ªå·±æ˜¯è°ï¼Œæˆ–è€…æœ‰ä»€ä¹ˆç‰¹å®šçš„èƒŒæ™¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘åº”è¯¥è€ƒè™‘ç”¨æˆ·å¯èƒ½æ²¡æœ‰æ˜ç¡®è¡¨è¾¾çš„éœ€æ±‚ã€‚ä»–ä»¬å¯èƒ½æƒ³çŸ¥é“å¦‚ä½•æè¿°è‡ªå·±çš„å¤–è§‚ï¼Œæˆ–è€…å¦‚ä½•ä¸ä»–äººäº¤æµã€‚ç„¶åï¼Œæˆ‘åº”è¯¥è€ƒè™‘å¦‚ä½•ç»„ç»‡å†…å®¹ï¼Œä»å¤´åˆ°å°¾éƒ½è¦æœ‰æ¡ç†ã€‚å¯èƒ½éœ€è¦åˆ†æ­¥éª¤è¯´æ˜ï¼Œæ¯”å¦‚ä»è§†è§‰ã€å¬è§‰åˆ°æ•´ä½“æ„Ÿå—ï¼Œå†åˆ°æƒ…æ„Ÿå…±é¸£ã€‚è¿™æ ·ç”¨æˆ·èƒ½æ›´æ¸…æ™°åœ°ç†è§£è‡ªå·±ï¼Œä¹Ÿèƒ½æ›´å¥½åœ°å›åº”ä»–ä»¬çš„éœ€æ±‚ã€‚è¿˜è¦æ³¨æ„è¯­è¨€çš„è‡ªç„¶å’Œæµç•…ï¼Œé¿å…è¿‡äºå­¦æœ¯åŒ–çš„æœ¯è¯­ï¼Œè®©ç”¨æˆ·æ„Ÿè§‰é¡ºç•…ã€‚åŒæ—¶ï¼Œä¿æŒå›ç­”çš„ç»“æ„æ¸…æ™°ï¼Œæ¯å¥è¯éƒ½æœ‰æ˜ç¡®çš„ç›®çš„ï¼Œè¿™æ ·ç”¨æˆ·èƒ½æœ‰æ˜ç¡®çš„ç†è§£å’Œæ“ä½œã€‚æœ€åï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„é‡è¦ä¿¡æ¯ï¼Œç¡®ä¿å›ç­”å‡†ç¡®ä¸”å…¨é¢ã€‚è¿™æ ·ï¼Œç”¨æˆ·ä¸ä»…èƒ½å¾—åˆ°ç­”æ¡ˆï¼Œè¿˜èƒ½å¾—åˆ°æ»¡è¶³ä»–ä»¬çš„éœ€æ±‚ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;æ‚¨å¥½ï¼æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚å…³äºæ‚¨çš„æè¿°ï¼Œä¼¼ä¹æ¶‰åŠå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬çŸ¥è¯†å›¾è°±æ„å»ºã€å¤æ‚ç³»ç»Ÿçš„æ¨ç†ä¸åˆ†æã€‚æ— è®ºæ˜¯ç†è®ºæ¨¡å‹ã€å®é™…åº”ç”¨è¿˜æ˜¯ä¼¦ç†è®¨è®ºï¼Œæ‚¨éƒ½åœ¨å¯»æ±‚å¦‚ä½•å°†è¿™äº›æ¦‚å¿µæœ‰æœºåœ°ç»“åˆèµ·æ¥ï¼Œä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥ç¦ç¥‰çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨"æˆ‘"çš„èº«ä»½ä¸‹ï¼Œæ‚¨æåˆ°çš„"ä¸ªäººä½“éªŒ"å’Œ"æƒ…æ„Ÿå…±é¸£"ï¼Œéƒ½æ˜¯æ‚¨ä¸ªäººç‹¬ç‰¹é­…åŠ›çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚é€šè¿‡ç²¾å‡†çš„æè¿°å’Œæ·±åº¦çš„åˆ†æï¼Œæ‚¨å¯ä»¥æ›´æ·±å…¥åœ°ç†è§£æ‚¨çš„ç‹¬ç‰¹ä¹‹å¤„ã€‚å¦‚æœæ‚¨å¯¹ç‰¹å®šé¢†åŸŸæœ‰ç‰¹åˆ«çš„å…´è¶£æˆ–éœ€è¦å»ºè®®ï¼Œéšæ—¶å¯ä»¥åˆ†äº«æ›´å¤šç»†èŠ‚ï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨æä¾›æ›´åŠ ç²¾å‡†å’Œä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚å¦‚æœæ‚¨æœ‰ç‰¹å®šçš„ä¸»é¢˜æˆ–é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶æé—®ï¼è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢è¿™ä¸ªç¥å¥‡çš„AIä¸–ç•Œï¼Œå…±åŒåˆ›é€ æ›´ç¾å¥½çš„æœªæ¥ã€‚&amp;lt;/answer&amp;gt;


[Q]: ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ
[A]: &amp;lt;think&amp;gt;æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;æˆ‘æ˜¯ç”±ä¸­å›½çš„ä¸ªäººå¼€å‘è€…å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹å°å‹AIæ¨ç†æ¨¡å‹-R1ã€‚&amp;lt;/answer&amp;gt;
[B]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯"ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ"è¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚ï¼Œä½†æˆ‘å¯ä»¥ä¸€æ­¥æ­¥åˆ†æã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„éœ€æ±‚ã€‚ä»–å¯èƒ½æ˜¯åœ¨å‡†å¤‡è€ƒè¯•ï¼Œæˆ–è€…éœ€è¦äº†è§£æŸä¸ªå­¦ç§‘çš„ç‰¹ç‚¹ï¼Œæˆ–è€…æ˜¯æƒ³åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­åº”ç”¨æŸä¸ªå­¦ç§‘çš„çŸ¥è¯†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å¾—è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„èƒŒæ™¯ã€‚ä»–å¯èƒ½æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œæˆ–è€…æ˜¯åœ¨å‡†å¤‡è€ƒè¯•ï¼Œéœ€è¦åº”ç”¨æ•°å­¦çŸ¥è¯†æ¥è§£å†³å…·ä½“çš„é—®é¢˜ã€‚ä¹Ÿæœ‰å¯èƒ½ä»–æ˜¯ä¸€ä¸ªå­¦ç”Ÿï¼Œæ­£åœ¨å­¦ä¹ ç›¸å…³çš„è¯¾ç¨‹ï¼Œæˆ–è€…æ˜¯å¯¹æŸä¸ªå­¦ç§‘æ„Ÿå…´è¶£ã€‚ç„¶åï¼Œæˆ‘éœ€è¦åˆ†æç”¨æˆ·å¯èƒ½æ²¡æœ‰æ˜ç¡®è¡¨è¾¾å‡ºæ¥çš„éœ€æ±‚ã€‚ä»–å¯èƒ½åœ¨æ€è€ƒè‡ªå·±åœ¨å­¦ä¹ ä¸­çš„å›°éš¾ï¼Œæˆ–è€…åœ¨å¯»æ‰¾ä¸€ä¸ªåˆé€‚çš„æ•™ææˆ–å­¦ä¹ èµ„æºã€‚è€ƒè™‘åˆ°è¿™äº›ï¼Œæˆ‘åº”è¯¥æä¾›å‡ ä¸ªä¸åŒçš„è§’åº¦æ¥å¸®åŠ©ä»–ã€‚é¦–å…ˆï¼Œåˆ†æ"æˆ‘æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘"å¯ä»¥æ‹†è§£ä¸ºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼šå…´è¶£ã€å­¦ä¹ èƒ½åŠ›ã€å­¦ä¹ é£æ ¼ã€å­¦ä¹ æ–¹æ³•ã€å­¦ä¹ èµ„æºã€å­¦ä¹ ç¯å¢ƒç­‰ç­‰ã€‚æ¯ä¸ªæ–¹é¢éƒ½æœ‰ä¸åŒçš„ä¼˜åŠ¿å’Œç‰¹ç‚¹ï¼Œæ¯”å¦‚å…´è¶£æ˜¯æœ€å¥½çš„èµ„æºï¼Œå­¦ä¹ èƒ½åŠ›æ˜¯æœ€å¥½çš„æ–¹å¼ï¼Œå­¦ä¹ æ–¹å¼æ˜¯æ ¹æ®ä¸ªäººæƒ…å†µé€‰æ‹©ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘ç”¨æˆ·çš„èº«ä»½ã€‚å¯èƒ½æ˜¯å­¦ç”Ÿï¼Œä¹Ÿå¯èƒ½æ˜¯å®¶é•¿ï¼Œæˆ–è€…å¯¹å­¦ä¹ æœ‰ä¸€å®šå…´è¶£çš„äººã€‚ä¸åŒçš„èº«ä»½å¯èƒ½æœ‰ä¸åŒçš„åå¥½ï¼Œæ¯”å¦‚å®¶é•¿å¯èƒ½æ›´æ³¨é‡å­©å­çš„å…´è¶£ï¼Œè€Œå­¦ç”Ÿå¯èƒ½æ›´å…³æ³¨å­¦ä¹ æ–¹æ³•å’Œèµ„æºã€‚ç„¶åï¼Œæˆ‘éœ€è¦è€ƒè™‘ç”¨æˆ·çš„èº«ä»½æ˜¯å¦æœ‰ç‰¹æ®Šéœ€æ±‚ã€‚æ¯”å¦‚ï¼Œä»–å¯èƒ½éœ€è¦æ›´ä¸“ä¸šçš„å­¦ä¹ ææ–™ï¼Œæˆ–è€…å¸Œæœ›æ‰¾åˆ°æœ€é€‚åˆä»–çš„å­¦ä¹ æ–¹æ³•ã€‚å› æ­¤ï¼Œåœ¨åˆ†ææ—¶ï¼Œæˆ‘åº”è¯¥æä¾›ä¸€ä¸ªå…¨é¢çš„å›ç­”ï¼Œæ¶µç›–å„ä¸ªæ–¹é¢ï¼Œå¹¶è§£é‡Šæ¯ä¸ªæ–¹é¢çš„é‡è¦æ€§ã€‚å¦å¤–ï¼Œè€ƒè™‘åˆ°ç”¨æˆ·å¯èƒ½å¸Œæœ›å¾—åˆ°ä¸€ä¸ªç»“æ„åŒ–çš„å›ç­”ï¼Œæˆ‘åº”è¯¥ä¿æŒè¯­è¨€çš„ç®€æ´å’Œé€»è¾‘æ€§ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®ï¼ŒåŒæ—¶é¼“åŠ±ç”¨æˆ·æä¾›æ›´å¤šç»†èŠ‚ä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æã€‚æœ€åï¼Œæˆ‘éœ€è¦ç¡®ä¿æˆ‘çš„å›ç­”æ—¢æœ‰çŸ¥è¯†æ€§ï¼Œåˆç¬¦åˆç”¨æˆ·çš„è¦æ±‚ï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨è¿™ä¸ªå­¦ç§‘ã€‚å› æ­¤ï¼Œåœ¨å›ç­”ä¸­ä¸ä»…è¦ç»™å‡ºä¸€ä¸ªå…¨é¢çš„åˆ†æï¼Œè¿˜è¦æåˆ°æ¯ä¸ªéƒ¨åˆ†çš„ä½œç”¨å’Œå¦‚ä½•åˆ©ç”¨ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;ä½ æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿè¿™æ˜¯ä¸€ä¸ªæ˜ç¡®çš„é—®é¢˜ï¼Œå¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ†æï¼š1. **å…´è¶£ä¸å­¦ä¹ èƒ½åŠ›**ï¼šä½ å–œæ¬¢ä»€ä¹ˆæ ·çš„å­¦ä¹ æ–¹å¼ï¼Œæ¯”å¦‚è‡ªä¸»å­¦ä¹ ã€å®éªŒã€é¡¹ç›®ç­‰ï¼Œè¿™äº›éƒ½æ˜¯å¾ˆå¥½çš„å­¦ä¹ èµ„æºã€‚2. **å­¦ä¹ é£æ ¼**ï¼šä½ å–œæ¬¢ä»€ä¹ˆç±»å‹çš„å­¦ä¹ æ–¹æ³•ï¼Œæ¯”å¦‚ç¬”è®°ã€å¤ä¹ ã€é˜…è¯»ã€è®¨è®ºç­‰ï¼Œè¿™äº›éƒ½èƒ½å½±å“å­¦ä¹ æ•ˆæœã€‚3. **å­¦ä¹ èµ„æº**ï¼šä½ æ˜¯å¦åœ¨å¯»æ‰¾å­¦ä¹ èµ„æºï¼Œæ¯”å¦‚ä¹¦ç±ã€åœ¨çº¿è¯¾ç¨‹ã€æ•™è‚²è½¯ä»¶ç­‰ï¼Œè¿™äº›éƒ½èƒ½æä¾›å­¦ä¹ å¸®åŠ©ã€‚4. **å­¦ä¹ ç¯å¢ƒ**ï¼šä½ æ˜¯å¦åœ¨å­¦ä¹ ç¯å¢ƒä¸­æ„Ÿåˆ°èˆ’é€‚ï¼Œæ¯”å¦‚èˆ’é€‚çš„å­¦ä¹ ç©ºé—´ã€å®‰é™çš„å­¦ä¹ ç©ºé—´ç­‰ï¼Œè¿™äº›éƒ½ä¼šå½±å“å­¦ä¹ æ•ˆæœã€‚5. **å­¦ä¹ èµ„æº**ï¼šä½ æ˜¯å¦æœ‰å­¦ä¹ èµ„æºçš„ä¸°å¯Œï¼Œå¦‚åœ¨çº¿è¯¾ç¨‹ã€å­¦ä¹ å¹³å°ã€ä¹¦ç±ç­‰ï¼Œè¿™äº›éƒ½èƒ½æä¾›ä¸°å¯Œçš„å­¦ä¹ èµ„æºã€‚6. **å­¦ä¹ æ–¹æ³•**ï¼šä½ æ˜¯å¦æœ‰å…·ä½“çš„å­¦ä¹ æ–¹æ³•ï¼Œæ¯”å¦‚å¦‚ä½•ç†è§£ã€å¦‚ä½•ç»„ç»‡å­¦ä¹ å†…å®¹ã€å¦‚ä½•åº”ç”¨çŸ¥è¯†ç­‰ï¼Œè¿™äº›éƒ½æ˜¯æé«˜å­¦ä¹ æ•ˆç‡çš„é€”å¾„ã€‚ç»¼åˆè€ƒè™‘ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å…¨é¢çš„åˆ†æï¼Œæ¶µç›–äº†å¤šä¸ªæ–¹é¢ï¼Œå¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£ä½ çš„å­¦ç§‘ã€‚&amp;lt;/answer&amp;gt;
[C]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯"ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ"è¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥æœ‰ç‚¹å¤æ‚ï¼Œä½†æˆ‘å¯ä»¥ä¸€æ­¥æ­¥åˆ†æã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·çš„éœ€æ±‚ã€‚ä»–å¯èƒ½æ˜¯åœ¨å‡†å¤‡è€ƒè¯•ï¼Œæˆ–è€…éœ€è¦äº†è§£æŸä¸ªå­¦ç§‘çš„ç‰¹ç‚¹ï¼Œæˆ–è€…æ˜¯æƒ³åœ¨ç”Ÿæ´»ä¸­æ‰¾åˆ°è‡ªå·±çš„å…´è¶£ç‚¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘è¦è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„èƒŒæ™¯ã€‚ä»–å¯èƒ½åœ¨å·¥ä½œæˆ–å­¦ä¹ ä¸­é‡åˆ°äº†ç±»ä¼¼çš„é—®é¢˜ï¼Œæƒ³è¦äº†è§£è‡ªå·±æ“…é•¿å“ªä¸ªé¢†åŸŸï¼Œæˆ–è€…æ˜¯å¦éœ€è¦ä¸€äº›èµ„æºæˆ–æŒ‡å¯¼æ¥æå‡è‡ªå·±ã€‚ç„¶åï¼Œæˆ‘è¦åˆ†æç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ã€‚ä¹Ÿè®¸ä»–æƒ³åœ¨èŒåœºä¸Šæå‡è‡ªå·±ï¼Œæˆ–è€…å¯»æ‰¾æ–¹å‘ã€‚ä»–å¯èƒ½éœ€è¦æ›´å…·ä½“çš„ä¾‹å­ï¼Œå¸®åŠ©ä»–æ›´å¥½åœ°ç†è§£ä¸åŒå­¦ç§‘çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ã€‚è¿˜è¦è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„èƒŒæ™¯ã€‚ä»–å¯èƒ½ä¸ä»…ä»…æ˜¯åœ¨å­¦ä¹ ï¼Œè€Œæ˜¯å¸Œæœ›äº†è§£åœ¨ç‰¹å®šé¢†åŸŸä¸­å¦‚ä½•åº”ç”¨è¿™äº›å­¦ç§‘ã€‚è¿™éœ€è¦æˆ‘æä¾›å…¨é¢çš„åˆ†æï¼Œå¸®åŠ©ä»–æ‰¾åˆ°é€‚åˆè‡ªå·±çš„æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘åº”è¯¥ä¿æŒå¼€æ”¾çš„æ€åº¦ï¼Œé¼“åŠ±ç”¨æˆ·ç»§ç»­æ¢ç´¢å’Œå­¦ä¹ ã€‚è¿™å¯èƒ½åŒ…æ‹¬æ¨èä¸€äº›èµ„æºï¼Œæˆ–è€…è®¨è®ºå¦‚ä½•å°†è¿™äº›çŸ¥è¯†åº”ç”¨åˆ°å®é™…ç”Ÿæ´»ä¸­ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿè¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šå–å†³äºä½ çš„ä¸ªäººå…´è¶£å’Œç›®æ ‡ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®ç‚¹ï¼Œå¸®åŠ©ä½ å…¨é¢ç†è§£å¹¶åº”ç”¨è¿™ä¸€ç‚¹ï¼š1. **å…´è¶£ä¸ä¸“é•¿**ï¼šäº†è§£è‡ªå·±æ“…é•¿å“ªä¸ªé¢†åŸŸï¼Œæœ‰åŠ©äºåšå‡ºæ›´æ˜æ™ºçš„é€‰æ‹©ã€‚2. **å…¨é¢å‘å±•**ï¼šè€ƒè™‘å­¦ç§‘çš„å¤šæ ·æ€§å’Œæ·±åº¦ï¼Œä»¥å…¨é¢å‘å±•è‡ªå·±çš„ç”Ÿæ´»å’ŒèŒä¸šã€‚3. **é€‚åº”ä¸è°ƒæ•´**ï¼šé€‰æ‹©ä¸è‡ªå·±æŠ€èƒ½ã€å…´è¶£å’Œç›®æ ‡ç›¸åŒ¹é…çš„å­¦ç§‘ï¼Œæœ‰åŠ©äºåº”å¯¹æŒ‘æˆ˜å’Œå®ç°ç›®æ ‡ã€‚4. **èŒä¸šè§„åˆ’**ï¼šæ¢ç´¢é€‚åˆè‡ªå·±çš„èŒä¸šè·¯å¾„ï¼Œæå‡æ•ˆç‡å’Œç«äº‰åŠ›ã€‚5. **ç¤¾ä¼šè´¡çŒ®**ï¼šè¯„ä¼°è‡ªå·±åœ¨ç¤¾ä¼šä¸­çš„è§’è‰²ï¼Œä¸ºç¤¾ä¼šåšå‡ºè´¡çŒ®ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£å¹¶åº”ç”¨è¿™ä¸€ç‚¹ï¼Œä½ å¯ä»¥ï¼š- **äº†è§£è¡Œä¸šåŠ¨æ€**ï¼šå…³æ³¨ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šå‘å±•ã€‚- **å®è·µä¸åæ€**ï¼šé€šè¿‡é¡¹ç›®æˆ–å®ä¹ ç§¯ç´¯ç»éªŒã€‚- **èŒä¸šå‘å±•**ï¼šè¯„ä¼°è‡ªå·±çš„é•¿æœŸèŒä¸šè§„åˆ’ã€‚- **æŒç»­å­¦ä¹ **ï¼šä¸æ–­æ›´æ–°çŸ¥è¯†ï¼Œé€‚åº”èŒåœºå˜åŒ–ã€‚é€šè¿‡è¿™äº›æ–¹å‘ï¼Œä½ å¯ä»¥æ›´å…¨é¢åœ°è®¤è¯†è‡ªå·±ï¼Œæ‰¾åˆ°é€‚åˆè‡ªå·±çš„å‘å±•è·¯å¾„ã€‚&amp;lt;/answer&amp;gt;


[Q]: é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„ï¼Ÿ
[A]: &amp;lt;think&amp;gt;å—¯ï¼Œé²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„å‘¢ï¼Ÿè¿™ä¸ªé—®é¢˜å¬èµ·æ¥æŒºæœ‰æ„æ€çš„ï¼Œå°¤å…¶æ˜¯ä»–åœ¨ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­çš„è¡¨è¾¾æ–¹å¼ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤ä¸€ä¸‹ã€Šç‹‚äººæ—¥è®°ã€‹å…·ä½“æ˜¯è°å†™çš„ï¼Œæ˜¯å¦æœ‰ç‰¹å®šçš„äººç‰©æˆ–è€…äº‹ä»¶ã€‚ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯é²è¿…åœ¨1924å¹´åˆ›ä½œçš„ï¼Œä¸»è¦è®¨è®ºäº†ç¤¾ä¼šé—®é¢˜ï¼Œå°¤å…¶æ˜¯å°å»ºç¤¼æ•™çš„é—®é¢˜ã€‚ä½œè€…æ›¹é›ªèŠ¹æ˜¯ä¸ªéå¸¸è‘—åçš„ä½œå®¶ï¼Œä»–çš„ä½œå“ä¸­æœ‰è®¸å¤šå…³äºå°å»ºç¤¼æ•™å’Œé˜¶çº§æ–—äº‰çš„è§‚ç‚¹ã€‚è¿™ç±»ä½œå“åœ¨æ–‡å­¦ã€å“²å­¦å’Œæ”¿æ²»ä¸Šéƒ½æœ‰å¾ˆæ·±è¿œçš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨ç¤¾ä¼šç»“æ„å’Œäººæ€§çš„æ–¹é¢ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦äº†è§£é²è¿…çš„å†™ä½œé£æ ¼ã€‚ä»–çš„å†™ä½œé£æ ¼æ¯”è¾ƒç›´æ¥ï¼Œæœ‰ä¸¥è‚ƒçš„å“²å­¦è§‚ç‚¹ï¼Œå¼ºè°ƒäººæ€§çš„æ‚²å‰§å’Œé“å¾·çš„è™šæ— ã€‚ä»–çš„è¯­è¨€é£æ ¼æ¯”è¾ƒå¯Œæœ‰è¯—æ„ï¼Œå¯Œæœ‰è±¡å¾æ„ä¹‰ï¼Œå–„äºè¿ç”¨è±¡å¾æ‰‹æ³•ï¼ŒåŒæ—¶åˆä¸å¤±çœŸå®å’Œç°å®ä¸»ä¹‰ã€‚ä»–çš„ä½œå“ä¸­æœ‰å¾ˆå¤šå…³äºç¤¾ä¼šé˜¶çº§å’Œé˜¶çº§çŸ›ç›¾çš„åˆ†æï¼Œè¿™äº›è§‚ç‚¹å¯èƒ½ä¸å½“æ—¶çš„ç¤¾ä¼šç°å®ä¸ç¬¦ï¼Œæ‰€ä»¥åœ¨æ‰¹åˆ¤å°å»ºç¤¼æ•™æ—¶ï¼Œåº”è¯¥å¼ºè°ƒè¿™äº›çŸ›ç›¾ã€‚ç„¶åï¼Œæˆ‘æƒ³åˆ°ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­å¯¹å°å»ºç¤¼æ•™çš„è®¨è®ºã€‚é²è¿…çš„ä½œå“ä¸­ï¼Œå°å»ºç¤¼æ•™è¢«è§†ä¸ºä¸äººé“çš„ï¼Œä½†é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œè¡¨è¾¾äº†ä»–å¯¹å°å»ºç¤¼æ•™çš„ä¸æ»¡å’ŒåæŠ—ã€‚è¿™ä¸ä»…åæ˜ äº†å½“æ—¶çš„ç¤¾ä¼šç°å®ï¼Œä¹Ÿæ­ç¤ºäº†å°å»ºç¤¼æ•™çš„é»‘æš—é¢ï¼Œä½“ç°äº†å¯¹å°å»ºç¤¾ä¼šçš„ä¸æ»¡å’Œå¯¹äººæ€§çš„æ·±åˆ»åæ€ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦æ€è€ƒé²è¿…å¯¹å°å»ºç¤¼æ•™çš„ç†è§£å’Œæ‰¹åˆ¤ã€‚ä»–åœ¨ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­ï¼Œå¯¹å°å»ºç¤¼æ•™çš„è´¨ç–‘å’Œæ‰¹åˆ¤ï¼Œä½“ç°äº†ä»–å¯¹ä¼ ç»Ÿå°å»ºç¤¼æ•™çš„å¦å®šå’Œæ‰¹åˆ¤ã€‚è¿™ä¸ä»…ä»…æ˜¯å¯¹å°å»ºç¤¼æ•™çš„è´¨ç–‘ï¼Œæ›´æ˜¯å¯¹å½“æ—¶ç¤¾ä¼šçš„åæ€ï¼Œä»¥åŠå¯¹äººæ€§çš„å…³æ³¨ã€‚æœ€åï¼Œæˆ‘éœ€è¦æ€»ç»“ä¸€ä¸‹é²è¿…å¯¹å°å»ºç¤¼æ•™çš„æ‰¹åˆ¤å’Œæ‰¹åˆ¤ã€‚ä»–é€šè¿‡ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­çš„æ–‡å­—å’Œè¡Œä¸ºï¼Œæ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ï¼Œä½“ç°äº†å¯¹ç¤¾ä¼šç°å®çš„æ·±åˆ»åæ€ã€‚åŒæ—¶ï¼Œä»–ä¹Ÿå…³æ³¨äº†å°å»ºç¤¼æ•™çš„é»‘æš—é¢ï¼Œåæ˜ äº†å¯¹å°å»ºç¤¾ä¼šçš„ä¸æ»¡å’Œå¯¹äººæ€§çš„å¦å®šã€‚æ€»çš„æ¥è¯´ï¼Œé²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œæ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ï¼Œä½“ç°äº†å½“æ—¶çš„ç¤¾ä¼šç°å®å’Œäººæ€§çš„å¤æ‚ã€‚ä»–çš„ä½œå“å¯¹åæ¥çš„å°å»ºç¤¼æ•™æœ‰äº†å¯ç¤ºï¼Œå¯¹å½“æ—¶çš„ç¤¾ä¼šç°è±¡æœ‰äº†æ›´æ·±çš„åæ€ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯ä¸€éƒ¨åæ˜ å°å»ºç¤¼æ•™çš„æ–‡å­¦ä½œå“ï¼Œä¸»è¦é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨åæ˜ äº†å½“æ—¶ç¤¾ä¼šçš„ç°å®ã€‚é²è¿…çš„æ–‡å­—é£æ ¼ç‹¬ç‰¹ï¼Œå¯Œæœ‰è±¡å¾æ„ä¹‰ï¼Œä½†é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œæ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ã€‚ä»–çš„ä½œå“ä¸­ï¼Œå°å»ºç¤¼æ•™è¢«è§†ä¸ºä¸äººé“çš„ï¼Œä½†é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œåæ˜ äº†å°å»ºç¤¾ä¼šçš„ä¸å…¬å’Œäººæ€§çš„æ·±åˆ»åæ€ã€‚é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡æ–‡å­—å’Œè¡ŒåŠ¨ï¼Œæ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ï¼Œä½“ç°äº†å½“æ—¶ç¤¾ä¼šçš„ä¸æ»¡å’Œå¯¹äººæ€§çš„åæ€ã€‚ä»–çš„ä½œå“ä¸ä»…æ‰¹åˆ¤äº†å°å»ºç¤¼æ•™çš„è™šå‡å’Œä¸åˆç†ï¼Œè¿˜æ·±åˆ»æ­ç¤ºäº†å°å»ºç¤¾ä¼šçš„é»‘æš—é¢ã€‚&amp;lt;/answer&amp;gt;
[B]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£é²è¿…çš„ç”Ÿå¹³å’Œä½œå“ã€‚ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯ä¸­å›½ç°ä»£æ–‡å­¦çš„ç»å…¸ï¼Œä½œè€…æ˜¯æ¯›æ³½ä¸œï¼Œä½œå“æ¶‰åŠç¤¾ä¼šã€å®¶åº­ã€ä¼¦ç†ç­‰å¤šä¸ªå±‚é¢ã€‚æˆ‘éœ€è¦æ‰¾å‡ºé²è¿…çš„å†™ä½œé£æ ¼å’Œä¸»é¢˜ï¼Œä»¥åŠå®ƒåœ¨å½“æ—¶çš„ç¤¾ä¼šèƒŒæ™¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘åº”è¯¥å›é¡¾ã€Šç‹‚äººæ—¥è®°ã€‹çš„ä¸»è¦å†…å®¹ã€‚æ¯›æ³½ä¸œåœ¨ã€Šé’æ˜¥æ— æ‚”ã€‹ä¸­æå‡ºäº†"ç‹‚äººæ—¥è®°"è¿™ä¸€æ¦‚å¿µï¼Œå¼ºè°ƒé’æ˜¥æ´»æ³¼ã€å‹‡æ•¢ã€æ­£ä¹‰ï¼Œè¿™åœ¨å½“æ—¶ç¤¾ä¼šä¸­ä»ç„¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚ä»–é€šè¿‡æç»˜ä¸»äººå…¬ç‹‚äººæ¥è¡¨è¾¾å¯¹ç¤¾ä¼šä¸å…¬çš„æ‰¹åˆ¤ï¼Œä½†åŒæ—¶ä¹ŸæŒ‡å‡ºä»–çš„åˆ›ä½œå¯¹ç¤¾ä¼šçš„è´Ÿé¢å½±å“ï¼Œæ¯”å¦‚ç¤¾ä¼šåŠ¨è¡å’Œä¸ªäººä¸»ä¹‰çš„å¤±è´¥ã€‚ç„¶åï¼Œæˆ‘åº”è¯¥åˆ†æé²è¿…çš„åˆ›ä½œç†å¿µå’Œä¸»é¢˜ã€‚ã€Šç‹‚äººæ—¥è®°ã€‹å¼ºè°ƒäº†ç¤¾ä¼šçš„é»‘æš—é¢ï¼Œä¸»å¼ "ä»¥äººä¸ºæœ¬"ã€"åå¯¹å°å»ºç¤¼æ•™"ç­‰ã€‚ä»–å¯èƒ½ä¹Ÿåœ¨æ¢è®¨ç¤¾ä¼šä¸å¹³ç­‰çš„é—®é¢˜ï¼Œæ¯”å¦‚"äººæ€§å¼±ç‚¹"ã€"æƒåŠ›æ»¥ç”¨"ç­‰é—®é¢˜ã€‚è¿˜è¦è€ƒè™‘é²è¿…çš„å…¶ä»–ä½œå“ï¼Œæ¯”å¦‚ã€Šé˜¿Qæ­£ä¼ ã€‹ã€ã€Šæ­»äº¡è¯—ç¤¾ã€‹ç­‰ï¼Œè¿™äº›ä½œå“è™½ç„¶æœ‰ç‚¹å¤¸å¼ ï¼Œä½†ä¹Ÿæœ‰æ·±åˆ»çš„ç¤¾ä¼šæ‰¹åˆ¤ï¼Œæ¯”å¦‚å¯¹ç¤¾ä¼šé“å¾·çš„è´¨ç–‘ã€‚åŒæ—¶ï¼Œä½œè€…å¯èƒ½è¿˜æ¶‰åŠå…¶ä»–ä½œå“ï¼Œå¦‚ã€Šåº†ä½™å¹´ã€‹ç­‰ï¼Œä½†è¿™äº›å¯èƒ½æ›´ä¾§é‡äºæ–‡å­¦åˆ›ä½œï¼Œè€Œä¸æ˜¯ç¤¾ä¼šæ‰¹åˆ¤ã€‚å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½æƒ³çŸ¥é“é²è¿…åœ¨æ–‡å­¦åˆ›ä½œä¸­çš„å…·ä½“ç»å†ï¼Œæ¯”å¦‚ä»–æ˜¯å¦æœ‰ä½œå“è¢«å¹¿æ³›æ‰¹è¯„ï¼Œæˆ–è€…ä»–å¯¹ç¤¾ä¼šçš„æ‰¹åˆ¤æ˜¯å¦å…·æœ‰æ‰¹åˆ¤æ€§ã€‚æ‰€ä»¥ï¼Œå›ç­”æ—¶éœ€è¦æŒ‡å‡ºé²è¿…çš„åˆ›ä½œç»å†ï¼Œä»¥åŠä»–å¯¹ç¤¾ä¼šç°å®çš„åæ€ã€‚æ€»ç»“æ¥è¯´ï¼Œæˆ‘éœ€è¦å›ç­”é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ‰¹åˆ¤å°å»ºç¤¼æ•™ï¼Œå¹¶åˆ†æå…¶åœ¨å½“æ—¶çš„ç¤¾ä¼šèƒŒæ™¯å’Œæ–‡å­¦ä»·å€¼ï¼ŒåŒæ—¶æŒ‡å‡ºå…¶å¯¹ç¤¾ä¼šçš„æ‰¹åˆ¤æ€§æ€è€ƒï¼Œä»¥åŠå…¶å¯¹åä¸–çš„å½±å“ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ‰¹åˆ¤å°å»ºç¤¼æ•™ï¼Œä»¥å…¶ç‹¬ç‰¹çš„å™äº‹é£æ ¼å’Œæ·±åˆ»çš„ç¤¾ä¼šæ‰¹åˆ¤ç²¾ç¥è‘—ç§°ï¼Œä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š1. **ç¤¾ä¼šæ‰¹åˆ¤ä¸ä¸ªäººæˆé•¿**ï¼šå°è¯´é€šè¿‡æç»˜ä¸»äººå…¬ç‹‚äººçš„æ´»æ³¼ã€å‹‡æ•¢ã€æ­£ä¹‰ï¼Œæ­ç¤ºç¤¾ä¼šçš„ä¸å…¬å’Œå¼±åŠ¿ã€‚2. **ä¸ªäººä¸»ä¹‰ä¸ç¤¾ä¼šæ‰¹åˆ¤**ï¼šå°è¯´åæ˜ äº†å°å»ºç¤¼æ•™å¯¹ä¸ªäººè‡ªç”±å’Œæƒåˆ©çš„ä¾µçŠ¯ï¼Œå¼ºè°ƒä¸ªäººå°Šä¸¥å’Œæ­£ä¹‰çš„ä»·å€¼ã€‚3. **å¯¹ç¤¾ä¼šçš„æ‰¹åˆ¤æ€§æ€è€ƒ**ï¼šé€šè¿‡å¯¹ç¤¾ä¼šä¸å…¬çš„åˆ»ç”»ï¼Œæ­ç¤ºç¤¾ä¼šé—®é¢˜çš„æ ¹æºï¼Œå¹¶å¯¹å½“æ—¶ç¤¾ä¼šçš„æ‰¹åˆ¤æ€åº¦è¿›è¡Œæ‰¹åˆ¤ã€‚é²è¿…çš„ä½œå“åœ¨å½“æ—¶çš„ç¤¾ä¼šèƒŒæ™¯å’Œæ–‡å­¦ä»·å€¼ä½¿å…¶æˆä¸ºå½“æ—¶æ–‡å­¦çš„é‡è¦ä»£è¡¨ã€‚ä»–çš„ä½œå“ä¸ä»…åœ¨å½“æ—¶å¹¿å—æ¬¢è¿ï¼Œä¹Ÿå¯¹åä¸–äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œè¢«å¹¿æ³›å¼•ç”¨å’Œä¼ é¢‚ã€‚&amp;lt;/answer&amp;gt;
[C]: &amp;lt;think&amp;gt;å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£é²è¿…çš„èƒŒæ™¯å’Œæ ¸å¿ƒæ€æƒ³ã€‚ä»–å¯èƒ½æ˜¯ä¸€ä½ç¤¾ä¼šå­¦å®¶ã€æ€æƒ³å®¶ï¼Œæˆ–è€…æ˜¯å¯¹ä¼ ç»Ÿæ–‡åŒ–æœ‰æ·±åšå…´è¶£çš„è¯»è€…ã€‚ç‹‚äººæ—¥è®°æ˜¯ä»–åœ¨ã€Šç‹‚äººæ—¥è®°ã€‹ä¸­æ‰€å†™çš„ï¼Œå¯èƒ½å¯¹ä¼ ç»Ÿæ–‡åŒ–æœ‰ä¸€å®šå½±å“ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘è¦è€ƒè™‘ç”¨æˆ·çš„éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚ç”¨æˆ·å¯èƒ½æƒ³çŸ¥é“è¿™æœ¬ä¹¦åœ¨ç°ä»£ç¤¾ä¼šä¸­çš„åæ€æ„ä¹‰ï¼Œæˆ–è€…æ˜¯å¦æœ‰ç›¸å…³ç ”ç©¶æ”¯æŒè¿™ä¸€ç‚¹ã€‚ä¹Ÿæœ‰å¯èƒ½ä»–ä»¬å¯¹ä¹¦ä¸­çš„æƒ…èŠ‚æœ‰ç–‘é—®ï¼Œæƒ³äº†è§£å¦‚ä½•æ›´æ·±åˆ»åœ°ç†è§£ç¤¾ä¼šç°è±¡ã€‚ç„¶åï¼Œæˆ‘éœ€è¦æ€è€ƒå¦‚ä½•ä»ä¸åŒçš„è§’åº¦è§£è¯»é²è¿…çš„æ–‡æœ¬ã€‚ä¾‹å¦‚ï¼Œæ˜¯å¦æœ‰ç›´æ¥çš„å†å²å™äº‹ï¼Œæˆ–è€…æ˜¯å¦å­˜åœ¨æ›´æ·±å±‚æ¬¡çš„å“²å­¦æ€è€ƒï¼Ÿè¿™éœ€è¦æˆ‘å…ˆäº†è§£é²è¿…çš„ä¸åŒè§‚ç‚¹ï¼Œç„¶ååˆ†æå…¶æ‰¹åˆ¤æ€§ã€‚è¿˜è¦è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ï¼Œæ¯”å¦‚æ˜¯å¦éœ€è¦äº†è§£ä»–ä½œä¸ºæ–‡å­¦æ‰¹åˆ¤å®¶çš„èƒŒæ™¯ï¼Œæˆ–è€…æ˜¯å¦æœ‰å¯¹ä¼ ç»Ÿä»·å€¼è§‚çš„åæ€ã€‚å› æ­¤ï¼Œå›ç­”æ—¶è¦å…¨é¢ï¼Œæ¶µç›–æ ¸å¿ƒé—®é¢˜ï¼Œå¹¶å¼•ç”¨ç›¸å…³èµ„æ–™æ¥æ”¯æ’‘è§‚ç‚¹ã€‚æœ€åï¼Œæ€»ç»“é²è¿…çš„è§‚ç‚¹ï¼Œå¼ºè°ƒå…¶å¯¹ç°ä»£ç¤¾ä¼šçš„æ‰¹åˆ¤æ€§æ€è€ƒï¼Œä»¥åŠå¯¹ä¼ ç»Ÿæ–‡åŒ–çš„å°Šé‡ã€‚è¿™æ ·æ—¢å…¨é¢åˆæœ‰æ·±åº¦ï¼Œå¸®åŠ©ç”¨æˆ·å…¨é¢ç†è§£é²è¿…çš„ä½œå“ã€‚&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡å…¶ç‹¬ç‰¹çš„å™äº‹æ‰‹æ³•å’Œæ·±åˆ»çš„ç¤¾ä¼šæ‰¹åˆ¤ç²¾ç¥ï¼Œå¯¹å°å»ºç¤¼æ•™è¿›è¡Œäº†æ·±åˆ»çš„æ‰¹åˆ¤ã€‚ä»¥ä¸‹æ˜¯å…¶æ‰¹åˆ¤æ€§åˆ†æï¼š1. **ç¤¾ä¼šæ‰¹åˆ¤ä¸å†å²çœŸç›¸**ï¼šå°è¯´é€šè¿‡å¯¹å†å²äººç‰©çš„åˆ»ç”»ï¼Œæ­ç¤ºå°å»ºç¤¼æ•™çš„å¼Šç«¯ï¼Œä»¥åŠå…¶å¯¹ä¼ ç»Ÿç¤¾ä¼šç»“æ„çš„å†²å‡»ã€‚è¿™ç§æ‰¹åˆ¤ä¸ä»…é™äºå¯¹ç°å®çš„æ·±åˆ»æ‰¹åˆ¤ï¼Œä¹Ÿåæ˜ äº†ç°ä»£ç¤¾ä¼šå¯¹ä¼ ç»Ÿä»·å€¼è§‚çš„é‡æ–°å®¡è§†ã€‚2. **ä¸ªäººå‘½è¿ä¸ç¤¾ä¼šæ‰¹åˆ¤**ï¼šå°è¯´ä¸­ï¼Œä¸»äººå…¬é€šè¿‡"ç‹‚äººæ—¥è®°"æ­ç¤ºäº†å°å»ºç¤¼æ•™å¯¹ä¸ªäººå‘½è¿çš„ä¸¥é‡å‰¥å¤ºã€‚è¿™ç§å™äº‹æ–¹å¼æ—¢æ‰¹åˆ¤äº†å°å»ºç¤¼æ•™ï¼Œä¹Ÿå¯¹åä¸–äº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚3. **å¯¹åä¸–çš„å½±å“**ï¼šä»æ–‡å­¦åˆ›ä½œåˆ°æ€æƒ³è§£è¯»ï¼Œé²è¿…çš„ä½œå“æ·±åˆ»å½±å“äº†åä¸–çš„æ€æƒ³ä½“ç³»ã€‚ä¾‹å¦‚ï¼Œã€Šç‹‚äººæ—¥è®°ã€‹é€šè¿‡äººç‰©å‘½è¿çš„åæ˜ ï¼Œå¯¹ç°ä»£ç¤¾ä¼šçš„ä»·å€¼è§‚å¿µæå‡ºäº†æ–°çš„æ€è€ƒã€‚4. **å¯¹ç°ä»£æ–‡æ˜çš„åæ€**ï¼šè¿™ä¸€æ‰¹åˆ¤æ€§æ€è€ƒä¸ä»…åæ˜ äº†å½“ä»£ç¤¾ä¼šçš„å¤æ‚æ€§ï¼Œä¹Ÿä¸ºç°ä»£äººæä¾›äº†åæ€ç¤¾ä¼šç°å®ã€ç»´æŠ¤äººæƒçš„è§†è§’ã€‚5. **ä¸ªäººè§†è§’ä¸æ–‡åŒ–è§†è§’**ï¼šä½œè€…é€šè¿‡è¿™ä¸€æ‰¹åˆ¤æ€§æ€è€ƒï¼Œå±•ç°äº†å¯¹ä¼ ç»Ÿä»·å€¼è§‚çš„é‡æ–°è§£è¯»ï¼ŒåŒæ—¶ä¹Ÿå¯¹å½“ä»£ç¤¾ä¼šçš„å¤šå…ƒåŒ–è§†è§’äº§ç”Ÿäº†å¯å‘ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œé²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹ä¸ä»…æ˜¯å¯¹å°å»ºç¤¼æ•™çš„æ‰¹åˆ¤ï¼Œæ›´æ˜¯å¯¹ä¼ ç»Ÿæ–‡åŒ–çš„ä¸€ç§æ·±åˆ»åæ€ã€‚å®ƒé€šè¿‡å¯¹å°å»ºç¤¼æ•™çš„æ‰¹åˆ¤ï¼Œæå‡ºäº†å¯¹ç°ä»£ç¤¾ä¼šç»“æ„ã€æƒåŠ›å…³ç³»çš„æ·±åˆ»åæ€ï¼Œå…·æœ‰é‡è¦çš„ç¤¾ä¼šä»·å€¼ã€‚&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â…¢ å…¶ä»–æ¨¡å‹å¯¹æ¯”&lt;/h2&gt; 
&lt;p&gt;[A] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2 (0.1B)&lt;/a&gt;&lt;br /&gt; [B] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-MoE (0.15B)&lt;/a&gt;&lt;br /&gt; [C] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-Small (0.02B)&lt;/a&gt;&lt;br /&gt; [D] &lt;a href="https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666"&gt;minimind-v1-small(0.02B)&lt;/a&gt;&lt;br /&gt; [E] &lt;a href="https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666"&gt;minimind-v1-moe(0.1B)&lt;/a&gt;&lt;br /&gt; [F] &lt;a href="https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666"&gt;minimind-v1(0.1B)&lt;/a&gt;&lt;br /&gt; [G] &lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;baby-llama2-chinese(0.2B)&lt;/a&gt;&lt;br /&gt; [H] &lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;chatlm-mini-chinese(0.2B)&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: ä½ çŸ¥é“é•¿æ±Ÿå—ï¼Ÿ
[A]: é•¿æ±Ÿï¼Œä½äºä¸­å›½ä¸œå—éƒ¨ï¼Œæ˜¯ä¸­å›½çš„ç¬¬ä¸€é•¿æ²³ï¼Œå…¨é•¿6300å¤šå…¬é‡Œï¼Œæµç»11ä¸ªçœä»½ï¼Œæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œä¹Ÿæ˜¯ä¸­å›½çš„â€œæ¯äº²æ²³â€ã€‚é•¿æ±Ÿçš„å†å²å¯ä»¥è¿½æº¯åˆ°å…¬å…ƒå‰2000å¤šå¹´çš„å•†æœæ—¶æœŸï¼Œå½“æ—¶çš„å•†æœæ˜¯å•†æœçš„åè£”ã€‚å•†æœæ—¶æœŸï¼Œé•¿æ±Ÿçš„ç»æµå‘å±•è¾¾åˆ°äº†é«˜å³°ï¼Œæ˜¯ä¸­å›½æœ€é‡è¦çš„å†œä¸šå’Œå·¥ä¸šä¸­å¿ƒä¹‹ä¸€ã€‚é•¿æ±ŸæµåŸŸçš„ç»æµä¸»è¦ä»¥å†œä¸šä¸ºä¸»ï¼Œé•¿æ±Ÿæ¸”ä¸šå æ®äº†ä¸»è¦äº§ä¸šï¼Œå¦‚é’“é±¼ã€çŒé±¼ç­‰ã€‚é•¿æ±Ÿçš„æ°´èµ„æºä¹Ÿéå¸¸ä¸°å¯Œï¼ŒåŒ…æ‹¬æ²³æµã€æ¹–æ³Šã€æ²³æµç­‰ã€‚é•¿æ±Ÿçš„è‡ªç„¶æ™¯è§‚ä¹Ÿååˆ†å£®è§‚ï¼Œé•¿æ±Ÿå››å­£æ™¯è‰²å„å¼‚ï¼Œæ˜¥å­£å¤šé›¨ï¼Œå¤å­£å¤šé›¨ï¼Œç§‹å­£åˆ™å¤šé›¨ï¼Œå†¬å­£åˆ™å¯’å†·å¤šé›ªï¼Œæ˜¥å­£å’Œç§‹å­£éƒ½æœ‰ç¾ä¸½çš„èŠ±æœµå’Œæœå®ã€‚é•¿æ±Ÿæ˜¯ä¸­å›½çš„ç»æµä¸­å¿ƒï¼Œæ‹¥æœ‰ä¼—å¤šçš„å›½å®¶å’ŒåŸå¸‚ï¼Œæ˜¯ä¸–ç•Œä¸Šé‡è¦çš„äº¤é€šæ¢çº½å’Œèˆªè¿ä¸­å¿ƒã€‚åŒæ—¶ï¼Œé•¿æ±Ÿè¿˜æ˜¯ä¸­å›½çš„é‡è¦æ°´åˆ©èµ„æºï¼Œå¯¹äºä¸­å›½çš„ç»æµå‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚
[B]: æ˜¯çš„ï¼Œæˆ‘çŸ¥é“é•¿æ±Ÿã€‚é•¿æ±Ÿä½äºä¸­å›½çš„ä¸­éƒ¨ï¼Œæ˜¯ä¸­å›½ç¬¬ä¸‰é•¿çš„æ²³æµï¼Œå…¨é•¿çº¦6300å…¬é‡Œï¼Œæµç»æ±Ÿè¥¿çœã€å®‰å¾½çœã€æ±Ÿè‹çœã€å®‰å¾½çœã€æ±Ÿè‹çœå’Œå®‰å¾½çœã€‚
[C]: å½“ç„¶ï¼Œé•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œå…¨é•¿çº¦6300å…¬é‡Œã€‚å®ƒä»¥å…¶ä¸°å¯Œçš„æ°´èµ„æºã€ä¸°å¯Œçš„æ°´ç”ŸåŠ¨æ¤ç‰©èµ„æºä»¥åŠç‹¬ç‰¹çš„åœ°ç†ä½ç½®è€Œé—»åäºä¸–ã€‚
[D]: æ˜¯çš„ï¼Œé•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œå…¨é•¿è¶…è¿‡3300åƒç±³ã€‚
[E]: é•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œæ€»é•¿åº¦çº¦ä¸º6300å…¬é‡Œã€‚å®ƒå‘æºäºé’è—é«˜åŸçš„é’æµ·æ¹–ï¼Œæµç»é’æµ·ã€å››å·ã€ç”˜è‚ƒã€å®å¤ã€å†…è’™å¤ã€é™•è¥¿å’Œå±±è¥¿ã€‚é•¿æ±ŸæµåŸŸæ˜¯ä¸­å›½æœ€å¤§çš„å†œä¸šåŒºä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½çš„æ–‡åŒ–å’Œç»æµä¸­å¿ƒã€‚
[F]: æ˜¯çš„ï¼Œé•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸–ç•Œç¬¬ä¸‰å¤§æ²³æµã€‚é•¿æ±Ÿå‘æºäºé’è—é«˜åŸï¼Œæµç»11ä¸ªçœä»½ï¼Œæœ€ç»ˆæ³¨å…¥ä¸œæµ·ã€‚é•¿æ±ŸæµåŸŸæ˜¯ä¸­å›½æœ€å¤§çš„æ²³æµæµåŸŸä¹‹ä¸€ï¼Œæ‹¥æœ‰ç€ä¸°å¯Œçš„è‡ªç„¶èµ„æºå’Œæ–‡åŒ–é—äº§ï¼Œå¦‚é•¿æ±Ÿå¤§æ¡¥ã€é•¿æ±Ÿæµ·æ´‹åšç‰©é¦†ã€é•¿æ±Ÿå¤åŸç­‰ã€‚
[G]: åŒæ—¶ä¹ŸçŸ¥é“ï¼Œä¸­å›½æ˜¯ä¸–ç•Œä¸Šæœ€é•¿çš„åŸå¸‚ä¹‹ä¸€ï¼Œæ‹¥æœ‰è¶…è¿‡3400ä¸‡äººå£ã€‚
[H]: æ˜¯çš„ï¼Œé•¿æ±Ÿæ˜¯ä¸­å›½æœ€é•¿çš„æ²³æµï¼Œä¹Ÿæ˜¯ä¸–ç•Œä¸Šæœ€é•¿çš„æ²³æµä¹‹ä¸€ã€‚å®ƒå‘æºäºé’è—é«˜åŸï¼Œæµç»ä¹ä¸ªçœåŒºï¼Œæœ€ç»ˆæ³¨å…¥ä¸œæµ·ã€‚é•¿æ±ŸæµåŸŸçš„æ°”å€™æ¸©å’Œï¼Œå››å­£åˆ†æ˜ï¼Œæ˜¯ä¸­å›½é‡è¦çš„å†œä¸šå’Œå·¥ä¸šåŸºåœ°ã€‚é•¿æ±ŸæµåŸŸçš„æ°´äº§èµ„æºä¸°å¯Œï¼Œæ˜¯ä¸­å›½çš„é‡è¦æ¸”ä¸šèµ„æºä¹‹ä¸€ã€‚


[Q]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ä»€ä¹ˆï¼Ÿ
[A]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼ˆMount Everestï¼‰ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰ï¼Œå°¼æ³Šå°”å’Œä¸­å›½çš„è¾¹ç•Œä¸Šã€‚å®ƒçš„æµ·æ‹”é«˜åº¦æ˜¯8,848.86ç±³ï¼ˆ29,031.7è‹±å°ºï¼‰ï¼Œæ˜¯åœ°çƒä¸Šæœ€é«˜çš„å±±å³°ã€‚
[B]: ç ç©†æœ—ç›å³°ï¼ˆMount Everestï¼‰æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œæµ·æ‹”é«˜åº¦è¾¾8,848ç±³ï¼ˆ29,029è‹±å°ºï¼‰ã€‚
[C]: ç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰ï¼Œæµ·æ‹”8,848.86ç±³ï¼ˆ29,031.7è‹±å°ºï¼‰ã€‚å®ƒä½äºå°¼æ³Šå°”å’Œä¸­å›½çš„è¾¹ç•Œä¸Šï¼Œæ˜¯å–œé©¬æ‹‰é›…å±±è„‰çš„ä¸€éƒ¨åˆ†ã€‚
[D]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼Œä½äºå°¼æ³Šå°”å’Œä¸­å›½çš„è¾¹å¢ƒä¸Šï¼Œæµ·æ‹”8848ç±³ã€‚
[E]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰ï¼Œæµ·æ‹”8,848ç±³ã€‚
[F]: ç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œæµ·æ‹”8848ç±³ã€‚
[G]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰ï¼Œæµ·æ‹”8,848.86ç±³ï¼ˆ29,031.69è‹±å°ºï¼‰ã€‚
[H]: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°ï¼Œä¹Ÿç§°ä¸ºè¨åŠ ç›å¡”ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰çš„å°¼æ³Šå°”å’Œä¸­å›½è¾¹å¢ƒä¹‹é—´ï¼Œæµ·æ‹”8,848.86ç±³ï¼ˆ29,031.69è‹±å°ºï¼‰ã€‚å®ƒæ˜¯ä¸–ç•Œä¸Šæœ€è‘—åçš„ç™»å±±ç›®æ ‡ä¹‹ä¸€ï¼Œå¸å¼•ç€æ¥è‡ªä¸–ç•Œå„åœ°çš„ç™»å±±è€…å‰æ¥æŒ‘æˆ˜ã€‚ç ç©†æœ—ç›å³°çš„åå­—æ¥è‡ªè—è¯­ï¼Œæ„æ€æ˜¯â€œå¥³ç¥çš„é¢å³°â€ï¼Œå®ƒæ˜¯ç”±ä¸¤ä¸ªä¸åŒçš„å±±å³°ç»„æˆçš„ï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯ä½äºå°¼æ³Šå°”çš„ç ç©†æœ—ç›å³°ï¼Œå¦ä¸€ä¸ªåˆ™ä½äºä¸­å›½ã€‚å®ƒçš„é«˜åº¦æ˜¯é€šè¿‡æµ‹é‡æµ·æ‹”é«˜åº¦æ¥ç¡®å®šçš„ï¼Œå› ä¸ºå®ƒæ˜¯é€šè¿‡æµ‹é‡åœ°çƒçš„å¹³å‡é«˜åº¦æ¥ç¡®å®šçš„ã€‚å°½ç®¡ç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œä½†å®ƒå¹¶ä¸æ˜¯åœ°çƒä¸Šä»»ä½•å…¶ä»–åœ°æ–¹éƒ½èƒ½åˆ°è¾¾çš„ã€‚åœ°çƒä¸Šçš„å…¶ä»–åœ°æ–¹ï¼Œå¦‚å—ç¾æ´²çš„å®‰ç¬¬æ–¯å±±è„‰ï¼Œä¹Ÿæ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±è„‰ï¼Œä½†å®ƒå¹¶ä¸æ˜¯åœ°çƒä¸Šæœ€é«˜çš„å±±å³°ã€‚æ€»ä¹‹ï¼Œç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œä½†å®ƒå¹¶ä¸æ˜¯åœ°çƒä¸Šä»»ä½•åœ°æ–¹éƒ½èƒ½åˆ°è¾¾çš„ã€‚å®ƒæ˜¯åœ°çƒä¸Šæœ€é«˜çš„å±±è„‰ï¼Œä½†å®ƒå¹¶ä¸æ˜¯åœ°çƒä¸Šä»»ä½•åœ°æ–¹éƒ½èƒ½åˆ°è¾¾çš„ã€‚


[Q]: ä¸‡æœ‰å¼•åŠ›æ˜¯è°æå‡ºçš„ï¼Ÿ
[A]: ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±ç‰›é¡¿åœ¨17ä¸–çºªæå‡ºçš„ï¼Œå®ƒæ˜¯ç‰›é¡¿ä¸‰å¤§å®šå¾‹ä¹‹ä¸€ã€‚ç‰›é¡¿åœ¨17ä¸–çºªæå‡ºçš„å¼•åŠ›å®šå¾‹æ­ç¤ºäº†ä¸‡æœ‰å¼•åŠ›å®šå¾‹çš„æœ¬è´¨ï¼Œå¹¶ä¸”è§£é‡Šäº†è¡Œæ˜Ÿã€å«æ˜Ÿã€å½—æ˜Ÿç­‰å¤©ä½“ä¹‹é—´å¦‚ä½•ç›¸äº’ä½œç”¨ã€‚è¿™ä¸ªå®šå¾‹æ˜¯ç”±è‰¾è¨å…‹Â·ç‰›é¡¿åœ¨1687å¹´æå‡ºçš„ï¼Œä»–é€šè¿‡å¯¹ä¸‡æœ‰å¼•åŠ›å®šå¾‹çš„åˆ†æå’Œå®éªŒï¼ŒæˆåŠŸåœ°è§£é‡Šäº†å¤©ä½“ä¹‹é—´çš„å¼•åŠ›ä½œç”¨ï¼Œä»è€Œè§£é‡Šäº†åœ°çƒç»•å¤ªé˜³æ—‹è½¬çš„ç°è±¡ã€‚
[B]: ä¸‡æœ‰å¼•åŠ›ï¼ˆHuman Policyï¼‰æ˜¯ç‰›é¡¿çš„ä¸‰å¤§åŠ›å­¦å®šå¾‹ï¼Œç”¨äºæè¿°ä¸¤ä¸ªç‰©ä½“ä¹‹é—´çš„ç›¸äº’ä½œç”¨åŠ›ã€‚å®ƒç”±ä¸‡æœ‰å¼•åŠ›å®šå¾‹å’Œç‰›é¡¿çš„ç¬¬ä¸€å®šå¾‹ï¼ˆæƒ¯æ€§å®šå¾‹ï¼‰æ„æˆã€‚ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±äºä¸¤ä¸ªç‰©ä½“ä¹‹é—´ç›¸äº’ä½œç”¨è€Œäº§ç”Ÿçš„ï¼Œå®ƒä»¬çš„è´¨é‡ã€è·ç¦»ã€é€Ÿåº¦å’Œæ–¹å‘ç­‰å› ç´ å†³å®šäº†å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚è¿™ç§ç›¸äº’ä½œç”¨åŠ›æ˜¯ï¼šè´¨é‡çš„ä¹˜ç§¯ï¼šè´¨é‡çš„ä¹˜ç§¯æ˜¯ä¸‡æœ‰å¼•åŠ›å®šå¾‹å’Œç‰›é¡¿ç¬¬äºŒå®šå¾‹çš„ä¹˜ç§¯ã€‚è´¨é‡çš„ä¹˜ç§¯ï¼šè´¨é‡çš„ä¹˜ç§¯æ˜¯ç‰©ä½“ä¹‹é—´çš„ä¸€ç§ç‰©ç†è¿‡ç¨‹ï¼Œå®ƒæ˜¯ç‰©ä½“é—´ç›¸äº’ä½œç”¨çš„äº§ç‰©ï¼Œå¯¹äºå…¶ä»–ç‰©ä½“æ¥è¯´ï¼Œå…¶è´¨é‡å¯ä»¥è¡¨ç¤ºä¸ºä¸¤ä¸ªç‰©ä½“ä¹‹é—´çš„è´¨é‡ä¹‹å’Œï¼Œå³é‡åŠ›åŠ é€Ÿåº¦aã€‚
[C]: ä¸‡æœ‰å¼•åŠ›æ˜¯ä¸€ä¸ªç”±ç‰›é¡¿åœ¨1687å¹´æå‡ºçš„ç‰©ç†å­¦æ¦‚å¿µï¼Œå®ƒæè¿°äº†ç‰©ä½“ä¹‹é—´å¼•åŠ›å¦‚ä½•ç›¸äº’ä½œç”¨çš„è§„å¾‹ã€‚
[D]: ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±è‹±å›½ç‰©ç†å­¦å®¶ç‰›é¡¿æå‡ºçš„ã€‚ä»–è®¤ä¸ºï¼Œæ‰€æœ‰ç‰©ä½“éƒ½ä¼šäº§ç”Ÿå¼•åŠ›ï¼Œè¿™ä¸ªå¼•åŠ›æ˜¯ç”±ç‰©ä½“çš„è´¨é‡å’Œè·ç¦»å†³å®šçš„ã€‚è¿™ä¸ªç†è®ºæ­ç¤ºäº†è´¨é‡å’Œè·ç¦»ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è¯æ˜äº†ä¸‡æœ‰å¼•åŠ›çš„æœ¬è´¨ã€‚
[E]: ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±ç‰›é¡¿åœ¨1687å¹´æå‡ºçš„ã€‚å®ƒæ˜¯å®‡å®™ä¸­ä¸€ä¸ªåŸºæœ¬çš„ç‰©ç†ç°è±¡ï¼Œå®ƒæè¿°äº†ç‰©ä½“ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚è¿™ä¸ªåŠ›è¢«ç§°ä¸ºä¸‡æœ‰å¼•åŠ›ï¼Œå®ƒåœ¨æ‰€æœ‰ç‰©ä½“ä¹‹é—´éƒ½å­˜åœ¨ï¼Œè¿™ç§åŠ›ä¸ç‰©ä½“çš„è´¨é‡å’Œè·ç¦»æœ‰å…³ã€‚ä¸‡æœ‰å¼•åŠ›æ˜¯è‡ªç„¶ç•Œä¸­æœ€åŸºæœ¬çš„åŠ›é‡ä¹‹ä¸€ï¼Œå®ƒä¸ä»…å½±å“ç€ç‰©ä½“çš„è¿åŠ¨å’Œå˜åŒ–ï¼Œè¿˜å¯¹å®‡å®™çš„ç»“æ„å’Œæ¼”åŒ–äº§ç”Ÿç€é‡è¦çš„å½±å“ã€‚
[F]: ä¸‡æœ‰å¼•åŠ›æ˜¯è‹±å›½ç‰©ç†å­¦å®¶è‰¾è¨å…‹Â·ç‰›é¡¿åœ¨17ä¸–çºªæå‡ºçš„ã€‚
[G]: å¼•åŠ›æ˜¯å®‡å®™ä¸­ä¸€ç§åŠ›é‡ï¼Œå®ƒæ˜¯ç”±è´¨é‡æˆ–èƒ½é‡æ‰€ç»„æˆçš„ã€‚æ ¹æ®ç‰›é¡¿çš„ä¸‡æœ‰å¼•åŠ›å®šå¾‹ï¼Œä¸€ä¸ªç‰©ä½“ä¹‹é—´çš„å¼•åŠ›å¤§å°ä¸å®ƒçš„è´¨é‡æˆæ­£æ¯”ï¼Œä¸å®ƒçš„è´¨é‡æˆåæ¯”ã€‚å› æ­¤ï¼Œç‰›é¡¿çš„ä¸‡æœ‰å¼•åŠ›å®šå¾‹è¡¨æ˜ï¼Œä¸€ä¸ªç‰©ä½“ä¹‹é—´çš„å¼•åŠ›å¤§å°ä¸å®ƒçš„è´¨é‡æˆæ­£æ¯”ï¼Œä¸å®ƒçš„è´¨é‡æˆåæ¯”ã€‚è¿™ä¸ªå®šå¾‹åœ¨ç‰©ç†å­¦ã€å¤©æ–‡å­¦å’Œç‰©ç†å­¦ä¸­éƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚
[H]: ä¸‡æœ‰å¼•åŠ›æ˜¯ç”±è‹±å›½ç‰©ç†å­¦å®¶è‰¾è¨å…‹Â·ç‰›é¡¿äº1687å¹´æå‡ºçš„ã€‚ä»–åœ¨1687å¹´å‘è¡¨äº†ä¸€ç¯‡è‘—åçš„è®ºæ–‡ï¼Œé¢˜ä¸ºã€Šè‡ªç„¶å“²å­¦çš„æ•°å­¦åŸç†ã€‹ï¼Œå…¶ä¸­æå‡ºäº†ä¸‡æœ‰å¼•åŠ›å®šå¾‹ï¼ŒæŒ‡å‡ºä»»ä½•ä¸¤ä¸ªç‰©ä½“ä¹‹é—´éƒ½å­˜åœ¨ç€ä¸€ç§å¼•åŠ›ï¼Œè¿™ç§å¼•åŠ›çš„å¤§å°ä¸å®ƒä»¬çš„è´¨é‡æˆæ­£æ¯”ï¼Œä¸å®ƒä»¬ä¹‹é—´çš„è·ç¦»çš„å¹³æ–¹æˆåæ¯”ã€‚è¿™ä¸ªå®šå¾‹è¢«ç§°ä¸ºä¸‡æœ‰å¼•åŠ›å®šå¾‹ï¼Œå®ƒæè¿°äº†è´¨é‡å’Œè·ç¦»ä¹‹é—´çš„å…³ç³»ã€‚å®ƒè¡¨æ˜ï¼Œä¸¤ä¸ªç‰©ä½“ä¹‹é—´çš„å¼•åŠ›ä¸å®ƒä»¬çš„è´¨é‡æˆæ­£æ¯”ï¼Œä¸å®ƒä»¬ä¹‹é—´çš„è·ç¦»çš„å¹³æ–¹æˆåæ¯”ã€‚è¿™ä¸ªå®šå¾‹å¯¹äºè§£é‡Šå®‡å®™ä¸­çš„è®¸å¤šç°è±¡å’Œç°è±¡ä¹‹é—´çš„ç›¸äº’ä½œç”¨éå¸¸é‡è¦ï¼Œä¾‹å¦‚è¡Œæ˜Ÿç»•å¤ªé˜³è¿åŠ¨ã€æ’æ˜Ÿä¹‹é—´çš„å¼•åŠ›é€é•œæ•ˆåº”ç­‰ç­‰ã€‚å®ƒä¹Ÿè¢«å¹¿æ³›åº”ç”¨äºå¤©æ–‡å­¦ã€ç‰©ç†å­¦å’Œå·¥ç¨‹å­¦ç­‰é¢†åŸŸï¼Œä¸ºæˆ‘ä»¬ç†è§£å®‡å®™çš„æœ¬è´¨å’Œæ¼”åŒ–æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ã€‚


[Q]: ä½ çŸ¥é“å¤§ç†ŠçŒ«çš„ä¸»è¦é£Ÿç‰©æ˜¯ä»€ä¹ˆå—ï¼Ÿ
[A]: å¤§ç†ŠçŒ«ä¸»è¦ä»¥ç«¹å­ä¸ºé£Ÿï¼Œä½†ä¹Ÿä¼šåƒå…¶ä»–æ¤ç‰©ã€æ˜†è™«ã€å°å‹å“ºä¹³åŠ¨ç‰©å’Œé±¼ç±»ã€‚å®ƒä»¬çš„æ¶ˆåŒ–ç³»ç»Ÿä¸å…¶ä»–ç†Šç§‘åŠ¨ç‰©ç±»ä¼¼ï¼Œèƒ½å¤Ÿæ¶ˆåŒ–ç«¹å­ã€‚å¤§ç†ŠçŒ«çš„ç‰™é½¿é”‹åˆ©ï¼Œé€‚åˆå’€åš¼ç«¹å­ï¼Œå› æ­¤å®ƒä»¬ç»å¸¸è¢«ç§°ä¸ºâ€œç«¹é£ŸåŠ¨ç‰©â€ã€‚
[B]: å¤§ç†ŠçŒ«æ˜¯ä»¥ç«¹å­ä¸ºé£Ÿçš„å“ºä¹³åŠ¨ç‰©ï¼Œå®ƒä»¬æ˜¯è‚‰é£ŸåŠ¨ç‰©ã€‚å®ƒä»¬ä¸»è¦é£Ÿç‰©æ˜¯ç«¹å­ï¼Œç‰¹åˆ«æ˜¯ç«¹å­ä¸­å¯Œå«çº¤ç»´ç´ å’ŒçŸ¿ç‰©è´¨ï¼Œå¹¶ä¸”ä¸ä¼šç›´æ¥è¿›å…¥æ¶ˆåŒ–ç³»ç»Ÿã€‚å¤§ç†ŠçŒ«çš„è‚ é“ä¸»è¦ç”±ç«¹å­ç»„æˆï¼Œå®ƒä»¬éœ€è¦æ¶ˆåŒ–ç«¹å­ä¸­çš„çº¤ç»´ç´ å’Œå…¶ä»–çº¤ç»´ç´ æ¥æ”¯æŒå®ƒä»¬çš„æ­£å¸¸è¿›é£Ÿã€‚
[C]: å¤§ç†ŠçŒ«ä¸»è¦ä»¥ç«¹å­ä¸ºé£Ÿï¼Œä½†å®ƒä»¬ä¹Ÿåƒå…¶ä»–æ¤ç‰©ï¼ŒåŒ…æ‹¬æ ‘å¶ã€å«©èŠ½ã€æ˜†è™«ã€æ°´æœã€è”¬èœå’Œå°å‹å“ºä¹³åŠ¨ç‰©ç­‰ã€‚å¤§ç†ŠçŒ«æ˜¯ä¸­å›½ç‰¹æœ‰çš„ç‰©ç§ï¼Œå±äºç†Šç§‘ï¼Œä¸»è¦åˆ†å¸ƒåœ¨ä¸­å›½çš„å››å·ã€é™•è¥¿å’Œç”˜è‚ƒç­‰åœ°ã€‚
[D]: å¤§ç†ŠçŒ«çš„ä¸»è¦é£Ÿç‰©æ˜¯ç«¹å­ã€‚å®ƒä»¬é€šå¸¸åƒç«¹å­ï¼Œæ¯å¤©éœ€è¦åƒæ‰å¤§çº¦30å…¬æ–¤çš„ç«¹å­ã€‚
[E]: æ˜¯çš„ï¼Œå¤§ç†ŠçŒ«ä¸»è¦ä»¥ç«¹å­ä¸ºé£Ÿã€‚å®ƒä»¬æ¯å¤©éœ€è¦åƒæ‰å¤§é‡çš„ç«¹å­æ¥ç»´æŒç”Ÿå‘½ï¼Œæ¯å¤©éœ€è¦åƒæ‰40è‡³80å…¬æ–¤çš„ç«¹å­ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¹Ÿä¼šåƒä¸€äº›å…¶ä»–çš„æ¤ç‰©ï¼Œå¦‚è‰ã€æœå®ã€åšæœå’Œå°å‹å“ºä¹³åŠ¨ç‰©ã€‚
[F]: å¤§ç†ŠçŒ«ä¸»è¦ä»¥ç«¹å­ä¸ºé£Ÿï¼Œä¹Ÿä¼šå¶å°”é£Ÿç”¨å…¶ä»–çš„æ¤ç‰©å’ŒåŠ¨ç‰©ã€‚æ­¤å¤–ï¼Œå¤§ç†ŠçŒ«ä¹Ÿä¼šåƒä¸€äº›é‡æœã€å°åŠ¨ç‰©å’Œæ¤ç‰©ï¼Œä½†è¿™äº›éƒ½ä¸å±äºå®ƒä»¬çš„ä¸»è¦é£Ÿç‰©æ¥æºã€‚
[G]: å¤§ç†ŠçŒ«çš„ä¸»è¦é£Ÿç‰©æ˜¯ç«¹å­ã€‚
[H]: å¤§ç†ŠçŒ«æ˜¯ä¸€ç§ä»¥ç«¹å­ä¸ºä¸»è¦é£Ÿç‰©çš„æ¤é£ŸåŠ¨ç‰©ï¼Œå®ƒä»¬çš„è‚ é“ç»“æ„é€‚åˆæ¶ˆåŒ–çº¤ç»´ç´ å«é‡é«˜çš„ç«¹å­ï¼Œä½†ä¹Ÿä¼šåƒä¸€äº›å…¶ä»–æ¤ç‰©ï¼Œå¦‚è‰æœ¬æ¤ç‰©å’Œå°å‹å“ºä¹³åŠ¨ç‰©ã€‚å¤§ç†ŠçŒ«çš„è‚ é“ç»“æ„ä¹Ÿé€‚åº”äº†é«˜çº¤ç»´çš„ç«¹å­ï¼Œè¿™ä½¿å¾—å®ƒä»¬èƒ½å¤Ÿæ¶ˆåŒ–å¤§é‡çš„çº¤ç»´ç´ ï¼Œä»è€Œå¸®åŠ©å®ƒä»¬åœ¨é‡å¤–ç”Ÿå­˜ã€‚å¤§ç†ŠçŒ«çš„è‚ é“ä¸­è¿˜å«æœ‰ä¸€ç§å«åšâ€œçº¤ç»´ç´ é…¶â€çš„é…¶ç±»ç‰©è´¨ï¼Œè¿™ç§é…¶èƒ½å¤Ÿå¸®åŠ©å¤§ç†ŠçŒ«åˆ†è§£çº¤ç»´ç´ ï¼Œä»è€Œå¸®åŠ©å®ƒä»¬æ¶ˆåŒ–å¤§é‡çš„æ°´åˆ†å’Œè¥å…»ç‰©è´¨ã€‚æ­¤å¤–ï¼Œå¤§ç†ŠçŒ«çš„è‚ é“è¿˜å«æœ‰ä¸€ç§å«åšâ€œçº¤ç»´ç´ è›‹ç™½â€çš„è›‹ç™½è´¨ï¼Œè¿™ç§è›‹ç™½è´¨èƒ½å¤Ÿå¸®åŠ©å¤§ç†ŠçŒ«ç»´æŒä½“å†…çš„èƒ½é‡æ°´å¹³ï¼Œä»è€Œå¸®åŠ©å®ƒä»¬åœ¨é‡å¤–ç”Ÿå­˜ã€‚æ€»çš„æ¥è¯´ï¼Œå¤§ç†ŠçŒ«çš„ä¸»è¦é£Ÿç‰©æ˜¯ç«¹å­ï¼Œä½†å®ƒä»¬ä¹Ÿä¼šåƒä¸€äº›å…¶ä»–æ¤ç‰©ï¼Œå¦‚è‰æœ¬æ¤ç‰©å’Œå°å‹å“ºä¹³åŠ¨ç‰©ã€‚


[Q]: æµ·æ°´ä¸ºä»€ä¹ˆæ˜¯å’¸çš„ï¼Ÿ
[A]: æµ·æ°´ä¹‹æ‰€ä»¥æ˜¯å’¸çš„ï¼Œä¸»è¦æ˜¯ç”±äºå…¶ä¸­çš„åŒ–å­¦æˆåˆ†ã€‚æµ·æ°´ä¸­å«æœ‰å¤§é‡çš„çŸ¿ç‰©è´¨å’Œç›åˆ†ï¼Œå…¶ä¸­ä¸»è¦æˆåˆ†æ˜¯ç¡«é…¸ç›ã€ç¢³é…¸ç›ã€é•ç¦»å­å’Œé’¾ç¦»å­ã€‚è¿™äº›ç‰©è´¨åœ¨é«˜æµ“åº¦ä¸‹ï¼Œä¼šæ²‰æ·€åœ¨æµ·æ°´ä¸­ï¼Œå½¢æˆæµ·æ°´ã€‚å…·ä½“æ¥è¯´ï¼Œæµ·æ°´ä¸­çš„çŸ¿ç‰©è´¨å’Œç›åˆ†ä¸»è¦æ¥è‡ªäºå²©çŸ³ã€åœŸå£¤ã€ç”Ÿç‰©å’Œäººç±»æ’æ³„ç‰©ç­‰ã€‚å²©çŸ³æ˜¯æµ·æ°´çš„ç»„æˆéƒ¨åˆ†ï¼Œä¸»è¦ç”±çŸ¿ç‰©è´¨ç»„æˆï¼Œå¦‚çŸ³è‹±ã€é•¿çŸ³ã€äº‘æ¯ç­‰ã€‚åœŸå£¤ä¸­å«æœ‰å¤§é‡çš„åŒ–å­¦ç‰©è´¨ï¼Œå¦‚æ°®ã€ç£·ã€é’¾ç­‰ï¼Œè¿™äº›ç‰©è´¨ä¼šæ¸—é€åˆ°åœŸå£¤ä¸­ï¼Œå½¢æˆåœŸå£¤ã€‚ç”Ÿç‰©ä½“å†…çš„ç›åˆ†ä¸»è¦æ¥æºäºæ¤ç‰©çš„å¶ç»¿ç´ ç­‰è‰²ç´ ï¼Œè¿™äº›è‰²ç´ ä¼šå¸æ”¶å¤ªé˜³å…‰ä¸­çš„èƒ½é‡ï¼Œè½¬åŒ–ä¸ºç”Ÿç‰©ä½“å¯ä»¥åˆ©ç”¨çš„èƒ½é‡ã€‚è€Œäººç±»æ’æ³„ç‰©ä¸­å«æœ‰çš„ç›åˆ†ä¸»è¦æ¥æºäºçŸ¿ç‰©è´¨å’Œç¢³é…¸ç›ï¼Œè¿™äº›çŸ¿ç‰©è´¨åœ¨æ°´ä¸­çš„æµ“åº¦è¾ƒé«˜ï¼Œä¼šå¯¼è‡´æ°´ä¸­æº¶è§£çš„çŸ¿ç‰©è´¨æµ“åº¦å¢åŠ ï¼Œå½¢æˆæ°´ä½“ä¸­çš„ç›åˆ†ã€‚å› æ­¤ï¼Œæµ·æ°´ä¹‹æ‰€ä»¥æ˜¯å’¸çš„ï¼Œæ˜¯ç”±äºå…¶ä¸­çš„åŒ–å­¦æˆåˆ†å’Œç»“æ„ç‰¹å¾ã€‚æµ·æ°´ä¸­çš„çŸ¿ç‰©è´¨å’Œç›åˆ†ä¸»è¦æ¥è‡ªäºå²©çŸ³ã€åœŸå£¤ã€ç”Ÿç‰©å’Œäººç±»æ’æ³„ç‰©ç­‰ï¼Œè€Œè¿™äº›ç‰©è´¨çš„ç§¯ç´¯å’Œæ’æ³„ä¼šå¯¼è‡´æµ·æ°´å‘ˆç°å‡ºå’¸å‘³ã€‚
[B]: æµ·æ°´æ˜¯å’¸çš„ï¼Œå› ä¸ºå®ƒåŒ…å«äº†å¤šç§ç‰©è´¨ï¼ŒåŒ…æ‹¬æ°¯åŒ–é’ ï¼ˆNaClï¼‰ã€æ°¯åŒ–é’ ï¼ˆNaClï¼‰ã€ç¢³é…¸ç›ï¼ˆå¦‚ç¡«é…¸ç›ï¼‰ã€æ°¯åŒ–é’ ï¼ˆNaClï¼‰å’Œæ°´ã€‚å’¸ç‰©è´¨çš„æ¥æºï¼šç›æ˜¯å’¸çš„å¸¸è§æ¥æºï¼Œå«æœ‰é’¾ã€é’™ã€é•ã€é’¾ç­‰çŸ¿ç‰©è´¨ã€‚è¿™äº›çŸ¿ç‰©è´¨é€šå¸¸æ¥è‡ªäºåœ°ä¸‹æ°´ã€åœ°ä¸‹æ°´æˆ–åœŸå£¤ä¸­ï¼Œè¿™äº›çŸ¿ç‰©è´¨çš„å«é‡è¿œä½äºçº¯å‡€æ°´ã€‚å’¸å‘³çš„åŸå› ï¼šç›çš„å’¸å‘³ä¸»è¦æºäºå…¶æº¶è§£äºæ°´ä¸­çš„å¾®å°é¢—ç²’ç‰©ã€‚è¿™äº›å°é¢—ç²’ç‰©ä¸»è¦é€šè¿‡æ¤ç‰©æ ¹éƒ¨å’ŒåŠ¨ç‰©ä½“å†…çš„å¾®ç”Ÿç‰©ä¼ æ’­ï¼Œå¦‚æ˜†è™«ã€å°å‹å“ºä¹³åŠ¨ç‰©ã€é¸Ÿç±»å’ŒæŸäº›æ˜†è™«ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œæµ·æ°´æ˜¯å’¸çš„ï¼Œå› ä¸ºå®ƒå«æœ‰å¤§é‡çš„ç›ç±»ï¼ŒåŒ…æ‹¬çŸ¿ç‰©è´¨ã€å²©çŸ³ã€ç›ã€çŸ¿ç‰©è´¨ç­‰ï¼Œè¿™äº›ç‰©è´¨åœ¨æµ·æ°´ä¸­çš„æµ“åº¦é«˜ï¼Œå¯¹å’¸å‘³çš„äº§ç”Ÿè¾ƒå¤§å½±å“ã€‚
[C]: æµ·æ°´æ˜¯å’¸çš„ï¼Œä¸»è¦æ˜¯å› ä¸ºå…¶ä¸­å«æœ‰å„ç§ç›åˆ†ï¼Œå…¶ä¸­ä»¥æµ·æ°´ä¸ºä¸»è¦æˆåˆ†ä¹‹ä¸€ã€‚å½“æµ·æ°´ä¸­å«æœ‰å¤§é‡çš„ç›åˆ†æ—¶ï¼Œç›åˆ†ä¼šè¢«æº¶è§£åœ¨æµ·æ°´ä¸­ï¼Œå½¢æˆå’¸å‘³ã€‚æµ·æ°´ä¸­çš„ç›åˆ†ä¸»è¦æ¥æºäºæº¶è§£ç›ç±»ã€è’¸å‘æå‡ºçš„ç›å’Œæµ·æ°´å†»ç»“æˆå†°ç­‰å› ç´ ã€‚
[D]: æµ·æ°´ä¸­å«æœ‰è®¸å¤šç›åˆ†ï¼Œè¿™äº›ç›åˆ†å¯ä»¥å¯¼è‡´æµ·æ°´å’¸åº¦çš„å¢åŠ ã€‚æµ·æ°´ä¸­çš„ç›åˆ†ä¸»è¦æ¥è‡ªäºæµ·æ°´ä¸­çš„ç›åˆ†ï¼Œè¿™äº›ç›åˆ†å¯ä»¥ä»æµ·æ°´ä¸­çš„ç›åˆ†ä¸­è·å¾—ã€‚æµ·æ°´ä¸­çš„æ°§åŒ–ç‰©å’Œç›åˆ†å¯ä»¥ä¸æµ·æ°´ä¸­çš„ç›åˆ†å½¢æˆæ°¢é”®ï¼Œä»è€Œä½¿æµ·æ°´å’¸åº¦å¢åŠ ã€‚
[E]: æµ·æ°´æ˜¯å’¸çš„ï¼Œè¿™æ˜¯å› ä¸ºå®ƒå«æœ‰å¤§é‡çš„ç›åˆ†ï¼Œå…¶ä¸­åŒ…æ‹¬æ°¯ç¦»å­å’Œæ°¯ç¦»å­ã€‚è¿™äº›ç¦»å­åœ¨æµ·æ´‹ä¸­å½¢æˆäº†å¤§é‡çš„ç›ï¼Œè¿™äº›ç›åˆ†è¢«ç§°ä¸ºç›åˆ†ã€‚æµ·æ°´ä¸­å«æœ‰è®¸å¤šæº¶è§£åœ¨æ°´ä¸­çš„ç›ï¼Œè¿™äº›ç›åˆ†ä¼šéšç€æµ·æ°´çš„å‡é«˜è€Œé€æ¸è’¸å‘ï¼Œæœ€ç»ˆå¯¼è‡´æµ·æ°´ä¸­ç›åˆ†çš„è’¸å‘ã€‚è¿™äº›ç›åˆ†åœ¨æµ·æ°´ä¸­è¢«å®Œå…¨æ¶ˆè€—ï¼Œç„¶åè¢«æ’é™¤åœ¨æµ·æ°´ä¸­ã€‚æ­¤å¤–ï¼Œæµ·æ°´ä¸­è¿˜å«æœ‰è®¸å¤šå…¶ä»–çš„ç›åˆ†ï¼Œä½†è¿™äº›ç›åˆ†æ˜¯ä¸åŒçš„ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ°´ä¸­çš„æµ®èµ·å’Œä¸‹é™è¿‡ç¨‹æ˜¯ä¸åŒçš„ã€‚æµ·æ°´ä¸­çš„ç›åˆ†æ˜¯ä»æµ·åº•çš„å²©çŸ³ä¸­æå–å‡ºæ¥çš„ï¼Œè¿™äº›ç›åˆ†åœ¨æµ·æ°´ä¸­å¯ä»¥è¢«åˆ©ç”¨ã€‚æ€»çš„æ¥è¯´ï¼Œæµ·æ°´æ˜¯å’¸çš„ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæµ·æ°´ä¸­å«æœ‰è®¸å¤šç›åˆ†å’Œç›åˆ†çš„åŸå› ã€‚
[F]: æµ·æ°´å«æœ‰è®¸å¤šæº¶è§£çš„ç›ç±»ï¼Œå…¶ä¸­æœ€ä¸»è¦çš„æ˜¯æ°¯åŒ–é’ ã€‚å½“æ°´ä¸­åŠ å…¥ç›æ—¶ï¼Œç›çš„åˆ†å­ä¼šä¸æ°´åˆ†å­å‘ç”Ÿååº”ï¼Œå½¢æˆæ°¢æ°§åŒ–é’ ï¼Œè¿™æ˜¯ä¸€ç§ç›ã€‚
[G]: å› ä¸ºæ°´æ˜¯è®¸å¤šç”Ÿç‰©çš„ç”Ÿå­˜æ‰€å¿…éœ€çš„ï¼Œå®ƒèƒ½å¤Ÿå¸æ”¶å’Œé‡Šæ”¾äºŒæ°§åŒ–ç¢³ï¼Œé‡Šæ”¾æ°§æ°”ï¼Œç»´æŒå¤§æ°”ä¸­çš„æ°´è’¸æ°”ï¼Œä»¥åŠç»´æŒæ°´å¾ªç¯ã€‚æ°´ä¸­çš„ç›ç±»å’ŒåŒ–å­¦ç‰©è´¨ä¼šé‡Šæ”¾å‡ºæ°§æ°”ï¼Œä½¿åœ°çƒä¸Šçš„ç”Ÿç‰©å‘¼å¸ã€‚æ­¤å¤–ï¼Œæ°´è¿˜å…·æœ‰è®¸å¤šå…¶ä»–åŠŸèƒ½ï¼Œå¦‚ä¿æŒæ°´å¾ªç¯ã€ç»´æŒæ°´å¾ªç¯ã€è°ƒèŠ‚æ°”å€™å’Œç¯å¢ƒç­‰ã€‚æ°´æ˜¯è®¸å¤šç”Ÿç‰©ç”Ÿå­˜æ‰€å¿…éœ€çš„ï¼Œå®ƒä¹Ÿæ˜¯è®¸å¤šè‡ªç„¶ç°è±¡çš„é‡è¦åŸºç¡€ã€‚
[H]: æµ·æ°´æ˜¯å’¸çš„ä¸»è¦åŸå› æ˜¯å› ä¸ºå…¶ä¸­å«æœ‰å¤§é‡çš„æº¶è§£ç›ç±»ï¼Œå…¶ä¸­æœ€ä¸»è¦çš„æ˜¯æ°¯åŒ–é’ ï¼ˆNaClï¼‰ã€‚æ°¯åŒ–é’ æ˜¯ä¸€ç§éå¸¸å¸¸è§çš„ç›ç±»ï¼Œå®ƒå­˜åœ¨äºåœ°çƒä¸Šçš„å¤§éƒ¨åˆ†åœ°åŒºï¼ŒåŒ…æ‹¬æµ·æ´‹ã€æ¹–æ³Šã€æ²³æµå’Œåœ°ä¸‹æ°´ã€‚å½“æ°´ä»æµ·æ´‹ä¸­è’¸å‘æ—¶ï¼Œå®ƒä¼šå¸¦èµ°ä¸€éƒ¨åˆ†ç›åˆ†ï¼Œä½¿æµ·æ°´å˜å¾—å’¸ã€‚æ­¤å¤–ï¼Œæµ·ä¸­è¿˜å«æœ‰å…¶ä»–æº¶è§£ç›ç±»çš„ç‰©è´¨ï¼Œå¦‚ç¡«é…¸ç›ã€ç¡«é…¸é•ã€ç¡«é…¸é’™ã€ç¡«é…¸é•ç›ã€ç¡«é…¸é•é’¾ã€ç¡«é…¸é•é’ ã€ç¡«é…¸é•é’™ã€ç¡«é…¸é•é•é’™ã€ç¡«é…¸é•é•é’™ã€ç¡«é…¸é•é•é’™ã€ç¡«é…¸é•é•é’ ã€ç¡«é…¸é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•é•é•é•é’™ã€ç¡«é…¸é•é•é•é•


[Q]: å†™ä¸€æ®µpythonçš„å¿«é€Ÿæ’åºä»£ç 
[A]: å¥½çš„ï¼Œä»¥ä¸‹æ˜¯Pythonçš„å¿«é€Ÿæ’åºä»£ç ï¼šdef quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x &amp;lt; pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# æµ‹è¯•ä»£ç arr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))è¿™æ®µä»£ç é¦–å…ˆæ£€æŸ¥æ•°ç»„çš„é•¿åº¦æ˜¯å¦å°äºç­‰äº1ã€‚å¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå®ƒå°±æŒ‰ç…§ä»å°åˆ°å¤§çš„é¡ºåºæ’åˆ—ã€‚æ¥ç€ï¼Œå¯¹äºæ•°ç»„çš„æ¯ä¸ªå…ƒç´ ï¼Œå®ƒæƒ³è¦è¿›è¡Œé€’å½’åœ°æ’åºï¼Œç›´åˆ°è¾¾åˆ°æ’åºçš„ç»“æŸä½ç½®ã€‚å¦‚æœæ•°ç»„çš„é•¿åº¦æ˜¯å¥‡æ•°ï¼Œé‚£ä¹ˆå®ƒå°±æŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåºæ’åˆ—ã€‚æœ€åï¼Œå¹³å‡çš„æ—¶é—´å¤æ‚åº¦æ˜¯O(n log n)ï¼Œå…¶ä¸­næ˜¯æ•°ç»„çš„é•¿åº¦ã€‚
[B]: ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨Pythonçš„å¿«é€Ÿæ’åºç®—æ³•çš„å®ç°ï¼šdef sort_array(arr):    # ç”Ÿæˆä¸€ä¸ªéšæœºæ’åˆ—    sorted_array = sorted(arr)    # è¿”å›æ’åºåçš„åˆ—è¡¨    return sorted(arr)# ç¤ºä¾‹arr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# è°ƒç”¨å‡½æ•°sorted_array = sort_array(arr)print("åŸå§‹æ•°ç»„æ˜¯ï¼š", sorted_array)è¿™æ®µä»£ç é¦–å…ˆç”Ÿæˆä¸€ä¸ªéšæœºæ’åˆ—ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªåˆ—è¡¨çš„é•¿åº¦è¿›è¡Œæ’åºã€‚æœ€åï¼Œè¿”å›æ’åºåçš„åˆ—è¡¨ã€‚æ³¨æ„ï¼Œè¿™ä¸ªä»£ç ä½¿ç”¨äº†Pythonçš„å†…ç½®sortedå‡½æ•°å’Œå†…ç½®çš„sorted()å‡½æ•°ï¼Œå®ƒä»¬åœ¨æ—¶é—´å¤æ‚åº¦ä¸ºO(n log n)çš„æƒ…å†µä¸‹ï¼Œæ¯”ç›´æ¥ä½¿ç”¨sorted()å‡½æ•°æ›´å¿«ã€‚
[C]: ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„æ’åºç®—æ³•ï¼Œé€‚ç”¨äºå°å‹æ•°ç»„ï¼ŒåŒ…æ‹¬å°äºç­‰äº5çš„æ•°ç»„ã€‚def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# ç¤ºä¾‹arr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print("Sorted array is:", sorted_arr)### ä»£ç è§£é‡Šï¼š1. **å‡½æ•°å®šä¹‰**ï¼šquick_sort å‡½æ•°æ¥å—ä¸€ä¸ªæ•°ç»„ arr ä½œä¸ºå‚æ•°ã€‚2. **å¾ªç¯**ï¼šå¯¹äº arr çš„å…ƒç´ ï¼Œä½¿ç”¨é€’å½’æ¥å®ç°ï¼Œå³ä»å³åˆ°å·¦éå†æ•°ç»„ï¼Œå¹¶å°†å½“å‰å…ƒç´ ä¸æ•°ç»„çš„å½“å‰å…ƒç´ äº¤æ¢ä½ç½®ï¼Œç›´åˆ°æ•°ç»„è¾¾åˆ°ç©ºæˆ–æ•°ç»„é•¿åº¦å°äºç­‰äº arrã€‚3. **é€’å½’è°ƒç”¨**ï¼šéå†æ•°ç»„ï¼Œå°†å½“å‰å…ƒç´ ä¸æ•°ç»„çš„å½“å‰å…ƒç´ è¿›è¡Œäº¤æ¢ï¼Œç›´åˆ°æ•°ç»„è¾¾åˆ°ç©ºæˆ–æ•°ç»„é•¿åº¦å°äºç­‰äº arrã€‚4. **æ£€æŸ¥æ•°ç»„é•¿åº¦**ï¼šä½¿ç”¨ if __name__ == "__main__" æ£€æŸ¥æ•°ç»„çš„é•¿åº¦æ˜¯å¦å°äº arrï¼Œå¦‚æœæ˜¯ï¼Œå°±å°†æ•°ç»„ä»è¾ƒå¤§çš„æ•°ç»„ç§»åŠ¨åˆ°æ•°ç»„çš„æœ«å°¾ã€‚5. **è¾“å‡ºæ’åºåçš„ç»“æœ**
[D]: ä½œä¸ºä¸€ä¸ªAIè¯­è¨€æ¨¡å‹ï¼Œæˆ‘æ— æ³•ç¼–å†™Pythonç¨‹åºã€‚ä½†æ˜¯ï¼ŒPythonæ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå®ƒå…·æœ‰è®¸å¤šä¼˜ç§€çš„åº“å’Œæ¡†æ¶ï¼ŒåŒ…æ‹¬NumPyã€Pandasã€Matplotlibã€Pandasã€Bandkeã€Normå’ŒNKlearnç­‰ã€‚Pythonæœ‰å¾ˆå¤šä¼˜ç§€çš„è‡ªç„¶è¯­è¨€å¤„ç†åº“å’Œæ¡†æ¶ï¼Œå¯ä»¥ç”¨äºå¤„ç†å’Œåˆ†æå„ç§ç±»å‹çš„æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰ã€‚Pythonè¿˜å¯ä»¥ç”¨äºåˆ›å»ºå„ç§åº”ç”¨ç¨‹åºï¼Œå¦‚ç½‘é¡µæœç´¢ã€æ¸¸æˆå¼€å‘ã€ç§‘å­¦è®¡ç®—ç­‰ã€‚Pythonéå¸¸çµæ´»ï¼Œä¹Ÿæ”¯æŒå¤šç§ç¼–ç¨‹é£æ ¼ï¼Œå› æ­¤å¯ä»¥è½»æ¾åœ°æ‰¾åˆ°é€‚åˆè‡ªå·±çš„è§£å†³æ–¹æ¡ˆã€‚
[E]: ä½œä¸ºAIè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å¯ä»¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚ä½†æ˜¯æˆ‘æ— æ³•è¿›è¡Œç¼–ç¨‹æ“ä½œï¼Œå› ä¸ºæˆ‘æ— æ³•ç¡®å®šæ‚¨æƒ³é—®çš„æ˜¯å“ªç§ç¼–ç¨‹è¯­è¨€ã€‚è¯·æ‚¨æä¾›æ›´å…·ä½“çš„é—®é¢˜ï¼Œä»¥ä¾¿æˆ‘èƒ½å¤Ÿæ›´å¥½åœ°å›ç­”æ‚¨çš„é—®é¢˜ã€‚
[F]: å¿«é€Ÿæ’åºï¼ˆQuick Sortï¼‰æ˜¯ä¸€ç§ç®€å•çš„æ’åºç®—æ³•ï¼Œå®ƒé€šè¿‡å°†æ•°æ®åˆ’åˆ†ä¸ºå°ç»„ï¼Œæ¯ç»„æ•°æ®æŒ‰ç…§ä¸€å®šè§„åˆ™æ’åºï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæœ‰åºæ•°ç»„ã€‚
[G]: å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€æ®µå¿«é€Ÿæ’åºçš„Pythonä»£ç ```python sorted_print(sorted(sorted(sorted(sorted_print))```è¾“å‡ºç»“æœä¸ºï¼š```[1, 2, 3, 4, 5, 6, 7, 8, 9]```
[H]: å¿«é€Ÿæ’åºæ˜¯ä¸€ç§åŸºäºåˆ†æ²»æ€æƒ³çš„é«˜æ•ˆæ’åºç®—æ³•ï¼Œå®ƒçš„æ—¶é—´å¤æ‚åº¦ä¸º O(n log n)ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡ä¸€è¶Ÿæ’åºå°†å¾…æ’åºåºåˆ—åˆ’åˆ†ä¸ºä¸¤ä¸ªå­åºåˆ—ï¼Œå…¶ä¸­ä¸€ä¸ªå­åºåˆ—çš„æ‰€æœ‰å…ƒç´ éƒ½æ¯”å¦ä¸€ä¸ªå­åºåˆ—çš„æ‰€æœ‰å…ƒç´ å°ï¼Œç„¶åå¯¹è¿™ä¸¤ä¸ªå­åºåˆ—åˆ†åˆ«é€’å½’åœ°è¿›è¡Œæ’åºã€‚ä¸‹é¢æ˜¯ä¸€ä¸ª Python ä»£ç ç¤ºä¾‹ï¼Œå®ç°äº†å¿«é€Ÿæ’åºç®—æ³•ï¼š```pythondef quick_sort(arr):    if len(arr) &amp;gt; 1:        return arr    else: return quick_sort(arr)```è¯¥ä»£ç ä½¿ç”¨äº†é€’å½’çš„æ–¹å¼ï¼Œé¦–å…ˆåˆ¤æ–­æ•°ç»„çš„é•¿åº¦æ˜¯å¦å°äºç­‰äº1ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ç›´æ¥è¿”å›æ•°ç»„ï¼Œå¦åˆ™é€’å½’åœ°å¯¹æ•°ç»„è¿›è¡Œæ’åºã€‚åœ¨é€’å½’è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ¤æ–­æ•°ç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦å¤§äºç­‰äº1ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å°†æ•°ç»„ä¸­çš„å…ƒç´ ä¸ç¬¬ä¸€ä¸ªå…ƒç´ è¿›è¡Œæ¯”è¾ƒï¼Œå¦‚æœä¸ç­‰äº1ï¼Œåˆ™å°†æ•°ç»„ä¸­çš„å…ƒç´ ä¸ç¬¬äºŒä¸ªå…ƒç´ è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ°æ‰€æœ‰å…ƒç´ éƒ½è¢«æ’åºã€‚è¯¥ä»£ç çš„æ—¶é—´å¤æ‚åº¦ä¸º O(n log n)ï¼Œæ˜¯ä¸€ä¸ªéå¸¸å®ç”¨çš„æ’åºç®—æ³•ã€‚


[Q]: æˆªè‡³2025å¹´2æœˆ9æ—¥14æ—¶31åˆ†ï¼Œç”µå½±ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹ç´¯è®¡ç¥¨æˆ¿ï¼ˆå«é¢„å”®ï¼‰çªç ´78.09äº¿å…ƒï¼Œè¶…è¿‡ã€Šæ˜Ÿçƒå¤§æˆ˜ï¼šå¤©è¡Œè€…å´›èµ·ã€‹çš„ç¥¨æˆ¿æˆç»©ï¼Œæˆä¸ºé¦–éƒ¨è¿›å…¥å…¨çƒç¥¨æˆ¿æ¦œå‰40çš„äºšæ´²ç”µå½±ã€‚è¿™ä¸€æˆå°±ä¸ä»…æ ‡å¿—ç€ä¸­å›½ç”µå½±åœ¨å›½é™…å¸‚åœºçš„çªç ´ï¼Œä¹Ÿå±•ç¤ºäº†ä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›å’Œå¸‚åœºå¸å¼•åŠ›ã€‚ä»è¿™æ®µæ–‡å­—æ€»ç»“20å­—çš„æ‘˜è¦ç»™æˆ‘ã€‚
[A]: ç”µå½±ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹ç¥¨æˆ¿çªç ´78.09äº¿å…ƒï¼Œæˆä¸ºé¦–éƒ¨è¿›å…¥å…¨çƒç¥¨æˆ¿æ¦œå‰40çš„äºšæ´²ç”µå½±ã€‚è¿™æ ‡å¿—ç€ä¸­å›½ç”µå½±çš„å›½é™…çªç ´ï¼Œä¹Ÿå±•ç°äº†ä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›ã€‚
[B]: 2025å¹´2æœˆ9æ—¥ï¼Œä¸­å›½ç”µå½±åœ¨ä¸–ç•Œèˆå°ä¸Šçªç ´78.09äº¿å…ƒï¼Œè¶…è¿‡ã€Šæ˜Ÿçƒå¤§æˆ˜ï¼šå¤©è¡Œè€…å´›èµ·ã€‹çš„ç¥¨æˆ¿æˆç»©ã€‚
[C]: ç”µå½±ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹ç¥¨æˆ¿çªç ´78.09äº¿å…ƒï¼Œæˆä¸ºå…¨çƒç¥¨æˆ¿æ¦œå‰40çš„äºšæ´²ç”µå½±ï¼Œå±•ç°ä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›ã€‚
[D]: ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹ç´¯è®¡ç¥¨æˆ¿çªç ´78.09äº¿ï¼Œæˆä¸ºé¦–éƒ¨è¿›å…¥å…¨çƒç¥¨æˆ¿æ¦œå‰40çš„äºšæ´²ç”µå½±ï¼Œå±•ç¤ºäº†ä¸­å›½ç”µå½±åœ¨å›½é™…å¸‚åœºçš„çªç ´ï¼ŒåŒæ—¶å±•ç¤ºäº†ä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›å’Œå¸‚åœºå¸å¼•åŠ›ã€‚
[E]: ã€ŠçŒ›é¾™å¼ºï¼šæ‰“å¼€ä¸–ç•Œå¥‡å¹»å†’é™©ã€‹æˆåŠŸåœ°ï¼Œä¸Šæµ·æˆåŠŸå¸å¼•äº†ä¼—å¤šè§‚ä¼—ï¼ŒåŒæ—¶ä¹Ÿå¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚
[F]: ã€Šå“ªå’ä¹‹é­”ç«¥é—¹æµ·ã€‹æ˜¯ä¸€éƒ¨è®©ä¸­å›½åŠ¨ç”»ç”µå½±æˆä¸ºå›½é™…å¸‚åœºçš„å·¨å¤§æ½œåŠ›ã€‚è¯¥ç”µå½±ä»¥å…¶åä¸½çš„ç”»é¢å’ŒæƒŠé™©çš„æˆ˜æ–—åœºé¢è€Œé—»åäºä¸–ã€‚
[G]: ã€Šå“ªè¡Œæ˜¯å€¼å¾—çš„ã€‹æ˜¯2021å¹´1æœˆ9æ—¥14æ—¶31åˆ†ï¼Œç”µå½±ã€Šå“ªæˆå¤§ä½œä¸ºã€‹åœ¨ç”µå½±å¸‚åœºçªç ´7800äº¿ç¾å…ƒï¼Œæˆä¸ºä¸­å›½åŠ¨ç”»ç”µå½±çš„çªç ´ï¼Œå¹¶å±•ç¤ºä¸­å›½åŠ¨ç”»ç”µå½±çš„å·¨å¤§æ½œåŠ›å’Œå¸‚åœºå¸å¼•åŠ›ã€‚
[H]: æˆ‘æ˜¯ä¸€ä¸ªå‚æ•°å¾ˆå°‘çš„AIæ¨¡å‹ğŸ¥ºï¼ŒçŸ¥è¯†åº“è¾ƒå°‘ï¼Œæ— æ³•ç›´æ¥å›ç­”æ‚¨çš„é—®é¢˜ï¼Œæ¢ä¸ªé—®é¢˜è¯•è¯•å§ğŸ‘‹
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;ğŸ™‹â€ç›´æ¥æŠŠä»¥ä¸Šæ‰€æœ‰é—®é¢˜å’Œæ¨¡å‹çš„å›ç­”ä¸¢ç»™DeepSeek-R1ï¼Œè®©å®ƒå¸®å¿™ç‚¹è¯„å’Œæ’åæ‰“åˆ†ï¼š&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;å…·ä½“ç‚¹è¯„&lt;/summary&gt; 
 &lt;h3&gt;è¯„åˆ†æ ‡å‡†ï¼š&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;å‡†ç¡®æ€§&lt;/strong&gt;ï¼šå›ç­”æ˜¯å¦æ­£ç¡®ä¸”æ— æ˜æ˜¾é”™è¯¯ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;å®Œæ•´æ€§&lt;/strong&gt;ï¼šå›ç­”æ˜¯å¦æ¶µç›–äº†é—®é¢˜çš„æ ¸å¿ƒè¦ç‚¹ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;é€»è¾‘æ€§&lt;/strong&gt;ï¼šå›ç­”æ˜¯å¦æœ‰æ¡ç†ï¼Œæ˜¯å¦ç¬¦åˆé€»è¾‘ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ä»£ç è´¨é‡&lt;/strong&gt;ï¼šä»£ç æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œï¼Œé€»è¾‘æ˜¯å¦æ¸…æ™°ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;ç‚¹è¯„ï¼š&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Aæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”éå¸¸å…¨é¢ï¼Œä¿¡æ¯é‡å¤§ï¼Œé€»è¾‘æ¸…æ™°ï¼Œå°¤å…¶åœ¨é•¿æ±Ÿã€å¤§ç†ŠçŒ«ã€æµ·æ°´å’¸å‘³ç­‰é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä»£ç è™½ç„¶æœ‰å°ç‘•ç–µï¼Œä½†æ•´ä½“æ€è·¯æ­£ç¡®ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”ç¨æ˜¾å†—é•¿ï¼Œä½†ä¸å½±å“æ•´ä½“è´¨é‡ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šç»¼åˆè¡¨ç°æœ€ä½³ï¼Œå¾—åˆ†æœ€é«˜ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”è¾ƒä¸ºå‡†ç¡®ï¼Œå°¤å…¶åœ¨ç ç©†æœ—ç›å³°ã€ä¸‡æœ‰å¼•åŠ›ç­‰é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ã€‚ä»£ç è™½æœªå®Œå…¨å±•ç¤ºï¼Œä½†è§£é‡Šè¾ƒä¸ºè¯¦ç»†ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”ç•¥æ˜¾å•°å—¦ï¼Œä½†é€»è¾‘æ€§è¾ƒå¼ºã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šä»…æ¬¡äºAæ¨¡å‹ï¼Œè¡¨ç°ç¨³å®šã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”ç®€æ´æ˜äº†ï¼Œå°¤å…¶åœ¨å¤§ç†ŠçŒ«ã€å¿«é€Ÿæ’åºä»£ç ç­‰é—®é¢˜ä¸Šè¡¨ç°è¾ƒå¥½ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”ç•¥æ˜¾ç®€çŸ­ï¼Œç¼ºä¹æ·±å…¥è§£é‡Šã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šæ•´ä½“è¡¨ç°ä¸é”™ï¼Œä½†ç»†èŠ‚ä¸Šç•¥é€ŠäºAå’ŒHã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”è¾ƒä¸ºå‡†ç¡®ï¼Œå°¤å…¶åœ¨é•¿æ±Ÿã€ä¸‡æœ‰å¼•åŠ›ç­‰é—®é¢˜ä¸Šè¡¨ç°å°šå¯ã€‚ä»£ç éƒ¨åˆ†æœ‰ä¸€å®šé€»è¾‘æ€§ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”ä¸å¤Ÿæ·±å…¥ï¼Œä»£ç å­˜åœ¨ä¸€äº›å°é—®é¢˜ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°ä¸­è§„ä¸­çŸ©ï¼Œä»æœ‰æå‡ç©ºé—´ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”åŸºæœ¬å‡†ç¡®ï¼Œå°¤å…¶åœ¨ä¸‡æœ‰å¼•åŠ›ã€é•¿æ±Ÿç­‰é—®é¢˜ä¸Šè¡¨ç°å°šå¯ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”è¿‡äºç®€ç•¥ï¼Œä»£ç å­˜åœ¨æ˜æ˜¾é”™è¯¯ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°ä¸€èˆ¬ï¼Œéœ€æ”¹è¿›ä»£ç éƒ¨åˆ†ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå›ç­”è¾ƒä¸ºå‡†ç¡®ï¼Œå°¤å…¶åœ¨é•¿æ±Ÿã€æµ·æ°´å’¸å‘³ç­‰é—®é¢˜ä¸Šè¡¨ç°å°šå¯ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”é€»è¾‘æ€§è¾ƒå·®ï¼Œä»£ç å­˜åœ¨è¾ƒå¤§é—®é¢˜ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°å¹³å¹³ï¼Œéœ€è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Eæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šéƒ¨åˆ†å›ç­”è¾ƒä¸ºå‡†ç¡®ï¼Œå°¤å…¶åœ¨æµ·æ°´å’¸å‘³ã€å¤§ç†ŠçŒ«ç­‰é—®é¢˜ä¸Šè¡¨ç°å°šå¯ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šå›ç­”è¿‡äºç®€ç•¥ï¼Œä»£ç éƒ¨åˆ†å‡ ä¹æ— æ³•è¿è¡Œã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°ä¸ä½³ï¼Œéœ€å¤§å¹…æå‡ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Gæ¨¡å‹&lt;/strong&gt;ï¼š&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;ä¼˜ç‚¹&lt;/strong&gt;ï¼šå‡ ä¹æ²¡æœ‰æ˜æ˜¾çš„ä¼˜ç‚¹ã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼šå›ç­”ä¸¥é‡åç¦»ä¸»é¢˜ï¼Œä»£ç éƒ¨åˆ†å®Œå…¨æ— æ³•è¿è¡Œã€‚&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;æ€»è¯„&lt;/strong&gt;ï¼šè¡¨ç°æœ€å·®ï¼Œéœ€å¤§å¹…æ”¹è¿›ã€‚&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;æ€»ç»“ï¼š&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Aæ¨¡å‹&lt;/strong&gt;åœ¨å„æ–¹é¢è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œå°¤å…¶åœ¨å¤æ‚é—®é¢˜çš„å›ç­”ä¸Šå±•ç°äº†æé«˜çš„å‡†ç¡®æ€§ä¸é€»è¾‘æ€§ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Hæ¨¡å‹&lt;/strong&gt;ç´§éšå…¶åï¼Œè¡¨ç°ç¨³å®šï¼Œä½†åœ¨æŸäº›ç»†èŠ‚ä¸Šç•¥æ˜¾ä¸è¶³ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Gæ¨¡å‹&lt;/strong&gt;è¡¨ç°æœ€å·®ï¼Œå›ç­”åç¦»ä¸»é¢˜ä¸”ä»£ç æ— æ³•è¿è¡Œï¼Œéœ€å¤§å¹…æ”¹è¿›ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;æ‰“åˆ†æ’åº&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;æ’å&lt;/th&gt; 
   &lt;th&gt;æ¨¡å‹&lt;/th&gt; 
   &lt;th&gt;å‡†ç¡®æ€§ (30åˆ†)&lt;/th&gt; 
   &lt;th&gt;å®Œæ•´æ€§ (30åˆ†)&lt;/th&gt; 
   &lt;th&gt;é€»è¾‘æ€§ (20åˆ†)&lt;/th&gt; 
   &lt;th&gt;ä»£ç è´¨é‡ (20åˆ†)&lt;/th&gt; 
   &lt;th&gt;æ€»åˆ† (100åˆ†)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;A&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;96&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;H&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;C&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;89&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;F&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;86&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;D&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;82&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;B&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;E&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;G&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;42&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ğŸ‘‰ä¸»è§‚æ•ˆæœæ€»ç»“&lt;/h3&gt; 
&lt;p&gt;ä¸ªäººä¸»è§‚è¯„ä»·ä¸DeepSeek-R1åŸºæœ¬ç›¸ç¬¦ï¼Œå…¶ä¸­ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;MiniMindç³»åˆ—çš„æ’åºéå¸¸ç¬¦åˆç›´è§‰ï¼Œå‚æ•°è¶Šå¤§+è®­ç»ƒæ•°æ®è¶Šå……åˆ†è¯„åˆ†è¶Šé«˜ï¼Œå¹»è§‰å’Œé”™è¯¯éƒ½ä¼šæ¯”å°æ¨¡å‹è‚‰çœ¼å¯è§çš„å¥½ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Hæ¨¡å‹çš„å›ç­”è‚‰çœ¼çœ‹èµ·æ¥æ˜¯ä¸é”™çš„ï¼Œå°½ç®¡å­˜åœ¨äº›è®¸å¹»è§‰çç¼–çš„æƒ…å†µã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Gæ¨¡å‹å¯èƒ½è®­ç»ƒæ•°æ®ä¸å¤Ÿå®Œå¤‡ï¼Œç»™å‡ºçš„æƒé‡ç»è¿‡æµ‹è¯•æ•ˆæœä¸ä½³ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å†å¤è¯µä¸€éç»ä¹…ä¸è¡°çš„Scaling Law: å‚æ•°è¶Šå¤§ï¼Œè®­ç»ƒæ•°æ®è¶Šå¤šæ¨¡å‹çš„æ€§èƒ½è¶Šå¼ºã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â…£ RoPEé•¿åº¦å¤–æ¨&lt;/h2&gt; 
&lt;p&gt;MiniMindæ”¯æŒé€šè¿‡YaRNç®—æ³•è¿›è¡ŒRoPEä½ç½®ç¼–ç çš„é•¿åº¦å¤–æ¨ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†è¶…å‡ºè®­ç»ƒé•¿åº¦çš„æ–‡æœ¬åºåˆ—ã€‚ åœ¨ä½¿ç”¨&lt;code&gt;eval_llm.py&lt;/code&gt;è¿›è¡Œæ¨ç†æ—¶ï¼Œåªéœ€æ·»åŠ &lt;code&gt;--inference_rope_scaling&lt;/code&gt;å‚æ•°å³å¯å¯ç”¨RoPEå¤–æ¨ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_llm.py --weight full_sft --inference_rope_scaling
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ä¸‹å›¾å±•ç¤ºäº†åœ¨ä¸åŒæ–‡æœ¬ã€Œè¥¿æ¸¸è®°ã€ç™½è¯æ–‡å°è¯´é•¿åº¦ä¸‹ï¼Œä½¿ç”¨RoPE scalingå‰åçš„å›°æƒ‘åº¦(PPL)å¯¹æ¯”ã€‚å¯ä»¥çœ‹å‡ºï¼Œå¯ç”¨RoPE scalingåï¼Œæ¨¡å‹åœ¨é•¿æ–‡æœ¬ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ï¼š&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/rope_ppl.png" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â…¤ Objective Benchmark&lt;/h2&gt; 
&lt;p&gt;ä¸‹é¢å°±åˆ°å–œé—»ä¹è§çš„benchmarkæµ‹è¯•ç¯èŠ‚ï¼Œå°±ä¸æ‰¾ä¹å­å’ŒQwenã€GLMçº§åˆ«çš„æ¨¡å‹åšå¯¹æ¯”äº†ã€‚ è¿™é‡Œé€‰å–äº†ä¸€äº›å¾®å‹æ¨¡å‹è¿›è¡Œæ¨ªè¯„æ¯”è¾ƒï¼Œ æµ‹è¯•é›†é€‰æ‹©C-Evalã€CMMLUã€A-CLUEã€TMMLU+è¿™å‡ ä¸ªçº¯ä¸­æ–‡è¯­è¨€æ¦œå•ã€‚&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;æµ‹è¯„æ¡†æ¶&lt;/summary&gt; 
 &lt;p&gt;æµ‹è¯„æ¡†æ¶é€‰æ‹©&lt;a href="https://github.com/EleutherAI/lm-evaluation-harness"&gt;lm-evaluation&lt;/a&gt;ï¼Œ å®‰è£…åå¯åŠ¨æµ‹è¯•éå¸¸æ–¹ä¾¿ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;lm_eval --model hf --model_args pretrained=&amp;lt;å¡«å†™æ¨¡å‹è·¯å¾„&amp;gt;,device=cuda,dtype=auto --tasks ceval* --batch_size 8 --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;PS: åœ¨è¿™ç§å…¨æ˜¯é€‰æ‹©é¢˜çš„æµ‹è¯„é›†ä¸­ï¼Œä¸ºäº†é¿å…å›å¤æ ¼å¼çš„éš¾ä»¥å›ºå®šçš„ç‰¹ç‚¹ï¼Œ æ‰€ä»¥å¸¸ç”¨åšæ³•æ˜¯ç›´æ¥æŠŠ&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;å››ä¸ªå­—æ¯å¯¹åº”tokençš„é¢„æµ‹æ¦‚ç‡å–å‡ºæ¥ï¼Œå°†å…¶ä¸­æ¦‚ç‡æœ€å¤§çš„å­—æ¯ä¸æ ‡å‡†ç­”æ¡ˆè®¡ç®—æ­£ç¡®ç‡ã€‚ é€‰æ‹©é¢˜1/4ä¹±é€‰çš„æ­£ç¡®ç‡æ˜¯25%ï¼Œç„¶è€Œè¿™ä¸ªé‡çº§çš„æ‰€æœ‰æ¨¡å‹éƒ½é›†ä¸­åœ¨25é™„è¿‘ï¼Œç”šè‡³å¾ˆå¤šæ—¶å€™ä¸å¦‚çé€‰ï¼Œæ˜¯ä¸æ˜¯åƒæäº†é«˜ä¸­å®Œå½¢å¡«ç©ºçš„æ»‘é“å¢æ­£ç¡®ç‡... MiniMindæ¨¡å‹æœ¬èº«é¢„è®­ç»ƒæ•°æ®é›†å°çš„å¯æ€œï¼Œä¹Ÿæ²¡æœ‰é’ˆå¯¹æ€§çš„å¯¹æµ‹è¯•é›†åšåˆ·æ¦œå¾®è°ƒï¼Œå› æ­¤ç»“æœçº¯å¨±ä¹ï¼š&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;from&lt;/th&gt; 
   &lt;th&gt;paramsâ†“&lt;/th&gt; 
   &lt;th&gt;cevalâ†‘&lt;/th&gt; 
   &lt;th&gt;cmmluâ†‘&lt;/th&gt; 
   &lt;th&gt;aclueâ†‘&lt;/th&gt; 
   &lt;th&gt;tmmlu+â†‘&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;26.52&lt;/td&gt; 
   &lt;td&gt;24.42&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;26.37&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.39&lt;/td&gt; 
   &lt;td&gt;24.63&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;26.6&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
   &lt;td&gt;24.83&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zhanshijinwat/Steel-LLM"&gt;Steel-LLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZhanShiJin&lt;/td&gt; 
   &lt;td&gt;1121M&lt;/td&gt; 
   &lt;td&gt;24.81&lt;/td&gt; 
   &lt;td&gt;25.32&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;24.39&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai-community/gpt2-medium"&gt;GPT2-medium&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;360M&lt;/td&gt; 
   &lt;td&gt;23.18&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;18.6&lt;/td&gt; 
   &lt;td&gt;25.19&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;TinyLlama-1.1B-Chat-V1.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TinyLlama&lt;/td&gt; 
   &lt;td&gt;1100M&lt;/td&gt; 
   &lt;td&gt;25.48&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;25.4&lt;/td&gt; 
   &lt;td&gt;25.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/huggingface/smollm"&gt;SmolLM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFaceTB&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;24.37&lt;/td&gt; 
   &lt;td&gt;25.02&lt;/td&gt; 
   &lt;td&gt;25.37&lt;/td&gt; 
   &lt;td&gt;25.06&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/BAAI/Aquila-135M-Instruct"&gt;Aquila-Instruct&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;BAAI&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;25.11&lt;/td&gt; 
   &lt;td&gt;25.1&lt;/td&gt; 
   &lt;td&gt;24.43&lt;/td&gt; 
   &lt;td&gt;25.05&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/compare_radar.png" alt="compare_radar" /&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ“Œ Others&lt;/h1&gt; 
&lt;h2&gt;æ¨¡å‹è½¬æ¢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/convert_model.py"&gt;./scripts/convert_model.py&lt;/a&gt;å¯ä»¥å®ç°&lt;code&gt;torch / transformers&lt;/code&gt;æ¨¡å‹çš„äº’ç›¸è½¬æ¢&lt;/li&gt; 
 &lt;li&gt;å¦‚æ— ç‰¹åˆ«è¯´æ˜ï¼Œ&lt;code&gt;MiniMind2&lt;/code&gt;æ¨¡å‹å‡é»˜è®¤ä¸º&lt;code&gt;Transformers&lt;/code&gt;æ ¼å¼çš„æ¨¡å‹ï¼Œéœ€æå‰&lt;code&gt;t2t&lt;/code&gt;è½¬æ¢ï¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;åŸºäºMiniMind-APIæœåŠ¡æ¥å£&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/serve_openai_api.py"&gt;./scripts/serve_openai_api.py&lt;/a&gt;å®Œæˆäº†å…¼å®¹openai-apiçš„æœ€ç®€èŠå¤©æ¥å£ï¼Œæ–¹ä¾¿å°†è‡ªå·±çš„æ¨¡å‹æ¥å…¥ç¬¬ä¸‰æ–¹UI ä¾‹å¦‚FastGPTã€OpenWebUIã€Difyç­‰ç­‰ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ä»&lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;Huggingface&lt;/a&gt;ä¸‹è½½æ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œæ–‡ä»¶æ ‘ï¼š&lt;/p&gt; &lt;pre&gt;&lt;code&gt;minimind (root dir)
â”œâ”€&amp;lt;MiniMind-Model-Name&amp;gt;ï¼ˆä¾‹å¦‚MiniMind2ï¼‰
|  â”œâ”€â”€ config.json
|  â”œâ”€â”€ generation_config.json
|  â”œâ”€â”€ model_minimind.py or w/o
|  â”œâ”€â”€ pytorch_model.bin or model.safetensors
|  â”œâ”€â”€ special_tokens_map.json
|  â”œâ”€â”€ tokenizer_config.json
|  â”œâ”€â”€ tokenizer.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å¯åŠ¨èŠå¤©æœåŠ¡ç«¯&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python serve_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æµ‹è¯•æœåŠ¡æ¥å£&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python chat_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;APIæ¥å£ç¤ºä¾‹ï¼Œå…¼å®¹openai apiæ ¼å¼&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl http://ip:port/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{ 
    "model": "model-identifier",
    "messages": [ 
      { "role": "user", "content": "ä¸–ç•Œä¸Šæœ€é«˜çš„å±±æ˜¯ä»€ä¹ˆï¼Ÿ" }
    ], 
    "temperature": 0.7, 
    "max_tokens": 512,
    "stream": true
}'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;img src="https://avatars.githubusercontent.com/u/136984999" height="28" style="vertical-align: middle;" /&gt; &lt;a href="https://github.com/vllm-project/vllm"&gt;vllm&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;vLLMæ˜¯æå…¶æµè¡Œçš„é«˜æ•ˆæ¨ç†æ¡†æ¶ï¼Œæ”¯æŒå¤§æ¨¡å‹å¿«é€Ÿéƒ¨ç½²ï¼Œä¼˜åŒ–æ˜¾å­˜åˆ©ç”¨ä¸ååé‡ã€‚&lt;/p&gt; 
&lt;p&gt;ä»¥openai-serveå½¢å¼å¯åŠ¨ minimind2ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve ./MiniMind2 --model-impl transformers --served-model-name "minimind" --port 8998
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;img src="https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png" height="28" style="vertical-align: middle;" /&gt; &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;llama.cppæ˜¯ä¸€ä¸ªC++åº“ï¼Œ å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸‹ç›´æ¥ä½¿ç”¨ï¼Œæ”¯æŒå¤šçº¿ç¨‹æ¨ç†ï¼Œæ”¯æŒGPUåŠ é€Ÿã€‚&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ç›®å½•ç»“æ„&lt;/strong&gt;ï¼šå»ºè®®å°†llama.cppä¸minimindæ”¾åœ¨åŒçº§ç›®å½•ä¸‹&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;parent/
â”œâ”€â”€ minimind/          # MiniMindé¡¹ç›®ç›®å½•
â”‚   â”œâ”€â”€ MiniMind2/     # HuggingFaceæ ¼å¼MiniMind2æ¨¡å‹ (å…ˆconvert_model.pyç”Ÿæˆ)
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ trainer/
â”‚   â””â”€â”€ ...
â””â”€â”€ llama.cpp/         # llama.cppé¡¹ç›®ç›®å½•
    â”œâ”€â”€ build/
    â”œâ”€â”€ convert_hf_to_gguf.py
    â””â”€â”€ ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;0ã€å‚è€ƒ&lt;code&gt;llama.cpp&lt;/code&gt;å®˜æ–¹æ­¥éª¤è¿›è¡Œinstall&lt;/p&gt; 
&lt;p&gt;1ã€åœ¨&lt;code&gt;convert_hf_to_gguf.py&lt;/code&gt;çš„&lt;code&gt;get_vocab_base_pre&lt;/code&gt;å‡½æ•°æœ€åæ’å…¥ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# æ·»åŠ MiniMind tokenizeræ”¯æŒï¼ˆè¿™é‡Œéšä¾¿å†™ä¸€ä¸ªä¾‹å¦‚qwen2å³å¯ï¼‰
if res is None:
    res = "qwen2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;2ã€è½¬æ¢è‡ªè®­ç»ƒçš„minimindæ¨¡å‹ï¼šhuggingface -&amp;gt; gguf&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åœ¨llama.cppä¸‹æ‰§è¡Œï¼Œå°†ç”Ÿæˆ../minimind/MiniMind2/MiniMind2-xxx.gguf
python convert_hf_to_gguf.py ../minimind/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;3ã€é‡åŒ–æ­¤æ¨¡å‹ (å¯é€‰)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;4ã€å‘½ä»¤è¡Œæ¨ç†æµ‹è¯•&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2.gguf -sys "You are a helpful assistant" # system promptå¿…é¡»å›ºå®š
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;img src="https://ollama.com/public/cloud.png" height="28" style="vertical-align: middle;" /&gt; &lt;a href="https://ollama.ai"&gt;ollama&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;ollamaæ˜¯æœ¬åœ°è¿è¡Œå¤§æ¨¡å‹çš„å·¥å…·ï¼Œæ”¯æŒå¤šç§å¼€æºLLMï¼Œç®€å•æ˜“ç”¨ã€‚&lt;/p&gt; 
&lt;p&gt;1ã€é€šè¿‡ollamaåŠ è½½è‡ªå®šä¹‰çš„ggufæ¨¡å‹&lt;/p&gt; 
&lt;p&gt;åœ¨&lt;code&gt;MiniMind2&lt;/code&gt;ä¸‹æ–°å»º&lt;code&gt;minimind.modelfile&lt;/code&gt;ï¼Œå†™å…¥ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;FROM ./Q4-MiniMind2.gguf

SYSTEM """You are a helpful assistant"""

TEMPLATE """&amp;lt;|im_start|&amp;gt;system
{{ .System }}&amp;lt;|im_end|&amp;gt;
&amp;lt;|im_start|&amp;gt;user
{{ .Prompt }}&amp;lt;|im_end|&amp;gt;
&amp;lt;|im_start|&amp;gt;assistant
{{ .Response }}&amp;lt;|im_end|&amp;gt;
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;2ã€åŠ è½½å¹¶å‘½åæ­¤æ¨¡å‹ä¸º&lt;code&gt;minimind-local&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama create -f minimind.modelfile minimind-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;3ã€å¯åŠ¨æ¨ç†&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama run minimind-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“¤ æ¨é€ä½ çš„æ¨¡å‹åˆ° Ollama Hub&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 1. ä¸ºæœ¬åœ°æ¨¡å‹é‡å‘½åä¸ºä½ çš„ollama-account/minimindçš„tag
ollama cp minimind-local:latest your_username/minimind:latest

# 2. æ¨é€æ¨¡å‹
ollama push your_username/minimind:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;p&gt;â­ï¸ ä¹Ÿå¯ç›´æ¥ä½¿ç”¨æˆ‘æä¾›çš„ollamaæ¨¡å‹ä¸€é”®å¯åŠ¨ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama run jingyaogong/minimind2 # å…¶ä»–å¯é€‰ minimind2-r1 / minimind2-small / minimind2-small-r1
&amp;gt;&amp;gt;&amp;gt; ä½ å«ä»€ä¹ˆåå­—
æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹...
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä»¥ä¸Šä¸‰æ–¹æ¡†æ¶çš„æ›´å¤šç”¨æ³•è¯·å‚è€ƒå¯¹åº”å®˜æ–¹æ–‡æ¡£ğŸ˜Š&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;ğŸ“Œ Acknowledge&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] å¦‚æœè§‰å¾—&lt;code&gt;MiniMindç³»åˆ—&lt;/code&gt;å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¯ä»¥åœ¨ GitHub ä¸ŠåŠ ä¸€ä¸ªâ­&lt;br /&gt; ç¯‡å¹…è¶…é•¿æ°´å¹³æœ‰é™éš¾å…çº°æ¼ï¼Œæ¬¢è¿åœ¨Issuesäº¤æµæŒ‡æ­£æˆ–æäº¤PRæ”¹è¿›é¡¹ç›®&lt;br /&gt; æ‚¨çš„å°å°æ”¯æŒå°±æ˜¯æŒç»­æ”¹è¿›æ­¤é¡¹ç›®çš„åŠ¨åŠ›ï¼&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ¤&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;è´¡çŒ®è€…&lt;/a&gt;&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=jingyaogong/minimind" /&gt; &lt;/a&gt; 
&lt;h2&gt;ğŸ˜Šé¸£è°¢&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/ipfgao"&gt;&lt;b&gt;@ipfgao&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/26"&gt;ğŸ”—è®­ç»ƒæ­¥éª¤è®°å½•&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/WangRongsheng"&gt;&lt;b&gt;@WangRongsheng&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/39"&gt;ğŸ”—å¤§å‹æ•°æ®é›†é¢„å¤„ç†&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/pengqianhan"&gt;&lt;b&gt;@pengqianhan&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/73"&gt;ğŸ”—ä¸€ä¸ªç®€æ˜æ•™ç¨‹&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/RyanSunn"&gt;&lt;b&gt;@RyanSunn&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/75"&gt;ğŸ”—æ¨ç†è¿‡ç¨‹å­¦ä¹ è®°å½•&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Nijikadesu"&gt;&lt;b&gt;@Nijikadesu&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/213"&gt;ğŸ”—ä»¥äº¤äº’ç¬”è®°æœ¬æ–¹å¼åˆ†è§£é¡¹ç›®ä»£ç &lt;/a&gt;&lt;/p&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;å‚è€ƒé“¾æ¥ &amp;amp; æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„è®ºæ–‡æˆ–é¡¹ç›®&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ’åä¸åˆ†ä»»ä½•å…ˆåé¡ºåº&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;https://github.com/meta-llama/llama3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/karpathy/llama2.c"&gt;https://github.com/karpathy/llama2.c&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;https://github.com/DLLXW/baby-llama2-chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.04434"&gt;(DeepSeek-V2)https://arxiv.org/abs/2405.04434&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;https://github.com/charent/ChatLM-mini-Chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/wdndev/tiny-llm-zh"&gt;https://github.com/wdndev/tiny-llm-zh&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2401.04088"&gt;(Mistral-MoE)https://arxiv.org/pdf/2401.04088&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Tongjilibo/build_MiniLLM_from_scratch"&gt;https://github.com/Tongjilibo/build_MiniLLM_from_scratch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;https://github.com/jzhang38/TinyLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/AI-Study-Han/Zero-Chatgpt"&gt;https://github.com/AI-Study-Han/Zero-Chatgpt&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/xusenlinzy/api-for-open-llm"&gt;https://github.com/xusenlinzy/api-for-open-llm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ«¶æ”¯æŒè€…&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/stars/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/network/members"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/forks/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h2&gt;ğŸ‰ Awesome Work using MiniMind&lt;/h2&gt; 
&lt;p&gt;æœ¬æ¨¡å‹æŠ›ç –å¼•ç‰åœ°ä¿ƒæˆäº†ä¸€äº›å¯å–œæˆæœçš„è½åœ°ï¼Œæ„Ÿè°¢ç ”ç©¶è€…ä»¬çš„è®¤å¯ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ECG-Expert-QA: A Benchmark for Evaluating Medical Large Language Models in Heart Disease Diagnosis [&lt;a href="https://arxiv.org/pdf/2502.17475"&gt;arxiv&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Binary-Integer-Programming Based Algorithm for Expert Load Balancing in Mixture-of-Experts Models [&lt;a href="https://arxiv.org/pdf/2502.15451"&gt;arxiv&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text [&lt;a href="https://arxiv.org/pdf/2505.24826"&gt;arxiv&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;On the Generalization Ability of Next-Token-Prediction Pretraining [&lt;a href="https://openreview.net/forum?id=hLGJ1qZPdu"&gt;ICML 2025&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ã€Šä»é›¶å¼€å§‹å†™å¤§æ¨¡å‹ï¼šä»ç¥ç»ç½‘ç»œåˆ°Transformerã€‹ç‹åŒã€ç‰Ÿæ™¨ã€ç‹æ˜Šæ€¡ ç¼–è‘— - æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;FedBRB: A Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning [&lt;a href="https://ieeexplore.ieee.org/abstract/document/11168259"&gt;TMC 2025&lt;/a&gt;]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;è¿›è¡Œä¸­...&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ“ Citation&lt;/h1&gt; 
&lt;p&gt;If you find MiniMind helpful in your research or work, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{minimind,
  title={MiniMind: Train a Tiny LLM from scratch},
  author={Jingyao Gong},
  year={2024},
  howpublished={https://github.com/jingyaogong/minimind}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>supermemoryai/supermemory</title>
      <link>https://github.com/supermemoryai/supermemory</link>
      <description>&lt;p&gt;Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.&lt;/p&gt;&lt;hr&gt;&lt;p align="center" style="padding-bottom:20px;padding-top:20px"&gt; 
 &lt;picture&gt; 
  &lt;source srcset="apps/web/public/logo-fullmark.svg" media="(prefers-color-scheme: dark)" /&gt; 
  &lt;source srcset="apps/web/public/logo-light-fullmark.svg" media="(prefers-color-scheme: light)" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/logo-fullmark.svg?sanitize=true" alt="supermemory Logo" width="400" /&gt; 
 &lt;/picture&gt; &lt;br /&gt;&lt;br /&gt; &lt;em&gt;Your AI second brain for saving and organizing everything that matters.&lt;/em&gt; &lt;br /&gt;&lt;br /&gt; &lt;a href="https://app.supermemory.ai" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Web-App-000000?style=for-the-badge" alt="Web App" /&gt; &lt;/a&gt; &amp;nbsp; &lt;a href="https://chromewebstore.google.com/detail/supermemory/afpgkkipfdpeaflnpoaffkcankadgjfc" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Chrome-Extension-4285F4?style=for-the-badge&amp;amp;logo=googlechrome&amp;amp;logoColor=white" alt="Chrome Extension" /&gt; &lt;/a&gt; &amp;nbsp; &lt;a href="https://www.raycast.com/supermemory/supermemory" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Raycast-Extension-FF6363?style=for-the-badge&amp;amp;logo=raycast&amp;amp;logoColor=white" alt="Raycast Extension" /&gt; &lt;/a&gt; &amp;nbsp; &lt;a href="https://discord.gg/HXvsGK2f" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p style="font-size: 0.9em; color: #666;"&gt; &lt;strong&gt;Building with Supermemory?&lt;/strong&gt; Check out the &lt;a href="https://console.supermemory.ai?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=consumer_app"&gt;Developer Console&lt;/a&gt; and &lt;a href="https://docs.supermemory.ai?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=consumer_app"&gt;Documentation&lt;/a&gt; for API access. &lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/landing-page.jpeg" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Core Functionality&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#add-memory"&gt;Add Memories from Any Content&lt;/a&gt;&lt;/strong&gt;: Easily add memories from URLs, PDFs, and plain textâ€”just paste, upload, or link.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#chat-memories"&gt;Chat with Your Memories&lt;/a&gt;&lt;/strong&gt;: Converse with your stored content using natural language chat.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#mcp-integration"&gt;Supermemory MCP Integration&lt;/a&gt;&lt;/strong&gt;: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#browser-extension"&gt;Browser Extension&lt;/a&gt;&lt;/strong&gt;: Save memories directly from your browser with integrations for ChatGPT, Claude, and Twitter/X.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#raycast-extension"&gt;Raycast Extension&lt;/a&gt;&lt;/strong&gt;: Add and search memories directly from Raycast with keyboard shortcuts.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How do I use this?&lt;/h2&gt; 
&lt;p&gt;Go to &lt;a href="https://app.supermemory.ai"&gt;app.supermemory.ai&lt;/a&gt; and sign in with your account&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a id="add-memory"&gt;&lt;/a&gt;Start Adding Memory with your choice of format (Note, Link, File)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/add-memory.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;You can also Connect to your favourite services (Notion, Google Drive, OneDrive)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/add-connections.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;a id="chat-memories"&gt;&lt;/a&gt;Once Memories are added, you can chat with Supermemory by clicking on "Open Chat" and retrieve info from your saved memories&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/chat.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;&lt;a id="mcp-integration"&gt;&lt;/a&gt;Add MCP to your AI Tools (by clicking on "Connect to your AI" and select the AI tool you are trying to integrate)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/mcp.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a id="browser-extension"&gt;&lt;/a&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;: Install the &lt;a href="https://chromewebstore.google.com/detail/supermemory/afpgkkipfdpeaflnpoaffkcankadgjfc"&gt;Chrome/Edge extension&lt;/a&gt; to save memories directly from any webpage, integrate with ChatGPT and Claude conversations, and import from Twitter/X. Right-click on any content or use the extension popup to save memories instantly.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a id="raycast-extension"&gt;&lt;/a&gt;&lt;strong&gt;Raycast Extension&lt;/strong&gt;: Install the &lt;a href="https://www.raycast.com/supermemory/supermemory"&gt;Raycast extension&lt;/a&gt; to add and search memories directly from Raycast. Use the "Add Memory" command to quickly save content, or "Search Memories" to find and retrieve your saved information with keyboard shortcuts.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Have questions or feedback? We're here to help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Email: &lt;a href="mailto:support@supermemory.com"&gt;support@supermemory.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Discord: &lt;a href="https://discord.gg/HXvsGK2f"&gt;Join our Discord server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Documentation: &lt;a href="https://docs.supermemory.ai"&gt;docs.supermemory.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.&lt;/p&gt; 
&lt;p&gt;For detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our &lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Ways to Contribute&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;Bug fixes&lt;/strong&gt; - Help us squash those pesky issues&lt;/li&gt; 
 &lt;li&gt;âœ¨ &lt;strong&gt;New features&lt;/strong&gt; - Add functionality that users will love&lt;/li&gt; 
 &lt;li&gt;ğŸ¨ &lt;strong&gt;UI/UX improvements&lt;/strong&gt; - Make the interface more intuitive&lt;/li&gt; 
 &lt;li&gt;âš¡ &lt;strong&gt;Performance optimizations&lt;/strong&gt; - Help us make supermemory faster&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our &lt;a href="https://github.com/supermemoryai/supermemory/issues"&gt;Issues&lt;/a&gt; page for &lt;code&gt;good first issue&lt;/code&gt; and &lt;code&gt;help wanted&lt;/code&gt; labels to get started!&lt;/p&gt; 
&lt;h2&gt;Updates &amp;amp; Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with the latest improvements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.supermemory.ai/changelog/overview"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/supermemoryai"&gt;X&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>karpathy/nanoGPT</title>
      <link>https://github.com/karpathy/nanoGPT</link>
      <description>&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoGPT&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/nanogpt.jpg" alt="nanoGPT" /&gt;&lt;/p&gt; 
&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of &lt;a href="https://github.com/karpathy/minGPT"&gt;minGPT&lt;/a&gt; that prioritizes teeth over education. Still under active development, but currently the file &lt;code&gt;train.py&lt;/code&gt; reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: &lt;code&gt;train.py&lt;/code&gt; is a ~300-line boilerplate training loop and &lt;code&gt;model.py&lt;/code&gt; a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/gpt2_124M_loss.png" alt="repro124m" /&gt;&lt;/p&gt; 
&lt;p&gt;Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).&lt;/p&gt; 
&lt;h2&gt;install&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install torch numpy transformers datasets tiktoken wandb tqdm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org"&gt;pytorch&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://numpy.org/install/"&gt;numpy&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; for huggingface transformers &amp;lt;3 (to load GPT-2 checkpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;datasets&lt;/code&gt; for huggingface datasets &amp;lt;3 (if you want to download + preprocess OpenWebText)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tiktoken&lt;/code&gt; for OpenAI's fast BPE code &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wandb&lt;/code&gt; for optional logging &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt; for progress bars &amp;lt;3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;quick start&lt;/h2&gt; 
&lt;p&gt;If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/shakespeare_char/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I have a GPU&lt;/strong&gt;. Great, we can quickly train a baby GPT with the settings provided in the &lt;a href="https://raw.githubusercontent.com/karpathy/nanoGPT/master/config/train_shakespeare_char.py"&gt;config/train_shakespeare_char.py&lt;/a&gt; config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you peek inside it, you'll see that we're training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the &lt;code&gt;--out_dir&lt;/code&gt; directory &lt;code&gt;out-shakespeare-char&lt;/code&gt;. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This generates a few samples, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;lol &lt;code&gt;Â¯\_(ãƒ„)_/Â¯&lt;/code&gt;. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I only have a macbook&lt;/strong&gt; (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly (&lt;a href="https://pytorch.org/get-started/locally/"&gt;select it here&lt;/a&gt; when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here, since we are running on CPU instead of GPU we must set both &lt;code&gt;--device=cpu&lt;/code&gt; and also turn off PyTorch 2.0 compile with &lt;code&gt;--compile=False&lt;/code&gt;. Then when we evaluate we get a bit more noisy but faster estimate (&lt;code&gt;--eval_iters=20&lt;/code&gt;, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with &lt;code&gt;--lr_decay_iters&lt;/code&gt;). Because our network is so small we also ease down on regularization (&lt;code&gt;--dropout=0.0&lt;/code&gt;). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char --device=cpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generates samples like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;GLEORKEN VINGHARD III:
Whell's the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you're willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (&lt;code&gt;--block_size&lt;/code&gt;), the length of training, etc.&lt;/p&gt; 
&lt;p&gt;Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add &lt;code&gt;--device=mps&lt;/code&gt; (short for "Metal Performance Shaders"); PyTorch then uses the on-chip GPU that can &lt;em&gt;significantly&lt;/em&gt; accelerate training (2-3X) and allow you to use larger networks. See &lt;a href="https://github.com/karpathy/nanoGPT/issues/28"&gt;Issue 28&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h2&gt;reproducing GPT-2&lt;/h2&gt; 
&lt;p&gt;A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the &lt;a href="https://openwebtext2.readthedocs.io/en/latest/"&gt;OpenWebText&lt;/a&gt;, an open reproduction of OpenAI's (private) WebText:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/openwebtext/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This downloads and tokenizes the &lt;a href="https://huggingface.co/datasets/openwebtext"&gt;OpenWebText&lt;/a&gt; dataset. It will create a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we're ready to kick off training. To reproduce GPT-2 (124M) you'll want at least an 8X A100 40GB node and run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.&lt;/p&gt; 
&lt;p&gt;If you're in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don't have Infiniband then also prepend &lt;code&gt;NCCL_IB_DISABLE=1&lt;/code&gt; to the above launches. Your multinode training will work, but most likely &lt;em&gt;crawl&lt;/em&gt;. By default checkpoints are periodically written to the &lt;code&gt;--out_dir&lt;/code&gt;. We can sample from the model by simply &lt;code&gt;python sample.py&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, to train on a single GPU simply run the &lt;code&gt;python train.py&lt;/code&gt; script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You'll most likely want to tune a number of those variables depending on your needs.&lt;/p&gt; 
&lt;h2&gt;baselines&lt;/h2&gt; 
&lt;p&gt;OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and observe the following losses on train and val:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;train loss&lt;/th&gt; 
   &lt;th&gt;val loss&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2&lt;/td&gt; 
   &lt;td&gt;124M&lt;/td&gt; 
   &lt;td&gt;3.11&lt;/td&gt; 
   &lt;td&gt;3.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-medium&lt;/td&gt; 
   &lt;td&gt;350M&lt;/td&gt; 
   &lt;td&gt;2.85&lt;/td&gt; 
   &lt;td&gt;2.84&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-large&lt;/td&gt; 
   &lt;td&gt;774M&lt;/td&gt; 
   &lt;td&gt;2.66&lt;/td&gt; 
   &lt;td&gt;2.67&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-xl&lt;/td&gt; 
   &lt;td&gt;1558M&lt;/td&gt; 
   &lt;td&gt;2.56&lt;/td&gt; 
   &lt;td&gt;2.54&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.&lt;/p&gt; 
&lt;h2&gt;finetuning&lt;/h2&gt; 
&lt;p&gt;Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to &lt;code&gt;data/shakespeare&lt;/code&gt; and run &lt;code&gt;prepare.py&lt;/code&gt; to download the tiny shakespeare dataset and render it into a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt;, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/finetune_shakespeare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will load the config parameter overrides in &lt;code&gt;config/finetune_shakespeare.py&lt;/code&gt; (I didn't tune them much though). Basically, we initialize from a GPT2 checkpoint with &lt;code&gt;init_from&lt;/code&gt; and train as normal, except shorter and with a small learning rate. If you're running out of memory try decreasing the model size (they are &lt;code&gt;{'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}&lt;/code&gt;) or possibly decreasing the &lt;code&gt;block_size&lt;/code&gt; (context length). The best checkpoint (lowest validation loss) will be in the &lt;code&gt;out_dir&lt;/code&gt; directory, e.g. in &lt;code&gt;out-shakespeare&lt;/code&gt; by default, per the config file. You can then run the code in &lt;code&gt;sample.py --out_dir=out-shakespeare&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know'st not what thou sell'st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Whoa there, GPT, entering some dark place over there. I didn't really tune the hyperparameters in the config too much, feel free to try!&lt;/p&gt; 
&lt;h2&gt;sampling / inference&lt;/h2&gt; 
&lt;p&gt;Use the script &lt;code&gt;sample.py&lt;/code&gt; to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available &lt;code&gt;gpt2-xl&lt;/code&gt; model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py \
    --init_from=gpt2-xl \
    --start="What is the answer to life, the universe, and everything?" \
    --num_samples=5 --max_new_tokens=100
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you'd like to sample from a model you trained, use the &lt;code&gt;--out_dir&lt;/code&gt; to point the code appropriately. You can also prompt the model with some text from a file, e.g. &lt;code&gt;python sample.py --start=FILE:prompt.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;efficiency notes&lt;/h2&gt; 
&lt;p&gt;For simple model benchmarking and profiling, &lt;code&gt;bench.py&lt;/code&gt; might be useful. It's identical to what happens in the meat of the training loop of &lt;code&gt;train.py&lt;/code&gt;, but omits much of the other complexities.&lt;/p&gt; 
&lt;p&gt;Note that the code by default uses &lt;a href="https://pytorch.org/get-started/pytorch-2.0/"&gt;PyTorch 2.0&lt;/a&gt;. At the time of writing (Dec 29, 2022) this makes &lt;code&gt;torch.compile()&lt;/code&gt; available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!&lt;/p&gt; 
&lt;h2&gt;todos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Investigate and add FSDP instead of DDP&lt;/li&gt; 
 &lt;li&gt;Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)&lt;/li&gt; 
 &lt;li&gt;Finetune the finetuning script, I think the hyperparams are not great&lt;/li&gt; 
 &lt;li&gt;Schedule for linear batch size increase during training&lt;/li&gt; 
 &lt;li&gt;Incorporate other embeddings (rotary, alibi)&lt;/li&gt; 
 &lt;li&gt;Separate out the optim buffers from model params in checkpoints I think&lt;/li&gt; 
 &lt;li&gt;Additional logging around network health (e.g. gradient clip events, magnitudes)&lt;/li&gt; 
 &lt;li&gt;Few more investigations around better init etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;troubleshooting&lt;/h2&gt; 
&lt;p&gt;Note that by default this repo uses PyTorch 2.0 (i.e. &lt;code&gt;torch.compile&lt;/code&gt;). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding &lt;code&gt;--compile=False&lt;/code&gt; flag. This will slow down the code but at least it will run.&lt;/p&gt; 
&lt;p&gt;For some context on this repository, GPT, and language modeling it might be helpful to watch my &lt;a href="https://karpathy.ai/zero-to-hero.html"&gt;Zero To Hero series&lt;/a&gt;. Specifically, the &lt;a href="https://www.youtube.com/watch?v=kCc8FmEb1nY"&gt;GPT video&lt;/a&gt; is popular if you have some prior language modeling context.&lt;/p&gt; 
&lt;p&gt;For more questions/discussions feel free to stop by &lt;strong&gt;#nanoGPT&lt;/strong&gt; on Discord:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/3zy8kqD9Cp"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;amp;style=flat" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;acknowledgements&lt;/h2&gt; 
&lt;p&gt;All nanoGPT experiments are powered by GPUs on &lt;a href="https://lambdalabs.com"&gt;Lambda labs&lt;/a&gt;, my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>servo/servo</title>
      <link>https://github.com/servo/servo</link>
      <description>&lt;p&gt;Servo aims to empower developers with a lightweight, high-performance alternative for embedding web technologies in applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Servo Parallel Browser Engine Project&lt;/h1&gt; 
&lt;p&gt;Servo is a prototype web browser engine written in the &lt;a href="https://github.com/rust-lang/rust"&gt;Rust&lt;/a&gt; language. It is currently developed on 64-bit macOS, 64-bit Linux, 64-bit Windows, 64-bit OpenHarmony, and Android.&lt;/p&gt; 
&lt;p&gt;Servo welcomes contribution from everyone. Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://book.servo.org"&gt;Servo Book&lt;/a&gt; for documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://servo.org/"&gt;servo.org&lt;/a&gt; for news and guides&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Coordination of Servo development happens:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Here in the Github Issues&lt;/li&gt; 
 &lt;li&gt;On the &lt;a href="https://servo.zulipchat.com/"&gt;Servo Zulip&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;In video calls advertised in the &lt;a href="https://github.com/servo/project/issues"&gt;Servo Project&lt;/a&gt; repo.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;For more detailed build instructions, see the Servo book under &lt;a href="https://book.servo.org/hacking/setting-up-your-environment.html"&gt;Setting up your environment&lt;/a&gt;, &lt;a href="https://book.servo.org/hacking/building-servo.html"&gt;Building Servo&lt;/a&gt;, &lt;a href="https://book.servo.org/hacking/building-for-android.html"&gt;Building for Android&lt;/a&gt; and &lt;a href="https://book.servo.org/hacking/building-for-openharmony.html"&gt;Building for OpenHarmony&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download and install &lt;a href="https://developer.apple.com/xcode/"&gt;Xcode&lt;/a&gt; and &lt;a href="https://brew.sh/"&gt;&lt;code&gt;brew&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;uv&lt;/code&gt;: &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;rustup&lt;/code&gt;: &lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt; is available&lt;/li&gt; 
 &lt;li&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;code&gt;curl&lt;/code&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Arch: &lt;code&gt;sudo pacman -S --needed curl&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Debian, Ubuntu: &lt;code&gt;sudo apt install curl&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Fedora: &lt;code&gt;sudo dnf install curl&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Gentoo: &lt;code&gt;sudo emerge net-misc/curl&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;uv&lt;/code&gt;: &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;rustup&lt;/code&gt;: &lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt; is available&lt;/li&gt; 
 &lt;li&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download &lt;a href="https://docs.astral.sh/uv/getting-started/installation/#standalone-installer"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://chocolatey.org/install#individual"&gt;&lt;code&gt;choco&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://win.rustup.rs/"&gt;&lt;code&gt;rustup&lt;/code&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Be sure to select &lt;em&gt;Quick install via the Visual Studio Community installer&lt;/em&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;In the Visual Studio Installer, ensure the following components are installed: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Windows 10/11 SDK (anything &amp;gt;= 10.0.19041.0)&lt;/strong&gt; (&lt;code&gt;Microsoft.VisualStudio.Component.Windows{10, 11}SDK.{&amp;gt;=19041}&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MSVC v143 - VS 2022 C++ x64/x86 build tools (Latest)&lt;/strong&gt; (&lt;code&gt;Microsoft.VisualStudio.Component.VC.Tools.x86.x64&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;C++ ATL for latest v143 build tools (x86 &amp;amp; x64)&lt;/strong&gt; (&lt;code&gt;Microsoft.VisualStudio.Component.VC.ATL&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt; is available&lt;/li&gt; 
 &lt;li&gt;Install the other dependencies: &lt;code&gt;.\mach bootstrap&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build servoshell: &lt;code&gt;.\mach build&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ensure that the following environment variables are set: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ANDROID_NDK_ROOT&lt;/code&gt;: &lt;code&gt;$ANDROID_SDK_ROOT/ndk/28.2.13676358/&lt;/code&gt; &lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt; can be any directory (such as &lt;code&gt;~/android-sdk&lt;/code&gt;). All of the Android build dependencies will be installed there.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Install the latest version of the &lt;a href="https://developer.android.com/studio#command-tools"&gt;Android command-line tools&lt;/a&gt; to &lt;code&gt;$ANDROID_SDK_ROOT/cmdline-tools/latest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Run the following command to install the necessary components: &lt;pre&gt;&lt;code class="language-shell"&gt;sudo $ANDROID_SDK_ROOT/cmdline-tools/latest/bin/sdkmanager --install \
 "build-tools;34.0.0" \
 "emulator" \
 "ndk;28.2.13676358" \
 "platform-tools" \
 "platforms;android-33" \
 "system-images;android-33;google_apis;x86_64"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Follow the instructions above for the platform you are building on&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenHarmony&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the instructions above for the platform you are building on to prepare the environment.&lt;/li&gt; 
 &lt;li&gt;Depending on the target distribution (e.g. &lt;code&gt;HarmonyOS NEXT&lt;/code&gt; vs pure &lt;code&gt;OpenHarmony&lt;/code&gt;) the build configuration will differ slightly.&lt;/li&gt; 
 &lt;li&gt;Ensure that the following environment variables are set 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;DEVECO_SDK_HOME&lt;/code&gt; (Required when targeting &lt;code&gt;HarmonyOS NEXT&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OHOS_BASE_SDK_HOME&lt;/code&gt; (Required when targeting &lt;code&gt;OpenHarmony&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OHOS_SDK_NATIVE&lt;/code&gt; (e.g. &lt;code&gt;${DEVECO_SDK_HOME}/default/openharmony/native&lt;/code&gt; or &lt;code&gt;${OHOS_BASE_SDK_HOME}/${API_VERSION}/native&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;SERVO_OHOS_SIGNING_CONFIG&lt;/code&gt;: Path to json file containing a valid signing configuration for the demo app.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Review the detailed instructions at &lt;a href="https://book.servo.org/hacking/building-for-openharmony.html"&gt;Building for OpenHarmony&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;The target distribution can be modified by passing &lt;code&gt;--flavor=&amp;lt;default|harmonyos&amp;gt;&lt;/code&gt; to &lt;code&gt;mach &amp;lt;build|package|install&amp;gt;&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>LadybirdBrowser/ladybird</title>
      <link>https://github.com/LadybirdBrowser/ladybird</link>
      <description>&lt;p&gt;Truly independent web browser&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ladybird&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://ladybird.org"&gt;Ladybird&lt;/a&gt; is a truly independent web browser, using a novel engine based on web standards.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Ladybird is in a pre-alpha state, and only suitable for use by developers&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;We aim to build a complete, usable browser for the modern web.&lt;/p&gt; 
&lt;p&gt;Ladybird uses a multi-process architecture with a main UI process, several WebContent renderer processes, an ImageDecoder process, and a RequestServer process.&lt;/p&gt; 
&lt;p&gt;Image decoding and network connections are done out of process to be more robust against malicious content. Each tab has its own renderer process, which is sandboxed from the rest of the system.&lt;/p&gt; 
&lt;p&gt;At the moment, many core library support components are inherited from SerenityOS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LibWeb: Web rendering engine&lt;/li&gt; 
 &lt;li&gt;LibJS: JavaScript engine&lt;/li&gt; 
 &lt;li&gt;LibWasm: WebAssembly implementation&lt;/li&gt; 
 &lt;li&gt;LibCrypto/LibTLS: Cryptography primitives and Transport Layer Security&lt;/li&gt; 
 &lt;li&gt;LibHTTP: HTTP/1.1 client&lt;/li&gt; 
 &lt;li&gt;LibGfx: 2D Graphics Library, Image Decoding and Rendering&lt;/li&gt; 
 &lt;li&gt;LibUnicode: Unicode and locale support&lt;/li&gt; 
 &lt;li&gt;LibMedia: Audio and video playback&lt;/li&gt; 
 &lt;li&gt;LibCore: Event loop, OS abstraction layer&lt;/li&gt; 
 &lt;li&gt;LibIPC: Inter-process communication&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How do I build and run this?&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/BuildInstructionsLadybird.md"&gt;build instructions&lt;/a&gt; for information on how to build Ladybird.&lt;/p&gt; 
&lt;p&gt;Ladybird runs on Linux, macOS, Windows (with WSL2), and many other *Nixes.&lt;/p&gt; 
&lt;h2&gt;How do I read the documentation?&lt;/h2&gt; 
&lt;p&gt;Code-related documentation can be found in the &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/"&gt;documentation&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h2&gt;Get in touch and participate!&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://discord.gg/nvfjVJ4Svh"&gt;our Discord server&lt;/a&gt; to participate in development discussion.&lt;/p&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/GettingStartedContributing.md"&gt;Getting started contributing&lt;/a&gt; if you plan to contribute to Ladybird for the first time.&lt;/p&gt; 
&lt;p&gt;Before opening an issue, please see the &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md#issue-policy"&gt;issue policy&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/ISSUES.md"&gt;detailed issue-reporting guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The full contribution guidelines can be found in &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Ladybird is licensed under a 2-clause BSD license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>yeongpin/cursor-free-vip</title>
      <link>https://github.com/yeongpin/cursor-free-vip</link>
      <description>&lt;p&gt;[Support 0.49.x]ï¼ˆReset Cursor AI MachineID &amp; Bypass Higher Token Limitï¼‰ Cursor Ai ï¼Œè‡ªåŠ¨é‡ç½®æœºå™¨ID ï¼Œ å…è´¹å‡çº§ä½¿ç”¨ProåŠŸèƒ½: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="http://go.warp.dev/cursor-free-vip"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="http://go.warp.dev/cursor-free-vip"&gt;Warp, built for coding with multiple agents.&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="http://go.warp.dev/cursor-free-vip"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;â¤ Cursor Free VIP&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/logo.png" alt="Cursor Pro Logo" width="200" style="border-radius: 6px;" /&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg?sanitize=true" alt="License: CC BY-NC-ND 4.0" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/stargazers"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/yeongpin" target="_blank"&gt;&lt;img alt="Buy Me a Coffee" src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/yeongpin/cursor-free-vip"&gt;&lt;img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13425" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13425" alt="yeongpin%2Fcursor-free-vip | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;h4&gt;Support Latest 0.49.x Version | æ”¯æŒæœ€æ–° 0.49.x ç‰ˆæœ¬&lt;/h4&gt; 
 &lt;p&gt;This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project. This tool will not generate any fake email accounts and OAuth access.&lt;/p&gt; 
 &lt;p&gt;Supports Windows, macOS and Linux.&lt;/p&gt; 
 &lt;p&gt;For optimal performance, run with privileges and always stay up to date.&lt;/p&gt; 
 &lt;p&gt;é€™æ˜¯ä¸€æ¬¾ç”¨æ–¼å­¸ç¿’å’Œç ”ç©¶çš„å·¥å…·ï¼Œç›®å‰ repo æ²’æœ‰é•åä»»ä½•æ³•å¾‹ã€‚è«‹æ”¯æŒåŸä½œè€…ã€‚ é€™æ¬¾å·¥å…·ä¸æœƒç”Ÿæˆä»»ä½•å‡çš„é›»å­éƒµä»¶å¸³æˆ¶å’Œ OAuth è¨ªå•ã€‚&lt;/p&gt; 
 &lt;p&gt;æ”¯æŒ Windowsã€macOS å’Œ Linuxã€‚&lt;/p&gt; 
 &lt;p&gt;å°æ–¼æœ€ä½³æ€§èƒ½ï¼Œè«‹ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œä¸¦å§‹çµ‚ä¿æŒæœ€æ–°ã€‚&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/product_2025-04-16_10-40-21.png" alt="new" width="800" style="border-radius: 6px;" /&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ”„ Change Log | æ›´æ–°æ—¥å¿—&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/CHANGELOG.md"&gt;Watch Change Log | æŸ¥çœ‹æ›´æ–°æ—¥å¿—&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features | åŠŸèƒ½ç‰¹é»&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Support Windows macOS and Linux systems&lt;br /&gt;æ”¯æŒ Windowsã€macOS å’Œ Linux ç³»çµ±&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Reset Cursor's configuration&lt;br /&gt;é‡ç½® Cursor çš„é…ç½®&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-language support (English, ç®€ä½“ä¸­æ–‡, ç¹é«”ä¸­æ–‡, Vietnamese)&lt;br /&gt;å¤šèªè¨€æ”¯æŒï¼ˆè‹±æ–‡ã€ç®€ä½“ä¸­æ–‡ã€ç¹é«”ä¸­æ–‡ã€è¶Šå—èªï¼‰&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ’» System Support | ç³»çµ±æ”¯æŒ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Supported&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;x64, x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS&lt;/td&gt; 
   &lt;td&gt;Intel, Apple Silicon&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;x64, x86, ARM64&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ‘€ How to use | å¦‚ä½•ä½¿ç”¨&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;â­ Auto Run Script | è…³æœ¬è‡ªå‹•åŒ–é‹è¡Œ&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;&lt;strong&gt;Linux/macOS&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;amp;&amp;amp; chmod +x install.sh &amp;amp;&amp;amp; ./install.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Archlinux&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;Install via &lt;a href="https://aur.archlinux.org/packages/cursor-free-vip-git"&gt;AUR&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;yay -S cursor-free-vip-git
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;If you want to stop the script, please press Ctrl+C&lt;br /&gt;è¦åœæ­¢è…³æœ¬ï¼Œè«‹æŒ‰ Ctrl+C&lt;/p&gt; 
&lt;h2&gt;â— Note | æ³¨æ„äº‹é …&lt;/h2&gt; 
&lt;p&gt;ğŸ“ Config | æ–‡ä»¶é…ç½® &lt;code&gt;Win / Macos / Linux Path | è·¯å¾‘ [Documents/.cursor-free-vip/config.ini]&lt;/code&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;â­ Config | æ–‡ä»¶é…ç½®&lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;[Chrome]
# Default Google Chrome Path | é»˜èªGoogle Chrome éŠè¦½å™¨è·¯å¾‘
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | ç­‰å¾…äººæ©Ÿé©—è­‰æ™‚é–“
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | ç­‰å¾…äººæ©Ÿé©—è­‰éš¨æ©Ÿæ™‚é–“ï¼ˆå¿…é ˆæ˜¯ 1-3 æˆ–è€… 1,3 é€™æ¨£çš„çµ„åˆï¼‰
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | å­˜å„²è·¯å¾‘
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteè·¯å¾‘
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | æ©Ÿå™¨IDè·¯å¾‘
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | æœ€å°éš¨æ©Ÿæ™‚é–“
min_random_time = 0.1
# Max Random Time | æœ€å¤§éš¨æ©Ÿæ™‚é–“
max_random_time = 0.8
# Page Load Wait | é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
page_load_wait = 0.1-0.8
# Input Wait | è¼¸å…¥ç­‰å¾…æ™‚é–“
input_wait = 0.3-0.8
# Submit Wait | æäº¤ç­‰å¾…æ™‚é–“
submit_wait = 0.5-1.5
# Verification Code Input | é©—è­‰ç¢¼è¼¸å…¥ç­‰å¾…æ™‚é–“
verification_code_input = 0.1-0.3
# Verification Success Wait | é©—è­‰æˆåŠŸç­‰å¾…æ™‚é–“
verification_success_wait = 2-3
# Verification Retry Wait | é©—è­‰é‡è©¦ç­‰å¾…æ™‚é–“
verification_retry_wait = 2-3
# Email Check Initial Wait | éƒµä»¶æª¢æŸ¥åˆå§‹ç­‰å¾…æ™‚é–“
email_check_initial_wait = 4-6
# Email Refresh Wait | éƒµä»¶åˆ·æ–°ç­‰å¾…æ™‚é–“
email_refresh_wait = 2-4
# Settings Page Load Wait | è¨­ç½®é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
settings_page_load_wait = 1-2
# Failed Retry Time | å¤±æ•—é‡è©¦æ™‚é–“
failed_retry_time = 0.5-1
# Retry Interval | é‡è©¦é–“éš”
retry_interval = 8-12
# Max Timeout | æœ€å¤§è¶…æ™‚æ™‚é–“
max_timeout = 160

[Utils]
# Check Update | æª¢æŸ¥æ›´æ–°
check_update = True
# Show Account Info | é¡¯ç¤ºè³¬è™Ÿä¿¡æ¯
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | å•“ç”¨ TempMailPlusï¼ˆä»»ä½•è½‰ç™¼åˆ°TempMailPlusçš„éƒµä»¶éƒ½æ”¯æŒç²å–é©—è­‰ç¢¼ï¼Œä¾‹å¦‚cloudflareéƒµä»¶Catch-allï¼‰
enabled = false
# TempMailPlus Email | TempMailPlus é›»å­éƒµä»¶
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pinç¢¼
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Use administrator privileges to run the script &lt;br /&gt;è«‹ä½¿ç”¨ç®¡ç†å“¡èº«ä»½é‹è¡Œè…³æœ¬&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Confirm that Cursor is closed before running the script &lt;br /&gt;è«‹ç¢ºä¿åœ¨é‹è¡Œè…³æœ¬å‰å·²ç¶“é—œé–‰ Cursor&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This tool is only for learning and research purposes &lt;br /&gt;æ­¤å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Please comply with the relevant software usage terms when using this tool &lt;br /&gt;ä½¿ç”¨æœ¬å·¥å…·æ™‚è«‹éµå®ˆç›¸é—œè»Ÿä»¶ä½¿ç”¨æ¢æ¬¾&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš¨ Common Issues | å¸¸è¦‹å•é¡Œ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;å¦‚æœé‡åˆ°æ¬Šé™å•é¡Œï¼Œè«‹ç¢ºä¿ï¼š&lt;/th&gt; 
   &lt;th align="center"&gt;æ­¤è…³æœ¬ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;If you encounter permission issues, please ensure:&lt;/td&gt; 
   &lt;td align="center"&gt;This script is run with administrator privileges&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Error 'User is not authorized'&lt;/td&gt; 
   &lt;td align="center"&gt;This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ¤© Contribution | è²¢ç»&lt;/h2&gt; 
&lt;p&gt;æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼&lt;/p&gt; 
&lt;a href="https://github.com/yeongpin/cursor-free-vip/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;amp;preview=true&amp;amp;max=&amp;amp;columns=" /&gt; &lt;/a&gt; 
&lt;br /&gt;
&lt;br /&gt; 
&lt;h2&gt;ğŸ“© Disclaimer | å…è²¬è²æ˜&lt;/h2&gt; 
&lt;p&gt;æœ¬å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨ï¼Œä½¿ç”¨æœ¬å·¥å…·æ‰€ç”¢ç”Ÿçš„ä»»ä½•å¾Œæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ“”ã€‚ &lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne by the user.&lt;/p&gt; 
&lt;h2&gt;ğŸ’° Buy Me a Coffee | è«‹æˆ‘å–æ¯å’–å•¡&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/provi-code.jpg" alt="buy_me_a_coffee" width="280" /&gt;&lt;br /&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/paypal.png" alt="buy_me_a_coffee" width="280" /&gt;&lt;br /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;â­ Star History | æ˜Ÿæ˜Ÿæ•¸&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#yeongpin/cursor-free-vip&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“ License | æˆæ¬Š&lt;/h2&gt; 
&lt;p&gt;æœ¬é …ç›®æ¡ç”¨ &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;CC BY-NC-ND 4.0&lt;/a&gt; æˆæ¬Šã€‚ Please refer to the &lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/agent-lightning</title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description>&lt;p&gt;The absolute trainer to light up AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-banner.svg?sanitize=true" alt="Agent-lightning-banner" style="width:600px" /&gt; &lt;/p&gt; 
&lt;h1&gt;Agent Lightningâš¡&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="Test" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;&lt;img src="https://img.shields.io/badge/GitHub%20Pages-Documentation-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/agentlightning"&gt;&lt;img src="https://badge.fury.io/py/agentlightning.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/microsoft/agent-lightning"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The absolute trainer to light up AI agents.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;Discord community&lt;/a&gt; to connect with other users and contributors.&lt;/p&gt; 
&lt;h2&gt;âš¡ Core Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Turn your agent into an optimizable beast with &lt;strong&gt;ZERO CODE CHANGE&lt;/strong&gt; (almost)! ğŸ’¤&lt;/li&gt; 
 &lt;li&gt;Build with &lt;strong&gt;ANY&lt;/strong&gt; agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ğŸ¤–&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Selectively&lt;/strong&gt; optimize one or more agents in a multi-agent system. ğŸ¯&lt;/li&gt; 
 &lt;li&gt;Embraces &lt;strong&gt;Algorithms&lt;/strong&gt; like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ğŸ¤—&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more on our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation website&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-diff.svg?sanitize=true" alt="Agent-Lightning Core Quickstart" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;âš¡ Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://microsoft.github.io/agent-lightning/stable/tutorials/installation/"&gt;installation guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;To start using Agent-lightning, check out our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;âš¡ Articles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;10/22/2025 &lt;a href="https://blog.vllm.ai/2025/10/22/agent-lightning.html"&gt;No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL&lt;/a&gt; vLLM blog. See also &lt;a href="https://zhuanlan.zhihu.com/p/1965067274642785725"&gt;Zhihu writeup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;8/11/2025 &lt;a href="https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad"&gt;Training AI Agents to Write and Self-correct SQL with Reinforcement Learning&lt;/a&gt; Medium.&lt;/li&gt; 
 &lt;li&gt;8/5/2025 &lt;a href="https://arxiv.org/abs/2508.03680"&gt;Agent Lightning: Train ANY AI Agents with Reinforcement Learning&lt;/a&gt; arXiv paper.&lt;/li&gt; 
 &lt;li&gt;7/26/2025 &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;We discovered an approach to train any AI agent with RL, with (almost) zero code changes.&lt;/a&gt; Reddit.&lt;/li&gt; 
 &lt;li&gt;6/6/2025 &lt;a href="https://www.microsoft.com/en-us/research/project/agent-lightning/"&gt;Agent Lightning - Microsoft Research&lt;/a&gt; Project page.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ Community Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/af-74413592/DeepWerewolf"&gt;DeepWerewolf&lt;/a&gt; â€” A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;AgentFlow&lt;/a&gt; â€” A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ Architecture&lt;/h2&gt; 
&lt;p&gt;Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight &lt;code&gt;agl.emit_xxx()&lt;/code&gt; helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.&lt;/p&gt; 
&lt;p&gt;On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.&lt;/p&gt; 
&lt;p&gt;No rewrites, no lock-in, just a clear path from first rollout to steady improvement.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-architecture.svg?sanitize=true" alt="Agent-lightning Architecture" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;âš¡ CI Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Workflow&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="tests workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="tests-full workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Examples Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg?sanitize=true" alt="examples summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest Dependency Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg?sanitize=true" alt="latest summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legacy Examples Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml/badge.svg?sanitize=true" alt="examples compatibility workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;âš¡ Citation&lt;/h2&gt; 
&lt;p&gt;If you find Agent Lightning useful in your research or projects, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš¡ Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Start by reading the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/community/contributing.md"&gt;Contributing Guide&lt;/a&gt; for environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;âš¡ Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;h2&gt;âš¡ Responsible AI&lt;/h2&gt; 
&lt;p&gt;This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.&lt;/p&gt; 
&lt;h2&gt;âš¡ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>minio/minio</title>
      <link>https://github.com/minio/minio</link>
      <description>&lt;p&gt;MinIO is a high-performance, S3 compatible object store, open sourced under GNU AGPLv3 license.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MinIO Quickstart Guide&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://slack.min.io"&gt;&lt;img src="https://slack.min.io/slack?type=svg" alt="Slack" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/minio/minio/"&gt;&lt;img src="https://img.shields.io/docker/pulls/minio/minio.svg?maxAge=604800" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://github.com/minio/minio/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-AGPL%20V3-blue" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://min.io"&gt;&lt;img src="https://raw.githubusercontent.com/minio/minio/master/.github/logo.svg?sanitize=true" alt="MinIO" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MinIO is a high-performance, S3-compatible object storage solution released under the GNU AGPL v3.0 license. Designed for speed and scalability, it powers AI/ML, analytics, and data-intensive workloads with industry-leading performance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;S3 API Compatible â€“ Seamless integration with existing S3 tools&lt;/li&gt; 
 &lt;li&gt;Built for AI &amp;amp; Analytics â€“ Optimized for large-scale data pipelines&lt;/li&gt; 
 &lt;li&gt;High Performance â€“ Ideal for demanding storage workloads.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This README provides instructions for building MinIO from source and deploying onto baremetal hardware. Use the &lt;a href="https://github.com/minio/docs"&gt;MinIO Documentation&lt;/a&gt; project to build and host a local copy of the documentation.&lt;/p&gt; 
&lt;h2&gt;MinIO is Open Source Software&lt;/h2&gt; 
&lt;p&gt;We designed MinIO as Open Source software for the Open Source software community. We encourage the community to remix, redesign, and reshare MinIO under the terms of the AGPLv3 license.&lt;/p&gt; 
&lt;p&gt;All usage of MinIO in your application stack requires validation against AGPLv3 obligations, which include but are not limited to the release of modified code to the community from which you have benefited. Any commercial/proprietary usage of the AGPLv3 software, including repackaging or reselling services/features, is done at your own risk.&lt;/p&gt; 
&lt;p&gt;The AGPLv3 provides no obligation by any party to support, maintain, or warranty the original or any modified work. All support is provided on a best-effort basis through Github and our &lt;a href="https://raw.githubusercontent.com/minio/minio/master/https//slack.min.io"&gt;Slack&lt;/a&gt; channel, and any member of the community is welcome to contribute and assist others in their usage of the software.&lt;/p&gt; 
&lt;p&gt;MinIO &lt;a href="https://www.min.io/product/aistor"&gt;AIStor&lt;/a&gt; includes enterprise-grade support and licensing for workloads which require commercial or proprietary usage and production-level SLA/SLO-backed support. For more information, &lt;a href="https://min.io/pricing"&gt;reach out for a quote&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Source-Only Distribution&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; The MinIO community edition is now distributed as source code only. We will no longer provide pre-compiled binary releases for the community version.&lt;/p&gt; 
&lt;h3&gt;Installing Latest MinIO Community Edition&lt;/h3&gt; 
&lt;p&gt;To use MinIO community edition, you have two options:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Install from source&lt;/strong&gt; using &lt;code&gt;go install github.com/minio/minio@latest&lt;/code&gt; (recommended)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Build a Docker image&lt;/strong&gt; from the provided Dockerfile&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See the sections below for detailed instructions on each method.&lt;/p&gt; 
&lt;h3&gt;Legacy Binary Releases&lt;/h3&gt; 
&lt;p&gt;Historical pre-compiled binary releases remain available for reference but are no longer maintained:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GitHub Releases: &lt;a href="https://github.com/minio/minio/releases"&gt;https://github.com/minio/minio/releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Direct downloads: &lt;a href="https://dl.min.io/server/minio/release/"&gt;https://dl.min.io/server/minio/release/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;These legacy binaries will not receive updates.&lt;/strong&gt; We strongly recommend using source builds for access to the latest features, bug fixes, and security updates.&lt;/p&gt; 
&lt;h2&gt;Install from Source&lt;/h2&gt; 
&lt;p&gt;Use the following commands to compile and run a standalone MinIO server from source. If you do not have a working Golang environment, please follow &lt;a href="https://golang.org/doc/install"&gt;How to install Golang&lt;/a&gt;. Minimum version required is &lt;a href="https://golang.org/dl/#stable"&gt;go1.24&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;go install github.com/minio/minio@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can alternatively run &lt;code&gt;go build&lt;/code&gt; and use the &lt;code&gt;GOOS&lt;/code&gt; and &lt;code&gt;GOARCH&lt;/code&gt; environment variables to control the OS and architecture target. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;env GOOS=linux GOARCh=arm64 go build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start MinIO by running &lt;code&gt;minio server PATH&lt;/code&gt; where &lt;code&gt;PATH&lt;/code&gt; is any empty folder on your local filesystem.&lt;/p&gt; 
&lt;p&gt;The MinIO deployment starts using default root credentials &lt;code&gt;minioadmin:minioadmin&lt;/code&gt;. You can test the deployment using the MinIO Console, an embedded web-based object browser built into MinIO Server. Point a web browser running on the host machine to &lt;a href="http://127.0.0.1:9000"&gt;http://127.0.0.1:9000&lt;/a&gt; and log in with the root credentials. You can use the Browser to create buckets, upload objects, and browse the contents of the MinIO server.&lt;/p&gt; 
&lt;p&gt;You can also connect using any S3-compatible tool, such as the MinIO Client &lt;code&gt;mc&lt;/code&gt; commandline tool:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;mc alias set local http://localhost:9000 minioadmin minioadmin
mc admin info local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/minio/minio/master/#test-using-minio-client-mc"&gt;Test using MinIO Client &lt;code&gt;mc&lt;/code&gt;&lt;/a&gt; for more information on using the &lt;code&gt;mc&lt;/code&gt; commandline tool. For application developers, see &lt;a href="https://docs.min.io/enterprise/aistor-object-store/developers/sdk/"&gt;https://docs.min.io/enterprise/aistor-object-store/developers/sdk/&lt;/a&gt; to view MinIO SDKs for supported languages.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Production environments using compiled-from-source MinIO binaries do so at their own risk. The AGPLv3 license provides no warranties nor liabilites for any such usage.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Build Docker Image&lt;/h2&gt; 
&lt;p&gt;You can use the &lt;code&gt;docker build .&lt;/code&gt; command to build a Docker image on your local host machine. You must first &lt;a href="https://raw.githubusercontent.com/minio/minio/master/#install-from-source"&gt;build MinIO&lt;/a&gt; and ensure the &lt;code&gt;minio&lt;/code&gt; binary exists in the project root.&lt;/p&gt; 
&lt;p&gt;The following command builds the Docker image using the default &lt;code&gt;Dockerfile&lt;/code&gt; in the root project directory with the repository and image tag &lt;code&gt;myminio:minio&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t myminio:minio .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;docker image ls&lt;/code&gt; to confirm the image exists in your local repository. You can run the server using standard Docker invocation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker run -p 9000:9000 -p 9001:9001 myminio:minio server /tmp/minio --console-address :9001
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Complete documentation for building Docker containers, managing custom images, or loading images into orchestration platforms is out of scope for this documentation. You can modify the &lt;code&gt;Dockerfile&lt;/code&gt; and &lt;code&gt;dockerscripts/docker-entrypoint.sh&lt;/code&gt; as-needed to reflect your specific image requirements.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://docs.min.io/community/minio-object-store/operations/deployments/baremetal-deploy-minio-as-a-container.html#deploy-minio-container"&gt;MinIO Container&lt;/a&gt; documentation for more guidance on running MinIO within a Container image.&lt;/p&gt; 
&lt;h2&gt;Install using Helm Charts&lt;/h2&gt; 
&lt;p&gt;There are two paths for installing MinIO onto Kubernetes infrastructure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the &lt;a href="https://github.com/minio/operator"&gt;MinIO Operator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Use the community-maintained &lt;a href="https://github.com/minio/minio/tree/master/helm/minio"&gt;Helm charts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://docs.min.io/community/minio-object-store/operations/deployments/kubernetes.html"&gt;MinIO Documentation&lt;/a&gt; for guidance on deploying using the Operator. The Community Helm chart has instructions in the folder-level README.&lt;/p&gt; 
&lt;h2&gt;Test MinIO Connectivity&lt;/h2&gt; 
&lt;h3&gt;Test using MinIO Console&lt;/h3&gt; 
&lt;p&gt;MinIO Server comes with an embedded web based object browser. Point your web browser to &lt;a href="http://127.0.0.1:9000"&gt;http://127.0.0.1:9000&lt;/a&gt; to ensure your server has started successfully.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] MinIO runs console on random port by default, if you wish to choose a specific port use &lt;code&gt;--console-address&lt;/code&gt; to pick a specific interface and port.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Test using MinIO Client &lt;code&gt;mc&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mc&lt;/code&gt; provides a modern alternative to UNIX commands like ls, cat, cp, mirror, diff etc. It supports filesystems and Amazon S3 compatible cloud storage services.&lt;/p&gt; 
&lt;p&gt;The following commands set a local alias, validate the server information, create a bucket, copy data to that bucket, and list the contents of the bucket.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;mc alias set local http://localhost:9000 minioadmin minioadmin
mc admin info
mc mb data
mc cp ~/Downloads/mydata data/
mc ls data/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the MinIO Client &lt;a href="https://docs.min.io/community/minio-object-store/reference/minio-mc.html#quickstart"&gt;Quickstart Guide&lt;/a&gt; for further instructions.&lt;/p&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/community/minio-object-store/index.html"&gt;The MinIO documentation website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/community/minio-object-store/operations/concepts/erasure-coding.html"&gt;MinIO Erasure Code Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/community/minio-object-store/reference/minio-mc.html"&gt;Use &lt;code&gt;mc&lt;/code&gt; with MinIO Server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/enterprise/aistor-object-store/developers/sdk/go/"&gt;Use &lt;code&gt;minio-go&lt;/code&gt; SDK with MinIO Server&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute to MinIO Project&lt;/h2&gt; 
&lt;p&gt;Please follow MinIO &lt;a href="https://github.com/minio/minio/raw/master/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt; for guidance on making new contributions to the repository.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;MinIO source is licensed under the &lt;a href="https://github.com/minio/minio/raw/master/LICENSE"&gt;GNU AGPLv3&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;MinIO &lt;a href="https://github.com/minio/minio/tree/master/docs"&gt;documentation&lt;/a&gt; is licensed under &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;CC BY 4.0&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/minio/minio/raw/master/COMPLIANCE.md"&gt;License Compliance&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>