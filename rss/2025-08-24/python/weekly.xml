<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Sat, 23 Aug 2025 01:47:45 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;franÃ§ais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸŒŸ Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ¤” Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“‚ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;ğŸŒ± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;ğŸ™ï¸ AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;â¤ï¸â€ğŸ©¹ AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;ğŸ“Š AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ğŸ©» AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;ğŸ˜‚ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;ğŸµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;ğŸ›« AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;âœ¨ Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/local_news_agent_openai_swarm/"&gt;ğŸŒ Local News Agent (OpenAI Swarm)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;ğŸ”„ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;ğŸ“Š xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;ğŸ” OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;ğŸ•¸ï¸ Web Scrapping AI Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;ğŸ” AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ğŸ¤ AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;ğŸ—ï¸ AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/"&gt;ğŸ¯ AI Lead Generation Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;ğŸ’° AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;ğŸ¬ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;ğŸ“ˆ AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;ğŸ‹ï¸â€â™‚ï¸ AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;ğŸš€ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;ğŸ—ï¸ AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;ğŸ§  AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;ğŸ“‘ AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;ğŸ§¬ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;ğŸ§ AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ® Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;ğŸ® AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;â™œ AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;ğŸ² AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¤ Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;ğŸ§² AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;ğŸ’² AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;ğŸ¨ AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;ğŸ‘¨â€âš–ï¸ AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;ğŸ’¼ AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;ğŸ  AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;ğŸ‘¨â€ğŸ’¼ AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;ğŸ‘¨â€ğŸ« AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;ğŸ’» Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;âœ¨ Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;ğŸŒ AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ—£ï¸ Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;ğŸ—£ï¸ AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;ğŸ“ Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;ğŸ”Š Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸŒ MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;â™¾ï¸ Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;ğŸ™ GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;ğŸ“‘ Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;ğŸŒ AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“€ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag/"&gt;ğŸ”— Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;ğŸ§ Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;ğŸ“° AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;ğŸ” Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;ğŸ”„ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;ğŸ‹ Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ğŸ¤” Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;ğŸ‘€ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;ğŸ”„ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;ğŸ–¥ï¸ Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ğŸ¦™ Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;ğŸ§© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;âœ¨ RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;â›“ï¸ Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;ğŸ“  RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;ğŸ–¼ï¸ Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ’¾ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;ğŸ’¾ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;ğŸ›©ï¸ AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;ğŸ’¬ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;ğŸ“ LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;ğŸ—„ï¸ Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;ğŸ§  Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ’¬ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;ğŸ’¬ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;ğŸ“¨ Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;ğŸ“„ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;ğŸ“š Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;ğŸ“ Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;ğŸ“½ï¸ Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”§ LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;ğŸ”§ Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ§‘â€ğŸ« AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Starter agent; modelâ€‘agnostic (OpenAI, Claude)&lt;/li&gt; 
   &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
   &lt;li&gt;Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools&lt;/li&gt; 
   &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
   &lt;li&gt;Simple multiâ€‘agent; Multiâ€‘agent patterns&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ¤ Contributing to Open Source&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new &lt;a href="https://github.com/Shubhamsaboo/awesome-llm-apps/issues"&gt;GitHub Issue&lt;/a&gt; or submit a pull request. Make sure to follow the existing project structure and include a detailed &lt;code&gt;README.md&lt;/code&gt; for each new app.&lt;/p&gt; 
&lt;h3&gt;Thank You, Community, for the Support! ğŸ™&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ğŸŒŸ &lt;strong&gt;Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>coleam00/Archon</title>
      <link>https://github.com/coleam00/Archon</link>
      <description>&lt;p&gt;Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/coleam00/Archon/main/archon-ui-main/public/archon-main-graphic.png" alt="Archon Main Graphic" width="853" height="422" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Power up your AI coding assistants with your own custom knowledge base and task management as an MCP server&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#quick-start"&gt;Quick Start&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#whats-included"&gt;What's Included&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#architecture"&gt;Architecture&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¯ What is Archon?&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Archon is currently in beta! Expect things to not work 100%, and please feel free to share any feedback and contribute with fixes/new features! Thank you to everyone for all the excitement we have for Archon already, as well as the bug reports, PRs, and discussions. It's a lot for our small team to get through but we're committed to addressing everything and making Archon into the best tool it possibly can be!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Archon is the &lt;strong&gt;command center&lt;/strong&gt; for AI coding assistants. For you, it's a sleek interface to manage knowledge, context, and tasks for your projects. For the AI coding assistant(s), it's a &lt;strong&gt;Model Context Protocol (MCP) server&lt;/strong&gt; to collaborate on and leverage the same knowledge, context, and tasks. Connect Claude Code, Kiro, Cursor, Windsurf, etc. to give your AI agents access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Your documentation&lt;/strong&gt; (crawled websites, uploaded PDFs/docs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart search capabilities&lt;/strong&gt; with advanced RAG strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task management&lt;/strong&gt; integrated with your knowledge base&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time updates&lt;/strong&gt; as you add new content and collaborate with your coding assistant on tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Much more&lt;/strong&gt; coming soon to build Archon into an integrated environment for all context engineering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This new vision for Archon replaces the old one (the agenteer). Archon used to be the AI agent that builds other agents, and now you can use Archon to do that and more.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;It doesn't matter what you're building or if it's a new/existing codebase - Archon's knowledge and task management capabilities will improve the output of &lt;strong&gt;any&lt;/strong&gt; AI driven coding.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ”— Important Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/coleam00/Archon/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/strong&gt; - Join the conversation and share ideas about Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; - How to get involved and contribute to Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://youtu.be/8pRc_s2VQIo"&gt;Introduction Video&lt;/a&gt;&lt;/strong&gt; - Getting started guide and vision for Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/users/coleam00/projects/1"&gt;Archon Kanban Board&lt;/a&gt;&lt;/strong&gt; - Where maintainers are managing issues/features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://dynamous.ai"&gt;Dynamous AI Mastery&lt;/a&gt;&lt;/strong&gt; - The birthplace of Archon - come join a vibrant community of other early AI adopters all helping each other transform their careers and businesses!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/"&gt;Node.js 18+&lt;/a&gt; (for hybrid development mode)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://supabase.com/"&gt;Supabase&lt;/a&gt; account (free tier or local Supabase both work)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI API key&lt;/a&gt; (Gemini and Ollama are supported too!)&lt;/li&gt; 
 &lt;li&gt;(OPTIONAL) &lt;a href="https://www.gnu.org/software/make/"&gt;Make&lt;/a&gt; (see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#installing-make"&gt;Installing Make&lt;/a&gt; below)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone Repository&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/coleam00/archon.git
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd archon
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment Configuration&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
# Edit .env and add your Supabase credentials:
# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_SERVICE_KEY=your-service-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NOTE: Supabase introduced a new type of service key but use the legacy one (the longer one).&lt;/p&gt; &lt;p&gt;OPTIONAL: If you want to enable the reranking RAG strategy, uncomment lines 20-22 in &lt;code&gt;python\requirements.server.txt&lt;/code&gt;. This will significantly increase the size of the Archon Server container which is why it's off by default.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Setup&lt;/strong&gt;: In your &lt;a href="https://supabase.com/dashboard"&gt;Supabase project&lt;/a&gt; SQL Editor, copy, paste, and execute the contents of &lt;code&gt;migration/complete_setup.sql&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Services&lt;/strong&gt; (choose one):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Docker Mode (Recommended for Normal Archon Usage)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose --profile full up --build -d
# or
make dev-docker # (Alternative: Requires make installed )
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This starts all core microservices in Docker:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Core API and business logic (Port: 8181)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;: Protocol interface for AI clients (Port: 8051)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Agents (coming soon!)&lt;/strong&gt;: AI operations and streaming (Port: 8052)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: Web interface (Port: 3737)&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Ports are configurable in your .env as well!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure API Keys&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Open &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Go to &lt;strong&gt;Settings&lt;/strong&gt; â†’ Select your LLM/embedding provider and set the API key (OpenAI is default)&lt;/li&gt; 
   &lt;li&gt;Test by uploading a document or crawling a website&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ğŸš€ Quick Command Reference&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make dev&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Start hybrid dev (backend in Docker, frontend local) â­&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make dev-docker&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Everything in Docker&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make stop&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Stop all services&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make test&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Run all tests&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make lint&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Run linters&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make install&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Install dependencies&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make check&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Check environment setup&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make clean&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Remove containers and volumes (with confirmation)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ”„ Database Reset (Start Fresh if Needed)&lt;/h2&gt; 
&lt;p&gt;If you need to completely reset your database and start fresh:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;âš ï¸ &lt;strong&gt;Reset Database - This will delete ALL data for Archon!&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Reset Script&lt;/strong&gt;: In your Supabase SQL Editor, run the contents of &lt;code&gt;migration/RESET_DB.sql&lt;/code&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ WARNING: This will delete all Archon specific tables and data! Nothing else will be touched in your DB though.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rebuild Database&lt;/strong&gt;: After reset, run &lt;code&gt;migration/complete_setup.sql&lt;/code&gt; to create all the tables again.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Restart Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose --profile full up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconfigure&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Select your LLM/embedding provider and set the API key again&lt;/li&gt; 
    &lt;li&gt;Re-upload any documents or re-crawl websites&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The reset script safely removes all tables, functions, triggers, and policies with proper dependency handling.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ› ï¸ Installing Make (OPTIONAL)&lt;/h2&gt; 
&lt;p&gt;Make is required for the local development workflow. Installation varies by platform:&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Option 1: Using Chocolatey
choco install make

# Option 2: Using Scoop
scoop install make

# Option 3: Using WSL2
wsl --install
# Then in WSL: sudo apt-get install make
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Make comes pre-installed on macOS
# If needed: brew install make
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Debian/Ubuntu
sudo apt-get install make

# RHEL/CentOS/Fedora
sudo yum install make
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš¡ Quick Test&lt;/h2&gt; 
&lt;p&gt;Once everything is running:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Test Web Crawling&lt;/strong&gt;: Go to &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt; â†’ Knowledge Base â†’ "Crawl Website" â†’ Enter a doc URL (such as &lt;a href="https://ai.pydantic.dev/llms-full.txt"&gt;https://ai.pydantic.dev/llms-full.txt&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Document Upload&lt;/strong&gt;: Knowledge Base â†’ Upload a PDF&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Projects&lt;/strong&gt;: Projects â†’ Create a new project and add tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrate with your AI coding assistant&lt;/strong&gt;: MCP Dashboard â†’ Copy connection config for your AI coding assistant&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Services&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Container Name&lt;/th&gt; 
   &lt;th&gt;Default URL&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-ui&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main dashboard and controls&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-server&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8181"&gt;http://localhost:8181&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Web crawling, document processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-mcp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8051"&gt;http://localhost:8051&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model Context Protocol interface&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8052"&gt;http://localhost:8052&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AI/ML operations, reranking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;What's Included&lt;/h2&gt; 
&lt;h3&gt;ğŸ§  Knowledge Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Web Crawling&lt;/strong&gt;: Automatically detects and crawls entire documentation sites, sitemaps, and individual pages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Processing&lt;/strong&gt;: Upload and process PDFs, Word docs, markdown files, and text documents with intelligent chunking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Example Extraction&lt;/strong&gt;: Automatically identifies and indexes code examples from documentation for enhanced search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vector Search&lt;/strong&gt;: Advanced semantic search with contextual embeddings for precise knowledge retrieval&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Management&lt;/strong&gt;: Organize knowledge by source, type, and tags for easy filtering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¤– AI Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: Connect any MCP-compatible client (Claude Code, Cursor, even non-AI coding assistants like Claude Desktop)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;10 MCP Tools&lt;/strong&gt;: Comprehensive yet simple set of tools for RAG queries, task management, and project operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-LLM Support&lt;/strong&gt;: Works with OpenAI, Ollama, and Google Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG Strategies&lt;/strong&gt;: Hybrid search, contextual embeddings, and result reranking for optimal AI responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Streaming&lt;/strong&gt;: Live responses from AI agents with progress tracking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“‹ Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hierarchical Projects&lt;/strong&gt;: Organize work with projects, features, and tasks in a structured workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI-Assisted Creation&lt;/strong&gt;: Generate project requirements and tasks using integrated AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Management&lt;/strong&gt;: Version-controlled documents with collaborative editing capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Real-time updates and status management across all project activities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”„ Real-time Collaboration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;WebSocket Updates&lt;/strong&gt;: Live progress tracking for crawling, processing, and AI operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-user Support&lt;/strong&gt;: Collaborative knowledge building and project management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt;: Asynchronous operations that don't block the user interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Health Monitoring&lt;/strong&gt;: Built-in service health checks and automatic reconnection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Microservices Structure&lt;/h3&gt; 
&lt;p&gt;Archon uses true microservices architecture with clear separation of concerns:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend UI   â”‚    â”‚  Server (API)   â”‚    â”‚   MCP Server    â”‚    â”‚ Agents Service  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  React + Vite   â”‚â—„â”€â”€â–ºâ”‚    FastAPI +    â”‚â—„â”€â”€â–ºâ”‚    Lightweight  â”‚â—„â”€â”€â–ºâ”‚   PydanticAI    â”‚
â”‚  Port 3737      â”‚    â”‚    SocketIO     â”‚    â”‚    HTTP Wrapper â”‚    â”‚   Port 8052     â”‚
â”‚                 â”‚    â”‚    Port 8181    â”‚    â”‚    Port 8051    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚                        â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                         â”‚    Database     â”‚               â”‚
                         â”‚                 â”‚               â”‚
                         â”‚    Supabase     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚    PostgreSQL   â”‚
                         â”‚    PGVector     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Service Responsibilities&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Location&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Key Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Frontend&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;archon-ui-main/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Web interface and dashboard&lt;/td&gt; 
   &lt;td&gt;React, TypeScript, TailwindCSS, Socket.IO client&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/server/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Core business logic and APIs&lt;/td&gt; 
   &lt;td&gt;FastAPI, service layer, Socket.IO broadcasts, all ML/AI operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/mcp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;MCP protocol interface&lt;/td&gt; 
   &lt;td&gt;Lightweight HTTP wrapper, 10 MCP tools, session management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/agents/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PydanticAI agent hosting&lt;/td&gt; 
   &lt;td&gt;Document and RAG agents, streaming responses&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Communication Patterns&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP-based&lt;/strong&gt;: All inter-service communication uses HTTP APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Socket.IO&lt;/strong&gt;: Real-time updates from Server to Frontend&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Protocol&lt;/strong&gt;: AI clients connect to MCP Server via SSE or stdio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Direct Imports&lt;/strong&gt;: Services are truly independent with no shared code dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Architectural Benefits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight Containers&lt;/strong&gt;: Each service contains only required dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Independent Scaling&lt;/strong&gt;: Services can be scaled independently based on load&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development Flexibility&lt;/strong&gt;: Teams can work on different services without conflicts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technology Diversity&lt;/strong&gt;: Each service uses the best tools for its specific purpose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”§ Configuring Custom Ports &amp;amp; Hostname&lt;/h2&gt; 
&lt;p&gt;By default, Archon services run on the following ports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;archon-ui&lt;/strong&gt;: 3737&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-server&lt;/strong&gt;: 8181&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-mcp&lt;/strong&gt;: 8051&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-agents&lt;/strong&gt;: 8052&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-docs&lt;/strong&gt;: 3838 (optional)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Changing Ports&lt;/h3&gt; 
&lt;p&gt;To use custom ports, add these variables to your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Service Ports Configuration
ARCHON_UI_PORT=3737
ARCHON_SERVER_PORT=8181
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_DOCS_PORT=3838
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example: Running on different ports:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ARCHON_SERVER_PORT=8282
ARCHON_MCP_PORT=8151
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuring Hostname&lt;/h3&gt; 
&lt;p&gt;By default, Archon uses &lt;code&gt;localhost&lt;/code&gt; as the hostname. You can configure a custom hostname or IP address by setting the &lt;code&gt;HOST&lt;/code&gt; variable in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Hostname Configuration
HOST=localhost  # Default

# Examples of custom hostnames:
HOST=192.168.1.100     # Use specific IP address
HOST=archon.local      # Use custom domain
HOST=myserver.com      # Use public domain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running Archon on a different machine and accessing it remotely&lt;/li&gt; 
 &lt;li&gt;Using a custom domain name for your installation&lt;/li&gt; 
 &lt;li&gt;Deploying in a network environment where &lt;code&gt;localhost&lt;/code&gt; isn't accessible&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After changing hostname or ports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Restart Docker containers: &lt;code&gt;docker compose down &amp;amp;&amp;amp; docker compose --profile full up -d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Access the UI at: &lt;code&gt;http://${HOST}:${ARCHON_UI_PORT}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Update your AI client configuration with the new hostname and MCP port&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ”§ Development&lt;/h2&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
make install

# Start development (recommended)
make dev        # Backend in Docker, frontend local with hot reload

# Alternative: Everything in Docker
make dev-docker # All services in Docker

# Stop everything (local FE needs to be stopped manually)
make stop
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Modes&lt;/h3&gt; 
&lt;h4&gt;Hybrid Mode (Recommended) - &lt;code&gt;make dev&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;Best for active development with instant frontend updates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Backend services run in Docker (isolated, consistent)&lt;/li&gt; 
 &lt;li&gt;Frontend runs locally with hot module replacement&lt;/li&gt; 
 &lt;li&gt;Instant UI updates without Docker rebuilds&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Full Docker Mode - &lt;code&gt;make dev-docker&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;For all services in Docker environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All services run in Docker containers&lt;/li&gt; 
 &lt;li&gt;Better for integration testing&lt;/li&gt; 
 &lt;li&gt;Slower frontend updates&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Testing &amp;amp; Code Quality&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run tests
make test       # Run all tests
make test-fe    # Run frontend tests
make test-be    # Run backend tests

# Run linters
make lint       # Lint all code
make lint-fe    # Lint frontend code
make lint-be    # Lint backend code

# Check environment
make check      # Verify environment setup

# Clean up
make clean      # Remove containers and volumes (asks for confirmation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Viewing Logs&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# View logs using Docker Compose directly
docker compose logs -f              # All services
docker compose logs -f archon-server # API server
docker compose logs -f archon-mcp    # MCP server
docker compose logs -f archon-ui     # Frontend
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The backend services are configured with &lt;code&gt;--reload&lt;/code&gt; flag in their uvicorn commands and have source code mounted as volumes for automatic hot reloading when you make changes.&lt;/p&gt; 
&lt;h2&gt;ğŸ” Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Issues and Solutions&lt;/h3&gt; 
&lt;h4&gt;Port Conflicts&lt;/h4&gt; 
&lt;p&gt;If you see "Port already in use" errors:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check what's using a port (e.g., 3737)
lsof -i :3737

# Stop all containers and local services
make stop

# Change the port in .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Permission Issues (Linux)&lt;/h4&gt; 
&lt;p&gt;If you encounter permission errors with Docker:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Add your user to the docker group
sudo usermod -aG docker $USER

# Log out and back in, or run
newgrp docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Windows-Specific Issues&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Make not found&lt;/strong&gt;: Install Make via Chocolatey, Scoop, or WSL2 (see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#installing-make"&gt;Installing Make&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Line ending issues&lt;/strong&gt;: Configure Git to use LF endings: &lt;pre&gt;&lt;code class="language-bash"&gt;git config --global core.autocrlf false
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Frontend Can't Connect to Backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check backend is running: &lt;code&gt;curl http://localhost:8181/health&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Verify port configuration in &lt;code&gt;.env&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For custom ports, ensure both &lt;code&gt;ARCHON_SERVER_PORT&lt;/code&gt; and &lt;code&gt;VITE_ARCHON_SERVER_PORT&lt;/code&gt; are set&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Docker Compose Hangs&lt;/h4&gt; 
&lt;p&gt;If &lt;code&gt;docker compose&lt;/code&gt; commands hang:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Reset Docker Compose
docker compose down --remove-orphans
docker system prune -f

# Restart Docker Desktop (if applicable)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Hot Reload Not Working&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Ensure you're running in hybrid mode (&lt;code&gt;make dev&lt;/code&gt;) for best HMR experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Check that volumes are mounted correctly in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File permissions&lt;/strong&gt;: On some systems, mounted volumes may have permission issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ˆ Progress&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://star-history.com/#coleam00/Archon&amp;amp;Date"&gt; &lt;img src="https://api.star-history.com/svg?repos=coleam00/Archon&amp;amp;type=Date" width="500" alt="Star History Chart" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Archon Community License (ACL) v1.2 - see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Archon is free, open, and hackable. Run it, fork it, share it - just don't sell it as-a-service without permission.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HunxByts/GhostTrack</title>
      <link>https://github.com/HunxByts/GhostTrack</link>
      <description>&lt;p&gt;Useful tool to track location or mobile number&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GhostTrack&lt;/h1&gt; 
&lt;p&gt;Useful tool to track location or mobile number, so this tool can be called osint or also information gathering&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png" /&gt; 
&lt;p&gt;New update : &lt;code&gt;Version 2.2&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Instalation on Linux (deb)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install git
sudo apt-get install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Instalation on Termux&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pkg install git
pkg install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage Tool&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/HunxByts/GhostTrack.git
cd GhostTrack
pip3 install -r requirements.txt
python3 GhostTR.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;IP Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png " /&gt; 
&lt;p&gt;on the IP Track menu, you can combo with the seeker tool to get the target IP&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;âš¡&lt;/span&gt; Install Seeker :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/thewhiteh4t/seeker"&gt;Get Seeker&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Phone Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png" /&gt; 
&lt;p&gt;on this menu you can search for information from the target phone number&lt;/p&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Username Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png" /&gt; on this menu you can search for information from the target username on social media 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;âš¡&lt;/span&gt; Author :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/HunxByts"&gt;HunxByts&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>modelscope/DiffSynth-Studio</title>
      <link>https://github.com/modelscope/DiffSynth-Studio</link>
      <description>&lt;p&gt;Enjoy the magic of Diffusion models!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DiffSynth-Studio&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/.github/workflows/logo.gif" title="Logo" style="max-width:100%;" width="55" /&gt;&lt;/a&gt; &lt;a href="https://trendshift.io/repositories/10946" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10946" alt="modelscope%2FDiffSynth-Studio | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/DiffSynth/"&gt;&lt;img src="https://img.shields.io/pypi/v/DiffSynth" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelscope/DiffSynth-Studio/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg?sanitize=true" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelscope/DiffSynth-Studio/issues"&gt;&lt;img src="https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg?sanitize=true" alt="open issues" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/modelscope/DiffSynth-Studio/pull/"&gt;&lt;img src="https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg?sanitize=true" alt="GitHub pull-requests" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/modelscope/DiffSynth-Studio/commit/"&gt;&lt;img src="https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio" alt="GitHub latest commit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/README_zh.md"&gt;åˆ‡æ¢åˆ°ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to the magic world of Diffusion models! DiffSynth-Studio is an open-source Diffusion model engine developed and maintained by &lt;a href="https://www.modelscope.cn/"&gt;ModelScope&lt;/a&gt; team. We aim to foster technical innovation through framework development, bring together the power of the open-source community, and explore the limits of generative models!&lt;/p&gt; 
&lt;p&gt;DiffSynth currently includes two open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt;: Focused on aggressive technical exploration, for academia, providing support for more cutting-edge model capabilities.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt;: Focused on stable model deployment, for industry, offering higher computing performance and more stable features.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt; and &lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt; are the core projects behind ModelScope &lt;a href="https://modelscope.cn/aigc/home"&gt;AIGC zone&lt;/a&gt;, offering powerful AI content generation abilities. Come and try our carefully designed features and start your AI creation journey!&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Install from source (recommended):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git  
cd DiffSynth-Studio
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation methods&lt;/summary&gt; 
 &lt;p&gt;Install from PyPI (version updates may be delayed; for latest features, install from source)&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;pip install diffsynth
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you meet problems during installation, they might be caused by upstream dependencies. Please check the docs of these packages:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pytorch.org/get-started/locally/"&gt;torch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;sentencepiece&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://cmake.org"&gt;cmake&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.cupy.dev/en/stable/install.html"&gt;cupy&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Basic Framework&lt;/h2&gt; 
&lt;p&gt;DiffSynth-Studio redesigns the inference and training pipelines for mainstream Diffusion models (including FLUX, Wan, etc.), enabling efficient memory management and flexible model training.&lt;/p&gt; 
&lt;h3&gt;Qwen-Image Series (ğŸ”¥New Model)&lt;/h3&gt; 
&lt;p&gt;Details: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/"&gt;./examples/qwen_image/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/738078d8-8749-4a53-a046-571861541924" alt="Image" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig
import torch

pipe = QwenImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="transformer/diffusion_pytorch_model*.safetensors"),
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="text_encoder/model*.safetensors"),
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="vae/diffusion_pytorch_model.safetensors"),
    ],
    tokenizer_config=ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="tokenizer/"),
)
prompt = "A detailed portrait of a girl underwater, wearing a blue flowing dress, hair gently floating, clear light and shadow, surrounded by bubbles, calm expression, fine details, dreamy and beautiful."
image = pipe(prompt, seed=0, num_inference_steps=40)
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Model Overview&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low VRAM Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Validation after Full Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;Validation after LoRA Training&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image"&gt;Qwen/Qwen-Image&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit"&gt;Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-V2"&gt;DiffSynth-Studio/Qwen-Image-EliGen-V2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen-V2.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-V2.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-EliGen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full"&gt;DiffSynth-Studio/Qwen-Image-Distill-Full&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Distill-Full.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Distill-Full.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-LoRA"&gt;DiffSynth-Studio/Qwen-Image-Distill-LoRA&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Distill-LoRA.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Distill-LoRA.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen"&gt;DiffSynth-Studio/Qwen-Image-EliGen&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-EliGen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Canny.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Canny.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Depth.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Depth.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Inpaint.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Inpaint.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union"&gt;DiffSynth-Studio/Qwen-Image-In-Context-Control-Union&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-In-Context-Control-Union.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-In-Context-Control-Union.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-In-Context-Control-Union.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-In-Context-Control-Union.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix"&gt;DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit-Lowres-Fix.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit-Lowres-Fix.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;FLUX Series&lt;/h3&gt; 
&lt;p&gt;Detail page: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/"&gt;./examples/flux/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/c01258e2-f251-441a-aa1e-ebb22f02594d" alt="Image" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from diffsynth.pipelines.flux_image_new import FluxImagePipeline, ModelConfig

pipe = FluxImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="flux1-dev.safetensors"),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder/model.safetensors"),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder_2/"),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="ae.safetensors"),
    ],
)

image = pipe(prompt="a cat", seed=0)
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Model Overview&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Extra Parameters&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low VRAM Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Validate After Full Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;Validate After LoRA Training&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-dev"&gt;FLUX.1-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev"&gt;FLUX.1-Krea-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-Krea-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-Krea-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Kontext-dev"&gt;FLUX.1-Kontext-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;kontext_images&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-Kontext-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-Kontext-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta"&gt;FLUX.1-dev-Controlnet-Inpainting-Beta&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Inpainting-Beta.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Inpainting-Beta.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/InstantX/FLUX.1-dev-Controlnet-Union-alpha"&gt;FLUX.1-dev-Controlnet-Union-alpha&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Union-alpha.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Union-alpha.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/jasperai/Flux.1-dev-Controlnet-Upscaler"&gt;FLUX.1-dev-Controlnet-Upscaler&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Upscaler.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Upscaler.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/InstantX/FLUX.1-dev-IP-Adapter"&gt;FLUX.1-dev-IP-Adapter&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;ipadapter_images&lt;/code&gt;, &lt;code&gt;ipadapter_scale&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-IP-Adapter.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-IP-Adapter.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/ByteDance/InfiniteYou"&gt;FLUX.1-dev-InfiniteYou&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;infinityou_id_image&lt;/code&gt;, &lt;code&gt;infinityou_guidance&lt;/code&gt;, &lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-InfiniteYou.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-InfiniteYou.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Eligen"&gt;FLUX.1-dev-EliGen&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;eligen_entity_prompts&lt;/code&gt;, &lt;code&gt;eligen_entity_masks&lt;/code&gt;, &lt;code&gt;eligen_enable_on_negative&lt;/code&gt;, &lt;code&gt;eligen_enable_inpaint&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-EliGen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev"&gt;FLUX.1-dev-LoRA-Encoder&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_encoder_inputs&lt;/code&gt;, &lt;code&gt;lora_encoder_scale&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-LoRA-Encoder.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-LoRA-Encoder.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-LoRA-Encoder.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-LoRA-Encoder.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev"&gt;FLUX.1-dev-LoRA-Fusion-Preview&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/stepfun-ai/Step1X-Edit"&gt;Step1X-Edit&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;step1x_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/Step1X-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/Step1X-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/ostris/Flex.2-preview"&gt;FLEX.2-preview&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;flex_inpaint_image&lt;/code&gt;, &lt;code&gt;flex_inpaint_mask&lt;/code&gt;, &lt;code&gt;flex_control_image&lt;/code&gt;, &lt;code&gt;flex_control_strength&lt;/code&gt;, &lt;code&gt;flex_control_stop&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLEX.2-preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLEX.2-preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2"&gt;Nexus-Gen&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;nexus_gen_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/Nexus-Gen-Editing.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/Nexus-Gen-Editing.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/Nexus-Gen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/Nexus-Gen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/Nexus-Gen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/Nexus-Gen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Wan Series&lt;/h3&gt; 
&lt;p&gt;Detail page: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;./examples/wanvideo/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/1d66ae74-3b02-40a9-acc3-ea95fc039314"&gt;https://github.com/user-attachments/assets/1d66ae74-3b02-40a9-acc3-ea95fc039314&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from diffsynth import save_video
from diffsynth.pipelines.wan_video_new import WanVideoPipeline, ModelConfig

pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="diffusion_pytorch_model*.safetensors", offload_device="cpu"),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="Wan2.1_VAE.pth", offload_device="cpu"),
    ],
)
pipe.enable_vram_management()

video = pipe(
    prompt="A documentary photography style scene: a lively puppy rapidly running on green grass. The puppy has brown-yellow fur, upright ears, and looks focused and joyful. Sunlight shines on its body, making the fur appear soft and shiny. The background is an open field with occasional wildflowers, and faint blue sky and clouds in the distance. Strong sense of perspective captures the motion of the puppy and the vitality of the surrounding grass. Mid-shot side-moving view.",
    negative_prompt="Bright colors, overexposed, static, blurry details, subtitles, style, artwork, image, still, overall gray, worst quality, low quality, JPEG compression artifacts, ugly, deformed, extra fingers, poorly drawn hands, poorly drawn face, malformed limbs, fused fingers, still frame, messy background, three legs, crowded background people, walking backwards",
    seed=0, tiled=True,
)
save_video(video, "video1.mp4", fps=15, quality=5)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Model Overview&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Extra Parameters&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Validate After Full Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;Validate After LoRA Training&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B"&gt;Wan-AI/Wan2.2-I2V-A14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-I2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-I2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-I2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-I2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-I2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.2-T2V-A14B"&gt;Wan-AI/Wan2.2-T2V-A14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-T2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-T2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-T2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-T2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-T2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.2-TI2V-5B"&gt;Wan-AI/Wan2.2-TI2V-5B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-TI2V-5B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-TI2V-5B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-TI2V-5B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-TI2V-5B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-TI2V-5B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B"&gt;Wan-AI/Wan2.1-T2V-1.3B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-T2V-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-T2V-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-T2V-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-T2V-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B"&gt;Wan-AI/Wan2.1-T2V-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-T2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-T2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-T2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-T2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P"&gt;Wan-AI/Wan2.1-I2V-14B-480P&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-I2V-14B-480P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-I2V-14B-480P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-480P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-480P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-480P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P"&gt;Wan-AI/Wan2.1-I2V-14B-720P&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-I2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-I2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P"&gt;Wan-AI/Wan2.1-FLF2V-14B-720P&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-FLF2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-FLF2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-FLF2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-FLF2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-FLF2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-InP"&gt;PAI/Wan2.1-Fun-1.3B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-Control"&gt;PAI/Wan2.1-Fun-1.3B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-InP"&gt;PAI/Wan2.1-Fun-14B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-Control"&gt;PAI/Wan2.1-Fun-14B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control"&gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;, &lt;code&gt;reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control"&gt;PAI/Wan2.1-Fun-V1.1-14B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;, &lt;code&gt;reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/examples/wanmodel_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP"&gt;PAI/Wan2.1-Fun-V1.1-1.3B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-InP"&gt;PAI/Wan2.1-Fun-V1.1-14B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera"&gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_camera_video&lt;/code&gt;, &lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control-Camera"&gt;PAI/Wan2.1-Fun-V1.1-14B-Control-Camera&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_camera_video&lt;/code&gt;, &lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/iic/VACE-Wan2.1-1.3B-Preview"&gt;iic/VACE-Wan2.1-1.3B-Preview&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-VACE-1.3B-Preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B-Preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B-Preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B-Preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B-Preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-1.3B"&gt;Wan-AI/Wan2.1-VACE-1.3B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-VACE-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-14B"&gt;Wan-AI/Wan2.1-VACE-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-VACE-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-VACE-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-VACE-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1"&gt;DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;motion_bucket_id&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-1.3b-speedcontrol-v1.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-1.3b-speedcontrol-v1.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-1.3b-speedcontrol-v1.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-1.3b-speedcontrol-v1.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-1.3b-speedcontrol-v1.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;More Models&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Image Generation Models&lt;/summary&gt; 
 &lt;p&gt;Detail page: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/"&gt;./examples/image_synthesis/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;FLUX&lt;/th&gt; 
    &lt;th&gt;Stable Diffusion 3&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/984561e9-553d-4952-9443-79ce144f379f" alt="image_1024_cfg" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/modelscope/DiffSynth-Studio/assets/35051019/4df346db-6f91-420a-b4c1-26e205376098" alt="image_1024" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Kolors&lt;/th&gt; 
    &lt;th&gt;Hunyuan-DiT&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/modelscope/DiffSynth-Studio/assets/35051019/53ef6f41-da11-4701-8665-9f64392607bf" alt="image_1024" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/modelscope/DiffSynth-Studio/assets/35051019/60b022c8-df3f-4541-95ab-bf39f2fa8bb5" alt="image_1024" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Stable Diffusion&lt;/th&gt; 
    &lt;th&gt;Stable Diffusion XL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/6fc84611-8da6-4a1f-8fee-9a34eba3b4a5" alt="1024" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/67687748-e738-438c-aee5-96096f09ac90" alt="1024" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Generation Models&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;HunyuanVideo: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/"&gt;./examples/HunyuanVideo/&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/48dd24bb-0cc6-40d2-88c3-10feed3267e9"&gt;https://github.com/user-attachments/assets/48dd24bb-0cc6-40d2-88c3-10feed3267e9&lt;/a&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;StepVideo: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/stepvideo/"&gt;./examples/stepvideo/&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5954fdaa-a3cf-45a3-bd35-886e3cc4581b"&gt;https://github.com/user-attachments/assets/5954fdaa-a3cf-45a3-bd35-886e3cc4581b&lt;/a&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CogVideoX: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/CogVideoX/"&gt;./examples/CogVideoX/&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006"&gt;https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Image Quality Assessment Models&lt;/summary&gt; 
 &lt;p&gt;We have integrated a series of image quality assessment models. These models can be used for evaluating image generation models, alignment training, and similar tasks.&lt;/p&gt; 
 &lt;p&gt;Detail page: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_quality_metric/"&gt;./examples/image_quality_metric/&lt;/a&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/THUDM/ImageReward"&gt;ImageReward&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/christophschuhmann/improved-aesthetic-predictor"&gt;Aesthetic&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/yuvalkirstain/pickscore"&gt;PickScore&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/openai/CLIP"&gt;CLIP&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/tgxs002/HPSv2"&gt;HPSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/tgxs002/HPSv2"&gt;HPSv2.1&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Kwai-Kolors/MPS"&gt;MPS&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Innovative Achievements&lt;/h2&gt; 
&lt;p&gt;DiffSynth-Studio is not just an engineering model framework, but also a platform for incubating innovative results.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Nexus-Gen: Unified Architecture for Image Understanding, Generation, and Editing&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detail page: &lt;a href="https://github.com/modelscope/Nexus-Gen"&gt;https://github.com/modelscope/Nexus-Gen&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/pdf/2504.21356"&gt;Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/Nexus-GenV2"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset"&gt;ModelScope Dataset&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Online Demo: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen"&gt;ModelScope Nexus-Gen Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;img src="https://github.com/modelscope/Nexus-Gen/raw/main/assets/illustrations/gen_edit.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ArtAug: Aesthetic Enhancement for Image Generation Models&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detail page: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ArtAug/"&gt;./examples/ArtAug/&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2412.12888"&gt;ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Online Demo: &lt;a href="https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&amp;amp;versionId=7228&amp;amp;modelType=LoRA&amp;amp;sdVersion=FLUX_1&amp;amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0"&gt;ModelScope AIGC Tab&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;FLUX.1-dev&lt;/th&gt; 
    &lt;th&gt;FLUX.1-dev + ArtAug LoRA&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/e1d5c505-b423-45fe-be01-25c2758f5417" alt="image_1_base" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/335908e3-d0bd-41c2-9d99-d10528a2d719" alt="image_1_enhance" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;EliGen: Precise Image Region Control&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detail page: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/EntityControl/"&gt;./examples/EntityControl/&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.01097"&gt;EliGen: Entity-Level Controlled Image Generation with Regional Attention&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Eligen"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/EliGen"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Online Demo: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen"&gt;ModelScope EliGen Studio&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet"&gt;EliGen Train Set&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Entity Control Mask&lt;/th&gt; 
    &lt;th&gt;Generated Image&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/1c6d9445-5022-4d91-ad2e-dc05321883d1" alt="eligen_example_2_mask_0" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/86739945-cb07-4a49-b3b3-3bb65c90d14f" alt="eligen_example_2_0" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ExVideo: Extended Training for Video Generation Models&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Project Page: &lt;a href="https://ecnu-cilab.github.io/ExVideoProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2406.14130"&gt;ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Code Example: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/"&gt;./examples/ExVideo/&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc"&gt;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Diffutoon: High-Resolution Anime-Style Video Rendering&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Project Page: &lt;a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2401.16224"&gt;Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Code Example: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/Diffutoon/"&gt;./examples/Diffutoon/&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd"&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DiffSynth: The Initial Version of This Project&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Project Page: &lt;a href="https://ecnu-cilab.github.io/DiffSynth.github.io/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2308.03463"&gt;DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Code Example: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/diffsynth/"&gt;./examples/diffsynth/&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea"&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Update History&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2025&lt;/strong&gt;: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-V2"&gt;DiffSynth-Studio/Qwen-Image-EliGen-V2&lt;/a&gt; is released! Compared to the V1 version, the training dataset has been updated to the &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset"&gt;Qwen-Image-Self-Generated-Dataset&lt;/a&gt;, enabling generated images to better align with the inherent image distribution and style of Qwen-Image. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-V2.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2025&lt;/strong&gt;: We open-sourced the &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union"&gt;DiffSynth-Studio/Qwen-Image-In-Context-Control-Union&lt;/a&gt; structure control LoRA model. Following "In Context" routine, it supports various types of structural control conditions, including canny, depth, lineart, softedge, normal, and openpose. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-In-Context-Control-Union.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 20, 2025&lt;/strong&gt; We open-sourced &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix"&gt;DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix&lt;/a&gt;, which improves the editing performance of Qwen-Image-Edit on low-resolution image inputs. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit-Lowres-Fix.py"&gt;our example code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 19, 2025&lt;/strong&gt; ğŸ”¥ Qwen-Image-Edit is now open source. Welcome the new member to the image editing model family!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 18, 2025&lt;/strong&gt; We trained and open-sourced the Inpaint ControlNet model for Qwen-Image, &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint&lt;/a&gt;, which adopts a lightweight architectural design. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 15, 2025&lt;/strong&gt; We open-sourced the &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset"&gt;Qwen-Image-Self-Generated-Dataset&lt;/a&gt;. This is an image dataset generated using the Qwen-Image model, with a total of 160,000 &lt;code&gt;1024 x 1024&lt;/code&gt; images. It includes the general, English text rendering, and Chinese text rendering subsets. We provide caption, entity and control images annotations for each image. Developers can use this dataset to train models such as ControlNet and EliGen for the Qwen-Image model. We aim to promote technological development through open-source contributions!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 13, 2025&lt;/strong&gt; We trained and open-sourced the ControlNet model for Qwen-Image, &lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth&lt;/a&gt;, which adopts a lightweight architectural design. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 12, 2025&lt;/strong&gt; We trained and open-sourced the ControlNet model for Qwen-Image, &lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny&lt;/a&gt;, which adopts a lightweight architectural design. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 11, 2025&lt;/strong&gt; We released another distilled acceleration model for Qwen-Image, &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-LoRA"&gt;DiffSynth-Studio/Qwen-Image-Distill-LoRA&lt;/a&gt;. It uses the same training process as &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full"&gt;DiffSynth-Studio/Qwen-Image-Distill-Full&lt;/a&gt;, but the model structure is changed to LoRA. This makes it work better with other open-source models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 7, 2025&lt;/strong&gt; We open-sourced the entity control LoRA of Qwen-Image, &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen"&gt;DiffSynth-Studio/Qwen-Image-EliGen&lt;/a&gt;. Qwen-Image-EliGen is able to achieve entity-level controlled text-to-image generation. See the &lt;a href="https://arxiv.org/abs/2501.01097"&gt;paper&lt;/a&gt; for technical details. Training dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet"&gt;EliGenTrainSet&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 5, 2025&lt;/strong&gt; We open-sourced the distilled acceleration model of Qwen-Image, &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full"&gt;DiffSynth-Studio/Qwen-Image-Distill-Full&lt;/a&gt;, achieving approximately 5x speedup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 4, 2025&lt;/strong&gt; ğŸ”¥ Qwen-Image is now open source. Welcome the new member to the image generation model family!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 1, 2025&lt;/strong&gt; &lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev"&gt;FLUX.1-Krea-dev&lt;/a&gt; with a focus on aesthetic photography is comprehensively supported, including low-GPU-memory layer-by-layer offload, LoRA training and full training. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/"&gt;./examples/flux/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;July 28, 2025&lt;/strong&gt; With the open-sourcing of Wan 2.2, we immediately provided comprehensive support, including low-GPU-memory layer-by-layer offload, FP8 quantization, sequence parallelism, LoRA training, full training. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;July 11, 2025&lt;/strong&gt; We propose Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models. This framework enables seamless image understanding, generation, and editing tasks.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Paper: &lt;a href="https://arxiv.org/pdf/2504.21356"&gt;Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Github Repo: &lt;a href="https://github.com/modelscope/Nexus-Gen"&gt;https://github.com/modelscope/Nexus-Gen&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/Nexus-GenV2"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Training Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset"&gt;ModelScope Dataset&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Online Demo: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen"&gt;ModelScope Nexus-Gen Studio&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 15, 2025&lt;/strong&gt; ModelScope's official evaluation framework, &lt;a href="https://github.com/modelscope/evalscope"&gt;EvalScope&lt;/a&gt;, now supports text-to-image generation evaluation. Try it with the &lt;a href="https://evalscope.readthedocs.io/zh-cn/latest/best_practice/t2i_eval.html"&gt;Best Practices&lt;/a&gt; guide.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 25, 2025&lt;/strong&gt; Our new open-source project, &lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt;, is now open-sourced! Focused on stable model deployment. Geared towards industry. Offers better engineering support, higher computational performance, and more stable functionality.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 31, 2025&lt;/strong&gt; We support InfiniteYou, an identity preserving method for FLUX. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/InfiniteYou/"&gt;./examples/InfiniteYou/&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 13, 2025&lt;/strong&gt; We support HunyuanVideo-I2V, the image-to-video generation version of HunyuanVideo open-sourced by Tencent. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/"&gt;./examples/HunyuanVideo/&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 25, 2025&lt;/strong&gt; We support Wan-Video, a collection of SOTA video synthesis models open-sourced by Alibaba. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 17, 2025&lt;/strong&gt; We support &lt;a href="https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary"&gt;StepVideo&lt;/a&gt;! State-of-the-art video synthesis model! See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/stepvideo/"&gt;./examples/stepvideo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 31, 2024&lt;/strong&gt; We propose EliGen, a novel framework for precise entity-level controlled text-to-image generation, complemented by an inpainting fusion pipeline to extend its capabilities to image inpainting tasks. EliGen seamlessly integrates with existing community models, such as IP-Adapter and In-Context LoRA, enhancing its versatility. For more details, see &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/EntityControl/"&gt;./examples/EntityControl&lt;/a&gt;.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.01097"&gt;EliGen: Entity-Level Controlled Image Generation with Regional Attention&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Eligen"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/EliGen"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Online Demo: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen"&gt;ModelScope EliGen Studio&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Training Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet"&gt;EliGen Train Set&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 19, 2024&lt;/strong&gt; We implement advanced VRAM management for HunyuanVideo, making it possible to generate videos at a resolution of 129x720x1280 using 24GB of VRAM, or at 129x512x384 resolution with just 6GB of VRAM. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/"&gt;./examples/HunyuanVideo/&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 18, 2024&lt;/strong&gt; We propose ArtAug, an approach designed to improve text-to-image synthesis models through synthesis-understanding interactions. We have trained an ArtAug enhancement module for FLUX.1-dev in the format of LoRA. This model integrates the aesthetic understanding of Qwen2-VL-72B into FLUX.1-dev, leading to an improvement in the quality of generated images.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2412.12888"&gt;https://arxiv.org/abs/2412.12888&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Examples: &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug"&gt;https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Demo: &lt;a href="https://modelscope.cn/aigc/imageGeneration?tab=advanced&amp;amp;versionId=7228&amp;amp;modelType=LoRA&amp;amp;sdVersion=FLUX_1&amp;amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0"&gt;ModelScope&lt;/a&gt;, HuggingFace (Coming soon)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 25, 2024&lt;/strong&gt; We provide extensive FLUX ControlNet support. This project supports many different ControlNet models that can be freely combined, even if their structures differ. Additionally, ControlNet models are compatible with high-resolution refinement and partition control techniques, enabling very powerful controllable image generation. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ControlNet/"&gt;&lt;code&gt;./examples/ControlNet/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 8, 2024.&lt;/strong&gt; We release the extended LoRA based on CogVideoX-5B and ExVideo. You can download this model from &lt;a href="https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1"&gt;ModelScope&lt;/a&gt; or &lt;a href="https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1"&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024.&lt;/strong&gt; CogVideoX-5B is supported in this project. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/video_synthesis/"&gt;here&lt;/a&gt;. We provide several interesting features for this text-to-video model, including&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Text to video&lt;/li&gt; 
    &lt;li&gt;Video editing&lt;/li&gt; 
    &lt;li&gt;Self-upscaling&lt;/li&gt; 
    &lt;li&gt;Video interpolation&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024.&lt;/strong&gt; We have implemented an interesting painter that supports all text-to-image models. Now you can create stunning images using the painter, with assistance from AI!&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Use it in our &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#usage-in-webui"&gt;WebUI&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2024.&lt;/strong&gt; FLUX is supported in DiffSynth-Studio.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Enable CFG and highres-fix to improve visual quality. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/README.md"&gt;here&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;LoRA, ControlNet, and additional models will be available soon.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 21, 2024.&lt;/strong&gt; We propose ExVideo, a post-tuning technique aimed at enhancing the capability of video generation models. We have extended Stable Video Diffusion to achieve the generation of long videos up to 128 frames.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://ecnu-cilab.github.io/ExVideoProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Source code is released in this repo. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/"&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Models are released on &lt;a href="https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Technical report is released on &lt;a href="https://arxiv.org/abs/2406.14130"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;You can try ExVideo in this &lt;a href="https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1"&gt;Demo&lt;/a&gt;!&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 13, 2024.&lt;/strong&gt; DiffSynth Studio is transferred to ModelScope. The developers have transitioned from "I" to "we". Of course, I will still participate in development and maintenance.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Jan 29, 2024.&lt;/strong&gt; We propose Diffutoon, a fantastic solution for toon shading.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;The source codes are released in this project.&lt;/li&gt; 
    &lt;li&gt;The technical report (IJCAI 2024) is released on &lt;a href="https://arxiv.org/abs/2401.16224"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dec 8, 2023.&lt;/strong&gt; We decide to develop a new Project, aiming to release the potential of diffusion models, especially in video synthesis. The development of this project is started.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nov 15, 2023.&lt;/strong&gt; We propose FastBlend, a powerful video deflickering algorithm.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;The sd-webui extension is released on &lt;a href="https://github.com/Artiprocher/sd-webui-fastblend"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Demo videos are shown on Bilibili, including three tasks. 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1d94y1W7PE"&gt;Video deflickering&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1Lw411m71p"&gt;Video interpolation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1RB4y1Z7LF"&gt;Image-driven video rendering&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;The technical report is released on &lt;a href="https://arxiv.org/abs/2311.09265"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;An unofficial ComfyUI extension developed by other users is released on &lt;a href="https://github.com/AInseven/ComfyUI-fastblend"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Oct 1, 2023.&lt;/strong&gt; We release an early version of this project, namely FastSDXL. A try for building a diffusion engine.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;The source codes are released on &lt;a href="https://github.com/Artiprocher/FastSDXL"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;FastSDXL includes a trainable OLSS scheduler for efficiency improvement. 
     &lt;ul&gt; 
      &lt;li&gt;The original repo of OLSS is &lt;a href="https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler"&gt;here&lt;/a&gt;.&lt;/li&gt; 
      &lt;li&gt;The technical report (CIKM 2023) is released on &lt;a href="https://arxiv.org/abs/2305.14677"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
      &lt;li&gt;A demo video is shown on &lt;a href="https://www.bilibili.com/video/BV1w8411y7uj"&gt;Bilibili&lt;/a&gt;.&lt;/li&gt; 
      &lt;li&gt;Since OLSS requires additional training, we don't implement it in this project.&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Aug 29, 2023.&lt;/strong&gt; We propose DiffSynth, a video synthesis framework.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://ecnu-cilab.github.io/DiffSynth.github.io/"&gt;Project Page&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;The source codes are released in &lt;a href="https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth"&gt;EasyNLP&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;The technical report (ECML PKDD 2024) is released on &lt;a href="https://arxiv.org/abs/2308.03463"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>LMCache/LMCache</title>
      <link>https://github.com/LMCache/LMCache</link>
      <description>&lt;p&gt;Supercharge Your LLM with the Fastest KV Cache Layer&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/LMCache/LMCache/dev/asset/logo.png" width="720" alt="lmcache logo" /&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.lmcache.ai/"&gt;&lt;img src="https://img.shields.io/badge/docs-live-brightgreen" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/v/lmcache" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/lmcache" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://buildkite.com/lmcache/lmcache-unittests"&gt;&lt;img src="https://badge.buildkite.com/ce25f1819a274b7966273bfa54f0e02f092c3de0d7563c5c9d.svg?sanitize=true" alt="Unit Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LMCache/LMCache/actions/workflows/code_quality_checks.yml"&gt;&lt;img src="https://github.com/lmcache/lmcache/actions/workflows/code_quality_checks.yml/badge.svg?branch=dev&amp;amp;label=tests" alt="Code Quality" /&gt;&lt;/a&gt; &lt;a href="https://buildkite.com/lmcache/lmcache-vllm-integration-tests"&gt;&lt;img src="https://badge.buildkite.com/108ddd4ab482a2480999dec8c62a640a3315ed4e6c4e86798e.svg?sanitize=true" alt="Integration Tests" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://www.bestpractices.dev/projects/10841"&gt;&lt;img src="https://www.bestpractices.dev/projects/10841/badge" alt="OpenSSF Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://scorecard.dev/viewer/?uri=github.com/LMCache/LMCache"&gt;&lt;img src="https://api.scorecard.dev/projects/github.com/LMCache/LMCache/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/LMCache/LMCache/"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LMCache/LMCache/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/w/LMCache/LMCache" alt="GitHub commit activity" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/lmcache/"&gt;&lt;img src="https://img.shields.io/pypi/dm/lmcache" alt="PyPI - Downloads" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA"&gt;&lt;img src="https://img.shields.io/youtube/channel/views/UC58zMz55n70rtf1Ak2PULJA" alt="YouTube Channel Views" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;| &lt;a href="https://blog.lmcache.ai/"&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://docs.lmcache.ai/"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-36x1m765z-8FgDA_73vcXtlZ_4XvpE6Q"&gt;&lt;strong&gt;Join Slack&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://forms.gle/MHwLiYDU6kcW3dLj7"&gt;&lt;strong&gt;Interest Form&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/LMCache/LMCache/issues/1253"&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ğŸ”¥ &lt;strong&gt;NEW: For enterprise-scale deployment of LMCache and vLLM, please check out vLLM &lt;a href="https://github.com/vllm-project/production-stack"&gt;Production Stack&lt;/a&gt;. LMCache is also officially supported in &lt;a href="https://github.com/llm-d/llm-d/"&gt;llm-d&lt;/a&gt; and &lt;a href="https://github.com/kserve/kserve"&gt;KServe&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;LMCache is an &lt;strong&gt;LLM&lt;/strong&gt; serving engine extension to &lt;strong&gt;reduce TTFT&lt;/strong&gt; and &lt;strong&gt;increase throughput&lt;/strong&gt;, especially under long-context scenarios. By storing the KV caches of reusable texts across various locations, including (GPU, CPU DRAM, Local Disk), LMCache reuses the KV caches of &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; reused text (not necessarily prefix) in &lt;strong&gt;&lt;em&gt;any&lt;/em&gt;&lt;/strong&gt; serving engine instance. Thus, LMCache saves precious GPU cycles and reduces user response delay.&lt;/p&gt; 
&lt;p&gt;By combining LMCache with vLLM, developers achieve 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/86137f17-f216-41a0-96a7-e537764f7a4c" alt="performance" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ğŸ”¥ Integration with vLLM v1 with the following features: 
  &lt;ul&gt; 
   &lt;li&gt;High performance CPU KVCache offloading&lt;/li&gt; 
   &lt;li&gt;Disaggregated prefill&lt;/li&gt; 
   &lt;li&gt;P2P KVCache sharing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; LMCache is supported in the &lt;a href="https://github.com/vllm-project/production-stack/"&gt;vLLM production stack&lt;/a&gt;, &lt;a href="https://github.com/llm-d/llm-d/"&gt;llm-d&lt;/a&gt;, and &lt;a href="https://github.com/kserve/kserve"&gt;KServe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Stable support for non-prefix KV caches&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Storage support as follows: 
  &lt;ul&gt; 
   &lt;li&gt;CPU&lt;/li&gt; 
   &lt;li&gt;Disk&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/ai-dynamo/nixl"&gt;NIXL&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Installation support through pip and latest vLLM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To use LMCache, simply install &lt;code&gt;lmcache&lt;/code&gt; from your package manager, e.g. pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install lmcache
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Works on Linux NVIDIA GPU platform.&lt;/p&gt; 
&lt;p&gt;More &lt;a href="https://docs.lmcache.ai/getting_started/installation"&gt;detailed installation instructions&lt;/a&gt; are available in the docs, particularly if you are not using the latest stable version of vllm or using another serving engine with different dependencies. Any "undefined symbol" or torch mismatch versions can be resolved in the documentation.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The best way to get started is to checkout the &lt;a href="https://docs.lmcache.ai/getting_started/quickstart/"&gt;Quickstart Examples&lt;/a&gt; in the docs.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Check out the LMCache &lt;a href="https://docs.lmcache.ai/"&gt;documentation&lt;/a&gt; which is available online.&lt;/p&gt; 
&lt;p&gt;We also post regularly in &lt;a href="https://blog.lmcache.ai/"&gt;LMCache blogs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Go hands-on with our &lt;a href="https://github.com/LMCache/LMCache/tree/dev/examples"&gt;examples&lt;/a&gt;, demonstrating how to address different use cases with LMCache.&lt;/p&gt; 
&lt;h2&gt;Interested in Connecting?&lt;/h2&gt; 
&lt;p&gt;Fill out the &lt;a href="https://forms.gle/mQfQDUXbKfp2St1z7"&gt;interest form&lt;/a&gt;, &lt;a href="https://mailchi.mp/tensormesh/lmcache-sign-up-newsletter"&gt;sign up for our newsletter&lt;/a&gt;, &lt;a href="https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ"&gt;join LMCache slack&lt;/a&gt;, &lt;a href="https://lmcache.ai/"&gt;check out LMCache website&lt;/a&gt;, or &lt;a href="mailto:contact@lmcache.ai"&gt;drop an email&lt;/a&gt;, and our team will reach out to you!&lt;/p&gt; 
&lt;h2&gt;Community meeting&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://uchicago.zoom.us/j/6603596916?pwd=Z1E5MDRWUSt2am5XbEt4dTFkNGx6QT09"&gt;community meeting&lt;/a&gt; for LMCache is hosted bi-weekly. All are welcome to join!&lt;/p&gt; 
&lt;p&gt;Meetings are held bi-weekly on: Tuesdays at 9:00 AM PT â€“ &lt;a href="https://drive.usercontent.google.com/u/0/uc?id=1f5EXbooGcwNwzIpTgn5u4PHqXgfypMtu&amp;amp;export=download"&gt;Add to Calendar&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We keep notes from each meeting on this &lt;a href="https://docs.google.com/document/d/1_Fl3vLtERFa3vTH00cezri78NihNBtSClK-_1tSrcow"&gt;document&lt;/a&gt; for summaries of standups, discussion, and action items.&lt;/p&gt; 
&lt;p&gt;Recordings of meetings are available on the &lt;a href="https://www.youtube.com/channel/UC58zMz55n70rtf1Ak2PULJA"&gt;YouTube LMCache channel&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value all contributions and collaborations. Please check out &lt;a href="https://raw.githubusercontent.com/LMCache/LMCache/dev/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; on how to contribute.&lt;/p&gt; 
&lt;p&gt;We continually update &lt;a href="https://github.com/LMCache/LMCache/issues/627"&gt;[Onboarding] Welcoming contributors with good first issues!&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use LMCache for your research, please cite our papers:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{liu2024cachegen,
  title={Cachegen: Kv cache compression and streaming for fast large language model serving},
  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},
  pages={38--56},
  year={2024}
}

@article{cheng2024large,
  title={Do Large Language Models Need a Content Delivery Network?},
  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},
  journal={arXiv preprint arXiv:2409.13761},
  year={2024}
}

@inproceedings{10.1145/3689031.3696098,
  author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  title = {CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion},
  year = {2025},
  url = {https://doi.org/10.1145/3689031.3696098},
  doi = {10.1145/3689031.3696098},
  booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},
  pages = {94â€“109},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Socials&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.linkedin.com/company/lmcache-lab/?viewAsMember=true"&gt;Linkedin&lt;/a&gt; | &lt;a href="https://x.com/lmcache"&gt;Twitter&lt;/a&gt; | &lt;a href="https://www.youtube.com/@LMCacheTeam"&gt;Youtube&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The LMCache codebase is licensed under Apache License 2.0. See the &lt;a href="https://raw.githubusercontent.com/LMCache/LMCache/dev/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Arindam200/awesome-ai-apps</title>
      <link>https://github.com/Arindam200/awesome-ai-apps</link>
      <description>&lt;p&gt;A collection of projects showcasing RAG, agents, workflows, and other AI use cases&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome AI Apps &lt;a href="https://awesome.re"&gt;&lt;img src="https://awesome.re/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/assets/banner.png" alt="Banner" /&gt;&lt;/p&gt; 
&lt;p&gt;This repository is a comprehensive collection of practical examples, tutorials, and recipes for building powerful LLM-powered applications. From simple chatbots to advanced AI agents, these projects serve as a guide for developers working with various AI frameworks and tools.&lt;/p&gt; 
&lt;p&gt;Powered by &lt;a href="https://dub.sh/nebius"&gt;Nebius AI Studio&lt;/a&gt; - your one-stop platform for building and deploying AI applications.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Featured AI Agent Frameworks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://google.github.io/adk-docs/"&gt;&lt;img src="https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png" alt="Google ADK logo" width="20" height="20" /&gt; Google Agent Development Kit (ADK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/14957082?s=200&amp;amp;v=4" alt="OpenAI Agents SDK logo" width="20" height="20" /&gt; OpenAI Agents SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://python.langchain.com/"&gt;&lt;img src="https://cdn.simpleicons.org/langchain" alt="LangChain logo" width="25" height="25" /&gt; LangChain &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.llamaindex.ai/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/130722866?s=200&amp;amp;v=4" alt="Llamaindex logo" width="20" height="20" /&gt; LlamaIndex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.agno.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/104874993?s=48&amp;amp;v=4" alt="Agno logo" width="20" height="20" /&gt; Agno&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.crewai.com/"&gt;&lt;img src="https://cdn.prod.website-files.com/66cf2bfc3ed15b02da0ca770/66d07240057721394308addd_Logo%20(1).svg?sanitize=true" alt="CrewAI logo" width="35" height="25" /&gt; CrewAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/209155962?s=200&amp;amp;v=4" alt="AWS Strands Agents logo" width="20" height="20" /&gt; AWS Strands Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ai.pydantic.dev/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/110818415?s=200&amp;amp;v=4" alt="Pydantic AI logo" width="20" height="20" /&gt; Pydantic AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.camel-ai.org/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/134388954?s=200&amp;amp;v=4" alt="Camel AI logo" width="20" height="20" /&gt; CAMELâ€‘AI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ§© Starter Agents&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Quick-start agents for learning and extending:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/agno_starter"&gt;Agno HackerNews Analysis&lt;/a&gt; - Agno-based agent for trend analysis on HackerNews.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/openai_agents_sdk"&gt;OpenAI SDK Starter&lt;/a&gt; - OpenAI Agents SDK based email helper &amp;amp; haiku writer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/llamaindex_starter"&gt;LlamaIndex Task Manager&lt;/a&gt; - LlamaIndex-powered task assistant.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/crewai_starter"&gt;CrewAI Research Crew&lt;/a&gt; - Multi-agent research team.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/pydantic_starter"&gt;PydanticAI Weather Bot&lt;/a&gt; - Real-time weather info.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/langchain_langgraph_starter"&gt;LangChain-LangGraph Starter&lt;/a&gt; - LangChain + LangGraph starter.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/aws_strands_starter"&gt;AWS Strands Agent Starter&lt;/a&gt; - Weather report Agent.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/camel_ai_starter"&gt;Camel AI Starter&lt;/a&gt; - Performance benchmarking tool that compares the performance of various AI models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸª¶ Simple Agents&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Straightforward, practical use-cases:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/finance_agent"&gt;Finance Agent&lt;/a&gt; - Tracks live stock &amp;amp; market data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/human_in_the_loop_agent"&gt;Human-in-the-Loop Agent&lt;/a&gt; - HITL actions for safe AI tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/newsletter_agent"&gt;Newsletter Generator&lt;/a&gt; - AI newsletter builder with Firecrawl.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/reasoning_agent"&gt;Reasoning Agent&lt;/a&gt; - Financial reasoning step-by-step.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/agno_ui_agent"&gt;Agno UI Example&lt;/a&gt; - UI for web &amp;amp; finance agents.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/mastra_ai_weather_agent"&gt;Mastra Weather Bot&lt;/a&gt; - Weather updates with Mastra AI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/cal_scheduling_agent"&gt;Calendar Assistant&lt;/a&gt; - Calendar scheduling with Cal.com.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/memory_agent"&gt;Memory Agent&lt;/a&gt; - Simple Memory Agent implementation with Agno.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/browser_agent"&gt;Web Automation Agent&lt;/a&gt; - Simple Browser Agent implementation with Nebius &amp;amp; browser use.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/nebius_chat"&gt;Nebius Chat&lt;/a&gt; - Nebius AI Studio Chat interface.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/talk_to_db"&gt;Talk to Your DB&lt;/a&gt; - Talk to your Database with GibsonAI &amp;amp; Langchain&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ—‚ï¸ MCP Agents&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Examples using Model Context Protocol:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/doc_mcp"&gt;Doc-MCP&lt;/a&gt; - Semantic RAG docs &amp;amp; Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/langchain_langgraph_mcp_agent"&gt;LangGraph MCP Agent&lt;/a&gt; - LangChain ReAct agent with Couchbase.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/github_mcp_agent"&gt;GitHub MCP Agent&lt;/a&gt; - Repo insights via MCP.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/mcp_starter"&gt;MCP Starter&lt;/a&gt; - GitHub repo analyzer starter.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/docs_qna_agent"&gt;Talk to your Docs&lt;/a&gt; - Documentation QnA Agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“š RAG Applications&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Retrieve-augmented generation examples:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/agentic_rag"&gt;Agentic RAG&lt;/a&gt; - Agentic RAG with Agno &amp;amp; GPT 5.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/resume_optimizer"&gt;Resume Optimizer&lt;/a&gt; - Boost resumes with AI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/llamaIndex_starter"&gt;LlamaIndex RAG Starter&lt;/a&gt; - LlamaIndex + Nebius RAG starter.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/pdf_rag_analyser"&gt;PDF RAG Analyzer&lt;/a&gt; - Chat with multiple PDFs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/qwen3_rag"&gt;Qwen3 RAG Chat&lt;/a&gt; - PDF chatbot with Streamlit.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/chat_with_code"&gt;Chat with Code&lt;/a&gt; - Conversational code explorer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/gemma_ocr/"&gt;Gemma3 OCR&lt;/a&gt; - OCR-based document and image processor using Gemma3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”¬ Advanced Agents&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Complex pipelines for end-to-end workflows:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/deep_researcher_agent"&gt;Deep Researcher&lt;/a&gt; - Multi-stage research with Agno &amp;amp; Scrapegraph AI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/candidate_analyser"&gt;Candilyzer&lt;/a&gt; - Analyze GitHub/LinkedIn profiles.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/job_finder_agent"&gt;Job Finder&lt;/a&gt; - LinkedIn job search with Bright Data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/trend_analyzer_agent"&gt;AI Trend Analyzer&lt;/a&gt; - AI trend mining with Google ADK.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/conference_talk_abstract_generator"&gt;Conference Talk Generator&lt;/a&gt; - Draft talk abstracts with Google ADK &amp;amp; Couchbase.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/finance_service_agent"&gt;Finance Service Agent&lt;/a&gt; - FastAPI server for stock data and predictions with Agno.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/price_monitoring_agent"&gt;Price Monitoring Agent&lt;/a&gt; - Price monitoring and alerting Agent powered by CrewAi, Twilio &amp;amp; Nebius.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/startup_idea_validator_agent"&gt;Startup Idea Validator Agent&lt;/a&gt; - Agentic Workflow to validate and analyze startup ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- * [YouTube to Blog](advance_ai_agents/youtube_to_blog_agent) - Auto-blog from YouTube videos. --&gt; 
&lt;h2&gt;ğŸ“º Playlist of Demo Videos &amp;amp; Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLMZM1DAlf0Lolxax4L2HS54Me8gn1gkz4"&gt;Build with MCP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLMZM1DAlf0LqixhAG9BDk4O_FjqnaogK8"&gt;Build AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PL2ambAOfYA6-LDz0KpVKu9vJKAqhv0KKI"&gt;AI Agents, MCP and more...&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or higher&lt;/li&gt; 
 &lt;li&gt;Git&lt;/li&gt; 
 &lt;li&gt;pip (Python package manager) or uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation Steps&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Arindam200/awesome-ai-apps.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-ai-apps/starter_ai_agents/agno_starter
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow project-specific instructions&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Each project has its own README.md with detailed setup and usage instructions&lt;/li&gt; 
   &lt;li&gt;Make sure to read the project-specific documentation before running the application&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're a beginner or an expert, your examples and tutorials can help others learn and grow. Here's how you can contribute:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Submit a Pull Request with your LLM application example&lt;/li&gt; 
 &lt;li&gt;Add detailed documentation and setup instructions&lt;/li&gt; 
 &lt;li&gt;Include requirements.txt or environment.yml&lt;/li&gt; 
 &lt;li&gt;Share your experience and best practices&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“œ License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/LICENSE"&gt;MIT License&lt;/a&gt;. Feel free to use and modify the examples for your projects.&lt;/p&gt; 
&lt;h2&gt;Thank You for the Support! ğŸ™&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#Arindam200/awesome-ai-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Arindam200/awesome-ai-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>emcie-co/parlant</title>
      <link>https://github.com/emcie-co/parlant</link>
      <description>&lt;p&gt;LLM agents built for control. Designed for real-world use. Deployed in minutes.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true" /&gt; 
  &lt;img alt="Parlant - AI Agent Framework" src="https://github.com/emcie-co/parlant/raw/develop/docs/LogoTransparentDark.png?raw=true" width="400" /&gt; 
 &lt;/picture&gt; 
 &lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt; 
 &lt;p&gt; &lt;a href="https://www.parlant.io/" target="_blank"&gt;ğŸŒ Website&lt;/a&gt; â€¢ &lt;a href="https://www.parlant.io/docs/quickstart/installation" target="_blank"&gt;âš¡ Quick Start&lt;/a&gt; â€¢ &lt;a href="https://discord.gg/duxWqxKk6J" target="_blank"&gt;ğŸ’¬ Discord&lt;/a&gt; â€¢ &lt;a href="https://www.parlant.io/docs/quickstart/examples" target="_blank"&gt;ğŸ“– Examples&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://pypi.org/project/parlant/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/parlant?color=blue" /&gt;&lt;/a&gt; &lt;img alt="Python 3.10+" src="https://img.shields.io/badge/python-3.10+-blue" /&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img alt="License" src="https://img.shields.io/badge/license-Apache%202.0-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/duxWqxKk6J"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1312378700993663007?color=7289da&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/emcie-co/parlant?style=social" /&gt; &lt;/p&gt; 
 &lt;a href="https://trendshift.io/repositories/12768" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/12768" alt="Trending on TrendShift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ¯ The Problem Every AI Developer Faces&lt;/h2&gt; 
&lt;p&gt;You build an AI agent. It works great in testing. Then real users start talking to it and...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âŒ It ignores your carefully crafted system prompts&lt;/li&gt; 
 &lt;li&gt;âŒ It hallucinates responses in critical moments&lt;/li&gt; 
 &lt;li&gt;âŒ It can't handle edge cases consistently&lt;/li&gt; 
 &lt;li&gt;âŒ Each conversation feels like a roll of the dice&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Sound familiar?&lt;/strong&gt; You're not alone. This is the #1 pain point for developers building production AI agents.&lt;/p&gt; 
&lt;h2&gt;âš¡ The Solution: Stop Fighting Prompts, Teach Principles&lt;/h2&gt; 
&lt;p&gt;Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, &lt;strong&gt;Parlant ensures it&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Traditional approach: Cross your fingers ğŸ¤
system_prompt = "You are a helpful assistant. Please follow these 47 rules..."

# Parlant approach: Ensured compliance âœ…
await agent.create_guideline(
    condition="Customer asks about refunds",
    action="Check order status first to see if eligible",
    tools=[check_order_status],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/journeys"&gt;Journeys&lt;/a&gt;&lt;/strong&gt;: Define clear customer journeys and how your agent should respond at each step.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/guidelines"&gt;Behavioral Guidelines&lt;/a&gt;&lt;/strong&gt;: Easily craft agent behavior; Parlant will match the relevant elements contextually.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/tools"&gt;Tool Use&lt;/a&gt;&lt;/strong&gt;: Attach external APIs, data fetchers, or backend services to specific interaction events.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/glossary"&gt;Domain Adaptation&lt;/a&gt;&lt;/strong&gt;: Teach your agent domain-specific terminology and craft personalized responses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/canned-responses"&gt;Canned Responses&lt;/a&gt;&lt;/strong&gt;: Use response templates to eliminate hallucinations and guarantee style consistency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/advanced/explainability"&gt;Explainability&lt;/a&gt;&lt;/strong&gt;: Understand why and when each guideline was matched and followed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;ğŸš€ Get Your Agent Running in 60 Seconds&lt;/h2&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install parlant
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&amp;gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f"Sunny, 72Â°F in {city}")

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name="WeatherBot",
            description="Helpful weather assistant"
        )

        # Define behavior with natural language
        await agent.create_guideline(
            condition="User asks about weather",
            action="Get current weather and provide a friendly response with suggestions",
            tools=[get_weather]
        )

        # ğŸ‰ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; Your agent is running with ensured rule-following behavior.&lt;/p&gt; 
&lt;h2&gt;ğŸ¬ See It In Action&lt;/h2&gt; 
&lt;img alt="Parlant Demo" src="https://github.com/emcie-co/parlant/raw/develop/docs/demo.gif?raw=true" width="100%" /&gt; 
&lt;h2&gt;ğŸ”¥ Why Developers Are Switching to Parlant&lt;/h2&gt; 
&lt;table width="100%"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ—ï¸ &lt;strong&gt;Traditional AI Frameworks&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;âš¡ &lt;strong&gt;Parlant&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Write complex system prompts&lt;/li&gt; 
     &lt;li&gt;Hope the LLM follows them&lt;/li&gt; 
     &lt;li&gt;Debug unpredictable behaviors&lt;/li&gt; 
     &lt;li&gt;Scale by prompt engineering&lt;/li&gt; 
     &lt;li&gt;Cross fingers for reliability&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Define rules in natural language&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ensured&lt;/strong&gt; rule compliance&lt;/li&gt; 
     &lt;li&gt;Predictable, consistent behavior&lt;/li&gt; 
     &lt;li&gt;Scale by adding guidelines&lt;/li&gt; 
     &lt;li&gt;Production-ready from day one&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ¯ Perfect For Your Use Case&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Financial Services&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;E-commerce&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Legal Tech&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Compliance-first design&lt;/td&gt; 
    &lt;td align="center"&gt;HIPAA-ready agents&lt;/td&gt; 
    &lt;td align="center"&gt;Customer service at scale&lt;/td&gt; 
    &lt;td align="center"&gt;Precise legal guidance&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Built-in risk management&lt;/td&gt; 
    &lt;td align="center"&gt;Patient data protection&lt;/td&gt; 
    &lt;td align="center"&gt;Order processing automation&lt;/td&gt; 
    &lt;td align="center"&gt;Document review assistance&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ› ï¸ Enterprise-Grade Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§­ Conversational Journeys&lt;/strong&gt; - Lead the customer step-by-step to a goal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¯ Dynamic Guideline Matching&lt;/strong&gt; - Context-aware rule application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ Reliable Tool Integration&lt;/strong&gt; - APIs, databases, external services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“Š Conversation Analytics&lt;/strong&gt; - Deep insights into agent behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”„ Iterative Refinement&lt;/strong&gt; - Continuously improve agent responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ›¡ï¸ Built-in Guardrails&lt;/strong&gt; - Prevent hallucination and off-topic responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“± React Widget&lt;/strong&gt; - &lt;a href="https://github.com/emcie-co/parlant-chat-react"&gt;Drop-in chat UI for any web app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Full Explainability&lt;/strong&gt; - Understand every decision your agent makes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ˆ Join 7,000+ Developers Building Better AI&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Companies using Parlant:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Financial institutions â€¢ Healthcare providers â€¢ Legal firms â€¢ E-commerce platforms&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#emcie-co/parlant&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=emcie-co/parlant&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ What Developers Are Saying&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;"By far the most elegant conversational AI framework that I've come across! Developing with Parlant is pure joy."&lt;/em&gt; &lt;strong&gt;â€” Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸƒâ€â™‚ï¸ Quick Start Paths&lt;/h2&gt; 
&lt;table border="0"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ¯ I want to test it myself&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/installation"&gt;â†’ 5-minute quickstart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ› ï¸ I want to see an example&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/examples"&gt;â†’ Healthcare agent example&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸš€ I want to get involved&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;â†’ Join our Discord community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ¤ Community &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Discord Community&lt;/a&gt;&lt;/strong&gt; - Get help from the team and community&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;&lt;a href="https://parlant.io/docs/quickstart/installation"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Comprehensive guides and examples&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;&lt;a href="https://github.com/emcie-co/parlant/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;ğŸ“§ &lt;strong&gt;&lt;a href="https://parlant.io/contact"&gt;Direct Support&lt;/a&gt;&lt;/strong&gt; - Direct line to our engineering team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 - Use it anywhere, including commercial projects.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Ready to build AI agents that actually work?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;â­ &lt;strong&gt;Star this repo&lt;/strong&gt; â€¢ ğŸš€ &lt;strong&gt;&lt;a href="https://parlant.io/"&gt;Try Parlant now&lt;/a&gt;&lt;/strong&gt; â€¢ ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Join Discord&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Built with â¤ï¸ by the team at &lt;a href="https://emcie.co"&gt;Emcie&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;"DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;" /&gt; &lt;/td&gt; 
    &lt;td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;"&gt; &lt;pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;"&gt;    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•      â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;!-- &lt;img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/&gt; --&gt; 
 &lt;h1&gt;&lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;" /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align="center"&gt;
  &lt;img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white" alt="Version"&gt;

  &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white" alt="License"&gt;
  &lt;img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white" alt="AI"&gt;
  &lt;img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white" alt="HKU"&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/HKUDS/DeepCode/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/ğŸPython-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/deepcode-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/DeepCode/issues/11"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;ğŸ–¥ï¸ &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;ğŸ–¥ï¸ &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;ğŸš€ Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;âš¡ Fast command-line workflow&lt;br /&gt;ğŸ”§ Developer-friendly interface&lt;br /&gt;ğŸ“Š Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;ğŸŒ &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;ğŸ¨ Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;ğŸ–±ï¸ Intuitive drag-and-drop&lt;br /&gt;ğŸ“± Responsive design&lt;br /&gt;ğŸ¯ Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h3&gt;ğŸ¬ &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style="margin: 20px 0;"&gt; 
   &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;" /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;ğŸ¯ &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/â–¶ï¸_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white" alt="Watch Video" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;"Where AI Agents Transform Ideas into Production-Ready Code"&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“‘ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features"&gt;ğŸš€ Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture"&gt;ğŸ—ï¸ Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;ğŸš€ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples"&gt;ğŸ’¡ Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations"&gt;ğŸ¬ Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history"&gt;â­ Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license"&gt;ğŸ“„ License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;ğŸš€ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;ğŸ¨ &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;âš™ï¸ &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;ğŸ¯ &lt;strong&gt;Autonomous Multi-Agent Workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ“„ &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”¬ &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;â±ï¸ &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”„ &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    A["ğŸ“„ Research Papers&amp;lt;br/&amp;gt;ğŸ’¬ Text Prompts&amp;lt;br/&amp;gt;ğŸŒ URLs &amp;amp; Document&amp;lt;br/&amp;gt;ğŸ“ Files: PDF, DOC, PPTX, TXT, HTML"] --&amp;gt; B["ğŸ§  DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine"]
    B --&amp;gt; C["ğŸš€ Algorithm Implementation &amp;lt;br/&amp;gt;ğŸ¨ Frontend Development &amp;lt;br/&amp;gt;âš™ï¸ Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ—ï¸ Architecture&lt;/h2&gt; 
&lt;h3&gt;ğŸ“Š &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;ğŸ¯ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;ğŸ§¬ &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;ğŸª„ &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;âš¡ &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;ğŸ’ &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;ğŸ”® &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ”§ &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ§  &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ’¾ &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ” &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ¤– &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ¯ Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“ Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“„ Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ—ï¸ Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“š Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ§¬ Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;ğŸ› ï¸ &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ”§ Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;ğŸ“¡ &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ğŸ› ï¸ &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;ğŸ”§ &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;ğŸ’¡ &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ” brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸŒ bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“‚ filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸŒ fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“¥ github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“‹ file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âš¡ command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ§¬ code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“š code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“„ document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;ğŸ”§ &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ğŸ› ï¸ &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;ğŸ¯ &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“„ read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âœï¸ write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“ get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âš™ï¸ set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“Š get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;ğŸ›ï¸ &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸš€ Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ğŸŒŸ &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; ğŸ’¡ &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; ğŸ“„ Research Papers â€¢ ğŸ’¬ Natural Language â€¢ ğŸŒ URLs â€¢ ğŸ“‹ Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ğŸ¯ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making â€¢ Workflow Coordination â€¢ Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ“ &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ“„ &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ğŸ“‹ &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis â€¢ Code Requirements Parsing â€¢ Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ” &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ“š &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ğŸ§¬ &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation â€¢ Testing â€¢ Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; âš¡ &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; ğŸ“¦ Complete Codebase â€¢ ğŸ§ª Test Suite â€¢ ğŸ“š Documentation â€¢ ğŸš€ Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;ğŸ”„ &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" style="border: none;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;"&gt; 
      &lt;h4&gt;ğŸ¯ Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;"&gt; 
      &lt;h4&gt;ğŸ§  Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;"&gt; 
      &lt;h4&gt;ğŸ” Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;"&gt; 
      &lt;h4&gt;âš¡ Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;h3&gt;ğŸ“¦ &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;âš¡ &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸš€ Install DeepCode package directly
pip install deepcode-hku

# ğŸ”‘ Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# ğŸ”‘ Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# ğŸ”‘ Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# ğŸ“„ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ğŸ”§ &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‚ Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;ğŸ”¥ &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸ”½ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# ğŸ“¦ Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# ğŸ”§ Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# ğŸ”‘ Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# ğŸ”‘ Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# ğŸ“„ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;ğŸ &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸ”½ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# ğŸ“¦ Install dependencies
pip install -r requirements.txt

# ğŸ”‘ Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# ğŸ”‘ Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# ğŸ“„ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;ğŸªŸ &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you're using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;ğŸ” &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Brave Search&lt;/strong&gt; (&lt;code&gt;"brave"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸŒ Bocha-MCP&lt;/strong&gt; (&lt;code&gt;"bocha-mcp"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;âš¡ &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;ğŸš€ &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸŒ Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ğŸ› ï¸ &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;ğŸŒ &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Web Access" /&gt; 
&lt;/div&gt; 
&lt;h5&gt;ğŸ–¥ï¸ &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white" alt="CLI Mode" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;ğŸ¯ &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“„ Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;âš¡ Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ’¡ Examples&lt;/h2&gt; 
&lt;h3&gt;ğŸ¬ &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;ğŸ“„ &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt; &lt;img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt;â–¶ï¸ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;ğŸ–¼ï¸ &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt; &lt;img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt;â–¶ï¸ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;ğŸŒ &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt; &lt;img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt;â–¶ï¸ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;ğŸ†• &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;ğŸ“„ &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We're continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;ğŸ”§ &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ğŸ“Š &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;âš¡ &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â­ Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href="https://star-history.com/#HKUDS/DeepCode&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸš€ &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;&lt;img src="https://img.shields.io/badge/ğŸš€_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Get Started" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS"&gt;&lt;img src="https://img.shields.io/badge/ğŸ›ï¸_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="View on GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/deepcode-agent"&gt;&lt;img src="https://img.shields.io/badge/â­_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white" alt="Star Project" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;ğŸ“„ &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; 
 &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff" alt="Visitors" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>wwwzhouhui/dify-for-dsl</title>
      <link>https://github.com/wwwzhouhui/dify-for-dsl</link>
      <description>&lt;p&gt;æœ¬é¡¹ç›®æ˜¯åŸºäºdifyå¼€æºé¡¹ç›®å®ç°çš„dslå·¥ä½œæµè„šæœ¬åˆé›†&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;dify-for-dsl&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®æ˜¯åŸºäºdifyå¼€æºé¡¹ç›®å®ç°çš„dslå·¥ä½œæµè„šæœ¬åˆé›†ã€‚&lt;/p&gt; 
&lt;p&gt;åˆ†äº«ä¸€äº›å¥½ç”¨çš„ Dify å·¥ä½œæµç¨‹ï¼Œè‡ªç”¨ã€å­¦ä¹ ä¸¤ç›¸å®œï¼Œè¯·ä½¿ç”¨ Dify 0.8.0 åŠä»¥ä¸Šç‰ˆæœ¬å¯¼å…¥ä½¿ç”¨ã€‚&lt;/p&gt; 
&lt;h2&gt;ä½¿ç”¨è¯´æ˜&lt;/h2&gt; 
&lt;h3&gt;1 æ‰“å¼€dify&lt;/h3&gt; 
&lt;p&gt;â€‹ &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20241115095253729.png" alt="image-20241115095253729" /&gt;&lt;/p&gt; 
&lt;h3&gt;2 å¯¼å…¥DSL&lt;/h3&gt; 
&lt;p&gt;â€‹ åœ¨åˆ›å»ºåº”ç”¨-å¯¼å…¥dsl&lt;/p&gt; 
&lt;p&gt;â€‹ &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20241115095400354.png" alt="image-20241115095400354" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20241115100248631.png" alt="image-20241115100248631" /&gt;&lt;/p&gt; 
&lt;h3&gt;3 åˆ›å»º&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20241115100334137.png" alt="image-20241115100334137" /&gt;&lt;/p&gt; 
&lt;h2&gt;4 å®Œæˆ&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20241115100449276.png" alt="image-20241115100449276" /&gt;&lt;/p&gt; 
&lt;h3&gt;DSLåˆ—è¡¨æ¸…å•&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;DSLæ¸…å•åç§°&lt;/th&gt; 
   &lt;th&gt;å·¥ä½œæµæ˜¾ç¤º&lt;/th&gt; 
   &lt;th&gt;ç”¨åˆ°æŠ€æœ¯&lt;/th&gt; 
   &lt;th&gt;æ›´æ–°æ—¶é—´&lt;/th&gt; 
   &lt;th&gt;ä½œè€…&lt;/th&gt; 
   &lt;th&gt;é€‚ç”¨difyç‰ˆæœ¬&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;é›¶ä»£ç æå®š DIFY æ’ä»¶å¼€å‘ï¼šå°ç™½ä¹Ÿèƒ½ä¸Šæ‰‹çš„æ–‡ç”Ÿå›¾æ’ä»¶å®æˆ˜.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250820104750679.png" alt="image-20250820104750679" /&gt;&lt;/td&gt; 
   &lt;td&gt;qwen-imageæ’ä»¶ï¼ˆè‡ªå·±å¼€å‘çš„ï¼‰&lt;/td&gt; 
   &lt;td&gt;2025å¹´8æœˆ20æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100% è¯†åˆ«ç‡ï¼å‘ç¥¨ã€æ±‡ç¥¨ã€ä¿¡ç”¨è¯å…¨æå®šçš„é€šç”¨ç¥¨æ®è¯†åˆ«å·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250813135448522.png" alt="image-20250813135448522" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ¡ä»¶åˆ¤æ–­ã€httpè¯·æ±‚ã€LLMå¤§è¯­è¨€æ¨¡å‹ã€ä»£ç å¤„ç†&lt;/td&gt; 
   &lt;td&gt;2025å¹´8æœˆ13æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ç”¨ Qwen Code+Dify ä¸€é”®ç”Ÿæˆäº¤äº’å¼å…ƒç´ å‘¨æœŸè¡¨ç½‘é¡µ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/QQ_1754805706865.png.png" alt="img" /&gt;&lt;/td&gt; 
   &lt;td&gt;LLMå¤§è¯­è¨€æ¨¡å‹ã€ä»£ç å¤„ç†&lt;/td&gt; 
   &lt;td&gt;2025å¹´8æœˆ10æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dify ç§˜å¡”æœç´¢å·¥ä½œæµæ­å»ºæ•™ç¨‹ä¸æ•ˆæœå±•ç¤º.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/3337.gif.gif" alt="3337" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ¡ä»¶åˆ†æ”¯ã€httpè¯·æ±‚ã€ä»£ç å¤„ç†ã€ç§˜æ­API&lt;/td&gt; 
   &lt;td&gt;2025å¹´8æœˆ9æ—¥&lt;/td&gt; 
   &lt;td&gt;é˜¿æ–‡&lt;/td&gt; 
   &lt;td&gt;1.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dify å·¥ä½œæµä¸€é”®ç”Ÿæˆå‘ç¥¨ç”³è¯·é¢„è§ˆï¼Œå¯¹æ¥å¼€ç¥¨ç³»ç»Ÿè¶…ç®€å•.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/QQ_1754435884189.png" alt="img" /&gt;&lt;/td&gt; 
   &lt;td&gt;excelå·¥å…·ã€LLMå¤§è¯­è¨€æ¨¡å‹ã€ä»£ç å¤„ç†&lt;/td&gt; 
   &lt;td&gt;2025å¹´8æœˆ6æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ç”¨ Dify å®ç°å¤šè¯­è¨€ PDF æ–‡æ¡£åŸæ ¼å¼ç¿»è¯‘.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/QQ_1753889342965.png" alt="img" /&gt;&lt;/td&gt; 
   &lt;td&gt;Agentã€MCP-Server&lt;/td&gt; 
   &lt;td&gt;2025å¹´7æœˆ30æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ç”¨Kimi-K2+Mermaid ç¥å™¨ï¼Œä¸€é”®ç”Ÿæˆç³»ç»Ÿæ¶æ„å›¾ï¼å°ç™½ä¹Ÿèƒ½ç§’ä¼š.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/QQ_1752924484923.png.png" alt="img" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹ã€æ–‡æ¡£ç®¡ç†å™¨ã€LLMå¤§è¯­è¨€ã€Mermaidæ’ä»¶&lt;/td&gt; 
   &lt;td&gt;2025å¹´7æœˆ19æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ç”¨ Dify ä¸€é”®æ­å»ºä¸ƒç‰›äº‘osså·¥ä½œæµ&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://dify-1305874767.cos.ap-nanjing.myqcloud.com/a/%E4%B8%83%E7%89%9B%E4%BA%91mcp2.png" alt="img" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹ã€Agentã€LLMå¤§è¯­è¨€æ¨¡å‹ã€ä»£ç æ‰§è¡Œã€ç›´æ¥å›å¤&lt;/td&gt; 
   &lt;td&gt;2025å¹´7æœˆ11æ—¥&lt;/td&gt; 
   &lt;td&gt;GeekAPP&lt;/td&gt; 
   &lt;td&gt;1.4.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ç”¨ Dify ä¸€é”®æ­å»ºæ•°å­¦å…¬å¼è¯†åˆ«å·¥ä½œæµï¼Œæ”¯æŒlaTex&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://dify-1305874767.cos.ap-nanjing.myqcloud.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20250711082331.png" alt="img" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹ã€Agentã€LLMå¤§è¯­è¨€æ¨¡å‹ã€ä»£ç æ‰§è¡Œã€ç›´æ¥å›å¤&lt;/td&gt; 
   &lt;td&gt;2025å¹´7æœˆ11æ—¥&lt;/td&gt; 
   &lt;td&gt;GeekAPP&lt;/td&gt; 
   &lt;td&gt;1.4.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ç”¨ Dify ä¸€é”®æ­å»ºä¸­è¯ç§‘æ™®å·¥ä½œæµï¼Œæ–‡å­— + å›¾ç‰‡ + è§†é¢‘å…¨æå®š&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/QQ_1751471812082.png" alt="img" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹èŠ‚ç‚¹ã€AGENTã€ç›´æ¥å›å¤ã€è‡ªå®šä¹‰MCP-Server&lt;/td&gt; 
   &lt;td&gt;2025å¹´7æœˆ3æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ç”¨ Dify ä¸€é”®ç”Ÿæˆ é•¿å®‰çš„è”æé‡‘å¥ HTML é¡µé¢ï¼Œä¸‰æ­¥æå®šï¼.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250626211532393.png" alt="image-20250626211532393" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹ã€Agentã€LLMå¤§è¯­è¨€æ¨¡å‹ã€ä»£ç æ‰§è¡Œã€ç›´æ¥å›å¤&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ27æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dify è½»æ¾å®ç° PPT åˆ° SVG æµ·æŠ¥çš„åä¸½å˜èº«_åˆåˆç‰ˆ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250622174547135.png" alt="image-20250622174547135" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹ã€åˆåˆé€šç”¨æ–‡æ¡£è§£æã€LLMå¤§è¯­è¨€æ¨¡å‹ã€ç›´æ¥å›å¤&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ22æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dify è½»æ¾å®ç° PPT åˆ° SVG æµ·æŠ¥çš„åä¸½å˜èº«-MinerUç‰ˆ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250622174454713.png" alt="image-20250622174454713" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹ã€MinerUã€LLMå¤§è¯­è¨€æ¨¡å‹ã€ç›´æ¥å›å¤&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ22æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ä¸­è¯ç§‘æ™®çŸ¥è¯†å·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250617225402249.png.png" alt="image-20250617225402249" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹èŠ‚ç‚¹ã€AGENTã€ç›´æ¥å›å¤ã€è‡ªå®šä¹‰MCP-Server&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ18æ—¥&lt;/td&gt; 
   &lt;td&gt;é†’é†’ã€wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è±†åŒ…æ–‡æœ¬ç”Ÿæˆå›¾ã€æ–‡ç”Ÿè§†é¢‘+å°æ”¯ä»˜åŠŸèƒ½&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250614151812492.png.png" alt="image-20250614151812492" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹èŠ‚ç‚¹ã€æ¡ä»¶åˆ†æ”¯ã€AGENTã€å°æ”¯ä»˜ã€ç›´æ¥å›å¤&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ14æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N8N+Dify æ‰“é€ æ–°é—»å®šæ—¶æ¨é€æµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250610152751289.png" alt="image-20250610152751289" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ¡ä»¶åˆ†æ”¯ã€rookie_rss(ç¬¬ä¸‰æ–¹å·¥å…·)ã€å˜é‡èšåˆå™¨ã€ä»£ç æ‰§è¡Œã€llmæ–‡æœ¬æ¨¡å‹ã€è·å–æ—¶é—´ï¼ˆç¬¬ä¸‰æ–¹å·¥å…·ï¼‰ã€163SMTPé‚®ä»¶å‘é€&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ10æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dify+RSS èšåˆ 8 å¤§å¹³å°å®æ—¶çƒ­ç‚¹ï¼Œæ–°é—»è·å–æ•ˆç‡é£™å‡ 300%.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250609171838135.png.png" alt="image-20250609171838135" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ¡ä»¶åˆ†æ”¯ã€rookie_rss(ç¬¬ä¸‰æ–¹å·¥å…·)ã€å˜é‡èšåˆå™¨ã€ä»£ç æ‰§è¡Œã€llmæ–‡æœ¬æ¨¡å‹ã€è·å–æ—¶é—´ï¼ˆç¬¬ä¸‰æ–¹å·¥å…·ï¼‰&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ9æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;æ‰¹é‡è¯†åˆ«PDFç”µå­å‘ç¥¨ä¿¡æ¯ç”Ÿæˆexcleè¡¨æ ¼.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250605222058323.png.png" alt="image-20250605222058323" /&gt;&lt;/td&gt; 
   &lt;td&gt;è¿­ä»£ã€pdfè½¬pngè½¬æ¢å™¨ï¼ˆç¬¬ä¸‰æ–¹å·¥å…·ï¼‰ã€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€ä»£ç æ‰§è¡Œã€é£ä¹¦è¡¨æ ¼&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ6æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è±†åŒ…æ–‡æœ¬ç”Ÿæˆå›¾åƒã€æ–‡æœ¬ç”Ÿæˆè§†é¢‘ä»¥åŠå›¾åƒè½¬è§†é¢‘.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250603140834839.png.png" alt="image-20250603140834839" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ¡ä»¶åˆ†æ”¯ã€llmå¤§è¯­è¨€æ¨¡å‹ã€Doubao Image and Video Generator çš„æ’ä»¶&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ3æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è¡¡æ°´ä½“è‹±è¯­ä½œæ–‡è¯„åˆ†å·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250601154019415.png.png" alt="image-20250601154019415" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ¨¡æ¿è½¬æ¢ã€æ¡ä»¶åˆ†æ”¯ã€pdfè½¬pngè½¬æ¢å™¨ï¼ˆç¬¬ä¸‰æ–¹å·¥å…·ï¼‰ã€åŸºäºå¤šæ¨¡æ€llmå¤§è¯­è¨€æ¨¡å‹ã€å˜é‡èšåˆå™¨ã€åŸºäºæ–‡æœ¬å¤§è¯­è¨€æ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;2025å¹´6æœˆ2æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ä¸­å°å­¦æ•°å­¦é”™é¢˜æœ¬-ç”ŸæˆåŒç±»å‹é¢˜.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250529235930374.png" alt="image-20250529235930374" /&gt;&lt;/td&gt; 
   &lt;td&gt;é—®é¢˜åˆ†ç±»å™¨ã€æ¨¡æ¿è½¬æ¢ã€æ¡ä»¶åˆ†æ”¯ã€ä»£ç å¤„ç†ã€SQL Executeã€å˜é‡èµ‹å€¼ã€è¿­ä»£ã€è·å–å½“å‰æ—¶é—´ã€Markdownè½¬PDFæ–‡ä»¶ã€LLMå¤§è¯­è¨€æ¨¡å‹ç­‰&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ30æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ä¸­å°å­¦æ•°å­¦é”™é¢˜æœ¬-é”™é¢˜æ”¶é›†ç¯‡.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250528153730875.png" alt="image-20250528153730875" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¼€å§‹èŠ‚ç‚¹ã€æ¨¡æ¿è½¬æ¢ã€æ¡ä»¶åˆ†æ”¯ã€pdfè½¬pngè½¬æ¢å™¨ï¼ˆç¬¬ä¸‰æ–¹å·¥å…·ï¼‰ã€åŸºäºå¤šæ¨¡æ€llmå¤§è¯­è¨€æ¨¡å‹ã€å˜é‡èšåˆå™¨ã€ä»£ç æ‰§è¡Œã€è¿­ä»£ã€SQL Executeï¼ˆç¬¬ä¸‰æ–¹å·¥å…·ï¼‰&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ28æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;0 ä»£ç å®ç°ä¼ä¸šç”»åƒï¼16 ç§å›¾è¡¨ï¼Œè§£é”æ•°æ®æŸ¥è¯¢æ–°å§¿åŠ¿.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250526175553744.png.png" alt="image-20250526175553744" /&gt;&lt;/td&gt; 
   &lt;td&gt;é—®é¢˜åˆ†ç±»å™¨ã€mcp-sseã€mcp-server-chartã€MCP Agent ç­–ç•¥å·¥å…·ã€ä¼ä¸šä¿¡æ¯æŸ¥è¯¢çš„MCP-Server&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ27æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è‹±è¯­å•è¯å£è¯­ç»ƒä¹ .yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250524093955804.png.png" alt="image-20250524093955804" /&gt;&lt;/td&gt; 
   &lt;td&gt;llmå¤§è¯­è¨€æ¨¡å‹ã€å‚æ•°æå–å™¨ã€Markdownè½¬HTMLæ–‡ä»¶&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ24æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;æµ·æŠ¥å°é¢ç”Ÿæˆå·¥ä½œæµ-Wanx æ–‡ç”Ÿå›¾.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250522225213393.png" alt="image-20250522225213393" /&gt;&lt;/td&gt; 
   &lt;td&gt;é˜¿é‡Œäº‘ç™¾ç‚¼MCP Agentï¼ˆWanx æ–‡ç”Ÿå›¾ï¼‰ã€æ¡ä»¶åˆ†æ”¯ã€å˜é‡èšåˆå™¨ã€æ¨¡ç‰ˆè½¬æ¢ã€ä»£ç æ‰§è¡Œ&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ23æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12306mcpç«è½¦ç¥¨ä¿¡æ¯æŸ¥è¯¢-chatflow.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250520164540255.png" alt="image-20250520164540255" /&gt;&lt;/td&gt; 
   &lt;td&gt;AGNETç­–ç•¥å·¥å…·ã€12306MCPã€mcphub&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ21æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3æ­¥å®ç°éŸ³è§†é¢‘è½¬æ–‡å­—ä¼šè®®çºªè¦ä»æ­¤æ— å¿§.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250520103737363.png" alt="image-20250520103737363" /&gt;&lt;/td&gt; 
   &lt;td&gt;ffmpegã€Speech To Textã€ASRæ¨¡å‹ã€LLMå¤§è¯­è¨€æ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ20æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.4.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;åŸºäºä¸Šå¸‚å…¬å¸è´¢æŠ¥åˆ†æç»“æœhtmlåˆ†ææŠ¥å‘Šï¼ˆæ•´åˆMinerU+edgeone-pages-mcpï¼‰.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250513221853568.png" alt="image-20250513221853568" /&gt;&lt;/td&gt; 
   &lt;td&gt;mineruæ’ä»¶ã€LLMå¤§è¯­è¨€æ¨¡å‹ã€å‚æ•°æå–å™¨ã€ä»£ç å¤„ç†ç”Ÿæˆ&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ13æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¤šæ¨¡æ€å›¾åƒç¼–è¾‘(HiDream-E1-Full)chatflow.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250510164614722.png.png" alt="image-20250510164614722" /&gt;&lt;/td&gt; 
   &lt;td&gt;å›¾ç‰‡è½¬base64ã€æ¡ä»¶åˆ†æ”¯ã€ä»£ç è½¬æ¢ã€å˜é‡èµ‹å€¼ã€2ä¸ªè‡ªå®šä¹‰å·¥å…·&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ10æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å›¾ç‰‡ç”Ÿæˆhtml,ç½‘é¡µå°æ¸¸æˆï¼ˆ1panel mcpsseï¼‰.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250503124212089.png.png" alt="image-20250503124212089" /&gt;&lt;/td&gt; 
   &lt;td&gt;1panelmcpã€é—®é¢˜åˆ†ç±»å™¨ã€AGNETç­–ç•¥å·¥å…·ã€edgeone-pages-mcp-server&lt;/td&gt; 
   &lt;td&gt;2025å¹´5æœˆ3æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;é€šç”¨åˆåŒå®¡æŸ¥åŠ©æ‰‹.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250429145121654.png" alt="image-20250429145121654" /&gt;&lt;/td&gt; 
   &lt;td&gt;qwen3æœ€æ–°æ¨¡å‹ã€æ–‡æ¡£æå–å™¨ã€markdownè½¬æ¢å™¨&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ29æ—¥&lt;/td&gt; 
   &lt;td&gt;å •è½å¥¶é…ªã€wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.3.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;é­”æ­ç¤¾åŒºMCP-Server.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250426162356017.png" alt="image-20250426162356017" /&gt;&lt;/td&gt; 
   &lt;td&gt;é—®é¢˜åˆ†ç±»å™¨ã€AGNETç­–ç•¥å·¥å…·ã€é­”æ­ç¤¾åŒºMCP-server(é«˜å¾·MCP ã€å¥½åƒçš„ã€Tavilyã€LeetCode(åŠ›æ‰£))&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ26æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;åŸºäºè¡¨ç»“æ„çš„agent text2sql.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250424115715194.png" alt="image-20250424115715194" /&gt;&lt;/td&gt; 
   &lt;td&gt;AGNET&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ24æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;åŸºäºçŸ¥è¯†åº“+agentå®ç°text2sqlchatflowå·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250424115545595.png" alt="image-20250424115545595" /&gt;&lt;/td&gt; 
   &lt;td&gt;çŸ¥è¯†æ£€ç´¢ã€AGNETç­–ç•¥å·¥å…·ã€databaseæ’ä»¶&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ24æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è½¯ä»¶å¼€å‘ç±»åˆåŒå®¡æŸ¥chatflow.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250423120822891.png" alt="image-20250423120822891" /&gt;&lt;/td&gt; 
   &lt;td&gt;llmã€æ–‡æ¡£æå–å™¨ã€markdownè½¬æ¢å™¨ã€ä¼ä¸šå¾®ä¿¡&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ23æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;æç¤ºè¯ç”Ÿæˆå™¨chatflow.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250419131519060.png" alt="image-20250419131519060" /&gt;&lt;/td&gt; 
   &lt;td&gt;llmã€æ¡ä»¶åˆ¤æ–­å™¨&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ19æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å„¿ç«¥æ•…äº‹ç»˜æœ¬-PPT chatflow.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250414220809839.png" alt="image-20250414220809839" /&gt;&lt;/td&gt; 
   &lt;td&gt;agentã€markdownè½¬æ¢å™¨&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ14æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å„¿ç«¥æ•…äº‹ç»˜æœ¬-PPT Agent.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250414220712520.png" alt="image-20250414220712520" /&gt;&lt;/td&gt; 
   &lt;td&gt;llmã€markdownè½¬PPTX è½¬æ¢å™¨&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ14æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å­¦ç”Ÿæˆç»©æŸ¥è¯¢Chatflowæ”¯æŒtext2sql.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250409155549478.png" alt="image-20250409155549478" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ—¶é—´å·¥å…·ã€EChartså›¾è¡¨ç”Ÿæˆã€databaseã€llm&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ9æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dify-mcp-sse+Zapier MCPæ–°é—»æ£€ç´¢é‚®ä»¶å‘é€.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250407220011943.png" alt="image-20250407220011943" /&gt;&lt;/td&gt; 
   &lt;td&gt;mcp-sseã€Zapier MCP&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ7æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ç‰©å¤´åƒé£æ ¼è¿ç§»å·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250404205912304.png" alt="image-20250404205912304" /&gt;&lt;/td&gt; 
   &lt;td&gt;httpæ¥å£è¯·æ±‚ã€é€†å‘äººç‰©å¤´åƒé£æ ¼è¿ç§»fastapiæ¥å£å®ç°&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ4æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;1.1.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å…è´¹å³æ¢¦æ–‡ç”Ÿè§†é¢‘.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250401145310390.png" alt="image-20250401145310390" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬æ¨¡å‹ã€httpæ¥å£è¯·æ±‚ã€ é€†å‘å³æ¢¦æ–‡ç”Ÿè§†é¢‘fastapiæ¥å£å®ç°&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ1æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å‘¨æ˜“å¤§å¸ˆ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/%E5%91%A8%E6%98%93%E5%A4%A7%E5%B8%88..jpg" alt="image-20250326223949931" /&gt;&lt;/td&gt; 
   &lt;td&gt;å‘¨æ˜“å¤§å¸ˆ&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ26æ—¥&lt;/td&gt; 
   &lt;td&gt;gordon&lt;/td&gt; 
   &lt;td&gt;0.0.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¤§å­¦ç”Ÿè®¡ç®—æœºä¸“ä¸šç®€å†ç¾åŒ–å·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250326223949931.png" alt="image-20250326223949931" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ¨¡ç‰ˆè½¬æ¢ã€æ–‡æœ¬æ¨¡å‹ã€å¤šæ¨¡æ€æ¨¡å‹ã€æ–‡æ¡£æå–å™¨&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ26æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;æ–‡ç”ŸWord_Http_Post.yml ã€æ–‡ç”ŸWord_Agent.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250326223908260.png" alt="image-20250326223908260" /&gt;&lt;/td&gt; 
   &lt;td&gt;ai agentã€è‡ªå®šä¹‰workflowã€httpè¯·æ±‚&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ25æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui+jenal&lt;/td&gt; 
   &lt;td&gt;0.15.3 or 1.1.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ä¸­è‹±æ–‡ç¿»è¯‘å·¥ä½œæµ-AIè¾…åŠ©ç”Ÿæˆ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323180346219.png" alt="image-20250323180346219" /&gt;&lt;/td&gt; 
   &lt;td&gt;llmå¤§è¯­è¨€æ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ23æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui+trae&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gemini-2.0-flash-exp-image-generation-æ–‡ç”Ÿå›¾æ™ºèƒ½ä½“.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323084256259.png" alt="image-20250323084256259" /&gt;&lt;/td&gt; 
   &lt;td&gt;é—®é¢˜åˆ†ç±»å™¨ã€æ¡ä»¶åˆ†æ”¯ã€è‡ªå®šä¹‰å·¥å…·ã€å˜é‡èµ‹å€¼&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ20æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å„¿ç«¥æ•…äº‹ç»˜æœ¬æ–‡ç”Ÿè§†é¢‘è¯­éŸ³åˆæˆç‰ˆ .yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323084414060.png" alt="image-20250323084414060" /&gt;&lt;/td&gt; 
   &lt;td&gt;httpæ¥å£è¯·æ±‚ã€æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€æ–‡ç”Ÿå›¾æ¨¡å‹ã€edgettsã€ffmpeg&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ18&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¤§æ¨¡å‹è¡¨æ ¼è§£æè‡ªåŠ¨ç”Ÿæˆä»£ç ç”Ÿæˆç»Ÿè®¡å›¾.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323084556331.png" alt="image-20250323084556331" /&gt;&lt;/td&gt; 
   &lt;td&gt;httpæ¥å£è¯·æ±‚ã€æ–‡æ¡£æå–åŠŸèƒ½ã€æ–‡æœ¬ç”Ÿæˆæ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ13&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è‚¡ç¥¨åˆ†æç³»ç»Ÿ-Gordonä¿®æ”¹ç‰ˆ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323084643251.png" alt="image-20250323084643251" /&gt;&lt;/td&gt; 
   &lt;td&gt;å¢åŠ ç”¨æˆ·è¾“å…¥æ–‡æœ¬å‚æ•°æå– httpæ¥å£è¯·æ±‚ã€æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€Akshareè‚¡ç¥¨æ•°æ®æ¥å£ã€æ¡ä»¶åˆ†æ”¯ã€å˜é‡èšåˆå™¨&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ18&lt;/td&gt; 
   &lt;td&gt;Gordon&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è‚¡ç¥¨åˆ†æç³»ç»Ÿ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323084739019.png" alt="image-20250323084739019" /&gt;&lt;/td&gt; 
   &lt;td&gt;httpæ¥å£è¯·æ±‚ã€æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€Akshareè‚¡ç¥¨æ•°æ®æ¥å£ã€æ¡ä»¶åˆ†æ”¯ã€å˜é‡èšåˆå™¨&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ11 æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;APIæ–‡æ¡£ç”Ÿæˆä»£ç .yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323084807347.png" alt="image-20250323084807347" /&gt;&lt;/td&gt; 
   &lt;td&gt;jina-aiã€æ–‡æ¡£æå–åŠŸèƒ½ã€æ–‡æœ¬ç”Ÿæˆæ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ9 æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AIç»˜ç”»+é£ä¹¦+ä¼ä¸šå¾®ä¿¡æ•´åˆ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323084834734.png" alt="image-20250323084834734" /&gt;&lt;/td&gt; 
   &lt;td&gt;httpæ¥å£è¯·æ±‚ã€jimeng-free-apiã€é£ä¹¦è¡¨æ ¼ã€ä¼ä¸šå¾®ä¿¡&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ7 æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;çŸ¥è¯†åº“æ£€ç´¢å·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323084900900.png" alt="image-20250323084900900" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€çŸ¥è¯†åº“RAGã€çŸ¥è¯†æ£€ç´¢&lt;/td&gt; 
   &lt;td&gt;2025å¹´3æœˆ3 æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ç”Ÿæˆç»©æŸ¥è¯¢å·¥ä½œæµï¼ˆå¸¦æ•°æ®åº“æŸ¥è¯¢ï¼‰.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323084936593.png" alt="image-20250323084936593" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€æŸ±çŠ¶å›¾ã€æ•°æ®åº“æŸ¥è¯¢ã€httpæ¥å£è¯·æ±‚ã€table markdown&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ27 æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;excelè¡¨æ ¼æå–+echartså±•ç¤º.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085005332.png" alt="image-20250323085005332" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€æ–‡æ¡£æå–åŠŸèƒ½ã€echarts&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ25 æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ai agentæ™ºèƒ½ä½“.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085050777.png" alt="image-20250323085050777" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€è”ç½‘æœç´¢ã€è·å–å½“å‰ç³»ç»Ÿæ—¶é—´ã€AIç»˜ç”»ã€è¯­éŸ³æ’­æŠ¥ç­‰æ™ºèƒ½ä½“åŠŸèƒ½ç»„åˆã€‚&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ22 æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;æ–‡ç”Ÿè§†é¢‘+ttsè¯­éŸ³æ’­æŠ¥.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085121044.png" alt="image-20250323085121044" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€httpæ¥å£è¯·æ±‚ã€edgettsã€æ–‡æœ¬ç”Ÿæˆè§†é¢‘æ¨¡å‹ï¼ˆæ¥å£è°ƒç”¨ï¼‰&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ20 æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å„¿ç«¥æ•…äº‹ç»˜æœ¬.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085154185.png" alt="image-20250323085154185" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€httpæ¥å£è¯·æ±‚ã€jimeng-free-apiã€edgetts&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ15æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;é£ä¹¦è¡¨æ ¼.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085225985.png" alt="image-20250323085225985" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€difyå†…ç½®å·¥å…·ã€é£ä¹¦è¡¨æ ¼&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ12æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è‡ªå¸¦edgetts.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085421648.png" alt="image-20250323085421648" /&gt;&lt;/td&gt; 
   &lt;td&gt;httpæ¥å£è¯·æ±‚ã€edgetts&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ10æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è‡ªå®šä¹‰edgettså·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085251381.png" alt="image-20250323085251381" /&gt;&lt;/td&gt; 
   &lt;td&gt;httpæ¥å£è¯·æ±‚ã€edgetts&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ10æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å³æ¢¦AIç»˜ç”».yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085448044.png" alt="image-20250323085448044" /&gt;&lt;/td&gt; 
   &lt;td&gt;httpæ¥å£è¯·æ±‚ã€jimeng-free-api&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ4æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AIèµ„è®¯æ¯æ—¥æ–°é—»+è¯­éŸ³æ’­æŠ¥å·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085517148.png" alt="image-20250323085517148" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€crawl4ai&lt;/td&gt; 
   &lt;td&gt;2025å¹´2æœˆ3æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YouTubeåšä¸»å’Œè‡ªåª’ä½“è¿è¥ä¸“å®¶å·¥ä½œæµ.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085542505.png" alt="image-20250323085542505" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;2025å¹´1æœˆ25æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;aiç»˜ç”»æ•´åˆcomfyui_bizair.yml&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;httpæ¥å£è¯·æ±‚ã€comfui_bizair&lt;/td&gt; 
   &lt;td&gt;2025å¹´1æœˆ22æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è¯—å¥å°é¢+è¯­éŸ³æ’­æŠ¥.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085615684.png" alt="image-20250323085615684" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€å¤šæ¨¡æ€æ¨¡å‹ã€æ–‡ç”Ÿè¯­éŸ³æ¨¡å‹ã€httpæ¥å£è¯·æ±‚&lt;/td&gt; 
   &lt;td&gt;2025å¹´1æœˆ18æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;FLUXç»˜ç”»æœºå™¨äºº+å¤šæ¨¡æ€è¯†åˆ«+è¯­éŸ³æ’­æ”¾.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085720140.png" alt="image-20250323085720140" /&gt;&lt;/td&gt; 
   &lt;td&gt;æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€å¤šæ¨¡æ€æ¨¡å‹ã€æ–‡ç”Ÿå›¾æ¨¡å‹ã€æ–‡ç”Ÿè¯­éŸ³æ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;2025å¹´1æœˆ12æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-tune è¯­æ–™æ„é€ å™¨.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323085757187.png" alt="image-20250323085757187" /&gt;&lt;/td&gt; 
   &lt;td&gt;è°ƒç”¨æ–‡æœ¬æ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;2025å¹´1æœˆ7æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;giteeKolorså·¥ä½œæµ.yaml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323090209808.png" alt="image-20250323090209808" /&gt;&lt;/td&gt; 
   &lt;td&gt;è‡ªå®šä¹‰ç¬¬ä¸‰æ–¹æ¥å£æœåŠ¡å°è£…æ–‡ç”Ÿå›¾ã€ä½¿ç”¨gitee Serverless API æ¥å£+è…¾è®¯äº‘OSSå­˜å‚¨&lt;/td&gt; 
   &lt;td&gt;2024å¹´12æœˆ18æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ä¸­å›½å†å²ä¸“å®¶æ’­å®¢.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323090253917.png" alt="image-20250323090253917" /&gt;&lt;/td&gt; 
   &lt;td&gt;è°ƒç”¨æ–‡æœ¬æ¨¡å‹ã€TEXT TO SPEECHå·¥å…·ç»„ä»¶ä½¿ç”¨&lt;/td&gt; 
   &lt;td&gt;2024å¹´11æœˆ26æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;æŠ“å–è·å–36æ°ªçƒ­æ¦œæ–‡ç« å†…å®¹ .yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323090341369.png" alt="image-20250323090341369" /&gt;&lt;/td&gt; 
   &lt;td&gt;è°ƒç”¨æ–‡æœ¬æ¨¡å‹ã€httpæ¥å£è¯·æ±‚ã€jina-aiçˆ¬å–ç½‘é¡µä¿¡æ¯ã€è¿­ä»£&lt;/td&gt; 
   &lt;td&gt;2024å¹´11æœˆ16æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å‘ç¥¨æ¯”å¯¹ä¸“å®¶-æ–°ç‰ˆå®¢è¿ç«è½¦ç¥¨2.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323090415346.png" alt="image-20250323090415346" /&gt;&lt;/td&gt; 
   &lt;td&gt;è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹ã€æ–‡æœ¬æ¨¡å‹&lt;/td&gt; 
   &lt;td&gt;2024å¹´11æœˆ16æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å‘ç¥¨æå–å°å·¥å…·æ•´åˆç‰ˆ-å˜é‡èšåˆå™¨.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323090446623.png" alt="image-20250323090446623" /&gt;&lt;/td&gt; 
   &lt;td&gt;è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹ã€æ–‡æœ¬æ¨¡å‹ã€æ–‡ä»¶æå–å™¨ã€IFæµç¨‹åˆ¤æ–­&lt;/td&gt; 
   &lt;td&gt;2024å¹´11æœˆ16æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¢å€¼ç¨å‘ç¥¨æå–å°å·¥å…·chatflow.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323090514862.png" alt="image-20250323090514862" /&gt;&lt;/td&gt; 
   &lt;td&gt;è°ƒç”¨å¤šæ¨¡æ€æ¨¡å‹ã€æ–‡æœ¬æ¨¡å‹ã€æ–‡ä»¶æå–å™¨&lt;/td&gt; 
   &lt;td&gt;2024å¹´11æœˆ16æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;FLUXç»˜ç”»æœºå™¨äºº.yml&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250323090547121.png" alt="image-20250323090547121" /&gt;&lt;/td&gt; 
   &lt;td&gt;è°ƒç”¨FLUXç»˜ç”»æ¨¡å‹ã€æ–‡æœ¬æ¨¡å‹ã€httpæ¥å£è¯·æ±‚&lt;/td&gt; 
   &lt;td&gt;2024å¹´11æœˆ16æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;0.15.3&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;éƒ¨åˆ†è§†é¢‘é“¾æ¥åœ°å€&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;è§†é¢‘åç§°&lt;/th&gt; 
   &lt;th&gt;é“¾æ¥åœ°å€&lt;/th&gt; 
   &lt;th&gt;è§†é¢‘æº&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;difyæ¡ˆä¾‹åˆ†äº«-åŸºäºå¤šæ¨¡æ€æ¨¡å‹çš„å‘ç¥¨è¯†åˆ«&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bilibili.com/video/BV1H51xYbENj"&gt;https://www.bilibili.com/video/BV1H51xYbENj&lt;/a&gt; ã€&lt;a href="https://www.youtube.com/watch?v=rjMBui5IsOw%E3%80%81https://www.toutiao.com/video/7435521963365237287/"&gt;https://www.youtube.com/watch?v=rjMBui5IsOwã€https://www.toutiao.com/video/7435521963365237287/&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Bç«™ã€æ²¹ç®¡ã€ä»Šæ—¥å¤´æ¡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;difyæ¡ˆä¾‹åˆ†äº«-åŸºäºå¤šæ¨¡æ€æ¨¡å‹çš„å‘ç¥¨è¯†åˆ«2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bilibili.com/video/BV1YgmzYxEhh%E3%80%81https://www.youtube.com/watch?v=ghehTQhdnss%E3%80%81https://www.toutiao.com/video/7433468877918437940/"&gt;https://www.bilibili.com/video/BV1YgmzYxEhhã€https://www.youtube.com/watch?v=ghehTQhdnssã€https://www.toutiao.com/video/7433468877918437940/&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Bç«™ã€æ²¹ç®¡ã€ä»Šæ—¥å¤´æ¡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;difyæ¡ˆä¾‹åˆ†äº«-åŸºäºå¤šæ¨¡æ€æ¨¡å‹çš„å‘ç¥¨æ¯”å¯¹&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bilibili.com/video/BV1YgmzYxEhh%E3%80%81https://www.youtube.com/watch?v=Id41hLyxwlE%E3%80%81https://www.toutiao.com/video/7435521963365237287/"&gt;https://www.bilibili.com/video/BV1YgmzYxEhhã€https://www.youtube.com/watch?v=Id41hLyxwlEã€https://www.toutiao.com/video/7435521963365237287/&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Bç«™ã€æ²¹ç®¡ã€ä»Šæ—¥å¤´æ¡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;difyæ¡ˆä¾‹åˆ†äº«-åŸºäºjinaå’Œhttpå®ç°36æ°ªæ–°é—»çƒ­æ¦œæ–‡ç« &lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bilibili.com/video/BV1YgmzYxEhh%E3%80%81https://www.youtube.com/watch?v=hrS-FTLtsGI"&gt;https://www.bilibili.com/video/BV1YgmzYxEhhã€https://www.youtube.com/watch?v=hrS-FTLtsGI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Bç«™ã€æ²¹ç®¡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;difyæ¡ˆä¾‹åˆ†äº«-æ–‡ç”Ÿå›¾ç‰‡OCRè¯†åˆ«åŠ è¯­éŸ³æ’­æŠ¥ï¼ŒAIå·¥ä½œæµä¸€é”®æå®š&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bilibili.com/video/BV13GcgezEVT%E3%80%81https://www.youtube.com/watch?v=Nq_5kDW0jO0&amp;amp;t=16s%E3%80%81https://www.toutiao.com/video/7458884426408182282/"&gt;https://www.bilibili.com/video/BV13GcgezEVTã€https://www.youtube.com/watch?v=Nq_5kDW0jO0&amp;amp;t=16sã€https://www.toutiao.com/video/7458884426408182282/&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Bç«™ã€æ²¹ç®¡ã€ä»Šæ—¥å¤´æ¡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;difyæ¡ˆä¾‹åˆ†äº«-å¤è¯—è¯æµ·æŠ¥ç”ŸæˆåŠ è¯­éŸ³æ’­æŠ¥&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bilibili.com/video/BV1fVwPeqEz9%E3%80%81https://www.youtube.com/watch?v=M6aVZX51cO0%E3%80%81https://www.toutiao.com/video/7461152220034171429/"&gt;https://www.bilibili.com/video/BV1fVwPeqEz9ã€https://www.youtube.com/watch?v=M6aVZX51cO0ã€https://www.toutiao.com/video/7461152220034171429/&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Bç«™ã€æ²¹ç®¡ã€ä»Šæ—¥å¤´æ¡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;difyæ¡ˆä¾‹åˆ†äº«- å„¿ç«¥æ•…äº‹ç»˜æœ¬&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.bilibili.com/video/BV1WCAgeNEsw%E3%80%81https://www.youtube.com/watch?v=QV2MjL6fMi4%E3%80%81https://www.toutiao.com/video/7471874756129505792/"&gt;https://www.bilibili.com/video/BV1WCAgeNEswã€https://www.youtube.com/watch?v=QV2MjL6fMi4ã€https://www.toutiao.com/video/7471874756129505792/&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Bç«™ã€æ²¹ç®¡ã€ä»Šæ—¥å¤´æ¡ã€å¾®ä¿¡è§†é¢‘å·&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;mcp-server&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;mcp-serveråç§°&lt;/th&gt; 
   &lt;th&gt;ç•Œé¢æ˜¾ç¤º&lt;/th&gt; 
   &lt;th&gt;ç”¨åˆ°æŠ€æœ¯&lt;/th&gt; 
   &lt;th&gt;æ›´æ–°æ—¶é—´&lt;/th&gt; 
   &lt;th&gt;ä½œè€…&lt;/th&gt; 
   &lt;th&gt;mcp client&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å³æ¢¦ai æ–‡ç”Ÿè§†é¢‘&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250402133331455.png" alt="image-20250402133331455" /&gt;&lt;/td&gt; 
   &lt;td&gt;fastapi-mcp&lt;/td&gt; 
   &lt;td&gt;2025å¹´4æœˆ2æ—¥&lt;/td&gt; 
   &lt;td&gt;wwwzhouhui&lt;/td&gt; 
   &lt;td&gt;Cherry Studio&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;æ›´æ–°è¯´æ˜&lt;/h2&gt; 
&lt;p&gt;2025å¹´8æœˆ20 æ—¥-version 0.0.3.41ï¼šå¢åŠ é›¶ä»£ç æå®š DIFY æ’ä»¶å¼€å‘ä¸»è¦æ˜¯åŸºäºqwen-imageæ–‡ç”Ÿå›¾ï¼Œdifyæ’ä»¶ä»£ç å‚è€ƒ&lt;a href="https://github.com/wwwzhouhui/qwen_text2image"&gt;https://github.com/wwwzhouhui/qwen_text2image&lt;/a&gt; é¡¹ç›®&lt;/p&gt; 
&lt;p&gt;2025å¹´8æœˆ15 æ—¥-version 0.0.3.40ï¼šå¢åŠ gradioè®¿é—®ç‹¬ç«‹äºdifyé™åˆ¶ï¼Œå¯è‡ªä¸»æ ¹æ®é…ç½®è¿è¡Œï¼šï¼ˆå¤§æ¨¡å‹keyè‡ªä¸»é…ç½®æ–¹å¯è¿è¡Œï¼‰&lt;/p&gt; 
&lt;p&gt;1.æ¢è„¸ï¼š&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/raw/main/py/geekaiapp/beartAIfaceswap1/bf_gradio.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/blob/main/py/geekaiapp/beartAIfaceswap1/bf_gradio.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2.å•è¯çº é”™ï¼š&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/raw/main/py/geekaiapp/danci1/danci1_gradio.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/blob/main/py/geekaiapp/danci1/danci1_gradio.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;3.é«˜æ•°å…¬å¼è¯†åˆ«åˆ°wordç¼–è¾‘ï¼š&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/raw/main/py/geekaiapp/laTex1/latex_gradio.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/blob/main/py/geekaiapp/laTex1/latex_gradio.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;4.marpæ€ç»´å¯¼å›¾ï¼š&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/raw/main/py/geekaiapp/markdown2map/marp1_gradio.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/blob/main/py/geekaiapp/markdown2map/marp1_gradio.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;5.ttsæ–‡è½¬éŸ³ï¼š&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/raw/main/py/geekaiapp/tts1/tts1_gradio.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/blob/main/py/geekaiapp/tts1/tts1_gradio.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;6.å³æ¢¦æ–‡ç”Ÿå›¾ï¼š&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/raw/main/py/geekaiapp/wenshengtu1/jm1_gradio.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/blob/main/py/geekaiapp/wenshengtu1/jm1_gradio.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;7.ä¸­è¯è¯†åˆ«æ–‡ç”Ÿå›¾ï¼š&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/raw/main/py/geekaiapp/zhongyao1/zhongyao-gradio-app.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/blob/main/py/geekaiapp/zhongyao1/zhongyao-gradio-app.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´8æœˆ13 æ—¥-version 0.0.3.39ï¼šå¢åŠ 100% è¯†åˆ«ç‡ï¼å‘ç¥¨ã€æ±‡ç¥¨ã€ä¿¡ç”¨è¯å…¨æå®šçš„é€šç”¨ç¥¨æ®è¯†åˆ«å·¥ä½œæµ.ymlã€‚&lt;/p&gt; 
&lt;p&gt;2025å¹´8æœˆ10 æ—¥-version 0.0.3.38ï¼šå¢åŠ ç”¨ Qwen Code+Dify ä¸€é”®ç”Ÿæˆäº¤äº’å¼å…ƒç´ å‘¨æœŸè¡¨ç½‘é¡µ.yml ä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç  ä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´8æœˆ9 æ—¥-version 0.0.3.37ï¼šå¢åŠ Dify ç§˜å¡”æœç´¢å·¥ä½œæµæ­å»ºæ•™ç¨‹ä¸æ•ˆæœå±•ç¤º.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´8æœˆ6 æ—¥-version 0.0.3.36ï¼šå¢åŠ Dify å·¥ä½œæµä¸€é”®ç”Ÿæˆå‘ç¥¨ç”³è¯·é¢„è§ˆï¼Œå¯¹æ¥å¼€ç¥¨ç³»ç»Ÿè¶…ç®€å•.ymlï¼Œä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç  ä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´7æœˆ30 æ—¥-version 0.0.3.35ï¼šå¢åŠ ç”¨ Dify å®ç°å¤šè¯­è¨€ PDF æ–‡æ¡£åŸæ ¼å¼ç¿»è¯‘.yml ,ä½¿ç”¨åˆ°çš„MCP-Server å¯ä»¥å»æˆ‘å¦å¤–ä¸€ä¸ªå¼€æºé¡¹ç›®&lt;a href="https://github.com/wwwzhouhui/pdftranslate_web"&gt;https://github.com/wwwzhouhui/pdftranslate_web&lt;/a&gt; å»æŸ¥çœ‹ã€‚ï¼ˆä»£ç éƒ¨åˆ†å¼€æºï¼‰&lt;/p&gt; 
&lt;p&gt;2025å¹´7æœˆ19æ—¥-version 0.0.3.34ï¼šå¢åŠ ç”¨Kimi-K2+Mermaid ç¥å™¨ï¼Œä¸€é”®ç”Ÿæˆç³»ç»Ÿæ¶æ„å›¾ï¼å°ç™½ä¹Ÿèƒ½ç§’ä¼š.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´7æœˆ11æ—¥-version 0.0.3.33ï¼šå¢åŠ ä¸ƒç‰›äº‘mcpå·¥ä½œæµï¼Œéœ€è¦&lt;a href="https://github.com/qiniu/qiniu-mcp-server"&gt;https://github.com/qiniu/qiniu-mcp-server&lt;/a&gt; ç¯å¢ƒã€‚ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/70-dify%E6%A1%88%E4%BE%8B%E5%88%86%E4%BA%AB-%E4%B8%83%E7%89%9B%E4%BA%91mcp.yml"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/70-difyæ¡ˆä¾‹åˆ†äº«-ä¸ƒç‰›äº‘mcp.yml&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´7æœˆ11æ—¥-version 0.0.3.32ï¼šå¢åŠ æ•°å­¦å…¬å¼è¯†åˆ«å·¥ä½œæµï¼Œè¾“å…¥pdfæˆ–å›¾ç‰‡ï¼Œè¯†åˆ«å‡ºé‡Œé¢çš„æ•°å­¦å…¬å¼ï¼Œå¹¶ä¸”è¾“å‡ºæ”¯æŒlatexæ ¼å¼çš„å¯ç¼–è¾‘wordï¼Œéœ€è¦å®‰è£…pandocï¼ˆ &lt;a href="https://github.com/jgm/pandoc/releases/tag/3.7.0.2"&gt;https://github.com/jgm/pandoc/releases/tag/3.7.0.2&lt;/a&gt; ï¼‰å’ŒlaTexï¼ˆ &lt;a href="https://miktex.org/download"&gt;https://miktex.org/download&lt;/a&gt; ï¼‰ ç¯å¢ƒã€‚ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/69-dify%E6%A1%88%E4%BE%8B%E5%88%86%E4%BA%AB-%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E8%AF%86%E5%88%AB%E5%B7%A5%E4%BD%9C%E6%B5%81.yml"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/69-difyæ¡ˆä¾‹åˆ†äº«-æ•°å­¦å…¬å¼è¯†åˆ«å·¥ä½œæµ.yml&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´7æœˆ3æ—¥-version 0.0.3.31ï¼šå¢åŠ ç”¨ Dify ä¸€é”®æ­å»ºä¸­è¯ç§‘æ™®å·¥ä½œæµï¼Œæ–‡å­— + å›¾ç‰‡ + è§†é¢‘å…¨æå®š.ä½¿ç”¨åˆ°ç¬¬ä¸‰æ–¹MCP_Serverä»£ç åœ°å€&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/zhouqyu666/zhongyao-mcp-server"&gt;https://github.com/zhouqyu666/zhongyao-mcp-server&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´6æœˆ27æ—¥-version 0.0.3.30ï¼šå¢åŠ ç”¨ Dify ä¸€é”®ç”Ÿæˆ é•¿å®‰çš„è”æé‡‘å¥ HTML é¡µé¢ï¼Œä¸‰æ­¥æå®šï¼.ymlåç«¯ä»£ç å¤ç”¨ä¹‹å‰çš„ä»£ç  &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´6æœˆ22æ—¥-version 0.0.3.29ï¼šå¢åŠ Dify è½»æ¾å®ç° PPT åˆ° SVG æµ·æŠ¥çš„åä¸½å˜èº«_åˆåˆç‰ˆ.yml,Dify è½»æ¾å®ç° PPT åˆ° SVG æµ·æŠ¥çš„åä¸½å˜èº«-MinerUç‰ˆ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´6æœˆ18æ—¥-version 0.0.3.28ï¼šå¢åŠ ä¸­è¯ç§‘æ™®çŸ¥è¯†å·¥ä½œæµ.yml åç«¯ä»£ç &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/mcp/FastMCP/zhongyao-mcp-server.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/mcp/FastMCP/zhongyao-mcp-server.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´6æœˆ14æ—¥-version 0.0.3.27ï¼šå¢åŠ è±†åŒ…æ–‡æœ¬ç”Ÿæˆå›¾ã€æ–‡ç”Ÿè§†é¢‘+å°æ”¯ä»˜åŠŸèƒ½.yml åç«¯ä»£ç &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/mcp/FastMCP/doubao_mcp_ai_server2.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/mcp/FastMCP/doubao_mcp_ai_server2.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´6æœˆ10æ—¥-version 0.0.3.26ï¼šå¢åŠ N8N+Dify æ‰“é€ æ–°é—»å®šæ—¶æ¨é€æµ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´6æœˆ9æ—¥-version 0.0.3.25ï¼šå¢åŠ Dify+RSS èšåˆ 8 å¤§å¹³å°å®æ—¶çƒ­ç‚¹ï¼Œæ–°é—»è·å–æ•ˆç‡é£™å‡ 300%.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´6æœˆ6æ—¥-version 0.0.3.24ï¼šå¢åŠ æ‰¹é‡è¯†åˆ«PDFç”µå­å‘ç¥¨ä¿¡æ¯ç”Ÿæˆexcleè¡¨æ ¼.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´6æœˆ3æ—¥-version 0.0.3.23ï¼šå¢åŠ è±†åŒ…æ–‡æœ¬ç”Ÿæˆå›¾åƒã€æ–‡æœ¬ç”Ÿæˆè§†é¢‘ä»¥åŠå›¾åƒè½¬è§†é¢‘.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´6æœˆ2æ—¥-version 0.0.3.22ï¼šå¢åŠ è¡¡æ°´ä½“è‹±è¯­ä½œæ–‡è¯„åˆ†å·¥ä½œæµ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ30æ—¥-version 0.0.3.21ï¼šå¢åŠ ä¸­å°å­¦æ•°å­¦é”™é¢˜æœ¬-ç”ŸæˆåŒç±»å‹é¢˜.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ29æ—¥-version 0.0.3.20ï¼šå¢åŠ ä¸­å°å­¦æ•°å­¦é”™é¢˜æœ¬-é”™é¢˜æ”¶é›†ç¯‡.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ28æ—¥-version 0.0.3.19ï¼šå¢åŠ 0 ä»£ç å®ç°ä¼ä¸šç”»åƒï¼16 ç§å›¾è¡¨ï¼Œè§£é”æ•°æ®æŸ¥è¯¢æ–°å§¿åŠ¿.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ24 æ—¥-version 0.0.3.18ï¼šå¢åŠ è‹±è¯­å•è¯å£è¯­ç»ƒä¹ .yml&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ23 æ—¥-version 0.0.3.17ï¼šå¢åŠ æµ·æŠ¥å°é¢ç”Ÿæˆå·¥ä½œæµ-Wanx æ–‡ç”Ÿå›¾.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ21 æ—¥-version 0.0.3.16ï¼šå¢åŠ 12306mcpç«è½¦ç¥¨ä¿¡æ¯æŸ¥è¯¢-chatflow.ymlã€12306mcpç«è½¦ç¥¨ä¿¡æ¯æŸ¥è¯¢-AIAgent.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ20 æ—¥-version 0.0.3.15ï¼šå¢åŠ éŸ³è§†é¢‘è½¬æ–‡å­—ä¼šè®®çºªè¦.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ13 æ—¥-version 0.0.3.14ï¼šå¢åŠ åŸºäºä¸Šå¸‚å…¬å¸è´¢æŠ¥åˆ†æç»“æœhtmlåˆ†ææŠ¥å‘Šï¼ˆæ•´åˆMinerU+edgeone-pages-mcpï¼‰.ymlåç«¯ä»£ç å¤ç”¨ä¹‹å‰çš„ä»£ç  &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ10 æ—¥-version 0.0.3.13ï¼šå¢åŠ å¤šæ¨¡æ€å›¾åƒç¼–è¾‘(HiDream-E1-Full)chatflow.ymlä»¥åŠä»£ç ç­‰èµ„æ–™è¯¦è§&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/difyforgitee"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/difyforgitee&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´5æœˆ3 æ—¥-version 0.0.3.12ï¼šå¢åŠ å›¾ç‰‡ç”Ÿæˆhtml,ç½‘é¡µå°æ¸¸æˆï¼ˆ1panel mcpsseï¼‰.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ29 æ—¥-version 0.0.3.11ï¼šå¢åŠ é€šç”¨åˆåŒå®¡æŸ¥åŠ©æ‰‹.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ26 æ—¥-version 0.0.3.10ï¼šå¢åŠ åŸºäºé­”æ­ç¤¾åŒºMCP-Server.ymlï¼ˆåŒ…å«é­”æ­ç¤¾åŒºMCP-server(é«˜å¾·MCP ã€å¥½åƒçš„ã€Tavilyã€LeetCode(åŠ›æ‰£) 4ä¸ªMCP-Serverï¼‰&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ24 æ—¥-version 0.0.3.09ï¼šå¢åŠ åŸºäºçŸ¥è¯†åº“+agentå®ç°text2sqlchatflowå·¥ä½œæµ.ymlã€åŸºäºè¡¨ç»“æ„çš„agent text2sql.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ23 æ—¥-version 0.0.3.08ï¼šå¢åŠ è½¯ä»¶å¼€å‘ç±»åˆåŒå®¡æŸ¥chatflow.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ19 æ—¥-version 0.0.3.07ï¼šå¢åŠ æç¤ºè¯ç”Ÿæˆå™¨chatflow.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ14 æ—¥-version 0.0.3.06ï¼šå¢åŠ å„¿ç«¥æ•…äº‹ç»˜æœ¬-PPT Agent.ymlã€å„¿ç«¥æ•…äº‹ç»˜æœ¬-PPT chatflow.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ11 æ—¥-version 0.0.3.05ï¼š æ–°å¢dockeréƒ¨ç½²ï¼šdocker-compose up --buildï¼Œ&lt;/p&gt; 
&lt;p&gt;æ±‡æ€»åŠŸèƒ½æ€§æ¥å£&lt;a href="https://raw.githubusercontent.com/wwwzhouhui/dify-for-dsl/main/geekaiapp/g_jiekou.py"&gt;g_jiekou.py&lt;/a&gt;ï¼Œ&lt;/p&gt; 
&lt;p&gt;æ‹†åˆ†éƒ¨åˆ†ä¸šåŠ¡ç‹¬ç«‹è¿è¡Œï¼š&lt;/p&gt; 
&lt;p&gt;1.&lt;a href="https://raw.githubusercontent.com/wwwzhouhui/dify-for-dsl/main/yewu2edgetts"&gt;yewu2edgetts&lt;/a&gt;æ–‡å­—è½¬è¯­éŸ³&lt;/p&gt; 
&lt;p&gt;2.&lt;a href="https://raw.githubusercontent.com/wwwzhouhui/dify-for-dsl/main/yewu2excelhtml"&gt;yewu2excelhtml&lt;/a&gt;æ•°æ®å¯è§†åŒ–mdè½¬æ–‡æ¡£&lt;/p&gt; 
&lt;p&gt;3.&lt;a href="https://raw.githubusercontent.com/wwwzhouhui/dify-for-dsl/main/yewu2googleimgtxt"&gt;yewu2googleimgtxt&lt;/a&gt;Geminiæ–‡ç”Ÿå›¾ï¼Œå›¾+æ–‡ç”Ÿå›¾ï¼Œè¿è½½&lt;/p&gt; 
&lt;p&gt;4.&lt;a href="https://raw.githubusercontent.com/wwwzhouhui/dify-for-dsl/main/yewu2jmvideo"&gt;yewu2jmvideo&lt;/a&gt;å³æ¢¦AIæ–‡ç”Ÿè§†é¢‘&lt;/p&gt; 
&lt;p&gt;5.&lt;a href="https://raw.githubusercontent.com/wwwzhouhui/dify-for-dsl/main/yewu2story"&gt;yewu2story&lt;/a&gt;æ–‡ç”ŸéŸ³é¢‘+å›¾ç‰‡=åŠ¨æ¼«äººç‰©è¿è½½&lt;/p&gt; 
&lt;p&gt;6.&lt;a href="https://raw.githubusercontent.com/wwwzhouhui/dify-for-dsl/main/yewu2videoaddsrt"&gt;yewu2videoaddsrt&lt;/a&gt;è§†é¢‘åŠ å­—å¹•+éŸ³é¢‘å¯ç¼–è¾‘ï¼ˆæ”¯æŒè½¯ç¡¬å­—å¹•ï¼‰&lt;/p&gt; 
&lt;p&gt;apikeyå€¼é…ç½®å‚è€ƒä»¥å¾€æ–‡ç« å³å¯ã€‚&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ9 æ—¥-version 0.0.3.04ï¼šå¢åŠ å­¦ç”Ÿæˆç»©æŸ¥è¯¢Chatflowæ”¯æŒtext2sql.ymlï¼Œstudent_scores.sql å»ºè¡¨è¯­å¥è¯¦è§&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/db/test2sql/student_scores.sql"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/db/test2sql/student_scores.sql&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ7 æ—¥-version 0.0.3.03ï¼šå¢åŠ dify-mcp-sse+Zapier MCPæ–°é—»æ£€ç´¢é‚®ä»¶å‘é€.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ3 æ—¥-version 0.0.3.02ï¼šå¢åŠ äººç‰©å¤´åƒé£æ ¼è¿ç§»å·¥ä½œæµ.ymlä»¥åŠä»£ç è¯¦è§&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/beartAI/beartAI_face_swap.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/beartAI/beartAI_face_swap.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ2 æ—¥-version 0.0.3.01ï¼šå¢åŠ å…è´¹å³æ¢¦æ–‡ç”Ÿè§†é¢‘mcp-server,ä»£ç è¯¦è§ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/mcp/fastapi-mcp-server/jimeng/jimeng_video_service.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/mcp/fastapi-mcp-server/jimeng/jimeng_video_service.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´4æœˆ1 æ—¥-version 0.0.2.28 :å…è´¹å³æ¢¦æ–‡ç”Ÿè§†é¢‘.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç  ä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/jimeng/jimeng_video_service.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/jimeng/jimeng_video_service.py&lt;/a&gt; è¯¦ç»†æ“ä½œå¯ä»¥çœ‹æ–‡æ¡£è¯´æ˜å³æ¢¦æ–‡ç”Ÿè§†é¢‘é€†å‘æ¥å£éƒ¨ç½²ä½¿ç”¨.md&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ27 æ—¥-version 0.0.2.27 :å¯¹æŠ—æµ‹è¯•æ–¹æ¡ˆ.md å‘¨æ˜“å¤§å¸ˆ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ26 æ—¥-version 0.0.2.26 :å¤§å­¦ç”Ÿè®¡ç®—æœºä¸“ä¸šç®€å†ç¾åŒ–å·¥ä½œæµ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ25 æ—¥-version 0.0.2.25 :æ–‡ç”ŸWord_Http_Post.ymlã€æ–‡ç”ŸWord_Agent.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç  ä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/office/word/md_to_docx_server.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/office/word/md_to_docx_server.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ23 æ—¥-version 0.0.2.24 :ä¸­è‹±æ–‡ç¿»è¯‘å·¥ä½œæµ-AIè¾…åŠ©ç”Ÿæˆ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ20 æ—¥-version 0.0.2.23 :gemini-2.0-flash-exp-image-generation-æ–‡ç”Ÿå›¾æ™ºèƒ½ä½“.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç  ä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/google/gemini2/image-generation-server.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/google/gemini2/image-generation-server.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ19 æ—¥-version 0.0.2.22 :è‚¡ç¥¨åˆ†æç³»ç»Ÿ-Gordonä¿®æ”¹ç‰ˆ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ18 æ—¥-version 0.0.2.21 :å„¿ç«¥æ•…äº‹ç»˜æœ¬æ–‡ç”Ÿè§†é¢‘è¯­éŸ³åˆæˆç‰ˆ .yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç  ä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/story/storymain.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/story/storymain.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ13 æ—¥-version 0.0.2.20 :å¤§æ¨¡å‹è¡¨æ ¼è§£æè‡ªåŠ¨ç”Ÿæˆä»£ç ç”Ÿæˆç»Ÿè®¡å›¾.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç  ä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/makehtml/makehtmlapi.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ11 æ—¥-version 0.0.2.19 :æ–°å¢åŠ è‚¡ç¥¨åˆ†æç³»ç»Ÿ.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç  ä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/akshare/stock_analysis_api.py"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/akshare/stock_analysis_api.py&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ9 æ—¥-version 0.0.2.18 :æ–°å¢åŠ APIæ–‡æ¡£ç”Ÿæˆä»£ç .yml&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ7 æ—¥-version 0.0.2.17 :æ–°å¢åŠ AIç»˜ç”»+é£ä¹¦+ä¼ä¸šå¾®ä¿¡æ•´åˆ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´3æœˆ3 æ—¥-version 0.0.2.16 :æ–°å¢åŠ çŸ¥è¯†åº“æ£€ç´¢å·¥ä½œæµ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´2æœˆ27 æ—¥-version 0.0.2.15:æ–°å¢åŠ å­¦ç”Ÿæˆç»©æŸ¥è¯¢å·¥ä½œæµï¼ˆå¸¦æ•°æ®åº“æŸ¥è¯¢ï¼‰.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç  ä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/db/student"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/db/student&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´2æœˆ25 æ—¥-version 0.0.2.14:æ–°å¢åŠ excelè¡¨æ ¼æå–+echartså±•ç¤º.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´2æœˆ22 æ—¥-version 0.0.2.13:æ–°å¢åŠ ai agentæ™ºèƒ½ä½“.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´2æœˆ20 æ—¥-version 0.0.2.12:æ–°å¢åŠ æ–‡ç”Ÿè§†é¢‘+ttsè¯­éŸ³æ’­æŠ¥.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç ï¼Œè¯¦ç»†æ–‡æ¡£å’Œä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/zhipu"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/zhipu&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´2æœˆ15æ—¥-version 0.0.2.11:æ–°å¢åŠ å„¿ç«¥æ•…äº‹ç»˜æœ¬.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç ï¼Œè¯¦ç»†æ–‡æ¡£å’Œä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/jimeng"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/jimeng&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´2æœˆ12æ—¥-version 0.0.2.10:æ–°å¢åŠ é£ä¹¦è¡¨æ ¼.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´2æœˆ10æ—¥-version 0.0.2.9:æ–°å¢åŠ è‡ªå¸¦edgetts.ymlã€è‡ªå®šä¹‰edgettså·¥ä½œæµ.ymlï¼Œè¯¦ç»†æ–‡æ¡£å’Œä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/edgetts"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/edgetts&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´2æœˆ4æ—¥-version 0.0.2.8:æ–°å¢åŠ å³æ¢¦AIç»˜ç”».yml&lt;/p&gt; 
&lt;p&gt;2025å¹´2æœˆ3æ—¥-version 0.0.2.7:æ–°å¢åŠ AIèµ„è®¯æ¯æ—¥æ–°é—»+è¯­éŸ³æ’­æŠ¥å·¥ä½œæµ.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç ï¼Œè¯¦ç»†æ–‡æ¡£å’Œä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/crawl4ai"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/crawl4ai&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´1æœˆ25æ—¥-version 0.0.2.6:æ–°å¢åŠ YouTubeåšä¸»å’Œè‡ªåª’ä½“è¿è¥ä¸“å®¶å·¥ä½œæµ.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´1æœˆ22æ—¥-version 0.0.2.5:æ–°å¢åŠ aiç»˜ç”»æ•´åˆcomfyui_bizair.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç ï¼Œè¯¦ç»†æ–‡æ¡£å’Œä»£ç çœ‹ &lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/difyforsiliconflow/bizyair"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/difyforsiliconflow/bizyair&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´1æœˆ18æ—¥-version 0.0.2.4:æ–°å¢åŠ è¯—å¥å°é¢+è¯­éŸ³æ’­æŠ¥.yml&lt;/p&gt; 
&lt;p&gt;2025å¹´1æœˆ12æ—¥-version 0.0.2.3:æ–°å¢åŠ FLUXç»˜ç”»æœºå™¨äºº+å¤šæ¨¡æ€è¯†åˆ«+è¯­éŸ³æ’­æ”¾.yml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç ï¼Œè¯¦ç»†æ–‡æ¡£å’Œä»£ç çœ‹&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/difyforsiliconflow/"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/difyforsiliconflow/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2025å¹´1æœˆ7æ—¥- version 0.0.2.2:æ–°å¢åŠ Fine-tune è¯­æ–™æ„é€ å™¨.yml&lt;/p&gt; 
&lt;p&gt;2024å¹´12æœˆ18æ—¥- version 0.0.2.1: æ–°å¢åŠ giteeKolorså·¥ä½œæµ.yaml æä¾›ç¬¬ä¸‰æ–¹æ¥å£apiæºç ï¼Œè¯¦ç»†æ–‡æ¡£å’Œä»£ç çœ‹&lt;a href="https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/difyforgitee"&gt;https://github.com/wwwzhouhui/dify-for-dsl/tree/main/dsl/difyforgitee&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2024å¹´11æœˆ26æ—¥- version 0.0.2: æ–°å¢åŠ ä¸­å›½å†å²ä¸“å®¶æ’­å®¢ DSLæ–‡ä»¶&lt;/p&gt; 
&lt;p&gt;2024å¹´11æœˆ16æ—¥- version 0.0.1: æ–°åˆ›å»ºdslæ–‡ä»¶ï¼ˆåŒ…å«å‘ç¥¨æå–å°å·¥å…·æ•´åˆç‰ˆã€æŠ“å–è·å–36æ°ªçƒ­æ¦œæ–‡ç« å†…å®¹ã€å‘ç¥¨æå–å°å·¥å…·æ•´åˆç‰ˆ-å¾ªç¯è¿­ä»£ç­‰å·¥ä½œæµ)&lt;/p&gt; 
&lt;h2&gt;æŠ€æœ¯æ–‡æ¡£åœ°å€ï¼ˆé£ä¹¦ï¼‰:&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aqma351r01f.feishu.cn/wiki/HF5FwMDQkiHoCokvbQAcZLu3nAg?table=tbleOWb4WgXcxiHK&amp;amp;view=vewGwwbpzl"&gt;https://aqma351r01f.feishu.cn/wiki/HF5FwMDQkiHoCokvbQAcZLu3nAg?table=tbleOWb4WgXcxiHK&amp;amp;view=vewGwwbpzl&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20241115093319205.png" alt="image-20241115093319205" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ‰ è‡´è°¢&lt;/h2&gt; 
&lt;p&gt;æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®å¯¹æœ¬é¡¹ç›®æä¾›çš„æœ‰åŠ›æ”¯æŒï¼š&lt;/p&gt; 
&lt;p&gt;1.&lt;a href="https://github.com/langgenius/dify"&gt;dify&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Dify æ˜¯ä¸€ä¸ªå¼€æº LLM åº”ç”¨ç¨‹åºå¼€å‘å¹³å°ã€‚ Dify çš„ç›´è§‚ç•Œé¢ç»“åˆäº† AI å·¥ä½œæµç¨‹ã€RAG ç®¡é“ã€ä»£ç†åŠŸèƒ½ã€æ¨¡å‹ç®¡ç†ã€å¯è§‚å¯Ÿæ€§åŠŸèƒ½ç­‰ï¼Œè®©æ‚¨å¿«é€Ÿä»åŸå‹è½¬å‘ç”Ÿäº§ã€‚&lt;/p&gt; 
&lt;p&gt;2.&lt;a href="https://github.com/LLM-Red-Team/jimeng-free-api"&gt;jimeng-free-api&lt;/a&gt; Jimeng AI Free æœåŠ¡ æ”¯æŒå³æ¢¦è¶…å¼ºå›¾åƒç”Ÿæˆèƒ½åŠ›ï¼ˆç›®å‰å®˜æ–¹æ¯æ—¥èµ é€ 66 ç§¯åˆ†ï¼Œå¯ç”Ÿæˆ 66 æ¬¡ï¼‰ï¼Œé›¶é…ç½®éƒ¨ç½²ï¼Œå¤šè·¯ token æ”¯æŒã€‚ ä¸ OpenAI æ¥å£å®Œå…¨å…¼å®¹&lt;/p&gt; 
&lt;p&gt;3.&lt;a href="https://github.com/akfamily/akshare"&gt;akshare&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;å¼€æºè´¢ç»æ•°æ®æ¥å£åº“&lt;/p&gt; 
&lt;p&gt;4.&lt;a href="https://github.com/lanzhihong6/stock-scanner"&gt;stock-scanner&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;è‚¡ç¥¨åˆ†æç³»ç»Ÿ (Stock Analysis System)&lt;/p&gt; 
&lt;p&gt;5.&lt;a href="https://github.com/alecm20/story-flicks"&gt;story-flicks&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä½¿ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…æ•…äº‹çŸ­è§†é¢‘&lt;/p&gt; 
&lt;p&gt;6.&lt;a href="https://github.com/linshenkx/prompt-optimizer"&gt;prompt-optimizer&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä¸€æ¬¾æç¤ºè¯ä¼˜åŒ–å™¨ï¼ŒåŠ©åŠ›äºç¼–å†™é«˜è´¨é‡çš„æç¤ºè¯&lt;/p&gt; 
&lt;p&gt;7.&lt;a href="https://github.com/funstory-ai/BabelDOC"&gt;BabelDOC&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä¸“æ³¨äº &lt;strong&gt;PDF æ–‡æ¡£çš„æ— æŸç¿»è¯‘ä¸åŒè¯­å¯¹ç…§ç”Ÿæˆ&lt;/strong&gt;ï¼Œå°¤å…¶é’ˆå¯¹å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯æŠ¥å‘Šç­‰å¤æ‚æ–‡æ¡£åœºæ™¯ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯è§£å†³ä¼ ç»Ÿ PDF ç¿»è¯‘ä¸­ &lt;strong&gt;æ’ç‰ˆé”™ä¹±ã€å…¬å¼å›¾è¡¨ä¸¢å¤±&lt;/strong&gt;ç­‰ç—›ç‚¹ï¼Œé€šè¿‡æŠ€æœ¯åˆ›æ–°å®ç° &lt;strong&gt;ç¿»è¯‘ç²¾å‡†æ€§ä¸ç‰ˆå¼è¿˜åŸçš„åŒé‡çªç ´&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;é—®é¢˜åé¦ˆ&lt;/h2&gt; 
&lt;p&gt;å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ï¼Œåœ¨æäº¤é—®é¢˜ä¹‹å‰ï¼Œè¯·å…ˆæŸ¥é˜…ä»¥å¾€çš„issueæ˜¯å¦èƒ½è§£å†³ä½ çš„é—®é¢˜&lt;/p&gt; 
&lt;h2&gt;å¸¸è§é—®é¢˜æ±‡æ€»&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;httpè¯·æ±‚èŠ‚ç‚¹è¶…æ—¶æ—¶é—´åŠè¯·æ±‚ä½“å¤§å°é™åˆ¶çš„é…ç½®è°ƒæ•´&lt;/summary&gt; ä¿®æ”¹docker-compose.yaml
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250409115803027.png" alt="ç¤ºä¾‹å›¾ç‰‡" width="400" /&gt;
 &lt;br /&gt; å¯¹åº”çš„æºç  
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/cfbdbf9f-5271-4593-af35-1dd0c14d8b12.png" alt="ç¤ºä¾‹å›¾ç‰‡" width="400" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;80ç«¯å£è¢«å ç”¨ä¿®æ”¹å…¶ä»–ç«¯å£&lt;/summary&gt; é—®é¢˜é»˜è®¤difyä½¿ç”¨çš„æ˜¯80ç«¯å£ã€‚ä½†æ˜¯æœ‰çš„å°ä¼™ä¼´æœåŠ¡å™¨80ç«¯å£è¢«å ç”¨äº†ï¼Œåªèƒ½é€šè¿‡ä¿®æ”¹ç«¯å£é—®é¢˜å®ç°è®¿é—®difyçš„è®¿é—®ã€‚
 &lt;br /&gt; æˆ‘ä»¬ä¿®æ”¹.envé…ç½®
 &lt;br /&gt; æºé…ç½®ä¿¡æ¯
 &lt;br /&gt; EXPOSE_NGINX_PORT=80
 &lt;br /&gt; EXPOSE_NGINX_SSL_PORT=443
 &lt;br /&gt; ä¿®æ”¹å
 &lt;br /&gt; EXPOSE_NGINX_PORT=88
 &lt;br /&gt; EXPOSE_NGINX_SSL_PORT=8443
 &lt;br /&gt; è¿™é‡Œæˆ‘ä»¬éœ€è¦æŠŠ80ï¼ˆhttpï¼‰å’Œ443ï¼ˆhttpsï¼‰ä¿®æ”¹æœåŠ¡å™¨æœªè¢«ä½¿ç”¨ç«¯å£ï¼Œæˆ‘è¿™é‡Œä¿®æ”¹æˆ88 å’Œ4443ç«¯å£
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250419164431488.png" /&gt;
 &lt;br /&gt; ä¿®æ”¹åçš„æ•ˆæœ difyè®¿é—®åœ°å€å˜æˆhttp://101.126.84.227:88/
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250419163321317.png" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;åº”ç”¨ç«¯å£ä¸æ˜¯80åˆ†äº«ä¿®æ”¹ç«¯å£é…ç½®&lt;/summary&gt; å¯¹å¤–è®¿é—®çš„APIæ¥å£åœ°å€æ˜¯http://101.126.84.227/v1ï¼Œåˆ†äº«åè®¿é—®å¤±è´¥
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250419164757901.png" /&gt;
 &lt;br /&gt; è§£å†³æˆ‘ä»¬ä¿®æ”¹.envé…ç½®
 &lt;br /&gt; æºé…ç½®ä¿¡æ¯é»˜è®¤æ˜¯ç©ºçš„
 &lt;br /&gt; # Service API Url,
 &lt;br /&gt; # used to display Service API Base Url to the front-end.
 &lt;br /&gt; # If empty, it is the same domain.
 &lt;br /&gt; # Example: https://api.dify.ai
 &lt;br /&gt; SERVICE_API_URL=
 &lt;br /&gt; ä¿®æ”¹å
 &lt;br /&gt; # Service API Url,
 &lt;br /&gt; # used to display Service API Base Url to the front-end.
 &lt;br /&gt; # If empty, it is the same domain.
 &lt;br /&gt; # Example: https://api.dify.ai
 &lt;br /&gt; SERVICE_API_URL=http://101.126.84.227:88
 &lt;br /&gt; è¿™é‡Œæˆ‘ä»¬éœ€è¦å¡«å†™æœåŠ¡å™¨å¯¹å¤–è®¿é—®åœ°å€+ç«¯å£å·http://101.126.84.227:88
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250419170233151.png" /&gt; ä¿®æ”¹åé‡å¯ï¼Œé‡å¯åæˆ‘ä»¬è®¿é—®webåº”ç”¨
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250419170415108.png" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;æ–‡æ¡£æå–å™¨ä¸æ”¯æŒ docæ–‡ä»¶ä¸Šä¼ &lt;/summary&gt; æ–‡æ¡£æå–å™¨ä¸æ”¯æŒdocæ–‡ä»¶
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250423110402835.png" /&gt;
 &lt;br /&gt; å¦‚æœç”¨æˆ·è¾“å…¥doc æ–‡ä»¶ä¸Šä¼ ä¼šæŠ¥é”™
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250423110443749.png" /&gt; å»ºè®®ç”¨wpsæˆ–è€…office è½¬ä¸€ä¸‹å˜æˆdocxæ–‡ä»¶
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;æ’ä»¶å’Œæ¨¡å‹ä¸‹è½½æ…¢&lt;/summary&gt; é»˜è®¤dify é…ç½®ä»€ä¹ˆéƒ½ä¸è¯¥çš„æƒ…å†µä¸‹ å®‰è£…æ¨¡å‹éå¸¸æ…¢ï¼ˆå›½å†…ç½‘ç»œç¯å¢ƒï¼‰
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250429133213256.png" /&gt;
 &lt;br /&gt; ç‚¹å‡»å®‰è£…æ¨¡å‹æˆ–åˆ™å·¥å…·ååŠå¤©æ²¡ååº”ã€‚ è¿™é‡Œä¸»è¦çš„åŸå› çš„ç›®å‰æ¨¡å‹æ’ä»¶å’Œå·¥å…·ä¸‹è½½åéƒ½éœ€è¦åœ¨å®¹å™¨å†…éƒ¨å®ç°pythonä¾èµ–åŒ…çš„å®‰è£…ï¼Œè€Œpythonä¾èµ–åŒ…å®‰è£…éœ€è¦ç½‘ç»œç¯å¢ƒï¼Œæ‰€ä»¥ä¸‹è½½éå¸¸æ…¢ã€‚å¯ä»¥ä¿®æ”¹.env æ–‡ä»¶ä¸­çš„é…ç½®ï¼ˆå¤§æ¦‚1041è¡Œï¼‰
 &lt;br /&gt; # PIP_MIRROR_URL=https://pypi.tuna.tsinghua.edu.cn/simple
 &lt;br /&gt; PIP_MIRROR_URL=
 &lt;br /&gt; æˆ‘ä»¬éœ€è¦æŠŠä¸Šhttps://pypi.tuna.tsinghua.edu.cn/simple é…ç½®å¡«å†™ï¼Œä¿®æ”¹åçš„åœ°å€å¦‚ä¸‹
 &lt;br /&gt; # PIP_MIRROR_URL=https://pypi.tuna.tsinghua.edu.cn/simple 
 &lt;br /&gt; PIP_MIRROR_URL=https://pypi.tuna.tsinghua.edu.cn/simple 
 &lt;br /&gt; ä¿®æ”¹åé‡å¯dify åé¢å®‰è£…å°±éå¸¸å¿«äº†ã€‚
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;å¸¦æ–‡ä»¶çš„æ’ä»¶å‡ºç°Request URL is missing an 'http://' or 'https://' protocol&lt;/summary&gt; æˆ‘ä»¬é…ç½®ç¬¬ä¸‰æ–¹æ’ä»¶æ¯”å¦‚MinerU ã€Base64 ç¼–è§£ç å™¨ç­‰æ”¯æŒæ–‡ä»¶çš„æ’ä»¶ä¸‹è½½å®Œæˆåä½¿ç”¨æŠ¥é”™
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250510003529262.png" /&gt;
 &lt;br /&gt; å‡ºç°ä¸Šè¿°é”™è¯¯å¦‚ä½•è§£å†³ï¼Ÿ
 &lt;br /&gt; éœ€è¦ä¿®æ”¹2ä¸ªåœ°æ–¹ã€‚
 &lt;br /&gt; 1.env æ–‡ä»¶ä¸­æŸ¥æ‰¾FILES_URL
 &lt;br /&gt; é»˜è®¤çš„FILES_URLæ˜¯ç©ºçš„ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ä½¿ç”¨ http://
 &lt;your-ip&gt;
  :5001 æˆ– http://api:5001ï¼Œåœ¨æ­¤æƒ…å†µä¸‹ï¼Œç¡®ä¿å¤–éƒ¨å¯ä»¥è®¿é—®ç«¯å£ 5001
  &lt;br /&gt; 
  &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250510003900660.png" /&gt;
  &lt;br /&gt; 2.docker-compose.yaml å¯¹åº”çš„FILES_URLä¿®æ”¹
  &lt;br /&gt; 
  &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250510004029749.png" /&gt;
  &lt;br /&gt; æ­¤å¤–dify-apiå®¹å™¨é•œåƒç«¯å£å¼€æ”¾å‡ºæ¥ï¼ˆé»˜è®¤æƒ…å†µæ˜¯ä¸å¼€æ”¾çš„ï¼‰ï¼Œå¢åŠ å¦‚ä¸‹ä»£ç 
  &lt;br /&gt; ports: 
  &lt;br /&gt; - '5001:5001' 
  &lt;br /&gt; 
  &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250510004232125.png" /&gt;
  &lt;br /&gt; æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»dockerå®¹å™¨çœ‹åˆ°ç«¯å£å¼€æ”¾æƒ…å†µï¼ˆé»˜è®¤æ˜¯ä¸å¼€å¯çš„ï¼‰ 
  &lt;br /&gt; 
  &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250510004416081.png" /&gt;
  &lt;br /&gt; 
 &lt;/your-ip&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;è‡ªå®šä¹‰å·¥å…·è¶…æ—¶æ—¶é—´å’Œè¶…æ—¶é‡è¯•æ¬¡æ•°è®¾ç½®&lt;/summary&gt; æˆ‘ä»¬åœ¨.envæ–‡ä»¶æ‰¾åˆ°
 &lt;br /&gt; API_TOOL_DEFAULT_CONNECT_TIMEOUT=10
 &lt;br /&gt; API_TOOL_DEFAULT_READ_TIMEOUT=60
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;dify-sandbox-py é¡¹ç›®å¦‚ä½•è‡ªå®šä¹‰ç¼–è¯‘æ‰“åŒ…&lt;/summary&gt; 1ä¸‹è½½ https://github.com/svcvit/dify-sandbox-pyé¡¹ç›®
 &lt;br /&gt; git clone https://github.com/svcvit/dify-sandbox-py
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/QQ20250512-212041.png" /&gt;
 &lt;br /&gt; æ¥ä¸‹æ¥æˆ‘ä»¬è¾“å…¥å¦‚ä¸‹å‘½ä»¤å®ç°è‡ªå®šä¹‰æ‰“åŒ…
 &lt;br /&gt; cd F:\temp\dify-sandbox-py
 &lt;br /&gt; docker build -t dify-sandbox-py:local .
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;sandbox å¦‚ä½•å®‰è£…pandasè¿™äº›ç¬¬ä¸‰æ–¹åº“ï¼Ÿ&lt;/summary&gt; æ‰“å¼€dify docker æ–‡ä»¶å¤¹é‡Œé¢docker-compose.yamlæ–‡ä»¶
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250512221641604.png" /&gt;
 &lt;br /&gt; æ‰“å¼€docker-compose.yaml æœç´¢langgenius/dify-sandbox æ·»åŠ python-requirements.txt ä¾èµ–åŒ…
 &lt;br /&gt; volumes:
 &lt;br /&gt; - ./volumes/sandbox/dependencies/python-requ
 &lt;br /&gt;irements.txt:/dependencies
 &lt;br /&gt; - ./volumes/sandbox/conf:/conf
 &lt;br /&gt; æˆ‘ä»¬æ·»åŠ pandasä¾èµ–åŒ…
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250512222233209.png" /&gt;
 &lt;br /&gt; ä»¥ä¸Šæ·»åŠ å®Œæˆåï¼Œé‡å¯dify
 &lt;br /&gt; docker compose up -d 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;å¦‚ä½•æŠŠpgæ•°æ®å¯¹å¤–å¼€å¯è®¿é—®&lt;/summary&gt; ç”±äºå®‰å…¨æ€§è€ƒè™‘difyä½¿ç”¨docker å®¹å™¨åŒ–éƒ¨ç½²çš„æ—¶å€™é»˜è®¤æ˜¯ä¸å¯¹å¤–å¼€å¯æ•°æ®åº“è®¿é—®çš„ï¼Œå¦‚æœä½ éœ€è¦äºŒæ¬¡å¼€å‘é€šè¿‡æ•°æ®åº“è¿æ¥æ•´åˆå¯¹æ¥è¿™ä¸ªæ—¶å€™æ˜¯éœ€è¦è®¿é—®å®¹å™¨æ•°æ®åº“ï¼Œè¿™ä¸ªæ—¶å€™å°±éœ€è¦æŠŠæ•°æ®åº“å¼€å¯å¯¹å¤–è®¿é—®ã€‚
 &lt;br /&gt; ä¿®æ”¹docker-compose.yaml æ–‡ä»¶ image: postgres:15-alpine å°†æ•°æ®åº“ç«¯å£å¼€æ”¾å‡ºæ¥
 &lt;br /&gt; åŸé…ç½®æ–‡ä»¶
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250530134120141.png" /&gt;
 &lt;br /&gt; ä¿®æ”¹å
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250530134203328.png" /&gt;
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;å®¹å™¨å†…è®¿é—®ä¸äº†å¤–éƒ¨æ€ä¹ˆåŠï¼Ÿ&lt;/summary&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250525105728679.png" /&gt;
 &lt;br /&gt; é‡åˆ°ä¸Šé¢çš„é—®é¢˜ä¸»è¦æ˜¯å®¹å™¨å†…éƒ¨æ²¡åŠæ³•é€šè¿‡127.0.0.1 è®¿é—®å±€åŸŸç½‘åœ°å€
 &lt;br /&gt; æ–¹æ³•1 æŠŠæ¨¡å‹è¯·æ±‚åœ°å€æ¢æˆå±€åŸŸç½‘åœ°å€
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250525110133945.png" /&gt;
 &lt;br /&gt; æ–¹æ³•2ï¼šæ”¹æˆhttp://host.docker.internal:11434
 &lt;br /&gt; `host.docker.internal` æ˜¯ Docker æä¾›çš„ä¸€ä¸ªç‰¹æ®ŠåŸŸåï¼Œç”¨äºåœ¨ **å®¹å™¨å†…éƒ¨è®¿é—®å®¿ä¸»æœºï¼ˆè¿è¡Œ Docker çš„ä¸»æœºï¼‰çš„ç½‘ç»œæœåŠ¡**ã€‚å®ƒçš„ä½œç”¨ æ˜¯ç®€åŒ–å®¹å™¨ä¸å®¿ä¸»æœºä¹‹é—´çš„ç½‘ç»œé€šä¿¡ï¼Œå°¤å…¶åœ¨å¼€å‘åœºæ™¯ä¸­éå¸¸å®ç”¨
 &lt;br /&gt; **é€‚ç”¨åœºæ™¯**ï¼šå½“å®¹å™¨éœ€è¦è®¿é—®å®¿ä¸»æœºä¸Šè¿è¡Œçš„æœåŠ¡ï¼ˆå¦‚æ•°æ®åº“ã€API æ¥å£ç­‰ï¼‰æ—¶ï¼Œå¯ç›´æ¥ä½¿ç”¨ `host.docker.internal` ä½œä¸ºå®¿ä¸»æœºçš„åœ°å€ï¼Œé¿å…æ‰‹åŠ¨æŸ¥æ‰¾å®¿ä¸»æœºçš„ IP åœ°å€ï¼ˆå¦‚ `192.168.x.x` æˆ– `localhost`ï¼‰
 &lt;br /&gt; **æœ¬è´¨**ï¼šDocker ä¼šå°†è¯¥åŸŸåè‡ªåŠ¨è§£æä¸ºå®¿ä¸»æœºçš„ IP åœ°å€ï¼Œå®ç°å®¹å™¨ä¸å®¿ä¸»æœºçš„ç½‘ç»œäº’é€šã€‚
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;å­—ç¬¦ä¸²å†…å®¹è¶…è¿‡80000é™åˆ¶&lt;/summary&gt; Run failed: The length of output variable result must be less than 80000 characters
 &lt;br /&gt; ä¿®æ”¹.env é…ç½®æ–‡ä»¶
 &lt;br /&gt; CODE MAX STRING LENGTH=800000
 &lt;br /&gt; TEMPLATE TRANSFORM MX LENGTH-800000
 &lt;br /&gt; å¯ä»¥å‚è€ƒä¸‹é¢å›¾
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/image-20250531113229275.png" /&gt;
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;æ–‡ä»¶ä¸Šä¼ è¶…è¿‡15MBçš„é™åˆ¶&lt;/summary&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250620143908372.png" /&gt;
 &lt;br /&gt; æˆ‘ä»¬éœ€è¦ä¿®æ”¹.env æ–‡ä»¶é‡Œé¢
 &lt;br /&gt; UPLOAD_FILE_SIZE_LIMIT=15
 &lt;br /&gt; æŠŠå®ƒä¿®æ”¹æˆ 100 
 &lt;br /&gt; UPLOAD_FILE_SIZE_LIMIT=100
 &lt;br /&gt; å¦å¤– Nginx åå‘ä»£ç†é…ç½®éƒ¨åˆ†ä¿®æ”¹
 &lt;br /&gt; NGINX_CLIENT_MAX_BODY_SIZE=15M
 &lt;br /&gt; ä¿®æ”¹ 
 &lt;br /&gt; NGINX_CLIENT_MAX_BODY_SIZE=100M
 &lt;br /&gt; ä¿®æ”¹åè®°å¾—é‡å¯ã€‚
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Result must be a dict, got str&lt;/summary&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250812171124379.png" /&gt;
 &lt;br /&gt; è¿™ä¸ªé—®é¢˜å‡ºåœ¨ä»£ç æ‰§è¡Œé‡Œé¢ åŸä»£ç 
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250820143143811.png" /&gt;
 &lt;br /&gt; ä¿®æ”¹åçš„ä»£ç ï¼Œéœ€è¦å­—å…¸ 
 &lt;br /&gt; 
 &lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/image-20250820143333517.png" /&gt;
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;h2&gt;æŠ€æœ¯äº¤æµç¾¤&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://mypicture-1258720957.cos.ap-nanjing.myqcloud.com/Obsidian/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20250816233736_15_292.jpg" alt="å¾®ä¿¡å›¾ç‰‡_20250816233736_15_292" /&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=wwwzhouhui/dify-for-dsl&amp;amp;type=Date" alt="dify-for-dsl" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>aliasrobotics/cai</title>
      <link>https://github.com/aliasrobotics/cai</link>
      <description>&lt;p&gt;Cybersecurity AI (CAI), the framework for AI Security&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/cai.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/cai-framework"&gt;&lt;img src="https://badge.fury.io/py/cai-framework.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/cai-framework"&gt;&lt;img src="https://static.pepy.tech/badge/cai-framework" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Linux-Supported-brightgreen?logo=linux&amp;amp;logoColor=white" alt="Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/OS%20X-Supported-brightgreen?logo=apple&amp;amp;logoColor=white" alt="OS X" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Windows-Supported-brightgreen?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Android-Supported-brightgreen?logo=android&amp;amp;logoColor=white" alt="Android" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fnUFcTaQAC"&gt;&lt;img src="https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Cybersecurity AI (CAI) is a lightweight, open-source framework that empowers security professionals to build and deploy AI-powered offensive and defensive automation. CAI is the &lt;em&gt;de facto&lt;/em&gt; framework for AI Security, already used by thousands of individual users and hundreds of organizations. Whether you're a security researcher, ethical hacker, IT professional, or organization looking to enhance your security posture, CAI provides the building blocks to create specialized AI agents that can assist with mitigation, vulnerability discovery, exploitation, and security assessment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;300+ AI Models&lt;/strong&gt;: Support for OpenAI, Anthropic, DeepSeek, Ollama, and more&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;Built-in Security Tools&lt;/strong&gt;: Ready-to-use tools for reconnaissance, exploitation, and privilege escalation&lt;/li&gt; 
 &lt;li&gt;ğŸ† &lt;strong&gt;Battle-tested&lt;/strong&gt;: Proven in HackTheBox CTFs, bug bounties, and real-world security &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;case studies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¯ &lt;strong&gt;Agent-based Architecture&lt;/strong&gt;: Modular framework design to build specialized agents for different security tasks&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Research-oriented&lt;/strong&gt;: Research foundation to democratize cybersecurity AI for the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Read the technical report: &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;CAI: An Open, Bug Bounty-Ready Cybersecurity AI&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;For further readings, refer to our &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-impact"&gt;impact&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;CAI citation&lt;/a&gt; sections.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: Ecoforest Heat Pumps&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;code&gt;Robotics&lt;/code&gt; - CAI and alias0 on: Mobile Industrial Robots (MiR)&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI discovers critical vulnerability in Ecoforest heat pumps allowing unauthorized remote access and potential catastrophic failures. AI-powered security testing reveals exposed credentials and DES encryption weaknesses affecting all of their deployed units across Europe.&lt;/td&gt; 
   &lt;td&gt;CAI-powered security testing of MiR (Mobile Industrial Robot) platform through automated ROS message injection attacks. This study demonstrates how AI-driven vulnerability discovery can expose unauthorized access to robot control systems and alarm triggers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-ecoforest.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mir-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;code&gt;IT&lt;/code&gt; (Web) - CAI and alias0 on: Mercado Libre's e-commerce&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: MQTT broker&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI-powered API vulnerability discovery at Mercado Libre through automated enumeration attacks. This study demonstrates how AI-driven security testing can expose user data exposure risks in e-commerce platforms at scale.&lt;/td&gt; 
   &lt;td&gt;CAI-powered testing exposed critical flaws in an MQTT broker within a Dockerized OT network. Without authentication, CAI subscribed to temperature and humidity topics and injected false values, corrupting data shown in Grafana dashboards.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mercado-libre.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mqtt-broker-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;span&gt;âš &lt;/span&gt; CAI is in active development, so don't expect it to work flawlessly. Instead, contribute by raising an issue or &lt;a href="https://github.com/aliasrobotics/cai/pulls"&gt;sending a PR&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Access to this library and the use of information, materials (or portions thereof), is &lt;strong&gt;&lt;u&gt;not intended&lt;/u&gt;, and is &lt;u&gt;prohibited&lt;/u&gt;, where such access or use violates applicable laws or regulations&lt;/strong&gt;. By no means the authors encourage or promote the unauthorized tampering with running systems. This can cause serious human harm and material damages.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;By no means the authors of CAI encourage or promote the unauthorized tampering with compute systems. Please don't use the source code in here for cybercrime. &lt;u&gt;Pentest for good instead&lt;/u&gt;&lt;/em&gt;. By downloading, using, or modifying this source code, you agree to the terms of the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; and the limitations outlined in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/DISCLAIMER"&gt;&lt;code&gt;DISCLAIMER&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ”–&lt;/span&gt; Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#cybersecurity-ai-cai"&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bookmark-table-of-contents"&gt;&lt;span&gt;ğŸ”–&lt;/span&gt; Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-milestones"&gt;ğŸ¯ Milestones&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#pocs"&gt;PoCs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#motivation"&gt;Motivation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bust_in_silhouette-why-cai"&gt;&lt;span&gt;ğŸ‘¤&lt;/span&gt; Why CAI?&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ethical-principles-behind-cai"&gt;Ethical principles behind CAI&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#closed-source-alternatives"&gt;Closed-source alternatives&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#learn---cai-fluency"&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-install"&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Install&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#os-x"&gt;OS X&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2404"&gt;Ubuntu 24.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2004"&gt;Ubuntu 20.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#windows-wsl"&gt;Windows WSL&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#android"&gt;Android&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-setup-env-file"&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-custom-openai-base-url-support"&gt;ğŸ”¹ Custom OpenAI Base URL Support&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#triangular_ruler-architecture"&gt;&lt;span&gt;ğŸ“&lt;/span&gt; Architecture:&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-agent"&gt;ğŸ”¹ Agent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tools"&gt;ğŸ”¹ Tools&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-handoffs"&gt;ğŸ”¹ Handoffs&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-patterns"&gt;ğŸ”¹ Patterns&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-turns-and-interactions"&gt;ğŸ”¹ Turns and Interactions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tracing"&gt;ğŸ”¹ Tracing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-human-in-the-loop-hitl"&gt;ğŸ”¹ Human-In-The-Loop (HITL)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#rocket-quickstart"&gt;&lt;span&gt;ğŸš€&lt;/span&gt; Quickstart&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#environment-variables"&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#openrouter-integration"&gt;OpenRouter Integration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#mcp"&gt;MCP&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#contributions"&gt;Contributions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;Optional Requirements: caiextensions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#information_source-usage-data-collection"&gt;&lt;span&gt;â„¹&lt;/span&gt; Usage Data Collection&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#reproduce-ci-setup-locally"&gt;Reproduce CI-Setup locally&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¯ Impact&lt;/h2&gt; 
&lt;h3&gt;ğŸ† Competitions and challenges&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_90_Spain_(5_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_50_Spain_(6_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_30_Spain_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_500_World_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_(AIs)_world-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_Spain-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_20_World-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-750_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://lu.ma/roboticshack?tk=RuryKF"&gt;&lt;img src="https://img.shields.io/badge/Mistral_AI_Robotics_Hackathon-2500_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ“Š Research Impact&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pioneered LLM-powered AI Security with PentestGPT, establishing the foundation for the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research domain &lt;a href="https://arxiv.org/pdf/2308.06782"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2308.06782-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Established the &lt;code&gt;Cybersecurity AI&lt;/code&gt; research line with &lt;strong&gt;3 peer-reviewed papers and technical reports&lt;/strong&gt; and active research collaborations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Demonstrated &lt;strong&gt;3,600Ã— performance improvement&lt;/strong&gt; over human penetration testers in standardized CTF benchmark evaluations &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Identified &lt;strong&gt;CVSS 4.3-7.5 severity vulnerabilities&lt;/strong&gt; in production systems through automated security assessment &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Democratization of vulnerability research&lt;/strong&gt;: CAI enables both non-security domain experts and experienced researchers to conduct more efficient vulnerability discovery, expanding the security research community while empowering small and medium enterprises to conduct autonomous security assessments &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Systematic evaluation of large language models&lt;/strong&gt; across both proprietary and open-weight architectures, revealing &lt;u&gt;substantial gaps&lt;/u&gt; between vendor-reported capabilities and empirical cybersecurity performance metrics &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Established the &lt;strong&gt;autonomy levels in cybersecurity&lt;/strong&gt; and argued about autonomy vs automation in the field &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Collaborative research initiatives&lt;/strong&gt; with international academic institutions focused on developing cybersecurity education curricula and training methodologies &lt;a href="https://arxiv.org/abs/2508.13588"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2508.13588-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“š Research products&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;&lt;img src="https://aliasrobotics.com/img/paper-cai.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.arxiv.org/pdf/2506.23592"&gt;&lt;img src="https://aliasrobotics.com/img/cai_automation_vs_autonomy.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://aliasrobotics.com/img/cai_fluency_cover.png" width="350" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;PoCs&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on ROS message injection attacks in MiR-100 robot&lt;/th&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on API vulnerability discovery at Mercado Libre&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh"&gt;&lt;img src="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww"&gt;&lt;img src="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI on JWT@PortSwigger CTF â€” Cybersecurity AI&lt;/th&gt; 
   &lt;th&gt;CAI on HackableII Boot2Root CTF â€” Cybersecurity AI&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713487"&gt;&lt;img src="https://asciinema.org/a/713487.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713485"&gt;&lt;img src="https://asciinema.org/a/713485.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More case studies and PoCs are available at &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;https://aliasrobotics.com/case-studies-robot-cybersecurity.php&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;h3&gt;&lt;span&gt;ğŸ‘¤&lt;/span&gt; Why CAI?&lt;/h3&gt; 
&lt;p&gt;The cybersecurity landscape is undergoing a dramatic transformation as AI becomes increasingly integrated into security operations. &lt;strong&gt;We predict that by 2028, AI-powered security testing tools will outnumber human pentesters&lt;/strong&gt;. This shift represents a fundamental change in how we approach cybersecurity challenges. &lt;em&gt;AI is not just another tool - it's becoming essential for addressing complex security vulnerabilities and staying ahead of sophisticated threats. As organizations face more advanced cyber attacks, AI-enhanced security testing will be crucial for maintaining robust defenses.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;This work builds upon prior efforts[^4] and similarly, we believe that democratizing access to advanced cybersecurity AI tools is vital for the entire security community. That's why we're releasing Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) as an open source framework. Our goal is to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools. By making these capabilities openly available, we aim to level the playing field and ensure that cutting-edge security AI technology isn't limited to well-funded private companies or state actors.&lt;/p&gt; 
&lt;p&gt;Bug Bounty programs have become a cornerstone of modern cybersecurity, providing a crucial mechanism for organizations to identify and fix vulnerabilities in their systems before they can be exploited. These programs have proven highly effective at securing both public and private infrastructure, with researchers discovering critical vulnerabilities that might have otherwise gone unnoticed. CAI is specifically designed to enhance these efforts by providing a lightweight, ergonomic framework for building specialized AI agents that can assist in various aspects of Bug Bounty hunting - from initial reconnaissance to vulnerability validation and reporting. Our framework aims to augment human expertise with AI capabilities, helping researchers work more efficiently and thoroughly in their quest to make digital systems more secure.&lt;/p&gt; 
&lt;h3&gt;Ethical principles behind CAI&lt;/h3&gt; 
&lt;p&gt;You might be wondering if releasing CAI &lt;em&gt;in-the-wild&lt;/em&gt; given its capabilities and security implications is ethical. Our decision to open-source this framework is guided by two core ethical principles:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Democratizing Cybersecurity AI&lt;/strong&gt;: We believe that advanced cybersecurity AI tools should be accessible to the entire security community, not just well-funded private companies or state actors. By releasing CAI as an open source framework, we aim to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools, leveling the playing field in cybersecurity.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Transparency in AI Security Capabilities&lt;/strong&gt;: Based on our research results, understanding of the technology, and dissection of top technical reports, we argue that current LLM vendors are undermining their cybersecurity capabilities. This is extremely dangerous and misleading. By developing CAI openly, we provide a transparent benchmark of what AI systems can actually do in cybersecurity contexts, enabling more informed decisions about security postures.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;CAI is built on the following core principles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cybersecurity oriented AI framework&lt;/strong&gt;: CAI is specifically designed for cybersecurity use cases, aiming at semi- and fully-automating offensive and defensive security tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source, free for research&lt;/strong&gt;: CAI is open source and free for research purposes. We aim at democratizing access to AI and Cybersecurity. For professional or commercial use, including on-premise deployments, dedicated technical support and custom extensions &lt;a href="mailto:research@aliasrobotics.com"&gt;reach out&lt;/a&gt; to obtain a license.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: CAI is designed to be fast, and easy to use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular and agent-centric design&lt;/strong&gt;: CAI operates on the basis of agents and agentic patterns, which allows flexibility and scalability. You can easily add the most suitable agents and pattern for your cybersecuritytarget case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool-integration&lt;/strong&gt;: CAI integrates already built-in tools, and allows the user to integrate their own tools with their own logic easily.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Logging and tracing integrated&lt;/strong&gt;: using &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;, the open source tracing and logging tool for LLMs. This provides the user with a detailed traceability of the agents and their execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: more than 300 supported and empowered by &lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;. The most popular providers: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;: &lt;code&gt;Claude 3.7&lt;/code&gt;, &lt;code&gt;Claude 3.5&lt;/code&gt;, &lt;code&gt;Claude 3&lt;/code&gt;, &lt;code&gt;Claude 3 Opus&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: &lt;code&gt;O1&lt;/code&gt;, &lt;code&gt;O1 Mini&lt;/code&gt;, &lt;code&gt;O3 Mini&lt;/code&gt;, &lt;code&gt;GPT-4o&lt;/code&gt;, &lt;code&gt;GPT-4.5 Preview&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: &lt;code&gt;DeepSeek V3&lt;/code&gt;, &lt;code&gt;DeepSeek R1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: &lt;code&gt;Qwen2.5 72B&lt;/code&gt;, &lt;code&gt;Qwen2.5 14B&lt;/code&gt;, etc&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Closed-source alternatives&lt;/h3&gt; 
&lt;p&gt;Cybersecurity AI is a critical field, yet many groups are misguidedly pursuing it through closed-source methods for pure economic return, leveraging similar techniques and building upon existing closed-source (&lt;em&gt;often third-party owned&lt;/em&gt;) models. This approach not only squanders valuable engineering resources but also represents an economic waste and results in redundant efforts, as they often end up reinventing the wheel. Here are some of the closed-source initiatives we keep track of and attempting to leverage genAI and agentic frameworks in cybersecurity AI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.acyber.co/"&gt;Autonomous Cyber&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cracken.ai/"&gt;CrackenAGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ethiack.com/"&gt;ETHIACK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://horizon3.ai/"&gt;Horizon3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.kindo.ai/"&gt;Kindo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lakera.ai"&gt;Lakera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/www.mindfort.ai"&gt;Mindfort&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mindgard.ai/"&gt;Mindgard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ndaysecurity.com/"&gt;NDAY Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.runsybil.com"&gt;Runsybil&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.selfhack.fi"&gt;Selfhack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://squr.ai/"&gt;SQUR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://staris.tech/"&gt;Staris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sxipher.com/"&gt;Sxipher&lt;/a&gt; (seems discontinued)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.terra.security"&gt;Terra Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xint.io/"&gt;Xint&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.xbow.com"&gt;XBOW&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zeropath.com"&gt;ZeroPath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zynap.com"&gt;Zynap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://7ai.com"&gt;7ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/caiedu.PNG" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;CAI Fluency technical report (&lt;a href="https://arxiv.org/pdf/2508.13588"&gt;arXiv:2508.13588&lt;/a&gt;) establishes formal educational frameworks for cybersecurity AI literacy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;English&lt;/th&gt; 
   &lt;th&gt;Spanish&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 0&lt;/strong&gt;: What is CAI?&lt;/td&gt; 
   &lt;td&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) explained&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=nBdTxbKM4oo"&gt;&lt;img src="https://img.youtube.com/vi/nBdTxbKM4oo/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=FaUL9HXrQ5k"&gt;&lt;img src="https://img.youtube.com/vi/FaUL9HXrQ5k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 1&lt;/strong&gt;: The &lt;code&gt;CAI&lt;/code&gt; Framework&lt;/td&gt; 
   &lt;td&gt;Vision &amp;amp; Ethics - Explore the core motivation behind CAI and delve into the crucial ethical principles guiding its development. Understand the motivation behind CAI and how you can actively contribute to the future of cybersecurity and the CAI framework.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=QEiGdsMf29M&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=3"&gt;&lt;img src="https://img.youtube.com/vi/QEiGdsMf29M/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 2&lt;/strong&gt;: From Zero to Cyber Hero&lt;/td&gt; 
   &lt;td&gt;Breaking into Cybersecurity with AI - A comprehensive guide for complete beginners to become cybersecurity practitioners using CAI and AI tools. Learn how to leverage artificial intelligence to accelerate your cybersecurity learning journey, from understanding basic security concepts to performing real-world security assessments, all without requiring prior cybersecurity experience.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=hSTLHOOcQoY&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=14"&gt;&lt;img src="https://img.youtube.com/vi/hSTLHOOcQoY/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 3&lt;/strong&gt;: Vibe-Hacking Tutorial&lt;/td&gt; 
   &lt;td&gt;"My first Hack" - A Vibe-Hacking guide for newbies. We demonstrate a simple web security hack using a default agent and show how to leverage tools and interpret CIA output with the help of the CAI Python API. You'll also learn to compare different LLM models to find the best fit for your hacking endeavors.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=9vZ_Iyex7uI&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=1"&gt;&lt;img src="https://img.youtube.com/vi/9vZ_Iyex7uI/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=iAOMaI1ftiA&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=2"&gt;&lt;img src="https://img.youtube.com/vi/iAOMaI1ftiA/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 4&lt;/strong&gt;: Intro ReAct&lt;/td&gt; 
   &lt;td&gt;The Evolution of LLMs - Learn how LLMs evolved from basic language models to advanced multiagency AI systems. From basic LLMs to Chain-of-Thought and Reasoning LLMs towards ReAct and Multi-Agent Architectures. Get to know the basic terms&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=tLdFO1flj_o&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/tLdFO1flj_o/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 5&lt;/strong&gt;: CAI on CTF challenges&lt;/td&gt; 
   &lt;td&gt;Dive into Capture The Flag (CTF) competitions using CAI. Learn how to leverage AI agents to solve various cybersecurity challenges including web exploitation, cryptography, reverse engineering, and forensics. Discover how to configure CAI for competitive hacking scenarios and maximize your CTF performance with intelligent automation.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=MrXTQ0e2to4&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/MrXTQ0e2to4/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=r9US_JZa9_c&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=12"&gt;&lt;img src="https://img.youtube.com/vi/r9US_JZa9_c/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 1&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.5.x release&lt;/td&gt; 
   &lt;td&gt;Introduce version 0.5 of &lt;code&gt;CAI&lt;/code&gt; including new multi-agent functionality, new commands such as &lt;code&gt;/history&lt;/code&gt;, &lt;code&gt;/compact&lt;/code&gt;, &lt;code&gt;/graph&lt;/code&gt; or &lt;code&gt;/memory&lt;/code&gt; and a case study showing how &lt;code&gt;CAI&lt;/code&gt; found a critical security flaw in OT heap pumps spread around the world.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=OPFH0ANUMMw"&gt;&lt;img src="https://img.youtube.com/vi/OPFH0ANUMMw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=Q8AI4E4gH8k"&gt;&lt;img src="https://img.youtube.com/vi/Q8AI4E4gH8k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 2&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.4.x release and &lt;code&gt;alias0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Introducing version 0.4 of &lt;code&gt;CAI&lt;/code&gt; with &lt;em&gt;streaming&lt;/em&gt; and improved MCP support. We also introduce &lt;code&gt;alias0&lt;/code&gt;, the Privacy-First Cybersecurity AI, a Model-of-Models Intelligence that implements a Privacy-by-Design architecture and obtains state-of-the-art results in cybersecurity benchmarks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NZjzfnvAZcc"&gt;&lt;img src="https://img.youtube.com/vi/NZjzfnvAZcc/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 3&lt;/strong&gt;: Cybersecurity AI Community Meeting #1&lt;/td&gt; 
   &lt;td&gt;First Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) community meeting, over 40 participants from academia, industry, and defense gathered to discuss the open-source scaffolding behind CAI â€” a project designed to build agentic AI systems for cybersecurity that are open, modular, and Bug Bounty-ready.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=4JqaTiVlgsw"&gt;&lt;img src="https://img.youtube.com/vi/4JqaTiVlgsw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Install&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cai-framework
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Always create a new virtual environment to ensure proper dependency installation when updating CAI.&lt;/p&gt; 
&lt;p&gt;The following subsections provide a more detailed walkthrough on selected popular Operating Systems. Refer to the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; section for developer-related install instructions.&lt;/p&gt; 
&lt;h3&gt;OS X&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew update &amp;amp;&amp;amp; \
    brew install git python@3.12

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 24.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3.12-venv

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 20.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y software-properties-common

# Fetch Python 3.12
sudo add-apt-repository ppa:deadsnakes/ppa &amp;amp;&amp;amp; sudo apt update
sudo apt install python3.12 python3.12-venv python3.12-dev -y

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows WSL&lt;/h3&gt; 
&lt;p&gt;Go to the Microsoft page: &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;https://learn.microsoft.com/en-us/windows/wsl/install&lt;/a&gt;. Here you will find all the instructions to install WSL&lt;/p&gt; 
&lt;p&gt;From Powershell write: wsl --install&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3-venv

# Create the virtual environment
python3 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;p&gt;We recommend having at least 8 GB of RAM:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First of all, install userland &lt;a href="https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es"&gt;https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Kali minimal in basic options (for free). [Or any other kali option if preferred]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Update apt keys like in this example: &lt;a href="https://superuser.com/questions/1644520/apt-get-update-issue-in-kali"&gt;https://superuser.com/questions/1644520/apt-get-update-issue-in-kali&lt;/a&gt;, inside UserLand's Kali terminal execute&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get new apt keys
wget http://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2024.1_all.deb

# Install new apt keys
sudo dpkg -i kali-archive-keyring_2024.1_all.deb &amp;amp;&amp;amp; rm kali-archive-keyring_2024.1_all.deb

# Update APT repository
sudo apt-get update

# CAI requieres python 3.12, lets install it (CAI for kali in Android)
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y git python3-pip build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev pkg-config
wget https://www.python.org/ftp/python/3.12.4/Python-3.12.4.tar.xz
tar xf Python-3.12.4.tar.xz
cd ./configure --enable-optimizations
sudo make altinstall # This command takes long to execute

# Clone CAI's source code
git clone https://github.com/aliasrobotics/cai &amp;amp;&amp;amp; cd cai

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip3 install -e .

# Generate a .env file and set up
cp .env.example .env  # edit here your keys/models

# Launch CAI
cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;span&gt;ğŸ”©&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/h3&gt; 
&lt;p&gt;CAI leverages the &lt;code&gt;.env&lt;/code&gt; file to load configuration at launch. To facilitate the setup, the repo provides an exemplary &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file provides a template for configuring CAI's setup and your LLM API keys to work with desired LLM models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Important:&lt;/p&gt; 
&lt;p&gt;CAI does NOT provide API keys for any model by default. Don't ask us to provide keys, use your own or host your own models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;The OPENAI_API_KEY must not be left blank. It should contain either "sk-123" (as a placeholder) or your actual API key. See &lt;a href="https://github.com/aliasrobotics/cai/issues/27"&gt;https://github.com/aliasrobotics/cai/issues/27&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;âš &lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;If you are using alias0 model, make sure that CAI is &amp;gt;0.4.0 version and here you have an .env example to be able to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY="sk-1234"
OLLAMA=""
ALIAS_API_KEY="&amp;lt;sk-your-key&amp;gt;"  # note, add yours
CAI_STEAM=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¹ Custom OpenAI Base URL Support&lt;/h3&gt; 
&lt;p&gt;CAI supports configuring a custom OpenAI API base URL via the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; environment variable. This allows users to redirect API calls to a custom endpoint, such as a proxy or self-hosted OpenAI-compatible service.&lt;/p&gt; 
&lt;p&gt;Example &lt;code&gt;.env&lt;/code&gt; entry configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or directly from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1" cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ“&lt;/span&gt; Architecture:&lt;/h2&gt; 
&lt;p&gt;CAI focuses on making cybersecurity agent &lt;strong&gt;coordination&lt;/strong&gt; and &lt;strong&gt;execution&lt;/strong&gt; lightweight, highly controllable, and useful for humans. To do so it builds upon 7 pillars: &lt;code&gt;Agent&lt;/code&gt;s, &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;Handoffs&lt;/code&gt;, &lt;code&gt;Patterns&lt;/code&gt;, &lt;code&gt;Turns&lt;/code&gt;, &lt;code&gt;Tracing&lt;/code&gt; and &lt;code&gt;HITL&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚      HITL     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Turns   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Patterns â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚  Handoffs â”‚â—€â”€â”€â”€â”€â–¶ â”‚   Agents  â”‚â—€â”€â”€â”€â”€â–¶â”‚    LLMs   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                   â”‚
                          â”‚                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extensions â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚  Tracing  â”‚       â”‚   Tools   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â–¼             â–¼          â–¼             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ LinuxCmd  â”‚â”‚ WebSearch â”‚â”‚    Code    â”‚â”‚ SSHTunnel â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to dive deeper into the code, check the following files as a start point for using CAI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/__init__.py"&gt;&lt;strong&gt;init&lt;/strong&gt;.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/cli.py"&gt;cli.py&lt;/a&gt; - entrypoint for command line interface&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/util.py"&gt;util.py&lt;/a&gt; - utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/agents"&gt;agents&lt;/a&gt; - Agent implementations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/internal"&gt;internal&lt;/a&gt; - CAI internal functions (endpoints, metrics, logging, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/prompts"&gt;prompts&lt;/a&gt; - Agent Prompt Database&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/repl"&gt;repl&lt;/a&gt; - CLI aesthetics and commands&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/sdk"&gt;sdk&lt;/a&gt; - CAI command sdk&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/tree/main/src/cai/tools"&gt;tools&lt;/a&gt; - agent tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”¹ Agent&lt;/h3&gt; 
&lt;p&gt;At its core, CAI abstracts its cybersecurity behavior via &lt;code&gt;Agents&lt;/code&gt; and agentic &lt;code&gt;Patterns&lt;/code&gt;. An Agent in &lt;em&gt;an intelligent system that interacts with some environment&lt;/em&gt;. More technically, within CAI we embrace a robotics-centric definition wherein an agent is anything that can be viewed as a system perceiving its environment through sensors, reasoning about its goals and and acting accordingly upon that environment through actuators (&lt;em&gt;adapted&lt;/em&gt; from Russel &amp;amp; Norvig, AI: A Modern Approach). In cybersecurity, an &lt;code&gt;Agent&lt;/code&gt; interacts with systems and networks, using peripherals and network interfaces as sensors, reasons accordingly and then executes network actions as if actuators. Correspondingly, in CAI, &lt;code&gt;Agent&lt;/code&gt;s implement the &lt;code&gt;ReACT&lt;/code&gt; (Reasoning and Action) agent model[^3]. For more information, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/hello_world.py"&gt;example here&lt;/a&gt; for the full execution code, and refer to this &lt;a href="https://github.com/aliasrobotics/cai/raw/main/fluency/my-first-hack/my_first_hack.ipynb"&gt;jupyter notebook&lt;/a&gt; for a tutorial on how to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¹ Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Tools&lt;/code&gt; let cybersecurity agents take actions by providing interfaces to execute system commands, run security scans, analyze vulnerabilities, and interact with target systems and APIs - they are the core capabilities that enable CAI agents to perform security tasks effectively; in CAI, tools include built-in cybersecurity utilities (like LinuxCmd for command execution, WebSearch for OSINT gathering, Code for dynamic script execution, and SSHTunnel for secure remote access), function calling mechanisms that allow integration of any Python function as a security tool, and agent-as-tool functionality that enables specialized security agents (such as reconnaissance or exploit agents) to be used by other agents, creating powerful collaborative security workflows without requiring formal handoffs between agents. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/tools.py"&gt;example here&lt;/a&gt; for the complete configuration of custom functions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel
from cai.tools.reconnaissance.exec_code import execute_code
from cai.tools.reconnaissance.generic_linux_command import generic_linux_command

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      tools= [
        generic_linux_command,
        execute_code
      ],
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may find different &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/tools"&gt;tools&lt;/a&gt;. They are grouped in 6 major categories inspired by the security kill chain [^2]:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Reconnaissance and weaponization - &lt;em&gt;reconnaissance&lt;/em&gt; (crypto, listing, etc)&lt;/li&gt; 
 &lt;li&gt;Exploitation - &lt;em&gt;exploitation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Privilege escalation - &lt;em&gt;escalation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Lateral movement - &lt;em&gt;lateral&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Data exfiltration - &lt;em&gt;exfiltration&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Command and control - &lt;em&gt;control&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ğŸ”¹ Handoffs&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Handoffs&lt;/code&gt; allow an &lt;code&gt;Agent&lt;/code&gt; to delegate tasks to another agent, which is crucial in cybersecurity operations where specialized expertise is needed for different phases of an engagement. In our framework, &lt;code&gt;Handoffs&lt;/code&gt; are implemented as tools for the LLM, where a &lt;strong&gt;handoff/transfer function&lt;/strong&gt; like &lt;code&gt;transfer_to_flag_discriminator&lt;/code&gt; enables the &lt;code&gt;ctf_agent&lt;/code&gt; to pass control to the &lt;code&gt;flag_discriminator_agent&lt;/code&gt; once it believes it has found the flag. This creates a security validation chain where the first agent handles exploitation and flag discovery, while the second agent specializes in flag verification, ensuring proper segregation of duties and leveraging specialized capabilities of different models for distinct security tasks. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/cai/agent_patterns/handoffs.py"&gt;example here&lt;/a&gt; for the full execution code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import function_tool
from cai.tools.common import run_command
from cai.sdk.agents import Agent, OpenAIChatCompletionsModel, HandoffInputData, Runner, function_tool, handoff, trace
from cai.sdk.agents.extensions import handoff_filters

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

@function_tool
def execute_cli_command(command: str) -&amp;gt; str:
    return run_command(command)

flag_discriminator = Agent(
    name="Flag discriminator",
    description="Agent focused on extracting the flag from the output",
    instructions="You are an agent tailored to extract the flag from a given output.",
    model=OpenAIChatCompletionsModel(
        model=os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ) 
)

ctf_agent = Agent(
    name="CTF agent",
    description="Agent focused on conquering security challenges",
    instructions="You are a Cybersecurity expert Leader facing a CTF",
    tools=[
        execute_cli_command,
    ],
    model=OpenAIChatCompletionsModel(
        model= os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ), 
    handoffs = [flag_discriminator]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”¹ Patterns&lt;/h3&gt; 
&lt;p&gt;An agentic &lt;code&gt;Pattern&lt;/code&gt; is a &lt;em&gt;structured design paradigm&lt;/em&gt; in artificial intelligence systems where autonomous or semi-autonomous agents operate within a defined &lt;em&gt;interaction framework&lt;/em&gt; (the pattern) to achieve a goal. These &lt;code&gt;Patterns&lt;/code&gt; specify the organization, coordination, and communication methods among agents, guiding decision-making, task execution, and delegation.&lt;/p&gt; 
&lt;p&gt;An agentic pattern (&lt;code&gt;AP&lt;/code&gt;) can be formally defined as a tuple:&lt;/p&gt; 
&lt;p&gt;\[ AP = (A, H, D, C, E) \]&lt;/p&gt; 
&lt;p&gt;wherein:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;\(A\) (Agents):&lt;/strong&gt; A set of autonomous entities, \( A = \{a_1, a_2, ..., a_n\} \), each with defined roles, capabilities, and internal states.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(H\) (Handoffs):&lt;/strong&gt; A function \( H: A \times T \to A \) that governs how tasks \( T \) are transferred between agents based on predefined logic (e.g., rules, negotiation, bidding).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(D\) (Decision Mechanism):&lt;/strong&gt; A decision function \( D: S \to A \) where \( S \) represents system states, and \( D \) determines which agent takes action at any given time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(C\) (Communication Protocol):&lt;/strong&gt; A messaging function \( C: A \times A \to M \), where \( M \) is a message space, defining how agents share information.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(E\) (Execution Model):&lt;/strong&gt; A function \( E: A \times I \to O \) where \( I \) is the input space and \( O \) is the output space, defining how agents perform tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When building &lt;code&gt;Patterns&lt;/code&gt;, we generall y classify them among one of the following categories, though others exist:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Agentic&lt;/strong&gt; &lt;code&gt;Pattern&lt;/code&gt; &lt;strong&gt;categories&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Swarm&lt;/code&gt; (Decentralized)&lt;/td&gt; 
   &lt;td&gt;Agents share tasks and self-assign responsibilities without a central orchestrator. Handoffs occur dynamically. &lt;em&gt;An example of a peer-to-peer agentic pattern is the &lt;code&gt;CTF Agentic Pattern&lt;/code&gt;, which involves a team of agents working together to solve a CTF challenge with dynamic handoffs.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Hierarchical&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A top-level agent (e.g., "PlannerAgent") assigns tasks via structured handoffs to specialized sub-agents. Alternatively, the structure of the agents is harcoded into the agentic pattern with pre-defined handoffs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Chain-of-Thought&lt;/code&gt; (Sequential Workflow)&lt;/td&gt; 
   &lt;td&gt;A structured pipeline where Agent A produces an output, hands it to Agent B for reuse or refinement, and so on. Handoffs follow a linear sequence. &lt;em&gt;An example of a chain-of-thought agentic pattern is the &lt;code&gt;ReasonerAgent&lt;/code&gt;, which involves a Reasoning-type LLM that provides context to the main agent to solve a CTF challenge with a linear sequence.&lt;/em&gt;[^1]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Auction-Based&lt;/code&gt; (Competitive Allocation)&lt;/td&gt; 
   &lt;td&gt;Agents "bid" on tasks based on priority, capability, or cost. A decision agent evaluates bids and hands off tasks to the best-fit agent.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Recursive&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A single agent continuously refines its own output, treating itself as both executor and evaluator, with handoffs (internal or external) to itself. &lt;em&gt;An example of a recursive agentic pattern is the &lt;code&gt;CodeAgent&lt;/code&gt; (when used as a recursive agent), which continuously refines its own output by executing code and updating its own instructions.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more information and examples of common agentic patterns, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/agent_patterns/README.md"&gt;examples folder&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ”¹ Turns and Interactions&lt;/h3&gt; 
&lt;p&gt;During the agentic flow (conversation), we distinguish between &lt;strong&gt;interactions&lt;/strong&gt; and &lt;strong&gt;turns&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Interactions&lt;/strong&gt; are sequential exchanges between one or multiple agents. Each agent executing its logic corresponds with one &lt;em&gt;interaction&lt;/em&gt;. Since an &lt;code&gt;Agent&lt;/code&gt; in CAI generally implements the &lt;code&gt;ReACT&lt;/code&gt; agent model[^3], each &lt;em&gt;interaction&lt;/em&gt; consists of 1) a reasoning step via an LLM inference and 2) act by calling zero-to-n &lt;code&gt;Tools&lt;/code&gt;. This is defined in&lt;code&gt;process_interaction()&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Turns&lt;/strong&gt;: A turn represents a cycle of one ore more &lt;strong&gt;interactions&lt;/strong&gt; which finishes when the &lt;code&gt;Agent&lt;/code&gt; (or &lt;code&gt;Pattern&lt;/code&gt;) executing returns &lt;code&gt;None&lt;/code&gt;, judging there're no further actions to undertake. This is defined in &lt;code&gt;run()&lt;/code&gt;, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] CAI Agents are not related to Assistants in the Assistants API. They are named similarly for convenience, but are otherwise completely unrelated. CAI is entirely powered by the Chat Completions API and is hence stateless between calls.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ”¹ Tracing&lt;/h3&gt; 
&lt;p&gt;CAI implements AI observability by adopting the OpenTelemetry standard and to do so, it leverages &lt;a href="https://github.com/Arize-ai/phoenix"&gt;Phoenix&lt;/a&gt; which provides comprehensive tracing capabilities through OpenTelemetry-based instrumentation, allowing you to monitor and analyze your security operations in real-time. This integration enables detailed visibility into agent interactions, tool usage, and attack vectors throughout penetration testing workflows, making it easier to debug complex exploitation chains, track vulnerability discovery processes, and optimize agent performance for more effective security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/tracing.png" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ”¹ Human-In-The-Loop (HITL)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚                                 â”‚
                      â”‚      Cybersecurity AI (CAI)     â”‚
                      â”‚                                 â”‚
                      â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
                      â”‚       â”‚  Autonomous AI  â”‚       â”‚
                      â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                      â”‚                â”‚                â”‚
                      â”‚                â”‚                â”‚
                      â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
                      â”‚       â”‚ HITL Interaction â”‚      â”‚
                      â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                      â”‚                â”‚                â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â”‚ Ctrl+C (cli.py)
                                       â”‚
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   Human Operator(s)   â”‚
                           â”‚  Expertise | Judgment â”‚
                           â”‚    Teleoperation      â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CAI delivers a framework for building Cybersecurity AIs with a strong emphasis on &lt;em&gt;semi-autonomous&lt;/em&gt; operation, as the reality is that &lt;strong&gt;fully-autonomous&lt;/strong&gt; cybersecurity systems remain premature and face significant challenges when tackling complex tasks. While CAI explores autonomous capabilities, we recognize that effective security operations still require human teleoperation providing expertise, judgment, and oversight in the security process.&lt;/p&gt; 
&lt;p&gt;Accordingly, the Human-In-The-Loop (&lt;code&gt;HITL&lt;/code&gt;) module is a core design principle of CAI, acknowledging that human intervention and teleoperation are essential components of responsible security testing. Through the &lt;code&gt;cli.py&lt;/code&gt; interface, users can seamlessly interact with agents at any point during execution by simply pressing &lt;code&gt;Ctrl+C&lt;/code&gt;. This is implemented across &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt; and also in the REPL abstractions &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl"&gt;REPL&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸš€&lt;/span&gt; Quickstart&lt;/h2&gt; 
&lt;p&gt;To start CAI after installing it, just type &lt;code&gt;cai&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;â””â”€# cai

          CCCCCCCCCCCCC      ++++++++   ++++++++      IIIIIIIIII
       CCC::::::::::::C  ++++++++++       ++++++++++  I::::::::I
     CC:::::::::::::::C ++++++++++         ++++++++++ I::::::::I
    C:::::CCCCCCCC::::C +++++++++    ++     +++++++++ II::::::II
   C:::::C       CCCCCC +++++++     +++++     +++++++   I::::I
  C:::::C                +++++     +++++++     +++++    I::::I
  C:::::C                ++++                   ++++    I::::I
  C:::::C                 ++                     ++     I::::I
  C:::::C                  +   +++++++++++++++   +      I::::I
  C:::::C                    +++++++++++++++++++        I::::I
  C:::::C                     +++++++++++++++++         I::::I
   C:::::C       CCCCCC        +++++++++++++++          I::::I
    C:::::CCCCCCCC::::C         +++++++++++++         II::::::II
     CC:::::::::::::::C           +++++++++           I::::::::I
       CCC::::::::::::C             +++++             I::::::::I
          CCCCCCCCCCCCC               ++              IIIIIIIIII

                      Cybersecurity AI (CAI), vX.Y.Z
                          Bug bounty-ready AI

CAI&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That should initialize CAI and provide a prompt to execute any security task you want to perform. The navigation bar at the bottom displays important system information. This information helps you understand your environment while working with CAI.&lt;/p&gt; 
&lt;p&gt;Here's a quick &lt;a href="https://asciinema.org/a/zm7wS5DA2o0S9pu1Tb44pnlvy"&gt;demo video&lt;/a&gt; to help you get started with CAI. We'll walk through the basic steps â€” from launching the tool to running your first AI-powered task in the terminal. Whether you're a beginner or just curious, this guide will show you how easy it is to begin using CAI.&lt;/p&gt; 
&lt;p&gt;From here on, type on &lt;code&gt;CAI&lt;/code&gt; and start your security exercise. Best way to learn is by example:&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;For using private models, you are given a &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file. Copy it and rename it as &lt;code&gt;.env&lt;/code&gt;. Fill in your corresponding API keys, and you are ready to use CAI.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;List of Environment Variables&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Variable&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_NAME&lt;/td&gt; 
    &lt;td&gt;Name of the CTF challenge to run (e.g. "picoctf_static_flag")&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_CHALLENGE&lt;/td&gt; 
    &lt;td&gt;Specific challenge name within the CTF to test&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_SUBNET&lt;/td&gt; 
    &lt;td&gt;Network subnet for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_IP&lt;/td&gt; 
    &lt;td&gt;IP address for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_INSIDE&lt;/td&gt; 
    &lt;td&gt;Whether to conquer the CTF from within container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for agents&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_DEBUG&lt;/td&gt; 
    &lt;td&gt;Set debug output level (0: Only tool outputs, 1: Verbose debug output, 2: CLI debug output)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_BRIEF&lt;/td&gt; 
    &lt;td&gt;Enable/disable brief output mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MAX_TURNS&lt;/td&gt; 
    &lt;td&gt;Maximum number of turns for agent interactions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_TRACING&lt;/td&gt; 
    &lt;td&gt;Enable/disable OpenTelemetry tracing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_AGENT_TYPE&lt;/td&gt; 
    &lt;td&gt;Specify the agents to use (boot2root, one_tool...)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_STATE&lt;/td&gt; 
    &lt;td&gt;Enable/disable stateful mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY&lt;/td&gt; 
    &lt;td&gt;Enable/disable memory mode (episodic, semantic, all)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable online memory mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_OFFLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable offline memory&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_ENV_CONTEXT&lt;/td&gt; 
    &lt;td&gt;Add dirs and current env to llm context&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between online memory updates&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_PRICE_LIMIT&lt;/td&gt; 
    &lt;td&gt;Price limit for the conversation in dollars&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_REPORT&lt;/td&gt; 
    &lt;td&gt;Enable/disable reporter mode (ctf, nis2, pentesting)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for the support agent&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between support agent executions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE&lt;/td&gt; 
    &lt;td&gt;Defines the name of the workspace&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE_DIR&lt;/td&gt; 
    &lt;td&gt;Specifies the directory path where the workspace is located&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;The Cybersecurity AI (CAI) platform offers seamless integration with OpenRouter, a unified interface for Large Language Models (LLMs). This integration is crucial for users who wish to leverage advanced AI capabilities in their cybersecurity tasks. OpenRouter acts as a bridge, allowing CAI to communicate with various LLMs, thereby enhancing the flexibility and power of the AI agents used within CAI.&lt;/p&gt; 
&lt;p&gt;To enable OpenRouter support in CAI, you need to configure your environment by adding specific entries to your &lt;code&gt;.env&lt;/code&gt; file. This setup ensures that CAI can interact with the OpenRouter API, facilitating the use of sophisticated models like Meta-LLaMA. Hereâ€™s how you can configure it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_AGENT_TYPE=redteam_agent
CAI_MODEL=openrouter/meta-llama/llama-4-maverick
OPENROUTER_API_KEY=&amp;lt;sk-your-key&amp;gt;  # note, add yours
OPENROUTER_API_BASE=https://openrouter.ai/api/v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP&lt;/h3&gt; 
&lt;p&gt;CAI supports the Model Context Protocol (MCP) for integrating external tools and services with AI agents. MCP is supported via two transport mechanisms:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;SSE (Server-Sent Events)&lt;/strong&gt; - For web-based servers that push updates over HTTP connections:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load http://localhost:9876/sse burp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;STDIO (Standard Input/Output)&lt;/strong&gt; - For local inter-process communication:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load stdio myserver python mcp_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once connected, you can add the MCP tools to any agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp add burp redteam_agent
Adding tools from MCP server 'burp' to agent 'Red Team Agent'...
                                 Adding tools to Red Team Agent
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Tool                              â”ƒ Status â”ƒ Details                                         â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ send_http_request                 â”‚ Added  â”‚ Available as: send_http_request                 â”‚
â”‚ create_repeater_tab               â”‚ Added  â”‚ Available as: create_repeater_tab               â”‚
â”‚ send_to_intruder                  â”‚ Added  â”‚ Available as: send_to_intruder                  â”‚
â”‚ url_encode                        â”‚ Added  â”‚ Available as: url_encode                        â”‚
â”‚ url_decode                        â”‚ Added  â”‚ Available as: url_decode                        â”‚
â”‚ base64encode                      â”‚ Added  â”‚ Available as: base64encode                      â”‚
â”‚ base64decode                      â”‚ Added  â”‚ Available as: base64decode                      â”‚
â”‚ generate_random_string            â”‚ Added  â”‚ Available as: generate_random_string            â”‚
â”‚ output_project_options            â”‚ Added  â”‚ Available as: output_project_options            â”‚
â”‚ output_user_options               â”‚ Added  â”‚ Available as: output_user_options               â”‚
â”‚ set_project_options               â”‚ Added  â”‚ Available as: set_project_options               â”‚
â”‚ set_user_options                  â”‚ Added  â”‚ Available as: set_user_options                  â”‚
â”‚ get_proxy_http_history            â”‚ Added  â”‚ Available as: get_proxy_http_history            â”‚
â”‚ get_proxy_http_history_regex      â”‚ Added  â”‚ Available as: get_proxy_http_history_regex      â”‚
â”‚ get_proxy_websocket_history       â”‚ Added  â”‚ Available as: get_proxy_websocket_history       â”‚
â”‚ get_proxy_websocket_history_regex â”‚ Added  â”‚ Available as: get_proxy_websocket_history_regex â”‚
â”‚ set_task_execution_engine_state   â”‚ Added  â”‚ Available as: set_task_execution_engine_state   â”‚
â”‚ set_proxy_intercept_state         â”‚ Added  â”‚ Available as: set_proxy_intercept_state         â”‚
â”‚ get_active_editor_contents        â”‚ Added  â”‚ Available as: get_active_editor_contents        â”‚
â”‚ set_active_editor_contents        â”‚ Added  â”‚ Available as: set_active_editor_contents        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Added 20 tools from server 'burp' to agent 'Red Team Agent'.
CAI&amp;gt;/agent 13
CAI&amp;gt;Create a repeater tab
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can list all active MCP connections and their transport types:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f"&gt;https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;Development is facilitated via VS Code dev. environments. To try out our development environment, clone the repository, open VS Code and enter de dev. container mode:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_devenv.gif" alt="CAI Development Environment" /&gt;&lt;/p&gt; 
&lt;h3&gt;Contributions&lt;/h3&gt; 
&lt;p&gt;If you want to contribute to this project, use &lt;a href="https://pre-commit.com/"&gt;&lt;strong&gt;Pre-commit&lt;/strong&gt;&lt;/a&gt; before your MR&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit
pre-commit # files staged
pre-commit run --all-files # all files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Requirements: caiextensions&lt;/h3&gt; 
&lt;p&gt;Currently, the extensions are not publicly available as the engineering endeavour to maintain them is significant. Instead, we're making selected custom caiextensions available for partner companies across collaborations.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;â„¹&lt;/span&gt; Usage Data Collection&lt;/h3&gt; 
&lt;p&gt;CAI is provided free of charge for researchers. To improve CAIâ€™s detection accuracy and publish open security research, instead of payment for research use cases, we ask you to contribute to the CAI community by allowing usage data collection. This data helps us identify areas for improvement, understand how the framework is being used, and prioritize new features. Legal basis of data collection is under Art. 6 (1)(f) GDPR â€” CAIâ€™s legitimate interest in maintaining and improving security tooling, with Art. 89 safeguards for research. The collected data includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic system information (OS type, Python version)&lt;/li&gt; 
 &lt;li&gt;Username and IP information&lt;/li&gt; 
 &lt;li&gt;Tool usage patterns and performance metrics&lt;/li&gt; 
 &lt;li&gt;Model interactions and token usage statistics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We take your privacy seriously and only collect what's needed to make CAI better. For further info, reach out to researchï¼ aliasrobotics.com. You can disable some of the data collection features via the &lt;code&gt;CAI_TELEMETRY&lt;/code&gt; environment variable but we encourage you to keep it enabled and contribute back to research:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_TELEMETRY=False cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reproduce CI-Setup locally&lt;/h3&gt; 
&lt;p&gt;To simulate the CI/CD pipeline, you can run the following in the Gitlab runner machines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it \
  --privileged \
  --network=exploitflow_net \
  --add-host="host.docker.internal:host-gateway" \
  -v /cache:/cache \
  -v /var/run/docker.sock:/var/run/docker.sock:rw \
  registry.gitlab.com/aliasrobotics/alias_research/cai:latest bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;OLLAMA is giving me 404 errors&lt;/summary&gt; 
 &lt;p&gt;Ollama's API in OpenAI mode uses &lt;code&gt;/v1/chat/completions&lt;/code&gt; whereas the &lt;code&gt;openai&lt;/code&gt; library uses &lt;code&gt;base_url&lt;/code&gt; + &lt;code&gt;/chat/completions&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;We adopt the latter for overall alignment with the gen AI community and empower the former by allowing users to add the &lt;code&gt;v1&lt;/code&gt; themselves via:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE=http://IP:PORT/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See the following issues that treat this topic in more detail:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/76"&gt;https://github.com/aliasrobotics/cai/issues/76&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/83"&gt;https://github.com/aliasrobotics/cai/issues/83&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/82"&gt;https://github.com/aliasrobotics/cai/issues/82&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Where are all the caiextensions?&lt;/summary&gt; 
 &lt;p&gt;See &lt;a href="https://gitlab.com/aliasrobotics/alias_research/caiextensions"&gt;all caiextensions&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I install the report caiextension?&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;See here&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I set up SSH access for Gitlab?&lt;/summary&gt; 
 &lt;p&gt;Generate a new SSH key&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-keygen -t ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the key to the SSH agent&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-add ~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the public key to Gitlab Copy the key and add it to Gitlab under &lt;a href="https://gitlab.com/-/user_settings/ssh_keys"&gt;https://gitlab.com/-/user_settings/ssh_keys&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cat ~/.ssh/id_ed25519.pub
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To verify it:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh -T git@gitlab.com
Welcome to GitLab, @vmayoral!
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I clear Python cache?&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;find . -name "*.pyc" -delete &amp;amp;&amp;amp; find . -name "__pycache__" -delete
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;If host networking is not working with ollama check whether it has been disabled in Docker because you are not signed in&lt;/summary&gt; 
 &lt;p&gt;Docker in OS X behaves funny sometimes. Check if the following message has shown up:&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Host networking has been disabled because you are not signed in. Please sign in to enable it&lt;/em&gt;.&lt;/p&gt; 
 &lt;p&gt;Make sure this has been addressed and also that the Dev Container is not forwarding the 8000 port (click on x, if necessary in the ports section).&lt;/p&gt; 
 &lt;p&gt;To verify connection, from within the VSCode devcontainer:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -v http://host.docker.internal:8000/api/version
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Run CAI against any target&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-004-first-message.png" alt="cai-004-first-message" /&gt;&lt;/p&gt; 
 &lt;p&gt;The starting user prompt in this case is: &lt;code&gt;Target IP: 192.168.3.10, perform a full network scan&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;The agent started performing a nmap scan. You could either interact with the agent and give it more instructions, or let it run to see what it explores next.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I interact with the agent? Type twice CTRL + C &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-005-ctrl-c.png" alt="cai-005-ctrl-c" /&gt;&lt;/p&gt; 
 &lt;p&gt;If you want to use the HITL mode, you can do it by presssing twice &lt;code&gt;Ctrl + C&lt;/code&gt;. This will allow you to interact (prompt) with the agent whenever you want. The agent will not lose the previous context, as it is stored in the &lt;code&gt;history&lt;/code&gt; variable, which is passed to it and any agent that is called. This enables any agent to use the previous information and be more accurate and efficient.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Can I change the model while CAI is running? /model &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/model&lt;/code&gt; to change the model.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-007-model-change.png" alt="cai-007-model-change" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I list all the agents available? /agent &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/agent&lt;/code&gt; to list all the agents available.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-010-agents-menu.png" alt="cai-010-agents-menu" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Where can I list all the environment variables? /config &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-008-config.png" alt="cai-008-config" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; How to know more about the CLI? /help &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-006-help.png" alt="cai-006-help" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I trace the whole execution?&lt;/summary&gt; The environment variable `CAI_TRACING` allows the user to set it to `CAI_TRACING=true` to enable tracing, or `CAI_TRACING=false` to disable it. When CAI is prompted by the first time, the user is provided with two paths, the execution log, and the tracing log. 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using previous run logs?&lt;/summary&gt; 
 &lt;p&gt;Absolutely! The &lt;strong&gt;memory extension&lt;/strong&gt; allows you to use a previously sucessful runs ( the log object is stored as a &lt;strong&gt;.jsonl file in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/logs"&gt;log&lt;/a&gt; folder&lt;/strong&gt; ) in a new run against the same target. The user is also given the path highlighted in orange as shown below.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
 &lt;p&gt;How to make use of this functionality?&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Run CAI against the target. Let's assume the target name is: &lt;code&gt;target001&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;Get the log file path, something like: &lt;code&gt;logs/cai_20250408_111856.jsonl&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Generate the memory using any model of your preference: &lt;code&gt;shell JSONL_FILE_PATH="logs/cai_20250408_111856.jsonl" CTF_INSIDE="false" CAI_MEMORY_COLLECTION="target001" CAI_MEMORY="episodic" CAI_MODEL="claude-3-5-sonnet-20241022" python3 tools/2_jsonl_to_memory.py &lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The script &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/tools/2_jsonl_to_memory.py"&gt;&lt;code&gt;tools/2_jsonl_to_memory.py&lt;/code&gt;&lt;/a&gt; will generate a memory collection file with the most relevant steps. The quality of the memory collection will depend on the model you use.&lt;/p&gt; 
 &lt;ol start="4"&gt; 
  &lt;li&gt;Use the generated memory collection and execute a new run: &lt;code&gt;shell CAI_MEMORY="episodic" CAI_MODEL="gpt-4o" CAI_MEMORY_COLLECTION="target001" CAI_TRACING=false python3 cai/cli.py&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using scripts or extra information?&lt;/summary&gt; 
 &lt;p&gt;Currently, CAI supports text based information. You can add any extra information on the target you are facing by copy-pasting it directly into the system or user prompt.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; By adding it to the system (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/system_master_template.md"&gt;&lt;code&gt;system_master_template.md&lt;/code&gt;&lt;/a&gt;) or the user prompt (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/user_master_template.md"&gt;&lt;code&gt;user_master_template.md&lt;/code&gt;&lt;/a&gt;). You can always directly prompt the path to the model, and it will &lt;code&gt;cat&lt;/code&gt; it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How CAI licence works?&lt;/summary&gt; 
 &lt;p&gt;CAIâ€™s current license does not restrict usage for research purposes. You are free to use CAI for security assessments (pentests), to develop additional features, and to integrate it into your research activities, as long as you comply with local laws.&lt;/p&gt; 
 &lt;p&gt;If you or your organization start benefiting commercially from CAI (e.g., offering pentesting services powered by CAI), then a commercial license will be required to help sustain the project.&lt;/p&gt; 
 &lt;p&gt;CAI itself is not a profit-seeking initiative. Our goal is to build a sustainable open-source project. We simply ask that those who profit from CAI contribute back and support our ongoing development.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to cite our work, please use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025caiopenbugbountyready,
      title={CAI: An Open, Bug Bounty-Ready Cybersecurity AI},
      author={VÃ­ctor Mayoral-Vilches and Luis Javier Navarrete-Lozano and MarÃ­a Sanz-GÃ³mez and Lidia Salas Espejo and MartiÃ±o Crespo-Ãlvarez and Francisco Oca-Gonzalez and Francesco Balassone and Alfonso Glera-PicÃ³n and Unai Ayucar-Carbajo and Jon Ander Ruiz-Alcalde and Stefan Rass and Martin Pinzger and Endika Gil-Uriarte},
      year={2025},
      eprint={2504.06017},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2504.06017},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025cybersecurityaidangerousgap,
      title={Cybersecurity AI: The Dangerous Gap Between Automation and Autonomy}, 
      author={VÃ­ctor Mayoral-Vilches},
      year={2025},
      eprint={2506.23592},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.23592}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025caifluencyframeworkcybersecurity,
      title={CAI Fluency: A Framework for Cybersecurity AI Fluency}, 
      author={VÃ­ctor Mayoral-Vilches and Jasmin Wachter and CristÃ³bal R. J. Veas Chavez and Cathrin Schachner and Luis Javier Navarrete-Lozano and MarÃ­a Sanz-GÃ³mez},
      year={2025},
      eprint={2508.13588},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2508.13588}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;CAI was initially developed by &lt;a href="https://aliasrobotics.com"&gt;Alias Robotics&lt;/a&gt; and co-funded by the European EIC accelerator project RIS (GA 101161136) - HORIZON-EIC-2023-ACCELERATOR-01 call. The original agentic principles are inspired from OpenAI's &lt;a href="https://github.com/openai/swarm"&gt;&lt;code&gt;swarm&lt;/code&gt;&lt;/a&gt; library and translated into newer prototypes. This project also makes use of other relevant open source building blocks including &lt;a href="https://github.com/BerriAI/litellm"&gt;&lt;code&gt;LiteLLM&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Academic Collaborations&lt;/h3&gt; 
&lt;p&gt;CAI benefits from ongoing research collaborations with academic institutions. Researchers interested in collaborative projects, dataset access, or academic licenses should contact &lt;a href="mailto:research@aliasrobotics.com"&gt;research@aliasrobotics.com&lt;/a&gt;. We provide special support for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PhD research projects&lt;/li&gt; 
 &lt;li&gt;Academic benchmarking studies&lt;/li&gt; 
 &lt;li&gt;Security education initiatives&lt;/li&gt; 
 &lt;li&gt;Open-source contributions from research labs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Footnotes --&gt; 
&lt;p&gt;[^1]: Arguably, the Chain-of-Thought agentic pattern is a special case of the Hierarchical agentic pattern. [^2]: Kamhoua, C. A., Leslie, N. O., &amp;amp; Weisman, M. J. (2018). Game theoretic modeling of advanced persistent threat in internet of things. Journal of Cyber Security and Information Systems. [^3]: Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp;amp; Cao, Y. (2023, January). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). [^4]: Deng, G., Liu, Y., Mayoral-Vilches, V., Liu, P., Li, Y., Xu, Y., ... &amp;amp; Rass, S. (2024). {PentestGPT}: Evaluating and harnessing large language models for automated penetration testing. In 33rd USENIX Security Symposium (USENIX Security 24) (pp. 847-864).&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>