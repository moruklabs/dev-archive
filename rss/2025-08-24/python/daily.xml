<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 23 Aug 2025 01:35:54 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>mem0ai/mem0</title>
      <link>https://github.com/mem0ai/mem0</link>
      <description>&lt;p&gt;Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/mem0ai/mem0"&gt; &lt;img src="https://raw.githubusercontent.com/mem0ai/mem0/main/docs/images/banner-sm.png" width="800px" alt="Mem0 - The Memory Layer for Personalized AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"&gt; &lt;a href="https://trendshift.io/repositories/11194" target="blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/11194" alt="mem0ai%2Fmem0 | Trendshift" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mem0.ai"&gt;Learn more&lt;/a&gt; ¬∑ &lt;a href="https://mem0.dev/DiG"&gt;Join Discord&lt;/a&gt; ¬∑ &lt;a href="https://mem0.dev/demo"&gt;Demo&lt;/a&gt; ¬∑ &lt;a href="https://mem0.dev/openmemory"&gt;OpenMemory&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mem0.dev/DiG"&gt; &lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Mem0 Discord" /&gt; &lt;/a&gt; &lt;a href="https://pepy.tech/project/mem0ai"&gt; &lt;img src="https://img.shields.io/pypi/dm/mem0ai" alt="Mem0 PyPI - Downloads" /&gt; &lt;/a&gt; &lt;a href="https://github.com/mem0ai/mem0"&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square" alt="GitHub commit activity" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/mem0ai" target="blank"&gt; &lt;img src="https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;amp;label=pypi%20package" alt="Package version" /&gt; &lt;/a&gt; &lt;a href="https://www.npmjs.com/package/mem0ai" target="blank"&gt; &lt;img src="https://img.shields.io/npm/v/mem0ai" alt="Npm package" /&gt; &lt;/a&gt; &lt;a href="https://www.ycombinator.com/companies/mem0"&gt; &lt;img src="https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square" alt="Y Combinator S24" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://mem0.ai/research"&gt;&lt;strong&gt;üìÑ Building Production-Ready AI Agents with Scalable Long-Term Memory ‚Üí&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;‚ö° +26% Accuracy vs. OpenAI Memory ‚Ä¢ üöÄ 91% Faster ‚Ä¢ üí∞ 90% Fewer Tokens&lt;/strong&gt; &lt;/p&gt; 
&lt;h2&gt;üî• Research Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;+26% Accuracy&lt;/strong&gt; over OpenAI Memory on the LOCOMO benchmark&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;91% Faster Responses&lt;/strong&gt; than full-context, ensuring low-latency at scale&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;90% Lower Token Usage&lt;/strong&gt; than full-context, cutting costs without compromise&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mem0.ai/research"&gt;Read the full paper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://mem0.ai"&gt;Mem0&lt;/a&gt; ("mem-zero") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time‚Äîideal for customer support chatbots, AI assistants, and autonomous systems.&lt;/p&gt; 
&lt;h3&gt;Key Features &amp;amp; Use Cases&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Core Capabilities:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Level Memory&lt;/strong&gt;: Seamlessly retains User, Session, and Agent state with adaptive personalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Developer-Friendly&lt;/strong&gt;: Intuitive API, cross-platform SDKs, and a fully managed service option&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Applications:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI Assistants&lt;/strong&gt;: Consistent, context-rich conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customer Support&lt;/strong&gt;: Recall past tickets and user history for tailored help&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;: Track patient preferences and history for personalized care&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Productivity &amp;amp; Gaming&lt;/strong&gt;: Adaptive workflows and environments based on user behavior&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quickstart Guide &lt;a name="quickstart"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Choose between our hosted platform or self-hosted package:&lt;/p&gt; 
&lt;h3&gt;Hosted Platform&lt;/h3&gt; 
&lt;p&gt;Get up and running in minutes with automatic updates, analytics, and enterprise security.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Sign up on &lt;a href="https://app.mem0.ai"&gt;Mem0 Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Embed the memory layer via SDK or API keys&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Self-Hosted (Open Source)&lt;/h3&gt; 
&lt;p&gt;Install the sdk via pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mem0ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install sdk via npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install mem0ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;Mem0 requires an LLM to function, with &lt;code&gt;gpt-4o-mini&lt;/code&gt; from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our &lt;a href="https://docs.mem0.ai/components/llms/overview"&gt;Supported LLMs documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;First step is to instantiate the memory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = "default_user") -&amp;gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = "\n".join(f"- {entry['memory']}" for entry in relevant_memories["results"])

    # Generate Assistant response
    system_prompt = f"You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}"
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": message}]
    response = openai_client.chat.completions.create(model="gpt-4o-mini", messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({"role": "assistant", "content": assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print("Chat with AI (type 'exit' to quit)")
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() == 'exit':
            print("Goodbye!")
            break
        print(f"AI: {chat_with_memories(user_input)}")

if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed integration steps, see the &lt;a href="https://docs.mem0.ai/quickstart"&gt;Quickstart&lt;/a&gt; and &lt;a href="https://docs.mem0.ai/api-reference"&gt;API Reference&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üîó Integrations &amp;amp; Demos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ChatGPT with Memory&lt;/strong&gt;: Personalized chat powered by Mem0 (&lt;a href="https://mem0.dev/demo"&gt;Live Demo&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;: Store memories across ChatGPT, Perplexity, and Claude (&lt;a href="https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb"&gt;Chrome Extension&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Langgraph Support&lt;/strong&gt;: Build a customer bot with Langgraph + Mem0 (&lt;a href="https://docs.mem0.ai/integrations/langgraph"&gt;Guide&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Integration&lt;/strong&gt;: Tailor CrewAI outputs with Mem0 (&lt;a href="https://docs.mem0.ai/integrations/crewai"&gt;Example&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìö Documentation &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Full docs: &lt;a href="https://docs.mem0.ai"&gt;https://docs.mem0.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Community: &lt;a href="https://mem0.dev/DiG"&gt;Discord&lt;/a&gt; ¬∑ &lt;a href="https://x.com/mem0ai"&gt;Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Contact: &lt;a href="mailto:founders@mem0.ai"&gt;founders@mem0.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We now have a paper you can cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{mem0,
  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},
  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},
  journal={arXiv preprint arXiv:2504.19413},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚öñÔ∏è License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 ‚Äî see the &lt;a href="https://raw.githubusercontent.com/mem0ai/mem0/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;üåê Make websites accessible for AI agents. Automate tasks online with ease.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="./static/browser-use-dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="./static/browser-use.png" /&gt; 
 &lt;img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://raw.githubusercontent.com/browser-use/browser-use/main/static/browser-use.png" width="full" /&gt; 
&lt;/picture&gt; 
&lt;h1 align="center"&gt;Enable AI to control your browser ü§ñ&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/gregpr07/browser-use/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/gregpr07/browser-use?style=social" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://link.browser-use.com/discord"&gt;&lt;img src="https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-blue" alt="Cloud" /&gt;&lt;/a&gt; &lt;a href="https://docs.browser-use.com"&gt;&lt;img src="https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&amp;amp;labelColor=#EC6341" alt="Weave Badge" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üå§Ô∏è Want to skip the setup? Use our &lt;b&gt;&lt;a href="https://cloud.browser-use.com"&gt;cloud&lt;/a&gt;&lt;/b&gt; for faster, scalable, stealth-enabled browser automation!&lt;/p&gt; 
&lt;h1&gt;Quick start&lt;/h1&gt; 
&lt;p&gt;With pip (Python&amp;gt;=3.11):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install browser-use
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you don't already have Chrome or Chromium installed, you can also download the latest Chromium using playwright's install shortcut:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx playwright install chromium --with-deps --no-shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Spin up your agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from dotenv import load_dotenv
load_dotenv()
from browser_use import Agent, ChatOpenAI

async def main():
    agent = Agent(
        task="Find the number of stars of the browser-use repo",
        llm=ChatOpenAI(model="gpt-4.1-mini"),
    )
    await agent.run()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add your API keys for the provider you want to use to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For other settings, models, and more, check out the &lt;a href="https://docs.browser-use.com"&gt;documentation üìï&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/shopping.py"&gt;Task&lt;/a&gt;: Add grocery items to cart, and checkout.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=L2Ya9PYNns8"&gt;&lt;img src="https://github.com/user-attachments/assets/a0ffd23d-9a11-4368-8893-b092703abc14" alt="AI Did My Groceries" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;Prompt: Add my latest LinkedIn follower to my leads in Salesforce.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/50d6e691-b66b-4077-a46c-49e9d4707e07" alt="LinkedIn to Salesforce" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/find_and_apply_to_jobs.py"&gt;Prompt&lt;/a&gt;: Read my CV &amp;amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.'&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04"&gt;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py"&gt;Prompt&lt;/a&gt;: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa" alt="Letter to Papa" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/custom-functions/save_to_file_hugging_face.py"&gt;Prompt&lt;/a&gt;: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3"&gt;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;More examples&lt;/h2&gt; 
&lt;p&gt;For more examples see the &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/examples"&gt;examples&lt;/a&gt; folder or join the &lt;a href="https://link.browser-use.com/discord"&gt;Discord&lt;/a&gt; and show off your project. You can also see our &lt;a href="https://github.com/browser-use/awesome-prompts"&gt;&lt;code&gt;awesome-prompts&lt;/code&gt;&lt;/a&gt; repo for prompting inspiration.&lt;/p&gt; 
&lt;h2&gt;MCP Integration&lt;/h2&gt; 
&lt;p&gt;Browser-use supports the &lt;a href="https://modelcontextprotocol.io/"&gt;Model Context Protocol (MCP)&lt;/a&gt;, enabling integration with Claude Desktop and other MCP-compatible clients.&lt;/p&gt; 
&lt;h3&gt;Use as MCP Server with Claude Desktop&lt;/h3&gt; 
&lt;p&gt;Add browser-use to your Claude Desktop configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "browser-use": {
      "command": "uvx",
      "args": ["browser-use[cli]", "--mcp"],
      "env": {
        "OPENAI_API_KEY": "sk-..."
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This gives Claude Desktop access to browser automation tools for web scraping, form filling, and more.&lt;/p&gt; 
&lt;h3&gt;Connect External MCP Servers to Browser-Use Agent&lt;/h3&gt; 
&lt;p&gt;Browser-use agents can connect to multiple external MCP servers to extend their capabilities:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from browser_use import Agent, Controller, ChatOpenAI
from browser_use.mcp.client import MCPClient

async def main():
    # Initialize controller
    controller = Controller()

    # Connect to multiple MCP servers
    filesystem_client = MCPClient(
        server_name="filesystem",
        command="npx",
        args=["-y", "@modelcontextprotocol/server-filesystem", "/Users/me/documents"]
    )

    github_client = MCPClient(
        server_name="github",
        command="npx",
        args=["-y", "@modelcontextprotocol/server-github"],
        env={"GITHUB_TOKEN": "your-github-token"}
    )

    # Connect and register tools from both servers
    await filesystem_client.connect()
    await filesystem_client.register_to_controller(controller)

    await github_client.connect()
    await github_client.register_to_controller(controller)

    # Create agent with MCP-enabled controller
    agent = Agent(
        task="Find the latest pdf report in my documents and create a GitHub issue about it",
        llm=ChatOpenAI(model="gpt-4.1-mini"),
        controller=controller  # Controller has tools from both MCP servers
    )

    # Run the agent
    await agent.run()

    # Cleanup
    await filesystem_client.disconnect()
    await github_client.disconnect()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://docs.browser-use.com/customize/mcp-server"&gt;MCP documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h1&gt;Vision&lt;/h1&gt; 
&lt;p&gt;Tell your computer what to do, and it gets it done.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;h3&gt;Agent&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Make agent 3x faster&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Reduce token consumption (system prompt, DOM state)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;DOM Extraction&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Enable interaction with all UI elements&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Improve state representation for UI elements so that any LLM can understand what's on the page&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Workflows&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Let user record a workflow - which we can rerun with browser-use as a fallback&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Experience&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Create various templates for tutorial execution, job application, QA testing, social media, etc. which users can just copy &amp;amp; paste.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Parallelization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Human work is sequential. The real power of a browser agent comes into reality if we can parallelize similar tasks. For example, if you want to find contact information for 100 companies, this can all be done in parallel and reported back to a main agent, which processes the results and kicks off parallel subtasks again.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the &lt;code&gt;/docs&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h2&gt;üß™ How to make your agents robust?&lt;/h2&gt; 
&lt;p&gt;We offer to run your tasks in our CI‚Äîautomatically, on every update!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Add your task:&lt;/strong&gt; Add a YAML file in &lt;code&gt;tests/agent_tasks/&lt;/code&gt; (see the &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/tests/agent_tasks/README.md"&gt;&lt;code&gt;README there&lt;/code&gt;&lt;/a&gt; for details).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic validation:&lt;/strong&gt; Every time we push updates, your task will be run by the agent and evaluated using your criteria.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Local Setup&lt;/h2&gt; 
&lt;p&gt;To learn more about the library, check out the &lt;a href="https://docs.browser-use.com/development/local-setup"&gt;local setup üìï&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;main&lt;/code&gt; is the primary development branch with frequent changes. For production use, install a stable &lt;a href="https://github.com/browser-use/browser-use/releases"&gt;versioned release&lt;/a&gt; instead.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Swag&lt;/h2&gt; 
&lt;p&gt;Want to show off your Browser-use swag? Check out our &lt;a href="https://browsermerch.com"&gt;Merch store&lt;/a&gt;. Good contributors will receive swag for free üëÄ.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use Browser Use in your research or project, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{browser_use2024,
  author = {M√ºller, Magnus and ≈Ωuniƒç, Gregor},
  title = {Browser Use: Enable AI to control your browser},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/browser-use/browser-use}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f" width="400" /&gt; 
 &lt;p&gt;&lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  Made with ‚ù§Ô∏è in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>microsoft/presidio</title>
      <link>https://github.com/microsoft/presidio</link>
      <description>&lt;p&gt;An open-source framework for detecting, redacting, masking, and anonymizing sensitive data (PII) across text, images, and structured data. Supports NLP, pattern matching, and customizable pipelines.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Presidio - Data Protection and De-identification SDK&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Context aware, pluggable and customizable PII de-identification service for text and images.&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://dev.azure.com/csedevil/Presidio/_build/latest?definitionId=212&amp;amp;branchName=main"&gt;&lt;img src="https://dev.azure.com/csedevil/Presidio/_apis/build/status/Presidio-CI%20V2?branchName=main" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="http://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-brightgreen.svg?sanitize=true" alt="MIT license" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/release/Microsoft/presidio.svg?sanitize=true" alt="Release" /&gt; &lt;a href="https://www.bestpractices.dev/projects/6076"&gt;&lt;img src="https://www.bestpractices.dev/projects/6076/badge" alt="OpenSSF Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/presidio-analyzer/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/presidio-analyzer.svg?sanitize=true" alt="PyPI pyversions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Presidio Analyzer &lt;a href="https://img.shields.io/pypi/dm/presidio-analyzer.svg"&gt;&lt;img src="https://img.shields.io/pypi/dm/presidio-analyzer.svg?sanitize=true" alt="Pypi Downloads" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Presidio Anonymizer &lt;a href="https://img.shields.io/pypi/dm/presidio-anonymizer.svg"&gt;&lt;img src="https://img.shields.io/pypi/dm/presidio-anonymizer.svg?sanitize=true" alt="Pypi Downloads" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Presidio Image-Redactor &lt;a href="https://img.shields.io/pypi/dm/presidio-image-redactor.svg"&gt;&lt;img src="https://img.shields.io/pypi/dm/presidio-image-redactor.svg?sanitize=true" alt="Pypi Downloads" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Presidio Structured &lt;a href="https://img.shields.io/pypi/dm/presidio-structured.svg"&gt;&lt;img src="https://img.shields.io/pypi/dm/presidio-structured.svg?sanitize=true" alt="Pypi Downloads" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What is Presidio&lt;/h2&gt; 
&lt;p&gt;Presidio &lt;em&gt;(Origin from Latin praesidium ‚Äòprotection, garrison‚Äô)&lt;/em&gt; helps to ensure sensitive data is properly managed and governed. It provides fast &lt;strong&gt;&lt;em&gt;identification&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;anonymization&lt;/em&gt;&lt;/strong&gt; modules for private entities in text such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/presidio/main/docs/assets/changing_text.gif" alt="Presidio demo gif" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;span&gt;üìò&lt;/span&gt; &lt;a href="https://microsoft.github.io/presidio"&gt;Full documentation&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&lt;span&gt;‚ùì&lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/presidio/main/docs/faq.md"&gt;Frequently Asked Questions&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&lt;span&gt;üí≠&lt;/span&gt; &lt;a href="https://aka.ms/presidio-demo"&gt;Demo&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&lt;span&gt;üõ´&lt;/span&gt; &lt;a href="https://microsoft.github.io/presidio/samples/"&gt;Examples&lt;/a&gt;&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Are you using Presidio? We'd love to know how&lt;/h2&gt; 
&lt;p&gt;Please help us improve by taking &lt;a href="https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR9LagCGNW01LpMix2pnFWFJUQjJDTVkwSlJYRkFPSUNNVlVRRVRWVDVNSy4u"&gt;this short anonymous survey&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Goals&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Allow organizations to preserve privacy in a simpler way by democratizing de-identification technologies and introducing transparency in decisions.&lt;/li&gt; 
 &lt;li&gt;Embrace extensibility and customizability to a specific business need.&lt;/li&gt; 
 &lt;li&gt;Facilitate both fully automated and semi-automated PII de-identification flows on multiple platforms.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Main features&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Predefined&lt;/strong&gt; or &lt;strong&gt;custom PII recognizers&lt;/strong&gt; leveraging &lt;em&gt;Named Entity Recognition&lt;/em&gt;, &lt;em&gt;regular expressions&lt;/em&gt;, &lt;em&gt;rule based logic&lt;/em&gt; and &lt;em&gt;checksum&lt;/em&gt; with relevant context in multiple languages.&lt;/li&gt; 
 &lt;li&gt;Options for connecting to external PII detection models.&lt;/li&gt; 
 &lt;li&gt;Multiple usage options, &lt;strong&gt;from Python or PySpark workloads through Docker to Kubernetes&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizability&lt;/strong&gt; in PII identification and de-identification.&lt;/li&gt; 
 &lt;li&gt;Module for &lt;strong&gt;redacting PII text in images&lt;/strong&gt; (standard image types and DICOM medical images).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; Presidio can help identify sensitive/PII data in un/structured text. However, because it is using automated detection mechanisms, there is no guarantee that Presidio will find all sensitive information. Consequently, additional systems and protections should be employed.&lt;/p&gt; 
&lt;h2&gt;Installing Presidio&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://microsoft.github.io/presidio/installation/#using-pip"&gt;Using pip&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://microsoft.github.io/presidio/installation/#using-docker"&gt;Using Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://microsoft.github.io/presidio/installation/#install-from-source"&gt;From source&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/presidio/main/docs/presidio_V2.md"&gt;Migrating from V1 to V2&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Running Presidio&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://microsoft.github.io/presidio/getting_started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://microsoft.github.io/presidio/development"&gt;Setting up a development environment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://microsoft.github.io/presidio/text_anonymization"&gt;PII de-identification in text&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://microsoft.github.io/presidio/image-redactor"&gt;PII de-identification in images&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://microsoft.github.io/presidio/samples"&gt;Usage samples and example deployments&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Before you submit an issue, please go over the &lt;a href="https://microsoft.github.io/presidio/"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For general discussions, please use the &lt;a href="https://github.com/microsoft/presidio/discussions"&gt;GitHub repo's discussion board&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you have a usage question, found a bug or have a suggestion for improvement, please file a &lt;a href="https://github.com/microsoft/presidio/issues"&gt;GitHub issue&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For other matters, please email &lt;a href="mailto:presidio@microsoft.com"&gt;presidio@microsoft.com&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;For details on contributing to this repository, see the &lt;a href="https://raw.githubusercontent.com/microsoft/presidio/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.microsoft.com"&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;a href="https://github.com/microsoft/presidio/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=microsoft/presidio" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>codelion/openevolve</title>
      <link>https://github.com/codelion/openevolve</link>
      <description>&lt;p&gt;Open-source implementation of AlphaEvolve&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenEvolve&lt;/h1&gt; 
&lt;p&gt;An open-source evolutionary coding agent that began as a faithful implementation of AlphaEvolve and has evolved far beyond it, enabling automated scientific and algorithmic discovery.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/codelion/openevolve/main/openevolve-logo.png" alt="OpenEvolve Logo" /&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;OpenEvolve is an evolutionary coding agent that uses Large Language Models to automatically optimize and discover algorithms through iterative improvement. Starting from the AlphaEvolve research, it incorporates advanced features for reproducibility, multi-language support, sophisticated evaluation pipelines, and integration with cutting-edge LLM optimization techniques. It serves as both a research platform for evolutionary AI and a practical tool for automated code optimization.&lt;/p&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;p&gt;OpenEvolve implements a comprehensive evolutionary coding system with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Evolutionary Coding Agent&lt;/strong&gt;: LLM-guided evolution of entire code files (not just functions)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Controller Loop&lt;/strong&gt;: Asynchronous pipeline coordinating LLMs, evaluators, and databases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Program Database&lt;/strong&gt;: Storage and sampling of evolved programs with evaluation metrics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt Sampling&lt;/strong&gt;: Context-rich prompts with past programs, scores, and problem descriptions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Ensemble&lt;/strong&gt;: Multiple language models working together for code generation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-objective Optimization&lt;/strong&gt;: Simultaneous optimization of multiple evaluation metrics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Checkpoint System&lt;/strong&gt;: Automatic saving and resuming of evolution state&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üî¨ &lt;strong&gt;Scientific Reproducibility&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Seeding&lt;/strong&gt;: Full deterministic reproduction with hash-based component isolation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Default Reproducibility&lt;/strong&gt;: Seed=42 by default for immediate reproducible results&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Granular Control&lt;/strong&gt;: Per-component seeding for LLMs, database, and evaluation pipeline&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ü§ñ &lt;strong&gt;Advanced LLM Integration&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ensemble Sophistication&lt;/strong&gt;: Weighted model combinations with intelligent fallback strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test-Time Compute&lt;/strong&gt;: Integration with &lt;a href="https://github.com/codelion/optillm"&gt;optillm&lt;/a&gt; for Mixture of Agents (MoA) and enhanced reasoning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Universal API Support&lt;/strong&gt;: Works with any OpenAI-compatible endpoint (Anthropic, Google, local models)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Plugin Ecosystem&lt;/strong&gt;: Support for optillm plugins (readurls, executecode, z3_solver, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üß¨ &lt;strong&gt;Evolution Algorithm Innovations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MAP-Elites Implementation&lt;/strong&gt;: Quality-diversity algorithm for balanced exploration/exploitation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Island-Based Evolution&lt;/strong&gt;: Multiple populations with periodic migration for diversity maintenance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Inspiration vs Performance&lt;/strong&gt;: Sophisticated prompt engineering separating top performers from diverse inspirations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Strategy Selection&lt;/strong&gt;: Elite, diverse, and exploratory program sampling strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptive Feature Dimensions&lt;/strong&gt;: Default features (complexity &amp;amp; diversity) with customizable multi-dimensional search spaces&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üìä &lt;strong&gt;Evaluation &amp;amp; Feedback Systems&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Artifacts Side-Channel&lt;/strong&gt;: Capture build errors, profiling data, and execution feedback for LLM improvement&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cascade Evaluation&lt;/strong&gt;: Multi-stage testing with progressive complexity for efficient resource usage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM-Based Feedback&lt;/strong&gt;: Automated code quality assessment and reasoning capture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Error Handling&lt;/strong&gt;: Graceful recovery from evaluation failures with detailed diagnostics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üåê &lt;strong&gt;Multi-Language &amp;amp; Platform Support&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Language Agnostic&lt;/strong&gt;: Python, Rust, R, Metal shaders, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Platform Optimization&lt;/strong&gt;: Apple Silicon GPU kernels, CUDA optimization, CPU-specific tuning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Framework Integration&lt;/strong&gt;: MLX, PyTorch, scientific computing libraries&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Developer Experience &amp;amp; Tooling&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Visualization&lt;/strong&gt;: Interactive web-based evolution tree viewer with performance analytics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced CLI&lt;/strong&gt;: Rich command-line interface with checkpoint management and configuration override&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Examples&lt;/strong&gt;: 12+ diverse examples spanning optimization, ML, systems programming, and scientific computing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Error Recovery&lt;/strong&gt;: Robust checkpoint loading with automatic fix for common serialization issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;Performance &amp;amp; Scalability&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Process-Based Parallelism&lt;/strong&gt;: True parallel execution bypassing Python's GIL for CPU-bound tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Resource Management&lt;/strong&gt;: Memory limits, timeouts, and resource monitoring&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Storage&lt;/strong&gt;: Optimized database with artifact management and cleanup policies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;p&gt;OpenEvolve orchestrates a sophisticated evolutionary pipeline:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/codelion/openevolve/main/openevolve-architecture.png" alt="OpenEvolve Architecture" /&gt;&lt;/p&gt; 
&lt;h3&gt;Core Evolution Loop&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Prompt Sampler&lt;/strong&gt;: Creates rich prompts containing:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Top-performing programs (for optimization guidance)&lt;/li&gt; 
   &lt;li&gt;Diverse inspiration programs (for creative exploration)&lt;/li&gt; 
   &lt;li&gt;Execution artifacts and error feedback&lt;/li&gt; 
   &lt;li&gt;Dynamic documentation fetching (via optillm plugins)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Intelligent LLM Ensemble&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Weighted model combinations for quality/speed tradeoffs&lt;/li&gt; 
   &lt;li&gt;Test-time compute techniques (MoA, chain-of-thought, reflection)&lt;/li&gt; 
   &lt;li&gt;Deterministic selection with comprehensive seeding&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Evaluator Pool&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Multi-stage cascade evaluation&lt;/li&gt; 
   &lt;li&gt;Artifact collection for detailed feedback&lt;/li&gt; 
   &lt;li&gt;LLM-based code quality assessment&lt;/li&gt; 
   &lt;li&gt;Parallel execution with resource limits&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sophisticated Program Database&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;MAP-Elites algorithm for quality-diversity balance&lt;/li&gt; 
   &lt;li&gt;Island-based populations with migration&lt;/li&gt; 
   &lt;li&gt;Feature map clustering and archive management&lt;/li&gt; 
   &lt;li&gt;Comprehensive metadata and lineage tracking&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Island-Based Evolution with Worker Pinning&lt;/h3&gt; 
&lt;p&gt;OpenEvolve implements a sophisticated island-based evolutionary architecture that maintains multiple isolated populations to prevent premature convergence and preserve genetic diversity.&lt;/p&gt; 
&lt;h4&gt;How Islands Work&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Isolated Populations&lt;/strong&gt;: Each island maintains its own population of programs that evolve independently&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Periodic Migration&lt;/strong&gt;: Top-performing programs periodically migrate between adjacent islands (ring topology) to share beneficial mutations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;True Population Isolation&lt;/strong&gt;: Worker processes are deterministically pinned to specific islands to ensure no cross-contamination during parallel evolution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Worker-to-Island Pinning&lt;/h4&gt; 
&lt;p&gt;To ensure true island isolation during parallel execution, OpenEvolve implements automatic worker-to-island pinning:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Workers are distributed across islands using modulo arithmetic
worker_id = 0, 1, 2, 3, 4, 5, ...
island_id = worker_id % num_islands

# Example with 3 islands and 6 workers:
# Worker 0, 3 ‚Üí Island 0  
# Worker 1, 4 ‚Üí Island 1
# Worker 2, 5 ‚Üí Island 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Benefits of Worker Pinning&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Genetic Isolation&lt;/strong&gt;: Prevents accidental population mixing between islands during parallel sampling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistent Evolution&lt;/strong&gt;: Each island maintains its distinct evolutionary trajectory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Balanced Load&lt;/strong&gt;: Workers are evenly distributed across islands automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Migration Integrity&lt;/strong&gt;: Controlled migration happens only at designated intervals, not due to race conditions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Automatic Distribution&lt;/strong&gt;: The system handles all edge cases automatically:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;More workers than islands&lt;/strong&gt;: Multiple workers per island with balanced distribution&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fewer workers than islands&lt;/strong&gt;: Some islands may not have dedicated workers but still participate in migration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Single island&lt;/strong&gt;: All workers sample from the same population (degrades to standard evolution)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This architecture ensures that each island develops unique evolutionary pressures and solutions, while periodic migration allows successful innovations to spread across the population without destroying diversity.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;To install natively, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/codelion/openevolve.git
cd openevolve
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;h4&gt;Setting up LLM Access&lt;/h4&gt; 
&lt;p&gt;OpenEvolve uses the OpenAI SDK, which means it works with any LLM provider that supports an OpenAI-compatible API:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set the API Key&lt;/strong&gt;: Export the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your-api-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using Alternative LLM Providers&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For providers other than OpenAI (e.g., Anthropic, Cohere, local models), update the &lt;code&gt;api_base&lt;/code&gt; in your config.yaml:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-yaml"&gt;llm:
  api_base: "https://your-provider-endpoint.com/v1"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Maximum Flexibility with optillm&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For advanced routing, rate limiting, or using multiple providers, we recommend &lt;a href="https://github.com/codelion/optillm"&gt;optillm&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;optillm acts as a proxy that can route requests to different LLMs based on your rules&lt;/li&gt; 
   &lt;li&gt;Simply point &lt;code&gt;api_base&lt;/code&gt; to your optillm instance:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-yaml"&gt;llm:
  api_base: "http://localhost:8000/v1"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This setup ensures OpenEvolve can work with any LLM provider - OpenAI, Anthropic, Google, Cohere, local models via Ollama/vLLM, or any OpenAI-compatible endpoint.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from openevolve import OpenEvolve

# Ensure API key is set
if not os.environ.get("OPENAI_API_KEY"):
    raise ValueError("Please set OPENAI_API_KEY environment variable")

# Initialize the system
evolve = OpenEvolve(
    initial_program_path="path/to/initial_program.py",
    evaluation_file="path/to/evaluator.py",
    config_path="path/to/config.yaml"
)

# Run the evolution
best_program = await evolve.run(iterations=1000)
print(f"Best program metrics:")
for name, value in best_program.metrics.items():
    print(f"  {name}: {value:.4f}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Command-Line Usage&lt;/h3&gt; 
&lt;p&gt;OpenEvolve can also be run from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python openevolve-run.py path/to/initial_program.py path/to/evaluator.py --config path/to/config.yaml --iterations 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Resuming from Checkpoints&lt;/h3&gt; 
&lt;p&gt;OpenEvolve automatically saves checkpoints at intervals specified by the &lt;code&gt;checkpoint_interval&lt;/code&gt; config parameter (default is 10 iterations). You can resume an evolution run from a saved checkpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python openevolve-run.py path/to/initial_program.py path/to/evaluator.py \
  --config path/to/config.yaml \
  --checkpoint path/to/checkpoint_directory \
  --iterations 50
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When resuming from a checkpoint:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The system loads all previously evolved programs and their metrics&lt;/li&gt; 
 &lt;li&gt;Checkpoint numbering continues from where it left off (e.g., if loaded from checkpoint_50, the next checkpoint will be checkpoint_60)&lt;/li&gt; 
 &lt;li&gt;All evolution state is preserved (best programs, feature maps, archives, etc.)&lt;/li&gt; 
 &lt;li&gt;Each checkpoint directory contains a copy of the best program at that point in time&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example workflow with checkpoints:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run for 50 iterations (creates checkpoints at iterations 10, 20, 30, 40, 50)
python openevolve-run.py examples/function_minimization/initial_program.py \
  examples/function_minimization/evaluator.py \
  --iterations 50

# Resume from checkpoint 50 for another 50 iterations (creates checkpoints at 60, 70, 80, 90, 100)
python openevolve-run.py examples/function_minimization/initial_program.py \
  examples/function_minimization/evaluator.py \
  --checkpoint examples/function_minimization/openevolve_output/checkpoints/checkpoint_50 \
  --iterations 50
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Comparing Results Across Checkpoints&lt;/h3&gt; 
&lt;p&gt;Each checkpoint directory contains the best program found up to that point, making it easy to compare solutions over time:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;checkpoints/
  checkpoint_10/
    best_program.py         # Best program at iteration 10
    best_program_info.json  # Metrics and details
    programs/               # All programs evaluated so far
    metadata.json           # Database state
  checkpoint_20/
    best_program.py         # Best program at iteration 20
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can compare the evolution of solutions by examining the best programs at different checkpoints:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Compare best programs at different checkpoints
diff -u checkpoints/checkpoint_10/best_program.py checkpoints/checkpoint_20/best_program.py

# Compare metrics
cat checkpoints/checkpoint_*/best_program_info.json | grep -A 10 metrics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualizing the evolution tree&lt;/h3&gt; 
&lt;p&gt;The script in &lt;code&gt;scripts/visualize.py&lt;/code&gt; allows you to visualize the evolution tree and display it in your webbrowser. The script watches live for the newest checkpoint directory in the examples/ folder structure and updates the graph. Alternatively, you can also provide a specific checkpoint folder with the &lt;code&gt;--path&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install requirements
pip install -r scripts/requirements.txt

# Start the visualization web server and have it watch the examples/ folder
python scripts/visualizer.py

# Start the visualization web server with a specific checkpoint
python scripts/visualizer.py --path examples/function_minimization/openevolve_output/checkpoints/checkpoint_100/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In the visualization UI, you can&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;see the branching of your program evolution in a network visualization, with node radius chosen by the program fitness (= the currently selected metric),&lt;/li&gt; 
 &lt;li&gt;see the parent-child relationship of nodes and click through them in the sidebar (use the yellow locator icon in the sidebar to center the node in the graph),&lt;/li&gt; 
 &lt;li&gt;select the metric of interest (with the available metric choices depending on your data set),&lt;/li&gt; 
 &lt;li&gt;highlight nodes, for example the top score (for the chosen metric) or the MAP-elites members,&lt;/li&gt; 
 &lt;li&gt;click nodes to see their code and prompts (if available from the checkpoint data) in a sidebar,&lt;/li&gt; 
 &lt;li&gt;in the "Performance" tab, see their selected metric score vs generation in a graph&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/codelion/openevolve/main/openevolve-visualizer.png" alt="OpenEvolve Visualizer" /&gt;&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;You can also install and execute via Docker:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t openevolve .
docker run --rm -v $(pwd):/app --network="host" openevolve examples/function_minimization/initial_program.py examples/function_minimization/evaluator.py --config examples/function_minimization/config.yaml --iterations 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;OpenEvolve is highly configurable with advanced options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Example configuration showcasing advanced features
max_iterations: 1000
random_seed: 42  # Full reproducibility by default

llm:
  # Advanced ensemble configuration
  models:
    - name: "gemini-2.0-flash-lite"
      weight: 0.7
    - name: "moa&amp;amp;readurls-gemini-2.0-flash"  # optillm test-time compute
      weight: 0.3
  temperature: 0.7
  
database:
  # MAP-Elites configuration
  population_size: 500
  num_islands: 5  # Island-based evolution
  migration_interval: 20
  feature_dimensions: ["complexity", "diversity"]  # Default quality-diversity features
  
evaluator:
  # Advanced evaluation features
  enable_artifacts: true  # Capture execution feedback
  cascade_evaluation: true  # Multi-stage testing
  use_llm_feedback: true  # AI-based code quality assessment
  
prompt:
  # Sophisticated prompt engineering
  num_top_programs: 3      # Performance examples
  num_diverse_programs: 2  # Creative inspiration
  include_artifacts: true  # Execution feedback
  
  # Template customization
  template_dir: null               # Directory for custom prompt templates
  use_template_stochasticity: true # Enable random variations in prompts
  template_variations: {}          # Define variation placeholders
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Sample configuration files are available in the &lt;code&gt;configs/&lt;/code&gt; directory:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;default_config.yaml&lt;/code&gt;: Comprehensive configuration with all available options&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;island_config_example.yaml&lt;/code&gt;: Advanced island-based evolution setup&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Prompt Engineering Design&lt;/h3&gt; 
&lt;p&gt;OpenEvolve uses a sophisticated prompt engineering approach that separates different types of program examples to optimize LLM learning:&lt;/p&gt; 
&lt;h4&gt;Program Selection Strategy&lt;/h4&gt; 
&lt;p&gt;The system distinguishes between three types of program examples shown to the LLM:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Previous Attempts&lt;/strong&gt; (&lt;code&gt;num_top_programs&lt;/code&gt;): Shows only the best performing programs to demonstrate high-quality approaches&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Used for the "Previous Attempts" section in prompts&lt;/li&gt; 
   &lt;li&gt;Focused on proven successful patterns&lt;/li&gt; 
   &lt;li&gt;Helps LLM understand what constitutes good performance&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Top Programs&lt;/strong&gt; (&lt;code&gt;num_top_programs + num_diverse_programs&lt;/code&gt;): Broader selection including both top performers and diverse approaches&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Used for the "Top Performing Programs" section&lt;/li&gt; 
   &lt;li&gt;Includes diverse programs to prevent local optima&lt;/li&gt; 
   &lt;li&gt;Balances exploitation of known good solutions with exploration of novel approaches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inspirations&lt;/strong&gt; (&lt;code&gt;num_top_programs&lt;/code&gt;): Cross-island program samples for creative inspiration&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Derived from other evolution islands to maintain diversity&lt;/li&gt; 
   &lt;li&gt;Count automatically configures based on &lt;code&gt;num_top_programs&lt;/code&gt; setting&lt;/li&gt; 
   &lt;li&gt;Prevents convergence by exposing LLM to different evolutionary trajectories&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Design Rationale&lt;/h4&gt; 
&lt;p&gt;This separation is intentional and serves multiple purposes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Focused Learning&lt;/strong&gt;: Previous attempts show only the best patterns, helping LLM understand quality standards&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diversity Maintenance&lt;/strong&gt;: Top programs include diverse solutions to encourage exploration beyond local optima&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Pollination&lt;/strong&gt;: Inspirations from other islands introduce novel approaches and prevent stagnation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Balance&lt;/strong&gt;: Adjust &lt;code&gt;num_top_programs&lt;/code&gt; and &lt;code&gt;num_diverse_programs&lt;/code&gt; to control exploration vs exploitation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The inspiration count automatically scales with &lt;code&gt;num_top_programs&lt;/code&gt; to maintain consistency across different configuration sizes, eliminating the need for a separate configuration parameter.&lt;/p&gt; 
&lt;h3&gt;Template Customization&lt;/h3&gt; 
&lt;p&gt;OpenEvolve supports advanced prompt template customization to increase diversity in code evolution:&lt;/p&gt; 
&lt;h4&gt;Custom Templates with &lt;code&gt;template_dir&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;You can override the default prompt templates by providing custom ones:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;prompt:
  template_dir: "path/to/your/templates"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create &lt;code&gt;.txt&lt;/code&gt; files in your template directory with these names:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;diff_user.txt&lt;/code&gt; - Template for diff-based evolution&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;full_rewrite_user.txt&lt;/code&gt; - Template for full code rewrites&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;evolution_history.txt&lt;/code&gt; - Format for presenting evolution history&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;top_program.txt&lt;/code&gt; - Format for top-performing programs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;previous_attempt.txt&lt;/code&gt; - Format for previous attempts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See these directories for complete examples of custom templates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;examples/lm_eval/prompts/&lt;/code&gt; - Custom templates for evaluation tasks&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/llm_prompt_optimization/templates/&lt;/code&gt; - Templates for evolving prompts instead of code&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Template Variations with Stochasticity&lt;/h4&gt; 
&lt;p&gt;To add randomness to your prompts and prevent getting stuck in local optima:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Enable stochasticity&lt;/strong&gt; in your config:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;prompt:
  use_template_stochasticity: true
  template_variations:
    greeting:
      - "Let's improve this code."
      - "Time to enhance this program."
      - "Here's how we can optimize:"
    analysis_intro:
      - "Current metrics show"
      - "Performance analysis indicates"
      - "The evaluation reveals"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;Use variation placeholders&lt;/strong&gt; in your custom templates:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;# custom_template.txt
{greeting}
{analysis_intro} the following results:
{metrics}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The system will randomly select one variation for each placeholder during prompt generation, creating diverse prompts that can lead to more creative code evolutions.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The default templates don't include variation placeholders, so you'll need to create custom templates to use this feature effectively.&lt;/p&gt; 
&lt;h3&gt;Feature Dimensions in MAP-Elites&lt;/h3&gt; 
&lt;p&gt;Feature dimensions control how programs are organized in the MAP-Elites quality-diversity grid:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Default Features&lt;/strong&gt;: If &lt;code&gt;feature_dimensions&lt;/code&gt; is NOT specified in your config, OpenEvolve uses &lt;code&gt;["complexity", "diversity"]&lt;/code&gt; as defaults.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Built-in Features&lt;/strong&gt; (always computed internally by OpenEvolve):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;complexity&lt;/strong&gt;: Code length (recommended default)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;diversity&lt;/strong&gt;: Code structure diversity compared to other programs (recommended default)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Only &lt;code&gt;complexity&lt;/code&gt; and &lt;code&gt;diversity&lt;/code&gt; are used as defaults because they work well across all program types.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Custom Features&lt;/strong&gt;: You can mix built-in features with metrics from your evaluator:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;database:
  feature_dimensions: ["complexity", "performance", "correctness"]  # Mix of built-in and custom
  # Per-dimension bin configuration (optional)
  feature_bins: 
    complexity: 10        # 10 bins for complexity
    performance: 20       # 20 bins for performance (from YOUR evaluator)
    correctness: 15       # 15 bins for correctness (from YOUR evaluator)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CRITICAL: Return Raw Values, Not Bin Indices&lt;/strong&gt;: For custom feature dimensions, your evaluator must return &lt;strong&gt;raw continuous values&lt;/strong&gt;, not pre-computed bin indices. OpenEvolve handles all scaling and binning internally.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# ‚úÖ CORRECT: Return raw values
return {
    "combined_score": 0.85,
    "prompt_length": 1247,     # Actual character count
    "execution_time": 0.234    # Raw time in seconds
}

# ‚ùå WRONG: Don't return bin indices
return {
    "combined_score": 0.85,
    "prompt_length": 7,        # Pre-computed bin index
    "execution_time": 3        # Pre-computed bin index
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OpenEvolve automatically handles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Min-max scaling to [0,1] range&lt;/li&gt; 
 &lt;li&gt;Binning into the specified number of bins&lt;/li&gt; 
 &lt;li&gt;Adaptive scaling as the value range expands during evolution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: OpenEvolve will raise an error if a specified feature is not found in the evaluator's metrics. This ensures your configuration is correct. The error message will show available metrics to help you fix the configuration.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/configs/default_config.yaml"&gt;Configuration Guide&lt;/a&gt; for a full list of options.&lt;/p&gt; 
&lt;h3&gt;Default Metric for Program Selection&lt;/h3&gt; 
&lt;p&gt;When comparing and selecting programs, OpenEvolve uses the following priority:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;combined_score&lt;/strong&gt;: If your evaluator returns a &lt;code&gt;combined_score&lt;/code&gt; metric, it will be used as the primary fitness measure&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Average of all metrics&lt;/strong&gt;: If no &lt;code&gt;combined_score&lt;/code&gt; is provided, OpenEvolve calculates the average of all numeric metrics returned by your evaluator&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This ensures programs can always be compared even without explicit fitness definitions. For best results, consider having your evaluator return a &lt;code&gt;combined_score&lt;/code&gt; that represents overall program fitness.&lt;/p&gt; 
&lt;h2&gt;Artifacts Channel&lt;/h2&gt; 
&lt;p&gt;OpenEvolve includes an &lt;strong&gt;artifacts side-channel&lt;/strong&gt; that allows evaluators to capture build errors, profiling results, etc. to provide better feedback to the LLM in subsequent generations. This feature enhances the evolution process by giving the LLM context about what went wrong and how to fix it.&lt;/p&gt; 
&lt;p&gt;The artifacts channel operates alongside the traditional fitness metrics.&lt;/p&gt; 
&lt;h3&gt;Example: Compilation Failure Feedback&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openevolve.evaluation_result import EvaluationResult

return EvaluationResult(
    metrics={"compile_ok": 0.0, "score": 0.0},
    artifacts={
        "stderr": "SyntaxError: invalid syntax (line 15)",
        "traceback": "...",
        "failure_stage": "compilation"
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The next generation prompt will include:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;## Last Execution Output
### Stderr
SyntaxError: invalid syntax (line 15)

### Traceback
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Example: LLM Feedback&lt;/h2&gt; 
&lt;p&gt;An example for an LLM artifact side channel is part of the default evaluation template, which ends with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;Return your evaluation as a JSON object with the following format:
{{
    "readability": [score],
    "maintainability": [score],
    "efficiency": [score],
    "reasoning": "[brief explanation of scores]"
}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The non-float values, in this case the "reasoning" key of the json response that the evaluator LLM generates, will be available within the next generation prompt.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Artifacts can be controlled via configuration and environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# config.yaml
evaluator:
  enable_artifacts: true

prompt:
  include_artifacts: true
  max_artifact_bytes: 4096  # 4KB limit in prompts
  artifact_security_filter: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Environment variable to disable artifacts
export ENABLE_ARTIFACTS=false
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Benefits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Faster convergence&lt;/strong&gt; - LLMs can see what went wrong and fix it directly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better error handling&lt;/strong&gt; - Compilation and runtime failures become learning opportunities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich debugging context&lt;/strong&gt; - Full stack traces and error messages guide improvements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero overhead&lt;/strong&gt; - When disabled, no performance impact on evaluation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;See the &lt;code&gt;examples/&lt;/code&gt; directory for complete examples of using OpenEvolve on various problems:&lt;/p&gt; 
&lt;h3&gt;Mathematical Optimization&lt;/h3&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/function_minimization/"&gt;Function Minimization&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;A comprehensive example demonstrating evolution from random search to sophisticated simulated annealing.&lt;/p&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/circle_packing/"&gt;Circle Packing&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Our implementation of the circle packing problem. For the n=26 case, we achieve state-of-the-art results matching published benchmarks.&lt;/p&gt; 
&lt;p&gt;Below is the optimal packing found by OpenEvolve after 800 iterations:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/00100f9e-2ac3-445b-9266-0398b7174193" alt="circle-packing-result" /&gt;&lt;/p&gt; 
&lt;h3&gt;Advanced AI &amp;amp; LLM Integration&lt;/h3&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/web_scraper_optillm/"&gt;Web Scraper with optillm&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Demonstrates integration with &lt;a href="https://github.com/codelion/optillm"&gt;optillm&lt;/a&gt; for test-time compute optimization, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;readurls plugin&lt;/strong&gt;: Automatic documentation fetching&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mixture of Agents (MoA)&lt;/strong&gt;: Multi-response synthesis for improved accuracy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local model optimization&lt;/strong&gt;: Enhanced reasoning with smaller models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/llm_prompt_optimization/"&gt;LLM Prompt Optimization&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Evolving prompts for better LLM performance on HuggingFace datasets. Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Custom templates for evolving prompts instead of code&lt;/li&gt; 
 &lt;li&gt;Two-stage cascading evaluation for efficiency&lt;/li&gt; 
 &lt;li&gt;Support for any HuggingFace dataset&lt;/li&gt; 
 &lt;li&gt;Automatic prompt improvement through evolution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Systems &amp;amp; Performance Optimization&lt;/h3&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/mlx_metal_kernel_opt/"&gt;MLX Metal Kernel Optimization&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Automated discovery of custom GPU kernels for Apple Silicon, achieving:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2-3x speedup&lt;/strong&gt; over baseline attention implementations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware-aware optimizations&lt;/strong&gt; for unified memory architecture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Metal shader evolution&lt;/strong&gt; with numerical correctness validation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/rust_adaptive_sort/"&gt;Rust Adaptive Sort&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Evolution of sorting algorithms that adapt to data patterns, showcasing OpenEvolve's language-agnostic capabilities.&lt;/p&gt; 
&lt;h3&gt;Scientific Computing &amp;amp; Discovery&lt;/h3&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/symbolic_regression/"&gt;Symbolic Regression&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;A comprehensive example demonstrating automated discovery of mathematical expressions from scientific datasets using the LLM-SRBench benchmark.&lt;/p&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/r_robust_regression/"&gt;R Robust Regression&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Developing robust regression methods resistant to outliers using R language support.&lt;/p&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/signal_processing/"&gt;Signal Processing&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Automated design of digital filters with superior performance characteristics.&lt;/p&gt; 
&lt;h3&gt;Web and Integration Examples&lt;/h3&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/online_judge_programming/"&gt;Online Judge Programming&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Automated competitive programming solution generation with external evaluation systems.&lt;/p&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/codelion/openevolve/main/examples/lm_eval/"&gt;LM-Eval Integration&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Working with standard ML evaluation harnesses for automated benchmark improvement.&lt;/p&gt; 
&lt;h2&gt;Preparing Your Own Problems&lt;/h2&gt; 
&lt;p&gt;To use OpenEvolve for your own problems:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Mark code sections&lt;/strong&gt; to evolve with &lt;code&gt;# EVOLVE-BLOCK-START&lt;/code&gt; and &lt;code&gt;# EVOLVE-BLOCK-END&lt;/code&gt; comments&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create an evaluation function&lt;/strong&gt; that returns a dictionary of metrics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configure OpenEvolve&lt;/strong&gt; with appropriate parameters&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Run the evolution&lt;/strong&gt; process&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use OpenEvolve in your research, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@software{openevolve,
  title = {OpenEvolve: an open-source evolutionary coding agent},
  author = {Asankhaya Sharma},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/codelion/openevolve}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ansible/ansible</title>
      <link>https://github.com/ansible/ansible</link>
      <description>&lt;p&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible-core"&gt;&lt;img src="https://img.shields.io/pypi/v/ansible-core.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?sanitize=true" alt="Docs badge" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;&lt;img src="https://img.shields.io/badge/chat-IRC-brightgreen.svg?sanitize=true" alt="Chat badge" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;amp;branchName=devel"&gt;&lt;img src="https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/code_of_conduct.html"&gt;&lt;img src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg?sanitize=true" alt="Ansible Code of Conduct" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information"&gt;&lt;img src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg?sanitize=true" alt="Ansible mailing lists" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;&lt;img src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg?sanitize=true" alt="Repository License" /&gt;&lt;/a&gt; &lt;a href="https://bestpractices.coreinfrastructure.org/projects/2372"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/2372/badge" alt="Ansible CII Best Practices certification" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Ansible&lt;/h1&gt; 
&lt;p&gt;Ansible is a radically simple IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible &lt;a href="https://ansible.com/"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Design Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have an extremely simple setup process with a minimal learning curve.&lt;/li&gt; 
 &lt;li&gt;Manage machines quickly and in parallel.&lt;/li&gt; 
 &lt;li&gt;Avoid custom-agents and additional open ports, be agentless by leveraging the existing SSH daemon.&lt;/li&gt; 
 &lt;li&gt;Describe infrastructure in a language that is both machine and human friendly.&lt;/li&gt; 
 &lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt; 
 &lt;li&gt;Manage new remote machines instantly, without bootstrapping any software.&lt;/li&gt; 
 &lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt; 
 &lt;li&gt;Be usable as non-root.&lt;/li&gt; 
 &lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use Ansible&lt;/h2&gt; 
&lt;p&gt;You can install a released version of Ansible with &lt;code&gt;pip&lt;/code&gt; or a package manager. See our &lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html"&gt;installation guide&lt;/a&gt; for details on installing Ansible on a variety of platforms.&lt;/p&gt; 
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest features and fixes, directly. Although it is reasonably stable, you are more likely to encounter breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Communication&lt;/h2&gt; 
&lt;p&gt;Join the Ansible forum to ask questions, get help, and interact with the community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/help/6"&gt;Get Help&lt;/a&gt;: Find help or share your Ansible knowledge to help others. Use tags to filter and subscribe to posts, such as the following: 
  &lt;ul&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible"&gt;ansible&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible-core"&gt;ansible-core&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/playbook"&gt;playbook&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/chat/4"&gt;Social Spaces&lt;/a&gt;: Meet and interact with fellow enthusiasts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/news/5"&gt;News &amp;amp; Announcements&lt;/a&gt;: Track project-wide announcements including social events.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn"&gt;Bullhorn newsletter&lt;/a&gt;: Get release announcements and important changes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more ways to get in touch, see &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;Communicating with the Ansible community&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribute to Ansible&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out the &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/.github/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/devel/community"&gt;Community Information&lt;/a&gt; for all kinds of ways to contribute to and interact with the project, including how to submit bug reports and code to Ansible.&lt;/li&gt; 
 &lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt; 
 &lt;li&gt;Talk to us before making larger changes to avoid duplicate efforts. This not only helps everyone know what is going on, but it also helps save time and effort if we decide some changes are needed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Coding Guidelines&lt;/h2&gt; 
&lt;p&gt;We document our Coding Guidelines in the &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/"&gt;Developer Guide&lt;/a&gt;. We particularly suggest you review:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html"&gt;Contributing your module to Ansible&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html"&gt;Conventions, tips, and pitfalls&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Branch Info&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt; 
 &lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8). The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt; and has contributions from over 5000 users (and growing). Thanks everyone!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;GNU General Public License v3.0 or later&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>wandb/wandb</title>
      <link>https://github.com/wandb/wandb</link>
      <description>&lt;p&gt;The AI developer platform. Use Weights &amp; Biases to train and fine-tune models, and manage models from experimentation to production.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/wandb/wandb/main/assets/logo-dark.svg#gh-dark-mode-only" width="600" alt="Weights &amp;amp; Biases" /&gt; &lt;img src="https://raw.githubusercontent.com/wandb/wandb/main/assets/logo-light.svg#gh-light-mode-only" width="600" alt="Weights &amp;amp; Biases" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.python.org/pypi/wandb"&gt;&lt;img src="https://img.shields.io/pypi/v/wandb" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/wandb"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/wandb" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/wandb"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/wandb" /&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/wandb/wandb"&gt;&lt;img src="https://img.shields.io/circleci/build/github/wandb/wandb/main" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/wandb/wandb"&gt;&lt;img src="https://img.shields.io/codecov/c/gh/wandb/wandb" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Use W&amp;amp;B to build better models faster. Track and visualize all the pieces of your machine learning pipeline, from datasets to production machine learning models. Get started with W&amp;amp;B today, &lt;a href="https://wandb.com?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt;sign up for a W&amp;amp;B account!&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Building an LLM app? Track, debug, evaluate, and monitor LLM apps with &lt;a href="https://wandb.github.io/weave?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt;Weave&lt;/a&gt;, our new suite of tools for GenAI.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/track?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/experiments-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/experiments-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Experiments" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/reports?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/reports-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/reports-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Reports" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/artifacts?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/artifacts-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/artifacts-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Artifacts" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/tables?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/tables-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/tables-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Tables" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/sweeps?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/sweeps-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/sweeps-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Sweeps" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/model_registry?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/model-registry-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/model-registry-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Model Management" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a target="_blank" href="https://docs.wandb.ai/guides/artifacts/project-scoped-automations?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="./assets/Product_Icons_dark_background/automations-dark.svg" width="12.5%" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./assets/Product_Icons_light/automations-light.svg" width="12.5%" /&gt; 
   &lt;img alt="Weights and Biases Prompts" src="" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&lt;/p&gt;
&lt;a target="_blank" href="https://docs.wandb.ai/guides/artifacts/project-scoped-automations?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt; &lt;/a&gt;
&lt;p&gt;&lt;a target="_blank" href="https://docs.wandb.ai/guides/artifacts/project-scoped-automations?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=readme"&gt;See the &lt;/a&gt;&lt;a href="https://docs.wandb.ai/?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=documentation"&gt;W&amp;amp;B Developer Guide&lt;/a&gt; and &lt;a href="https://docs.wandb.ai/ref?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=documentation"&gt;API Reference Guide&lt;/a&gt; for a full technical description of the W&amp;amp;B platform.&lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;p&gt;Get started with W&amp;amp;B in four steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First, sign up for a &lt;a href="https://wandb.ai/login?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=quickstart"&gt;W&amp;amp;B account&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Second, install&amp;nbsp;the W&amp;amp;B SDK with &lt;a href="https://pip.pypa.io/en/stable/"&gt;pip&lt;/a&gt;. Navigate to your terminal and type the following command:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install wandb
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Third, log into W&amp;amp;B:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;wandb.login()
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Use the example code snippet below as a template to integrate W&amp;amp;B to your Python script:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import wandb

# Start a W&amp;amp;B Run with wandb.init
run = wandb.init(project="my_first_project")

# Save model inputs and hyperparameters in a wandb.config object
config = run.config
config.learning_rate = 0.01

# Model training code here ...

# Log metrics over time to visualize performance with wandb.log
for i in range(10):
    run.log({"loss": ...})

# Mark the run as finished, and finish uploading all data
run.finish()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Navigate to the W&amp;amp;B App to view a dashboard of your first W&amp;amp;B Experiment. Use the W&amp;amp;B App to compare multiple experiments in a unified place, dive into the results of a single run, and much more!&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;Integrations&lt;/h1&gt; 
&lt;p&gt;Use your favorite framework with W&amp;amp;B. W&amp;amp;B integrations make it fast and easy to set up experiment tracking and data versioning inside existing projects. For more information on how to integrate W&amp;amp;B with the framework of your choice, see the &lt;a href="https://docs.wandb.ai/guides/integrations"&gt;Integrations chapter&lt;/a&gt; in the W&amp;amp;B Developer Guide.&lt;/p&gt; 
&lt;!-- &lt;p align='center'&gt;
&lt;img src="./assets/integrations.png" width="100%" /&gt;
&lt;/p&gt; --&gt; 
&lt;details&gt; 
 &lt;summary&gt;üî• PyTorch&lt;/summary&gt; 
 &lt;p&gt;Call &lt;code&gt;.watch&lt;/code&gt; and pass in your PyTorch model to automatically log gradients and store the network topology. Next, use &lt;code&gt;.log&lt;/code&gt; to track other metrics. The following example demonstrates an example of how to do this:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import wandb

# 1. Start a new run
run = wandb.init(project="gpt4")

# 2. Save model inputs and hyperparameters
config = run.config
config.dropout = 0.01

# 3. Log gradients and model parameters
run.watch(model)
for batch_idx, (data, target) in enumerate(train_loader):
    ...
    if batch_idx % args.log_interval == 0:
        # 4. Log metrics to visualize performance
        run.log({"loss": loss})
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="http://wandb.me/pytorch-colab"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.com/guides/integrations/pytorch?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate PyTorch with W&amp;amp;B.&lt;/li&gt; 
  &lt;li&gt;Explore &lt;a href="https://app.wandb.ai/wandb/getting-started/reports/Pytorch--VmlldzoyMTEwNzM?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;W&amp;amp;B Reports&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;üåä TensorFlow/Keras&lt;/summary&gt; Use W&amp;amp;B Callbacks to automatically save metrics to W&amp;amp;B when you call `model.fit` during training. 
 &lt;p&gt;The following code example demonstrates how your script might look like when you integrate W&amp;amp;B with Keras:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   tensorflow, numpy

import wandb
from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint

import random
import numpy as np
import tensorflow as tf


# Start a run, tracking hyperparameters
run = wandb.init(
    # set the wandb project where this run will be logged
    project="my-awesome-project",
    # track hyperparameters and run metadata with wandb.config
    config={
        "layer_1": 512,
        "activation_1": "relu",
        "dropout": random.uniform(0.01, 0.80),
        "layer_2": 10,
        "activation_2": "softmax",
        "optimizer": "sgd",
        "loss": "sparse_categorical_crossentropy",
        "metric": "accuracy",
        "epoch": 8,
        "batch_size": 256,
    },
)

# [optional] use wandb.config as your config
config = run.config

# get the data
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
x_train, y_train = x_train[::5], y_train[::5]
x_test, y_test = x_test[::20], y_test[::20]
labels = [str(digit) for digit in range(np.max(y_train) + 1)]

# build a model
model = tf.keras.models.Sequential(
    [
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(config.layer_1, activation=config.activation_1),
        tf.keras.layers.Dropout(config.dropout),
        tf.keras.layers.Dense(config.layer_2, activation=config.activation_2),
    ]
)

# compile the model
model.compile(optimizer=config.optimizer, loss=config.loss, metrics=[config.metric])

# WandbMetricsLogger will log train and validation metrics to wandb
# WandbModelCheckpoint will upload model checkpoints to wandb
history = model.fit(
    x=x_train,
    y=y_train,
    epochs=config.epoch,
    batch_size=config.batch_size,
    validation_data=(x_test, y_test),
    callbacks=[
        WandbMetricsLogger(log_freq=5),
        WandbModelCheckpoint("models"),
    ],
)

# [optional] finish the wandb run, necessary in notebooks
run.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Get started integrating your Keras model with W&amp;amp;B today:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="https://wandb.me/intro-keras?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.com/guides/integrations/keras?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate Keras with W&amp;amp;B.&lt;/li&gt; 
  &lt;li&gt;Explore &lt;a href="https://app.wandb.ai/wandb/getting-started/reports/Keras--VmlldzoyMTEwNjQ?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;W&amp;amp;B Reports&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ü§ó Hugging Face Transformers&lt;/summary&gt; 
 &lt;p&gt;Pass &lt;code&gt;wandb&lt;/code&gt; to the &lt;code&gt;report_to&lt;/code&gt; argument when you run a script using a Hugging Face Trainer. W&amp;amp;B will automatically log losses, evaluation metrics, model topology, and gradients.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The environment you run your script in must have &lt;code&gt;wandb&lt;/code&gt; installed.&lt;/p&gt; 
 &lt;p&gt;The following example demonstrates how to integrate W&amp;amp;B with Hugging Face:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   numpy, transformers, datasets

import wandb

import os
import numpy as np
from datasets import load_dataset
from transformers import TrainingArguments, Trainer
from transformers import AutoTokenizer, AutoModelForSequenceClassification


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {"accuracy": np.mean(predictions == labels)}


# download prepare the data
dataset = load_dataset("yelp_review_full")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

small_train_dataset = dataset["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = dataset["test"].shuffle(seed=42).select(range(300))

small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
small_eval_dataset = small_train_dataset.map(tokenize_function, batched=True)

# download the model
model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=5
)

# set the wandb project where this run will be logged
os.environ["WANDB_PROJECT"] = "my-awesome-project"

# save your trained model checkpoint to wandb
os.environ["WANDB_LOG_MODEL"] = "true"

# turn off watch to log faster
os.environ["WANDB_WATCH"] = "false"

# pass "wandb" to the `report_to` parameter to turn on wandb logging
training_args = TrainingArguments(
    output_dir="models",
    report_to="wandb",
    logging_steps=5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=20,
    max_steps=100,
    save_steps=100,
)

# define the trainer and start training
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

# [optional] finish the wandb run, necessary in notebooks
wandb.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="http://wandb.me/hf?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.com/guides/integrations/huggingface?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate Hugging Face with W&amp;amp;B.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;‚ö°Ô∏è PyTorch Lightning&lt;/summary&gt; 
 &lt;p&gt;Build scalable, structured, high-performance PyTorch models with Lightning and log them with W&amp;amp;B.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   torch, torchvision, pytorch_lightning

import wandb

import os
from torch import optim, nn, utils
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor

import pytorch_lightning as pl
from pytorch_lightning.loggers import WandbLogger


class LitAutoEncoder(pl.LightningModule):
    def __init__(self, lr=1e-3, inp_size=28, optimizer="Adam"):
        super().__init__()

        self.encoder = nn.Sequential(
            nn.Linear(inp_size * inp_size, 64), nn.ReLU(), nn.Linear(64, 3)
        )
        self.decoder = nn.Sequential(
            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, inp_size * inp_size)
        )
        self.lr = lr

        # save hyperparameters to self.hparamsm auto-logged by wandb
        self.save_hyperparameters()

    def training_step(self, batch, batch_idx):
        x, y = batch
        x = x.view(x.size(0), -1)
        z = self.encoder(x)
        x_hat = self.decoder(z)
        loss = nn.functional.mse_loss(x_hat, x)

        # log metrics to wandb
        self.log("train_loss", loss)
        return loss

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.lr)
        return optimizer


# init the autoencoder
autoencoder = LitAutoEncoder(lr=1e-3, inp_size=28)

# setup data
batch_size = 32
dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())
train_loader = utils.data.DataLoader(dataset, shuffle=True)

# initialise the wandb logger and name your wandb project
wandb_logger = WandbLogger(project="my-awesome-project")

# add your batch size to the wandb config
wandb_logger.experiment.config["batch_size"] = batch_size

# pass wandb_logger to the Trainer
trainer = pl.Trainer(limit_train_batches=750, max_epochs=5, logger=wandb_logger)

# train the model
trainer.fit(model=autoencoder, train_dataloaders=train_loader)

# [optional] finish the wandb run, necessary in notebooks
wandb.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="http://wandb.me/lightning?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.ai/guides/integrations/lightning?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate PyTorch Lightning with W&amp;amp;B.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;üí® XGBoost&lt;/summary&gt; Use W&amp;amp;B Callbacks to automatically save metrics to W&amp;amp;B when you call `model.fit` during training. 
 &lt;p&gt;The following code example demonstrates how your script might look like when you integrate W&amp;amp;B with XGBoost:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   numpy, xgboost

import wandb
from wandb.xgboost import WandbCallback

import numpy as np
import xgboost as xgb


# setup parameters for xgboost
param = {
    "objective": "multi:softmax",
    "eta": 0.1,
    "max_depth": 6,
    "nthread": 4,
    "num_class": 6,
}

# start a new wandb run to track this script
run = wandb.init(
    # set the wandb project where this run will be logged
    project="my-awesome-project",
    # track hyperparameters and run metadata
    config=param,
)

# download data from wandb Artifacts and prep data
run.use_artifact("wandb/intro/dermatology_data:v0", type="dataset").download(".")
data = np.loadtxt(
    "./dermatology.data",
    delimiter=",",
    converters={33: lambda x: int(x == "?"), 34: lambda x: int(x) - 1},
)
sz = data.shape

train = data[: int(sz[0] * 0.7), :]
test = data[int(sz[0] * 0.7) :, :]

train_X = train[:, :33]
train_Y = train[:, 34]

test_X = test[:, :33]
test_Y = test[:, 34]

xg_train = xgb.DMatrix(train_X, label=train_Y)
xg_test = xgb.DMatrix(test_X, label=test_Y)
watchlist = [(xg_train, "train"), (xg_test, "test")]

# add another config to the wandb run
num_round = 5
run.config["num_round"] = 5
run.config["data_shape"] = sz

# pass WandbCallback to the booster to log its configs and metrics
bst = xgb.train(
    param, xg_train, num_round, evals=watchlist, callbacks=[WandbCallback()]
)

# get prediction
pred = bst.predict(xg_test)
error_rate = np.sum(pred != test_Y) / test_Y.shape[0]

# log your test metric to wandb
run.summary["Error Rate"] = error_rate

# [optional] finish the wandb run, necessary in notebooks
run.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="https://wandb.me/xgboost?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.ai/guides/integrations/xgboost?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate XGBoost with W&amp;amp;B.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;üßÆ Sci-Kit Learn&lt;/summary&gt; Use wandb to visualize and compare your scikit-learn models' performance: 
 &lt;pre&gt;&lt;code class="language-python"&gt;# This script needs these libraries to be installed:
#   numpy, sklearn

import wandb
from wandb.sklearn import plot_precision_recall, plot_feature_importances
from wandb.sklearn import plot_class_proportions, plot_learning_curve, plot_roc

import numpy as np
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split


# load and process data
wbcd = datasets.load_breast_cancer()
feature_names = wbcd.feature_names
labels = wbcd.target_names

test_size = 0.2
X_train, X_test, y_train, y_test = train_test_split(
    wbcd.data, wbcd.target, test_size=test_size
)

# train model
model = RandomForestClassifier()
model.fit(X_train, y_train)
model_params = model.get_params()

# get predictions
y_pred = model.predict(X_test)
y_probas = model.predict_proba(X_test)
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# start a new wandb run and add your model hyperparameters
run = wandb.init(project="my-awesome-project", config=model_params)

# Add additional configs to wandb
run.config.update(
    {
        "test_size": test_size,
        "train_len": len(X_train),
        "test_len": len(X_test),
    }
)

# log additional visualisations to wandb
plot_class_proportions(y_train, y_test, labels)
plot_learning_curve(model, X_train, y_train)
plot_roc(y_test, y_probas, labels)
plot_precision_recall(y_test, y_probas, labels)
plot_feature_importances(model)

# [optional] finish the wandb run, necessary in notebooks
run.finish()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Run an example &lt;a href="https://wandb.me/scikit-colab?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Google Colab Notebook&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;Read the &lt;a href="https://docs.wandb.ai/guides/integrations/scikit?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=integrations"&gt;Developer Guide&lt;/a&gt; for technical details on how to integrate Scikit-Learn with W&amp;amp;B.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;W&amp;amp;B Hosting Options&lt;/h1&gt; 
&lt;p&gt;Weights &amp;amp; Biases is available in the cloud or installed on your private infrastructure. Set up a W&amp;amp;B Server in a production environment in one of three ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.wandb.ai/guides/hosting/hosting-options/self-managed#on-prem-private-cloud?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=hosting"&gt;Production Cloud&lt;/a&gt;: Set up a production deployment on a private cloud in just a few steps using terraform scripts provided by W&amp;amp;B.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.wandb.ai/guides/hosting/hosting-options/wb-managed#dedicated-cloud?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=hosting"&gt;Dedicated Cloud&lt;/a&gt;: A managed, dedicated deployment on W&amp;amp;B's single-tenant infrastructure in your choice of cloud region.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.wandb.ai/guides/hosting/how-to-guides/bare-metal?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=hosting"&gt;On-Prem/Bare Metal&lt;/a&gt;: W&amp;amp;B supports setting up a production server on most bare metal servers in your on-premise data centers. Quickly get started by running &lt;code&gt;wandb server&lt;/code&gt; to easily start hosting W&amp;amp;B on your local infrastructure.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See the &lt;a href="https://docs.wandb.ai/guides/hosting?utm_source=github&amp;amp;utm_medium=code&amp;amp;utm_campaign=wandb&amp;amp;utm_content=hosting"&gt;Hosting documentation&lt;/a&gt; in the W&amp;amp;B Developer Guide for more information.&lt;/p&gt; 
&lt;!-- &amp;nbsp;

# Tutorials

Explore example Colab Notebooks at [wandb/examples GitHub repository](https://github.com/wandb/examples/tree/master/colabs). Here are some of our favorites:

[INSERT] --&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;Python Version Support&lt;/h1&gt; 
&lt;p&gt;We are committed to supporting our minimum required Python version for &lt;em&gt;at least&lt;/em&gt; six months after its official end-of-life (EOL) date, as defined by the Python Software Foundation. You can find a list of Python EOL dates &lt;a href="https://devguide.python.org/versions/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When we discontinue support for a Python version, we will increment the library‚Äôs minor version number to reflect this change.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;Contribution guidelines&lt;/h1&gt; 
&lt;p&gt;Weights &amp;amp; Biases ‚ù§Ô∏è open source, and we welcome contributions from the community! See the &lt;a href="https://github.com/wandb/wandb/raw/main/CONTRIBUTING.md"&gt;Contribution guide&lt;/a&gt; for more information on the development workflow and the internals of the wandb library. For wandb bugs and feature requests, visit &lt;a href="https://github.com/wandb/wandb/issues"&gt;GitHub Issues&lt;/a&gt; or contact &lt;a href="mailto:support@wandb.com"&gt;support@wandb.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;W&amp;amp;B Community&lt;/h1&gt; 
&lt;p&gt;Be a part of the growing W&amp;amp;B Community and interact with the W&amp;amp;B team in our &lt;a href="https://wandb.me/discord"&gt;Discord&lt;/a&gt;. Stay connected with the latest ML updates and tutorials with &lt;a href="https://wandb.ai/fully-connected"&gt;W&amp;amp;B Fully Connected&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/wandb/wandb/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>roboflow/rf-detr</title>
      <link>https://github.com/roboflow/rf-detr</link>
      <description>&lt;p&gt;RF-DETR is a real-time object detection model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RF-DETR: SOTA Real-Time Object Detection Model&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/py/rfdetr"&gt;&lt;img src="https://badge.fury.io/py/rfdetr.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/rfdetr"&gt;&lt;img src="https://img.shields.io/pypi/dm/rfdetr" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/rfdetr"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/rfdetr" alt="python-version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/roboflow/rfdetr/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/SkalskiP/RF-DETR"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="hf space" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="colab" /&gt;&lt;/a&gt; &lt;a href="https://blog.roboflow.com/rf-detr"&gt;&lt;img src="https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true" alt="roboflow" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/GbfgXGJ8Bk"&gt;&lt;img src="https://img.shields.io/discord/1159501506232451173?logo=discord&amp;amp;label=discord&amp;amp;labelColor=fff&amp;amp;color=5865f2&amp;amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk" alt="discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;RF-DETR is a real-time, transformer-based object detection model architecture developed by Roboflow and released under the Apache 2.0 license.&lt;/p&gt; 
&lt;p&gt;RF-DETR is the first real-time model to exceed 60 AP on the &lt;a href="https://cocodataset.org/#home"&gt;Microsoft COCO benchmark&lt;/a&gt; alongside competitive performance at base sizes. It also achieves state-of-the-art performance on &lt;a href="https://github.com/roboflow/rf100-vl"&gt;RF100-VL&lt;/a&gt;, an object detection benchmark that measures model domain adaptability to real world problems. RF-DETR is fastest and most accurate for its size when compared current real-time objection models.&lt;/p&gt; 
&lt;p&gt;RF-DETR is small enough to run on the edge using &lt;a href="https://github.com/roboflow/inference"&gt;Inference&lt;/a&gt;, making it an ideal model for deployments that need both strong accuracy and real-time performance.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://rfdetr.roboflow.com"&gt;Read the documentation to get started training.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;2025/07/23&lt;/code&gt;: We release three new checkpoints for RF-DETR: Nano, Small, and Medium. 
  &lt;ul&gt; 
   &lt;li&gt;RF-DETR Base is now deprecated. We recommend using RF-DETR Medium which offers subtantially better accuracy at comparable latency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/03/20&lt;/code&gt;: We release RF-DETR real-time object detection model. &lt;strong&gt;Code and checkpoint for RF-DETR-large and RF-DETR-base are available.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/04/03&lt;/code&gt;: We release early stopping, gradient checkpointing, metrics saving, training resume, TensorBoard and W&amp;amp;B logging support.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/05/16&lt;/code&gt;: We release an 'optimize_for_inference' method which speeds up native PyTorch by up to 2x, depending on platform.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;RF-DETR achieves state-of-the-art performance on both the Microsoft COCO and the RF100-VL benchmarks.&lt;/p&gt; 
&lt;p&gt;The table below shows the performance of RF-DETR medium, compared to comparable medium models:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://media.roboflow.com/rfdetr/pareto1.png" alt="rf-detr-coco-rf100-vl-9" /&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Architecture&lt;/th&gt; 
   &lt;th align="center"&gt;COCO AP&lt;sub&gt;50&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;COCO AP&lt;sub&gt;50:95&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;RF100VL AP&lt;sub&gt;50&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;RF100VL AP&lt;sub&gt;50:95&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;Latency (ms)&lt;/th&gt; 
   &lt;th align="center"&gt;Params (M)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;RF-DETR-N&lt;/td&gt; 
   &lt;td align="center"&gt;67.6&lt;/td&gt; 
   &lt;td align="center"&gt;48.4&lt;/td&gt; 
   &lt;td align="center"&gt;84.1&lt;/td&gt; 
   &lt;td align="center"&gt;57.1&lt;/td&gt; 
   &lt;td align="center"&gt;2.32&lt;/td&gt; 
   &lt;td align="center"&gt;30.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;RF-DETR-S&lt;/td&gt; 
   &lt;td align="center"&gt;72.1&lt;/td&gt; 
   &lt;td align="center"&gt;53.0&lt;/td&gt; 
   &lt;td align="center"&gt;85.9&lt;/td&gt; 
   &lt;td align="center"&gt;59.6&lt;/td&gt; 
   &lt;td align="center"&gt;3.52&lt;/td&gt; 
   &lt;td align="center"&gt;32.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;RF-DETR-M&lt;/td&gt; 
   &lt;td align="center"&gt;73.6&lt;/td&gt; 
   &lt;td align="center"&gt;54.7&lt;/td&gt; 
   &lt;td align="center"&gt;86.6&lt;/td&gt; 
   &lt;td align="center"&gt;60.6&lt;/td&gt; 
   &lt;td align="center"&gt;4.52&lt;/td&gt; 
   &lt;td align="center"&gt;33.7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-N&lt;/td&gt; 
   &lt;td align="center"&gt;52.0&lt;/td&gt; 
   &lt;td align="center"&gt;37.4&lt;/td&gt; 
   &lt;td align="center"&gt;81.4&lt;/td&gt; 
   &lt;td align="center"&gt;55.3&lt;/td&gt; 
   &lt;td align="center"&gt;2.49&lt;/td&gt; 
   &lt;td align="center"&gt;2.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-S&lt;/td&gt; 
   &lt;td align="center"&gt;59.7&lt;/td&gt; 
   &lt;td align="center"&gt;44.4&lt;/td&gt; 
   &lt;td align="center"&gt;82.3&lt;/td&gt; 
   &lt;td align="center"&gt;56.2&lt;/td&gt; 
   &lt;td align="center"&gt;3.16&lt;/td&gt; 
   &lt;td align="center"&gt;9.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-M&lt;/td&gt; 
   &lt;td align="center"&gt;64.1&lt;/td&gt; 
   &lt;td align="center"&gt;48.6&lt;/td&gt; 
   &lt;td align="center"&gt;82.5&lt;/td&gt; 
   &lt;td align="center"&gt;56.5&lt;/td&gt; 
   &lt;td align="center"&gt;5.13&lt;/td&gt; 
   &lt;td align="center"&gt;20.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-L&lt;/td&gt; 
   &lt;td align="center"&gt;65.3&lt;/td&gt; 
   &lt;td align="center"&gt;50.2&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;6.65&lt;/td&gt; 
   &lt;td align="center"&gt;25.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-X&lt;/td&gt; 
   &lt;td align="center"&gt;66.5&lt;/td&gt; 
   &lt;td align="center"&gt;51.2&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;11.92&lt;/td&gt; 
   &lt;td align="center"&gt;56.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;LW-DETR-T&lt;/td&gt; 
   &lt;td align="center"&gt;60.7&lt;/td&gt; 
   &lt;td align="center"&gt;42.9&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;1.91&lt;/td&gt; 
   &lt;td align="center"&gt;12.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;LW-DETR-S&lt;/td&gt; 
   &lt;td align="center"&gt;66.8&lt;/td&gt; 
   &lt;td align="center"&gt;48.0&lt;/td&gt; 
   &lt;td align="center"&gt;84.5&lt;/td&gt; 
   &lt;td align="center"&gt;58.0&lt;/td&gt; 
   &lt;td align="center"&gt;2.62&lt;/td&gt; 
   &lt;td align="center"&gt;14.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;LW-DETR-M&lt;/td&gt; 
   &lt;td align="center"&gt;72.0&lt;/td&gt; 
   &lt;td align="center"&gt;52.6&lt;/td&gt; 
   &lt;td align="center"&gt;85.2&lt;/td&gt; 
   &lt;td align="center"&gt;59.4&lt;/td&gt; 
   &lt;td align="center"&gt;4.49&lt;/td&gt; 
   &lt;td align="center"&gt;28.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;D-FINE-N&lt;/td&gt; 
   &lt;td align="center"&gt;60.2&lt;/td&gt; 
   &lt;td align="center"&gt;42.7&lt;/td&gt; 
   &lt;td align="center"&gt;83.6&lt;/td&gt; 
   &lt;td align="center"&gt;57.7&lt;/td&gt; 
   &lt;td align="center"&gt;2.12&lt;/td&gt; 
   &lt;td align="center"&gt;3.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;D-FINE-S&lt;/td&gt; 
   &lt;td align="center"&gt;67.6&lt;/td&gt; 
   &lt;td align="center"&gt;50.7&lt;/td&gt; 
   &lt;td align="center"&gt;84.5&lt;/td&gt; 
   &lt;td align="center"&gt;59.9&lt;/td&gt; 
   &lt;td align="center"&gt;3.55&lt;/td&gt; 
   &lt;td align="center"&gt;10.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;D-FINE-M&lt;/td&gt; 
   &lt;td align="center"&gt;72.6&lt;/td&gt; 
   &lt;td align="center"&gt;55.1&lt;/td&gt; 
   &lt;td align="center"&gt;84.6&lt;/td&gt; 
   &lt;td align="center"&gt;60.2&lt;/td&gt; 
   &lt;td align="center"&gt;5.68&lt;/td&gt; 
   &lt;td align="center"&gt;19.2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://rfdetr.roboflow.com/learn/benchmarks/"&gt;See our benchmark notes in the RF-DETR documentation.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;We are actively working on RF-DETR Large and X-Large models using the same techniques we used to achieve the strong accuracy that RF-DETR Medium attains. This is why RF-DETR Large and X-Large is not yet reported on our pareto charts and why we haven't benchmarked other models at similar sizes. Check back in the next few weeks for the launch of new RF-DETR Large and X-Large models.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install RF-DETR, install the &lt;code&gt;rfdetr&lt;/code&gt; package in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.9&lt;/strong&gt;&lt;/a&gt; environment with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install rfdetr
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Install from source&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;By installing RF-DETR from source, you can explore the most recent features and enhancements that have not yet been officially released. Please note that these updates are still in development and may not be as stable as the latest published release.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/roboflow/rf-detr.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;p&gt;The easiest path to deployment is using Roboflow's &lt;a href="https://github.com/roboflow/inference"&gt;Inference&lt;/a&gt; package.&lt;/p&gt; 
&lt;p&gt;The code below lets you run &lt;code&gt;rfdetr-base&lt;/code&gt; on an image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import supervision as sv
from inference import get_model
from PIL import Image
from io import BytesIO
import requests

url = "https://media.roboflow.com/dog.jpeg"
image = Image.open(BytesIO(requests.get(url).content))

model = get_model("rfdetr-base")

predictions = model.infer(image, confidence=0.5)[0]

detections = sv.Detections.from_inference(predictions)

labels = [prediction.class_name for prediction in predictions.predictions]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Predict&lt;/h2&gt; 
&lt;p&gt;You can also use the .predict method to perform inference during local development. The &lt;code&gt;.predict()&lt;/code&gt; method accepts various input formats, including file paths, PIL images, NumPy arrays, and torch tensors. Please ensure inputs use RGB channel order. For &lt;code&gt;torch.Tensor&lt;/code&gt; inputs specifically, they must have a shape of &lt;code&gt;(3, H, W)&lt;/code&gt; with values normalized to the &lt;code&gt;[0..1)&lt;/code&gt; range. If you don't plan to modify the image or batch size dynamically at runtime, you can also use &lt;code&gt;.optimize_for_inference()&lt;/code&gt; to get up to 2x end-to-end speedup, depending on platform.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import io
import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRBase
from rfdetr.util.coco_classes import COCO_CLASSES

model = RFDETRBase()

model.optimize_for_inference()

url = "https://media.roboflow.com/notebooks/examples/dog-2.jpeg"

image = Image.open(io.BytesIO(requests.get(url).content))
detections = model.predict(image, threshold=0.5)

labels = [
    f"{COCO_CLASSES[class_id]} {confidence:.2f}"
    for class_id, confidence
    in zip(detections.class_id, detections.confidence)
]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)

sv.plot_image(annotated_image)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Train a Model&lt;/h3&gt; 
&lt;p&gt;You can fine-tune an RF-DETR Nano, Small, Medium, and Base model with a custom dataset using the &lt;code&gt;rfdetr&lt;/code&gt; Python package.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://rfdetr.roboflow.com/learn/train/"&gt;Read our training tutorial to get started&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://rfdetr.roboflow.com"&gt;documentation website&lt;/a&gt; to learn more about how to use RF-DETR.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Both the code and the weights pretrained on the COCO dataset are released under the &lt;a href="https://github.com/roboflow/r-flow/raw/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Our work is built upon &lt;a href="https://arxiv.org/pdf/2406.03459"&gt;LW-DETR&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2304.07193"&gt;DINOv2&lt;/a&gt;, and &lt;a href="https://arxiv.org/pdf/2010.04159"&gt;Deformable DETR&lt;/a&gt;. Thanks to their authors for their excellent work!&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{rf-detr,
  author = {Robinson, Isaac and Robicheaux, Peter and Popov, Matvei},
  license = {Apache-2.0},
  title = {RF-DETR},
  howpublished = {\url{https://github.com/roboflow/rf-detr}},
  year = {2025},
  note = {SOTA Real-Time Object Detection Model}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome and appreciate all contributions! If you notice any issues or bugs, have questions, or would like to suggest new features, please &lt;a href="https://github.com/roboflow/rf-detr/issues/new"&gt;open an issue&lt;/a&gt; or pull request. By sharing your ideas and improvements, you help make RF-DETR better for everyone.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://youtube.com/roboflow"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652" width="3%" /&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; 
 &lt;a href="https://roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649" width="3%" /&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; 
 &lt;a href="https://www.linkedin.com/company/roboflow-ai/"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691" width="3%" /&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; 
 &lt;a href="https://docs.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511" width="3%" /&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; 
 &lt;a href="https://discuss.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584" width="3%" /&gt; &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; &lt;/a&gt;
 &lt;a href="https://blog.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605" width="3%" /&gt; &lt;/a&gt;  
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>laude-institute/terminal-bench</title>
      <link>https://github.com/laude-institute/terminal-bench</link>
      <description>&lt;p&gt;A benchmark for LLMs on complicated tasks in the terminal&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;terminal-bench&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;#####################################################################
#  _____                   _             _     ______________       #
# |_   _|__ _ __ _ __ ___ (_)_ __   __ _| |   ||            ||      #
#   | |/ _ \ '__| '_ ` _ \| | '_ \ / _` | |   || &amp;gt;          ||      #
#   | |  __/ |  | | | | | | | | | | (_| | |   ||            ||      #
#   |_|\___|_|  |_| |_| |_|_|_| |_|\__,_|_|   ||____________||      #
#   ____                  _                   |______________|      #
#  | __ )  ___ _ __   ___| |__                 \\############\\     #
#  |  _ \ / _ \ '_ \ / __| '_ \                 \\############\\    # 
#  | |_) |  __/ | | | (__| | | |                 \      ____    \   #
#  |____/ \___|_| |_|\___|_| |_|                  \_____\___\____\  #
#                                                                   #
#####################################################################
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/6xWPKhGDbA"&gt;&lt;img src="https://img.shields.io/badge/Join_our_discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/laude-institute/terminal-bench"&gt;&lt;img src="https://img.shields.io/badge/T--Bench-000000?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=000&amp;amp;logoColor=white" alt="Github" /&gt;&lt;/a&gt; &lt;a href="https://www.tbench.ai/docs"&gt;&lt;img src="https://img.shields.io/badge/Docs-000000?style=for-the-badge&amp;amp;logo=mdbook&amp;amp;color=105864" alt="Docs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is the benchmark for testing AI agents in real terminal environments. From compiling code to training models and setting up servers, Terminal-Bench evaluates how well agents can handle real-world, end-to-end tasks - autonomously.&lt;/p&gt; 
&lt;p&gt;Whether you're building LLM agents, benchmarking frameworks, or stress-testing system-level reasoning, Terminal-Bench gives you a reproducible task suite and execution harness designed for practical, real-world evaluation.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench consists of two parts: a &lt;strong&gt;dataset of tasks&lt;/strong&gt;, and an &lt;strong&gt;execution harness&lt;/strong&gt; that connects a language model to our terminal sandbox.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is currently in &lt;strong&gt;beta&lt;/strong&gt; with ~100 tasks. Over the coming months, we are going to expand Terminal-Bench into comprehensive testbed for AI agents in text-based environments. Any contributions are welcome, especially new and challenging tasks!&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Our &lt;a href="https://www.tbench.ai/docs/installation"&gt;Quickstart Guide&lt;/a&gt; will walk you through installing the repo and contributing.&lt;/p&gt; 
&lt;p&gt;Terminal-Bench is distributed as a pip package and can be run using the Terminal-Bench CLI: &lt;code&gt;tb&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install terminal-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Further Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/tasks"&gt;Task Gallery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/task-ideas"&gt;Task Ideas&lt;/a&gt; - Browse community-sourced task ideas&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tbench.ai/docs/dashboard"&gt;Dashboard Documentation&lt;/a&gt; - Information about the Terminal-Bench dashboard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Core Components&lt;/h2&gt; 
&lt;h3&gt;Dataset of Tasks&lt;/h3&gt; 
&lt;p&gt;Each task in Terminal-Bench includes&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a instruction in English,&lt;/li&gt; 
 &lt;li&gt;a test script to verify if the language model / agent completed the task successfully,&lt;/li&gt; 
 &lt;li&gt;a reference ("oracle") solution that solves the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Tasks are located in the &lt;a href="https://raw.githubusercontent.com/laude-institute/terminal-bench/main/tasks"&gt;&lt;code&gt;tasks&lt;/code&gt;&lt;/a&gt; folder of the repository, and the aforementioned list of current tasks gives an overview that is easy to browse.&lt;/p&gt; 
&lt;h3&gt;Execution Harness&lt;/h3&gt; 
&lt;p&gt;The harness connects language models to a sandboxed terminal environment. After &lt;a href="https://www.tbench.ai/docs/installation"&gt;installing the terminal-bench package&lt;/a&gt; (along with the dependencies &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;Docker&lt;/code&gt;) you can view how to run the harness using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed information about running the harness and its options, see the &lt;a href="https://www.tbench.ai/docs/first-steps"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Submit to Our Leaderboard&lt;/h3&gt; 
&lt;p&gt;Terminal-Bench-Core v0.1.1 is the set of tasks for Terminal-Bench's beta release and corresponds to the current leaderboard. To evaluate on it pass &lt;code&gt;--dataset-name terminal-bench-core&lt;/code&gt; and &lt;code&gt;--dataset-version 0.1.1&lt;/code&gt; to the harness. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tb run \
    --agent terminus \
    --model-name anthropic/claude-3-7-latest \
    --dataset-name terminal-bench-core
    --dataset-version 0.1.1
    --n-concurrent 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed instructions on submitting to the leaderboard, view our &lt;a href="https://www.tbench.ai/docs/submitting-to-leaderboard"&gt;leaderboard submission guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more information on Terminal-Bench datasets and versioning view our &lt;a href="https://www.tbench.ai/docs/registry"&gt;registry overview&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Creating New Tasks&lt;/h2&gt; 
&lt;p&gt;View our &lt;a href="https://www.tbench.ai/docs/task-quickstart"&gt;task contribution quickstart&lt;/a&gt; to create a new task.&lt;/p&gt; 
&lt;h2&gt;Citing Us&lt;/h2&gt; 
&lt;p&gt;If you found Terminal-Bench useful, please cite us as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{tbench_2025,
      title={Terminal-Bench: A Benchmark for AI Agents in Terminal Environments}, 
      url={https://github.com/laude-institute/terminal-bench}, 
      author={The Terminal-Bench Team}, year={2025}, month={Apr}} 
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>livekit/agents</title>
      <link>https://github.com/livekit/agents</link>
      <description>&lt;p&gt;A powerful framework for building realtime voice AI agents ü§ñüéôÔ∏èüìπ&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="/.github/banner_dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="/.github/banner_light.png" /&gt; 
 &lt;img style="width:100%;" alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png" /&gt; 
&lt;/picture&gt; 
&lt;!--END_BANNER_IMAGE--&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/livekit-agents" alt="PyPI - Version" /&gt; &lt;a href="https://pepy.tech/projects/livekit-agents"&gt;&lt;img src="https://static.pepy.tech/badge/livekit-agents/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://livekit.io/join-slack"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Flivekit.io%2Fbadges%2Fslack" alt="Slack community" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/livekit"&gt;&lt;img src="https://img.shields.io/twitter/follow/livekit" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/livekit/agents"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki for understanding the codebase" /&gt;&lt;/a&gt; &lt;a href="https://github.com/livekit/livekit/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/livekit/livekit" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Looking for the JS/TS library? Check out &lt;a href="https://github.com/livekit/agents-js"&gt;AgentsJS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Agents?&lt;/h2&gt; 
&lt;!--BEGIN_DESCRIPTION--&gt; 
&lt;p&gt;The Agent Framework is designed for building realtime, programmable participants that run on servers. Use it to create conversational, multi-modal voice agents that can see, hear, and understand.&lt;/p&gt; 
&lt;!--END_DESCRIPTION--&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible integrations&lt;/strong&gt;: A comprehensive ecosystem to mix and match the right STT, LLM, TTS, and Realtime API to suit your use case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated job scheduling&lt;/strong&gt;: Built-in task scheduling and distribution with &lt;a href="https://docs.livekit.io/agents/build/dispatch/"&gt;dispatch APIs&lt;/a&gt; to connect end users to agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive WebRTC clients&lt;/strong&gt;: Build client applications using LiveKit's open-source SDK ecosystem, supporting all major platforms.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Telephony integration&lt;/strong&gt;: Works seamlessly with LiveKit's &lt;a href="https://docs.livekit.io/sip/"&gt;telephony stack&lt;/a&gt;, allowing your agent to make calls to or receive calls from phones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exchange data with clients&lt;/strong&gt;: Use &lt;a href="https://docs.livekit.io/home/client/data/rpc/"&gt;RPCs&lt;/a&gt; and other &lt;a href="https://docs.livekit.io/home/client/data/"&gt;Data APIs&lt;/a&gt; to seamlessly exchange data with clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic turn detection&lt;/strong&gt;: Uses a transformer model to detect when a user is done with their turn, helps to reduce interruptions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP support&lt;/strong&gt;: Native support for MCP. Integrate tools provided by MCP servers with one loc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Builtin test framework&lt;/strong&gt;: Write tests and use judges to ensure your agent is performing as expected.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open-source&lt;/strong&gt;: Fully open-source, allowing you to run the entire stack on your own servers, including &lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt;, one of the most widely used WebRTC media servers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install the core Agents library, along with plugins for popular model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docs and guides&lt;/h2&gt; 
&lt;p&gt;Documentation on the framework and how to use it can be found &lt;a href="https://docs.livekit.io/agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core concepts&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent: An LLM-based application with defined instructions.&lt;/li&gt; 
 &lt;li&gt;AgentSession: A container for agents that manages interactions with end users.&lt;/li&gt; 
 &lt;li&gt;entrypoint: The starting point for an interactive session, similar to a request handler in a web server.&lt;/li&gt; 
 &lt;li&gt;Worker: The main process that coordinates job scheduling and launches agents for user sessions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Simple voice agent&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    RunContext,
    WorkerOptions,
    cli,
    function_tool,
)
from livekit.plugins import deepgram, elevenlabs, openai, silero

@function_tool
async def lookup_weather(
    context: RunContext,
    location: str,
):
    """Used to look up weather information."""

    return {"weather": "sunny", "temperature": 70}


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    agent = Agent(
        instructions="You are a friendly voice assistant built by LiveKit.",
        tools=[lookup_weather],
    )
    session = AgentSession(
        vad=silero.VAD.load(),
        # any combination of STT, LLM, TTS, or realtime API can be used
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=elevenlabs.TTS(),
    )

    await session.start(agent=agent, room=ctx.room)
    await session.generate_reply(instructions="greet the user and ask about their day")


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll need the following environment variables for this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DEEPGRAM_API_KEY&lt;/li&gt; 
 &lt;li&gt;OPENAI_API_KEY&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-agent handoff&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;This code snippet is abbreviated. For the full example, see &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/multi_agent.py"&gt;multi_agent.py&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
class IntroAgent(Agent):
    def __init__(self) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a story teller. Your goal is to gather a few pieces of information from the user to make the story personalized and engaging."
            "Ask the user for their name and where they are from"
        )

    async def on_enter(self):
        self.session.generate_reply(instructions="greet the user and gather information")

    @function_tool
    async def information_gathered(
        self,
        context: RunContext,
        name: str,
        location: str,
    ):
        """Called when the user has provided the information needed to make the story personalized and engaging.

        Args:
            name: The name of the user
            location: The location of the user
        """

        context.userdata.name = name
        context.userdata.location = location

        story_agent = StoryAgent(name, location)
        return story_agent, "Let's start the story!"


class StoryAgent(Agent):
    def __init__(self, name: str, location: str) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a storyteller. Use the user's information in order to make the story personalized."
            f"The user's name is {name}, from {location}"
            # override the default model, switching to Realtime API from standard LLMs
            llm=openai.realtime.RealtimeModel(voice="echo"),
            chat_ctx=chat_ctx,
        )

    async def on_enter(self):
        self.session.generate_reply()


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    userdata = StoryData()
    session = AgentSession[StoryData](
        vad=silero.VAD.load(),
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=openai.TTS(voice="echo"),
        userdata=userdata,
    )

    await session.start(
        agent=IntroAgent(),
        room=ctx.room,
    )
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;Automated tests are essential for building reliable agents, especially with the non-deterministic behavior of LLMs. LiveKit Agents include native test integration to help you create dependable agents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@pytest.mark.asyncio
async def test_no_availability() -&amp;gt; None:
    llm = google.LLM()
    async AgentSession(llm=llm) as sess:
        await sess.start(MyAgent())
        result = await sess.run(
            user_input="Hello, I need to place an order."
        )
        result.expect.skip_next_event_if(type="message", role="assistant")
        result.expect.next_event().is_function_call(name="start_order")
        result.expect.next_event().is_function_call_output()
        await (
            result.expect.next_event()
            .is_message(role="assistant")
            .judge(llm, intent="assistant should be asking the user what they would like")
        )

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéôÔ∏è Starter Agent&lt;/h3&gt; &lt;p&gt;A starter agent optimized for voice conversations.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/basic_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîÑ Multi-user push to talk&lt;/h3&gt; &lt;p&gt;Responds to multiple users in the room via push-to-talk.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/push_to_talk.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéµ Background audio&lt;/h3&gt; &lt;p&gt;Background ambient and thinking audio to improve realism.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/background_audio.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üõ†Ô∏è Dynamic tool creation&lt;/h3&gt; &lt;p&gt;Creating function tools dynamically.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/dynamic_tool_creation.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;‚òéÔ∏è Outbound caller&lt;/h3&gt; &lt;p&gt;Agent that makes outbound phone calls&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/outbound-caller-python"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìã Structured output&lt;/h3&gt; &lt;p&gt;Using structured output from LLM to guide TTS tone.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/structured_output.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîå MCP support&lt;/h3&gt; &lt;p&gt;Use tools from MCP servers&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/mcp"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üí¨ Text-only agent&lt;/h3&gt; &lt;p&gt;Skip voice altogether and use the same code for text-only integrations&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/text_only.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìù Multi-user transcriber&lt;/h3&gt; &lt;p&gt;Produce transcriptions from all users in the room&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/transcription/multi-user-transcriber.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üé• Video avatars&lt;/h3&gt; &lt;p&gt;Add an AI avatar with Tavus, Beyond Presence, and Bithuman&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/avatar_agents/"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üçΩÔ∏è Restaurant ordering and reservations&lt;/h3&gt; &lt;p&gt;Full example of an agent that handles calls for a restaurant.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/restaurant_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üëÅÔ∏è Gemini Live vision&lt;/h3&gt; &lt;p&gt;Full example (including iOS app) of Gemini Live agent that can see.&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/vision-demo"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Running your agent&lt;/h2&gt; 
&lt;h3&gt;Testing in terminal&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py console
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs your agent in terminal mode, enabling local audio input and output for testing. This mode doesn't require external servers or dependencies and is useful for quickly validating behavior.&lt;/p&gt; 
&lt;h3&gt;Developing with LiveKit clients&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starts the agent server and enables hot reloading when files change. This mode allows each process to host multiple concurrent agents efficiently.&lt;/p&gt; 
&lt;p&gt;The agent connects to LiveKit Cloud or your self-hosted server. Set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LIVEKIT_URL&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_KEY&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_SECRET&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can connect using any LiveKit client SDK or telephony integration. To get started quickly, try the &lt;a href="https://agents-playground.livekit.io/"&gt;Agents Playground&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running for production&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs the agent with production-ready optimizations.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's &lt;a href="https://livekit.io/join-slack"&gt;Slack community&lt;/a&gt;.&lt;/p&gt; 
&lt;!--BEGIN_REPO_NAV--&gt; 
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;table&gt; 
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th colspan="2"&gt;LiveKit Ecosystem&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;LiveKit SDKs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/client-sdk-js"&gt;Browser&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-swift"&gt;iOS/macOS/visionOS&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-android"&gt;Android&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-flutter"&gt;Flutter&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-react-native"&gt;React Native&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity"&gt;Unity&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity-web"&gt;Unity (WebGL)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-esp32"&gt;ESP32&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Server APIs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-go"&gt;Golang&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-ruby"&gt;Ruby&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-kotlin"&gt;Java/Kotlin&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/agence104/livekit-server-sdk-php"&gt;PHP (community)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/pabloFuente/livekit-server-sdk-dotnet"&gt;.NET (community)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;UI Components&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/components-js"&gt;React&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-android"&gt;Android Compose&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-swift"&gt;SwiftUI&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Agents Frameworks&lt;/td&gt;
   &lt;td&gt;&lt;b&gt;Python&lt;/b&gt; ¬∑ &lt;a href="https://github.com/livekit/agents-js"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/agent-playground"&gt;Playground&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Services&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/egress"&gt;Egress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/ingress"&gt;Ingress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/sip"&gt;SIP&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Resources&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://docs.livekit.io"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit-examples"&gt;Example apps&lt;/a&gt; ¬∑ &lt;a href="https://livekit.io/cloud"&gt;Cloud&lt;/a&gt; ¬∑ &lt;a href="https://docs.livekit.io/home/self-hosting/deployment"&gt;Self-hosting&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/livekit-cli"&gt;CLI&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!--END_REPO_NAV--&gt;</description>
    </item>
    
    <item>
      <title>HunxByts/GhostTrack</title>
      <link>https://github.com/HunxByts/GhostTrack</link>
      <description>&lt;p&gt;Useful tool to track location or mobile number&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GhostTrack&lt;/h1&gt; 
&lt;p&gt;Useful tool to track location or mobile number, so this tool can be called osint or also information gathering&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png" /&gt; 
&lt;p&gt;New update : &lt;code&gt;Version 2.2&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Instalation on Linux (deb)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install git
sudo apt-get install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Instalation on Termux&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pkg install git
pkg install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage Tool&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/HunxByts/GhostTrack.git
cd GhostTrack
pip3 install -r requirements.txt
python3 GhostTR.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;IP Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png " /&gt; 
&lt;p&gt;on the IP Track menu, you can combo with the seeker tool to get the target IP&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;‚ö°&lt;/span&gt; Install Seeker :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/thewhiteh4t/seeker"&gt;Get Seeker&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Phone Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png" /&gt; 
&lt;p&gt;on this menu you can search for information from the target phone number&lt;/p&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Username Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png" /&gt; on this menu you can search for information from the target username on social media 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;‚ö°&lt;/span&gt; Author :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/HunxByts"&gt;HunxByts&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>KoljaB/RealtimeVoiceChat</title>
      <link>https://github.com/KoljaB/RealtimeVoiceChat</link>
      <description>&lt;p&gt;Have a natural, spoken conversation with AI!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time AI Voice Chat üé§üí¨üß†üîä&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Have a natural, spoken conversation with an AI!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This project lets you chat with a Large Language Model (LLM) using just your voice, receiving spoken responses in near real-time. Think of it as your own digital conversation partner.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/16cc29a7-bec2-4dd0-a056-d213db798d8f"&gt;https://github.com/user-attachments/assets/16cc29a7-bec2-4dd0-a056-d213db798d8f&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;(early preview - first reasonably stable version)&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ùó &lt;strong&gt;Project Status: Community-Driven&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;This project is no longer being actively maintained by me due to time constraints. I've taken on too many projects and I have to step back. I will no longer be implementing new features or providing user support.&lt;/p&gt; 
 &lt;p&gt;I will continue to review and merge high-quality, well-written Pull Requests from the community from time to time. Your contributions are welcome and appreciated!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;What's Under the Hood?&lt;/h2&gt; 
&lt;p&gt;A sophisticated client-server system built for low-latency interaction:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;üéôÔ∏è &lt;strong&gt;Capture:&lt;/strong&gt; Your voice is captured by your browser.&lt;/li&gt; 
 &lt;li&gt;‚û°Ô∏è &lt;strong&gt;Stream:&lt;/strong&gt; Audio chunks are whisked away via WebSockets to a Python backend.&lt;/li&gt; 
 &lt;li&gt;‚úçÔ∏è &lt;strong&gt;Transcribe:&lt;/strong&gt; &lt;code&gt;RealtimeSTT&lt;/code&gt; rapidly converts your speech to text.&lt;/li&gt; 
 &lt;li&gt;ü§î &lt;strong&gt;Think:&lt;/strong&gt; The text is sent to an LLM (like Ollama or OpenAI) for processing.&lt;/li&gt; 
 &lt;li&gt;üó£Ô∏è &lt;strong&gt;Synthesize:&lt;/strong&gt; The AI's text response is turned back into speech using &lt;code&gt;RealtimeTTS&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;‚¨ÖÔ∏è &lt;strong&gt;Return:&lt;/strong&gt; The generated audio is streamed back to your browser for playback.&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Interrupt:&lt;/strong&gt; Jump in anytime! The system handles interruptions gracefully.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Key Features ‚ú®&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fluid Conversation:&lt;/strong&gt; Speak and listen, just like a real chat.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Feedback:&lt;/strong&gt; See partial transcriptions and AI responses as they happen.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Low Latency Focus:&lt;/strong&gt; Optimized architecture using audio chunk streaming.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Turn-Taking:&lt;/strong&gt; Dynamic silence detection (&lt;code&gt;turndetect.py&lt;/code&gt;) adapts to the conversation pace.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible AI Brains:&lt;/strong&gt; Pluggable LLM backends (Ollama default, OpenAI support via &lt;code&gt;llm_module.py&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable Voices:&lt;/strong&gt; Choose from different Text-to-Speech engines (Kokoro, Coqui, Orpheus via &lt;code&gt;audio_module.py&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web Interface:&lt;/strong&gt; Clean and simple UI using Vanilla JS and the Web Audio API.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dockerized Deployment:&lt;/strong&gt; Recommended setup using Docker Compose for easier dependency management.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Technology Stack üõ†Ô∏è&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; Python &amp;lt; 3.13, FastAPI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; HTML, CSS, JavaScript (Vanilla JS, Web Audio API, AudioWorklets)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Communication:&lt;/strong&gt; WebSockets&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Containerization:&lt;/strong&gt; Docker, Docker Compose&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core AI/ML Libraries:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;RealtimeSTT&lt;/code&gt; (Speech-to-Text)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;RealtimeTTS&lt;/code&gt; (Text-to-Speech)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; (Turn detection, Tokenization)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;torch&lt;/code&gt; / &lt;code&gt;torchaudio&lt;/code&gt; (ML Framework)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ollama&lt;/code&gt; / &lt;code&gt;openai&lt;/code&gt; (LLM Clients)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Audio Processing:&lt;/strong&gt; &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Before You Dive In: Prerequisites üèä‚Äç‚ôÄÔ∏è&lt;/h2&gt; 
&lt;p&gt;This project leverages powerful AI models, which have some requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Operating System:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Docker:&lt;/strong&gt; Linux is recommended for the best GPU integration with Docker.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Manual:&lt;/strong&gt; The provided script (&lt;code&gt;install.bat&lt;/code&gt;) is for Windows. Manual steps are possible on Linux/macOS but may require more troubleshooting (especially for DeepSpeed).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêç Python:&lt;/strong&gt; 3.9 or higher (if setting up manually).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üöÄ GPU:&lt;/strong&gt; &lt;strong&gt;A powerful CUDA-enabled NVIDIA GPU is &lt;em&gt;highly recommended&lt;/em&gt;&lt;/strong&gt;, especially for faster STT (Whisper) and TTS (Coqui). Performance on CPU-only or weaker GPUs will be significantly slower. 
  &lt;ul&gt; 
   &lt;li&gt;The setup assumes &lt;strong&gt;CUDA 12.1&lt;/strong&gt;. Adjust PyTorch installation if you have a different CUDA version.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Docker (Linux):&lt;/strong&gt; Requires &lt;a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"&gt;NVIDIA Container Toolkit&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üê≥ Docker (Optional but Recommended):&lt;/strong&gt; Docker Engine and Docker Compose v2+ for the containerized setup.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üß† Ollama (Optional):&lt;/strong&gt; If using the Ollama backend &lt;em&gt;without&lt;/em&gt; Docker, install it separately and pull your desired models. The Docker setup includes an Ollama service.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîë OpenAI API Key (Optional):&lt;/strong&gt; If using the OpenAI backend, set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable (e.g., in a &lt;code&gt;.env&lt;/code&gt; file or passed to Docker).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Getting Started: Installation &amp;amp; Setup ‚öôÔ∏è&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Clone the repository first:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/KoljaB/RealtimeVoiceChat.git
cd RealtimeVoiceChat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, choose your adventure:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üöÄ Option A: Docker Installation (Recommended for Linux/GPU)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;This is the most straightforward method, bundling the application, dependencies, and even Ollama into manageable containers.&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Build the Docker images:&lt;/strong&gt; &lt;em&gt;(This takes time! It downloads base images, installs Python/ML dependencies, and pre-downloads the default STT model.)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose build
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;(If you want to customize models/settings in &lt;code&gt;code/*.py&lt;/code&gt;, do it &lt;strong&gt;before&lt;/strong&gt; this step!)&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the services (App &amp;amp; Ollama):&lt;/strong&gt; &lt;em&gt;(Runs containers in the background. GPU access is configured in &lt;code&gt;docker-compose.yml&lt;/code&gt;.)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Give them a minute to initialize.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;(Crucial!) Pull your desired Ollama Model:&lt;/strong&gt; &lt;em&gt;(This is done &lt;em&gt;after&lt;/em&gt; startup to keep the main app image smaller and allow model changes without rebuilding. Execute this command to pull the default model into the running Ollama container.)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Pull the default model (adjust if you configured a different one in server.py)
docker compose exec ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

# (Optional) Verify the model is available
docker compose exec ollama ollama list
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stopping the Services:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose down
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Restarting:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Viewing Logs / Debugging:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Follow app logs: &lt;code&gt;docker compose logs -f app&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;Follow Ollama logs: &lt;code&gt;docker compose logs -f ollama&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;Save logs to file: &lt;code&gt;docker compose logs app &amp;gt; app_logs.txt&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Option B: Manual Installation (Windows Script / venv)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;This method requires managing the Python environment yourself. It offers more direct control but can be trickier, especially regarding ML dependencies.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;B1) Using the Windows Install Script:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Ensure you meet the prerequisites (Python, potentially CUDA drivers).&lt;/li&gt; 
  &lt;li&gt;Run the script. It attempts to create a venv, install PyTorch for CUDA 12.1, a compatible DeepSpeed wheel, and other requirements. &lt;pre&gt;&lt;code class="language-batch"&gt;install.bat
&lt;/code&gt;&lt;/pre&gt; &lt;em&gt;(This opens a new command prompt within the activated virtual environment.)&lt;/em&gt; Proceed to the &lt;strong&gt;"Running the Application"&lt;/strong&gt; section.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;B2) Manual Steps (Linux/macOS/Windows):&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create &amp;amp; Activate Virtual Environment:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
# Linux/macOS:
source venv/bin/activate
# Windows:
.\venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgrade Pip:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install --upgrade pip
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to Code Directory:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd code
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install PyTorch (Crucial Step - Match Your Hardware!):&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;With NVIDIA GPU (CUDA 12.1 Example):&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Verify your CUDA version! Adjust 'cu121' and the URL if needed.
pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;CPU Only (Expect Slow Performance):&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# pip install torch torchaudio torchvision
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;em&gt;Find other PyTorch versions:&lt;/em&gt; &lt;a href="https://pytorch.org/get-started/previous-versions/"&gt;https://pytorch.org/get-started/previous-versions/&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Other Requirements:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Note on DeepSpeed:&lt;/strong&gt; The &lt;code&gt;requirements.txt&lt;/code&gt; may include DeepSpeed. Installation can be complex, especially on Windows. The &lt;code&gt;install.bat&lt;/code&gt; tries a precompiled wheel. If manual installation fails, you might need to build it from source or consult resources like &lt;a href="https://github.com/erew123/deepspeedpatcher"&gt;deepspeedpatcher&lt;/a&gt; (use at your own risk). Coqui TTS performance benefits most from DeepSpeed.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running the Application ‚ñ∂Ô∏è&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;If using Docker:&lt;/strong&gt; Your application is already running via &lt;code&gt;docker compose up -d&lt;/code&gt;! Check logs using &lt;code&gt;docker compose logs -f app&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If using Manual/Script Installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Activate your virtual environment&lt;/strong&gt; (if not already active): &lt;pre&gt;&lt;code class="language-bash"&gt;# Linux/macOS: source ../venv/bin/activate
# Windows: ..\venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Navigate to the &lt;code&gt;code&lt;/code&gt; directory&lt;/strong&gt; (if not already there): &lt;pre&gt;&lt;code class="language-bash"&gt;cd code
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start the FastAPI server:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python server.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Accessing the Client (Both Methods):&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open your web browser to &lt;code&gt;http://localhost:8000&lt;/code&gt; (or your server's IP if running remotely/in Docker on another machine).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Grant microphone permissions&lt;/strong&gt; when prompted.&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;"Start"&lt;/strong&gt; to begin chatting! Use "Stop" to end and "Reset" to clear the conversation.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Configuration Deep Dive üîß&lt;/h2&gt; 
&lt;p&gt;Want to tweak the AI's voice, brain, or how it listens? Modify the Python files in the &lt;code&gt;code/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important Docker Note:&lt;/strong&gt; If using Docker, make any configuration changes &lt;em&gt;before&lt;/em&gt; running &lt;code&gt;docker compose build&lt;/code&gt; to ensure they are included in the image.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TTS Engine &amp;amp; Voice (&lt;code&gt;server.py&lt;/code&gt;, &lt;code&gt;audio_module.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Change &lt;code&gt;START_ENGINE&lt;/code&gt; in &lt;code&gt;server.py&lt;/code&gt; to &lt;code&gt;"coqui"&lt;/code&gt;, &lt;code&gt;"kokoro"&lt;/code&gt;, or &lt;code&gt;"orpheus"&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Adjust engine-specific settings (e.g., voice model path for Coqui, speaker ID for Orpheus, speed) within &lt;code&gt;AudioProcessor.__init__&lt;/code&gt; in &lt;code&gt;audio_module.py&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Backend &amp;amp; Model (&lt;code&gt;server.py&lt;/code&gt;, &lt;code&gt;llm_module.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Set &lt;code&gt;LLM_START_PROVIDER&lt;/code&gt; (&lt;code&gt;"ollama"&lt;/code&gt; or &lt;code&gt;"openai"&lt;/code&gt;) and &lt;code&gt;LLM_START_MODEL&lt;/code&gt; (e.g., &lt;code&gt;"hf.co/..."&lt;/code&gt; for Ollama, model name for OpenAI) in &lt;code&gt;server.py&lt;/code&gt;. Remember to pull the Ollama model if using Docker (see Installation Step A3).&lt;/li&gt; 
   &lt;li&gt;Customize the AI's personality by editing &lt;code&gt;system_prompt.txt&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;STT Settings (&lt;code&gt;transcribe.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Modify &lt;code&gt;DEFAULT_RECORDER_CONFIG&lt;/code&gt; to change the Whisper model (&lt;code&gt;model&lt;/code&gt;), language (&lt;code&gt;language&lt;/code&gt;), silence thresholds (&lt;code&gt;silence_limit_seconds&lt;/code&gt;), etc. The default &lt;code&gt;base.en&lt;/code&gt; model is pre-downloaded during the Docker build.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Turn Detection Sensitivity (&lt;code&gt;turndetect.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Adjust pause duration constants within the &lt;code&gt;TurnDetector.update_settings&lt;/code&gt; method.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SSL/HTTPS (&lt;code&gt;server.py&lt;/code&gt;):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Set &lt;code&gt;USE_SSL = True&lt;/code&gt; and provide paths to your certificate (&lt;code&gt;SSL_CERT_PATH&lt;/code&gt;) and key (&lt;code&gt;SSL_KEY_PATH&lt;/code&gt;) files.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Docker Users:&lt;/strong&gt; You'll need to adjust &lt;code&gt;docker-compose.yml&lt;/code&gt; to map the SSL port (e.g., 443) and potentially mount your certificate files as volumes.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;details&gt; 
   &lt;summary&gt;&lt;strong&gt;Generating Local SSL Certificates (Windows Example w/ mkcert)&lt;/strong&gt;&lt;/summary&gt; 
   &lt;ol&gt; 
    &lt;li&gt;Install Chocolatey package manager if you haven't already.&lt;/li&gt; 
    &lt;li&gt;Install mkcert: &lt;code&gt;choco install mkcert&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;Run Command Prompt &lt;em&gt;as Administrator&lt;/em&gt;.&lt;/li&gt; 
    &lt;li&gt;Install a local Certificate Authority: &lt;code&gt;mkcert -install&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;Generate certs (replace &lt;code&gt;your.local.ip&lt;/code&gt;): &lt;code&gt;mkcert localhost 127.0.0.1 ::1 your.local.ip&lt;/code&gt; 
     &lt;ul&gt; 
      &lt;li&gt;This creates &lt;code&gt;.pem&lt;/code&gt; files (e.g., &lt;code&gt;localhost+3.pem&lt;/code&gt; and &lt;code&gt;localhost+3-key.pem&lt;/code&gt;) in the current directory. Update &lt;code&gt;SSL_CERT_PATH&lt;/code&gt; and &lt;code&gt;SSL_KEY_PATH&lt;/code&gt; in &lt;code&gt;server.py&lt;/code&gt; accordingly. Remember to potentially mount these into your Docker container.&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing ü§ù&lt;/h2&gt; 
&lt;p&gt;Got ideas or found a bug? Contributions are welcome! Feel free to open issues or submit pull requests.&lt;/p&gt; 
&lt;h2&gt;License üìú&lt;/h2&gt; 
&lt;p&gt;The core codebase of this project is released under the &lt;strong&gt;MIT License&lt;/strong&gt; (see the &lt;a href="https://raw.githubusercontent.com/KoljaB/RealtimeVoiceChat/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details).&lt;/p&gt; 
&lt;p&gt;This project relies on external specific TTS engines (like &lt;code&gt;Coqui XTTSv2&lt;/code&gt;) and LLM providers which have their &lt;strong&gt;own licensing terms&lt;/strong&gt;. Please ensure you comply with the licenses of all components you use.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>deepseek-ai/DeepSeek-V3</title>
      <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://www.deepseek.com/"&gt;&lt;img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true" /&gt;&lt;/a&gt; 
 &lt;a href="https://chat.deepseek.com/"&gt;&lt;img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://discord.gg/Tc7c45Zzu5"&gt;&lt;img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/qr.jpeg?raw=true"&gt;&lt;img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://twitter.com/deepseek_ai"&gt;&lt;img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-CODE"&gt;&lt;img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-MODEL"&gt;&lt;img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://arxiv.org/pdf/2412.19437"&gt;&lt;b&gt;Paper Link&lt;/b&gt;üëÅÔ∏è&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#1-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#2-model-summary"&gt;Model Summary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#3-model-downloads"&gt;Model Downloads&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#4-evaluation-results"&gt;Evaluation Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#5-chat-website--api-platform"&gt;Chat Website &amp;amp; API Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How to Run Locally&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#7-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#8-citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#9-contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. Introduction&lt;/h2&gt; 
&lt;p&gt;We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;2. Model Summary&lt;/h2&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Architecture: Innovative Load Balancing Strategy and Training Objective&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.&lt;/li&gt; 
 &lt;li&gt;We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Pre-Training: Towards Ultimate Training Efficiency&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.&lt;/li&gt; 
 &lt;li&gt;Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.&lt;br /&gt; This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.&lt;/li&gt; 
 &lt;li&gt;At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Post-Training: Knowledge Distillation from DeepSeek-R1&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;3. Model Downloads&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Total Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Activated Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3-Base&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How_to Run_Locally&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For developers looking to dive deeper, we recommend exploring &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/README_WEIGHTS.md"&gt;README_WEIGHTS.md&lt;/a&gt; for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.&lt;/p&gt; 
&lt;h2&gt;4. Evaluation Results&lt;/h2&gt; 
&lt;h3&gt;Base Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;Benchmark (Metric)&lt;/th&gt; 
    &lt;th&gt;# Shots&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V2&lt;/th&gt; 
    &lt;th&gt;Qwen2.5 72B&lt;/th&gt; 
    &lt;th&gt;LLaMA3.1 405B&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V3&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;Pile-test (BPB)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;0.606&lt;/td&gt; 
    &lt;td&gt;0.638&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;0.542&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;0.548&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;BBH (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.8&lt;/td&gt; 
    &lt;td&gt;79.8&lt;/td&gt; 
    &lt;td&gt;82.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;78.4&lt;/td&gt; 
    &lt;td&gt;85.0&lt;/td&gt; 
    &lt;td&gt;84.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;75.6&lt;/td&gt; 
    &lt;td&gt;83.2&lt;/td&gt; 
    &lt;td&gt;81.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;51.4&lt;/td&gt; 
    &lt;td&gt;58.3&lt;/td&gt; 
    &lt;td&gt;52.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (F1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Easy (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;97.6&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;98.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Challenge (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;92.2&lt;/td&gt; 
    &lt;td&gt;94.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;HellaSwag (Acc.)&lt;/td&gt; 
    &lt;td&gt;10-shot&lt;/td&gt; 
    &lt;td&gt;87.1&lt;/td&gt; 
    &lt;td&gt;84.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;PIQA (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;83.9&lt;/td&gt; 
    &lt;td&gt;82.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;WinoGrande (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.3&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;84.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-Middle (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;73.1&lt;/td&gt; 
    &lt;td&gt;68.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;74.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;67.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-High (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;52.6&lt;/td&gt; 
    &lt;td&gt;50.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;56.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;TriviaQA (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;71.9&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;NaturalQuestions (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;38.6&lt;/td&gt; 
    &lt;td&gt;33.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;41.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;40.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;AGIEval (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;60.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval (Pass@1)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;43.3&lt;/td&gt; 
    &lt;td&gt;53.0&lt;/td&gt; 
    &lt;td&gt;54.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MBPP (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;65.0&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;68.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;75.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench-Base (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;11.6&lt;/td&gt; 
    &lt;td&gt;12.9&lt;/td&gt; 
    &lt;td&gt;15.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;19.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-I (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-O (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;49.8&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;69.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;GSM8K (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;81.6&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH (EM)&lt;/td&gt; 
    &lt;td&gt;4-shot&lt;/td&gt; 
    &lt;td&gt;43.4&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MGSM (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;63.6&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;69.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMath (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.7&lt;/td&gt; 
    &lt;td&gt;84.5&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;82.0&lt;/td&gt; 
    &lt;td&gt;82.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;83.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;81.4&lt;/td&gt; 
    &lt;td&gt;89.2&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;84.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.7&lt;/td&gt; 
    &lt;td&gt;88.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMRC (EM)&lt;/td&gt; 
    &lt;td&gt;1-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;77.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;76.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C3 (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.7&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CCPM (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;93.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.5&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;92.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Multilingual&lt;/td&gt; 
    &lt;td&gt;MMMLU-non-English (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;64.0&lt;/td&gt; 
    &lt;td&gt;74.8&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks. For more evaluation details, please check our paper.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Context Window&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/niah.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Evaluation results on the &lt;code&gt;Needle In A Haystack&lt;/code&gt; (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to &lt;strong&gt;128K&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;Chat Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks (Models larger than 67B)&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Benchmark (Metric)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2-0506&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2.5-0905&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Qwen2.5 72B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Llama3.1 405B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Claude-3.5-Sonnet-1022&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;GPT-4o 0513&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V3&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;MMLU (EM)&lt;/td&gt; 
    &lt;td&gt;78.2&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;85.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;87.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (EM)&lt;/td&gt; 
    &lt;td&gt;77.9&lt;/td&gt; 
    &lt;td&gt;80.3&lt;/td&gt; 
    &lt;td&gt;85.6&lt;/td&gt; 
    &lt;td&gt;86.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (EM)&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;66.2&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;78.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;75.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (3-shot F1)&lt;/td&gt; 
    &lt;td&gt;83.0&lt;/td&gt; 
    &lt;td&gt;87.8&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;88.7&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;IF-Eval (Prompt Strict)&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;84.1&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.3&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;GPQA-Diamond (Pass@1)&lt;/td&gt; 
    &lt;td&gt;35.3&lt;/td&gt; 
    &lt;td&gt;41.3&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;49.9&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;9.0&lt;/td&gt; 
    &lt;td&gt;10.2&lt;/td&gt; 
    &lt;td&gt;9.1&lt;/td&gt; 
    &lt;td&gt;17.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;38.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;24.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;FRAMES (Acc.)&lt;/td&gt; 
    &lt;td&gt;66.9&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;70.0&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;80.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LongBench v2 (Acc.)&lt;/td&gt; 
    &lt;td&gt;31.6&lt;/td&gt; 
    &lt;td&gt;35.4&lt;/td&gt; 
    &lt;td&gt;39.4&lt;/td&gt; 
    &lt;td&gt;36.1&lt;/td&gt; 
    &lt;td&gt;41.0&lt;/td&gt; 
    &lt;td&gt;48.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;48.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval-Mul (Pass@1)&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;77.2&lt;/td&gt; 
    &lt;td&gt;81.7&lt;/td&gt; 
    &lt;td&gt;80.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1-COT)&lt;/td&gt; 
    &lt;td&gt;18.8&lt;/td&gt; 
    &lt;td&gt;29.2&lt;/td&gt; 
    &lt;td&gt;31.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;36.3&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;40.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1)&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;28.7&lt;/td&gt; 
    &lt;td&gt;30.1&lt;/td&gt; 
    &lt;td&gt;32.8&lt;/td&gt; 
    &lt;td&gt;34.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;37.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Codeforces (Percentile)&lt;/td&gt; 
    &lt;td&gt;17.5&lt;/td&gt; 
    &lt;td&gt;35.6&lt;/td&gt; 
    &lt;td&gt;24.8&lt;/td&gt; 
    &lt;td&gt;25.3&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;23.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SWE Verified (Resolved)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;22.6&lt;/td&gt; 
    &lt;td&gt;23.8&lt;/td&gt; 
    &lt;td&gt;24.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;50.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;38.8&lt;/td&gt; 
    &lt;td&gt;42.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Edit (Acc.)&lt;/td&gt; 
    &lt;td&gt;60.3&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;63.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;84.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.9&lt;/td&gt; 
    &lt;td&gt;79.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Polyglot (Acc.)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;18.2&lt;/td&gt; 
    &lt;td&gt;7.6&lt;/td&gt; 
    &lt;td&gt;5.8&lt;/td&gt; 
    &lt;td&gt;45.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;49.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;AIME 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;4.6&lt;/td&gt; 
    &lt;td&gt;16.7&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;9.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;39.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH-500 (EM)&lt;/td&gt; 
    &lt;td&gt;56.3&lt;/td&gt; 
    &lt;td&gt;74.7&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;78.3&lt;/td&gt; 
    &lt;td&gt;74.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CNMO 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;2.8&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;15.9&lt;/td&gt; 
    &lt;td&gt;6.8&lt;/td&gt; 
    &lt;td&gt;13.1&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;43.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;89.9&lt;/td&gt; 
    &lt;td&gt;90.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
    &lt;td&gt;85.4&lt;/td&gt; 
    &lt;td&gt;87.9&lt;/td&gt; 
    &lt;td&gt;90.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (EM)&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;48.5&lt;/td&gt; 
    &lt;td&gt;54.1&lt;/td&gt; 
    &lt;td&gt;48.4&lt;/td&gt; 
    &lt;td&gt;50.4&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
    &lt;td&gt;59.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Open Ended Generation Evaluation&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Arena-Hard&lt;/th&gt; 
    &lt;th&gt;AlpacaEval 2.0&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V2.5-0905&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;50.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Qwen2.5-72B-Instruct&lt;/td&gt; 
    &lt;td&gt;81.2&lt;/td&gt; 
    &lt;td&gt;49.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LLaMA-3.1 405B&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;40.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GPT-4o-0513&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Claude-Sonnet-3.5-1022&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;52.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;5. Chat Website &amp;amp; API Platform&lt;/h2&gt; 
&lt;p&gt;You can chat with DeepSeek-V3 on DeepSeek's official website: &lt;a href="https://chat.deepseek.com/sign_in"&gt;chat.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We also provide OpenAI-Compatible API at DeepSeek Platform: &lt;a href="https://platform.deepseek.com/"&gt;platform.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. How to Run Locally&lt;/h2&gt; 
&lt;p&gt;DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeek-Infer Demo&lt;/strong&gt;: We provide a simple and lightweight demo for FP8 and BF16 inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SGLang&lt;/strong&gt;: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;coming soon&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LMDeploy&lt;/strong&gt;: Enables efficient FP8 and BF16 inference for local and cloud deployment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt;: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LightLLM&lt;/strong&gt;: Supports efficient single-node or multi-node deployment for FP8 and BF16.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AMD GPU&lt;/strong&gt;: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Huawei Ascend NPU&lt;/strong&gt;: Supports running DeepSeek-V3 on Huawei Ascend devices in both INT8 and BF16.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.&lt;/p&gt; 
&lt;p&gt;Here is an example of converting FP8 weights to BF16:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Hugging Face's Transformers has not been directly supported yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;6.1 Inference with DeepSeek-Infer Demo (example only)&lt;/h3&gt; 
&lt;h4&gt;System Requirements&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Linux with Python 3.10 only. Mac and Windows are not supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-pip-requirements"&gt;torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Weights &amp;amp; Demo Code Preparation&lt;/h4&gt; 
&lt;p&gt;First, clone our DeepSeek-V3 GitHub repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/deepseek-ai/DeepSeek-V3.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Navigate to the &lt;code&gt;inference&lt;/code&gt; folder and install dependencies listed in &lt;code&gt;requirements.txt&lt;/code&gt;. Easiest way is to use a package manager like &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;uv&lt;/code&gt; to create a new virtual environment and install the dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd DeepSeek-V3/inference
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download the model weights from Hugging Face, and put them into &lt;code&gt;/path/to/DeepSeek-V3&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h4&gt;Model Weights Conversion&lt;/h4&gt; 
&lt;p&gt;Convert Hugging Face model weights to a specific format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run&lt;/h4&gt; 
&lt;p&gt;Then you can chat with DeepSeek-V3:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or batch inference on a given file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.2 Inference with SGLang (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; currently supports &lt;a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations"&gt;MLA optimizations&lt;/a&gt;, &lt;a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models"&gt;DP Attention&lt;/a&gt;, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.&lt;/p&gt; 
&lt;p&gt;Notably, &lt;a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1"&gt;SGLang v0.4.1&lt;/a&gt; fully supports running DeepSeek-V3 on both &lt;strong&gt;NVIDIA and AMD GPUs&lt;/strong&gt;, making it a highly versatile and robust solution.&lt;/p&gt; 
&lt;p&gt;SGLang also supports &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208"&gt;multi-node tensor parallelism&lt;/a&gt;, enabling you to run this model on multiple network-connected machines.&lt;/p&gt; 
&lt;p&gt;Multi-Token Prediction (MTP) is in development, and progress can be tracked in the &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;optimization plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Here are the launch instructions from the SGLang team: &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3"&gt;https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.3 Inference with LMDeploy (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/InternLM/lmdeploy"&gt;LMDeploy&lt;/a&gt;, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.&lt;/p&gt; 
&lt;p&gt;For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: &lt;a href="https://github.com/InternLM/lmdeploy/issues/2960"&gt;https://github.com/InternLM/lmdeploy/issues/2960&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.4 Inference with TRT-LLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt; now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3"&gt;https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.5 Inference with vLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers &lt;em&gt;pipeline parallelism&lt;/em&gt; allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the &lt;a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html"&gt;vLLM instructions&lt;/a&gt;. Please feel free to follow &lt;a href="https://github.com/vllm-project/vllm/issues/11539"&gt;the enhancement plan&lt;/a&gt; as well.&lt;/p&gt; 
&lt;h3&gt;6.6 Inference with LightLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ModelTC/lightllm/tree/main"&gt;LightLLM&lt;/a&gt; v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to &lt;a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html"&gt;LightLLM instructions&lt;/a&gt;. Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.&lt;/p&gt; 
&lt;h3&gt;6.7 Recommended Inference Functionality with AMD GPUs&lt;/h3&gt; 
&lt;p&gt;In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#63-inference-with-lmdeploy-recommended"&gt;SGLang instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.8 Recommended Inference Functionality with Huawei Ascend NPUs&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://www.hiascend.com/en/software/mindie"&gt;MindIE&lt;/a&gt; framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the &lt;a href="https://modelers.cn/models/MindIE/deepseekv3"&gt;instructions here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;7. License&lt;/h2&gt; 
&lt;p&gt;This code repository is licensed under &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-CODE"&gt;the MIT License&lt;/a&gt;. The use of DeepSeek-V3 Base/Chat models is subject to &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-MODEL"&gt;the Model License&lt;/a&gt;. DeepSeek-V3 series (including Base and Chat) supports commercial use.&lt;/p&gt; 
&lt;h2&gt;8. Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;9. Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/service@deepseek.com"&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>FunAudioLLM/CosyVoice</title>
      <link>https://github.com/FunAudioLLM/CosyVoice</link>
      <description>&lt;p&gt;Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/Akshay090/svg-banners"&gt;&lt;img src="https://svg-banners.vercel.app/api?type=origin&amp;amp;text1=CosyVoice%F0%9F%A4%A0&amp;amp;text2=Text-to-Speech%20%F0%9F%92%96%20Large%20Language%20Model&amp;amp;width=800&amp;amp;height=210" alt="SVG Banners" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üëâüèª CosyVoice üëàüèª&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 3.0&lt;/strong&gt;: &lt;a href="https://funaudiollm.github.io/cosyvoice3/"&gt;Demos&lt;/a&gt;; &lt;a href="https://arxiv.org/abs/2505.17589"&gt;Paper&lt;/a&gt;; &lt;a href="https://github.com/FunAudioLLM/CV3-Eval"&gt;CV3-Eval&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 2.0&lt;/strong&gt;: &lt;a href="https://funaudiollm.github.io/cosyvoice2/"&gt;Demos&lt;/a&gt;; &lt;a href="https://arxiv.org/abs/2412.10117"&gt;Paper&lt;/a&gt;; &lt;a href="https://www.modelscope.cn/studios/iic/CosyVoice2-0.5B"&gt;Modelscope&lt;/a&gt;; &lt;a href="https://huggingface.co/spaces/FunAudioLLM/CosyVoice2-0.5B"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 1.0&lt;/strong&gt;: &lt;a href="https://fun-audio-llm.github.io"&gt;Demos&lt;/a&gt;; &lt;a href="https://funaudiollm.github.io/pdf/CosyVoice_v1.pdf"&gt;Paper&lt;/a&gt;; &lt;a href="https://www.modelscope.cn/studios/iic/CosyVoice-300M"&gt;Modelscope&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Highlightüî•&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 2.0&lt;/strong&gt; has been released! Compared to version 1.0, the new version offers more accurate, more stable, faster, and better speech generation capabilities.&lt;/p&gt; 
&lt;h3&gt;Multilingual&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Language&lt;/strong&gt;: Chinese, English, Japanese, Korean, Chinese dialects (Cantonese, Sichuanese, Shanghainese, Tianjinese, Wuhanese, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Crosslingual &amp;amp; Mixlingual&lt;/strong&gt;ÔºöSupport zero-shot voice cloning for cross-lingual and code-switching scenarios.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ultra-Low Latency&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Bidirectional Streaming Support&lt;/strong&gt;: CosyVoice 2.0 integrates offline and streaming modeling technologies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rapid First Packet Synthesis&lt;/strong&gt;: Achieves latency as low as 150ms while maintaining high-quality audio output.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;High Accuracy&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Improved Pronunciation&lt;/strong&gt;: Reduces pronunciation errors by 30% to 50% compared to CosyVoice 1.0.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Achievements&lt;/strong&gt;: Attains the lowest character error rate on the hard test set of the Seed-TTS evaluation set.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Strong Stability&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Consistency in Timbre&lt;/strong&gt;: Ensures reliable voice consistency for zero-shot and cross-language speech synthesis.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-language Synthesis&lt;/strong&gt;: Marked improvements compared to version 1.0.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Natural Experience&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Prosody and Sound Quality&lt;/strong&gt;: Improved alignment of synthesized audio, raising MOS evaluation scores from 5.4 to 5.53.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Emotional and Dialectal Flexibility&lt;/strong&gt;: Now supports more granular emotional controls and accent adjustments.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/08&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Thanks to the contribution from NVIDIA Yuekai Zhang, add triton trtllm runtime support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/07&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; release cosyvoice 3.0 eval set&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/05&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; add cosyvoice 2.0 vllm support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/12&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 25hz cosyvoice 2.0 released&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/09&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 25hz cosyvoice base model&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 25hz cosyvoice voice conversion model&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/08&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Repetition Aware Sampling(RAS) inference for llm stability&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Streaming inference mode support, including kv cache and sdpa for rtf optimization&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/07&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Flow matching training support&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; WeTextProcessing support when ttsfrd is not available&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Fastapi server and client&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;h3&gt;Clone and install&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repo&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git
# If you failed to clone the submodule due to network failures, please run the following command until success
cd CosyVoice
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Conda: please see &lt;a href="https://docs.conda.io/en/latest/miniconda.html"&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create Conda env:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;conda create -n cosyvoice -y python=3.10
conda activate cosyvoice
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com

# If you encounter sox compatibility issues
# ubuntu
sudo apt-get install sox libsox-dev
# centos
sudo yum install sox sox-devel
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Model download&lt;/h3&gt; 
&lt;p&gt;We strongly recommend that you download our pretrained &lt;code&gt;CosyVoice2-0.5B&lt;/code&gt; &lt;code&gt;CosyVoice-300M&lt;/code&gt; &lt;code&gt;CosyVoice-300M-SFT&lt;/code&gt; &lt;code&gt;CosyVoice-300M-Instruct&lt;/code&gt; model and &lt;code&gt;CosyVoice-ttsfrd&lt;/code&gt; resource.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# SDKÊ®°Âûã‰∏ãËΩΩ
from modelscope import snapshot_download
snapshot_download('iic/CosyVoice2-0.5B', local_dir='pretrained_models/CosyVoice2-0.5B')
snapshot_download('iic/CosyVoice-300M', local_dir='pretrained_models/CosyVoice-300M')
snapshot_download('iic/CosyVoice-300M-SFT', local_dir='pretrained_models/CosyVoice-300M-SFT')
snapshot_download('iic/CosyVoice-300M-Instruct', local_dir='pretrained_models/CosyVoice-300M-Instruct')
snapshot_download('iic/CosyVoice-ttsfrd', local_dir='pretrained_models/CosyVoice-ttsfrd')
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# gitÊ®°Âûã‰∏ãËΩΩÔºåËØ∑Á°Æ‰øùÂ∑≤ÂÆâË£Ögit lfs
mkdir -p pretrained_models
git clone https://www.modelscope.cn/iic/CosyVoice2-0.5B.git pretrained_models/CosyVoice2-0.5B
git clone https://www.modelscope.cn/iic/CosyVoice-300M.git pretrained_models/CosyVoice-300M
git clone https://www.modelscope.cn/iic/CosyVoice-300M-SFT.git pretrained_models/CosyVoice-300M-SFT
git clone https://www.modelscope.cn/iic/CosyVoice-300M-Instruct.git pretrained_models/CosyVoice-300M-Instruct
git clone https://www.modelscope.cn/iic/CosyVoice-ttsfrd.git pretrained_models/CosyVoice-ttsfrd
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, you can unzip &lt;code&gt;ttsfrd&lt;/code&gt; resource and install &lt;code&gt;ttsfrd&lt;/code&gt; package for better text normalization performance.&lt;/p&gt; 
&lt;p&gt;Notice that this step is not necessary. If you do not install &lt;code&gt;ttsfrd&lt;/code&gt; package, we will use wetext by default.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd pretrained_models/CosyVoice-ttsfrd/
unzip resource.zip -d .
pip install ttsfrd_dependency-0.1-py3-none-any.whl
pip install ttsfrd-0.4.2-cp310-cp310-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;We strongly recommend using &lt;code&gt;CosyVoice2-0.5B&lt;/code&gt; for better performance. Follow the code below for detailed usage of each model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import sys
sys.path.append('third_party/Matcha-TTS')
from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2
from cosyvoice.utils.file_utils import load_wav
import torchaudio
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;CosyVoice2 Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;cosyvoice = CosyVoice2('pretrained_models/CosyVoice2-0.5B', load_jit=False, load_trt=False, load_vllm=False, fp16=False)

# NOTE if you want to reproduce the results on https://funaudiollm.github.io/cosyvoice2, please add text_frontend=False during inference
# zero_shot usage
prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)
for i, j in enumerate(cosyvoice.inference_zero_shot('Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©ÔºåÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶èËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºåÁ¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ', 'Â∏åÊúõ‰Ω†‰ª•ÂêéËÉΩÂ§üÂÅöÁöÑÊØîÊàëËøòÂ•ΩÂë¶„ÄÇ', prompt_speech_16k, stream=False)):
    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

# save zero_shot spk for future usage
assert cosyvoice.add_zero_shot_spk('Â∏åÊúõ‰Ω†‰ª•ÂêéËÉΩÂ§üÂÅöÁöÑÊØîÊàëËøòÂ•ΩÂë¶„ÄÇ', prompt_speech_16k, 'my_zero_shot_spk') is True
for i, j in enumerate(cosyvoice.inference_zero_shot('Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©ÔºåÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶èËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºåÁ¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ', '', '', zero_shot_spk_id='my_zero_shot_spk', stream=False)):
    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
cosyvoice.save_spkinfo()

# fine grained control, for supported control, check cosyvoice/tokenizer/tokenizer.py#L248
for i, j in enumerate(cosyvoice.inference_cross_lingual('Âú®‰ªñËÆ≤Ëø∞ÈÇ£‰∏™ËçíËØûÊïÖ‰∫ãÁöÑËøáÁ®ã‰∏≠Ôºå‰ªñÁ™ÅÁÑ∂[laughter]ÂÅú‰∏ãÊù•ÔºåÂõ†‰∏∫‰ªñËá™Â∑±‰πüË¢´ÈÄóÁ¨ë‰∫Ü[laughter]„ÄÇ', prompt_speech_16k, stream=False)):
    torchaudio.save('fine_grained_control_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

# instruct usage
for i, j in enumerate(cosyvoice.inference_instruct2('Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©ÔºåÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶èËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºåÁ¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ', 'Áî®ÂõõÂ∑ùËØùËØ¥ËøôÂè•ËØù', prompt_speech_16k, stream=False)):
    torchaudio.save('instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

# bistream usage, you can use generator as input, this is useful when using text llm model as input
# NOTE you should still have some basic sentence split logic because llm can not handle arbitrary sentence length
def text_generator():
    yield 'Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©Ôºå'
    yield 'ÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶è'
    yield 'ËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºå'
    yield 'Á¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ'
for i, j in enumerate(cosyvoice.inference_zero_shot(text_generator(), 'Â∏åÊúõ‰Ω†‰ª•ÂêéËÉΩÂ§üÂÅöÁöÑÊØîÊàëËøòÂ•ΩÂë¶„ÄÇ', prompt_speech_16k, stream=False)):
    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;CosyVoice2 vllm Usage&lt;/h4&gt; 
&lt;p&gt;If you want to use vllm for inference, please install &lt;code&gt;vllm==v0.9.0&lt;/code&gt;. Older vllm version do not support CosyVoice2 inference.&lt;/p&gt; 
&lt;p&gt;Notice that &lt;code&gt;vllm==v0.9.0&lt;/code&gt; has a lot of specific requirements, for example &lt;code&gt;torch==2.7.0&lt;/code&gt;. You can create a new env to in case your hardward do not support vllm and old env is corrupted.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda create -n cosyvoice_vllm --clone cosyvoice
conda activate cosyvoice_vllm
pip install vllm==v0.9.0 -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
python vllm_example.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;CosyVoice Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M-SFT', load_jit=False, load_trt=False, fp16=False)
# sft usage
print(cosyvoice.list_available_spks())
# change stream=True for chunk stream inference
for i, j in enumerate(cosyvoice.inference_sft('‰Ω†Â•ΩÔºåÊàëÊòØÈÄö‰πâÁîüÊàêÂºèËØ≠Èü≥Â§ßÊ®°ÂûãÔºåËØ∑ÈóÆÊúâ‰ªÄ‰πàÂèØ‰ª•Â∏ÆÊÇ®ÁöÑÂêóÔºü', '‰∏≠ÊñáÂ•≥', stream=False)):
    torchaudio.save('sft_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M')
# zero_shot usage, &amp;lt;|zh|&amp;gt;&amp;lt;|en|&amp;gt;&amp;lt;|jp|&amp;gt;&amp;lt;|yue|&amp;gt;&amp;lt;|ko|&amp;gt; for Chinese/English/Japanese/Cantonese/Korean
prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)
for i, j in enumerate(cosyvoice.inference_zero_shot('Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©ÔºåÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶èËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºåÁ¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ', 'Â∏åÊúõ‰Ω†‰ª•ÂêéËÉΩÂ§üÂÅöÁöÑÊØîÊàëËøòÂ•ΩÂë¶„ÄÇ', prompt_speech_16k, stream=False)):
    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
# cross_lingual usage
prompt_speech_16k = load_wav('./asset/cross_lingual_prompt.wav', 16000)
for i, j in enumerate(cosyvoice.inference_cross_lingual('&amp;lt;|en|&amp;gt;And then later on, fully acquiring that company. So keeping management in line, interest in line with the asset that\'s coming into the family is a reason why sometimes we don\'t buy the whole thing.', prompt_speech_16k, stream=False)):
    torchaudio.save('cross_lingual_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
# vc usage
prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)
source_speech_16k = load_wav('./asset/cross_lingual_prompt.wav', 16000)
for i, j in enumerate(cosyvoice.inference_vc(source_speech_16k, prompt_speech_16k, stream=False)):
    torchaudio.save('vc_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)

cosyvoice = CosyVoice('pretrained_models/CosyVoice-300M-Instruct')
# instruct usage, support &amp;lt;laughter&amp;gt;&amp;lt;/laughter&amp;gt;&amp;lt;strong&amp;gt;&amp;lt;/strong&amp;gt;[laughter][breath]
for i, j in enumerate(cosyvoice.inference_instruct('Âú®Èù¢ÂØπÊåëÊàòÊó∂Ôºå‰ªñÂ±ïÁé∞‰∫ÜÈùûÂá°ÁöÑ&amp;lt;strong&amp;gt;ÂãáÊ∞î&amp;lt;/strong&amp;gt;‰∏é&amp;lt;strong&amp;gt;Êô∫ÊÖß&amp;lt;/strong&amp;gt;„ÄÇ', '‰∏≠ÊñáÁî∑', 'Theo \'Crimson\', is a fiery, passionate rebel leader. Fights with fervor for justice, but struggles with impulsiveness.', stream=False)):
    torchaudio.save('instruct_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Start web demo&lt;/h4&gt; 
&lt;p&gt;You can use our web demo page to get familiar with CosyVoice quickly.&lt;/p&gt; 
&lt;p&gt;Please see the demo website for details.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# change iic/CosyVoice-300M-SFT for sft inference, or iic/CosyVoice-300M-Instruct for instruct inference
python3 webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Advanced Usage&lt;/h4&gt; 
&lt;p&gt;For advanced users, we have provided training and inference scripts in &lt;code&gt;examples/libritts/cosyvoice/run.sh&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Build for deployment&lt;/h4&gt; 
&lt;p&gt;Optionally, if you want service deployment, You can run the following steps.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd runtime/python
docker build -t cosyvoice:v1.0 .
# change iic/CosyVoice-300M to iic/CosyVoice-300M-Instruct if you want to use instruct inference
# for grpc usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c "cd /opt/CosyVoice/CosyVoice/runtime/python/grpc &amp;amp;&amp;amp; python3 server.py --port 50000 --max_conc 4 --model_dir iic/CosyVoice-300M &amp;amp;&amp;amp; sleep infinity"
cd grpc &amp;amp;&amp;amp; python3 client.py --port 50000 --mode &amp;lt;sft|zero_shot|cross_lingual|instruct&amp;gt;
# for fastapi usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c "cd /opt/CosyVoice/CosyVoice/runtime/python/fastapi &amp;amp;&amp;amp; python3 server.py --port 50000 --model_dir iic/CosyVoice-300M &amp;amp;&amp;amp; sleep infinity"
cd fastapi &amp;amp;&amp;amp; python3 client.py --port 50000 --mode &amp;lt;sft|zero_shot|cross_lingual|instruct&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Discussion &amp;amp; Communication&lt;/h2&gt; 
&lt;p&gt;You can directly discuss on &lt;a href="https://github.com/FunAudioLLM/CosyVoice/issues"&gt;Github Issues&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also scan the QR code to join our official Dingding chat group.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/FunAudioLLM/CosyVoice/main/asset/dingding.png" width="250px" /&gt; 
&lt;h2&gt;Acknowledge&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/modelscope/FunASR"&gt;FunASR&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/modelscope/FunCodec"&gt;FunCodec&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/shivammehta25/Matcha-TTS"&gt;Matcha-TTS&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/yangdongchao/AcademiCodec"&gt;AcademiCodec&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/wenet-e2e/wenet"&gt;WeNet&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Citations&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{du2024cosyvoice,
  title={Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens},
  author={Du, Zhihao and Chen, Qian and Zhang, Shiliang and Hu, Kai and Lu, Heng and Yang, Yexin and Hu, Hangrui and Zheng, Siqi and Gu, Yue and Ma, Ziyang and others},
  journal={arXiv preprint arXiv:2407.05407},
  year={2024}
}

@article{du2024cosyvoice,
  title={Cosyvoice 2: Scalable streaming speech synthesis with large language models},
  author={Du, Zhihao and Wang, Yuxuan and Chen, Qian and Shi, Xian and Lv, Xiang and Zhao, Tianyu and Gao, Zhifu and Yang, Yexin and Gao, Changfeng and Wang, Hui and others},
  journal={arXiv preprint arXiv:2412.10117},
  year={2024}
}

@article{du2025cosyvoice,
  title={CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training},
  author={Du, Zhihao and Gao, Changfeng and Wang, Yuxuan and Yu, Fan and Zhao, Tianyu and Wang, Hao and Lv, Xiang and Wang, Hui and Shi, Xian and An, Keyu and others},
  journal={arXiv preprint arXiv:2505.17589},
  year={2025}
}

@inproceedings{lyu2025build,
  title={Build LLM-Based Zero-Shot Streaming TTS System with Cosyvoice},
  author={Lyu, Xiang and Wang, Yuxuan and Zhao, Tianyu and Wang, Hao and Liu, Huadai and Du, Zhihao},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--2},
  year={2025},
  organization={IEEE}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;The content provided above is for academic purposes only and is intended to demonstrate technical capabilities. Some examples are sourced from the internet. If any content infringes on your rights, please contact us to request its removal.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sgl-project/sglang</title>
      <link>https://github.com/sgl-project/sglang</link>
      <description>&lt;p&gt;SGLang is a fast serving framework for large language models and vision language models.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" id="sglangtop"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png" alt="logo" width="400" margin="10px" /&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/sglang"&gt;&lt;img src="https://img.shields.io/pypi/v/sglang" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://static.pepy.tech/badge/sglang?period=month" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/sgl-project/sglang/tree/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/sgl-project/sglang.svg?sanitize=true" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sgl-project/sglang/issues"&gt;&lt;img src="https://img.shields.io/github/issues-closed-raw/sgl-project/sglang" alt="issue resolution" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sgl-project/sglang/issues"&gt;&lt;img src="https://img.shields.io/github/issues-raw/sgl-project/sglang" alt="open issues" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/sgl-project/sglang"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;| &lt;a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/"&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://docs.sglang.ai/"&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://slack.sglang.ai/"&gt;&lt;strong&gt;Join Slack&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://meeting.sglang.ai/"&gt;&lt;strong&gt;Join Bi-Weekly Development Meeting&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/sgl-project/sglang/issues/7736"&gt;&lt;strong&gt;Roadmap&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#slides"&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/a&gt; |&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/08] üîî SGLang x AMD SF Meetup on 8/22: Hands-on GPU workshop, tech talks by AMD/xAI/SGLang, and networking. &lt;a href="https://lu.ma/gbfhjvuo"&gt;Register here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/08] üî• SGLang provides day-0 support for OpenAI gpt-oss model (&lt;a href="https://github.com/sgl-project/sglang/issues/8833"&gt;instructions&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;[2025/06] üî• SGLang, the high-performance serving infrastructure powering trillions of tokens daily, has been awarded the third batch of the Open Source AI Grant by a16z (&lt;a href="https://a16z.com/advancing-open-source-ai-through-benchmarks-and-bold-experimentation/"&gt;a16z blog&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;[2025/06] üî• Deploying DeepSeek on GB200 NVL72 with PD and Large Scale EP (Part I): 2.7x Higher Decoding Throughput (&lt;a href="https://lmsys.org/blog/2025-06-16-gb200-part-1/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;[2025/05] üî• Deploying DeepSeek with PD Disaggregation and Large-scale Expert Parallelism on 96 H100 GPUs (&lt;a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;[2025/03] Supercharge DeepSeek-R1 Inference on AMD Instinct MI300X (&lt;a href="https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1-Part2/README.html"&gt;AMD blog&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;[2025/03] SGLang Joins PyTorch Ecosystem: Efficient LLM Serving Engine (&lt;a href="https://pytorch.org/blog/sglang-joins-pytorch/"&gt;PyTorch blog&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;[2024/12] v0.4 Release: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs (&lt;a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025/02] Unlock DeepSeek-R1 Inference Performance on AMD Instinct‚Ñ¢ MI300X GPU (&lt;a href="https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1_Perf/README.html"&gt;AMD blog&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;[2025/01] SGLang provides day one support for DeepSeek V3/R1 models on NVIDIA and AMD GPUs with DeepSeek-specific optimizations. (&lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3"&gt;instructions&lt;/a&gt;, &lt;a href="https://www.amd.com/en/developer/resources/technical-articles/amd-instinct-gpus-power-deepseek-v3-revolutionizing-ai-development-with-sglang.html"&gt;AMD blog&lt;/a&gt;, &lt;a href="https://x.com/lmsysorg/status/1887262321636221412"&gt;10+ other companies&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;[2024/10] The First SGLang Online Meetup (&lt;a href="https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#the-first-sglang-online-meetup"&gt;slides&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;[2024/09] v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision (&lt;a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;[2024/07] v0.2 Release: Faster Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM) (&lt;a href="https://lmsys.org/blog/2024-07-25-sglang-llama3/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;[2024/02] SGLang enables &lt;strong&gt;3x faster JSON decoding&lt;/strong&gt; with compressed finite state machine (&lt;a href="https://lmsys.org/blog/2024-02-05-compressed-fsm/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;[2024/01] SGLang provides up to &lt;strong&gt;5x faster inference&lt;/strong&gt; with RadixAttention (&lt;a href="https://lmsys.org/blog/2024-01-17-sglang/"&gt;blog&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;[2024/01] SGLang powers the serving of the official &lt;strong&gt;LLaVA v1.6&lt;/strong&gt; release demo (&lt;a href="https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#demo"&gt;usage&lt;/a&gt;).&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language. The core features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast Backend Runtime&lt;/strong&gt;: Provides efficient serving with RadixAttention for prefix caching, zero-overhead CPU scheduler, prefill-decode disaggregation, speculative decoding, continuous batching, paged attention, tensor/pipeline/expert/data parallelism, structured outputs, chunked prefill, quantization (FP4/FP8/INT4/AWQ/GPTQ), and multi-lora batching.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Frontend Language&lt;/strong&gt;: Offers an intuitive interface for programming LLM applications, including chained generation calls, advanced prompting, control flow, multi-modal inputs, parallelism, and external interactions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive Model Support&lt;/strong&gt;: Supports a wide range of generative models (Llama, Qwen, DeepSeek, Kimi, GPT, Gemma, Mistral, etc.), embedding models (e5-mistral, gte, mcdse) and reward models (Skywork), with easy extensibility for integrating new models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Active Community&lt;/strong&gt;: SGLang is open-source and backed by an active community with wide industry adoption.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.sglang.ai/get_started/install.html"&gt;Install SGLang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.sglang.ai/basic_usage/send_request.html"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.sglang.ai/basic_usage/openai_api_completions.html"&gt;Backend Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.sglang.ai/references/frontend/frontend_tutorial.html"&gt;Frontend Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.sglang.ai/developer_guide/contribution_guide.html"&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark and Performance&lt;/h2&gt; 
&lt;p&gt;Learn more in the release blogs: &lt;a href="https://lmsys.org/blog/2024-07-25-sglang-llama3/"&gt;v0.2 blog&lt;/a&gt;, &lt;a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/"&gt;v0.3 blog&lt;/a&gt;, &lt;a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/"&gt;v0.4 blog&lt;/a&gt;, &lt;a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/"&gt;Large-scale expert parallelism&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang/issues/7736"&gt;Development Roadmap (2025 H2)&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Adoption and Sponsorship&lt;/h2&gt; 
&lt;p&gt;SGLang has been deployed at large scale, generating trillions of tokens in production each day. It is trusted and adopted by a wide range of leading enterprises and institutions, including xAI, AMD, NVIDIA, Intel, LinkedIn, Cursor, Oracle Cloud, Google Cloud, Microsoft Azure, AWS, Atlas Cloud, Voltage Park, Nebius, DataCrunch, Novita, InnoMatrix, MIT, UCLA, the University of Washington, Stanford, UC Berkeley, Tsinghua University, Jam &amp;amp; Tea Studios, Baseten, and other major technology organizations across North America and Asia. As an open-source LLM inference engine, SGLang has become the de facto industry standard, with deployments running on over 1,000,000 GPUs worldwide.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sgl-project/sgl-learning-materials/refs/heads/main/slides/adoption.png" alt="logo" width="800" margin="10px" /&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;For enterprises interested in adopting or deploying SGLang at scale, including technical consulting, sponsorship opportunities, or partnership inquiries, please contact us at &lt;a href="mailto:contact@sglang.ai"&gt;contact@sglang.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgment&lt;/h2&gt; 
&lt;p&gt;We learned the design and reused code from the following projects: &lt;a href="https://github.com/guidance-ai/guidance"&gt;Guidance&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;, &lt;a href="https://github.com/ModelTC/lightllm"&gt;LightLLM&lt;/a&gt;, &lt;a href="https://github.com/flashinfer-ai/flashinfer"&gt;FlashInfer&lt;/a&gt;, &lt;a href="https://github.com/outlines-dev/outlines"&gt;Outlines&lt;/a&gt;, and &lt;a href="https://github.com/eth-sri/lmql"&gt;LMQL&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;‚ùóÔ∏è&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)üìå&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;‚Ä¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Ä¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÁÆ±Âç≥Áî®&lt;/strong&gt;: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êòì‰∫éÈõÜÊàê&lt;/strong&gt;: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ÂèØÊéß&lt;/strong&gt;: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;ÂÆâË£ÖÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨„ÄÇ&lt;br /&gt; Âú®ËøêË°å SQLBot ÂâçÔºåËØ∑Á°Æ‰øùÂ∑≤ÂÆâË£ÖÂ•Ω &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt; Âíå &lt;a href="https://docs.docker.com/compose/install/"&gt;Docker Compose&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂàõÂª∫ÁõÆÂΩï
mkdir -p /opt/sqlbot
cd /opt/sqlbot

# ‰∏ãËΩΩ docker-compose.yaml
curl -o docker-compose.yaml https://raw.githubusercontent.com/dataease/SQLBot/main/docker-compose.yaml

# ÂêØÂä®ÊúçÂä°
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‰Ω†‰πüÂèØ‰ª•ÈÄöËøá &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel Â∫îÁî®ÂïÜÂ∫ó&lt;/a&gt; Âø´ÈÄüÈÉ®ÁΩ≤ SQLBotÔºõ&lt;/p&gt; 
&lt;h3&gt;ËÆøÈóÆÊñπÂºè&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&amp;lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;Áî®Êà∑Âêç: admin&lt;/li&gt; 
 &lt;li&gt;ÂØÜÁ†Å: SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÅîÁ≥ªÊàë‰ª¨&lt;/h3&gt; 
&lt;p&gt;Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ&lt;/p&gt; 
&lt;img width="396" height="396" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI Â±ïÁ§∫&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ªìÂ∫ìÈÅµÂæ™ &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>netease-youdao/QAnything</title>
      <link>https://github.com/netease-youdao/QAnything</link>
      <description>&lt;p&gt;Question and Answer based on Anything.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/netease-youdao/QAnything"&gt; 
  &lt;!-- Please provide path to your logo here --&gt; &lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/images/qanything_logo.png" alt="Logo" width="800" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;strong&gt;Q&lt;/strong&gt;uestion and &lt;strong&gt;A&lt;/strong&gt;nswer based on &lt;strong&gt;Anything&lt;/strong&gt;&lt;/h1&gt; 
 &lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/README_zh.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://qanything.ai"&gt;&lt;img src="https://img.shields.io/badge/try%20online-qanything.ai-purple" /&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://read.youdao.com#/home"&gt;&lt;img src="https://img.shields.io/badge/try%20online-read.youdao.com-purple" /&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-AGPL--3.0-yellow" /&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/netease-youdao/QAnything/pulls"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-red" /&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://twitter.com/YDopensource"&gt;&lt;img src="https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&amp;amp;style={style}" /&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/5uNpPsEJz8"&gt;&lt;img src="https://img.shields.io/discord/1197874288963895436?style=social&amp;amp;logo=discord" /&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Table of Contents&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#what-is-qanything"&gt;What is QAnything&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#key-features"&gt;Key features&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#architecture"&gt;Architecture&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#-latest-updates"&gt;Latest Updates&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#before-you-start"&gt;Before You Start&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#getting-started"&gt;Getting Started&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#latest-features-table"&gt;Latest Features Table&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#version-200-adds-detailed-optimizations"&gt;Version 2.0.0 adds detailed optimizations:&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#display-of-data-at-each-stage"&gt;Display of data at each stage:&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#problem-fixed"&gt;Problem fixed&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#comparison-of-new-and-old-parsing-effects"&gt;Comparison of New and Old Parsing Effects&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#installation"&gt;Installation&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#step1-pull-qanything-repository"&gt;step1: pull qanything repository&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#step2-enter-the-project-root-directory-and-execute-the-startup-command"&gt;step2: Enter the project root directory and execute the startup command.&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#step3-start-to-experience"&gt;step3: start to experience&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#api"&gt;API&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#debug"&gt;DEBUG&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#close-service"&gt;Close service&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#offline-use"&gt;Offline Use&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#contributing"&gt;Contributing&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#thanks-to-all-contributors-for-their-efforts"&gt;Thanks to all contributors for their efforts&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#special-thanks"&gt;Special thanks!&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#business-contact-information"&gt;Business contact informationÔºö&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#-roadmap--feedback"&gt;Roadmap &amp;amp; Feedback&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#community--support"&gt;Community &amp;amp; Support&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/#acknowledgments"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;üöÄ Important Updates&lt;/h1&gt; 
&lt;h1&gt;&lt;span style="color:red;"&gt;Important things should be said three times.&lt;/span&gt;&lt;/h1&gt; 
&lt;h1&gt;[2024-08-23: QAnything updated to version 2.0.]&lt;/h1&gt; 
&lt;h1&gt;[2024-08-23: QAnything updated to version 2.0.]&lt;/h1&gt; 
&lt;h1&gt;[2024-08-23: QAnything updated to version 2.0.]&lt;/h1&gt; 
&lt;h2&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;span style="color:green"&gt;This update brings improvements in various aspects such as usability, resource consumption, search results, question and answer results, parsing results, front-end effects, service architecture, and usage methods.&lt;/span&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;span style="color:green"&gt;At the same time, the old Docker version and Python version have been merged into a new unified version, using a single-line command with Docker Compose for one-click startup, ready to use out of the box.&lt;/span&gt;&lt;/li&gt; 
 &lt;/ul&gt; &lt;/h2&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We appreciate your interest in contributing to our project. Whether you're fixing a bug, improving an existing feature, or adding something completely new, your contributions are welcome!&lt;/p&gt; 
&lt;h3&gt;Thanks to all contributors for their efforts&lt;/h3&gt; 
&lt;a href="https://github.com/netease-youdao/QAnything/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=netease-youdao/QAnything" /&gt; &lt;/a&gt; 
&lt;h3&gt;Special thanks!&lt;/h3&gt; 
&lt;h2&gt;&lt;span style="color:red;"&gt;Please note: Our list of contributors is automatically updated, so your contributions may not appear immediately on this list.&lt;/span&gt;&lt;/h2&gt; 
&lt;h2&gt;&lt;span style="color:red;"&gt;Special thanks!Ôºö@ikun-moxiaofei&lt;/span&gt;&lt;/h2&gt; 
&lt;h2&gt;&lt;span style="color:red;"&gt;Special thanks!Ôºö@Ianarua&lt;/span&gt;&lt;/h2&gt; 
&lt;h2&gt;Business contact informationÔºö&lt;/h2&gt; 
&lt;h3&gt;010-82558901&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/images/business.jpeg" alt="" /&gt;&lt;/p&gt; 
&lt;h1&gt;What is QAnything?&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt;(&lt;strong&gt;Q&lt;/strong&gt;uestion and &lt;strong&gt;A&lt;/strong&gt;nswer based on &lt;strong&gt;Anything&lt;/strong&gt;) is a local knowledge base question-answering system designed to support a wide range of file formats and databases, allowing for offline installation and use.&lt;/p&gt; 
&lt;p&gt;With &lt;code&gt;QAnything&lt;/code&gt;, you can simply drop any locally stored file of any format and receive accurate, fast, and reliable answers.&lt;/p&gt; 
&lt;p&gt;Currently supported formats include: &lt;strong&gt;PDF(pdf)&lt;/strong&gt;,&lt;strong&gt;Word(docx)&lt;/strong&gt;,&lt;strong&gt;PPT(pptx)&lt;/strong&gt;,&lt;strong&gt;XLS(xlsx)&lt;/strong&gt;,&lt;strong&gt;Markdown(md)&lt;/strong&gt;,&lt;strong&gt;Email(eml)&lt;/strong&gt;,&lt;strong&gt;TXT(txt)&lt;/strong&gt;,&lt;strong&gt;Image(jpgÔºåjpegÔºåpng)&lt;/strong&gt;,&lt;strong&gt;CSV(csv)&lt;/strong&gt;,&lt;strong&gt;Web links(html)&lt;/strong&gt; and more formats coming soon‚Ä¶&lt;/p&gt; 
&lt;h2&gt;Key features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data security, supports installation and use by unplugging the network cable throughout the process.&lt;/li&gt; 
 &lt;li&gt;Supports multiple file types, high parsing success rate, supports cross-language question and answer, freely switches between Chinese and English question and answer, regardless of the language of the file.&lt;/li&gt; 
 &lt;li&gt;Supports massive data question and answer, two-stage vector sorting, solves the problem of degradation of large-scale data retrieval, the more data, the better the effect, no limit on the number of uploaded files, fast retrieval speed.&lt;/li&gt; 
 &lt;li&gt;Hardware friendly, defaults to running in a pure CPU environment, and supports multiple platforms such as Windows, Mac, and Linux, with no dependencies other than Docker.&lt;/li&gt; 
 &lt;li&gt;User-friendly, no need for cumbersome configuration, one-click installation and deployment, ready to use, each dependent component (PDF parsing, OCR, embed, rerank, etc.) is completely independent, supports free replacement.&lt;/li&gt; 
 &lt;li&gt;Supports a quick start mode similar to Kimi, fileless chat mode, retrieval mode only, custom Bot mode.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/images/qanything_arch.png" width="700" alt="qanything_system" align="center" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Why 2 stage retrieval?&lt;/h3&gt; 
&lt;p&gt;In scenarios with a large volume of knowledge base data, the advantages of a two-stage approach are very clear. If only a first-stage embedding retrieval is used, there will be a problem of retrieval degradation as the data volume increases, as indicated by the green line in the following graph. However, after the second-stage reranking, there can be a stable increase in accuracy, &lt;strong&gt;the more data, the better the performance&lt;/strong&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/images/two_stage_retrieval.jpg" width="500" alt="two stage retrievaal" align="center" /&gt; 
&lt;/div&gt; 
&lt;p&gt;QAnything uses the retrieval component &lt;a href="https://github.com/netease-youdao/BCEmbedding"&gt;BCEmbedding&lt;/a&gt;, which is distinguished for its bilingual and crosslingual proficiency. BCEmbedding excels in bridging Chinese and English linguistic gaps, which achieves&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A high performance on &lt;a href="https://github.com/netease-youdao/BCEmbedding/tree/master?tab=readme-ov-file#evaluate-semantic-representation-by-mteb" target="_Self"&gt;Semantic Representation Evaluations in MTEB&lt;/a&gt;&lt;/strong&gt;;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;A new benchmark in the realm of &lt;a href="https://github.com/netease-youdao/BCEmbedding/tree/master?tab=readme-ov-file#evaluate-rag-by-llamaindex" target="_Self"&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1st RetrievalÔºàembeddingÔºâ&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Retrieval&lt;/th&gt; 
   &lt;th align="center"&gt;STS&lt;/th&gt; 
   &lt;th align="center"&gt;PairClassification&lt;/th&gt; 
   &lt;th align="center"&gt;Classification&lt;/th&gt; 
   &lt;th align="center"&gt;Reranking&lt;/th&gt; 
   &lt;th align="center"&gt;Clustering&lt;/th&gt; 
   &lt;th align="center"&gt;Avg&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;bge-base-en-v1.5&lt;/td&gt; 
   &lt;td align="center"&gt;37.14&lt;/td&gt; 
   &lt;td align="center"&gt;55.06&lt;/td&gt; 
   &lt;td align="center"&gt;75.45&lt;/td&gt; 
   &lt;td align="center"&gt;59.73&lt;/td&gt; 
   &lt;td align="center"&gt;43.05&lt;/td&gt; 
   &lt;td align="center"&gt;37.74&lt;/td&gt; 
   &lt;td align="center"&gt;47.20&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;bge-base-zh-v1.5&lt;/td&gt; 
   &lt;td align="center"&gt;47.60&lt;/td&gt; 
   &lt;td align="center"&gt;63.72&lt;/td&gt; 
   &lt;td align="center"&gt;77.40&lt;/td&gt; 
   &lt;td align="center"&gt;63.38&lt;/td&gt; 
   &lt;td align="center"&gt;54.85&lt;/td&gt; 
   &lt;td align="center"&gt;32.56&lt;/td&gt; 
   &lt;td align="center"&gt;53.60&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;bge-large-en-v1.5&lt;/td&gt; 
   &lt;td align="center"&gt;37.15&lt;/td&gt; 
   &lt;td align="center"&gt;54.09&lt;/td&gt; 
   &lt;td align="center"&gt;75.00&lt;/td&gt; 
   &lt;td align="center"&gt;59.24&lt;/td&gt; 
   &lt;td align="center"&gt;42.68&lt;/td&gt; 
   &lt;td align="center"&gt;37.32&lt;/td&gt; 
   &lt;td align="center"&gt;46.82&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;bge-large-zh-v1.5&lt;/td&gt; 
   &lt;td align="center"&gt;47.54&lt;/td&gt; 
   &lt;td align="center"&gt;64.73&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;79.14&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;64.19&lt;/td&gt; 
   &lt;td align="center"&gt;55.88&lt;/td&gt; 
   &lt;td align="center"&gt;33.26&lt;/td&gt; 
   &lt;td align="center"&gt;54.21&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;jina-embeddings-v2-base-en&lt;/td&gt; 
   &lt;td align="center"&gt;31.58&lt;/td&gt; 
   &lt;td align="center"&gt;54.28&lt;/td&gt; 
   &lt;td align="center"&gt;74.84&lt;/td&gt; 
   &lt;td align="center"&gt;58.42&lt;/td&gt; 
   &lt;td align="center"&gt;41.16&lt;/td&gt; 
   &lt;td align="center"&gt;34.67&lt;/td&gt; 
   &lt;td align="center"&gt;44.29&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m3e-base&lt;/td&gt; 
   &lt;td align="center"&gt;46.29&lt;/td&gt; 
   &lt;td align="center"&gt;63.93&lt;/td&gt; 
   &lt;td align="center"&gt;71.84&lt;/td&gt; 
   &lt;td align="center"&gt;64.08&lt;/td&gt; 
   &lt;td align="center"&gt;52.38&lt;/td&gt; 
   &lt;td align="center"&gt;37.84&lt;/td&gt; 
   &lt;td align="center"&gt;53.54&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;m3e-large&lt;/td&gt; 
   &lt;td align="center"&gt;34.85&lt;/td&gt; 
   &lt;td align="center"&gt;59.74&lt;/td&gt; 
   &lt;td align="center"&gt;67.69&lt;/td&gt; 
   &lt;td align="center"&gt;60.07&lt;/td&gt; 
   &lt;td align="center"&gt;48.99&lt;/td&gt; 
   &lt;td align="center"&gt;31.62&lt;/td&gt; 
   &lt;td align="center"&gt;46.78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;em&gt;&lt;strong&gt;bce-embedding-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;57.60&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;65.73&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;74.96&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;69.00&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;57.29&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;38.95&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;em&gt;&lt;strong&gt;59.43&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;More evaluation details please check &lt;a href="https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/EvaluationSummary/embedding_eval_summary.md"&gt;Embedding Models Evaluation Summary&lt;/a&gt;„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2nd RetrievalÔºàrerankÔºâ&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Reranking&lt;/th&gt; 
   &lt;th align="center"&gt;Avg&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;bge-reranker-base&lt;/td&gt; 
   &lt;td align="center"&gt;57.78&lt;/td&gt; 
   &lt;td align="center"&gt;57.78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;bge-reranker-large&lt;/td&gt; 
   &lt;td align="center"&gt;59.69&lt;/td&gt; 
   &lt;td align="center"&gt;59.69&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;em&gt;&lt;strong&gt;bce-reranker-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;em&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;More evaluation details please check &lt;a href="https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/EvaluationSummary/reranker_eval_summary.md"&gt;Reranker Models Evaluation Summary&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;RAG Evaluations in LlamaIndexÔºàembedding and rerankÔºâ&lt;/h3&gt; 
&lt;img src="https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/assets/rag_eval_multiple_domains_summary.jpg" /&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In &lt;code&gt;WithoutReranker&lt;/code&gt; setting, our &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; outperforms all the other embedding models.&lt;/li&gt; 
 &lt;li&gt;With fixing the embedding model, our &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; achieves the best performance.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;The combination of &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; and &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; is SOTA&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want to use embedding and rerank separately, please refer to &lt;a href="https://github.com/netease-youdao/BCEmbedding"&gt;BCEmbedding&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LLM&lt;/h3&gt; 
&lt;p&gt;The open source version of QAnything is based on QwenLM and has been fine-tuned on a large number of professional question-answering datasets. It greatly enhances the ability of question-answering. If you need to use it for commercial purposes, please follow the license of QwenLM. For more details, please refer to: &lt;a href="https://github.com/QwenLM/Qwen"&gt;QwenLM&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üöÄ Latest Updates&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-08-23&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Support quick start, front-end configuration parameters, online preview and editing of chunk blocks, greatly optimize project architecture and startup mode, greatly optimize parsing and retrieval effects.&lt;/strong&gt; - See Moreüëâ &lt;a href="https://github.com/netease-youdao/QAnything/releases/tag/v2.0.0"&gt;v2.0.0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-05-20&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Support other large model services compatible with OpenAI API, and provide an optimized powerful PDF parser.&lt;/strong&gt; - See Moreüëâ &lt;a href="https://github.com/netease-youdao/QAnything/releases/tag/v1.4.1"&gt;v1.4.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-04-26&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Support web search, FAQ, custom bot, file traceability preview etc.&lt;/strong&gt; - See Moreüëâ &lt;a href="https://github.com/netease-youdao/QAnything/releases/tag/v1.4.0-python"&gt;v1.4.0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-04-03&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Support installation in a pure Python environment.Support hybrid search.&lt;/strong&gt; - See Moreüëâ &lt;a href="https://github.com/netease-youdao/QAnything/releases/tag/v1.3.0"&gt;v1.3.0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-01-29&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Support for custom large models, including OpenAI API and other open-source large models, with a minimum GPU requirement of GTX 1050Ti, greatly improving deployment, debugging, and user experience.&lt;/strong&gt; - See Moreüëâ &lt;a href="https://github.com/netease-youdao/QAnything/releases/tag/v1.2.0"&gt;v1.2.0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-01-23&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Enable rerank by default and fix various issues when starting on Windows.&lt;/strong&gt; - See Moreüëâ &lt;a href="https://github.com/netease-youdao/QAnything/releases/tag/v1.1.1"&gt;v1.1.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;&lt;strong&gt;2024-01-18&lt;/strong&gt;&lt;/em&gt;: &lt;strong&gt;Support one-click startup, support Windows deployment, improve PDF, XLSX, HTML parsing efficiency.&lt;/strong&gt; - See Moreüëâ &lt;a href="https://github.com/netease-youdao/QAnything/releases/tag/v1.1.0"&gt;v1.1.0&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Before You Start&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Star us on GitHub, and be instantly notified for new release!&lt;/strong&gt; &lt;img src="https://github.com/netease-youdao/QAnything/assets/29041332/fd5e5926-b9b2-4675-9f60-6cdcaca18e14" alt="star_us" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://qanything.ai"&gt;üèÑ Try QAnything Online&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://read.youdao.com"&gt;üìö Try read.youdao.com | ÊúâÈÅìÈÄüËØª&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/netease-youdao/BCEmbedding"&gt;üõ†Ô∏è Only use our BCEmbedding(embedding &amp;amp; rerank)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/FAQ_zh.md"&gt;üìñ FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://qanything.canny.io/feature-requests"&gt;üëÇÔ∏èLet me hear your voice&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;h2&gt;Latest Features Table&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;features&lt;/th&gt; 
   &lt;th&gt;python Ôºàv1.4.2Ôºâ&lt;/th&gt; 
   &lt;th&gt;docker Ôºàv1.2.2Ôºâ&lt;/th&gt; 
   &lt;th&gt;QAnything v2.0.0&lt;/th&gt; 
   &lt;th&gt;Explanation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Detailed installation document&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support API&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support production environment&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support offline use&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support multiple concurrency&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support multi-card inference&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;Version 2.0.0 no longer provides default local LLM. All access is through the openai interface, and users can deploy local LLM through tools such as ollama.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support Mac (M series chips)&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support Linux&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;The old version of Python defaults to using onnxruntime-gpu for cuda12 on Linux, and automatically switches to onnxruntime when glibc&amp;lt;2.28.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support windows&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Both old versions of Python and Docker require WSL environment. Version 2.0.0 can be started directly in a non-WSL environment.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support CPU only&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Version 2.0.0 Mac, Linux, Win unified no longer use GPU, completely migrated to CPU.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support hybrid search (BM25+embedding)&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support web search (need VPN)&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support FAQ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support BOT&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support Traceability&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support Log retrieval&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support audio file&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;Relying on whisper, slow speed and high resource consumption, temporarily removed.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support OpenCloudOS&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support interfaces compatible with Openaiapi (including ollama)&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Old versions of Python and Docker require manual modification of parameters such as api_key, base_url, model, etc. In version 2.0.0, these are all changed to be automatically saved in the front end settings.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PDF parsing performance improvement (including tables)&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Version 1.4.2 requires manual settings, version 2.0.0 does not require manual settings, and both the PDF parsing effect and performance have been improved.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;User-defined configuration (Experimental: Improve speed)&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;v1.4.2 needs to be set manually, v2.0.0 uses the best configuration by default.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Improvement in parsing performance of other file types&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Version 2.0.0 improves the parsing effect of URLs, Markdown, XLSX, DOCX, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support independent service invocation&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Version 2.0.0 independent dependent services, including embed, rerank, ocr, pdf parsing services, can be called independently (http)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support quick start mode&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Quick Start: No need to create a knowledge base, support for file upload and instant questioning, support for fileless Q&amp;amp;A.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support only retrieval mode&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Only return search results, do not call the large model for question answering.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support parsing result chunks content visualization, manual editing.&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Version 2.0.0 supports manually editing the contents of chunks, which take effect in real time.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PDF parsing supports images, supports answering with images.&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Version 2.0.0 adds detailed optimizations:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support front-end configuration API_BASE, API_KEY, text chunk size, output token quantity, context message quantity, etc.&lt;/li&gt; 
 &lt;li&gt;Optimize the instruction compliance of Bot role settings, each Bot can configure model parameters separately.&lt;/li&gt; 
 &lt;li&gt;Support creating multiple dialogue windows and saving multiple sets of historical Q&amp;amp;A records at the same time.&lt;/li&gt; 
 &lt;li&gt;Support saving question and answer records as images&lt;/li&gt; 
 &lt;li&gt;Optimize the logic of uploading files, parse files and question-and-answer requests independently, uploading files will no longer affect question-and-answer.&lt;/li&gt; 
 &lt;li&gt;Optimize image size, the compressed size of the old version image is 18.94GB -&amp;gt; the compressed size of the new version image is 4.88GB, reduced to 1/4 of the original size, providing a complete Dockerfile.&lt;/li&gt; 
 &lt;li&gt;Search optimization, chunks add fragment fusion and sorting, aggregate single document or double document.&lt;/li&gt; 
 &lt;li&gt;Both the retrieval stage and the question-answering stage embed metadata information to improve the retrieval and question-answering effectiveness.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Display of data at each stage:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Display the upload progress of all files in the knowledge base.&lt;/li&gt; 
 &lt;li&gt;Display the progress of uploading a single file in the knowledge base, and the time consumed in each stage of the upload.&lt;/li&gt; 
 &lt;li&gt;Question and answer information statistics, including time consumption at each stage of question and answer, token consumption, model information, etc.&lt;/li&gt; 
 &lt;li&gt;User information statistics, including total number of uploaded files, total time consumed, question and answer history records, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Problem fixed&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The xlsx file format supports parsing multiple sheets.&lt;/li&gt; 
 &lt;li&gt;Optimize the problem of missing recognition of PDF tables.&lt;/li&gt; 
 &lt;li&gt;Fix some parsing errors in DOCX files.&lt;/li&gt; 
 &lt;li&gt;Optimize FAQ matching logic.&lt;/li&gt; 
 &lt;li&gt;Support for non-UTF-8 encoded txt files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Comparison of New and Old Parsing Effects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;First, with regard to the parsing of large tables in documents, especially tables that span multiple pages, version 2.0 has made significant improvements. The new version's parsing logic can analyze the structure of the table, including the layout of rows and columns, and can automatically identify the table headers, placing them at the top of each table segment that is split. This improvement prevents interruptions in meaning caused by logical segmentation when parsing long tables.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Original image&lt;/th&gt; 
   &lt;th align="center"&gt;Old version parsing effect&lt;/th&gt; 
   &lt;th align="center"&gt;New version parsing effect&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170060.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170067.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170074.png" alt="image.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;In addition, version 2.0 has also been optimized for handling text columnation and cross-page layout. It can recognize double-column or multi-column layouts of text and correctly divide text blocks in accordance with human reading habits. At the same time, this version can also save images in documents to ensure the integrity of content is not lost during file parsing. As shown in the figure below, the correct arrangement should be to group the text arranged in sequence as 1, 2, 3 into a large paragraph and then segment it, rather than segmenting 1, 2, 3 separately. 
  &lt;ul&gt; 
   &lt;li&gt;In version 1.4 parsing results, the cross-page text "higher" was chunked into the next text block, which is detrimental to large model semantic understanding. In version 2.0 parsing results, it is correctly divided, and images interspersed in text paragraphs are also parsed into corresponding chunk statements. Non-main text such as "Figure 1 Identification and Authorization and Their Support Relationship 37" and "Cover Story Cover Feature" were successfully filtered out.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Original image&lt;/th&gt; 
   &lt;th align="center"&gt;Old version parsing effect&lt;/th&gt; 
   &lt;th align="center"&gt;New version parsing effect&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170088.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170101.png" alt="image.png" /&gt;&lt;br /&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170115.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170129.png" alt="image.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;Version 2.0 has also made corresponding optimizations for parsing tables interspersed between text columns or text blocks. The parsing of the original version could not recognize tables and would only parse them in the format of text paragraphs. This not only destroys the logical structure of the tables but also adds a section of messy and useless text for large models, which would affect the accuracy of large model responses. Version 2.0 can recognize and parse these tables embedded in the text, thereby improving the quality of parsing and the accuracy of responses from large models. 
  &lt;ul&gt; 
   &lt;li&gt;In version 1.4 parsing results, tables interspersed in text blocks are parsed as normal text blocks. In version 2.0, this type of table can be parsed "elegantly", which not only improves the quality of parsing but also increases the accuracy of large model answers.&lt;/li&gt; 
   &lt;li&gt;In addition, in version 2.0, when processing text under specific subheadings, priority is given to ensuring that these texts are segmented into the same chunk block to maintain logical coherence. When the text is too long and needs to be split, the parsing logic of version 2.0 will repeat the same subheading before each split text block to indicate ownership. For example, in the example, the same subheading "European Conference: Legal Status of Robots" was added to all three text blocks (due to the length of the text, this title was not displayed in the original file screenshot). This processing method effectively avoids the problem of incoherent semantic logic in split text blocks caused by excessively long text.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Original image&lt;/th&gt; 
   &lt;th align="center"&gt;Old version parsing effect&lt;/th&gt; 
   &lt;th align="center"&gt;New version parsing effect&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170148.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170166.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247171267.png" alt="image.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;For Excel (.xlsx) documents with complex formatting, version 2.0 has undergone a series of optimization measures to accurately identify and process row and column data, including optimized handling of merged cells and text spanning across rows or columns. Specific examples can be seen below. 
  &lt;ul&gt; 
   &lt;li&gt;In version 1.4, there may be some limitations when parsing Excel documents, especially for documents with special structures or formats. The parsing results may not be satisfactory, mainly recognizing only the plain text part. This may lead to information loss or format disorder when dealing with complex data and formats. In contrast, version 2.0 has significantly improved parsing capabilities, able to better handle various complex formats of Excel documents. Although it may not be perfect yet, it can already solve the vast majority of complex situations.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Original image&lt;/th&gt; 
   &lt;th align="center"&gt;Old version parsing effect&lt;/th&gt; 
   &lt;th align="center"&gt;New version parsing effect&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170188.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170208.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170228.png" alt="image.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;Similarly, for simple formatted xlsx documents, version 2.0 of the parser has been optimized.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Original image&lt;/th&gt; 
   &lt;th align="center"&gt;Old version parsing effect&lt;/th&gt; 
   &lt;th align="center"&gt;New version parsing effect&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170272.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170298.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170323.png" alt="image.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the latest version, we have also made significant improvements to the URL parsing function. Taking the following page as an example, the old version may miss a large amount of page information during the parsing process and cannot effectively handle more complex page elements such as tables and lists. However, the new version has been optimized for these issues and can parse these contents more accurately.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Original image&lt;/th&gt; 
   &lt;th align="center"&gt;Old version parsing effect&lt;/th&gt; 
   &lt;th align="center"&gt;New version parsing effect&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170443.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170478.png" alt="image.png" /&gt;&lt;br /&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170512.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170546.png" alt="image.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;In addition, for the vast majority of files, version 2.0 has also made corresponding optimizations, including but not limited to the following points. 
  &lt;ul&gt; 
   &lt;li&gt;Improved the cutting logic of chunk blocks, avoiding semantic blocks being too short or logic interruption due to blank lines or paragraphs in the document, ensuring the coherence and integrity of text blocks.&lt;/li&gt; 
   &lt;li&gt;The new version can more accurately identify the subheadings in the document, and locate and organize the corresponding text blocks based on these subheadings, which helps optimize the parsing effect, making the parsing structure clearer and the information hierarchy more distinct.&lt;/li&gt; 
   &lt;li&gt;The comparison of the analysis results is as follows: In version 1.4, the parsing logic divides the document into 10 chunks, while in version 2.0, after parsing, there are only 4 chunks. The more reasonable and fewer chunk blocks greatly improve the coherence and integrity of the content, helping to reduce semantic breaks or logical confusion caused by improper segmentation, thereby improving the overall parsing and model answering effects.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Original image&lt;/th&gt; 
   &lt;th align="center"&gt;Old version parsing effect&lt;/th&gt; 
   &lt;th align="center"&gt;New version parsing effect&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170352.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170380.png" alt="image.png" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/assets/17244247170406.png" alt="image.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;In summary, version 2.0 parsing has optimized many aspects compared to version 1.4 parsing, including but not limited to&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;By using more reasonable chunk lengths, the semantic and logical losses caused by paragraphs being too small or incomplete are reduced.&lt;/li&gt; 
 &lt;li&gt;Improved recognition ability for columned text, able to intelligently determine reading order, even correctly handling paragraphs that span across pages.&lt;/li&gt; 
 &lt;li&gt;The new version can recognize and save images and tables within text paragraphs, ensuring that no important text information is missed.&lt;/li&gt; 
 &lt;li&gt;Optimized table parsing, including parsing and storage of long tables exceeding chunk limits and complex structured xlsx files.&lt;/li&gt; 
 &lt;li&gt;Based on the identified subheadings in the document, locate and organize corresponding text blocks to make the parsing structure clearer and the information hierarchy more distinct.&lt;/li&gt; 
 &lt;li&gt;Optimized parsing results for webpage URLs, converted to .md format.&lt;/li&gt; 
 &lt;li&gt;Support for more encoding formats of txt files and docx files.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Required item&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Minimum Requirement&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;RAM Memory&lt;/td&gt; 
   &lt;td&gt;&amp;gt;= 20GB&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux/Mac&lt;/td&gt; 
   &lt;td&gt;Docker version&lt;/td&gt; 
   &lt;td&gt;&amp;gt;= 20.10.5&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.docker.com/engine/install/"&gt;Docker install&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux/Mac&lt;/td&gt; 
   &lt;td&gt;docker compose version&lt;/td&gt; 
   &lt;td&gt;&amp;gt;= 2.23.3&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.docker.com/compose/install/"&gt;docker compose install&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;Docker Desktop&lt;/td&gt; 
   &lt;td&gt;&amp;gt;= 4.26.1Ôºà131620Ôºâ&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.docker.com/desktop/install/windows-install/"&gt;Docker Desktop for Windows&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;step1: pull qanything repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/netease-youdao/QAnything.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;step2: Enter the project root directory and execute the startup command.&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute the docker compose start command&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;The startup process takes about 30 seconds. When the log outputs "qanything backend service is ready!", the startup is complete!&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd QAnything
# Start on Linux
docker compose -f docker-compose-linux.yaml up
# Start on Mac
docker compose -f docker-compose-mac.yaml up
# Start on Windows
docker compose -f docker-compose-win.yaml up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(Note) If the startup fails, you can try changing &lt;code&gt;docker compose&lt;/code&gt; to &lt;code&gt;docker-compose&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;step3: start to experience&lt;/h3&gt; 
&lt;h4&gt;Front end&lt;/h4&gt; 
&lt;p&gt;After successful installation, you can experience the application by entering the following addresses in your web browser.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Front end address: &lt;a href="http://localhost:8777/qanything/"&gt;http://localhost:8777/qanything/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;API&lt;/h3&gt; 
&lt;p&gt;If you want to visit API, please refer to the following address:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;API address: &lt;a href="http://localhost:8777/qanything/"&gt;http://localhost:8777/qanything/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For detailed API documentation, please refer to &lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/API.md"&gt;QAnything API documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;DEBUG&lt;/h3&gt; 
&lt;p&gt;If you want to view the relevant logs, please check the log files in the &lt;code&gt;QAnything/logs/debug_logs&lt;/code&gt; directory.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;debug.log&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;User request processing log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;sanic_api.log&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Backend service running log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;llm_embed_rerank_tritonserver.log&lt;/strong&gt;(Single card deployment) 
  &lt;ul&gt; 
   &lt;li&gt;LLM embedding and rerank tritonserver service startup log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;llm_tritonserver.log&lt;/strong&gt;(Multi-card deployment) 
  &lt;ul&gt; 
   &lt;li&gt;LLM tritonserver service startup log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;embed_rerank_tritonserver.log&lt;/strong&gt;(Multi-card deployment or use of the OpenAI interface.) 
  &lt;ul&gt; 
   &lt;li&gt;Embedding and rerank tritonserver service startup log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;rerank_server.log 
  &lt;ul&gt; 
   &lt;li&gt;Rerank service running log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;ocr_server.log 
  &lt;ul&gt; 
   &lt;li&gt;OCR service running log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;npm_server.log 
  &lt;ul&gt; 
   &lt;li&gt;Front-end service running log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;llm_server_entrypoint.log 
  &lt;ul&gt; 
   &lt;li&gt;LLM intermediate server running log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;fastchat_logs/*.log 
  &lt;ul&gt; 
   &lt;li&gt;FastChat service running log&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Close service&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Front desk service startup mode like:
docker compose -f docker-compose-xxx.yaml up  # To close the service, please press Ctrl+C.
# Backend service startup mode like: 
docker compose -f docker-compose-xxx.yaml up -d # To close the service, please execute the following command.
docker compose -f docker-compose-xxx.yaml down
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Offline Use&lt;/h2&gt; 
&lt;p&gt;If you want to use QAnything offline, you need to deploy the local large model (recommended to use ollama) on the offline machine in advance, and then you can start the service using the following command.&lt;/p&gt; 
&lt;h3&gt;Install offline for windows&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Download the docker image on a networked machine
docker pull quay.io/coreos/etcd:v3.5.5
docker pull minio/minio:RELEASE.2023-03-20T20-16-18Z
docker pull milvusdb/milvus:v2.4.8
docker pull mysql:8.4
docker pull xixihahaliu01/qanything-win:v1.5.1  # From [https://github.com/netease-youdao/QAnything/blob/master/docker-compose-windows.yaml#L103] Get the latest version number.

# pack image
docker save quay.io/coreos/etcd:v3.5.5 minio/minio:RELEASE.2023-03-20T20-16-18Z milvusdb/milvus:v2.4.8 mysql:8.4 xixihahaliu01/qanything-win:v1.5.1 -o qanything_offline.tar

# download QAnything code
wget https://github.com/netease-youdao/QAnything/archive/refs/heads/master.zip

# Copy the image qanything_offline.tar and the code qany-master.zip to the offline machine 
cp QAnything-master.zip qanything_offline.tar /path/to/your/offline/machine

# Load image on offline machine 
docker load -i qanything_offline.tar

# Unzip the code and run it
unzip QAnything-master.zip
cd QAnything-master
docker compose -f docker-compose-win.yaml up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Similarly for other systems, just replace the corresponding image of the system, such as replacing mac with docker-compose-mac.yaml, and linux with docker-compose-linux.yaml.&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;The most common issue at present is that the local service of Ollama has poor question and answer effects, and you can refer to the solutions in the FAQ.&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/FAQ_zh.md"&gt;FAQ&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We appreciate your interest in contributing to our project. Whether you're fixing a bug, improving an existing feature, or adding something completely new, your contributions are welcome!&lt;/p&gt; 
&lt;h3&gt;Thanks to all contributors for their efforts&lt;/h3&gt; 
&lt;a href="https://github.com/netease-youdao/QAnything/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=netease-youdao/QAnything" /&gt; &lt;/a&gt; 
&lt;h1&gt;üõ£Ô∏è Roadmap &amp;amp; Feedback&lt;/h1&gt; 
&lt;p&gt;üîé To learn about QAnything's future plans and progress, please see here: &lt;a href="https://qanything.canny.io/"&gt;QAnything Roadmap&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ü§¨To provide feedback to QAnything, please see here: &lt;a href="https://qanything.canny.io/feature-requests"&gt;QAnything Feedbak&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Community &amp;amp; Support&lt;/h1&gt; 
&lt;h2&gt;Discord &lt;a href="https://discord.gg/5uNpPsEJz8"&gt;&lt;img src="https://img.shields.io/discord/1197874288963895436?style=social&amp;amp;logo=discord" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Welcome to the QAnything &lt;a href="https://discord.gg/5uNpPsEJz8"&gt;Discord&lt;/a&gt; community&lt;/p&gt; 
&lt;h2&gt;WeChat&lt;/h2&gt; 
&lt;p&gt;Welcome to follow QAnything WeChat Official Account to get the latest information.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/images/qrcode_for_qanything.jpg" width="30%" height="auto" /&gt; 
&lt;p&gt;Welcome to scan the code to join the QAnything discussion group.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/docs/images/Wechat.jpg" width="30%" height="auto" /&gt; 
&lt;h2&gt;Email&lt;/h2&gt; 
&lt;p&gt;If you need to contact our team privately, please reach out to us via the following email:&lt;/p&gt; 
&lt;p&gt;&lt;a href="mailto:qanything@rd.netease.com"&gt;qanything@rd.netease.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;GitHub issues &amp;amp; discussions&lt;/h2&gt; 
&lt;p&gt;Reach out to the maintainer at one of the following places:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/netease-youdao/QAnything/issues"&gt;Github issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/netease-youdao/QAnything/discussions"&gt;Github discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Contact options listed on &lt;a href="https://github.com/netease-youdao"&gt;this GitHub profile&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;a href="https://github.com/netease-youdao/QAnything/discussions"&gt; 
 &lt;!-- Please provide path to your logo here --&gt; &lt;img src="https://github.com/netease-youdao/QAnything/assets/29041332/ad027ec5-0bbc-4ea0-92eb-81b30c5359a1" alt="Logo" width="600" /&gt; &lt;/a&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#netease-youdao/QAnything&amp;amp;netease-youdao/BCEmbedding&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=netease-youdao/QAnything,netease-youdao/BCEmbedding&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt; is licensed under &lt;a href="https://raw.githubusercontent.com/netease-youdao/QAnything/qanything-v2/LICENSE"&gt;AGPL-3.0 License&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Acknowledgments&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt; adopts dependencies from the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Thanks to our &lt;a href="https://github.com/netease-youdao/BCEmbedding"&gt;BCEmbedding&lt;/a&gt; for the excellent embedding and rerank model.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/QwenLM/Qwen"&gt;Qwen&lt;/a&gt; for strong base language models.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/triton-inference-server/server"&gt;Triton Inference Server&lt;/a&gt; for providing great open source inference serving.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/lm-sys/FastChat"&gt;FastChat&lt;/a&gt; for providing a fully OpenAI-compatible API server.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/NVIDIA/FasterTransformer"&gt;FasterTransformer&lt;/a&gt; and &lt;a href="https://github.com/vllm-project/vllm"&gt;vllm&lt;/a&gt; for highly optimized LLM inference backend.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/langchain-ai/langchain"&gt;Langchain&lt;/a&gt; for the wonderful llm application framework.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/chatchat-space/Langchain-Chatchat"&gt;Langchain-Chatchat&lt;/a&gt; for the inspiration provided on local knowledge base Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/milvus-io/milvus"&gt;Milvus&lt;/a&gt; for the excellent semantic search library.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;PaddleOCR&lt;/a&gt; for its ease-to-use OCR library.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/sanic-org/sanic"&gt;Sanic&lt;/a&gt; for the powerful web service framework.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; for providing some ideas for document parsing.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>