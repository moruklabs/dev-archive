<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Wed, 10 Dec 2025 01:37:35 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>commaai/openpilot</title>
      <link>https://github.com/commaai/openpilot</link>
      <description>&lt;p&gt;openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" style="text-align: center;"&gt; 
 &lt;h1&gt;openpilot&lt;/h1&gt; 
 &lt;p&gt; &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt; &lt;br /&gt; Currently, it upgrades the driver assistance system in 300+ supported cars. &lt;/p&gt; 
 &lt;h3&gt; &lt;a href="https://docs.comma.ai"&gt;Docs&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://docs.comma.ai/contributing/roadmap/"&gt;Roadmap&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://github.com/commaai/openpilot/raw/master/docs/CONTRIBUTING.md"&gt;Contribute&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://discord.comma.ai"&gt;Community&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://comma.ai/shop"&gt;Try it on a comma 3X&lt;/a&gt; &lt;/h3&gt; 
 &lt;p&gt;Quick start: &lt;code&gt;bash &amp;lt;(curl -fsSL openpilot.comma.ai)&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/commaai/openpilot/actions/workflows/tests.yaml"&gt;&lt;img src="https://github.com/commaai/openpilot/actions/workflows/tests.yaml/badge.svg?sanitize=true" alt="openpilot tests" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://x.com/comma_ai"&gt;&lt;img src="https://img.shields.io/twitter/follow/comma_ai" alt="X Follow" /&gt;&lt;/a&gt; &lt;a href="https://discord.comma.ai"&gt;&lt;img src="https://img.shields.io/discord/469524606043160576" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/NmBfgOanCyk" title="Video By Greer Viau"&gt;&lt;img src="https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/VHKyqZ7t8Gw" title="Video By Logan LeGrand"&gt;&lt;img src="https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/SUIZYzxtMQs" title="A drive to Taco Bell"&gt;&lt;img src="https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Using openpilot in a car&lt;/h2&gt; 
&lt;p&gt;To use openpilot in a car, you need four things:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Device:&lt;/strong&gt; a comma 3X, available at &lt;a href="https://comma.ai/shop/comma-3x"&gt;comma.ai/shop&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; The setup procedure for the comma 3X allows users to enter a URL for custom software. Use the URL &lt;code&gt;openpilot.comma.ai&lt;/code&gt; to install the release version.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Car:&lt;/strong&gt; Ensure that you have one of &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/docs/CARS.md"&gt;the 275+ supported cars&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Car Harness:&lt;/strong&gt; You will also need a &lt;a href="https://comma.ai/shop/car-harness"&gt;car harness&lt;/a&gt; to connect your comma 3X to your car.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We have detailed instructions for &lt;a href="https://comma.ai/setup"&gt;how to install the harness and device in a car&lt;/a&gt;. Note that it's possible to run openpilot on &lt;a href="https://blog.comma.ai/self-driving-car-for-free/"&gt;other hardware&lt;/a&gt;, although it's not plug-and-play.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;p&gt;Running &lt;code&gt;master&lt;/code&gt; and other branches directly is supported, but it's recommended to run one of the following prebuilt branches:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;comma four branch&lt;/th&gt; 
   &lt;th&gt;comma 3X branch&lt;/th&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;release-mici&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;release-tizi&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;openpilot.comma.ai&lt;/td&gt; 
   &lt;td&gt;This is openpilot's release branch.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;release-mici-staging&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;release-tizi-staging&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;openpilot-test.comma.ai&lt;/td&gt; 
   &lt;td&gt;This is the staging branch for releases. Use it to get new releases slightly early.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;openpilot-nightly.comma.ai&lt;/td&gt; 
   &lt;td&gt;This is the bleeding edge development branch. Do not expect this to be stable.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;nightly-dev&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;nightly-dev&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;installer.comma.ai/commaai/nightly-dev&lt;/td&gt; 
   &lt;td&gt;Same as nightly, but includes experimental development features for some cars.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;To start developing openpilot&lt;/h2&gt; 
&lt;p&gt;openpilot is developed by &lt;a href="https://comma.ai/"&gt;comma&lt;/a&gt; and by users like you. We welcome both pull requests and issues on &lt;a href="http://github.com/commaai/openpilot"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the &lt;a href="https://discord.comma.ai"&gt;community Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/docs/CONTRIBUTING.md"&gt;the contributing docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check out the &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/tools/"&gt;openpilot tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Code documentation lives at &lt;a href="https://docs.comma.ai"&gt;https://docs.comma.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Information about running openpilot lives on the &lt;a href="https://github.com/commaai/openpilot/wiki"&gt;community wiki&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want to get paid to work on openpilot? &lt;a href="https://comma.ai/jobs#open-positions"&gt;comma is hiring&lt;/a&gt; and offers lots of &lt;a href="https://comma.ai/bounties"&gt;bounties&lt;/a&gt; for external contributors.&lt;/p&gt; 
&lt;h2&gt;Safety and Testing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;openpilot observes &lt;a href="https://en.wikipedia.org/wiki/ISO_26262"&gt;ISO26262&lt;/a&gt; guidelines, see &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/docs/SAFETY.md"&gt;SAFETY.md&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;openpilot has software-in-the-loop &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/.github/workflows/tests.yaml"&gt;tests&lt;/a&gt; that run on every commit.&lt;/li&gt; 
 &lt;li&gt;The code enforcing the safety model lives in panda and is written in C, see &lt;a href="https://github.com/commaai/panda#code-rigor"&gt;code rigor&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;panda has software-in-the-loop &lt;a href="https://github.com/commaai/panda/tree/master/tests/safety"&gt;safety tests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.&lt;/li&gt; 
 &lt;li&gt;panda has additional hardware-in-the-loop &lt;a href="https://github.com/commaai/panda/raw/master/Jenkinsfile"&gt;tests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;MIT Licensed&lt;/summary&gt; 
 &lt;p&gt;openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.&lt;/p&gt; 
 &lt;p&gt;Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys‚Äô fees and costs) which arise out of, relate to or result from any use of this software by user.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT. YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS. NO WARRANTY EXPRESSED OR IMPLIED.&lt;/strong&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;User Data and comma Account&lt;/summary&gt; 
 &lt;p&gt;By default, openpilot uploads the driving data to our servers. You can also access your data through &lt;a href="https://connect.comma.ai/"&gt;comma connect&lt;/a&gt;. We use your data to train better models and improve openpilot for everyone.&lt;/p&gt; 
 &lt;p&gt;openpilot is open source software: the user is free to disable data collection if they wish to do so.&lt;/p&gt; 
 &lt;p&gt;openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs. The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.&lt;/p&gt; 
 &lt;p&gt;By using openpilot, you agree to &lt;a href="https://comma.ai/privacy"&gt;our Privacy Policy&lt;/a&gt;. You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.&lt;/p&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=microsoft" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader" alt="Technical Report" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/Status-New-brightgreen?style=flat" alt="New" /&gt; 
 &lt;img src="https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;amp;logo=soundcharts" alt="Realtime TTS" /&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-09: üì£ We‚Äôve added experimental speakers in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) for exploration‚Äîwelcome to try them out and share your feedback.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers. &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;(Launch your own realtime demo via the websocket example in &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo"&gt;Usage&lt;/a&gt;).&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/p&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;VibeVoice is a novel framework designed for generating &lt;strong&gt;expressive&lt;/strong&gt;, &lt;strong&gt;long-form&lt;/strong&gt;, &lt;strong&gt;multi-speaker&lt;/strong&gt; conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.&lt;/p&gt; 
&lt;p&gt;VibeVoice currently includes two model variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Long-form multi-speaker model&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; with up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt;, surpassing the typical 1‚Äì2 speaker limits of many prior models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;Realtime streaming TTS model&lt;/a&gt;&lt;/strong&gt;: Produces initial audible speech in ~&lt;strong&gt;300 ms&lt;/strong&gt; and supports &lt;strong&gt;streaming text input&lt;/strong&gt; for single-speaker &lt;strong&gt;real-time&lt;/strong&gt; speech generation; designed for low-latency generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/MOS-preference.png" alt="MOS Preference Results" height="260px" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice.jpg" alt="VibeVoice Overview" height="250px" style="margin-right: 10px;" /&gt; &lt;/p&gt; 
&lt;h3&gt;üéµ Demo Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Video Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We produced this video with &lt;a href="https://github.com/Wan-Video/Wan2.2"&gt;Wan2.2&lt;/a&gt;. We sincerely appreciate the Wan-Video team for their great work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;For more examples, see the &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Risks and limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.&lt;/p&gt; 
&lt;p&gt;Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.&lt;/p&gt; 
&lt;p&gt;Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CVHub520/X-AnyLabeling</title>
      <link>https://github.com/CVHub520/X-AnyLabeling</link>
      <description>&lt;p&gt;Effortless data labeling with AI support from Segment Anything and other awesome models.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://github.com/CVHub520/X-AnyLabeling/" target="_blank"&gt; &lt;img alt="X-AnyLabeling" height="200px" src="https://github.com/user-attachments/assets/0714a182-92bd-4b47-b48d-1c5d7c225176" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/README_zh-CN.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-LGPL%20v3-blue.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/github/v/release/CVHub520/X-AnyLabeling?color=ffa" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/pypi/v/x-anylabeling-cvhub?logo=pypi&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/python-3.10+-aff.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/github/downloads/CVHub520/X-AnyLabeling/total?label=downloads" /&gt;&lt;/a&gt; &lt;a href="https://modelscope.cn/collections/X-AnyLabeling-7b0e1798bcda43"&gt;&lt;img src="https://img.shields.io/badge/modelscope-X--AnyLabeling-6750FF?link=https%3A%2F%2Fmodelscope.cn%2Fcollections%2FX-AnyLabeling-7b0e1798bcda43" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/18329471/234640541-a6a65fbc-d7a5-4ec3-9b65-55305b01a7aa.png" alt="" /&gt;&lt;/p&gt; 
&lt;img src="https://github.com/user-attachments/assets/8b5f290a-dddf-410c-a004-21e5a7bcd1cc" width="100%" /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Auto-Training&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/c0ab2056-2743-4a2c-ba93-13f478d3481e" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Auto-Labeling&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/f517fa94-c49c-4f05-864e-96b34f592079" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Detect Anything&lt;/strong&gt;&lt;/summary&gt; 
 &lt;img src="https://github.com/user-attachments/assets/7f43bcec-96fd-48d1-bd36-9e5a440a66f6" width="100%" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Segment Anything&lt;/strong&gt;&lt;/summary&gt; 
 &lt;img src="https://github.com/user-attachments/assets/208dc9ed-b8c9-4127-9e5b-e76f53892f03" width="100%" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Promptable Concept Grounding&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/52cbdb5d-cc60-4be5-826f-903ea4330ca8" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;VQA&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/53adcff4-b962-41b7-a408-3afecd8d8c82" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Chatbot&lt;/strong&gt;&lt;/summary&gt; 
 &lt;img src="https://github.com/user-attachments/assets/56c9a20b-c836-47aa-8b54-bad5bb99b735" width="100%" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Image Classifier&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/0652adfb-48a4-4219-9b18-16ff5ce31be0" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;h2&gt;ü•≥ What's New&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add support for &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/grounding/sam3/README.md"&gt;Segment Anything 3&lt;/a&gt; model with text and visual promptable segmentation (#1207)&lt;/li&gt; 
 &lt;li&gt;Add TinyObj mode for Segment Anything Model to improve small object detection accuracy in high-resolution images by local cropping (#1193)&lt;/li&gt; 
 &lt;li&gt;For more details, please refer to the &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;X-AnyLabeling&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;X-AnyLabeling&lt;/strong&gt; is a powerful annotation tool that integrates an AI engine for fast and automatic labeling. It's designed for multi-modal data engineers, offering industrial-grade solutions for complex tasks.&lt;/p&gt; 
&lt;img src="https://github.com/user-attachments/assets/632e629b-0dec-407b-95a6-728052e1dd7b" width="100%" /&gt; 
&lt;p&gt;Also, we highly recommend trying out &lt;a href="https://github.com/CVHub520/X-AnyLabeling-Server"&gt;X-AnyLabeling-Server&lt;/a&gt;, a simple, lightweight, and extensible framework that enables remote inference capabilities for X-AnyLabeling.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;img src="https://github.com/user-attachments/assets/c65db18f-167b-49e8-bea3-fcf4b43a8ffd" width="100%" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports remote inference service.&lt;/li&gt; 
 &lt;li&gt;Processes both &lt;code&gt;images&lt;/code&gt; and &lt;code&gt;videos&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerates inference with &lt;code&gt;GPU&lt;/code&gt; support.&lt;/li&gt; 
 &lt;li&gt;Allows custom models and secondary development.&lt;/li&gt; 
 &lt;li&gt;Supports one-click inference for all images in the current task.&lt;/li&gt; 
 &lt;li&gt;Supports import/export for formats like COCO, VOC, YOLO, DOTA, MOT, MASK, PPOCR, MMGD, VLM-R1.&lt;/li&gt; 
 &lt;li&gt;Handles tasks like &lt;code&gt;classification&lt;/code&gt;, &lt;code&gt;detection&lt;/code&gt;, &lt;code&gt;segmentation&lt;/code&gt;, &lt;code&gt;caption&lt;/code&gt;, &lt;code&gt;rotation&lt;/code&gt;, &lt;code&gt;tracking&lt;/code&gt;, &lt;code&gt;estimation&lt;/code&gt;, &lt;code&gt;ocr&lt;/code&gt;, &lt;code&gt;vqa&lt;/code&gt;, &lt;code&gt;grounding&lt;/code&gt; and so on.&lt;/li&gt; 
 &lt;li&gt;Supports diverse annotation styles: &lt;code&gt;polygons&lt;/code&gt;, &lt;code&gt;rectangles&lt;/code&gt;, &lt;code&gt;rotated boxes&lt;/code&gt;, &lt;code&gt;circles&lt;/code&gt;, &lt;code&gt;lines&lt;/code&gt;, &lt;code&gt;points&lt;/code&gt;, and annotations for &lt;code&gt;text detection&lt;/code&gt;, &lt;code&gt;recognition&lt;/code&gt;, and &lt;code&gt;KIE&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Model library&lt;/h3&gt; 
&lt;img src="https://github.com/user-attachments/assets/7da2da2e-f182-4a1b-85f6-bfd0dfcc6a1b" width="100%" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;&lt;strong&gt;Task Category&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="left"&gt;&lt;strong&gt;Supported Models&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üñºÔ∏è Image Classification&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv5-Cls, YOLOv8-Cls, YOLO11-Cls, InternImage, PULC&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üéØ Object Detection&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv5/6/7/8/9/10, YOLO11/12, YOLOX, YOLO-NAS, D-FINE, DAMO-YOLO, Gold_YOLO, RT-DETR, RF-DETR, DEIMv2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üñåÔ∏è Instance Segmentation&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv5-Seg, YOLOv8-Seg, YOLO11-Seg, Hyper-YOLO-Seg, RF-DETR-Seg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üèÉ Pose Estimation&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv8-Pose, YOLO11-Pose, DWPose, RTMO&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üë£ Tracking&lt;/td&gt; 
   &lt;td align="left"&gt;Bot-SORT, ByteTrack&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üîÑ Rotated Object Detection&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv5-Obb, YOLOv8-Obb, YOLO11-Obb&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üìè Depth Estimation&lt;/td&gt; 
   &lt;td align="left"&gt;Depth Anything&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üß© Segment Anything&lt;/td&gt; 
   &lt;td align="left"&gt;SAM 1/2/3, SAM-HQ, SAM-Med2D, EdgeSAM, EfficientViT-SAM, MobileSAM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;‚úÇÔ∏è Image Matting&lt;/td&gt; 
   &lt;td align="left"&gt;RMBG 1.4/2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üí° Proposal&lt;/td&gt; 
   &lt;td align="left"&gt;UPN&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üè∑Ô∏è Tagging&lt;/td&gt; 
   &lt;td align="left"&gt;RAM, RAM++&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üìÑ OCR&lt;/td&gt; 
   &lt;td align="left"&gt;PP-OCRv4, PP-OCRv5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üó£Ô∏è Vision Foundation Models&lt;/td&gt; 
   &lt;td align="left"&gt;Florence2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üëÅÔ∏è Vision Language Models&lt;/td&gt; 
   &lt;td align="left"&gt;Qwen3-VL, Gemini, ChatGPT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üõ£Ô∏è Land Detection&lt;/td&gt; 
   &lt;td align="left"&gt;CLRNet&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üìç Grounding&lt;/td&gt; 
   &lt;td align="left"&gt;CountGD, GeCO, Grounding DINO, YOLO-World, YOLOE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;üìö Other&lt;/td&gt; 
   &lt;td align="left"&gt;üëâ &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/model_zoo.md"&gt;model_zoo&lt;/a&gt; üëà&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Docs&lt;/h2&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CVHub520/X-AnyLabeling-Server"&gt;Remote Inference Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/get_started.md"&gt;Installation &amp;amp; Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/user_guide.md"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/cli.md"&gt;Command Line Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/custom_model.md"&gt;Customize a model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/chatbot.md"&gt;Chatbot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/vqa.md"&gt;VQA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/image_classifier.md"&gt;Multi-class Image Classifier&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src="https://github.com/user-attachments/assets/0d67311c-f441-44b6-9ee0-932f25f51b1c" width="100%" /&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/classification/"&gt;Classification&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/classification/image-level/README.md"&gt;Image-Level&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/classification/shape-level/README.md"&gt;Shape-Level&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/detection/"&gt;Detection&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/detection/hbb/README.md"&gt;HBB Object Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/detection/obb/README.md"&gt;OBB Object Detection&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/segmentation/README.md"&gt;Segmentation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/segmentation/instance_segmentation/"&gt;Instance Segmentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/segmentation/binary_semantic_segmentation/"&gt;Binary Semantic Segmentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/segmentation/multiclass_semantic_segmentation/"&gt;Multiclass Semantic Segmentation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/description/"&gt;Description&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/description/tagging/README.md"&gt;Tagging&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/description/captioning/README.md"&gt;Captioning&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/estimation/"&gt;Estimation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/estimation/pose_estimation/README.md"&gt;Pose Estimation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/estimation/depth_estimation/README.md"&gt;Depth Estimation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/optical_character_recognition/"&gt;OCR&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/optical_character_recognition/text_recognition/"&gt;Text Recognition&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/optical_character_recognition/key_information_extraction/README.md"&gt;Key Information Extraction&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;MOT&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;Tracking by HBB Object Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;Tracking by OBB Object Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;Tracking by Instance Segmentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;Tracking by Pose Estimation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/interactive_video_object_segmentation/README.md"&gt;iVOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/matting/"&gt;Matting&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/matting/image_matting/README.md"&gt;Image Matting&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/vision_language/"&gt;Vision-Language&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/vision_language/florence2/README.md"&gt;Florence 2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/counting/"&gt;Counting&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/counting/geco/README.md"&gt;GeCo&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/grounding/"&gt;Grounding&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/grounding/yoloe/README.md"&gt;YOLOE&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/grounding/sam3/README.md"&gt;SAM 3&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/training/"&gt;Training&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/training/ultralytics/README.md"&gt;Ultralytics&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;We believe in open collaboration! &lt;strong&gt;X‚ÄëAnyLabeling&lt;/strong&gt; continues to grow with the support of the community. Whether you're fixing bugs, improving documentation, or adding new features, your contributions make a real impact.&lt;/p&gt; 
&lt;p&gt;To get started, please read our &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; and make sure to agree to the &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/CLA.md"&gt;Contributor License Agreement (CLA)&lt;/a&gt; before submitting a pull request.&lt;/p&gt; 
&lt;p&gt;If you find this project helpful, please consider giving it a ‚≠êÔ∏è star! Have questions or suggestions? Open an &lt;a href="https://github.com/CVHub520/X-AnyLabeling/issues"&gt;issue&lt;/a&gt; or email us at &lt;a href="mailto:cv_hub@163.com"&gt;cv_hub@163.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;A huge thank you üôè to everyone helping to make X‚ÄëAnyLabeling better.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/LICENSE"&gt;GPL-3.0 license&lt;/a&gt; and is completely open source and free. The original intention is to enable more developers, researchers, and enterprises to conveniently use this AI application platform, promoting the development of the entire industry. We encourage everyone to use it freely (including commercial use), and you can also add features based on this project and commercialize it, but you must retain the brand identity and indicate the source project address.&lt;/p&gt; 
&lt;p&gt;Additionally, to understand the ecosystem and usage of X-AnyLabeling, if you use this project for academic, research, teaching, or enterprise purposes, please fill out the &lt;a href="https://forms.gle/MZCKhU7UJ4TRSWxR7"&gt;registration form&lt;/a&gt;. This registration is only for statistical purposes and will not incur any fees. We will strictly keep all information confidential.&lt;/p&gt; 
&lt;p&gt;X-AnyLabeling is independently developed and maintained by an individual. If this project has been helpful to you, we welcome your support through the donation links below to help sustain the project's continued development. Your support is the greatest encouragement! If you have any questions about the project or would like to collaborate, please feel free to contact via WeChat: ww10874 or email provided above.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ko-fi.com/cvhub520"&gt;buy-me-a-coffee&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CVHub520/X-AnyLabeling/raw/main/README_zh-CN.md#%E8%B5%9E%E5%8A%A9"&gt;Wechat/Alipay&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;I extend my heartfelt thanks to the developers and contributors of &lt;a href="https://github.com/vietanhdev/anylabeling"&gt;AnyLabeling&lt;/a&gt;, &lt;a href="https://github.com/wkentaro/labelme"&gt;LabelMe&lt;/a&gt;, &lt;a href="https://github.com/tzutalin/labelImg"&gt;LabelImg&lt;/a&gt;, &lt;a href="https://github.com/cgvict/roLabelImg"&gt;roLabelImg&lt;/a&gt;, &lt;a href="https://github.com/PFCCLab/PPOCRLabel"&gt;PPOCRLabel&lt;/a&gt; and &lt;a href="https://github.com/opencv/cvat"&gt;CVAT&lt;/a&gt;, whose work has been crucial to the success of this project.&lt;/p&gt; 
&lt;h2&gt;Citing&lt;/h2&gt; 
&lt;p&gt;If you use this software in your research, please cite it as below:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{X-AnyLabeling,
  year = {2023},
  author = {Wei Wang},
  publisher = {Github},
  organization = {CVHub},
  journal = {Github repository},
  title = {Advanced Auto Labeling Solution with Added Features},
  howpublished = {\url{https://github.com/CVHub520/X-AnyLabeling}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=CVHub520/X-AnyLabeling&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt;
 &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#top"&gt;üîù Back to Top&lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>infiniflow/ragflow</title>
      <link>https://github.com/infiniflow/ragflow</link>
      <description>&lt;p&gt;RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://demo.ragflow.io/"&gt; &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow/main/web/src/assets/logo-with-text.svg?sanitize=true" width="520" alt="ragflow logo" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README.md"&gt;&lt;img alt="README in English" src="https://img.shields.io/badge/English-DBEDFA" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_zh.md"&gt;&lt;img alt="ÁÆÄ‰Ωì‰∏≠ÊñáÁâàËá™Ëø∞Êñá‰ª∂" src="https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_tzh.md"&gt;&lt;img alt="ÁπÅÈ´îÁâà‰∏≠ÊñáËá™Ëø∞Êñá‰ª∂" src="https://img.shields.io/badge/ÁπÅÈ´î‰∏≠Êñá-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ja.md"&gt;&lt;img alt="Êó•Êú¨Ë™û„ÅÆREADME" src="https://img.shields.io/badge/Êó•Êú¨Ë™û-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ko.md"&gt;&lt;img alt="ÌïúÍµ≠Ïñ¥" src="https://img.shields.io/badge/ÌïúÍµ≠Ïñ¥-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_id.md"&gt;&lt;img alt="Bahasa Indonesia" src="https://img.shields.io/badge/Bahasa Indonesia-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_pt_br.md"&gt;&lt;img alt="Portugu√™s(Brasil)" src="https://img.shields.io/badge/Portugu√™s(Brasil)-DFE0E5" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/intent/follow?screen_name=infiniflowai" target="_blank"&gt; &lt;img src="https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;amp;color=%20%23f5f5f5" alt="follow on X(Twitter)" /&gt; &lt;/a&gt; &lt;a href="https://demo.ragflow.io" target="_blank"&gt; &lt;img alt="Static Badge" src="https://img.shields.io/badge/Online-Demo-4e6b99" /&gt; &lt;/a&gt; &lt;a href="https://hub.docker.com/r/infiniflow/ragflow" target="_blank"&gt; &lt;img src="https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;amp;color=0db7ed&amp;amp;logo=docker&amp;amp;logoColor=white&amp;amp;style=flat-square" alt="docker pull infiniflow/ragflow:v0.22.1" /&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/releases/latest"&gt; &lt;img src="https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;amp;label=Latest%20Release" alt="Latest Release" /&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/raw/main/LICENSE"&gt; &lt;img height="21" src="https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;amp;color=2e6cc4" alt="license" /&gt; &lt;/a&gt; &lt;a href="https://deepwiki.com/infiniflow/ragflow"&gt; &lt;img alt="Ask DeepWiki" src="https://deepwiki.com/badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://ragflow.io/docs/dev/"&gt;Document&lt;/a&gt; | &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;Roadmap&lt;/a&gt; | &lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt; | &lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt; | &lt;a href="https://demo.ragflow.io"&gt;Demo&lt;/a&gt; &lt;/h4&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png" width="1200" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/9064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9064" alt="infiniflow%2Fragflow | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;üìï Table of Contents&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üí° &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-what-is-ragflow"&gt;What is RAGFlow?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üéÆ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-demo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üìå &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-latest-updates"&gt;Latest Updates&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üåü &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üîé &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-system-architecture"&gt;System Architecture&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üé¨ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-get-started"&gt;Get Started&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üîß &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-configurations"&gt;Configurations&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üîß &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-build-a-docker-image"&gt;Build a Docker image&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üî® &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-launch-service-from-source-for-development"&gt;Launch service from source for development&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üìö &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üìú &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üèÑ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-community"&gt;Community&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üôå &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üí° What is RAGFlow?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://ragflow.io/"&gt;RAGFlow&lt;/a&gt; is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.&lt;/p&gt; 
&lt;h2&gt;üéÆ Demo&lt;/h2&gt; 
&lt;p&gt;Try our demo at &lt;a href="https://demo.ragflow.io"&gt;https://demo.ragflow.io&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif" width="1200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif" width="1200" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;üî• Latest Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-11-19 Supports Gemini 3 Pro.&lt;/li&gt; 
 &lt;li&gt;2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.&lt;/li&gt; 
 &lt;li&gt;2025-10-23 Supports MinerU &amp;amp; Docling as document parsing methods.&lt;/li&gt; 
 &lt;li&gt;2025-10-15 Supports orchestrable ingestion pipeline.&lt;/li&gt; 
 &lt;li&gt;2025-08-08 Supports OpenAI's latest GPT-5 series models.&lt;/li&gt; 
 &lt;li&gt;2025-08-01 Supports agentic workflow and MCP.&lt;/li&gt; 
 &lt;li&gt;2025-05-23 Adds a Python/JavaScript code executor component to Agent.&lt;/li&gt; 
 &lt;li&gt;2025-05-05 Supports cross-language query.&lt;/li&gt; 
 &lt;li&gt;2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéâ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;‚≠êÔ∏è Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new releases! üåü&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba" width="1200" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;üåü Key Features&lt;/h2&gt; 
&lt;h3&gt;üç≠ &lt;strong&gt;"Quality in, quality out"&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/deepdoc/README.md"&gt;Deep document understanding&lt;/a&gt;-based knowledge extraction from unstructured data with complicated formats.&lt;/li&gt; 
 &lt;li&gt;Finds "needle in a data haystack" of literally unlimited tokens.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üç± &lt;strong&gt;Template-based chunking&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Intelligent and explainable.&lt;/li&gt; 
 &lt;li&gt;Plenty of template options to choose from.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üå± &lt;strong&gt;Grounded citations with reduced hallucinations&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visualization of text chunking to allow human intervention.&lt;/li&gt; 
 &lt;li&gt;Quick view of the key references and traceable citations to support grounded answers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üçî &lt;strong&gt;Compatibility with heterogeneous data sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üõÄ &lt;strong&gt;Automated and effortless RAG workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Streamlined RAG orchestration catered to both personal and large businesses.&lt;/li&gt; 
 &lt;li&gt;Configurable LLMs as well as embedding models.&lt;/li&gt; 
 &lt;li&gt;Multiple recall paired with fused re-ranking.&lt;/li&gt; 
 &lt;li&gt;Intuitive APIs for seamless integration with business.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîé System Architecture&lt;/h2&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2" width="1000" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;üé¨ Get Started&lt;/h2&gt; 
&lt;h3&gt;üìù Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;CPU &amp;gt;= 4 cores&lt;/li&gt; 
 &lt;li&gt;RAM &amp;gt;= 16 GB&lt;/li&gt; 
 &lt;li&gt;Disk &amp;gt;= 50 GB&lt;/li&gt; 
 &lt;li&gt;Docker &amp;gt;= 24.0.0 &amp;amp; Docker Compose &amp;gt;= v2.26.1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gvisor.dev/docs/user_guide/install/"&gt;gVisor&lt;/a&gt;: Required only if you intend to use the code executor (sandbox) feature of RAGFlow.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you have not installed Docker on your local machine (Windows, Mac, or Linux), see &lt;a href="https://docs.docker.com/engine/install/"&gt;Install Docker Engine&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üöÄ Start up the server&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;vm.max_map_count&lt;/code&gt; &amp;gt;= 262144:&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;To check the value of &lt;code&gt;vm.max_map_count&lt;/code&gt;:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;$ sysctl vm.max_map_count
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;Reset &lt;code&gt;vm.max_map_count&lt;/code&gt; to a value at least 262144 if it is not.&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# In this case, we set it to 262144:
$ sudo sysctl -w vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;This change will be reset after a system reboot. To ensure your change remains permanent, add or update the &lt;code&gt;vm.max_map_count&lt;/code&gt; value in &lt;strong&gt;/etc/sysctl.conf&lt;/strong&gt; accordingly:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ git clone https://github.com/infiniflow/ragflow.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start up the server using the pre-built Docker images:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] All Docker images are built for x86 platforms. We don't currently offer Docker images for ARM64. If you are on an ARM64 platform, follow &lt;a href="https://ragflow.io/docs/dev/build_docker_image"&gt;this guide&lt;/a&gt; to build a Docker image compatible with your system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The command below downloads the &lt;code&gt;v0.22.1&lt;/code&gt; edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from &lt;code&gt;v0.22.1&lt;/code&gt;, update the &lt;code&gt;RAGFLOW_IMAGE&lt;/code&gt; variable accordingly in &lt;strong&gt;docker/.env&lt;/strong&gt; before using &lt;code&gt;docker compose&lt;/code&gt; to start the server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;   $ cd ragflow/docker
  
   # git checkout v0.22.1
   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)
   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.
   
   # Use CPU for DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate DeepDoc tasks:
   # sed -i '1i DEVICE=gpu' .env
   # docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Prior to &lt;code&gt;v0.22.0&lt;/code&gt;, we provided both images with embedding models and slim images without embedding models. Details as follows:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;RAGFlow image tag&lt;/th&gt; 
   &lt;th&gt;Image size (GB)&lt;/th&gt; 
   &lt;th&gt;Has embedding models?&lt;/th&gt; 
   &lt;th&gt;Stable?&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.21.1&lt;/td&gt; 
   &lt;td&gt;‚âà9&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.21.1-slim&lt;/td&gt; 
   &lt;td&gt;‚âà2&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Starting with &lt;code&gt;v0.22.0&lt;/code&gt;, we ship only the slim edition and no longer append the &lt;strong&gt;-slim&lt;/strong&gt; suffix to the image tag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt; &lt;p&gt;Check the server status after having the server up and running:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker logs -f docker-ragflow-cpu-1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;
      ____   ___    ______ ______ __
     / __ \ /   |  / ____// ____// /____  _      __
    / /_/ // /| | / / __ / /_   / // __ \| | /| / /
   / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
  /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

 * Running on all addresses (0.0.0.0)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a &lt;code&gt;network anormal&lt;/code&gt; error because, at that moment, your RAGFlow may not be fully initialized.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In your web browser, enter the IP address of your server and log in to RAGFlow.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;With the default settings, you only need to enter &lt;code&gt;http://IP_OF_YOUR_MACHINE&lt;/code&gt; (&lt;strong&gt;sans&lt;/strong&gt; port number) as the default HTTP serving port &lt;code&gt;80&lt;/code&gt; can be omitted when using the default configurations.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;, select the desired LLM factory in &lt;code&gt;user_default_llm&lt;/code&gt; and update the &lt;code&gt;API_KEY&lt;/code&gt; field with the corresponding API key.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;See &lt;a href="https://ragflow.io/docs/dev/llm_api_key_setup"&gt;llm_api_key_setup&lt;/a&gt; for more information.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;The show is on!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üîß Configurations&lt;/h2&gt; 
&lt;p&gt;When it comes to system configurations, you will need to manage the following files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env"&gt;.env&lt;/a&gt;: Keeps the fundamental setups for the system, such as &lt;code&gt;SVR_HTTP_PORT&lt;/code&gt;, &lt;code&gt;MYSQL_PASSWORD&lt;/code&gt;, and &lt;code&gt;MINIO_PASSWORD&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;: Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;: The system relies on &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; to start up.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md"&gt;./docker/README&lt;/a&gt; file provides a detailed description of the environment settings and service configurations which can be used as &lt;code&gt;${ENV_VARS}&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To update the default HTTP serving port (80), go to &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; and change &lt;code&gt;80:80&lt;/code&gt; to &lt;code&gt;&amp;lt;YOUR_SERVING_PORT&amp;gt;:80&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Updates to the above configurations require a reboot of all containers to take effect:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Switch doc engine from Elasticsearch to Infinity&lt;/h3&gt; 
&lt;p&gt;RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to &lt;a href="https://github.com/infiniflow/infinity/"&gt;Infinity&lt;/a&gt;, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Stop all running containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker/docker-compose.yml down -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;code&gt;-v&lt;/code&gt; will delete the docker container volumes, and the existing data will be cleared.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;Set &lt;code&gt;DOC_ENGINE&lt;/code&gt; in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;infinity&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start the containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Switching to Infinity on a Linux/arm64 machine is not yet officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üîß Build a Docker image&lt;/h2&gt; 
&lt;p&gt;This image is approximately 2 GB in size and relies on external LLM and embedding services.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üî® Launch service from source for development&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;pre-commit&lt;/code&gt;, or skip this step if they are already installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pipx install uv pre-commit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source code and install Python dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
uv sync --python 3.12 # install RAGFlow dependent python modules
uv run download_deps.py
pre-commit install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose -f docker/docker-compose-base.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the following line to &lt;code&gt;/etc/hosts&lt;/code&gt; to resolve all hosts specified in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;127.0.0.1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you cannot access HuggingFace, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable to use a mirror site:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export HF_ENDPOINT=https://hf-mirror.com
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If your operating system does not have jemalloc, please install it as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Ubuntu
sudo apt-get install libjemalloc-dev
# CentOS
sudo yum install jemalloc
# OpenSUSE
sudo zypper install jemalloc
# macOS
sudo brew install jemalloc
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch backend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
export PYTHONPATH=$(pwd)
bash docker/launch_backend_service.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install frontend dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd web
npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch frontend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Stop RAGFlow front-end and back-end service after development is complete:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pkill -f "ragflow_server.py|task_executor.py"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/configurations"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/release_notes"&gt;Release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/guides"&gt;User guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/developers"&gt;Developer guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/references"&gt;References&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/faq"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìú Roadmap&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;RAGFlow Roadmap 2025&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üèÑ Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/orgs/infiniflow/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôå Contributing&lt;/h2&gt; 
&lt;p&gt;RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our &lt;a href="https://ragflow.io/docs/dev/contributing"&gt;Contribution Guidelines&lt;/a&gt; first.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/adk-samples</title>
      <link>https://github.com/google/adk-samples</link>
      <description>&lt;p&gt;A collection of sample agents built with Agent Development Kit (ADK)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Development Kit (ADK) Samples&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://github.com/google/adk-docs/raw/main/docs/assets/agent-development-kit.png" alt="Agent Development Kit Logo" width="150" /&gt; 
&lt;p&gt;Welcome to the ADK Sample Agents repository! This collection provides ready-to-use agents built on top of the &lt;a href="https://google.github.io/adk-docs/"&gt;Agent Development Kit&lt;/a&gt;, designed to accelerate your development process. These agents cover a range of common use cases and complexities, from simple conversational bots to complex multi-agent workflows.&lt;/p&gt; 
&lt;h2&gt;‚ú® Getting Started&lt;/h2&gt; 
&lt;p&gt;This repo contains ADK sample agents for &lt;strong&gt;Python&lt;/strong&gt;, &lt;strong&gt;Go&lt;/strong&gt; and &lt;strong&gt;Java.&lt;/strong&gt; Navigate to the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/python/"&gt;Python&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/go/"&gt;Go&lt;/a&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/java/"&gt;Java&lt;/a&gt;&lt;/strong&gt; subfolders to see language-specific setup instructions, and learn more about the available sample agents.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The agents in this repository are built using the &lt;strong&gt;Agent Development Kit (ADK)&lt;/strong&gt;. Before you can run any of the samples, you must have the ADK installed. For instructions, please refer to the &lt;a href="https://google.github.io/adk-docs/get-started"&gt;&lt;strong&gt;ADK Installation Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To learn more, check out the &lt;a href="https://google.github.io/adk-docs/"&gt;ADK Documentation&lt;/a&gt;, and the GitHub repositories for each language:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-python"&gt;ADK Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-go"&gt;ADK Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-java"&gt;ADK Java&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üå≥ Repository Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;‚îú‚îÄ‚îÄ go
‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ agents
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ llm-auditor
‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ java
‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ agents
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ software-bug-assistant
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ time-series-forecasting
‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ python
‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ agents
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ academic-research
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ antom-payment
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ blog-writer
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ brand-search-optimization
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ camel
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ customer-service
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ data-engineering
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ data-science
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ financial-advisor
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ fomc-research
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini-fullstack
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deep-search
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google-trends-agent
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ image-scoring
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm-auditor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ machine-learning-engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ marketing-agency
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ medical-pre-authorization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ personalized-shopping
‚îÇ&amp;nbsp;&amp;nbsp; ‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ plumber-data-engineering-assistant
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ realtime-conversational-agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ safety-plugins
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ short-movie-agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ software-bug-assistant
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ travel-concierge
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ÑπÔ∏è Getting help&lt;/h2&gt; 
&lt;p&gt;If you have any questions or if you found any problems with this repository, please report through &lt;a href="https://github.com/google/adk-samples/issues"&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our &lt;a href="https://github.com/google/adk-samples/raw/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guidelines&lt;/strong&gt;&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://github.com/google/adk-samples/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Disclaimers&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. This project is not eligible for the &lt;a href="https://bughunters.google.com/open-source-security"&gt;Google Open Source Software Vulnerability Rewards Program&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is intended for demonstration purposes only. It is not intended for use in a production environment.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BeehiveInnovations/pal-mcp-server</title>
      <link>https://github.com/BeehiveInnovations/pal-mcp-server</link>
      <description>&lt;p&gt;The power of Claude Code / GeminiCLI / CodexCLI + [Gemini / OpenAI / OpenRouter / Azure / Grok / Ollama / Custom Model / All Of The Above] working as one.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PAL MCP: Many Workflows. One Context.&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Your AI's PAL ‚Äì a Provider Abstraction Layer&lt;/em&gt;&lt;br /&gt; &lt;sub&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/name-change.md"&gt;Formerly known as Zen MCP&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0d26061e-5f21-4ab1-b7d0-f883ddc2c3da"&gt;PAL in action&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#-watch-tools-in-action"&gt;Watch more examples&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;h3&gt;Your CLI + Multiple Models = Your AI Dev Team&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;Use the ü§ñ CLI you love:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt; ¬∑ &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt; ¬∑ &lt;a href="https://github.com/openai/codex"&gt;Codex CLI&lt;/a&gt; ¬∑ &lt;a href="https://qwenlm.github.io/qwen-code-docs/"&gt;Qwen Code CLI&lt;/a&gt; ¬∑ &lt;a href="https://cursor.com"&gt;Cursor&lt;/a&gt; ¬∑ &lt;em&gt;and more&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;With multiple models within a single prompt:&lt;/strong&gt;&lt;br /&gt; Gemini ¬∑ OpenAI ¬∑ Anthropic ¬∑ Grok ¬∑ Azure ¬∑ Ollama ¬∑ OpenRouter ¬∑ DIAL ¬∑ On-Device Model&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üÜï Now with CLI-to-CLI Bridge&lt;/h2&gt; 
&lt;p&gt;The new &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/clink.md"&gt;&lt;code&gt;clink&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; (CLI + Link) tool connects external AI CLIs directly into your workflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connect external CLIs&lt;/strong&gt; like &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;, &lt;a href="https://github.com/openai/codex"&gt;Codex CLI&lt;/a&gt;, and &lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt; directly into your workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CLI Subagents&lt;/strong&gt; - Launch isolated CLI instances from &lt;em&gt;within&lt;/em&gt; your current CLI! Claude Code can spawn Codex subagents, Codex can spawn Gemini CLI subagents, etc. Offload heavy tasks (code reviews, bug hunting) to fresh contexts while your main session's context window remains unpolluted. Each subagent returns only final results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context Isolation&lt;/strong&gt; - Run separate investigations without polluting your primary workspace&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Role Specialization&lt;/strong&gt; - Spawn &lt;code&gt;planner&lt;/code&gt;, &lt;code&gt;codereviewer&lt;/code&gt;, or custom role agents with specialized system prompts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full CLI Capabilities&lt;/strong&gt; - Web search, file inspection, MCP tool access, latest documentation lookups&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Continuity&lt;/strong&gt; - Sub-CLIs participate as first-class members with full conversation context between tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Codex spawns Codex subagent for isolated code review in fresh context
clink with codex codereviewer to audit auth module for security issues
# Subagent reviews in isolation, returns final report without cluttering your context as codex reads each file and walks the directory structure

# Consensus from different AI models ‚Üí Implementation handoff with full context preservation between tools
Use consensus with gpt-5 and gemini-pro to decide: dark mode or offline support next
Continue with clink gemini - implement the recommended feature
# Gemini receives full debate context and starts coding immediately
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/clink.md"&gt;Learn more about clink&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Why PAL MCP?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Why rely on one AI model when you can orchestrate them all?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A Model Context Protocol server that supercharges tools like &lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt;, &lt;a href="https://developers.openai.com/codex/cli"&gt;Codex CLI&lt;/a&gt;, and IDE clients such as &lt;a href="https://cursor.com"&gt;Cursor&lt;/a&gt; or the &lt;a href="https://marketplace.visualstudio.com/items?itemName=Anthropic.claude-vscode"&gt;Claude Dev VS Code extension&lt;/a&gt;. &lt;strong&gt;PAL MCP connects your favorite AI tool to multiple AI models&lt;/strong&gt; for enhanced code analysis, problem-solving, and collaborative development.&lt;/p&gt; 
&lt;h3&gt;True AI Collaboration with Conversation Continuity&lt;/h3&gt; 
&lt;p&gt;PAL supports &lt;strong&gt;conversation threading&lt;/strong&gt; so your CLI can &lt;strong&gt;discuss ideas with multiple AI models, exchange reasoning, get second opinions, and even run collaborative debates between models&lt;/strong&gt; to help you reach deeper insights and better solutions.&lt;/p&gt; 
&lt;p&gt;Your CLI always stays in control but gets perspectives from the best AI for each subtask. Context carries forward seamlessly across tools and models, enabling complex workflows like: code reviews with multiple models ‚Üí automated planning ‚Üí implementation ‚Üí pre-commit validation.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;You're in control.&lt;/strong&gt; Your CLI of choice orchestrates the AI team, but you decide the workflow. Craft powerful prompts that bring in Gemini Pro, GPT 5, Flash, or local offline models exactly when needed.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Reasons to Use PAL MCP&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;A typical workflow with Claude Code as an example:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Model Orchestration&lt;/strong&gt; - Claude coordinates with Gemini Pro, O3, GPT-5, and 50+ other models to get the best analysis for each task&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Context Revival Magic&lt;/strong&gt; - Even after Claude's context resets, continue conversations seamlessly by having other models "remind" Claude of the discussion&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Guided Workflows&lt;/strong&gt; - Enforces systematic investigation phases that prevent rushed analysis and ensure thorough code examination&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extended Context Windows&lt;/strong&gt; - Break Claude's limits by delegating to Gemini (1M tokens) or O3 (200K tokens) for massive codebases&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;True Conversation Continuity&lt;/strong&gt; - Full context flows across tools and models - Gemini remembers what O3 said 10 steps ago&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-Specific Strengths&lt;/strong&gt; - Extended thinking with Gemini Pro, blazing speed with Flash, strong reasoning with O3, privacy with local Ollama&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Professional Code Reviews&lt;/strong&gt; - Multi-pass analysis with severity levels, actionable feedback, and consensus from multiple AI experts&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Smart Debugging Assistant&lt;/strong&gt; - Systematic root cause analysis with hypothesis tracking and confidence levels&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automatic Model Selection&lt;/strong&gt; - Claude intelligently picks the right model for each subtask (or you can specify)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vision Capabilities&lt;/strong&gt; - Analyze screenshots, diagrams, and visual content with vision-enabled models&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Local Model Support&lt;/strong&gt; - Run Llama, Mistral, or other models locally for complete privacy and zero API costs&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bypass MCP Token Limits&lt;/strong&gt; - Automatically works around MCP's 25K limit for large prompts and responses&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;The Killer Feature:&lt;/strong&gt; When Claude's context resets, just ask to "continue with O3" - the other model's response magically revives Claude's understanding without re-ingesting documents!&lt;/p&gt; 
 &lt;h4&gt;Example: Multi-Model Code Review Workflow&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;code&gt;Perform a codereview using gemini pro and o3 and use planner to generate a detailed plan, implement the fixes and do a final precommit check by continuing from the previous codereview&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;This triggers a &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/codereview.md"&gt;&lt;code&gt;codereview&lt;/code&gt;&lt;/a&gt; workflow where Claude walks the code, looking for all kinds of issues&lt;/li&gt; 
  &lt;li&gt;After multiple passes, collects relevant code and makes note of issues along the way&lt;/li&gt; 
  &lt;li&gt;Maintains a &lt;code&gt;confidence&lt;/code&gt; level between &lt;code&gt;exploring&lt;/code&gt;, &lt;code&gt;low&lt;/code&gt;, &lt;code&gt;medium&lt;/code&gt;, &lt;code&gt;high&lt;/code&gt; and &lt;code&gt;certain&lt;/code&gt; to track how confidently it's been able to find and identify issues&lt;/li&gt; 
  &lt;li&gt;Generates a detailed list of critical -&amp;gt; low issues&lt;/li&gt; 
  &lt;li&gt;Shares the relevant files, findings, etc with &lt;strong&gt;Gemini Pro&lt;/strong&gt; to perform a deep dive for a second &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/codereview.md"&gt;&lt;code&gt;codereview&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Comes back with a response and next does the same with o3, adding to the prompt if a new discovery comes to light&lt;/li&gt; 
  &lt;li&gt;When done, Claude takes in all the feedback and combines a single list of all critical -&amp;gt; low issues, including good patterns in your code. The final list includes new findings or revisions in case Claude misunderstood or missed something crucial and one of the other models pointed this out&lt;/li&gt; 
  &lt;li&gt;It then uses the &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/planner.md"&gt;&lt;code&gt;planner&lt;/code&gt;&lt;/a&gt; workflow to break the work down into simpler steps if a major refactor is required&lt;/li&gt; 
  &lt;li&gt;Claude then performs the actual work of fixing highlighted issues&lt;/li&gt; 
  &lt;li&gt;When done, Claude returns to Gemini Pro for a &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/precommit.md"&gt;&lt;code&gt;precommit&lt;/code&gt;&lt;/a&gt; review&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;All within a single conversation thread! Gemini Pro in step 11 &lt;em&gt;knows&lt;/em&gt; what was recommended by O3 in step 7! Taking that context and review into consideration to aid with its final pre-commit review.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Think of it as Claude Code &lt;em&gt;for&lt;/em&gt; Claude Code.&lt;/strong&gt; This MCP isn't magic. It's just &lt;strong&gt;super-glue&lt;/strong&gt;.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Remember:&lt;/strong&gt; Claude stays in full control ‚Äî but &lt;strong&gt;YOU&lt;/strong&gt; call the shots. PAL is designed to have Claude engage other models only when needed ‚Äî and to follow through with meaningful back-and-forth. &lt;strong&gt;You're&lt;/strong&gt; the one who crafts the powerful prompt that makes Claude bring in Gemini, Flash, O3 ‚Äî or fly solo. You're the guide. The prompter. The puppeteer.&lt;/p&gt; 
  &lt;h4&gt;You are the AI - &lt;strong&gt;Actually Intelligent&lt;/strong&gt;.&lt;/h4&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h4&gt;Recommended AI Stack&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;For Claude Code Users&lt;/summary&gt; 
 &lt;p&gt;For best results when using &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Sonnet 4.5&lt;/strong&gt; - All agentic work and orchestration&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Gemini 3.0 Pro&lt;/strong&gt; OR &lt;strong&gt;GPT-5-Pro&lt;/strong&gt; - Deep thinking, additional code reviews, debugging and validations, pre-commit analysis&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;For Codex Users&lt;/summary&gt; 
 &lt;p&gt;For best results when using &lt;a href="https://developers.openai.com/codex/cli"&gt;Codex CLI&lt;/a&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GPT-5 Codex Medium&lt;/strong&gt; - All agentic work and orchestration&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Gemini 3.0 Pro&lt;/strong&gt; OR &lt;strong&gt;GPT-5-Pro&lt;/strong&gt; - Deep thinking, additional code reviews, debugging and validations, pre-commit analysis&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quick Start (5 minutes)&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt; Python 3.10+, Git, &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installed&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. Get API Keys&lt;/strong&gt; (choose one or more):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt;&lt;/strong&gt; - Access multiple models with one API&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://makersuite.google.com/app/apikey"&gt;Gemini&lt;/a&gt;&lt;/strong&gt; - Google's latest models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI&lt;/a&gt;&lt;/strong&gt; - O3, GPT-5 series&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://learn.microsoft.com/azure/ai-services/openai/"&gt;Azure OpenAI&lt;/a&gt;&lt;/strong&gt; - Enterprise deployments of GPT-4o, GPT-4.1, GPT-5 family&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://console.x.ai/"&gt;X.AI&lt;/a&gt;&lt;/strong&gt; - Grok models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://dialx.ai/"&gt;DIAL&lt;/a&gt;&lt;/strong&gt; - Vendor-agnostic model access&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://ollama.ai/"&gt;Ollama&lt;/a&gt;&lt;/strong&gt; - Local models (free)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Install&lt;/strong&gt; (choose one):&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option A: Clone and Automatic Setup&lt;/strong&gt; (recommended)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/BeehiveInnovations/pal-mcp-server.git
cd pal-mcp-server

# Handles everything: setup, config, API keys from system environment. 
# Auto-configures Claude Desktop, Claude Code, Gemini CLI, Codex CLI, Qwen CLI
# Enable / disable additional settings in .env
./run-server.sh  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option B: Instant Setup with &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uvx&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;// Add to ~/.claude/settings.json or .mcp.json
// Don't forget to add your API keys under env
{
  "mcpServers": {
    "pal": {
      "command": "bash",
      "args": ["-c", "for p in $(which uvx 2&amp;gt;/dev/null) $HOME/.local/bin/uvx /opt/homebrew/bin/uvx /usr/local/bin/uvx uvx; do [ -x \"$p\" ] &amp;amp;&amp;amp; exec \"$p\" --from git+https://github.com/BeehiveInnovations/pal-mcp-server.git pal-mcp-server; done; echo 'uvx not found' &amp;gt;&amp;amp;2; exit 1"],
      "env": {
        "PATH": "/usr/local/bin:/usr/bin:/bin:/opt/homebrew/bin:~/.local/bin",
        "GEMINI_API_KEY": "your-key-here",
        "DISABLED_TOOLS": "analyze,refactor,testgen,secaudit,docgen,tracer",
        "DEFAULT_MODEL": "auto"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Start Using!&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Use pal to analyze this code for security issues with gemini pro"
"Debug this error with o3 and then get flash to suggest optimizations"
"Plan the migration strategy with pal, get consensus from multiple models"
"clink with cli_name=\"gemini\" role=\"planner\" to draft a phased rollout plan"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/getting-started.md"&gt;Complete Setup Guide&lt;/a&gt;&lt;/strong&gt; with detailed installation, configuration for Gemini / Codex / Qwen, and troubleshooting üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/getting-started.md#ide-clients"&gt;Cursor &amp;amp; VS Code Setup&lt;/a&gt;&lt;/strong&gt; for IDE integration instructions üì∫ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#-watch-tools-in-action"&gt;Watch tools in action&lt;/a&gt;&lt;/strong&gt; to see real-world examples&lt;/p&gt; 
&lt;h2&gt;Provider Configuration&lt;/h2&gt; 
&lt;p&gt;PAL activates any provider that has credentials in your &lt;code&gt;.env&lt;/code&gt;. See &lt;code&gt;.env.example&lt;/code&gt; for deeper customization.&lt;/p&gt; 
&lt;h2&gt;Core Tools&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Each tool comes with its own multi-step workflow, parameters, and descriptions that consume valuable context window space even when not in use. To optimize performance, some tools are disabled by default. See &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#tool-configuration"&gt;Tool Configuration&lt;/a&gt; below to enable them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Collaboration &amp;amp; Planning&lt;/strong&gt; &lt;em&gt;(Enabled by default)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/clink.md"&gt;&lt;code&gt;clink&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Bridge requests to external AI CLIs (Gemini planner, codereviewer, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/chat.md"&gt;&lt;code&gt;chat&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Brainstorm ideas, get second opinions, validate approaches. With capable models (GPT-5 Pro, Gemini 3.0 Pro), generates complete code / implementation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/thinkdeep.md"&gt;&lt;code&gt;thinkdeep&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Extended reasoning, edge case analysis, alternative perspectives&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/planner.md"&gt;&lt;code&gt;planner&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Break down complex projects into structured, actionable plans&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/consensus.md"&gt;&lt;code&gt;consensus&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Get expert opinions from multiple AI models with stance steering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Code Analysis &amp;amp; Quality&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/debug.md"&gt;&lt;code&gt;debug&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Systematic investigation and root cause analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/precommit.md"&gt;&lt;code&gt;precommit&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Validate changes before committing, prevent regressions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/codereview.md"&gt;&lt;code&gt;codereview&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Professional reviews with severity levels and actionable feedback&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/analyze.md"&gt;&lt;code&gt;analyze&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; &lt;em&gt;(disabled by default - &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#tool-configuration"&gt;enable&lt;/a&gt;)&lt;/em&gt; - Understand architecture, patterns, dependencies across entire codebases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Development Tools&lt;/strong&gt; &lt;em&gt;(Disabled by default - &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#tool-configuration"&gt;enable&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/refactor.md"&gt;&lt;code&gt;refactor&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Intelligent code refactoring with decomposition focus&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/testgen.md"&gt;&lt;code&gt;testgen&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Comprehensive test generation with edge cases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/secaudit.md"&gt;&lt;code&gt;secaudit&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Security audits with OWASP Top 10 analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/docgen.md"&gt;&lt;code&gt;docgen&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Generate documentation with complexity analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Utilities&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/apilookup.md"&gt;&lt;code&gt;apilookup&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Forces current-year API/SDK documentation lookups in a sub-process (saves tokens within the current context window), prevents outdated training data responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/challenge.md"&gt;&lt;code&gt;challenge&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Prevent "You're absolutely right!" responses with critical analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/tracer.md"&gt;&lt;code&gt;tracer&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; &lt;em&gt;(disabled by default - &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#tool-configuration"&gt;enable&lt;/a&gt;)&lt;/em&gt; - Static analysis prompts for call-flow mapping&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b id="tool-configuration"&gt;üëâ Tool Configuration&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;Default Configuration&lt;/h3&gt; 
 &lt;p&gt;To optimize context window usage, only essential tools are enabled by default:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Enabled by default:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;chat&lt;/code&gt;, &lt;code&gt;thinkdeep&lt;/code&gt;, &lt;code&gt;planner&lt;/code&gt;, &lt;code&gt;consensus&lt;/code&gt; - Core collaboration tools&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;codereview&lt;/code&gt;, &lt;code&gt;precommit&lt;/code&gt;, &lt;code&gt;debug&lt;/code&gt; - Essential code quality tools&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;apilookup&lt;/code&gt; - Rapid API/SDK information lookup&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;challenge&lt;/code&gt; - Critical thinking utility&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Disabled by default:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;analyze&lt;/code&gt;, &lt;code&gt;refactor&lt;/code&gt;, &lt;code&gt;testgen&lt;/code&gt;, &lt;code&gt;secaudit&lt;/code&gt;, &lt;code&gt;docgen&lt;/code&gt;, &lt;code&gt;tracer&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Enabling Additional Tools&lt;/h3&gt; 
 &lt;p&gt;To enable additional tools, remove them from the &lt;code&gt;DISABLED_TOOLS&lt;/code&gt; list:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Option 1: Edit your .env file&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Default configuration (from .env.example)
DISABLED_TOOLS=analyze,refactor,testgen,secaudit,docgen,tracer

# To enable specific tools, remove them from the list
# Example: Enable analyze tool
DISABLED_TOOLS=refactor,testgen,secaudit,docgen,tracer

# To enable ALL tools
DISABLED_TOOLS=
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Option 2: Configure in MCP settings&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;// In ~/.claude/settings.json or .mcp.json
{
  "mcpServers": {
    "pal": {
      "env": {
        // Tool configuration
        "DISABLED_TOOLS": "refactor,testgen,secaudit,docgen,tracer",
        "DEFAULT_MODEL": "pro",
        "DEFAULT_THINKING_MODE_THINKDEEP": "high",
        
        // API configuration
        "GEMINI_API_KEY": "your-gemini-key",
        "OPENAI_API_KEY": "your-openai-key",
        "OPENROUTER_API_KEY": "your-openrouter-key",
        
        // Logging and performance
        "LOG_LEVEL": "INFO",
        "CONVERSATION_TIMEOUT_HOURS": "6",
        "MAX_CONVERSATION_TURNS": "50"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Option 3: Enable all tools&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;// Remove or empty the DISABLED_TOOLS to enable everything
{
  "mcpServers": {
    "pal": {
      "env": {
        "DISABLED_TOOLS": ""
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Essential tools (&lt;code&gt;version&lt;/code&gt;, &lt;code&gt;listmodels&lt;/code&gt;) cannot be disabled&lt;/li&gt; 
  &lt;li&gt;After changing tool configuration, restart your Claude session for changes to take effect&lt;/li&gt; 
  &lt;li&gt;Each tool adds to context window usage, so only enable what you need&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üì∫ Watch Tools In Action&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Chat Tool&lt;/b&gt; - Collaborative decision making and multi-turn conversations&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Picking Redis vs Memcached:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/41076cfe-dd49-4dfc-82f5-d7461b34705d"&gt;Chat Redis or Memcached_web.webm&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-turn conversation with continuation:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/37bd57ca-e8a6-42f7-b5fb-11de271e95db"&gt;Chat With Gemini_web.webm&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Consensus Tool&lt;/b&gt; - Multi-model debate and decision making&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-model consensus debate:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/76a23dd5-887a-4382-9cf0-642f5cf6219e"&gt;PAL Consensus Debate&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;PreCommit Tool&lt;/b&gt; - Comprehensive change validation&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Pre-commit validation workflow:&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://github.com/user-attachments/assets/584adfa6-d252-49b4-b5b0-0cd6e97fb2c6" width="950" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;API Lookup Tool&lt;/b&gt; - Current vs outdated API documentation&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Without PAL - outdated APIs:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/01a79dc9-ad16-4264-9ce1-76a56c3580ee"&gt;API without PAL&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;With PAL - current APIs:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5c847326-4b66-41f7-8f30-f380453dce22"&gt;API with PAL&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Challenge Tool&lt;/b&gt; - Critical thinking vs reflexive agreement&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Without PAL:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/64f3c9fb-7ca9-4876-b687-25e847edfd87" alt="without_pal@2x" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;With PAL:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/9d72f444-ba53-4ab1-83e5-250062c6ee70" alt="with_pal@2x" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;AI Orchestration&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Auto model selection&lt;/strong&gt; - Claude picks the right AI for each task&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-model workflows&lt;/strong&gt; - Chain different models in single conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversation continuity&lt;/strong&gt; - Context preserved across tools and models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/context-revival.md"&gt;Context revival&lt;/a&gt;&lt;/strong&gt; - Continue conversations even after context resets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Support&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple providers&lt;/strong&gt; - Gemini, OpenAI, Azure, X.AI, OpenRouter, DIAL, Ollama&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Latest models&lt;/strong&gt; - GPT-5, Gemini 3.0 Pro, O3, Grok-4, local Llama&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/advanced-usage.md#thinking-modes"&gt;Thinking modes&lt;/a&gt;&lt;/strong&gt; - Control reasoning depth vs cost&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vision support&lt;/strong&gt; - Analyze images, diagrams, screenshots&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Developer Experience&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Guided workflows&lt;/strong&gt; - Systematic investigation prevents rushed analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart file handling&lt;/strong&gt; - Auto-expand directories, manage token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web search integration&lt;/strong&gt; - Access current documentation and best practices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/advanced-usage.md#working-with-large-prompts"&gt;Large prompt support&lt;/a&gt;&lt;/strong&gt; - Bypass MCP's 25K token limit&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Example Workflows&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Multi-model Code Review:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Perform a codereview using gemini pro and o3, then use planner to create a fix strategy"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Üí Claude reviews code systematically ‚Üí Consults Gemini Pro ‚Üí Gets O3's perspective ‚Üí Creates unified action plan&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Collaborative Debugging:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Debug this race condition with max thinking mode, then validate the fix with precommit"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Üí Deep investigation ‚Üí Expert analysis ‚Üí Solution implementation ‚Üí Pre-commit validation&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Architecture Planning:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Plan our microservices migration, get consensus from pro and o3 on the approach"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Üí Structured planning ‚Üí Multiple expert opinions ‚Üí Consensus building ‚Üí Implementation roadmap&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/advanced-usage.md"&gt;Advanced Usage Guide&lt;/a&gt;&lt;/strong&gt; for complex workflows, model configuration, and power-user features&lt;/p&gt; 
&lt;h2&gt;Quick Links&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;üìñ Documentation&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/index.md"&gt;Docs Overview&lt;/a&gt; - High-level map of major guides&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/getting-started.md"&gt;Getting Started&lt;/a&gt; - Complete setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/"&gt;Tools Reference&lt;/a&gt; - All tools with examples&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/advanced-usage.md"&gt;Advanced Usage&lt;/a&gt; - Power user features&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/configuration.md"&gt;Configuration&lt;/a&gt; - Environment variables, restrictions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/adding_providers.md"&gt;Adding Providers&lt;/a&gt; - Provider-specific setup (OpenAI, Azure, custom gateways)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/model_ranking.md"&gt;Model Ranking Guide&lt;/a&gt; - How intelligence scores drive auto-mode suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;üîß Setup &amp;amp; Support&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/wsl-setup.md"&gt;WSL Setup&lt;/a&gt; - Windows users&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt; - Common issues&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/contributions.md"&gt;Contributing&lt;/a&gt; - Code standards, PR process&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 License - see &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Built with the power of &lt;strong&gt;Multi-Model AI&lt;/strong&gt; collaboration ü§ù&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;ctual &lt;strong&gt;I&lt;/strong&gt;ntelligence by real Humans&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://modelcontextprotocol.com"&gt;MCP (Model Context Protocol)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developers.openai.com/codex/cli"&gt;Codex CLI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ai.google.dev/"&gt;Gemini&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/azure/ai-services/openai/"&gt;Azure OpenAI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Star History&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#BeehiveInnovations/pal-mcp-server&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=BeehiveInnovations/pal-mcp-server&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-quickstarts</title>
      <link>https://github.com/anthropics/claude-quickstarts</link>
      <description>&lt;p&gt;A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Quickstarts&lt;/h1&gt; 
&lt;p&gt;Claude Quickstarts is a collection of projects designed to help developers quickly get started with building applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To use these quickstarts, you'll need an Claude API key. If you don't have one yet, you can sign up for free at &lt;a href="https://console.anthropic.com"&gt;console.anthropic.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Available Quickstarts&lt;/h2&gt; 
&lt;h3&gt;Customer Support Agent&lt;/h3&gt; 
&lt;p&gt;A customer support agent powered by Claude. This project demonstrates how to leverage Claude's natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/customer-support-agent"&gt;Go to Customer Support Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Financial Data Analyst&lt;/h3&gt; 
&lt;p&gt;A financial data analyst powered by Claude. This project demonstrates how to leverage Claude's capabilities with interactive data visualization to analyze financial data via chat.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/financial-data-analyst"&gt;Go to Financial Data Analyst Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Computer Use Demo&lt;/h3&gt; 
&lt;p&gt;An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest &lt;code&gt;computer_use_20251124&lt;/code&gt; tool version with zoom actions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/computer-use-demo"&gt;Go to Computer Use Demo Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Autonomous Coding Agent&lt;/h3&gt; 
&lt;p&gt;An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/autonomous-coding"&gt;Go to Autonomous Coding Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General Usage&lt;/h2&gt; 
&lt;p&gt;Each quickstart project comes with its own README and setup instructions. Generally, you'll follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository&lt;/li&gt; 
 &lt;li&gt;Navigate to the specific quickstart directory&lt;/li&gt; 
 &lt;li&gt;Install the required dependencies&lt;/li&gt; 
 &lt;li&gt;Set up your Claude API key as an environment variable&lt;/li&gt; 
 &lt;li&gt;Run the quickstart application&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;To deepen your understanding of working with Claude and the Claude API, check out these resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.claude.com"&gt;Claude API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/claude-cookbooks"&gt;Claude Cookbooks&lt;/a&gt; - A collection of code snippets and guides for common tasks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals"&gt;Claude API Fundamentals Course&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.&lt;/p&gt; 
&lt;h2&gt;Community and Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join our &lt;a href="https://www.anthropic.com/discord"&gt;Anthropic Discord community&lt;/a&gt; for discussions and support&lt;/li&gt; 
 &lt;li&gt;Check out the &lt;a href="https://support.anthropic.com"&gt;Anthropic support documentation&lt;/a&gt; for additional help&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>srbhr/Resume-Matcher</title>
      <link>https://github.com/srbhr/Resume-Matcher</link>
      <description>&lt;p&gt;Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.resumematcher.fyi"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/page_2.png" alt="Resume Matcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h1&gt;Resume Matcher&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://dsc.gg/resume-matcher"&gt;ùôπùöòùöíùöó ùô≥ùöíùöúùöåùöòùöõùöç&lt;/a&gt; ‚ú¶ &lt;a href="https://resumematcher.fyi"&gt;ùöÜùöéùöãùöúùöíùöùùöé&lt;/a&gt; ‚ú¶ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#how-to-install"&gt;ùô∑ùöòùö† ùöùùöò ùô∏ùöóùöúùöùùöäùöïùöï&lt;/a&gt; ‚ú¶ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#contributors"&gt;ùô≤ùöòùöóùöùùöõùöíùöãùöûùöùùöòùöõùöú&lt;/a&gt; ‚ú¶ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#support-the-development-by-donating"&gt;ùô≥ùöòùöóùöäùöùùöé&lt;/a&gt; ‚ú¶ &lt;a href="https://twitter.com/ssrbhr"&gt;ùöÉùö†ùöíùöùùöùùöéùöõ/ùöá&lt;/a&gt; ‚ú¶ &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;ùôªùöíùöóùöîùöéùöçùô∏ùöó&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Stop getting auto-rejected by ATS bots.&lt;/strong&gt; Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands.&lt;/p&gt; 
 &lt;p&gt;Hoping to make this, &lt;strong&gt;VS Code for making resumes&lt;/strong&gt;.&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/github/stars/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Stars" /&gt; &lt;img src="https://img.shields.io/github/license/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Apache 2.0" /&gt; &lt;img src="https://img.shields.io/github/forks/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Forks" /&gt; &lt;img src="https://img.shields.io/badge/Version-0.1%20Veridis%20Quo-FFF?labelColor=black&amp;amp;logo=LinkedIn&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="version" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://dsc.gg/resume-matcher"&gt;&lt;img src="https://img.shields.io/discord/1122069176962531400?labelColor=black&amp;amp;logo=discord&amp;amp;logoColor=c20a71&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://resumematcher.fyi"&gt;&lt;img src="https://img.shields.io/badge/website-Resume%20Matcher-FFF?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-Resume%20Matcher-FFF?labelColor=black&amp;amp;logo=LinkedIn&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/565" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/565" alt="srbhr%2FResume-Matcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://vercel.com/oss/program-badge.svg?sanitize=true" alt="Vercel OSS Program" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;/p&gt; 
 &lt;p&gt;This project is in active development. New features are being added continuously, and we welcome contributions from the community. There are some breaking changes on the &lt;code&gt;main&lt;/code&gt; branch. If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting started with Resume Matcher&lt;/h2&gt; 
&lt;p&gt;Resume Matcher is designed to help you optimize your resume with the aim to highlight your skills and experience in a way that resonates with potential employers.&lt;/p&gt; 
&lt;p&gt;We're actively working on improving the platform, building towards a &lt;strong&gt;VS Code for making resumes&lt;/strong&gt;, and adding new features. The best way to stay updated is to join the Discord discussion and be part of the active development community.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Join our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; community üëá &lt;a href="https://dsc.gg/resume-matcher"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_discord.png" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Follow us on &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;LinkedIn&lt;/a&gt; ‚ú® &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_linkedin.png" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚≠ê Star Resume Matcher to support the development and get updates on GitHub. &lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/star_resume_matcher.png" alt="Star Resume Matcher" /&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_features.png" alt="resume_matcher_features" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Works locally&lt;/strong&gt;: No need to upload your resume to a server. Everything runs on your machine with open source AI models by Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ATS Compatibility&lt;/strong&gt;: Get a detailed analysis of your resume's compatibility with ATS systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant Match Score&lt;/strong&gt;: Upload resume &amp;amp; job description for a quick match score and key improvement areas.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Keyword Optimizer&lt;/strong&gt;: Align your resume with job keywords and identify critical content gaps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Guided Improvements&lt;/strong&gt;: Get clear suggestions to make your resume stand out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Roadmap&lt;/h3&gt; 
&lt;p&gt;If you have any suggestions or feature requests, please feel free to open an issue on GitHub. And discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visual keyword highlighting.&lt;/li&gt; 
 &lt;li&gt;AI Canvas, which can help to craft impactful, metric-driven resume content.&lt;/li&gt; 
 &lt;li&gt;Multi-job description optimization.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/how_to_install_resumematcher.png" alt="Installation" /&gt;&lt;/p&gt; 
&lt;p&gt;Follow the instructions in the &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/SETUP.md"&gt;SETUP.md&lt;/a&gt; file to set up the project locally. The setup script will install all the necessary dependencies and configure your environment.&lt;/p&gt; 
&lt;p&gt;The project is built using:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;FastAPI for the backend.&lt;/li&gt; 
 &lt;li&gt;Next.js for the frontend.&lt;/li&gt; 
 &lt;li&gt;Ollama for local AI model serving.&lt;/li&gt; 
 &lt;li&gt;Tailwind CSS for styling.&lt;/li&gt; 
 &lt;li&gt;SQLite for the database.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Technology&lt;/th&gt; 
   &lt;th&gt;Info/Version&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.12+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Next.js&lt;/td&gt; 
   &lt;td&gt;15+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;0.6.7&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Join Us and Contribute&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/how_to_contribute.png" alt="how to contribute" /&gt;&lt;/p&gt; 
&lt;p&gt;We welcome contributions from everyone! Whether you're a developer, designer, or just someone who wants to help out. All the contributors are listed in the &lt;a href="https://resumematcher.fyi/about"&gt;about page&lt;/a&gt; on our website and on the GitHub Readme here.&lt;/p&gt; 
&lt;p&gt;Check out the roadmap if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub and discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/contributors.png" alt="Contributors" /&gt;&lt;/p&gt; 
&lt;a href="https://github.com/srbhr/Resume-Matcher/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=srbhr/Resume-Matcher" /&gt; &lt;/a&gt; 
&lt;h2&gt;Support the Development by Donating&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/supporting_resume_matcher.png" alt="donate" /&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to support the development of Resume Matcher, you can do so by donating. Your contributions will help us keep the project alive and continue adding new features.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GitHub&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sponsors/srbhr"&gt;&lt;img src="https://img.shields.io/github/sponsors/srbhr?style=for-the-badge&amp;amp;color=c20a71&amp;amp;labelColor=black&amp;amp;logo=github" alt="GitHub Sponsors" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Buy Me a Coffee&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.buymeacoffee.com/srbhr"&gt;&lt;img src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;amp;logo=buy-me-a-coffee&amp;amp;color=c20a72&amp;amp;logoColor=white" alt="BuyMeACoffee" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;kbd&gt;Star History&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;amp;theme=dark&amp;amp;type=Date" /&gt; 
  &lt;img width="100%" src="https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;amp;theme=dark&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; 
&lt;/details&gt; 
&lt;h2&gt;Resume Matcher is a part of &lt;a href="https://vercel.com/oss"&gt;Vercel Open Source Program&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://vercel.com/oss/program-badge.svg?sanitize=true" alt="Vercel OSS Program" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenHands/OpenHands</title>
      <link>https://github.com/OpenHands/OpenHands</link>
      <description>&lt;p&gt;üôå OpenHands: AI-Driven Development&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenHands/docs/main/openhands/static/img/logo.png" alt="Logo" width="200" /&gt; 
 &lt;h1 align="center" style="border-bottom: none"&gt;OpenHands: AI-Driven Development&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/OpenHands/OpenHands/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/LICENSE-MIT-20B2AA?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt; 
 &lt;a href="https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=811504672#gid=811504672"&gt;&lt;img src="https://img.shields.io/badge/SWEBench-72.8-00cc00?logoColor=FFE165&amp;amp;style=for-the-badge" alt="Benchmark Score" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://docs.openhands.dev/sdk"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2511.03690"&gt;&lt;img src="https://img.shields.io/badge/Paper-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Tech Report" /&gt;&lt;/a&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;p&gt;&lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/OpenHands/OpenHands?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;üôå&amp;nbsp;Welcome to OpenHands, a &lt;a href="https://raw.githubusercontent.com/OpenHands/OpenHands/main/COMMUNITY.md"&gt;community&lt;/a&gt; focused on AI-driven development. We‚Äôd love for you to &lt;a href="https://dub.sh/openhands"&gt;join us on Slack&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;There are a few ways to work with OpenHands:&lt;/p&gt; 
&lt;h3&gt;OpenHands Software Agent SDK&lt;/h3&gt; 
&lt;p&gt;The SDK is a composable Python library that contains all of our agentic tech. It's the engine that powers everything else below.&lt;/p&gt; 
&lt;p&gt;Define agents in code, then run them locally, or scale to 1000s of agents in the cloud.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/sdk"&gt;Check out the docs&lt;/a&gt; or &lt;a href="https://github.com/OpenHands/software-agent-sdk/"&gt;view the source&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenHands CLI&lt;/h3&gt; 
&lt;p&gt;The CLI is the easiest way to start using OpenHands. The experience will be familiar to anyone who has worked with e.g. Claude Code or Codex. You can power it with Claude, GPT, or any other LLM.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/openhands/usage/run-openhands/cli-mode"&gt;Check out the docs&lt;/a&gt; or &lt;a href="https://github.com/OpenHands/OpenHands-CLI"&gt;view the source&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenHands Local GUI&lt;/h3&gt; 
&lt;p&gt;Use the Local GUI for running agents on your laptop. It comes with a REST API and a single-page React application. The experience will be familiar to anyone who has used Devin or Jules.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.openhands.dev/openhands/usage/run-openhands/local-setup"&gt;Check out the docs&lt;/a&gt; or view the source in this repo.&lt;/p&gt; 
&lt;h3&gt;OpenHands Cloud&lt;/h3&gt; 
&lt;p&gt;This is a deployment of OpenHands GUI, running on hosted infrastructure.&lt;/p&gt; 
&lt;p&gt;You can try it with a free $10 credit by &lt;a href="https://app.all-hands.dev"&gt;signing in with your GitHub account&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;OpenHands Cloud comes with source-available features and integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integrations with Slack, Jira, and Linear&lt;/li&gt; 
 &lt;li&gt;Multi-user support&lt;/li&gt; 
 &lt;li&gt;RBAC and permissions&lt;/li&gt; 
 &lt;li&gt;Collaboration features (e.g., conversation sharing)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenHands Enterprise&lt;/h3&gt; 
&lt;p&gt;Large enterprises can work with us to self-host OpenHands Cloud in their own VPC, via Kubernetes. OpenHands Enterprise can also work with the CLI and SDK above.&lt;/p&gt; 
&lt;p&gt;OpenHands Enterprise is source-available--you can see all the source code here in the enterprise/ directory, but you'll need to purchase a license if you want to run it for more than one month.&lt;/p&gt; 
&lt;p&gt;Enterprise contracts also come with extended support and access to our research team.&lt;/p&gt; 
&lt;p&gt;Learn more at &lt;a href="https://openhands.dev/enterprise"&gt;openhands.dev/enterprise&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Everything Else&lt;/h3&gt; 
&lt;p&gt;Check out our &lt;a href="https://github.com/orgs/openhands/projects/1"&gt;Product Roadmap&lt;/a&gt;, and feel free to &lt;a href="https://github.com/OpenHands/OpenHands/issues"&gt;open up an issue&lt;/a&gt; if there's something you'd like to see!&lt;/p&gt; 
&lt;p&gt;You might also be interested in our &lt;a href="https://github.com/OpenHands/benchmarks"&gt;evaluation infrastructure&lt;/a&gt;, our &lt;a href="https://github.com/OpenHands/openhands-chrome-extension/"&gt;chrome extension&lt;/a&gt;, or our &lt;a href="https://github.com/OpenHands/ToM-SWE"&gt;Theory-of-Mind module&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;All our work is available under the MIT license, except for the &lt;code&gt;enterprise/&lt;/code&gt; directory in this repository (see the &lt;a href="https://raw.githubusercontent.com/OpenHands/OpenHands/main/enterprise/LICENSE"&gt;enterprise license&lt;/a&gt; for details). The core &lt;code&gt;openhands&lt;/code&gt; and &lt;code&gt;agent-server&lt;/code&gt; Docker images are fully MIT-licensed as well.&lt;/p&gt; 
&lt;p&gt;If you need help with anything, or just want to chat, &lt;a href="https://dub.sh/openhands"&gt;come find us on Slack&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>wasserth/TotalSegmentator</title>
      <link>https://github.com/wasserth/TotalSegmentator</link>
      <description>&lt;p&gt;Tool for robust segmentation of &gt;100 important anatomical structures in CT and MR images&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TotalSegmentator&lt;/h1&gt; 
&lt;p&gt;Tool for segmentation of most major anatomical structures in any CT or MR image. It was trained on a wide range of different CT and MR images (different scanners, institutions, protocols,...) and therefore works well on most images. A large part of the training dataset can be downloaded here: &lt;a href="https://doi.org/10.5281/zenodo.6802613"&gt;CT dataset&lt;/a&gt; (1228 subjects) and &lt;a href="https://zenodo.org/doi/10.5281/zenodo.11367004"&gt;MR dataset&lt;/a&gt; (616 subjects). You can also try the tool online at &lt;a href="https://totalsegmentator.com/"&gt;totalsegmentator.com&lt;/a&gt; or as &lt;a href="https://github.com/lassoan/SlicerTotalSegmentator"&gt;3D Slicer extension&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ANNOUNCEMENT: We created a platform where anyone can help annotate more data to further improve TotalSegmentator: &lt;a href="https://annotate.totalsegmentator.com"&gt;TotalSegmentator Annotation Platform&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ANNOUNCEMENT: We created web applications for &lt;a href="https://compute.totalsegmentator.com/volume-report/"&gt;abdominal organ volume&lt;/a&gt;, &lt;a href="https://compute.totalsegmentator.com/evans-index/"&gt;Evans index&lt;/a&gt;, and &lt;a href="https://compute.totalsegmentator.com/aorta-report/"&gt;aorta diameter&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Main classes for CT and MR: &lt;img src="https://raw.githubusercontent.com/wasserth/TotalSegmentator/master/resources/imgs/overview_classes_v2.png" alt="Alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;TotalSegmentator supports a lot more structures. See &lt;a href="https://raw.githubusercontent.com/wasserth/TotalSegmentator/master/#subtasks"&gt;subtasks&lt;/a&gt; or &lt;a href="https://backend.totalsegmentator.com/find-task/"&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;Created by the department of &lt;a href="https://www.unispital-basel.ch/en/radiologie-nuklearmedizin/forschung-radiologie-nuklearmedizin"&gt;Research and Analysis at University Hospital Basel&lt;/a&gt;. If you use it please cite our &lt;a href="https://pubs.rsna.org/doi/10.1148/ryai.230024"&gt;Radiology AI paper&lt;/a&gt; (&lt;a href="https://arxiv.org/abs/2208.05868"&gt;free preprint&lt;/a&gt;). If you use it for MR images please cite the &lt;a href="https://pubs.rsna.org/doi/10.1148/radiol.241613"&gt;TotalSegmentator MRI Radiology paper&lt;/a&gt; (&lt;a href="https://arxiv.org/abs/2405.19492"&gt;free preprint&lt;/a&gt;). Please also cite &lt;a href="https://github.com/MIC-DKFZ/nnUNet"&gt;nnUNet&lt;/a&gt; since TotalSegmentator is heavily based on it.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;TotalSegmentator works on Ubuntu, Mac, and Windows and on CPU and GPU.&lt;/p&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python &amp;gt;= 3.9&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://pytorch.org/"&gt;PyTorch&lt;/a&gt; &amp;gt;= 2.0.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optionally:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if you use the option &lt;code&gt;--preview&lt;/code&gt; you have to install xvfb (&lt;code&gt;apt-get install xvfb&lt;/code&gt;) and fury (&lt;code&gt;pip install fury&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Install Totalsegmentator&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install TotalSegmentator
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;For CT images:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;TotalSegmentator -i ct.nii.gz -o segmentations
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For MR images:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;TotalSegmentator -i mri.nii.gz -o segmentations --task total_mr
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: A Nifti file or a folder (or zip file) with all DICOM slices of one patient is allowed as input.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: If you run on CPU use the option &lt;code&gt;--fast&lt;/code&gt; or &lt;code&gt;--roi_subset&lt;/code&gt; to greatly improve runtime.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: This is not a medical device and is not intended for clinical usage. However, it is part of several FDA-approved products, where it has been certified as a component of the overall system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Subtasks&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/wasserth/TotalSegmentator/master/resources/imgs/overview_subclasses_2.png" alt="Alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;Next to the default task (&lt;code&gt;total&lt;/code&gt;) there are more subtasks with more classes. If the taskname ends with &lt;code&gt;_mr&lt;/code&gt; it works for MR images, otherwise for CT images.&lt;/p&gt; 
&lt;p&gt;Openly available for any usage (Apache-2.0 license):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;total&lt;/strong&gt;: default task containing 117 main classes (see &lt;a href="https://github.com/wasserth/TotalSegmentator#class-details"&gt;here&lt;/a&gt; for a list of classes)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;total_mr&lt;/strong&gt;: default task containing 50 main classes on MR images (see &lt;a href="https://github.com/wasserth/TotalSegmentator#class-details"&gt;here&lt;/a&gt; for a list of classes)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;lung_vessels&lt;/strong&gt;: lung_vessels (cite &lt;a href="https://www.sciencedirect.com/science/article/pii/S0720048X22001097"&gt;paper&lt;/a&gt;), lung_trachea_bronchia&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;body&lt;/strong&gt;: body, body_trunc, body_extremities, skin&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;body_mr&lt;/strong&gt;: body_trunc, body_extremities (for MR images)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vertebrae_mr&lt;/strong&gt;: sacrum, vertebrae_L5, vertebrae_L4, vertebrae_L3, vertebrae_L2, vertebrae_L1, vertebrae_T12, vertebrae_T11, vertebrae_T10, vertebrae_T9, vertebrae_T8, vertebrae_T7, vertebrae_T6, vertebrae_T5, vertebrae_T4, vertebrae_T3, vertebrae_T2, vertebrae_T1, vertebrae_C7, vertebrae_C6, vertebrae_C5, vertebrae_C4, vertebrae_C3, vertebrae_C2, vertebrae_C1 (for CT this is part of the &lt;code&gt;total&lt;/code&gt; task)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;cerebral_bleed&lt;/strong&gt;: intracerebral_hemorrhage (cite &lt;a href="https://www.mdpi.com/2077-0383/12/7/2631"&gt;paper&lt;/a&gt;)*&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;hip_implant&lt;/strong&gt;: hip_implant*&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;pleural_pericard_effusion&lt;/strong&gt;: pleural_effusion (cite &lt;a href="http://dx.doi.org/10.1097/RLI.0000000000000869"&gt;paper&lt;/a&gt;), pericardial_effusion (cite &lt;a href="http://dx.doi.org/10.3390/diagnostics12051045"&gt;paper&lt;/a&gt;)*&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;head_glands_cavities&lt;/strong&gt;: eye_left, eye_right, eye_lens_left, eye_lens_right, optic_nerve_left, optic_nerve_right, parotid_gland_left, parotid_gland_right, submandibular_gland_right, submandibular_gland_left, nasopharynx, oropharynx, hypopharynx, nasal_cavity_right, nasal_cavity_left, auditory_canal_right, auditory_canal_left, soft_palate, hard_palate (cite &lt;a href="https://www.mdpi.com/2072-6694/16/2/415"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;head_muscles&lt;/strong&gt;: masseter_right, masseter_left, temporalis_right, temporalis_left, lateral_pterygoid_right, lateral_pterygoid_left, medial_pterygoid_right, medial_pterygoid_left, tongue, digastric_right, digastric_left&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;headneck_bones_vessels&lt;/strong&gt;: larynx_air, thyroid_cartilage, hyoid, cricoid_cartilage, zygomatic_arch_right, zygomatic_arch_left, styloid_process_right, styloid_process_left, internal_carotid_artery_right, internal_carotid_artery_left, internal_jugular_vein_right, internal_jugular_vein_left (cite &lt;a href="https://www.mdpi.com/2072-6694/16/2/415"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;headneck_muscles&lt;/strong&gt;: sternocleidomastoid_right, sternocleidomastoid_left, superior_pharyngeal_constrictor, middle_pharyngeal_constrictor, inferior_pharyngeal_constrictor, trapezius_right, trapezius_left, platysma_right, platysma_left, levator_scapulae_right, levator_scapulae_left, anterior_scalene_right, anterior_scalene_left, middle_scalene_right, middle_scalene_left, posterior_scalene_right, posterior_scalene_left, sterno_thyroid_right, sterno_thyroid_left, thyrohyoid_right, thyrohyoid_left, prevertebral_right, prevertebral_left (cite &lt;a href="https://www.mdpi.com/2072-6694/16/2/415"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;liver_vessels&lt;/strong&gt;: liver_vessels, liver_tumor (cite &lt;a href="https://arxiv.org/abs/1902.09063"&gt;paper&lt;/a&gt;)*&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;oculomotor_muscles&lt;/strong&gt;: skull, eyeball_right, lateral_rectus_muscle_right, superior_oblique_muscle_right, levator_palpebrae_superioris_right, superior_rectus_muscle_right, medial_rectus_muscle_left, inferior_oblique_muscle_right, inferior_rectus_muscle_right, optic_nerve_left, eyeball_left, lateral_rectus_muscle_left, superior_oblique_muscle_left, levator_palpebrae_superioris_left, superior_rectus_muscle_left, medial_rectus_muscle_right, inferior_oblique_muscle_left, inferior_rectus_muscle_left, optic_nerve_right*&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;lung_nodules&lt;/strong&gt;: lung, lung_nodules (provided by &lt;a href="https://bluemind.co/"&gt;BLUEMIND AI&lt;/a&gt;: Fitzjalen R., Aladin M., Nanyan G.) (trained on 1353 subjects, partly from LIDC-IDRI)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;kidney_cysts&lt;/strong&gt;: kidney_cyst_left, kidney_cyst_right (strongly improved accuracy compared to kidney_cysts inside of &lt;code&gt;total&lt;/code&gt; task)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;breasts&lt;/strong&gt;: breast&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;liver_segments&lt;/strong&gt;: liver_segment_1, liver_segment_2, liver_segment_3, liver_segment_4, liver_segment_5, liver_segment_6, liver_segment_7, liver_segment_8 (Couinaud segments) (cite &lt;a href="https://doi.org/10.1007/978-3-030-32692-0_32"&gt;paper&lt;/a&gt;)*&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;liver_segments_mr&lt;/strong&gt;: liver_segment_1, liver_segment_2, liver_segment_3, liver_segment_4, liver_segment_5, liver_segment_6, liver_segment_7, liver_segment_8 (for MR images) (Couinaud segments)*&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;craniofacial_structures&lt;/strong&gt;: mandible, teeth_lower, skull, head, sinus_maxillary, sinus_frontal, teeth_upper&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;abdominal_muscles&lt;/strong&gt;: pectoralis_major_right, pectoralis_major_left, rectus_abdominis_right, rectus_abdominis_left, serratus_anterior_right, serratus_anterior_left, latissimus_dorsi_right, latissimus_dorsi_left, trapezius_right, trapezius_left, external_oblique_right, external_oblique_left, internal_oblique_right, internal_oblique_left, erector_spinae_right, erector_spinae_left, transversospinalis_right, transversospinalis_left, psoas_major_right, psoas_major_left, quadratus_lumborum_right, quadratus_lumborum_left (cite &lt;a href="https://doi.org/10.1101/2025.01.13.25319967"&gt;paper&lt;/a&gt;) (only segments within T4-L4)*&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;teeth&lt;/strong&gt;: "lower_jawbone", "upper_jawbone", "left_inferior_alveolar_canal", "right_inferior_alveolar_canal", "left_maxillary_sinus", "right_maxillary_sinus", "pharynx", "bridge", "crown", "implant", "upper_right_central_incisor_fdi11", "upper_right_lateral_incisor_fdi12", "upper_right_canine_fdi13", "upper_right_first_premolar_fdi14", "upper_right_second_premolar_fdi15", "upper_right_first_molar_fdi16", "upper_right_second_molar_fdi17", "upper_right_third_molar_fdi18", "upper_left_central_incisor_fdi21", "upper_left_lateral_incisor_fdi22", "upper_left_canine_fdi23", "upper_left_first_premolar_fdi24", "upper_left_second_premolar_fdi25", "upper_left_first_molar_fdi26", "upper_left_second_molar_fdi27", "upper_left_third_molar_fdi28", "lower_left_central_incisor_fdi31", "lower_left_lateral_incisor_fdi32", "lower_left_canine_fdi33", "lower_left_first_premolar_fdi34", "lower_left_second_premolar_fdi35", "lower_left_first_molar_fdi36", "lower_left_second_molar_fdi37", "lower_left_third_molar_fdi38", "lower_right_central_incisor_fdi41", "lower_right_lateral_incisor_fdi42", "lower_right_canine_fdi43", "lower_right_first_premolar_fdi44", "lower_right_second_premolar_fdi45", "lower_right_first_molar_fdi46", "lower_right_second_molar_fdi47", "lower_right_third_molar_fdi48", "left_mandibular_incisive_canal_fdi103", "right_mandibular_incisive_canal_fdi104", "lingual_canal", "upper_right_central_incisor_pulp_fdi111", "upper_right_lateral_incisor_pulp_fdi112", "upper_right_canine_pulp_fdi113", "upper_right_first_premolar_pulp_fdi114", "upper_right_second_premolar_pulp_fdi115", "upper_right_first_molar_pulp_fdi116", "upper_right_second_molar_pulp_fdi117", "upper_right_third_molar_pulp_fdi118", "upper_left_central_incisor_pulp_fdi121", "upper_left_lateral_incisor_pulp_fdi122", "upper_left_canine_pulp_fdi123", "upper_left_first_premolar_pulp_fdi124", "upper_left_second_premolar_pulp_fdi125", "upper_left_first_molar_pulp_fdi126", "upper_left_second_molar_pulp_fdi127", "upper_left_third_molar_pulp_fdi128", "lower_left_central_incisor_pulp_fdi131", "lower_left_lateral_incisor_pulp_fdi132", "lower_left_canine_pulp_fdi133", "lower_left_first_premolar_pulp_fdi134", "lower_left_second_premolar_pulp_fdi135", "lower_left_first_molar_pulp_fdi136", "lower_left_second_molar_pulp_fdi137", "lower_left_third_molar_pulp_fdi138", "lower_right_central_incisor_pulp_fdi141", "lower_right_lateral_incisor_pulp_fdi142", "lower_right_canine_pulp_fdi143", "lower_right_first_premolar_pulp_fdi144", "lower_right_second_premolar_pulp_fdi145", "lower_right_first_molar_pulp_fdi146", "lower_right_second_molar_pulp_fdi147", "lower_right_third_molar_pulp_fdi148" (based on the ToothFairy3 dataset, cite &lt;a href="https://openaccess.thecvf.com/content/CVPR2025/html/Bolelli_Segmenting_Maxillofacial_Structures_in_CBCT_Volumes_CVPR_2025_paper.html"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;trunk_cavities&lt;/strong&gt;: abdominal_cavity, thoracic_cavity, pericardium, mediastinum&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;*: These models are not trained on the full totalsegmentator dataset but on some small other datasets. Therefore, expect them to work less robustly.&lt;/p&gt; 
&lt;p&gt;Available with a license (free licenses available for non-commercial usage &lt;a href="https://backend.totalsegmentator.com/license-academic/"&gt;here&lt;/a&gt;. For a commercial license contact &lt;a href="mailto:jakob.wasserthal@usb.ch"&gt;jakob.wasserthal@usb.ch&lt;/a&gt;):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;heartchambers_highres&lt;/strong&gt;: myocardium, atrium_left, ventricle_left, atrium_right, ventricle_right, aorta, pulmonary_artery (trained on sub-millimeter resolution)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;appendicular_bones&lt;/strong&gt;: patella, tibia, fibula, tarsal, metatarsal, phalanges_feet, ulna, radius, carpal, metacarpal, phalanges_hand&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;appendicular_bones_mr&lt;/strong&gt;: patella, tibia, fibula, tarsal, metatarsal, phalanges_feet, ulna, radius (for MR images)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tissue_types&lt;/strong&gt;: subcutaneous_fat, torso_fat, skeletal_muscle&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tissue_types_mr&lt;/strong&gt;: subcutaneous_fat, torso_fat, skeletal_muscle (for MR images)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tissue_4_types&lt;/strong&gt;: subcutaneous_fat, torso_fat, skeletal_muscle, intermuscular_fat (in contrast to &lt;code&gt;tissue_types&lt;/code&gt; skeletal_muscle is split into two classes: muscle and fat)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;brain_structures&lt;/strong&gt;: brainstem, subarachnoid_space, venous_sinuses, septum_pellucidum, cerebellum, caudate_nucleus, lentiform_nucleus, insular_cortex, internal_capsule, ventricle, central_sulcus, frontal_lobe, parietal_lobe, occipital_lobe, temporal_lobe, thalamus (NOTE: this is for CT) (cite &lt;a href="https://doi.org/10.1148/ryai.2020190183"&gt;paper&lt;/a&gt; as our model is partly based on this)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vertebrae_body&lt;/strong&gt;: vertebral body of all vertebrae (without the vertebral arch), intervertebral_discs (for MR this is part of the &lt;code&gt;total_mr&lt;/code&gt; task)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;face&lt;/strong&gt;: face_region (for anonymization)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;face_mr&lt;/strong&gt;: face_region (for anonymization)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;thigh_shoulder_muscles&lt;/strong&gt;: quadriceps_femoris_left, quadriceps_femoris_right, thigh_medial_compartment_left, thigh_medial_compartment_right, thigh_posterior_compartment_left, thigh_posterior_compartment_right, sartorius_left, sartorius_right, deltoid, supraspinatus, infraspinatus, subscapularis, coracobrachial, trapezius, pectoralis_minor, serratus_anterior, teres_major, triceps_brachii&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;thigh_shoulder_muscles_mr&lt;/strong&gt;: quadriceps_femoris_left, quadriceps_femoris_right, thigh_medial_compartment_left, thigh_medial_compartment_right, thigh_posterior_compartment_left, thigh_posterior_compartment_right, sartorius_left, sartorius_right, deltoid, supraspinatus, infraspinatus, subscapularis, coracobrachial, trapezius, pectoralis_minor, serratus_anterior, teres_major, triceps_brachii (for MR images)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;coronary_arteries&lt;/strong&gt;: coronary_arteries (also works on non-contrast images)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;TotalSegmentator -i ct.nii.gz -o segmentations -ta &amp;lt;task_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Confused by all the structures and tasks? Check &lt;a href="https://backend.totalsegmentator.com/find-task/"&gt;this&lt;/a&gt; to search through available structures and tasks.&lt;/p&gt; 
&lt;p&gt;The mapping from label ID to class name can be found &lt;a href="https://github.com/wasserth/TotalSegmentator/raw/master/totalsegmentator/map_to_binary.py"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you have a nnU-Net model for some structures not supported yet, you can contribute it. This will enable all TotalSegmentator users to easily use it and at the same time increase the reach of your work by more people citing your paper. Contact &lt;a href="mailto:jakob.wasserthal@usb.ch"&gt;jakob.wasserthal@usb.ch&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you to &lt;a href="https://www.ingedata.ai/"&gt;INGEDATA&lt;/a&gt; for providing a team of radiologists to support some of the data annotations.&lt;/p&gt; 
&lt;h3&gt;Advanced settings&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--device&lt;/code&gt;: Choose &lt;code&gt;cpu&lt;/code&gt; or &lt;code&gt;gpu&lt;/code&gt; or &lt;code&gt;gpu:X (e.g., gpu:1 -&amp;gt; cuda:1)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--fast&lt;/code&gt;: For faster runtime and less memory requirements use this option. It will run a lower resolution model (3mm instead of 1.5mm).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--roi_subset&lt;/code&gt;: Takes a space-separated list of class names (e.g. &lt;code&gt;spleen colon brain&lt;/code&gt;) and only predicts those classes. Saves a lot of runtime and memory. Might be less accurate especially for small classes (e.g. prostate).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--robust_crop&lt;/code&gt;: For some tasks and for roi_subset a 6mm low resolution model is used to crop to the region of interest. Sometimes this model is incorrect, which leads to artifacts like segmentations being cut off. robust_crop will use a better but slower 3mm model instead.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--preview&lt;/code&gt;: This will generate a 3D rendering of all classes, giving you a quick overview if the segmentation worked and where it failed (see &lt;code&gt;preview.png&lt;/code&gt; in output directory).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--ml&lt;/code&gt;: This will save one nifti file containing all labels instead of one file for each class. Saves runtime during saving of nifti files. (see &lt;a href="https://github.com/wasserth/TotalSegmentator#class-details"&gt;here&lt;/a&gt; for index to class name mapping).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--statistics&lt;/code&gt;: This will generate a file &lt;code&gt;statistics.json&lt;/code&gt; with volume (in mm¬≥) and mean intensity of each class.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--radiomics&lt;/code&gt;: This will generate a file &lt;code&gt;statistics_radiomics.json&lt;/code&gt; with the radiomics features of each class. You have to install pyradiomics to use this (&lt;code&gt;pip install pyradiomics&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--output_type&lt;/code&gt;: This will output the segmentation as DICOM. Supported are &lt;code&gt;dicom_seg&lt;/code&gt; requires (&lt;code&gt;pip install highdicom&lt;/code&gt;) and &lt;code&gt;dicom_rtstruct&lt;/code&gt; requires (&lt;code&gt;pip install rt_utils&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Other commands&lt;/h3&gt; 
&lt;p&gt;If you want to know which contrast phase a CT image is you can use the following command (requires &lt;code&gt;pip install xgboost&lt;/code&gt;). More details can be found &lt;a href="https://raw.githubusercontent.com/wasserth/TotalSegmentator/master/resources/contrast_phase_prediction.md"&gt;here&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;totalseg_get_phase -i ct.nii.gz -o contrast_phase.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to know which modality (CT or MR) an image is you can use the following command (requires &lt;code&gt;pip install xgboost&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;totalseg_get_modality -i image.nii.gz -o modality.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to combine some subclasses (e.g. lung lobes) into one binary mask (e.g. entire lung) you can use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;totalseg_combine_masks -i totalsegmentator_output_dir -o combined_mask.nii.gz -m lungcomm 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to calculate the &lt;a href="https://radiopaedia.org/articles/evans-index-2"&gt;Evans index&lt;/a&gt; you can use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;totalseg_evans_index -i ct_skull.nii.gz -o evans_index.json -p evans_index.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Normally weights are automatically downloaded when running TotalSegmentator. If you want to download the weights with an extra command (e.g. when building a docker container) use this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;totalseg_download_weights -t &amp;lt;task_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will download them to &lt;code&gt;~/.totalsegmentator/nnunet/results&lt;/code&gt;. You can change this path by doing &lt;code&gt;export TOTALSEG_HOME_DIR=/new/path/.totalsegmentator&lt;/code&gt;. If your machine has no internet, then download on another machine with internet and copy &lt;code&gt;~/.totalsegmentator&lt;/code&gt; to the machine without internet.&lt;/p&gt; 
&lt;p&gt;After acquiring a license number for the non-open tasks you can set it with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;totalseg_set_license -l aca_12345678910
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can output the softmax probabilities. This will give you a &lt;code&gt;.npz&lt;/code&gt; file you can load with numpy. The geometry might not be identical to your input image. There will also be a &lt;code&gt;.pkl&lt;/code&gt; output file with geometry information. This does not work well for the &lt;code&gt;total&lt;/code&gt; task since this is based on multiple models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;TotalSegmentator -i ct.nii.gz -o seg -ta lung_nodules --save_probabilities probs.npz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you do not have internet access on the machine you want to run TotalSegmentator on:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install TotalSegmentator [and set up the license] on a machine with internet.&lt;/li&gt; 
 &lt;li&gt;Run TotalSegmentator for one subject on this machine. This will download the weights and save them to &lt;code&gt;~/.totalsegmentator&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Copy the folder &lt;code&gt;~/.totalsegmentator&lt;/code&gt; from this machine to the machine without internet.&lt;/li&gt; 
 &lt;li&gt;TotalSegmentator should now work also on the machine without internet.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Web applications&lt;/h3&gt; 
&lt;p&gt;We provide the following web applications to easily process your images:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://totalsegmentator.com/"&gt;TotalSegmentator&lt;/a&gt;: Run totalsegmentator on your own images via a simple web interface.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://annotate.totalsegmentator.com/"&gt;TotalSegmentator Annotation Platform&lt;/a&gt;: Help annotate more data to further improve TotalSegmentator.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://compute.totalsegmentator.com/volume-report/"&gt;Volume Report&lt;/a&gt;: Get the volume of abdominal organs + tissue und bone density. Also show percentile in population.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://compute.totalsegmentator.com/evans-index/"&gt;Evans Index&lt;/a&gt;: Compute the Evans index.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://compute.totalsegmentator.com/aorta-report/"&gt;Aorta Report&lt;/a&gt;: Analyse the diameter along the aorta.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run via docker&lt;/h3&gt; 
&lt;p&gt;We also provide a docker container which can be used the following way&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --gpus 'device=0' --shm-size=16G -v /absolute/path/to/my/data/directory:/tmp wasserth/totalsegmentator:2.11.0 TotalSegmentator -i /tmp/ct.nii.gz -o /tmp/segmentations
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Resource Requirements&lt;/h3&gt; 
&lt;p&gt;Totalsegmentator has the following runtime and memory requirements (using an Nvidia RTX 3090 GPU): (1.5mm is the normal model and 3mm is the &lt;code&gt;--fast&lt;/code&gt; model. With v2 the runtimes have increased a bit since we added more classes.)&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/wasserth/TotalSegmentator/master/resources/imgs/runtime_table.png" alt="Alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;If you want to reduce memory consumption you can use the following options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--fast&lt;/code&gt;: This will use a lower-resolution model&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--body_seg&lt;/code&gt;: This will crop the image to the body region before processing it&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--roi_subset &amp;lt;list of classes&amp;gt;&lt;/code&gt;: This will only predict a subset of classes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--force_split&lt;/code&gt;: This will split the image into 3 parts and process them one after another. (Do not use this for small images. Splitting these into even smaller images will result in a field of view which is too small.)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--nr_thr_saving 1&lt;/code&gt;: Saving big images with several threads will take a lot of memory&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;You can run totalsegmentator via Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import nibabel as nib
from totalsegmentator.python_api import totalsegmentator

if __name__ == "__main__":
    # option 1: provide input and output as file paths
    totalsegmentator(input_path, output_path)
    
    # option 2: provide input and output as nifti image objects
    input_img = nib.load(input_path)
    output_img = totalsegmentator(input_img)
    nib.save(output_img, output_path)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can see all available arguments &lt;a href="https://github.com/wasserth/TotalSegmentator/raw/master/totalsegmentator/python_api.py"&gt;here&lt;/a&gt;. Running from within the main environment should avoid some multiprocessing issues.&lt;/p&gt; 
&lt;p&gt;The segmentation image contains the names of the classes in the extended header. If you want to load this additional header information you can use the following code (requires &lt;code&gt;pip install xmltodict&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from totalsegmentator.nifti_ext_header import load_multilabel_nifti

segmentation_nifti_img, label_map_dict = load_multilabel_nifti(image_path)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install latest master branch (contains latest bug fixes)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/wasserth/TotalSegmentator.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Train/validation/test split&lt;/h3&gt; 
&lt;p&gt;The exact split of the dataset can be found in the file &lt;code&gt;meta.csv&lt;/code&gt; inside of the &lt;a href="https://doi.org/10.5281/zenodo.6802613"&gt;dataset&lt;/a&gt;. This was used for the validation in our paper. The exact numbers of the results for the high-resolution model (1.5mm) can be found &lt;a href="https://raw.githubusercontent.com/wasserth/TotalSegmentator/master/resources/results_all_classes_v1.json"&gt;here&lt;/a&gt;. The paper shows these numbers in the supplementary materials Figure 11.&lt;/p&gt; 
&lt;h3&gt;Retrain model and run evaluation&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/wasserth/TotalSegmentator/master/resources/train_nnunet.md"&gt;here&lt;/a&gt; for more info on how to train a nnU-Net yourself on the TotalSegmentator dataset, how to split the data into train/validation/test set as in our paper, and how to run the same evaluation as in our paper.&lt;/p&gt; 
&lt;h3&gt;Typical problems&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;ITK loading Error&lt;/strong&gt; When you get the following error message&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;ITK ERROR: ITK only supports orthonormal direction cosines. No orthonormal definition was found!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;you should do&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install SimpleITK==2.0.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively you can try&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;fslorient -copysform2qform input_file
[fslreorient2std input_file output_file]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or use &lt;a href="https://github.com/MIC-DKFZ/nnDetection/issues/24#issuecomment-2627684467"&gt;this python command&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Bad segmentations&lt;/strong&gt; When you get bad segmentation results check the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;does your input image contain the original HU values or are the intensity values rescaled to a different range?&lt;/li&gt; 
 &lt;li&gt;is the patient normally positioned in the image? (In axial view is the spine at the bottom of the image? In the coronal view is the head at the top of the image?)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Running v1&lt;/h3&gt; 
&lt;p&gt;If you want to keep on using TotalSegmentator v1 (e.g. because you do not want to change your pipeline) you can install it with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install TotalSegmentator==1.5.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The documentation for v1 can be found &lt;a href="https://github.com/wasserth/TotalSegmentator/tree/v1.5.7"&gt;here&lt;/a&gt;. Bugfixes for v1 are developed in the branch &lt;code&gt;v1_bugfixes&lt;/code&gt;. Our Radiology AI publication refers to TotalSegmentator v1.&lt;/p&gt; 
&lt;h3&gt;Other&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;TotalSegmentator sends anonymous usage statistics to help us improve it further. You can deactivate it by setting &lt;code&gt;send_usage_stats&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; in &lt;code&gt;~/.totalsegmentator/config.json&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;At &lt;a href="https://raw.githubusercontent.com/wasserth/TotalSegmentator/master/resources/improvements_in_v2.md"&gt;changes and improvements&lt;/a&gt; you can see an overview of differences between v1 and v2.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Reference&lt;/h3&gt; 
&lt;p&gt;For more details see our &lt;a href="https://pubs.rsna.org/doi/10.1148/ryai.230024"&gt;Radiology AI paper&lt;/a&gt; (&lt;a href="https://arxiv.org/abs/2208.05868"&gt;freely available preprint&lt;/a&gt;). If you use this tool please cite it as follows&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;Wasserthal, J., Breit, H.-C., Meyer, M.T., Pradella, M., Hinck, D., Sauter, A.W., Heye, T., Boll, D., Cyriac, J., Yang, S., Bach, M., Segeroth, M., 2023. TotalSegmentator: Robust Segmentation of 104 Anatomic Structures in CT Images. Radiology: Artificial Intelligence. https://doi.org/10.1148/ryai.230024
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please also cite &lt;a href="https://github.com/MIC-DKFZ/nnUNet"&gt;nnUNet&lt;/a&gt; since TotalSegmentator is heavily based on it. Moreover, we would really appreciate it if you let us know what you are using this tool for. You can also tell us what classes we should add in future releases. You can do so &lt;a href="https://github.com/wasserth/TotalSegmentator/issues/1"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Class details&lt;/h3&gt; 
&lt;p&gt;The following table shows a list of all classes for task &lt;code&gt;total&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;TA2 is a standardized way to name anatomy. Mostly the TotalSegmentator names follow this standard. For some classes they differ which you can see in the table below.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/wasserth/TotalSegmentator/master/totalsegmentator/resources/totalsegmentator_snomed_mapping.csv"&gt;Here&lt;/a&gt; you can find a mapping of the TotalSegmentator classes to SNOMED-CT codes.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="right"&gt;Index&lt;/th&gt; 
   &lt;th&gt;TotalSegmentator name&lt;/th&gt; 
   &lt;th&gt;TA2 name&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;1&lt;/td&gt; 
   &lt;td&gt;spleen&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;2&lt;/td&gt; 
   &lt;td&gt;kidney_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;3&lt;/td&gt; 
   &lt;td&gt;kidney_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;4&lt;/td&gt; 
   &lt;td&gt;gallbladder&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;5&lt;/td&gt; 
   &lt;td&gt;liver&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;6&lt;/td&gt; 
   &lt;td&gt;stomach&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;7&lt;/td&gt; 
   &lt;td&gt;pancreas&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;8&lt;/td&gt; 
   &lt;td&gt;adrenal_gland_right&lt;/td&gt; 
   &lt;td&gt;suprarenal gland&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;9&lt;/td&gt; 
   &lt;td&gt;adrenal_gland_left&lt;/td&gt; 
   &lt;td&gt;suprarenal gland&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;10&lt;/td&gt; 
   &lt;td&gt;lung_upper_lobe_left&lt;/td&gt; 
   &lt;td&gt;superior lobe of left lung&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;11&lt;/td&gt; 
   &lt;td&gt;lung_lower_lobe_left&lt;/td&gt; 
   &lt;td&gt;inferior lobe of left lung&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;12&lt;/td&gt; 
   &lt;td&gt;lung_upper_lobe_right&lt;/td&gt; 
   &lt;td&gt;superior lobe of right lung&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;13&lt;/td&gt; 
   &lt;td&gt;lung_middle_lobe_right&lt;/td&gt; 
   &lt;td&gt;middle lobe of right lung&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;14&lt;/td&gt; 
   &lt;td&gt;lung_lower_lobe_right&lt;/td&gt; 
   &lt;td&gt;inferior lobe of right lung&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;15&lt;/td&gt; 
   &lt;td&gt;esophagus&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;16&lt;/td&gt; 
   &lt;td&gt;trachea&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;17&lt;/td&gt; 
   &lt;td&gt;thyroid_gland&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;18&lt;/td&gt; 
   &lt;td&gt;small_bowel&lt;/td&gt; 
   &lt;td&gt;small intestine&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;19&lt;/td&gt; 
   &lt;td&gt;duodenum&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;20&lt;/td&gt; 
   &lt;td&gt;colon&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;21&lt;/td&gt; 
   &lt;td&gt;urinary_bladder&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;22&lt;/td&gt; 
   &lt;td&gt;prostate&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;23&lt;/td&gt; 
   &lt;td&gt;kidney_cyst_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;24&lt;/td&gt; 
   &lt;td&gt;kidney_cyst_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;25&lt;/td&gt; 
   &lt;td&gt;sacrum&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;26&lt;/td&gt; 
   &lt;td&gt;vertebrae_S1&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;27&lt;/td&gt; 
   &lt;td&gt;vertebrae_L5&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;28&lt;/td&gt; 
   &lt;td&gt;vertebrae_L4&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;29&lt;/td&gt; 
   &lt;td&gt;vertebrae_L3&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;30&lt;/td&gt; 
   &lt;td&gt;vertebrae_L2&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;31&lt;/td&gt; 
   &lt;td&gt;vertebrae_L1&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;32&lt;/td&gt; 
   &lt;td&gt;vertebrae_T12&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;33&lt;/td&gt; 
   &lt;td&gt;vertebrae_T11&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;34&lt;/td&gt; 
   &lt;td&gt;vertebrae_T10&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;35&lt;/td&gt; 
   &lt;td&gt;vertebrae_T9&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;36&lt;/td&gt; 
   &lt;td&gt;vertebrae_T8&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;37&lt;/td&gt; 
   &lt;td&gt;vertebrae_T7&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;38&lt;/td&gt; 
   &lt;td&gt;vertebrae_T6&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;39&lt;/td&gt; 
   &lt;td&gt;vertebrae_T5&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;40&lt;/td&gt; 
   &lt;td&gt;vertebrae_T4&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;41&lt;/td&gt; 
   &lt;td&gt;vertebrae_T3&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;42&lt;/td&gt; 
   &lt;td&gt;vertebrae_T2&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;43&lt;/td&gt; 
   &lt;td&gt;vertebrae_T1&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;44&lt;/td&gt; 
   &lt;td&gt;vertebrae_C7&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;45&lt;/td&gt; 
   &lt;td&gt;vertebrae_C6&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;46&lt;/td&gt; 
   &lt;td&gt;vertebrae_C5&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;47&lt;/td&gt; 
   &lt;td&gt;vertebrae_C4&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;48&lt;/td&gt; 
   &lt;td&gt;vertebrae_C3&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;49&lt;/td&gt; 
   &lt;td&gt;vertebrae_C2&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;50&lt;/td&gt; 
   &lt;td&gt;vertebrae_C1&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;51&lt;/td&gt; 
   &lt;td&gt;heart&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;52&lt;/td&gt; 
   &lt;td&gt;aorta&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;53&lt;/td&gt; 
   &lt;td&gt;pulmonary_vein&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;54&lt;/td&gt; 
   &lt;td&gt;brachiocephalic_trunk&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;55&lt;/td&gt; 
   &lt;td&gt;subclavian_artery_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;56&lt;/td&gt; 
   &lt;td&gt;subclavian_artery_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;57&lt;/td&gt; 
   &lt;td&gt;common_carotid_artery_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;58&lt;/td&gt; 
   &lt;td&gt;common_carotid_artery_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;59&lt;/td&gt; 
   &lt;td&gt;brachiocephalic_vein_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;60&lt;/td&gt; 
   &lt;td&gt;brachiocephalic_vein_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;61&lt;/td&gt; 
   &lt;td&gt;atrial_appendage_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;62&lt;/td&gt; 
   &lt;td&gt;superior_vena_cava&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;63&lt;/td&gt; 
   &lt;td&gt;inferior_vena_cava&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;64&lt;/td&gt; 
   &lt;td&gt;portal_vein_and_splenic_vein&lt;/td&gt; 
   &lt;td&gt;hepatic portal vein&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;65&lt;/td&gt; 
   &lt;td&gt;iliac_artery_left&lt;/td&gt; 
   &lt;td&gt;common iliac artery&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;66&lt;/td&gt; 
   &lt;td&gt;iliac_artery_right&lt;/td&gt; 
   &lt;td&gt;common iliac artery&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;67&lt;/td&gt; 
   &lt;td&gt;iliac_vena_left&lt;/td&gt; 
   &lt;td&gt;common iliac vein&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;68&lt;/td&gt; 
   &lt;td&gt;iliac_vena_right&lt;/td&gt; 
   &lt;td&gt;common iliac vein&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;69&lt;/td&gt; 
   &lt;td&gt;humerus_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;70&lt;/td&gt; 
   &lt;td&gt;humerus_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;71&lt;/td&gt; 
   &lt;td&gt;scapula_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;72&lt;/td&gt; 
   &lt;td&gt;scapula_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;73&lt;/td&gt; 
   &lt;td&gt;clavicula_left&lt;/td&gt; 
   &lt;td&gt;clavicle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;74&lt;/td&gt; 
   &lt;td&gt;clavicula_right&lt;/td&gt; 
   &lt;td&gt;clavicle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;75&lt;/td&gt; 
   &lt;td&gt;femur_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;76&lt;/td&gt; 
   &lt;td&gt;femur_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;77&lt;/td&gt; 
   &lt;td&gt;hip_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;78&lt;/td&gt; 
   &lt;td&gt;hip_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;79&lt;/td&gt; 
   &lt;td&gt;spinal_cord&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;80&lt;/td&gt; 
   &lt;td&gt;gluteus_maximus_left&lt;/td&gt; 
   &lt;td&gt;gluteus maximus muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;81&lt;/td&gt; 
   &lt;td&gt;gluteus_maximus_right&lt;/td&gt; 
   &lt;td&gt;gluteus maximus muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;82&lt;/td&gt; 
   &lt;td&gt;gluteus_medius_left&lt;/td&gt; 
   &lt;td&gt;gluteus medius muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;83&lt;/td&gt; 
   &lt;td&gt;gluteus_medius_right&lt;/td&gt; 
   &lt;td&gt;gluteus medius muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;84&lt;/td&gt; 
   &lt;td&gt;gluteus_minimus_left&lt;/td&gt; 
   &lt;td&gt;gluteus minimus muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;85&lt;/td&gt; 
   &lt;td&gt;gluteus_minimus_right&lt;/td&gt; 
   &lt;td&gt;gluteus minimus muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;86&lt;/td&gt; 
   &lt;td&gt;autochthon_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;87&lt;/td&gt; 
   &lt;td&gt;autochthon_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;88&lt;/td&gt; 
   &lt;td&gt;iliopsoas_left&lt;/td&gt; 
   &lt;td&gt;iliopsoas muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;89&lt;/td&gt; 
   &lt;td&gt;iliopsoas_right&lt;/td&gt; 
   &lt;td&gt;iliopsoas muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;90&lt;/td&gt; 
   &lt;td&gt;brain&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;91&lt;/td&gt; 
   &lt;td&gt;skull&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;92&lt;/td&gt; 
   &lt;td&gt;rib_left_1&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;93&lt;/td&gt; 
   &lt;td&gt;rib_left_2&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;94&lt;/td&gt; 
   &lt;td&gt;rib_left_3&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;95&lt;/td&gt; 
   &lt;td&gt;rib_left_4&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;96&lt;/td&gt; 
   &lt;td&gt;rib_left_5&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;97&lt;/td&gt; 
   &lt;td&gt;rib_left_6&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;98&lt;/td&gt; 
   &lt;td&gt;rib_left_7&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;99&lt;/td&gt; 
   &lt;td&gt;rib_left_8&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;100&lt;/td&gt; 
   &lt;td&gt;rib_left_9&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;101&lt;/td&gt; 
   &lt;td&gt;rib_left_10&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;102&lt;/td&gt; 
   &lt;td&gt;rib_left_11&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;103&lt;/td&gt; 
   &lt;td&gt;rib_left_12&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;104&lt;/td&gt; 
   &lt;td&gt;rib_right_1&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;105&lt;/td&gt; 
   &lt;td&gt;rib_right_2&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;106&lt;/td&gt; 
   &lt;td&gt;rib_right_3&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;107&lt;/td&gt; 
   &lt;td&gt;rib_right_4&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;108&lt;/td&gt; 
   &lt;td&gt;rib_right_5&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;109&lt;/td&gt; 
   &lt;td&gt;rib_right_6&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;110&lt;/td&gt; 
   &lt;td&gt;rib_right_7&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;111&lt;/td&gt; 
   &lt;td&gt;rib_right_8&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;112&lt;/td&gt; 
   &lt;td&gt;rib_right_9&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;113&lt;/td&gt; 
   &lt;td&gt;rib_right_10&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;114&lt;/td&gt; 
   &lt;td&gt;rib_right_11&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;115&lt;/td&gt; 
   &lt;td&gt;rib_right_12&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;116&lt;/td&gt; 
   &lt;td&gt;sternum&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;117&lt;/td&gt; 
   &lt;td&gt;costal_cartilages&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Class map for task &lt;code&gt;total_mr&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="right"&gt;Index&lt;/th&gt; 
   &lt;th&gt;TotalSegmentator name&lt;/th&gt; 
   &lt;th&gt;TA2 name&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;1&lt;/td&gt; 
   &lt;td&gt;spleen&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;2&lt;/td&gt; 
   &lt;td&gt;kidney_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;3&lt;/td&gt; 
   &lt;td&gt;kidney_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;4&lt;/td&gt; 
   &lt;td&gt;gallbladder&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;5&lt;/td&gt; 
   &lt;td&gt;liver&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;6&lt;/td&gt; 
   &lt;td&gt;stomach&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;7&lt;/td&gt; 
   &lt;td&gt;pancreas&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;8&lt;/td&gt; 
   &lt;td&gt;adrenal_gland_right&lt;/td&gt; 
   &lt;td&gt;suprarenal gland&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;9&lt;/td&gt; 
   &lt;td&gt;adrenal_gland_left&lt;/td&gt; 
   &lt;td&gt;suprarenal gland&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;10&lt;/td&gt; 
   &lt;td&gt;lung_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;11&lt;/td&gt; 
   &lt;td&gt;lung_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;12&lt;/td&gt; 
   &lt;td&gt;esophagus&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;13&lt;/td&gt; 
   &lt;td&gt;small_bowel&lt;/td&gt; 
   &lt;td&gt;small intestine&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;14&lt;/td&gt; 
   &lt;td&gt;duodenum&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;15&lt;/td&gt; 
   &lt;td&gt;colon&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;16&lt;/td&gt; 
   &lt;td&gt;urinary_bladder&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;17&lt;/td&gt; 
   &lt;td&gt;prostate&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;18&lt;/td&gt; 
   &lt;td&gt;sacrum&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;19&lt;/td&gt; 
   &lt;td&gt;vertebrae&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;20&lt;/td&gt; 
   &lt;td&gt;intervertebral_discs&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;21&lt;/td&gt; 
   &lt;td&gt;spinal_cord&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;22&lt;/td&gt; 
   &lt;td&gt;heart&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;23&lt;/td&gt; 
   &lt;td&gt;aorta&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;24&lt;/td&gt; 
   &lt;td&gt;inferior_vena_cava&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;25&lt;/td&gt; 
   &lt;td&gt;portal_vein_and_splenic_vein&lt;/td&gt; 
   &lt;td&gt;hepatic portal vein&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;26&lt;/td&gt; 
   &lt;td&gt;iliac_artery_left&lt;/td&gt; 
   &lt;td&gt;common iliac artery&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;27&lt;/td&gt; 
   &lt;td&gt;iliac_artery_right&lt;/td&gt; 
   &lt;td&gt;common iliac artery&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;28&lt;/td&gt; 
   &lt;td&gt;iliac_vena_left&lt;/td&gt; 
   &lt;td&gt;common iliac vein&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;29&lt;/td&gt; 
   &lt;td&gt;iliac_vena_right&lt;/td&gt; 
   &lt;td&gt;common iliac vein&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;30&lt;/td&gt; 
   &lt;td&gt;humerus_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;31&lt;/td&gt; 
   &lt;td&gt;humerus_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;32&lt;/td&gt; 
   &lt;td&gt;scapula_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;33&lt;/td&gt; 
   &lt;td&gt;scapula_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;34&lt;/td&gt; 
   &lt;td&gt;clavicula_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;35&lt;/td&gt; 
   &lt;td&gt;clavicula_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;36&lt;/td&gt; 
   &lt;td&gt;femur_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;37&lt;/td&gt; 
   &lt;td&gt;femur_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;38&lt;/td&gt; 
   &lt;td&gt;hip_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;39&lt;/td&gt; 
   &lt;td&gt;hip_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;40&lt;/td&gt; 
   &lt;td&gt;gluteus_maximus_left&lt;/td&gt; 
   &lt;td&gt;gluteus maximus muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;41&lt;/td&gt; 
   &lt;td&gt;gluteus_maximus_right&lt;/td&gt; 
   &lt;td&gt;gluteus maximus muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;42&lt;/td&gt; 
   &lt;td&gt;gluteus_medius_left&lt;/td&gt; 
   &lt;td&gt;gluteus medius muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;43&lt;/td&gt; 
   &lt;td&gt;gluteus_medius_right&lt;/td&gt; 
   &lt;td&gt;gluteus medius muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;44&lt;/td&gt; 
   &lt;td&gt;gluteus_minimus_left&lt;/td&gt; 
   &lt;td&gt;gluteus minimus muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;45&lt;/td&gt; 
   &lt;td&gt;gluteus_minimus_right&lt;/td&gt; 
   &lt;td&gt;gluteus minimus muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;46&lt;/td&gt; 
   &lt;td&gt;autochthon_left&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;47&lt;/td&gt; 
   &lt;td&gt;autochthon_right&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;48&lt;/td&gt; 
   &lt;td&gt;iliopsoas_left&lt;/td&gt; 
   &lt;td&gt;iliopsoas muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;49&lt;/td&gt; 
   &lt;td&gt;iliopsoas_right&lt;/td&gt; 
   &lt;td&gt;iliopsoas muscle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="right"&gt;50&lt;/td&gt; 
   &lt;td&gt;brain&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>zhu-xlab/GlobalBuildingAtlas</title>
      <link>https://github.com/zhu-xlab/GlobalBuildingAtlas</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GlobalBuildingAtlas&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;In this project, we provide the level of detail 1 (LoD1) data of buildings across the globe.&lt;/p&gt; 
&lt;p&gt;A overview of the dataset is illustrated bellow:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/zhu-xlab/GlobalBuildingAtlas/main/figures/overview.png" width="800" /&gt; 
&lt;h2&gt;Access to the Data&lt;/h2&gt; 
&lt;h3&gt;Web Feature Service (WFS)&lt;/h3&gt; 
&lt;p&gt;A WFS is provided so that one can access the data using other websites or GIS softwares such as QGIS and ArcGIS.&lt;/p&gt; 
&lt;p&gt;Url: &lt;code&gt;https://tubvsig-so2sat-vm1.srv.mwn.de/geoserver/ows?&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Web Viewer&lt;/h3&gt; 
&lt;p&gt;A web interface for viewing the data is available at: &lt;a href="https://tubvsig-so2sat-vm1.srv.mwn.de"&gt;website&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Full Data Download&lt;/h3&gt; 
&lt;p&gt;The full data can be downloaded from &lt;a href="https://mediatum.ub.tum.de/1782307"&gt;mediaTUM&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development Code&lt;/h2&gt; 
&lt;h3&gt;Global Building Polygon Generation using Satellite Data (Sec. 4.3)&lt;/h3&gt; 
&lt;p&gt;For codes related to building map extraction, regularization, polygonization, and simplification, i.e., generating building polygons from satellite images (Sec. 4.3.2, Sec. 4.3.3, and Sec. 4.3.4), please refer to &lt;code&gt;./im2bf&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Global Building Height Estimation (Sec. 4.4)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;For codes related to monocular height estimation using HTC-DC Net (Sec. 4.4.2), please refer to &lt;code&gt;./im2bh&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;For codes related to the global inference and uncertainty quantification (Sec. 4.4.3), please refer to &lt;code&gt;./infer_height&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Global LoD1 Building Model Generation (Sec. 4.5)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;For codes related to quality-guided building polygon fusion (Sec. 4.5.1), please refer to &lt;code&gt;./fuse_bf&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;For codes related to LoD1 building model generation (Sec. 4.5.2), please refer to &lt;code&gt;./make_lod1&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Visualization Code&lt;/h2&gt; 
&lt;p&gt;For codes to reproduce the plots in the manuscript, please refer to &lt;code&gt;./make_plots&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Code License&lt;/h2&gt; 
&lt;p&gt;MIT with Commons Clause (no commercial use allowed). See &lt;a href="https://github.com/zhu-xlab/GlobalBuildingAtlas/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How to cite&lt;/h2&gt; 
&lt;p&gt;If you find this dataset helpful in your work, please cite the following paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@Article{essd-17-6647-2025,
AUTHOR = {Zhu, X. X. and Chen, S. and Zhang, F. and Shi, Y. and Wang, Y.},
TITLE = {GlobalBuildingAtlas: an open global and complete dataset of building polygons, heights and LoD1 3D models},
JOURNAL = {Earth System Science Data},
VOLUME = {17},
YEAR = {2025},
NUMBER = {12},
PAGES = {6647--6668},
URL = {https://essd.copernicus.org/articles/17/6647/2025/},
DOI = {10.5194/essd-17-6647-2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/cutile-python</title>
      <link>https://github.com/NVIDIA/cutile-python</link>
      <description>&lt;p&gt;cuTile is a programming model for writing parallel kernels for NVIDIA GPUs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;cuTile Python&lt;/h1&gt; 
&lt;p&gt;cuTile Python is a programming language for NVIDIA GPUs. The official documentation can be found on &lt;a href="https://docs.nvidia.com/cuda/cutile-python"&gt;docs.nvidia.com&lt;/a&gt;, or built from source located in the &lt;a href="https://raw.githubusercontent.com/NVIDIA/cutile-python/main/docs/"&gt;docs&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h2&gt;Installing from PyPI&lt;/h2&gt; 
&lt;p&gt;cuTile Python is published on &lt;a href="https://pypi.org/"&gt;PyPI&lt;/a&gt; under the &lt;a href="https://pypi.org/project/cuda-tile/"&gt;cuda-tile&lt;/a&gt; package name and can be installed with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install cuda-tile
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Currently, the &lt;a href="https://developer.nvidia.com/cuda-downloads"&gt;CUDA Toolkit 13.1+&lt;/a&gt; is required and needs to be installed separately.&lt;/p&gt; 
&lt;h2&gt;Building from Source&lt;/h2&gt; 
&lt;p&gt;cuTile is written mostly in Python, but includes a C++ extension which needs to be built. You will need:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A C++17-capable compiler, such as GNU C++ or MSVC;&lt;/li&gt; 
 &lt;li&gt;CMake 3.18+;&lt;/li&gt; 
 &lt;li&gt;GNU Make on Linux or msbuild on Windows;&lt;/li&gt; 
 &lt;li&gt;Python 3.10+ with development headers (&lt;code&gt;venv&lt;/code&gt; module is recommended but optional);&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/cuda-downloads"&gt;CUDA Toolkit 13.1+&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;On an Ubuntu system, the first four dependencies can be installed with APT:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install build-essential cmake python3-dev python3-venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The CMakeLists.txt script will also automatically download the &lt;a href="https://github.com/dmlc/dlpack"&gt;DLPack&lt;/a&gt; dependency from GitHub. If you wish to disable this behavior and provide your own copy of DLPack, set the &lt;code&gt;CUDA_TILE_CMAKE_DLPACK_PATH&lt;/code&gt; environment variable to a local path to the DLPack source tree.&lt;/p&gt; 
&lt;p&gt;Unless you are already using a Python virtual environment, it is recommended to create one in order to avoid installing cuTile globally:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m venv env
source env/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the build dependencies are in place, the simplest way to build cuTile is to install it in editable mode by running the following command in the source root directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create the &lt;code&gt;build&lt;/code&gt; directory and invoke the CMake-based build process. In editable mode, the compiled extension module will be placed in the build directory, and then a symbolic link to it will be created in the source directory. This makes sure that the &lt;code&gt;pip install -e .&lt;/code&gt; command above is needed only once, and recompiling the extension after making changes to the C++ code can be done with &lt;code&gt;make -C build&lt;/code&gt; which is much faster. This logic is defined in &lt;a href="https://raw.githubusercontent.com/NVIDIA/cutile-python/main/setup.py"&gt;setup.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Running Tests&lt;/h2&gt; 
&lt;p&gt;cuTile uses the &lt;a href="https://pytest.org"&gt;pytest&lt;/a&gt; framework for testing. Tests have extra dependencies, such as PyTorch, which can be installed with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -r test/requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The tests are located in the &lt;a href="https://raw.githubusercontent.com/NVIDIA/cutile-python/main/test/"&gt;test/&lt;/a&gt; directory. To run a specific test file, for example &lt;code&gt;test_copy.py&lt;/code&gt;, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pytest test/test_copy.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Copyright and License Information&lt;/h2&gt; 
&lt;p&gt;Copyright ¬© 2025 NVIDIA CORPORATION &amp;amp; AFFILIATES. All rights reserved.&lt;/p&gt; 
&lt;p&gt;cuTile-Python is under Apache 2.0 license. See the &lt;a href="https://raw.githubusercontent.com/NVIDIA/cutile-python/main/LICENSES/"&gt;LICENSES&lt;/a&gt; folder for the full license text.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;‚å®Ô∏è Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;üñ•Ô∏è Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;‚å®Ô∏è Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;üñ•Ô∏è Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03‚ÄØPM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>666ghj/BettaFish</title>
      <link>https://github.com/666ghj/BettaFish</link>
      <description>&lt;p&gt;ÂæÆËàÜÔºö‰∫∫‰∫∫ÂèØÁî®ÁöÑÂ§öAgentËàÜÊÉÖÂàÜÊûêÂä©ÊâãÔºåÊâìÁ†¥‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñÔºÅ‰ªé0ÂÆûÁé∞Ôºå‰∏ç‰æùËµñ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_compressed.png" alt="BettaFish Logo" width="100%" /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/15286" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15286" alt="666ghj%2FBettaFish | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://aihubmix.com/?aff=8Ds9" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_aihubmix.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;‚ÄÇ &lt;a href="https://lioncc.ai/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_loincc.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;‚ÄÇ &lt;a href="https://share.302.ai/P66Qe3" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_302ai.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://open.anspire.cn/?share_code=3E1FUOUH" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_anspire.png" alt="666ghj%2FBettaFish | Trendshift" height="50" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/666ghj/BettaFish/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/666ghj/BettaFish?style=flat-square" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/BettaFish/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/666ghj/BettaFish?style=flat-square" alt="GitHub Watchers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/BettaFish/network"&gt;&lt;img src="https://img.shields.io/github/forks/666ghj/BettaFish?style=flat-square" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/BettaFish/issues"&gt;&lt;img src="https://img.shields.io/github/issues/666ghj/BettaFish?style=flat-square" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/BettaFish/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/666ghj/BettaFish?style=flat-square" alt="GitHub Pull Requests" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/666ghj/BettaFish/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/666ghj/BettaFish?style=flat-square" alt="GitHub License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/BettaFish"&gt;&lt;img src="https://img.shields.io/badge/version-v1.2.1-green.svg?style=flat-square" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/"&gt;&lt;img src="https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/README-EN.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/README.md"&gt;‰∏≠ÊñáÊñáÊ°£&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ö° È°πÁõÆÊ¶ÇËø∞&lt;/h2&gt; 
&lt;p&gt;‚Äú&lt;strong&gt;ÂæÆËàÜ&lt;/strong&gt;‚Äù ÊòØ‰∏Ä‰∏™‰ªé0ÂÆûÁé∞ÁöÑÂàõÊñ∞Âûã Â§öÊô∫ËÉΩ‰Ωì ËàÜÊÉÖÂàÜÊûêÁ≥ªÁªüÔºåÂ∏ÆÂä©Â§ßÂÆ∂Á†¥Èô§‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñ„ÄÇÁî®Êà∑Âè™ÈúÄÂÉèËÅäÂ§©‰∏ÄÊ†∑ÊèêÂá∫ÂàÜÊûêÈúÄÊ±ÇÔºåÊô∫ËÉΩ‰ΩìÂºÄÂßãÂÖ®Ëá™Âä®ÂàÜÊûê ÂõΩÂÜÖÂ§ñ30+‰∏ªÊµÅÁ§æÂ™í ‰∏é Êï∞Áôæ‰∏áÊù°Â§ß‰ºóËØÑËÆ∫„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄúÂæÆËàÜ‚ÄùË∞êÈü≥‚ÄúÂæÆÈ±º‚ÄùÔºåBettaFishÊòØ‰∏ÄÁßç‰ΩìÂûãÂæàÂ∞è‰ΩÜÈùûÂ∏∏Â•ΩÊñó„ÄÅÊºÇ‰∫ÆÁöÑÈ±ºÔºåÂÆÉË±°ÂæÅÁùÄ‚ÄúÂ∞èËÄåÂº∫Â§ßÔºå‰∏çÁïèÊåëÊàò‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Êü•ÁúãÁ≥ªÁªü‰ª•‚ÄúÊ≠¶Ê±âÂ§ßÂ≠¶ËàÜÊÉÖ‚Äù‰∏∫‰æãÔºåÁîüÊàêÁöÑÁ†îÁ©∂Êä•ÂëäÔºö&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/final_reports/final_report__20250827_131630.html"&gt;Ê≠¶Ê±âÂ§ßÂ≠¶ÂìÅÁâåÂ£∞Ë™âÊ∑±Â∫¶ÂàÜÊûêÊä•Âëä&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Êü•ÁúãÁ≥ªÁªü‰ª•‚ÄúÊ≠¶Ê±âÂ§ßÂ≠¶ËàÜÊÉÖ‚Äù‰∏∫‰æãÔºå‰∏ÄÊ¨°ÂÆåÊï¥ËøêË°åÁöÑËßÜÈ¢ëÔºö&lt;a href="https://www.bilibili.com/video/BV1TH1WBxEWN/?vd_source=da3512187e242ce17dceee4c537ec7a6#reply279744466833"&gt;ËßÜÈ¢ë-Ê≠¶Ê±âÂ§ßÂ≠¶ÂìÅÁâåÂ£∞Ë™âÊ∑±Â∫¶ÂàÜÊûêÊä•Âëä&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;‰∏ç‰ªÖ‰ªÖ‰ΩìÁé∞Âú®Êä•ÂëäË¥®Èáè‰∏äÔºåÁõ∏ÊØîÂêåÁ±ª‰∫ßÂìÅÔºåÊàë‰ª¨Êã•ÊúâüöÄÂÖ≠Â§ß‰ºòÂäøÔºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AIÈ©±Âä®ÁöÑÂÖ®ÂüüÁõëÊéß&lt;/strong&gt;ÔºöAIÁà¨Ëô´ÈõÜÁæ§7x24Â∞èÊó∂‰∏çÈó¥Êñ≠‰Ωú‰∏öÔºåÂÖ®Èù¢Ë¶ÜÁõñÂæÆÂçö„ÄÅÂ∞èÁ∫¢‰π¶„ÄÅÊäñÈü≥„ÄÅÂø´ÊâãÁ≠â10+ÂõΩÂÜÖÂ§ñÂÖ≥ÈîÆÁ§æÂ™í„ÄÇ‰∏ç‰ªÖÂÆûÊó∂ÊçïËé∑ÁÉ≠ÁÇπÂÜÖÂÆπÔºåÊõ¥ËÉΩ‰∏ãÈíªËá≥Êµ∑ÈáèÁî®Êà∑ËØÑËÆ∫ÔºåËÆ©ÊÇ®Âê¨Âà∞ÊúÄÁúüÂÆû„ÄÅÊúÄÂπøÊ≥õÁöÑÂ§ß‰ºóÂ£∞Èü≥„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ë∂ÖË∂äLLMÁöÑÂ§çÂêàÂàÜÊûêÂºïÊìé&lt;/strong&gt;ÔºöÊàë‰ª¨‰∏ç‰ªÖ‰æùËµñËÆæËÆ°ÁöÑ5Á±ª‰∏ì‰∏öAgentÔºåÊõ¥ËûçÂêà‰∫ÜÂæÆË∞ÉÊ®°Âûã„ÄÅÁªüËÆ°Ê®°ÂûãÁ≠â‰∏≠Èó¥‰ª∂„ÄÇÈÄöËøáÂ§öÊ®°ÂûãÂçèÂêåÂ∑•‰ΩúÔºåÁ°Æ‰øù‰∫ÜÂàÜÊûêÁªìÊûúÁöÑÊ∑±Â∫¶„ÄÅÂáÜÂ∫¶‰∏éÂ§öÁª¥ËßÜËßí„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõ&lt;/strong&gt;ÔºöÁ™ÅÁ†¥ÂõæÊñáÈôêÂà∂ÔºåËÉΩÊ∑±Â∫¶Ëß£ÊûêÊäñÈü≥„ÄÅÂø´ÊâãÁ≠âÁü≠ËßÜÈ¢ëÂÜÖÂÆπÔºåÂπ∂Á≤æÂáÜÊèêÂèñÁé∞‰ª£ÊêúÁ¥¢ÂºïÊìé‰∏≠ÁöÑÂ§©Ê∞î„ÄÅÊó•ÂéÜ„ÄÅËÇ°Á•®Á≠âÁªìÊûÑÂåñÂ§öÊ®°ÊÄÅ‰ø°ÊÅØÂç°ÁâáÔºåËÆ©ÊÇ®ÂÖ®Èù¢ÊéåÊè°ËàÜÊÉÖÂä®ÊÄÅ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Agent‚ÄúËÆ∫Âùõ‚ÄùÂçè‰ΩúÊú∫Âà∂&lt;/strong&gt;Ôºö‰∏∫‰∏çÂêåAgentËµã‰∫àÁã¨ÁâπÁöÑÂ∑•ÂÖ∑ÈõÜ‰∏éÊÄùÁª¥Ê®°ÂºèÔºåÂºïÂÖ•Ëæ©ËÆ∫‰∏ªÊåÅ‰∫∫Ê®°ÂûãÔºåÈÄöËøá‚ÄúËÆ∫Âùõ‚ÄùÊú∫Âà∂ËøõË°åÈìæÂºèÊÄùÁª¥Á¢∞Êíû‰∏éËæ©ËÆ∫„ÄÇËøô‰∏ç‰ªÖÈÅøÂÖç‰∫ÜÂçï‰∏ÄÊ®°ÂûãÁöÑÊÄùÁª¥Â±ÄÈôê‰∏é‰∫§ÊµÅÂØºËá¥ÁöÑÂêåË¥®ÂåñÔºåÊõ¥ÂÇ¨ÁîüÂá∫Êõ¥È´òË¥®ÈáèÁöÑÈõÜ‰ΩìÊô∫ËÉΩ‰∏éÂÜ≥Á≠ñÊîØÊåÅ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÖ¨ÁßÅÂüüÊï∞ÊçÆÊó†ÁºùËûçÂêà&lt;/strong&gt;ÔºöÂπ≥Âè∞‰∏ç‰ªÖÂàÜÊûêÂÖ¨ÂºÄËàÜÊÉÖÔºåËøòÊèê‰æõÈ´òÂÆâÂÖ®ÊÄßÁöÑÊé•Âè£ÔºåÊîØÊåÅÊÇ®Â∞ÜÂÜÖÈÉ®‰∏öÂä°Êï∞ÊçÆÂ∫ì‰∏éËàÜÊÉÖÊï∞ÊçÆÊó†ÁºùÈõÜÊàê„ÄÇÊâìÈÄöÊï∞ÊçÆÂ£ÅÂûíÔºå‰∏∫ÂûÇÁõ¥‰∏öÂä°Êèê‰æõ‚ÄúÂ§ñÈÉ®Ë∂ãÂäø+ÂÜÖÈÉ®Ê¥ûÂØü‚ÄùÁöÑÂº∫Â§ßÂàÜÊûêËÉΩÂäõ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ËΩªÈáèÂåñ‰∏éÈ´òÊâ©Â±ïÊÄßÊ°ÜÊû∂&lt;/strong&gt;ÔºöÂü∫‰∫éÁ∫ØPythonÊ®°ÂùóÂåñËÆæËÆ°ÔºåÂÆûÁé∞ËΩªÈáèÂåñ„ÄÅ‰∏ÄÈîÆÂºèÈÉ®ÁΩ≤„ÄÇ‰ª£Á†ÅÁªìÊûÑÊ∏ÖÊô∞ÔºåÂºÄÂèëËÄÖÂèØËΩªÊùæÈõÜÊàêËá™ÂÆö‰πâÊ®°Âûã‰∏é‰∏öÂä°ÈÄªËæëÔºåÂÆûÁé∞Âπ≥Âè∞ÁöÑÂø´ÈÄüÊâ©Â±ï‰∏éÊ∑±Â∫¶ÂÆöÂà∂„ÄÇ&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Âßã‰∫éËàÜÊÉÖÔºåËÄå‰∏çÊ≠¢‰∫éËàÜÊÉÖ&lt;/strong&gt;„ÄÇ‚ÄúÂæÆËàÜ‚ÄùÁöÑÁõÆÊ†áÔºåÊòØÊàê‰∏∫È©±Âä®‰∏ÄÂàá‰∏öÂä°Âú∫ÊôØÁöÑÁÆÄÊ¥ÅÈÄöÁî®ÁöÑÊï∞ÊçÆÂàÜÊûêÂºïÊìé„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‰∏æ‰∏™‰æãÂ≠ê. ‰Ω†Âè™ÈúÄÁÆÄÂçï‰øÆÊîπAgentÂ∑•ÂÖ∑ÈõÜÁöÑapiÂèÇÊï∞‰∏épromptÔºåÂ∞±ÂèØ‰ª•Êää‰ªñÂèòÊàê‰∏Ä‰∏™ÈáëËûçÈ¢ÜÂüüÁöÑÂ∏ÇÂú∫ÂàÜÊûêÁ≥ªÁªü&lt;/p&gt; 
 &lt;p&gt;ÈôÑ‰∏Ä‰∏™ÊØîËæÉÊ¥ªË∑ÉÁöÑLÁ´ôÈ°πÁõÆËÆ®ËÆ∫Â∏ñÔºö&lt;a href="https://linux.do/t/topic/1009280"&gt;https://linux.do/t/topic/1009280&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Êü•ÁúãLÁ´ô‰Ω¨ÂèãÂÅöÁöÑÊµãËØÑ &lt;a href="https://linux.do/t/topic/1148040"&gt;ÂºÄÊ∫êÈ°πÁõÆ(ÂæÆËàÜ)‰∏émanus|minimax|ChatGPTÂØπÊØî&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/system_schematic.png" alt="banner" width="800" /&gt; 
 &lt;p&gt;ÂëäÂà´‰º†ÁªüÁöÑÊï∞ÊçÆÁúãÊùøÔºåÂú®‚ÄúÂæÆËàÜ‚ÄùÔºå‰∏ÄÂàáÁî±‰∏Ä‰∏™ÁÆÄÂçïÁöÑÈóÆÈ¢òÂºÄÂßãÔºåÊÇ®Âè™ÈúÄÂÉèÂØπËØù‰∏ÄÊ†∑ÔºåÊèêÂá∫ÊÇ®ÁöÑÂàÜÊûêÈúÄÊ±Ç&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ü™Ñ ËµûÂä©ÂïÜ&lt;/h2&gt; 
&lt;p&gt;LLMÊ®°ÂûãAPIËµûÂä©Ôºö&lt;a href="https://aihubmix.com/?aff=8Ds9" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_aihubmix.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ÊúâËµûÂä©LLMÁÆóÂäõÁ¶èÂà©ÔºÅÁºñÁ®ãÊãºËΩ¶codecodex.aiÔºõÁºñÁ®ãÁÆóÂäõVibeCodingAPI.aiÔºö&lt;span style="margin-left: 10px"&gt;&lt;a href="https://codecodex.ai/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_loincc.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;&lt;/span&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÊâÄÁΩóÈó®ÂçöÂÆ¢LionCC.aiÂ∑≤Êõ¥Êñ∞„ÄäBettaFish ÂæÆËàÜÁ≥ªÁªü - LionCC API ÈÉ®ÁΩ≤ÈÖçÁΩÆÂÆåÂÖ®ÊåáÂçó„ÄãÊ≠£Âú®‰∫åÂºÄ‰ºòÂåñ‰∏ÄÈîÆÈÉ®ÁΩ≤Âíå‰∫ëÊúçÂä°Âô®Ë∞ÉÁî®ÊñπÊ°à„ÄÇ&lt;/li&gt; 
  &lt;li&gt;VibeCodingapi.aiÁãÆÂ≠êÁÆóÂäõÂπ≥Âè∞Â∑≤ÁªèÈÄÇÈÖç„ÄäBettaFish ÂæÆËàÜÁ≥ªÁªü„ÄãÊâÄÊúâLLMÊ®°ÂûãÂê´claude codeÂíåopenai codexÂíågemini cliÁºñÁ®ãÂºÄÂèë‰∏âÂ∑®Â§¥ÁÆóÂäõ„ÄÇÈ¢ùÂ∫¶‰ª∑Ê†ºÔºåÂè™Ë¶Å‰∏ÄÊØî‰∏ÄÔºà100ÂÖÉÁ≠â‰∫é100ÁæéÂàÄÈ¢ùÂ∫¶Ôºâ&lt;/li&gt; 
  &lt;li&gt;Codecodex.aiÁãÆÂ≠êÁºñÁ®ãÊãºËΩ¶Á≥ªÁªüÔºåÂ∑≤ÂÆûÁé∞Êó†IPÈó®ÊßõÁªïËøáclaude codeÂíåopenai codexÂ∞ÅÈîÅÔºåÊåâÂÆòÊñπÈÉ®ÁΩ≤ÊïôÁ®ãÂêéÂàáÊç¢BASE_URLË∞ÉÁî®Âú∞ÂùÄÂíåToken keyË∞ÉÁî®ÂØÜÈí•Âç≥ÂèØ‰ΩøÁî®ÊúÄÂº∫ÁºñÁ®ãÊ®°Âûã„ÄÇ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;ÊâÄÁΩóÈó®LionCCËµûÂä©BettaFish ÂæÆËàÜÁ¶èÂà©ÔºöÊâìÂºÄcodecodex.aiÁãÆÂ≠êÁºñÁ®ãÈ¢ëÈÅìÊâ´Á†ÅÂä†ÂÖ•ÂæÆ‰ø°Á§æÁæ§ÔºåÊ≥®ÂÜåVibeCodingapi.aiÁãÆÂ≠êÁÆóÂäõÔºåÁªü‰∏ÄÈÄÅ20ÂàÄAPIÈ¢ùÂ∫¶Ôºà‰ªÖÈôêÂâç‰∏ÄÂçÉÂêçÔºâ&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ÊåâÁî®Èáè‰ªòË¥πÁöÑ‰ºÅ‰∏öÁ∫ßAIËµÑÊ∫êÂπ≥Âè∞ÔºåÊèê‰æõÂ∏ÇÂú∫‰∏äÂÖ®Èù¢ÁöÑAIÊ®°ÂûãÂíåAPIÔºå‰ª•ÂèäÂ§öÁßçÂú®Á∫øAIÂ∫îÁî®Ôºö&lt;span style="margin-left: 10px"&gt;&lt;a href="https://share.302.ai/P66Qe3" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_302ai.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;&lt;/span&gt;&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/banner_302ai_ch.jpg" alt="banner" /&gt;302.AIÊòØ‰∏Ä‰∏™ÊåâÁî®Èáè‰ªòË¥πÁöÑ‰ºÅ‰∏öÁ∫ßAIËµÑÊ∫êÂπ≥Âè∞ÔºåÊèê‰æõÂ∏ÇÂú∫‰∏äÊúÄÊñ∞„ÄÅÊúÄÂÖ®Èù¢ÁöÑAIÊ®°ÂûãÂíåAPIÔºå‰ª•ÂèäÂ§öÁßçÂºÄÁÆ±Âç≥Áî®ÁöÑÂú®Á∫øAIÂ∫îÁî®„ÄÇ 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;AIËÅîÁΩëÊêúÁ¥¢„ÄÅÊñá‰ª∂Ëß£ÊûêÂèäÁΩëÈ°µÂÜÖÂÆπÊäìÂèñÁ≠âÊô∫ËÉΩ‰ΩìÊ†∏ÂøÉËÉΩÂäõÊèê‰æõÂïÜÔºö&lt;span style="margin-left: 10px"&gt;&lt;a href="https://open.anspire.cn/?share_code=3E1FUOUH" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_anspire.png" alt="666ghj%2FBettaFish | Trendshift" height="50" /&gt;&lt;/a&gt;&lt;/span&gt;&lt;/summary&gt; ÂÆâÊÄùÊ¥æÂºÄÊîæÂπ≥Âè∞(Anspire Open)ÊòØÈù¢ÂêëÊô∫ËÉΩ‰ΩìÊó∂‰ª£ÁöÑÈ¢ÜÂÖàÁöÑÂü∫Á°ÄËÆæÊñΩÊèê‰æõÂïÜ„ÄÇÊàë‰ª¨‰∏∫ÂºÄÂèëËÄÖÊèê‰æõÊûÑÂª∫Âº∫Â§ßÊô∫ËÉΩ‰ΩìÊâÄÈúÄÁöÑÊ†∏ÂøÉËÉΩÂäõÊ†àÔºåÁé∞Â∑≤‰∏äÁ∫øAIËÅîÁΩëÊêúÁ¥¢„ÄêÂ§öÁâàÊú¨ÔºåÊûÅÂÖ∑Á´û‰∫âÂäõÁöÑ‰ª∑Ê†º„Äë„ÄÅÊñá‰ª∂Ëß£Êûê„ÄêÈôêÂÖç„ÄëÂèäÁΩëÈ°µÂÜÖÂÆπÊäìÂèñ„ÄêÈôêÂÖç„Äë„ÄÅ‰∫ëÁ´ØÊµèËßàÂô®Ëá™Âä®ÂåñÔºàAnspire Browser AgentÔºâ„ÄêÂÜÖÊµã„Äë„ÄÅÂ§öËΩÆÊîπÂÜôÁ≠âÊúçÂä°ÔºåÊåÅÁª≠‰∏∫Êô∫ËÉΩ‰ΩìËøûÊé•Âπ∂Êìç‰ΩúÂ§çÊùÇÁöÑÊï∞Â≠ó‰∏ñÁïåÊèê‰æõÂùöÂÆûÂü∫Á°Ä„ÄÇÂèØÊó†ÁºùÈõÜÊàêËá≥Dify„ÄÅCoze„ÄÅÂÖÉÂô®Á≠â‰∏ªÊµÅÊô∫ËÉΩ‰ΩìÂπ≥Âè∞„ÄÇÈÄöËøáÈÄèÊòéÁÇπÊï∞ËÆ°Ë¥π‰ΩìÁ≥ª‰∏éÊ®°ÂùóÂåñËÆæËÆ°Ôºå‰∏∫‰ºÅ‰∏öÊèê‰æõÈ´òÊïà„ÄÅ‰ΩéÊàêÊú¨ÁöÑÂÆöÂà∂ÂåñÊîØÊåÅÔºåÂä†ÈÄüÊô∫ËÉΩÂåñÂçáÁ∫ßËøõÁ®ã„ÄÇ 
&lt;/details&gt; 
&lt;h2&gt;üèóÔ∏è Á≥ªÁªüÊû∂ÊûÑ&lt;/h2&gt; 
&lt;h3&gt;Êï¥‰ΩìÊû∂ÊûÑÂõæ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Insight Agent&lt;/strong&gt; ÁßÅÊúâÊï∞ÊçÆÂ∫ìÊåñÊéòÔºöÁßÅÊúâËàÜÊÉÖÊï∞ÊçÆÂ∫ìÊ∑±Â∫¶ÂàÜÊûêAI‰ª£ÁêÜ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Media Agent&lt;/strong&gt; Â§öÊ®°ÊÄÅÂÜÖÂÆπÂàÜÊûêÔºöÂÖ∑Â§áÂº∫Â§ßÂ§öÊ®°ÊÄÅËÉΩÂäõÁöÑAI‰ª£ÁêÜ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Query Agent&lt;/strong&gt; Á≤æÂáÜ‰ø°ÊÅØÊêúÁ¥¢ÔºöÂÖ∑Â§áÂõΩÂÜÖÂ§ñÁΩëÈ°µÊêúÁ¥¢ËÉΩÂäõÁöÑAI‰ª£ÁêÜ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Report Agent&lt;/strong&gt; Êô∫ËÉΩÊä•ÂëäÁîüÊàêÔºöÂÜÖÁΩÆÊ®°ÊùøÁöÑÂ§öËΩÆÊä•ÂëäÁîüÊàêAI‰ª£ÁêÜ&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/framework.png" alt="banner" width="800" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;‰∏ÄÊ¨°ÂÆåÊï¥ÂàÜÊûêÊµÅÁ®ã&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ê≠•È™§&lt;/th&gt; 
   &lt;th&gt;Èò∂ÊÆµÂêçÁß∞&lt;/th&gt; 
   &lt;th&gt;‰∏ªË¶ÅÊìç‰Ωú&lt;/th&gt; 
   &lt;th&gt;ÂèÇ‰∏éÁªÑ‰ª∂&lt;/th&gt; 
   &lt;th&gt;Âæ™ÁéØÁâπÊÄß&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;Áî®Êà∑ÊèêÈóÆ&lt;/td&gt; 
   &lt;td&gt;Flask‰∏ªÂ∫îÁî®Êé•Êî∂Êü•ËØ¢&lt;/td&gt; 
   &lt;td&gt;Flask‰∏ªÂ∫îÁî®&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;Âπ∂Ë°åÂêØÂä®&lt;/td&gt; 
   &lt;td&gt;‰∏â‰∏™AgentÂêåÊó∂ÂºÄÂßãÂ∑•‰Ωú&lt;/td&gt; 
   &lt;td&gt;Query Agent„ÄÅMedia Agent„ÄÅInsight Agent&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;ÂàùÊ≠•ÂàÜÊûê&lt;/td&gt; 
   &lt;td&gt;ÂêÑAgent‰ΩøÁî®‰∏ìÂ±ûÂ∑•ÂÖ∑ËøõË°åÊ¶ÇËßàÊêúÁ¥¢&lt;/td&gt; 
   &lt;td&gt;ÂêÑAgent + ‰∏ìÂ±ûÂ∑•ÂÖ∑ÈõÜ&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;Á≠ñÁï•Âà∂ÂÆö&lt;/td&gt; 
   &lt;td&gt;Âü∫‰∫éÂàùÊ≠•ÁªìÊûúÂà∂ÂÆöÂàÜÂùóÁ†îÁ©∂Á≠ñÁï•&lt;/td&gt; 
   &lt;td&gt;ÂêÑAgentÂÜÖÈÉ®ÂÜ≥Á≠ñÊ®°Âùó&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5-N&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Âæ™ÁéØÈò∂ÊÆµ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ËÆ∫ÂùõÂçè‰Ωú + Ê∑±Â∫¶Á†îÁ©∂&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ForumEngine + ÊâÄÊúâAgent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Â§öËΩÆÂæ™ÁéØ&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.1&lt;/td&gt; 
   &lt;td&gt;Ê∑±Â∫¶Á†îÁ©∂&lt;/td&gt; 
   &lt;td&gt;ÂêÑAgentÂü∫‰∫éËÆ∫Âùõ‰∏ªÊåÅ‰∫∫ÂºïÂØºËøõË°å‰∏ìÈ°πÊêúÁ¥¢&lt;/td&gt; 
   &lt;td&gt;ÂêÑAgent + ÂèçÊÄùÊú∫Âà∂ + ËÆ∫ÂùõÂºïÂØº&lt;/td&gt; 
   &lt;td&gt;ÊØèËΩÆÂæ™ÁéØ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.2&lt;/td&gt; 
   &lt;td&gt;ËÆ∫ÂùõÂçè‰Ωú&lt;/td&gt; 
   &lt;td&gt;ForumEngineÁõëÊéßAgentÂèëË®ÄÂπ∂ÁîüÊàê‰∏ªÊåÅ‰∫∫ÂºïÂØº&lt;/td&gt; 
   &lt;td&gt;ForumEngine + LLM‰∏ªÊåÅ‰∫∫&lt;/td&gt; 
   &lt;td&gt;ÊØèËΩÆÂæ™ÁéØ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.3&lt;/td&gt; 
   &lt;td&gt;‰∫§ÊµÅËûçÂêà&lt;/td&gt; 
   &lt;td&gt;ÂêÑAgentÊ†πÊçÆËÆ®ËÆ∫Ë∞ÉÊï¥Á†îÁ©∂ÊñπÂêë&lt;/td&gt; 
   &lt;td&gt;ÂêÑAgent + forum_readerÂ∑•ÂÖ∑&lt;/td&gt; 
   &lt;td&gt;ÊØèËΩÆÂæ™ÁéØ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N+1&lt;/td&gt; 
   &lt;td&gt;ÁªìÊûúÊï¥Âêà&lt;/td&gt; 
   &lt;td&gt;Report AgentÊî∂ÈõÜÊâÄÊúâÂàÜÊûêÁªìÊûúÂíåËÆ∫ÂùõÂÜÖÂÆπ&lt;/td&gt; 
   &lt;td&gt;Report Agent&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N+2&lt;/td&gt; 
   &lt;td&gt;IR‰∏≠Èó¥Ë°®Á§∫&lt;/td&gt; 
   &lt;td&gt;Âä®ÊÄÅÈÄâÊã©Ê®°ÊùøÂíåÊ†∑ÂºèÔºåÂ§öËΩÆÁîüÊàêÂÖÉÊï∞ÊçÆÔºåË£ÖËÆ¢‰∏∫IR‰∏≠Èó¥Ë°®Á§∫&lt;/td&gt; 
   &lt;td&gt;Report Agent + Ê®°ÊùøÂºïÊìé&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N+3&lt;/td&gt; 
   &lt;td&gt;Êä•ÂëäÁîüÊàê&lt;/td&gt; 
   &lt;td&gt;ÂàÜÂùóËøõË°åË¥®ÈáèÊ£ÄÊµãÔºåÂü∫‰∫éIRÊ∏≤ÊüìÊàê‰∫§‰∫íÂºè HTML Êä•Âëä&lt;/td&gt; 
   &lt;td&gt;Report Agent + Ë£ÖËÆ¢ÂºïÊìé&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;È°πÁõÆ‰ª£Á†ÅÁªìÊûÑÊ†ë&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;BettaFish/
‚îú‚îÄ‚îÄ QueryEngine/                            # ÂõΩÂÜÖÂ§ñÊñ∞ÈóªÂπøÂ∫¶ÊêúÁ¥¢Agent
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                            # Agent‰∏ªÈÄªËæëÔºåÂçèË∞ÉÊêúÁ¥¢‰∏éÂàÜÊûêÊµÅÁ®ã
‚îÇ   ‚îú‚îÄ‚îÄ llms/                               # LLMÊé•Âè£Â∞ÅË£Ö
‚îÇ   ‚îú‚îÄ‚îÄ nodes/                              # Â§ÑÁêÜËäÇÁÇπÔºöÊêúÁ¥¢„ÄÅÊ†ºÂºèÂåñ„ÄÅÊÄªÁªìÁ≠â
‚îÇ   ‚îú‚îÄ‚îÄ tools/                              # ÂõΩÂÜÖÂ§ñÊñ∞ÈóªÊêúÁ¥¢Â∑•ÂÖ∑ÈõÜ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                              # Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îú‚îÄ‚îÄ state/                              # Áä∂ÊÄÅÁÆ°ÁêÜ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                            # ÊèêÁ§∫ËØçÊ®°Êùø
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ MediaEngine/                            # Âº∫Â§ßÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£Agent
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                            # Agent‰∏ªÈÄªËæëÔºåÂ§ÑÁêÜËßÜÈ¢ë/ÂõæÁâáÁ≠âÂ§öÊ®°ÊÄÅÂÜÖÂÆπ
‚îÇ   ‚îú‚îÄ‚îÄ llms/                               # LLMÊé•Âè£Â∞ÅË£Ö
‚îÇ   ‚îú‚îÄ‚îÄ nodes/                              # Â§ÑÁêÜËäÇÁÇπÔºöÊêúÁ¥¢„ÄÅÊ†ºÂºèÂåñ„ÄÅÊÄªÁªìÁ≠â
‚îÇ   ‚îú‚îÄ‚îÄ tools/                              # Â§öÊ®°ÊÄÅÊêúÁ¥¢Â∑•ÂÖ∑ÈõÜ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                              # Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îú‚îÄ‚îÄ state/                              # Áä∂ÊÄÅÁÆ°ÁêÜ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                            # ÊèêÁ§∫ËØçÊ®°Êùø
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ InsightEngine/                          # ÁßÅÊúâÊï∞ÊçÆÂ∫ìÊåñÊéòAgent
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                            # Agent‰∏ªÈÄªËæëÔºåÂçèË∞ÉÊï∞ÊçÆÂ∫ìÊü•ËØ¢‰∏éÂàÜÊûê
‚îÇ   ‚îú‚îÄ‚îÄ llms/                               # LLMÊé•Âè£Â∞ÅË£Ö
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base.py                         # Áªü‰∏ÄÁöÑOpenAIÂÖºÂÆπÂÆ¢Êà∑Á´Ø
‚îÇ   ‚îú‚îÄ‚îÄ nodes/                              # Â§ÑÁêÜËäÇÁÇπÔºöÊêúÁ¥¢„ÄÅÊ†ºÂºèÂåñ„ÄÅÊÄªÁªìÁ≠â
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_node.py                    # Âü∫Á°ÄËäÇÁÇπÁ±ª
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_node.py                  # ÊêúÁ¥¢ËäÇÁÇπ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formatting_node.py              # Ê†ºÂºèÂåñËäÇÁÇπ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_structure_node.py        # Êä•ÂëäÁªìÊûÑËäÇÁÇπ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary_node.py                 # ÊÄªÁªìËäÇÁÇπ
‚îÇ   ‚îú‚îÄ‚îÄ tools/                              # Êï∞ÊçÆÂ∫ìÊü•ËØ¢ÂíåÂàÜÊûêÂ∑•ÂÖ∑ÈõÜ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keyword_optimizer.py            # QwenÂÖ≥ÈîÆËØç‰ºòÂåñ‰∏≠Èó¥‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py                       # Êï∞ÊçÆÂ∫ìÊìç‰ΩúÂ∑•ÂÖ∑ÈõÜÔºàËØùÈ¢òÊêúÁ¥¢„ÄÅËØÑËÆ∫Ëé∑ÂèñÁ≠âÔºâ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sentiment_analyzer.py           # ÊÉÖÊÑüÂàÜÊûêÈõÜÊàêÂ∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ utils/                              # Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                       # ÈÖçÁΩÆÁÆ°ÁêÜ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db.py                           # SQLAlchemyÂºÇÊ≠•ÂºïÊìé‰∏éÂè™ËØªÊü•ËØ¢Â∞ÅË£Ö
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_processing.py              # ÊñáÊú¨Â§ÑÁêÜÂ∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ state/                              # Áä∂ÊÄÅÁÆ°ÁêÜ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py                        # AgentÁä∂ÊÄÅÂÆö‰πâ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                            # ÊèêÁ§∫ËØçÊ®°Êùø
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py                      # ÂêÑÁ±ªÊèêÁ§∫ËØç
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ ReportEngine/                           # Â§öËΩÆÊä•ÂëäÁîüÊàêAgent
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                            # ÊÄªË∞ÉÂ∫¶Âô®ÔºöÊ®°ÊùøÈÄâÊã©‚ÜíÂ∏ÉÂ±Ä‚ÜíÁØáÂπÖ‚ÜíÁ´†ËäÇ‚ÜíÊ∏≤Êüì
‚îÇ   ‚îú‚îÄ‚îÄ flask_interface.py                  # Flask/SSEÂÖ•Âè£ÔºåÁÆ°ÁêÜ‰ªªÂä°ÊéíÈòü‰∏éÊµÅÂºè‰∫ã‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ llms/                               # OpenAIÂÖºÂÆπLLMÂ∞ÅË£Ö
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base.py                         # Áªü‰∏ÄÁöÑÊµÅÂºè/ÈáçËØïÂÆ¢Êà∑Á´Ø
‚îÇ   ‚îú‚îÄ‚îÄ core/                               # Ê†∏ÂøÉÂäüËÉΩÔºöÊ®°ÊùøËß£Êûê„ÄÅÁ´†ËäÇÂ≠òÂÇ®„ÄÅÊñáÊ°£Ë£ÖËÆ¢
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ template_parser.py              # MarkdownÊ®°ÊùøÂàáÁâá‰∏éslugÁîüÊàê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chapter_storage.py              # Á´†ËäÇrunÁõÆÂΩï„ÄÅmanifest‰∏érawÊµÅÂÜôÂÖ•
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stitcher.py                     # Document IRË£ÖËÆ¢Âô®ÔºåË°•ÈΩêÈîöÁÇπ/ÂÖÉÊï∞ÊçÆ
‚îÇ   ‚îú‚îÄ‚îÄ ir/                                 # Êä•Âëä‰∏≠Èó¥Ë°®Á§∫ÔºàIRÔºâÂ•ëÁ∫¶‰∏éÊ†°È™å
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py                       # Âùó/Ê†áËÆ∞SchemaÂ∏∏ÈáèÂÆö‰πâ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validator.py                    # Á´†ËäÇJSONÁªìÊûÑÊ†°È™åÂô®
‚îÇ   ‚îú‚îÄ‚îÄ nodes/                              # ÂÖ®ÊµÅÁ®ãÊé®ÁêÜËäÇÁÇπ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_node.py                    # ËäÇÁÇπÂü∫Á±ª+Êó•Âøó/Áä∂ÊÄÅÈí©Â≠ê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ template_selection_node.py      # Ê®°ÊùøÂÄôÈÄâÊî∂ÈõÜ‰∏éLLMÁ≠õÈÄâ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_layout_node.py         # Ê†áÈ¢ò/ÁõÆÂΩï/‰∏ªÈ¢òËÆæËÆ°
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ word_budget_node.py             # ÁØáÂπÖËßÑÂàí‰∏éÁ´†ËäÇÊåá‰ª§ÁîüÊàê
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chapter_generation_node.py      # Á´†ËäÇÁ∫ßJSONÁîüÊàê+Ê†°È™å
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                            # ÊèêÁ§∫ËØçÂ∫ì‰∏éSchemaËØ¥Êòé
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py                      # Ê®°ÊùøÈÄâÊã©/Â∏ÉÂ±Ä/ÁØáÂπÖ/Á´†ËäÇÊèêÁ§∫ËØç
‚îÇ   ‚îú‚îÄ‚îÄ renderers/                          # IRÊ∏≤ÊüìÂô®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ html_renderer.py                # Document IR‚Üí‰∫§‰∫íÂºèHTML
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_renderer.py                 # HTML‚ÜíPDFÂØºÂá∫ÔºàWeasyPrintÔºâ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_layout_optimizer.py         # PDFÂ∏ÉÂ±Ä‰ºòÂåñÂô®
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chart_to_svg.py                 # ÂõæË°®ËΩ¨SVGÂ∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ state/                              # ‰ªªÂä°/ÂÖÉÊï∞ÊçÆÁä∂ÊÄÅÊ®°Âûã
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state.py                        # ReportState‰∏éÂ∫èÂàóÂåñÂ∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ utils/                              # ÈÖçÁΩÆ‰∏éËæÖÂä©Â∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                       # Pydantic Settings‰∏éÊâìÂç∞Âä©Êâã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency_check.py             # ‰æùËµñÊ£ÄÊü•Â∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ json_parser.py                  # JSONËß£ÊûêÂ∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chart_validator.py              # ÂõæË°®Ê†°È™åÂ∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chart_repair_api.py             # ÂõæË°®‰øÆÂ§çAPI
‚îÇ   ‚îú‚îÄ‚îÄ report_template/                    # MarkdownÊ®°ÊùøÂ∫ì
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ‰ºÅ‰∏öÂìÅÁâåÂ£∞Ë™âÂàÜÊûêÊä•Âëä.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ ForumEngine/                            # ËÆ∫ÂùõÂºïÊìéÔºöAgentÂçè‰ΩúÊú∫Âà∂
‚îÇ   ‚îú‚îÄ‚îÄ monitor.py                          # Êó•ÂøóÁõëÊéßÂíåËÆ∫ÂùõÁÆ°ÁêÜÊ†∏ÂøÉ
‚îÇ   ‚îú‚îÄ‚îÄ llm_host.py                         # ËÆ∫Âùõ‰∏ªÊåÅ‰∫∫LLMÊ®°Âùó
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ MindSpider/                             # Á§æ‰∫§Â™í‰ΩìÁà¨Ëô´Á≥ªÁªü
‚îÇ   ‚îú‚îÄ‚îÄ main.py                             # Áà¨Ëô´‰∏ªÁ®ãÂ∫èÂÖ•Âè£
‚îÇ   ‚îú‚îÄ‚îÄ config.py                           # Áà¨Ëô´ÈÖçÁΩÆÊñá‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ BroadTopicExtraction/               # ËØùÈ¢òÊèêÂèñÊ®°Âùó
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                         # ËØùÈ¢òÊèêÂèñ‰∏ªÁ®ãÂ∫è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database_manager.py             # Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂô®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ get_today_news.py               # ‰ªäÊó•Êñ∞ÈóªËé∑Âèñ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ topic_extractor.py              # ËØùÈ¢òÊèêÂèñÂô®
‚îÇ   ‚îú‚îÄ‚îÄ DeepSentimentCrawling/              # Ê∑±Â∫¶ËàÜÊÉÖÁà¨ÂèñÊ®°Âùó
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                         # Ê∑±Â∫¶Áà¨Âèñ‰∏ªÁ®ãÂ∫è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keyword_manager.py              # ÂÖ≥ÈîÆËØçÁÆ°ÁêÜÂô®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ platform_crawler.py             # Âπ≥Âè∞Áà¨Ëô´ÁÆ°ÁêÜ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MediaCrawler/                   # Á§æÂ™íÁà¨Ëô´Ê†∏ÂøÉ
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ config/                     # ÂêÑÂπ≥Âè∞ÈÖçÁΩÆ
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ media_platform/             # ÂêÑÂπ≥Âè∞Áà¨Ëô´ÂÆûÁé∞
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ schema/                             # Êï∞ÊçÆÂ∫ìÁªìÊûÑÂÆö‰πâ
‚îÇ       ‚îú‚îÄ‚îÄ db_manager.py                   # Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂô®
‚îÇ       ‚îú‚îÄ‚îÄ init_database.py                # Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñËÑöÊú¨
‚îÇ       ‚îú‚îÄ‚îÄ mindspider_tables.sql           # Êï∞ÊçÆÂ∫ìË°®ÁªìÊûÑSQL
‚îÇ       ‚îú‚îÄ‚îÄ models_bigdata.py               # Â§ßËßÑÊ®°Â™í‰ΩìËàÜÊÉÖË°®ÁöÑSQLAlchemyÊò†Â∞Ñ
‚îÇ       ‚îî‚îÄ‚îÄ models_sa.py                    # DailyTopic/TaskÁ≠âÊâ©Â±ïË°®ORMÊ®°Âûã
‚îú‚îÄ‚îÄ SentimentAnalysisModel/                 # ÊÉÖÊÑüÂàÜÊûêÊ®°ÂûãÈõÜÂêà
‚îÇ   ‚îú‚îÄ‚îÄ WeiboSentiment_Finetuned/           # ÂæÆË∞ÉBERT/GPT-2Ê®°Âûã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BertChinese-Lora/               # BERT‰∏≠ÊñáLoRAÂæÆË∞É
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ GPT2-Lora/                      # GPT-2 LoRAÂæÆË∞É
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ WeiboMultilingualSentiment/         # Â§öËØ≠Ë®ÄÊÉÖÊÑüÂàÜÊûê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ WeiboSentiment_SmallQwen/           # Â∞èÂèÇÊï∞Qwen3ÂæÆË∞É
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict_universal.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ WeiboSentiment_MachineLearning/     # ‰º†ÁªüÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ï
‚îÇ       ‚îú‚îÄ‚îÄ train.py
‚îÇ       ‚îú‚îÄ‚îÄ predict.py
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ SingleEngineApp/                        # ÂçïÁã¨AgentÁöÑStreamlitÂ∫îÁî®
‚îÇ   ‚îú‚îÄ‚îÄ query_engine_streamlit_app.py       # QueryEngineÁã¨Á´ãÂ∫îÁî®
‚îÇ   ‚îú‚îÄ‚îÄ media_engine_streamlit_app.py       # MediaEngineÁã¨Á´ãÂ∫îÁî®
‚îÇ   ‚îî‚îÄ‚îÄ insight_engine_streamlit_app.py     # InsightEngineÁã¨Á´ãÂ∫îÁî®
‚îú‚îÄ‚îÄ query_engine_streamlit_reports/         # QueryEngineÂçïÂ∫îÁî®ËøêË°åËæìÂá∫
‚îú‚îÄ‚îÄ media_engine_streamlit_reports/         # MediaEngineÂçïÂ∫îÁî®ËøêË°åËæìÂá∫
‚îú‚îÄ‚îÄ insight_engine_streamlit_reports/       # InsightEngineÂçïÂ∫îÁî®ËøêË°åËæìÂá∫
‚îú‚îÄ‚îÄ templates/                              # FlaskÂâçÁ´ØÊ®°Êùø
‚îÇ   ‚îî‚îÄ‚îÄ index.html                          # ‰∏ªÁïåÈù¢HTML
‚îú‚îÄ‚îÄ static/                                 # ÈùôÊÄÅËµÑÊ∫ê
‚îÇ   ‚îî‚îÄ‚îÄ image/                              # ÂõæÁâáËµÑÊ∫ê
‚îÇ       ‚îú‚îÄ‚îÄ logo_compressed.png
‚îÇ       ‚îú‚îÄ‚îÄ framework.png
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ logs/                                   # ËøêË°åÊó•ÂøóÁõÆÂΩï
‚îú‚îÄ‚îÄ final_reports/                          # ÊúÄÁªàÁîüÊàêÁöÑÊä•ÂëäÊñá‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ ir/                                 # Êä•ÂëäIR JSONÊñá‰ª∂
‚îÇ   ‚îî‚îÄ‚îÄ *.html                              # ÊúÄÁªàHTMLÊä•Âëä
‚îú‚îÄ‚îÄ utils/                                  # ÈÄöÁî®Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îú‚îÄ‚îÄ forum_reader.py                     # AgentÈó¥ËÆ∫ÂùõÈÄö‰ø°Â∑•ÂÖ∑
‚îÇ   ‚îú‚îÄ‚îÄ github_issues.py                    # Áªü‰∏ÄÁîüÊàêGitHub IssueÈìæÊé•‰∏éÈîôËØØÊèêÁ§∫
‚îÇ   ‚îî‚îÄ‚îÄ retry_helper.py                     # ÁΩëÁªúËØ∑Ê±ÇÈáçËØïÊú∫Âà∂Â∑•ÂÖ∑
‚îú‚îÄ‚îÄ tests/                                  # ÂçïÂÖÉÊµãËØï‰∏éÈõÜÊàêÊµãËØï
‚îÇ   ‚îú‚îÄ‚îÄ run_tests.py                        # pytestÂÖ•Âè£ËÑöÊú¨
‚îÇ   ‚îú‚îÄ‚îÄ test_monitor.py                     # ForumEngineÁõëÊéßÂçïÂÖÉÊµãËØï
‚îÇ   ‚îú‚îÄ‚îÄ test_report_engine_sanitization.py  # ReportEngineÂÆâÂÖ®ÊÄßÊµãËØï
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ app.py                                  # Flask‰∏ªÂ∫îÁî®ÂÖ•Âè£
‚îú‚îÄ‚îÄ config.py                               # ÂÖ®Â±ÄÈÖçÁΩÆÊñá‰ª∂
‚îú‚îÄ‚îÄ .env.example                            # ÁéØÂ¢ÉÂèòÈáèÁ§∫‰æãÊñá‰ª∂
‚îú‚îÄ‚îÄ docker-compose.yml                      # DockerÂ§öÊúçÂä°ÁºñÊéíÈÖçÁΩÆ
‚îú‚îÄ‚îÄ Dockerfile                              # DockerÈïúÂÉèÊûÑÂª∫Êñá‰ª∂
‚îú‚îÄ‚îÄ requirements.txt                        # Python‰æùËµñÂåÖÊ∏ÖÂçï
‚îú‚îÄ‚îÄ regenerate_latest_pdf.py                # PDFÈáçÊñ∞ÁîüÊàêÂ∑•ÂÖ∑ËÑöÊú¨
‚îú‚îÄ‚îÄ report_engine_only.py                   # Report EngineÂëΩ‰ª§Ë°åÁâàÊú¨
‚îú‚îÄ‚îÄ README.md                               # ‰∏≠ÊñáËØ¥ÊòéÊñáÊ°£
‚îú‚îÄ‚îÄ README-EN.md                            # Ëã±ÊñáËØ¥ÊòéÊñáÊ°£
‚îú‚îÄ‚îÄ CONTRIBUTING.md                         # ‰∏≠ÊñáË¥°ÁåÆÊåáÂçó
‚îú‚îÄ‚îÄ CONTRIBUTING-EN.md                      # Ëã±ÊñáË¥°ÁåÆÊåáÂçó
‚îî‚îÄ‚îÄ LICENSE                                 # GPL-2.0ÂºÄÊ∫êËÆ∏ÂèØËØÅ
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üöÄ Âø´ÈÄüÂºÄÂßãÔºàDockerÔºâ&lt;/h2&gt; 
&lt;h3&gt;1. ÂêØÂä®È°πÁõÆ&lt;/h3&gt; 
&lt;p&gt;Â§çÂà∂‰∏Ä‰ªΩ &lt;code&gt;.env.example&lt;/code&gt; Êñá‰ª∂ÔºåÂëΩÂêç‰∏∫ &lt;code&gt;.env&lt;/code&gt; ÔºåÂπ∂ÊåâÈúÄÈÖçÁΩÆ &lt;code&gt;.env&lt;/code&gt; Êñá‰ª∂‰∏≠ÁöÑÁéØÂ¢ÉÂèòÈáè&lt;/p&gt; 
&lt;p&gt;ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Âú®ÂêéÂè∞ÂêØÂä®ÊâÄÊúâÊúçÂä°Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Ê≥®ÔºöÈïúÂÉèÊãâÂèñÈÄüÂ∫¶ÊÖ¢&lt;/strong&gt;ÔºåÂú®Âéü &lt;code&gt;docker-compose.yml&lt;/code&gt; Êñá‰ª∂‰∏≠ÔºåÊàë‰ª¨Â∑≤ÁªèÈÄöËøá&lt;strong&gt;Ê≥®Èáä&lt;/strong&gt;ÁöÑÊñπÂºèÊèê‰æõ‰∫ÜÂ§áÁî®ÈïúÂÉèÂú∞ÂùÄ‰æõÊÇ®ÊõøÊç¢&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;2. ÈÖçÁΩÆËØ¥Êòé&lt;/h3&gt; 
&lt;h4&gt;Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàPostgreSQLÔºâ&lt;/h4&gt; 
&lt;p&gt;ËØ∑ÊåâÁÖß‰ª•‰∏ãÂèÇÊï∞ÈÖçÁΩÆÊï∞ÊçÆÂ∫ìËøûÊé•‰ø°ÊÅØÔºå‰πüÊîØÊåÅMysqlÂèØËá™Ë°å‰øÆÊîπÔºö&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;ÈÖçÁΩÆÈ°π&lt;/th&gt; 
   &lt;th align="left"&gt;Â°´ÂÜôÂÄº&lt;/th&gt; 
   &lt;th align="left"&gt;ËØ¥Êòé&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DB_HOST&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;db&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Êï∞ÊçÆÂ∫ìÊúçÂä°ÂêçÁß∞ (ÂØπÂ∫î &lt;code&gt;docker-compose.yml&lt;/code&gt; ‰∏≠ÁöÑÊúçÂä°Âêç)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DB_PORT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;5432&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;ÈªòËÆ§ PostgreSQL Á´ØÂè£&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DB_USER&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;bettafish&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Êï∞ÊçÆÂ∫ìÁî®Êà∑Âêç&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DB_PASSWORD&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;bettafish&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Êï∞ÊçÆÂ∫ìÂØÜÁ†Å&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DB_NAME&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;bettafish&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Êï∞ÊçÆÂ∫ìÂêçÁß∞&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;ÂÖ∂‰ªñ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;‰øùÊåÅÈªòËÆ§&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Êï∞ÊçÆÂ∫ìËøûÊé•Ê±†Á≠âÂÖ∂‰ªñÂèÇÊï∞ËØ∑‰øùÊåÅÈªòËÆ§ËÆæÁΩÆ„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Â§ßÊ®°ÂûãÈÖçÁΩÆ&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Êàë‰ª¨ÊâÄÊúâ LLM Ë∞ÉÁî®‰ΩøÁî® OpenAI ÁöÑ API Êé•Âè£Ê†áÂáÜ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Âú®ÂÆåÊàêÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÂêéÔºåËØ∑Ê≠£Â∏∏ÈÖçÁΩÆ&lt;strong&gt;ÊâÄÊúâÂ§ßÊ®°ÂûãÁõ∏ÂÖ≥ÁöÑÂèÇÊï∞&lt;/strong&gt;ÔºåÁ°Æ‰øùÁ≥ªÁªüËÉΩÂ§üËøûÊé•Âà∞ÊÇ®ÈÄâÊã©ÁöÑÂ§ßÊ®°ÂûãÊúçÂä°„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÂÆåÊàê‰∏äËø∞ÊâÄÊúâÈÖçÁΩÆÂπ∂‰øùÂ≠òÂêéÔºåÁ≥ªÁªüÂç≥ÂèØÊ≠£Â∏∏ËøêË°å„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üîß Ê∫êÁ†ÅÂêØÂä®ÊåáÂçó&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Â¶ÇÊûú‰Ω†ÊòØÂàùÊ¨°Â≠¶‰π†‰∏Ä‰∏™AgentÁ≥ªÁªüÁöÑÊê≠Âª∫ÔºåÂèØ‰ª•‰ªé‰∏Ä‰∏™ÈùûÂ∏∏ÁÆÄÂçïÁöÑdemoÂºÄÂßãÔºö&lt;a href="https://github.com/666ghj/DeepSearchAgent-Demo"&gt;Deep Search Agent Demo&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ÁéØÂ¢ÉË¶ÅÊ±Ç&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Êìç‰ΩúÁ≥ªÁªü&lt;/strong&gt;: Windows„ÄÅLinux„ÄÅMacOS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PythonÁâàÊú¨&lt;/strong&gt;: 3.9+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt;: AnacondaÊàñMiniconda&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êï∞ÊçÆÂ∫ì&lt;/strong&gt;: PostgreSQLÔºàÊé®ËçêÔºâÊàñMySQL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÜÖÂ≠ò&lt;/strong&gt;: Âª∫ËÆÆ2GB‰ª•‰∏ä&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. ÂàõÂª∫ÁéØÂ¢É&lt;/h3&gt; 
&lt;h4&gt;Â¶ÇÊûú‰ΩøÁî®Conda&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂàõÂª∫condaÁéØÂ¢É
conda create -n your_conda_name python=3.11
conda activate your_conda_name
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Â¶ÇÊûú‰ΩøÁî®uv&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂàõÂª∫uvÁéØÂ¢É
uv venv --python 3.11 # ÂàõÂª∫3.11ÁéØÂ¢É
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. ÂÆâË£Ö PDF ÂØºÂá∫ÊâÄÈúÄÁ≥ªÁªü‰æùËµñÔºàÂèØÈÄâÔºâ&lt;/h3&gt; 
&lt;p&gt;ËøôÈÉ®ÂàÜÊúâËØ¶ÁªÜÁöÑÈÖçÁΩÆËØ¥ÊòéÔºö&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/Partial%20README%20for%20PDF%20Exporting/README.md"&gt;ÈÖçÁΩÆÊâÄÈúÄ‰æùËµñ&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;3. ÂÆâË£Ö‰æùËµñÂåÖ&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Â¶ÇÊûúË∑≥Ëøá‰∫ÜÊ≠•È™§2ÔºåweasyprintÂ∫ìÂèØËÉΩÊó†Ê≥ïÂÆâË£ÖÔºåPDFÂäüËÉΩÂèØËÉΩÊó†Ê≥ïÊ≠£Â∏∏‰ΩøÁî®„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Âü∫Á°Ä‰æùËµñÂÆâË£Ö
pip install -r requirements.txt

# uvÁâàÊú¨ÂëΩ‰ª§ÔºàÊõ¥Âø´ÈÄüÂÆâË£ÖÔºâ
uv pip install -r requirements.txt
# Â¶ÇÊûú‰∏çÊÉ≥‰ΩøÁî®Êú¨Âú∞ÊÉÖÊÑüÂàÜÊûêÊ®°ÂûãÔºàÁÆóÂäõÈúÄÊ±ÇÂæàÂ∞èÔºåÈªòËÆ§ÂÆâË£ÖcpuÁâàÊú¨ÔºâÔºåÂèØ‰ª•Â∞ÜËØ•Êñá‰ª∂‰∏≠ÁöÑ"Êú∫Âô®Â≠¶‰π†"ÈÉ®ÂàÜÊ≥®ÈáäÊéâÂÜçÊâßË°åÊåá‰ª§
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. ÂÆâË£ÖPlaywrightÊµèËßàÂô®È©±Âä®&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂÆâË£ÖÊµèËßàÂô®È©±Âä®ÔºàÁî®‰∫éÁà¨Ëô´ÂäüËÉΩÔºâ
playwright install chromium
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;5. ÈÖçÁΩÆLLM‰∏éÊï∞ÊçÆÂ∫ì&lt;/h3&gt; 
&lt;p&gt;Â§çÂà∂‰∏Ä‰ªΩÈ°πÁõÆÊ†πÁõÆÂΩï &lt;code&gt;.env.example&lt;/code&gt; Êñá‰ª∂ÔºåÂëΩÂêç‰∏∫ &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;ÁºñËæë &lt;code&gt;.env&lt;/code&gt; Êñá‰ª∂ÔºåÂ°´ÂÖ•ÊÇ®ÁöÑAPIÂØÜÈí•ÔºàÊÇ®‰πüÂèØ‰ª•ÈÄâÊã©Ëá™Â∑±ÁöÑÊ®°Âûã„ÄÅÊêúÁ¥¢‰ª£ÁêÜÔºåËØ¶ÊÉÖËßÅÊ†πÁõÆÂΩï.env.exampleÊñá‰ª∂ÂÜÖÊàñÊ†πÁõÆÂΩïconfig.py‰∏≠ÁöÑËØ¥ÊòéÔºâÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yml"&gt;# ====================== Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ ======================
# Êï∞ÊçÆÂ∫ì‰∏ªÊú∫Ôºå‰æãÂ¶Çlocalhost Êàñ 127.0.0.1
DB_HOST=your_db_host
# Êï∞ÊçÆÂ∫ìÁ´ØÂè£Âè∑ÔºåÈªòËÆ§‰∏∫3306
DB_PORT=3306
# Êï∞ÊçÆÂ∫ìÁî®Êà∑Âêç
DB_USER=your_db_user
# Êï∞ÊçÆÂ∫ìÂØÜÁ†Å
DB_PASSWORD=your_db_password
# Êï∞ÊçÆÂ∫ìÂêçÁß∞
DB_NAME=your_db_name
# Êï∞ÊçÆÂ∫ìÂ≠óÁ¨¶ÈõÜÔºåÊé®Ëçêutf8mb4ÔºåÂÖºÂÆπemoji
DB_CHARSET=utf8mb4
# Êï∞ÊçÆÂ∫ìÁ±ªÂûãpostgresqlÊàñmysql
DB_DIALECT=postgresql
# Êï∞ÊçÆÂ∫ì‰∏çÈúÄË¶ÅÂàùÂßãÂåñÔºåÊâßË°åapp.pyÊó∂‰ºöËá™Âä®Ê£ÄÊµã

# ====================== LLMÈÖçÁΩÆ ======================
# ÊÇ®ÂèØ‰ª•Êõ¥ÊîπÊØè‰∏™ÈÉ®ÂàÜLLM‰ΩøÁî®ÁöÑAPIÔºåÂè™Ë¶ÅÂÖºÂÆπOpenAIËØ∑Ê±ÇÊ†ºÂºèÈÉΩÂèØ‰ª•
# ÈÖçÁΩÆÊñá‰ª∂ÂÜÖÈÉ®Áªô‰∫ÜÊØè‰∏Ä‰∏™AgentÁöÑÊé®ËçêLLMÔºåÂàùÊ¨°ÈÉ®ÁΩ≤ËØ∑ÂÖàÂèÇËÄÉÊé®ËçêËÆæÁΩÆ

# Insight Agent
INSIGHT_ENGINE_API_KEY=
INSIGHT_ENGINE_BASE_URL=
INSIGHT_ENGINE_MODEL_NAME=

# Media Agent
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6. ÂêØÂä®Á≥ªÁªü&lt;/h3&gt; 
&lt;h4&gt;6.1 ÂÆåÊï¥Á≥ªÁªüÂêØÂä®ÔºàÊé®ËçêÔºâ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÔºåÊøÄÊ¥ªcondaÁéØÂ¢É
conda activate your_conda_name

# ÂêØÂä®‰∏ªÂ∫îÁî®Âç≥ÂèØ
python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;uv ÁâàÊú¨ÂêØÂä®ÂëΩ‰ª§&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÔºåÊøÄÊ¥ªuvÁéØÂ¢É
.venv\Scripts\activate

# ÂêØÂä®‰∏ªÂ∫îÁî®Âç≥ÂèØ
python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®1Ôºö‰∏ÄÊ¨°ËøêË°åÁªàÊ≠¢ÂêéÔºåstreamlit appÂèØËÉΩÁªìÊùüÂºÇÂ∏∏‰ªçÁÑ∂Âç†Áî®Á´ØÂè£ÔºåÊ≠§Êó∂ÊêúÁ¥¢Âç†Áî®Á´ØÂè£ÁöÑËøõÁ®ãkillÊéâÂç≥ÂèØ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®2ÔºöÊï∞ÊçÆÁà¨ÂèñÈúÄË¶ÅÂçïÁã¨Êìç‰ΩúÔºåËßÅ6.3ÊåáÂºï&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ËÆøÈóÆ &lt;a href="http://localhost:5000"&gt;http://localhost:5000&lt;/a&gt; Âç≥ÂèØ‰ΩøÁî®ÂÆåÊï¥Á≥ªÁªü&lt;/p&gt; 
&lt;h4&gt;6.2 ÂçïÁã¨ÂêØÂä®Êüê‰∏™Agent&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂêØÂä®QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# ÂêØÂä®MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# ÂêØÂä®InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;6.3 Áà¨Ëô´Á≥ªÁªüÂçïÁã¨‰ΩøÁî®&lt;/h4&gt; 
&lt;p&gt;ËøôÈÉ®ÂàÜÊúâËØ¶ÁªÜÁöÑÈÖçÁΩÆÊñáÊ°£Ôºö&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/MindSpider/README.md"&gt;MindSpider‰ΩøÁî®ËØ¥Êòé&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="MindSpider\img\example.png" alt="banner" width="600" /&gt; 
 &lt;p&gt;MindSpider ËøêË°åÁ§∫‰æã&lt;/p&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ËøõÂÖ•Áà¨Ëô´ÁõÆÂΩï
cd MindSpider

# È°πÁõÆÂàùÂßãÂåñ
python main.py --setup

# ËøêË°åËØùÈ¢òÊèêÂèñÔºàËé∑ÂèñÁÉ≠ÁÇπÊñ∞ÈóªÂíåÂÖ≥ÈîÆËØçÔºâ
python main.py --broad-topic

# ËøêË°åÂÆåÊï¥Áà¨Ëô´ÊµÅÁ®ã
python main.py --complete --date 2024-01-20

# ‰ªÖËøêË°åËØùÈ¢òÊèêÂèñ
python main.py --broad-topic --date 2024-01-20

# ‰ªÖËøêË°åÊ∑±Â∫¶Áà¨Âèñ
python main.py --deep-sentiment --platforms xhs dy wb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;6.4 ÂëΩ‰ª§Ë°åÊä•ÂëäÁîüÊàêÂ∑•ÂÖ∑&lt;/h4&gt; 
&lt;p&gt;ËØ•Â∑•ÂÖ∑‰ºöË∑≥Ëøá‰∏â‰∏™ÂàÜÊûêÂºïÊìéÁöÑËøêË°åÈò∂ÊÆµÔºåÁõ¥Êé•ËØªÂèñÂÆÉ‰ª¨ÁöÑÊúÄÊñ∞Êó•ÂøóÊñá‰ª∂ÔºåÂπ∂Âú®Êó†ÈúÄ Web ÁïåÈù¢ÁöÑÊÉÖÂÜµ‰∏ãÁîüÊàêÁªºÂêàÊä•ÂëäÔºàÂêåÊó∂ÁúÅÁï•Êñá‰ª∂Â¢ûÈáèÊ†°È™åÊ≠•È™§Ôºâ„ÄÇÈÄöÂ∏∏Áî®‰∫éÂØπÊä•ÂëäÁîüÊàêÁªìÊûú‰∏çÊª°ÊÑè„ÄÅÈúÄË¶ÅÂø´ÈÄüÈáçËØïÁöÑÂú∫ÊôØÔºåÊàñÂú®Ë∞ÉËØï Report Engine Êó∂ÂêØÁî®„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Âü∫Êú¨‰ΩøÁî®ÔºàËá™Âä®‰ªéÊñá‰ª∂ÂêçÊèêÂèñ‰∏ªÈ¢òÔºâ
python report_engine_only.py

# ÊåáÂÆöÊä•Âëä‰∏ªÈ¢ò
python report_engine_only.py --query "ÂúüÊú®Â∑•Á®ãË°å‰∏öÂàÜÊûê"

# Ë∑≥ËøáPDFÁîüÊàêÔºàÂç≥‰ΩøÁ≥ªÁªüÊîØÊåÅÔºâ
python report_engine_only.py --skip-pdf

# ÊòæÁ§∫ËØ¶ÁªÜÊó•Âøó
python report_engine_only.py --verbose

# Êü•ÁúãÂ∏ÆÂä©‰ø°ÊÅØ
python report_engine_only.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ÂäüËÉΩËØ¥ÊòéÔºö&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Ëá™Âä®Ê£ÄÊü•‰æùËµñ&lt;/strong&gt;ÔºöÁ®ãÂ∫è‰ºöËá™Âä®Ê£ÄÊü•PDFÁîüÊàêÊâÄÈúÄÁöÑÁ≥ªÁªü‰æùËµñÔºåÂ¶ÇÊûúÁº∫Â§±‰ºöÁªôÂá∫ÂÆâË£ÖÊèêÁ§∫&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ëé∑ÂèñÊúÄÊñ∞Êñá‰ª∂&lt;/strong&gt;ÔºöËá™Âä®‰ªé‰∏â‰∏™ÂºïÊìéÁõÆÂΩïÔºà&lt;code&gt;insight_engine_streamlit_reports&lt;/code&gt;„ÄÅ&lt;code&gt;media_engine_streamlit_reports&lt;/code&gt;„ÄÅ&lt;code&gt;query_engine_streamlit_reports&lt;/code&gt;ÔºâËé∑ÂèñÊúÄÊñ∞ÁöÑÂàÜÊûêÊä•Âëä&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êñá‰ª∂Á°ÆËÆ§&lt;/strong&gt;ÔºöÊòæÁ§∫ÊâÄÊúâÈÄâÊã©ÁöÑÊñá‰ª∂Âêç„ÄÅË∑ØÂæÑÂíå‰øÆÊîπÊó∂Èó¥ÔºåÁ≠âÂæÖÁî®Êà∑Á°ÆËÆ§ÔºàÈªòËÆ§ËæìÂÖ• &lt;code&gt;y&lt;/code&gt; ÁªßÁª≠ÔºåËæìÂÖ• &lt;code&gt;n&lt;/code&gt; ÈÄÄÂá∫Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Áõ¥Êé•ÁîüÊàêÊä•Âëä&lt;/strong&gt;ÔºöË∑≥ËøáÊñá‰ª∂Â¢ûÂä†ÂÆ°Ê†∏Á®ãÂ∫èÔºåÁõ¥Êé•Ë∞ÉÁî®Report EngineÁîüÊàêÁªºÂêàÊä•Âëä&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ëá™Âä®‰øùÂ≠òÊñá‰ª∂&lt;/strong&gt;Ôºö 
  &lt;ul&gt; 
   &lt;li&gt;HTMLÊä•Âëä‰øùÂ≠òÂà∞ &lt;code&gt;final_reports/&lt;/code&gt; ÁõÆÂΩï&lt;/li&gt; 
   &lt;li&gt;PDFÊä•ÂëäÔºàÂ¶ÇÊûúÊúâ‰æùËµñÔºâ‰øùÂ≠òÂà∞ &lt;code&gt;final_reports/pdf/&lt;/code&gt; ÁõÆÂΩï&lt;/li&gt; 
   &lt;li&gt;Êñá‰ª∂ÂëΩÂêçÊ†ºÂºèÔºö&lt;code&gt;final_report_{‰∏ªÈ¢ò}_{Êó∂Èó¥Êà≥}.html/pdf&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Ê≥®ÊÑè‰∫ãÈ°πÔºö&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Á°Æ‰øù‰∏â‰∏™ÂºïÊìéÁõÆÂΩï‰∏≠Ëá≥Â∞ëÊúâ‰∏Ä‰∏™ÂåÖÂê´&lt;code&gt;.md&lt;/code&gt;Êä•ÂëäÊñá‰ª∂&lt;/li&gt; 
 &lt;li&gt;ÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑‰∏éWebÁïåÈù¢Áõ∏‰∫íÁã¨Á´ãÔºå‰∏ç‰ºöÁõ∏‰∫íÂΩ±Âìç&lt;/li&gt; 
 &lt;li&gt;PDFÁîüÊàêÈúÄË¶ÅÂÆâË£ÖÁ≥ªÁªü‰æùËµñÔºåËØ¶ËßÅ‰∏äÊñá"ÂÆâË£Ö PDF ÂØºÂá∫ÊâÄÈúÄÁ≥ªÁªü‰æùËµñ"ÈÉ®ÂàÜ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚öôÔ∏è È´òÁ∫ßÈÖçÁΩÆÔºàÂ∑≤ËøáÊó∂ÔºåÂ∑≤ÁªèÁªü‰∏Ä‰∏∫È°πÁõÆÊ†πÁõÆÂΩï.envÊñá‰ª∂ÁÆ°ÁêÜÔºåÂÖ∂‰ªñÂ≠êagentËá™Âä®ÁªßÊâøÊ†πÁõÆÂΩïÈÖçÁΩÆÔºâ&lt;/h2&gt; 
&lt;h3&gt;‰øÆÊîπÂÖ≥ÈîÆÂèÇÊï∞&lt;/h3&gt; 
&lt;h4&gt;AgentÈÖçÁΩÆÂèÇÊï∞&lt;/h4&gt; 
&lt;p&gt;ÊØè‰∏™AgentÈÉΩÊúâ‰∏ìÈó®ÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÂèØÊ†πÊçÆÈúÄÊ±ÇË∞ÉÊï¥Ôºå‰∏ãÈù¢ÊòØÈÉ®ÂàÜÁ§∫‰æãÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # ÂèçÊÄùËΩÆÊ¨°
    max_search_results = 15       # ÊúÄÂ§ßÊêúÁ¥¢ÁªìÊûúÊï∞
    max_content_length = 8000     # ÊúÄÂ§ßÂÜÖÂÆπÈïøÂ∫¶
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ÁªºÂêàÊêúÁ¥¢ÈôêÂà∂
    web_search_limit = 15           # ÁΩëÈ°µÊêúÁ¥¢ÈôêÂà∂
    
# InsightEngine/utils/config.py
class Config:
    default_search_topic_globally_limit = 200    # ÂÖ®Â±ÄÊêúÁ¥¢ÈôêÂà∂
    default_get_comments_limit = 500             # ËØÑËÆ∫Ëé∑ÂèñÈôêÂà∂
    max_search_results_for_llm = 50              # ‰º†ÁªôLLMÁöÑÊúÄÂ§ßÁªìÊûúÊï∞
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ÊÉÖÊÑüÂàÜÊûêÊ®°ÂûãÈÖçÁΩÆ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/tools/sentiment_analyzer.py
SENTIMENT_CONFIG = {
    'model_type': 'multilingual',     # ÂèØÈÄâ: 'bert', 'multilingual', 'qwen'Á≠â
    'confidence_threshold': 0.8,      # ÁΩÆ‰ø°Â∫¶ÈòàÂÄº
    'batch_size': 32,                 # ÊâπÂ§ÑÁêÜÂ§ßÂ∞è
    'max_sequence_length': 512,       # ÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Êé•ÂÖ•‰∏çÂêåÁöÑLLMÊ®°Âûã&lt;/h3&gt; 
&lt;p&gt;ÊîØÊåÅ‰ªªÊÑèopenAIË∞ÉÁî®Ê†ºÂºèÁöÑLLMÊèê‰æõÂïÜÔºåÂè™ÈúÄË¶ÅÂú®/config.py‰∏≠Â°´ÂÜôÂØπÂ∫îÁöÑKEY„ÄÅBASE_URL„ÄÅMODEL_NAMEÂç≥ÂèØ„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‰ªÄ‰πàÊòØopenAIË∞ÉÁî®Ê†ºÂºèÔºü‰∏ãÈù¢Êèê‰æõ‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‰æãÂ≠êÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI(api_key="your_api_key", 
               base_url="https://api.siliconflow.cn/v1")

response = client.chat.completions.create(
   model="Qwen/Qwen2.5-72B-Instruct",
   messages=[
       {'role': 'user', 
        'content': "Êé®ÁêÜÊ®°Âûã‰ºöÁªôÂ∏ÇÂú∫Â∏¶Êù•Âì™‰∫õÊñ∞ÁöÑÊú∫‰ºö"}
   ],
)

complete_response = response.choices[0].message.content
print(complete_response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Êõ¥ÊîπÊÉÖÊÑüÂàÜÊûêÊ®°Âûã&lt;/h3&gt; 
&lt;p&gt;Á≥ªÁªüÈõÜÊàê‰∫ÜÂ§öÁßçÊÉÖÊÑüÂàÜÊûêÊñπÊ≥ïÔºåÂèØÊ†πÊçÆÈúÄÊ±ÇÈÄâÊã©Ôºö&lt;/p&gt; 
&lt;h4&gt;1. Â§öËØ≠Ë®ÄÊÉÖÊÑüÂàÜÊûê&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboMultilingualSentiment
python predict.py --text "This product is amazing!" --lang "en"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Â∞èÂèÇÊï∞Qwen3ÂæÆË∞É&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_SmallQwen
python predict_universal.py --text "ËøôÊ¨°Ê¥ªÂä®ÂäûÂæóÂæàÊàêÂäü"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Âü∫‰∫éBERTÁöÑÂæÆË∞ÉÊ®°Âûã&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ‰ΩøÁî®BERT‰∏≠ÊñáÊ®°Âûã
cd SentimentAnalysisModel/WeiboSentiment_Finetuned/BertChinese-Lora
python predict.py --text "Ëøô‰∏™‰∫ßÂìÅÁúüÁöÑÂæà‰∏çÈîô"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. GPT-2 LoRAÂæÆË∞ÉÊ®°Âûã&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_Finetuned/GPT2-Lora
python predict.py --text "‰ªäÂ§©ÂøÉÊÉÖ‰∏çÂ§™Â•Ω"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. ‰º†ÁªüÊú∫Âô®Â≠¶‰π†ÊñπÊ≥ï&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_MachineLearning
python predict.py --model_type "svm" --text "ÊúçÂä°ÊÄÅÂ∫¶ÈúÄË¶ÅÊîπËøõ"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Êé•ÂÖ•Ëá™ÂÆö‰πâ‰∏öÂä°Êï∞ÊçÆÂ∫ì&lt;/h3&gt; 
&lt;h4&gt;1. ‰øÆÊîπÊï∞ÊçÆÂ∫ìËøûÊé•ÈÖçÁΩÆ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# config.py ‰∏≠Ê∑ªÂä†ÊÇ®ÁöÑ‰∏öÂä°Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ
BUSINESS_DB_HOST = "your_business_db_host"
BUSINESS_DB_PORT = 3306
BUSINESS_DB_USER = "your_business_user"
BUSINESS_DB_PASSWORD = "your_business_password"
BUSINESS_DB_NAME = "your_business_database"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. ÂàõÂª∫Ëá™ÂÆö‰πâÊï∞ÊçÆËÆøÈóÆÂ∑•ÂÖ∑&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/tools/custom_db_tool.py
class CustomBusinessDBTool:
    """Ëá™ÂÆö‰πâ‰∏öÂä°Êï∞ÊçÆÂ∫ìÊü•ËØ¢Â∑•ÂÖ∑"""
    
    def __init__(self):
        self.connection_config = {
            'host': config.BUSINESS_DB_HOST,
            'port': config.BUSINESS_DB_PORT,
            'user': config.BUSINESS_DB_USER,
            'password': config.BUSINESS_DB_PASSWORD,
            'database': config.BUSINESS_DB_NAME,
        }
    
    def search_business_data(self, query: str, table: str):
        """Êü•ËØ¢‰∏öÂä°Êï∞ÊçÆ"""
        # ÂÆûÁé∞ÊÇ®ÁöÑ‰∏öÂä°ÈÄªËæë
        pass
    
    def get_customer_feedback(self, product_id: str):
        """Ëé∑ÂèñÂÆ¢Êà∑ÂèçÈ¶àÊï∞ÊçÆ"""
        # ÂÆûÁé∞ÂÆ¢Êà∑ÂèçÈ¶àÊü•ËØ¢ÈÄªËæë
        pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. ÈõÜÊàêÂà∞InsightEngine&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/agent.py ‰∏≠ÈõÜÊàêËá™ÂÆö‰πâÂ∑•ÂÖ∑
from .tools.custom_db_tool import CustomBusinessDBTool

class DeepSearchAgent:
    def __init__(self, config=None):
        # ... ÂÖ∂‰ªñÂàùÂßãÂåñ‰ª£Á†Å
        self.custom_db_tool = CustomBusinessDBTool()
    
    def execute_custom_search(self, query: str):
        """ÊâßË°åËá™ÂÆö‰πâ‰∏öÂä°Êï∞ÊçÆÊêúÁ¥¢"""
        return self.custom_db_tool.search_business_data(query, "your_table")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ëá™ÂÆö‰πâÊä•ÂëäÊ®°Êùø&lt;/h3&gt; 
&lt;h4&gt;1. Âú®WebÁïåÈù¢‰∏≠‰∏ä‰º†&lt;/h4&gt; 
&lt;p&gt;Á≥ªÁªüÊîØÊåÅ‰∏ä‰º†Ëá™ÂÆö‰πâÊ®°ÊùøÊñá‰ª∂Ôºà.mdÊàñ.txtÊ†ºÂºèÔºâÔºåÂèØÂú®ÁîüÊàêÊä•ÂëäÊó∂ÈÄâÊã©‰ΩøÁî®„ÄÇ&lt;/p&gt; 
&lt;h4&gt;2. ÂàõÂª∫Ê®°ÊùøÊñá‰ª∂&lt;/h4&gt; 
&lt;p&gt;Âú® &lt;code&gt;ReportEngine/report_template/&lt;/code&gt; ÁõÆÂΩï‰∏ãÂàõÂª∫Êñ∞ÁöÑÊ®°ÊùøÔºåÊàë‰ª¨ÁöÑAgent‰ºöËá™Ë°åÈÄâÁî®ÊúÄÂêàÈÄÇÁöÑÊ®°Êùø„ÄÇ&lt;/p&gt; 
&lt;h2&gt;ü§ù Ë¥°ÁåÆÊåáÂçó&lt;/h2&gt; 
&lt;p&gt;Êàë‰ª¨Ê¨¢ËøéÊâÄÊúâÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºÅ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ËØ∑ÈòÖËØª‰ª•‰∏ãË¥°ÁåÆÊåáÂçóÔºö&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü¶ñ ‰∏ã‰∏ÄÊ≠•ÂºÄÂèëËÆ°Âàí&lt;/h2&gt; 
&lt;p&gt;Áé∞Âú®Á≥ªÁªüÂè™ÂÆåÊàê‰∫Ü"‰∏âÊùøÊñß"‰∏≠ÁöÑÂâç‰∏§Ê≠•ÔºåÂç≥ÔºöËæìÂÖ•Ë¶ÅÊ±Ç-&amp;gt;ËØ¶ÁªÜÂàÜÊûêÔºåËøòÁº∫Â∞ë‰∏ÄÊ≠•È¢ÑÊµãÔºåÁõ¥Êé•Â∞Ü‰ªñÁªßÁª≠‰∫§ÁªôLLMÊòØ‰∏çÂÖ∑ÊúâËØ¥ÊúçÂäõÁöÑ„ÄÇ&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/banner_compressed.png" alt="banner" width="800" /&gt; 
&lt;/div&gt; 
&lt;p&gt;ÁõÆÂâçÊàë‰ª¨ÁªèËøáÂæàÈïø‰∏ÄÊÆµÊó∂Èó¥ÁöÑÁà¨ÂèñÊî∂ÈõÜÔºåÊã•Êúâ‰∫ÜÂ§ßÈáèÂÖ®ÁΩëËØùÈ¢òÁÉ≠Â∫¶ÈöèÊó∂Èó¥„ÄÅÁàÜÁÇπÁ≠âÁöÑÂèòÂåñË∂ãÂäøÁÉ≠Â∫¶Êï∞ÊçÆÔºåÂ∑≤ÁªèÂÖ∑Â§á‰∫ÜÂèØ‰ª•ÂºÄÂèëÈ¢ÑÊµãÊ®°ÂûãÁöÑÊù°‰ª∂„ÄÇÊàë‰ª¨Âõ¢ÈòüÂ∞ÜËøêÁî®Êó∂Â∫èÊ®°Âûã„ÄÅÂõæÁ•ûÁªèÁΩëÁªú„ÄÅÂ§öÊ®°ÊÄÅËûçÂêàÁ≠âÈ¢ÑÊµãÊ®°ÂûãÊäÄÊúØÂÇ®Â§á‰∫éÊ≠§ÔºåÂÆûÁé∞ÁúüÊ≠£Âü∫‰∫éÊï∞ÊçÆÈ©±Âä®ÁöÑËàÜÊÉÖÈ¢ÑÊµãÂäüËÉΩ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;‚ö†Ô∏è ÂÖçË¥£Â£∞Êòé&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ÈáçË¶ÅÊèêÈÜíÔºöÊú¨È°πÁõÆ‰ªÖ‰æõÂ≠¶‰π†„ÄÅÂ≠¶ÊúØÁ†îÁ©∂ÂíåÊïôËÇ≤ÁõÆÁöÑ‰ΩøÁî®&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂêàËßÑÊÄßÂ£∞Êòé&lt;/strong&gt;Ôºö&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Êú¨È°πÁõÆ‰∏≠ÁöÑÊâÄÊúâ‰ª£Á†Å„ÄÅÂ∑•ÂÖ∑ÂíåÂäüËÉΩÂùá‰ªÖ‰æõÂ≠¶‰π†„ÄÅÂ≠¶ÊúØÁ†îÁ©∂ÂíåÊïôËÇ≤ÁõÆÁöÑ‰ΩøÁî®&lt;/li&gt; 
   &lt;li&gt;‰∏•Á¶ÅÂ∞ÜÊú¨È°πÁõÆÁî®‰∫é‰ªª‰ΩïÂïÜ‰∏öÁî®ÈÄîÊàñÁõàÂà©ÊÄßÊ¥ªÂä®&lt;/li&gt; 
   &lt;li&gt;‰∏•Á¶ÅÂ∞ÜÊú¨È°πÁõÆÁî®‰∫é‰ªª‰ΩïËøùÊ≥ï„ÄÅËøùËßÑÊàñ‰æµÁäØ‰ªñ‰∫∫ÊùÉÁõäÁöÑË°å‰∏∫&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Áà¨Ëô´ÂäüËÉΩÂÖçË¥£&lt;/strong&gt;Ôºö&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;È°πÁõÆ‰∏≠ÁöÑÁà¨Ëô´ÂäüËÉΩ‰ªÖÁî®‰∫éÊäÄÊúØÂ≠¶‰π†ÂíåÁ†îÁ©∂ÁõÆÁöÑ&lt;/li&gt; 
   &lt;li&gt;‰ΩøÁî®ËÄÖÂøÖÈ°ªÈÅµÂÆàÁõÆÊ†áÁΩëÁ´ôÁöÑrobots.txtÂçèËÆÆÂíå‰ΩøÁî®Êù°Ê¨æ&lt;/li&gt; 
   &lt;li&gt;‰ΩøÁî®ËÄÖÂøÖÈ°ªÈÅµÂÆàÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÔºå‰∏çÂæóËøõË°åÊÅ∂ÊÑèÁà¨ÂèñÊàñÊï∞ÊçÆÊª•Áî®&lt;/li&gt; 
   &lt;li&gt;Âõ†‰ΩøÁî®Áà¨Ëô´ÂäüËÉΩ‰∫ßÁîüÁöÑ‰ªª‰ΩïÊ≥ïÂæãÂêéÊûúÁî±‰ΩøÁî®ËÄÖËá™Ë°åÊâøÊãÖ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Êï∞ÊçÆ‰ΩøÁî®ÂÖçË¥£&lt;/strong&gt;Ôºö&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;È°πÁõÆÊ∂âÂèäÁöÑÊï∞ÊçÆÂàÜÊûêÂäüËÉΩ‰ªÖ‰æõÂ≠¶ÊúØÁ†îÁ©∂‰ΩøÁî®&lt;/li&gt; 
   &lt;li&gt;‰∏•Á¶ÅÂ∞ÜÂàÜÊûêÁªìÊûúÁî®‰∫éÂïÜ‰∏öÂÜ≥Á≠ñÊàñÁõàÂà©ÁõÆÁöÑ&lt;/li&gt; 
   &lt;li&gt;‰ΩøÁî®ËÄÖÂ∫îÁ°Æ‰øùÊâÄÂàÜÊûêÊï∞ÊçÆÁöÑÂêàÊ≥ïÊÄßÂíåÂêàËßÑÊÄß&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÊäÄÊúØÂÖçË¥£&lt;/strong&gt;Ôºö&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Êú¨È°πÁõÆÊåâ"Áé∞Áä∂"Êèê‰æõÔºå‰∏çÊèê‰æõ‰ªª‰ΩïÊòéÁ§∫ÊàñÊöóÁ§∫ÁöÑ‰øùËØÅ&lt;/li&gt; 
   &lt;li&gt;‰ΩúËÄÖ‰∏çÂØπ‰ΩøÁî®Êú¨È°πÁõÆÈÄ†ÊàêÁöÑ‰ªª‰ΩïÁõ¥Êé•ÊàñÈó¥Êé•ÊçüÂ§±ÊâøÊãÖË¥£‰ªª&lt;/li&gt; 
   &lt;li&gt;‰ΩøÁî®ËÄÖÂ∫îËá™Ë°åËØÑ‰º∞È°πÁõÆÁöÑÈÄÇÁî®ÊÄßÂíåÈ£éÈô©&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ë¥£‰ªªÈôêÂà∂&lt;/strong&gt;Ôºö&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;‰ΩøÁî®ËÄÖÂú®‰ΩøÁî®Êú¨È°πÁõÆÂâçÂ∫îÂÖÖÂàÜ‰∫ÜËß£Áõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑ&lt;/li&gt; 
   &lt;li&gt;‰ΩøÁî®ËÄÖÂ∫îÁ°Æ‰øùÂÖ∂‰ΩøÁî®Ë°å‰∏∫Á¨¶ÂêàÂΩìÂú∞Ê≥ïÂæãÊ≥ïËßÑË¶ÅÊ±Ç&lt;/li&gt; 
   &lt;li&gt;Âõ†ËøùÂèçÊ≥ïÂæãÊ≥ïËßÑ‰ΩøÁî®Êú¨È°πÁõÆËÄå‰∫ßÁîüÁöÑ‰ªª‰ΩïÂêéÊûúÁî±‰ΩøÁî®ËÄÖËá™Ë°åÊâøÊãÖ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;ËØ∑Âú®‰ΩøÁî®Êú¨È°πÁõÆÂâç‰ªîÁªÜÈòÖËØªÂπ∂ÁêÜËß£‰∏äËø∞ÂÖçË¥£Â£∞Êòé„ÄÇ‰ΩøÁî®Êú¨È°πÁõÆÂç≥Ë°®Á§∫ÊÇ®Â∑≤ÂêåÊÑèÂπ∂Êé•Âèó‰∏äËø∞ÊâÄÊúâÊù°Ê¨æ„ÄÇ&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üìÑ ËÆ∏ÂèØËØÅ&lt;/h2&gt; 
&lt;p&gt;Êú¨È°πÁõÆÈááÁî® &lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/LICENSE"&gt;GPL-2.0ËÆ∏ÂèØËØÅ&lt;/a&gt;„ÄÇËØ¶ÁªÜ‰ø°ÊÅØËØ∑ÂèÇÈòÖLICENSEÊñá‰ª∂„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üéâ ÊîØÊåÅ‰∏éËÅîÁ≥ª&lt;/h2&gt; 
&lt;h3&gt;Ëé∑ÂèñÂ∏ÆÂä©&lt;/h3&gt; 
&lt;p&gt;Â∏∏ËßÅÈóÆÈ¢òËß£Á≠îÔºö&lt;a href="https://github.com/666ghj/BettaFish/issues/185"&gt;https://github.com/666ghj/BettaFish/issues/185&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;È°πÁõÆ‰∏ªÈ°µ&lt;/strong&gt;Ôºö&lt;a href="https://github.com/666ghj/BettaFish"&gt;GitHub‰ªìÂ∫ì&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈóÆÈ¢òÂèçÈ¶à&lt;/strong&gt;Ôºö&lt;a href="https://github.com/666ghj/BettaFish/issues"&gt;IssuesÈ°µÈù¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂäüËÉΩÂª∫ËÆÆ&lt;/strong&gt;Ôºö&lt;a href="https://github.com/666ghj/BettaFish/discussions"&gt;DiscussionsÈ°µÈù¢&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÅîÁ≥ªÊñπÂºè&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìß &lt;strong&gt;ÈÇÆÁÆ±&lt;/strong&gt;Ôºö&lt;a href="mailto:hangjiang@bupt.edu.cn"&gt;hangjiang@bupt.edu.cn&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÂïÜÂä°Âêà‰Ωú&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;‰ºÅ‰∏öÂÆöÂà∂ÂºÄÂèë&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â§ßÊï∞ÊçÆÊúçÂä°&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â≠¶ÊúØÂêà‰Ωú&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊäÄÊúØÂüπËÆ≠&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üë• Ë¥°ÁåÆËÄÖ&lt;/h2&gt; 
&lt;p&gt;ÊÑüË∞¢‰ª•‰∏ã‰ºòÁßÄÁöÑË¥°ÁåÆËÄÖ‰ª¨Ôºö&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/666ghj/BettaFish/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=666ghj/BettaFish" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üåü Âä†ÂÖ•ÂÆòÊñπ‰∫§ÊµÅÁæ§&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://capsule-render.vercel.app/api?type=waving&amp;amp;color=gradient&amp;amp;height=200&amp;amp;section=header&amp;amp;text=Ê¨¢ËøéÂä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅQQÁæ§ÔºÅ&amp;amp;fontSize=40&amp;amp;fontAlignY=35&amp;amp;desc=Êâ´Êèè‰∏ãÊñπ‰∫åÁª¥Á†ÅÂä†ÂÖ•Áæ§ËÅä&amp;amp;descAlignY=55" alt="Ê¨¢ËøéÂä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅQQÁæ§ÔºÅ" style="width:60%; max-width:900px; display:block; margin:0 auto;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/QQ_Light_Horizenal.png" alt="BettaFish ÊäÄÊúØ‰∫§ÊµÅÁæ§‰∫åÁª¥Á†Å" style="width:60%; max-width:360px; display:block; margin:20px auto 0;" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìà È°πÁõÆÁªüËÆ°&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;theme=dark&amp;amp;legend=top-left" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/e04e3eea4674edc39c148a7845c8d09c1b7b1922.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelscope/DiffSynth-Studio</title>
      <link>https://github.com/modelscope/DiffSynth-Studio</link>
      <description>&lt;p&gt;Enjoy the magic of Diffusion models!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DiffSynth-Studio&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/.github/workflows/logo.gif" title="Logo" style="max-width:100%;" width="55" /&gt;&lt;/a&gt; &lt;a href="https://trendshift.io/repositories/10946" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10946" alt="modelscope%2FDiffSynth-Studio | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/DiffSynth/"&gt;&lt;img src="https://img.shields.io/pypi/v/DiffSynth" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelscope/DiffSynth-Studio/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg?sanitize=true" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelscope/DiffSynth-Studio/issues"&gt;&lt;img src="https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg?sanitize=true" alt="open issues" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/modelscope/DiffSynth-Studio/pull/"&gt;&lt;img src="https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg?sanitize=true" alt="GitHub pull-requests" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/modelscope/DiffSynth-Studio/commit/"&gt;&lt;img src="https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio" alt="GitHub latest commit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/README_zh.md"&gt;ÂàáÊç¢Âà∞‰∏≠ÊñáÁâà&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to the magical world of Diffusion models! DiffSynth-Studio is an open-source Diffusion model engine developed and maintained by the &lt;a href="https://www.modelscope.cn/"&gt;ModelScope Community&lt;/a&gt;. We hope to foster technological innovation through framework construction, aggregate the power of the open-source community, and explore the boundaries of generative model technology!&lt;/p&gt; 
&lt;p&gt;DiffSynth currently includes two open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt;: Focused on aggressive technical exploration, targeting academia, and providing cutting-edge model capability support.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt;: Focused on stable model deployment, targeting industry, and providing higher computational performance and more stable features.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt; and &lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt; are the core engines of the ModelScope AIGC zone. Welcome to experience our carefully crafted productized features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ModelScope AIGC Zone (for Chinese users): &lt;a href="https://modelscope.cn/aigc/home"&gt;https://modelscope.cn/aigc/home&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ModelScope Civision (for global users): &lt;a href="https://modelscope.ai/civision/home"&gt;https://modelscope.ai/civision/home&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;DiffSynth-Studio Documentation: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/zh/README.md"&gt;‰∏≠ÊñáÁâà&lt;/a&gt;„ÄÅ&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/README.md"&gt;English version&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We believe that a well-developed open-source code framework can lower the threshold for technical exploration. We have achieved many &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#innovative-achievements"&gt;interesting technologies&lt;/a&gt; based on this codebase. Perhaps you also have many wild ideas, and with DiffSynth-Studio, you can quickly realize these ideas. For this reason, we have prepared detailed documentation for developers. We hope that through these documents, developers can understand the principles of Diffusion models, and we look forward to expanding the boundaries of technology together with you.&lt;/p&gt; 
&lt;h2&gt;Update History&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;DiffSynth-Studio has undergone major version updates, and some old features are no longer maintained. If you need to use old features, please switch to the &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3"&gt;last historical version&lt;/a&gt; before the major version update.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Currently, the development personnel of this project are limited, with most of the work handled by &lt;a href="https://github.com/Artiprocher"&gt;Artiprocher&lt;/a&gt;. Therefore, the progress of new feature development will be relatively slow, and the speed of responding to and resolving issues is limited. We apologize for this and ask developers to understand.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 9, 2025&lt;/strong&gt; We release a wild model based on DiffSynth-Studio 2.0: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-i2L"&gt;Qwen-Image-i2L&lt;/a&gt; (Image-to-LoRA). This model takes an image as input and outputs a LoRA. Although this version still has significant room for improvement in terms of generalization, detail preservation, and other aspects, we are open-sourcing these models to inspire more innovative research.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 4, 2025&lt;/strong&gt; DiffSynth-Studio 2.0 released! Many new features online&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/README.md"&gt;Documentation&lt;/a&gt; online: Our documentation is still continuously being optimized and updated&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Pipeline_Usage/VRAM_management.md"&gt;VRAM Management&lt;/a&gt; module upgraded, supporting layer-level disk offload, releasing both memory and VRAM simultaneously&lt;/li&gt; 
   &lt;li&gt;New model support 
    &lt;ul&gt; 
     &lt;li&gt;Z-Image Turbo: &lt;a href="https://www.modelscope.ai/models/Tongyi-MAI/Z-Image-Turbo"&gt;Model&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/Z-Image.md"&gt;Documentation&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/"&gt;Code&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;FLUX.2-dev: &lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev"&gt;Model&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/FLUX2.md"&gt;Documentation&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/"&gt;Code&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Training framework upgrade 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/zh/Training/Split_Training.md"&gt;Split Training&lt;/a&gt;: Supports automatically splitting the training process into two stages: data processing and training (even for training ControlNet or any other model). Computations that do not require gradient backpropagation, such as text encoding and VAE encoding, are performed during the data processing stage, while other computations are handled during the training stage. Faster speed, less VRAM requirement.&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/zh/Training/Differential_LoRA.md"&gt;Differential LoRA Training&lt;/a&gt;: This is a training technique we used in &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;ArtAug&lt;/a&gt;, now available for LoRA training of any model.&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/zh/Training/FP8_Precision.md"&gt;FP8 Training&lt;/a&gt;: FP8 can be applied to any non-training model during training, i.e., models with gradients turned off or gradients that only affect LoRA weights.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;November 4, 2025&lt;/strong&gt; Supported the &lt;a href="https://modelscope.cn/models/ByteDance/Video-As-Prompt-Wan2.1-14B"&gt;ByteDance/Video-As-Prompt-Wan2.1-14B&lt;/a&gt; model, which is trained based on Wan 2.1 and supports generating corresponding actions based on reference videos.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 30, 2025&lt;/strong&gt; Supported the &lt;a href="https://www.modelscope.cn/models/meituan-longcat/LongCat-Video"&gt;meituan-longcat/LongCat-Video&lt;/a&gt; model, which supports text-to-video, image-to-video, and video continuation. This model uses the Wan framework for inference and training in this project.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 27, 2025&lt;/strong&gt; Supported the &lt;a href="https://www.modelscope.cn/models/krea/krea-realtime-video"&gt;krea/krea-realtime-video&lt;/a&gt; model, adding another member to the Wan model ecosystem.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;September 23, 2025&lt;/strong&gt; &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-Poster"&gt;DiffSynth-Studio/Qwen-Image-EliGen-Poster&lt;/a&gt; released! This model was jointly developed and open-sourced by us and Taobao Experience Design Team. Built upon Qwen-Image, the model is specifically designed for e-commerce poster scenarios, supporting precise partition layout control. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen-Poster.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;September 9, 2025&lt;/strong&gt; Our training framework supports various training modes. Currently adapted for Qwen-Image, in addition to the standard SFT training mode, Direct Distill is now supported. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Distill-LoRA.sh"&gt;our sample code&lt;/a&gt;. This feature is experimental, and we will continue to improve it to support more comprehensive model training functions.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 28, 2025&lt;/strong&gt; We support Wan2.2-S2V, an audio-driven cinematic video generation model. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2025&lt;/strong&gt; &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-V2"&gt;DiffSynth-Studio/Qwen-Image-EliGen-V2&lt;/a&gt; released! Compared to the V1 version, the training dataset has been changed to &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset"&gt;Qwen-Image-Self-Generated-Dataset&lt;/a&gt;, so the generated images better conform to Qwen-Image's own image distribution and style. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-V2.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2025&lt;/strong&gt; We open-sourced the &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union"&gt;DiffSynth-Studio/Qwen-Image-In-Context-Control-Union&lt;/a&gt; structural control LoRA model, adopting the In Context technical route, supporting multiple categories of structural control conditions, including canny, depth, lineart, softedge, normal, and openpose. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-In-Context-Control-Union.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 20, 2025&lt;/strong&gt; We open-sourced the &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix"&gt;DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix&lt;/a&gt; model, improving the editing effect of Qwen-Image-Edit on low-resolution image inputs. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit-Lowres-Fix.py"&gt;our sample code&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 19, 2025&lt;/strong&gt; üî• Qwen-Image-Edit open-sourced, welcome a new member to the image editing model family!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 18, 2025&lt;/strong&gt; We trained and open-sourced the Qwen-Image inpainting ControlNet model &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint&lt;/a&gt;. The model structure adopts a lightweight design. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 15, 2025&lt;/strong&gt; We open-sourced the &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset"&gt;Qwen-Image-Self-Generated-Dataset&lt;/a&gt; dataset. This is an image dataset generated using the Qwen-Image model, containing 160,000 &lt;code&gt;1024 x 1024&lt;/code&gt; images. It includes general, English text rendering, and Chinese text rendering subsets. We provide annotations for image descriptions, entities, and structural control images for each image. Developers can use this dataset to train Qwen-Image models' ControlNet and EliGen models. We aim to promote technological development through open-sourcing!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 13, 2025&lt;/strong&gt; We trained and open-sourced the Qwen-Image ControlNet model &lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth&lt;/a&gt;. The model structure adopts a lightweight design. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 12, 2025&lt;/strong&gt; We trained and open-sourced the Qwen-Image ControlNet model &lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny&lt;/a&gt;. The model structure adopts a lightweight design. Please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;our sample code&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 11, 2025&lt;/strong&gt; We open-sourced the distilled acceleration model &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-LoRA"&gt;DiffSynth-Studio/Qwen-Image-Distill-LoRA&lt;/a&gt; for Qwen-Image, following the same training process as &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full"&gt;DiffSynth-Studio/Qwen-Image-Distill-Full&lt;/a&gt;, but the model structure has been modified to LoRA, thus being better compatible with other open-source ecosystem models.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 7, 2025&lt;/strong&gt; We open-sourced the entity control LoRA model &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen"&gt;DiffSynth-Studio/Qwen-Image-EliGen&lt;/a&gt; for Qwen-Image. Qwen-Image-EliGen can achieve entity-level controlled text-to-image generation. Technical details can be found in &lt;a href="https://arxiv.org/abs/2501.01097"&gt;the paper&lt;/a&gt;. Training dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet"&gt;EliGenTrainSet&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 5, 2025&lt;/strong&gt; We open-sourced the distilled acceleration model &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full"&gt;DiffSynth-Studio/Qwen-Image-Distill-Full&lt;/a&gt; for Qwen-Image, achieving approximately 5x acceleration.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 4, 2025&lt;/strong&gt; üî• Qwen-Image open-sourced, welcome a new member to the image generation model family!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 1, 2025&lt;/strong&gt; &lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev"&gt;FLUX.1-Krea-dev&lt;/a&gt; open-sourced, a text-to-image model focused on aesthetic photography. We provided comprehensive support in a timely manner, including low VRAM layer-by-layer offload, LoRA training, and full training. For more details, please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/"&gt;./examples/flux/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;July 28, 2025&lt;/strong&gt; Wan 2.2 open-sourced. We provided comprehensive support in a timely manner, including low VRAM layer-by-layer offload, FP8 quantization, sequence parallelism, LoRA training, and full training. For more details, please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;July 11, 2025&lt;/strong&gt; We propose Nexus-Gen, a unified framework that combines the language reasoning capabilities of Large Language Models (LLMs) with the image generation capabilities of diffusion models. This framework supports seamless image understanding, generation, and editing tasks.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Paper: &lt;a href="https://arxiv.org/pdf/2504.21356"&gt;Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;GitHub Repository: &lt;a href="https://github.com/modelscope/Nexus-Gen"&gt;https://github.com/modelscope/Nexus-Gen&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/Nexus-GenV2"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Training Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset"&gt;ModelScope Dataset&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen"&gt;ModelScope Nexus-Gen Studio&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 15, 2025&lt;/strong&gt; ModelScope's official evaluation framework &lt;a href="https://github.com/modelscope/evalscope"&gt;EvalScope&lt;/a&gt; now supports text-to-image generation evaluation. Please refer to the &lt;a href="https://evalscope.readthedocs.io/zh-cn/latest/best_practice/t2i_eval.html"&gt;best practices&lt;/a&gt; guide to try it out.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 25, 2025&lt;/strong&gt; Our new open-source project &lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt; is now open-sourced! Focused on stable model deployment, targeting industry, providing better engineering support, higher computational performance, and more stable features.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 31, 2025&lt;/strong&gt; We support InfiniteYou, a face feature preservation method for FLUX. More details can be found in &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/InfiniteYou/"&gt;./examples/InfiniteYou/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;March 13, 2025&lt;/strong&gt; We support HunyuanVideo-I2V, the image-to-video generation version of Tencent's open-source HunyuanVideo. More details can be found in &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/"&gt;./examples/HunyuanVideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 25, 2025&lt;/strong&gt; We support Wan-Video, a series of state-of-the-art video synthesis models open-sourced by Alibaba. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;./examples/wanvideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;February 17, 2025&lt;/strong&gt; We support &lt;a href="https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary"&gt;StepVideo&lt;/a&gt;! Advanced video synthesis model! See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/stepvideo/"&gt;./examples/stepvideo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 31, 2024&lt;/strong&gt; We propose EliGen, a new framework for entity-level controlled text-to-image generation, supplemented with an inpainting fusion pipeline, extending its capabilities to image inpainting tasks. EliGen can seamlessly integrate existing community models such as IP-Adapter and In-Context LoRA, enhancing their versatility. For more details, see &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/EntityControl/"&gt;./examples/EntityControl&lt;/a&gt;.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.01097"&gt;EliGen: Entity-Level Controlled Image Generation with Regional Attention&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Eligen"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/EliGen"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen"&gt;ModelScope EliGen Studio&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Training Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet"&gt;EliGen Train Set&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 19, 2024&lt;/strong&gt; We implemented advanced VRAM management for HunyuanVideo, enabling video generation with resolutions of 129x720x1280 on 24GB VRAM or 129x512x384 on just 6GB VRAM. More details can be found in &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/HunyuanVideo/"&gt;./examples/HunyuanVideo/&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 18, 2024&lt;/strong&gt; We propose ArtAug, a method to improve text-to-image models through synthesis-understanding interaction. We trained an ArtAug enhancement module for FLUX.1-dev in LoRA format. This model incorporates the aesthetic understanding of Qwen2-VL-72B into FLUX.1-dev, thereby improving the quality of generated images.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2412.12888"&gt;https://arxiv.org/abs/2412.12888&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Example: &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug"&gt;https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Demo: &lt;a href="https://modelscope.cn/aigc/imageGeneration?tab=advanced&amp;amp;versionId=7228&amp;amp;modelType=LoRA&amp;amp;sdVersion=FLUX_1&amp;amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0"&gt;ModelScope&lt;/a&gt;, HuggingFace (coming soon)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 25, 2024&lt;/strong&gt; We provide extensive FLUX ControlNet support. This project supports many different ControlNet models and can be freely combined, even if their structures are different. Additionally, ControlNet models are compatible with high-resolution optimization and partition control technologies, enabling very powerful controllable image generation. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ControlNet/"&gt;&lt;code&gt;./examples/ControlNet/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 8, 2024&lt;/strong&gt; We released extended LoRAs based on CogVideoX-5B and ExVideo. You can download this model from &lt;a href="https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1"&gt;ModelScope&lt;/a&gt; or &lt;a href="https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1"&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024&lt;/strong&gt; This project now supports CogVideoX-5B. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/video_synthesis/"&gt;here&lt;/a&gt;. We provide several interesting features for this text-to-video model, including:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Text-to-video&lt;/li&gt; 
    &lt;li&gt;Video editing&lt;/li&gt; 
    &lt;li&gt;Self super-resolution&lt;/li&gt; 
    &lt;li&gt;Video interpolation&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 22, 2024&lt;/strong&gt; We implemented an interesting brush feature that supports all text-to-image models. Now you can create stunning images with the assistance of AI using the brush!&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Use it in our &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/#usage-in-webui"&gt;WebUI&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 21, 2024&lt;/strong&gt; DiffSynth-Studio now supports FLUX.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Enable CFG and high-resolution inpainting to improve visual quality. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/README.md"&gt;here&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;LoRA, ControlNet, and other addon models will be released soon.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 21, 2024&lt;/strong&gt; We propose ExVideo, a post-training fine-tuning technique aimed at enhancing the capabilities of video generation models. We extended Stable Video Diffusion to achieve long video generation of up to 128 frames.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://ecnu-cilab.github.io/ExVideoProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Source code has been released in this repository. See &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/"&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Model has been released at &lt;a href="https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Technical report has been released at &lt;a href="https://arxiv.org/abs/2406.14130"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;You can try ExVideo in this &lt;a href="https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1"&gt;demo&lt;/a&gt;!&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;June 13, 2024&lt;/strong&gt; DiffSynth Studio has migrated to ModelScope. The development team has also transitioned from "me" to "us". Of course, I will still participate in subsequent development and maintenance work.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;January 29, 2024&lt;/strong&gt; We propose Diffutoon, an excellent cartoon coloring solution.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Source code has been released in this project.&lt;/li&gt; 
    &lt;li&gt;Technical report (IJCAI 2024) has been released at &lt;a href="https://arxiv.org/abs/2401.16224"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;December 8, 2023&lt;/strong&gt; We decided to initiate a new project aimed at unleashing the potential of diffusion models, especially in video synthesis. The development work of this project officially began.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;November 15, 2023&lt;/strong&gt; We propose FastBlend, a powerful video deflickering algorithm.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;sd-webui extension has been released at &lt;a href="https://github.com/Artiprocher/sd-webui-fastblend"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Demonstration videos have been showcased on Bilibili, including three tasks: 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1d94y1W7PE"&gt;Video Deflickering&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1Lw411m71p"&gt;Video Interpolation&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1RB4y1Z7LF"&gt;Image-Driven Video Rendering&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;Technical report has been released at &lt;a href="https://arxiv.org/abs/2311.09265"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Unofficial ComfyUI extensions developed by other users have been released at &lt;a href="https://github.com/AInseven/ComfyUI-fastblend"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;October 1, 2023&lt;/strong&gt; We released an early version of the project named FastSDXL. This was an initial attempt to build a diffusion engine.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Source code has been released at &lt;a href="https://github.com/Artiprocher/FastSDXL"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;FastSDXL includes a trainable OLSS scheduler to improve efficiency. 
     &lt;ul&gt; 
      &lt;li&gt;The original repository of OLSS is located &lt;a href="https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler"&gt;here&lt;/a&gt;.&lt;/li&gt; 
      &lt;li&gt;Technical report (CIKM 2023) has been released at &lt;a href="https://arxiv.org/abs/2305.14677"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
      &lt;li&gt;Demonstration video has been released at &lt;a href="https://www.bilibili.com/video/BV1w8411y7uj"&gt;Bilibili&lt;/a&gt;.&lt;/li&gt; 
      &lt;li&gt;Since OLSS requires additional training, we did not implement it in this project.&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;August 29, 2023&lt;/strong&gt; We propose DiffSynth, a video synthesis framework.&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://ecnu-cilab.github.io/DiffSynth.github.io/"&gt;Project Page&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Source code has been released at &lt;a href="https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth"&gt;EasyNLP&lt;/a&gt;.&lt;/li&gt; 
    &lt;li&gt;Technical report (ECML PKDD 2024) has been released at &lt;a href="https://arxiv.org/abs/2308.03463"&gt;arXiv&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Install from source (recommended):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git  
cd DiffSynth-Studio
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation methods&lt;/summary&gt; 
 &lt;p&gt;Install from PyPI (version updates may be delayed; for latest features, install from source)&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;pip install diffsynth
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you meet problems during installation, they might be caused by upstream dependencies. Please check the docs of these packages:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pytorch.org/get-started/locally/"&gt;torch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/google/sentencepiece"&gt;sentencepiece&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://cmake.org"&gt;cmake&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.cupy.dev/en/stable/install.html"&gt;cupy&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Basic Framework&lt;/h2&gt; 
&lt;p&gt;DiffSynth-Studio redesigns the inference and training pipelines for mainstream Diffusion models (including FLUX, Wan, etc.), enabling efficient memory management and flexible model training.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Environment Variable Configuration&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Before running model inference or training, you can configure settings such as the model download source via &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Pipeline_Usage/Environment_Variables.md"&gt;environment variables&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;By default, this project downloads models from ModelScope. For users outside China, you can configure the system to download models from the ModelScope international site as follows:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["MODELSCOPE_DOMAIN"] = "www.modelscope.ai"
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;To download models from other sources, please modify the environment variable &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Pipeline_Usage/Environment_Variables.md#diffsynth_download_source"&gt;DIFFSYNTH_DOWNLOAD_SOURCE&lt;/a&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h3&gt;Image Synthesis&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/c01258e2-f251-441a-aa1e-ebb22f02594d" alt="Image" /&gt;&lt;/p&gt; 
&lt;h4&gt;Z-Image: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/Z-Image.md"&gt;/docs/en/Model_Details/Z-Image.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo"&gt;Tongyi-MAI/Z-Image-Turbo&lt;/a&gt; model for inference. FP8 quantization significantly degrades image quality, so we do not recommend enabling any quantization for the Z-Image Turbo model. CPU offloading is recommended, and the model can run with as little as 8 GB of GPU memory.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffsynth.pipelines.z_image import ZImagePipeline, ModelConfig
import torch

vram_config = {
    "offload_dtype": torch.bfloat16,
    "offload_device": "cpu",
    "onload_dtype": torch.bfloat16,
    "onload_device": "cpu",
    "preparing_dtype": torch.bfloat16,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = ZImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="transformer/*.safetensors", **vram_config),
        ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="text_encoder/*.safetensors", **vram_config),
        ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="vae/diffusion_pytorch_model.safetensors", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Tongyi-MAI/Z-Image-Turbo", origin_file_pattern="tokenizer/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)
prompt = "Young Chinese woman in red Hanfu, intricate embroidery. Impeccable makeup, red floral forehead pattern. Elaborate high bun, golden phoenix headdress, red flowers, beads. Holds round folding fan with lady, trees, bird. Neon lightning-bolt lamp (‚ö°Ô∏è), bright yellow glow, above extended left palm. Soft-lit outdoor night background, silhouetted tiered pagoda (Ë•øÂÆâÂ§ßÈõÅÂ°î), blurred colorful distant lights."
image = pipe(prompt=prompt, seed=42, rand_device="cuda")
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for Z-Image is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/"&gt;/examples/z_image/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low-VRAM Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Full Training Validation&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Tongyi-MAI/Z-Image-Turbo"&gt;Tongyi-MAI/Z-Image-Turbo&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_inference/Z-Image-Turbo.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_inference_low_vram/Z-Image-Turbo.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_training/full/Z-Image-Turbo.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_training/validate_full/Z-Image-Turbo.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_training/lora/Z-Image-Turbo.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/z_image/model_training/validate_lora/Z-Image-Turbo.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;FLUX.2: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/FLUX2.md"&gt;/docs/en/Model_Details/FLUX2.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev"&gt;black-forest-labs/FLUX.2-dev&lt;/a&gt; model for inference. VRAM management is enabled, and the framework automatically loads model parameters based on available GPU memory. The model can run with as little as 10 GB of VRAM.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffsynth.pipelines.flux2_image import Flux2ImagePipeline, ModelConfig
import torch

vram_config = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": torch.float8_e4m3fn,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e4m3fn,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = Flux2ImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="text_encoder/*.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="transformer/*.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="vae/diffusion_pytorch_model.safetensors"),
    ],
    tokenizer_config=ModelConfig(model_id="black-forest-labs/FLUX.2-dev", origin_file_pattern="tokenizer/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)
prompt = "High resolution. A dreamy underwater portrait of a serene young woman in a flowing blue dress. Her hair floats softly around her face, strands delicately suspended in the water. Clear, shimmering light filters through, casting gentle highlights, while tiny bubbles rise around her. Her expression is calm, her features finely detailed‚Äîcreating a tranquil, ethereal scene."
image = pipe(prompt, seed=42, rand_device="cuda", num_inference_steps=50)
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for FLUX.2 is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/"&gt;/examples/flux2/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low-VRAM Inference&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev"&gt;black-forest-labs/FLUX.2-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/model_inference/FLUX.2-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/model_inference_low_vram/FLUX.2-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/model_training/lora/FLUX.2-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux2/model_training/validate_lora/FLUX.2-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;Qwen-Image: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/Qwen-Image.md"&gt;/docs/en/Model_Details/Qwen-Image.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image"&gt;Qwen/Qwen-Image&lt;/a&gt; model for inference. VRAM management is enabled, and the framework automatically adjusts model parameter loading based on available GPU memory. The model can run with as little as 8 GB of VRAM.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig
import torch

vram_config = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": torch.float8_e4m3fn,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e4m3fn,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = QwenImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="transformer/diffusion_pytorch_model*.safetensors", **vram_config),
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="text_encoder/model*.safetensors", **vram_config),
        ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="vae/diffusion_pytorch_model.safetensors", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Qwen/Qwen-Image", origin_file_pattern="tokenizer/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 0.5,
)
prompt = "Á≤æËá¥ËÇñÂÉèÔºåÊ∞¥‰∏ãÂ∞ëÂ•≥ÔºåËìùË£ôÈ£òÈÄ∏ÔºåÂèë‰∏ùËΩªÊâ¨ÔºåÂÖâÂΩ±ÈÄèÊæàÔºåÊ∞îÊ≥°ÁéØÁªïÔºåÈù¢ÂÆπÊÅ¨ÈùôÔºåÁªÜËäÇÁ≤æËá¥ÔºåÊ¢¶ÂπªÂîØÁæé„ÄÇ"
image = pipe(prompt, seed=0, num_inference_steps=40)
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Model Lineage&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;graph LR;
    Qwen/Qwen-Image--&amp;gt;Qwen/Qwen-Image-Edit;
    Qwen/Qwen-Image-Edit--&amp;gt;Qwen/Qwen-Image-Edit-2509;
    Qwen/Qwen-Image--&amp;gt;EliGen-Series;
    EliGen-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-EliGen;
    DiffSynth-Studio/Qwen-Image-EliGen--&amp;gt;DiffSynth-Studio/Qwen-Image-EliGen-V2;
    EliGen-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-EliGen-Poster;
    Qwen/Qwen-Image--&amp;gt;Distill-Series;
    Distill-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Distill-Full;
    Distill-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Distill-LoRA;
    Qwen/Qwen-Image--&amp;gt;ControlNet-Series;
    ControlNet-Series--&amp;gt;Blockwise-ControlNet-Series;
    Blockwise-ControlNet-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny;
    Blockwise-ControlNet-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth;
    Blockwise-ControlNet-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint;
    ControlNet-Series--&amp;gt;DiffSynth-Studio/Qwen-Image-In-Context-Control-Union;
    Qwen/Qwen-Image--&amp;gt;DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for Qwen-Image is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/"&gt;/examples/qwen_image/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low-VRAM Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Full Training Validation&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image"&gt;Qwen/Qwen-Image&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit"&gt;Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit-2509"&gt;Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit-2509.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit-2509.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Edit-2509.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Edit-2509.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Edit-2509.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Edit-2509.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen"&gt;DiffSynth-Studio/Qwen-Image-EliGen&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-EliGen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-V2"&gt;DiffSynth-Studio/Qwen-Image-EliGen-V2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen-V2.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-V2.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-EliGen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-Poster"&gt;DiffSynth-Studio/Qwen-Image-EliGen-Poster&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-EliGen-Poster.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-Poster.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-EliGen-Poster.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-EliGen-Poster.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full"&gt;DiffSynth-Studio/Qwen-Image-Distill-Full&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Distill-Full.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Distill-Full.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Distill-Full.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-LoRA"&gt;DiffSynth-Studio/Qwen-Image-Distill-LoRA&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Distill-LoRA.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Distill-LoRA.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Distill-LoRA.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Distill-LoRA.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Canny.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Canny.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Canny.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Depth.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Depth.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Depth.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint"&gt;DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/full/Qwen-Image-Blockwise-ControlNet-Inpaint.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_full/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-Blockwise-ControlNet-Inpaint.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-Blockwise-ControlNet-Inpaint.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union"&gt;DiffSynth-Studio/Qwen-Image-In-Context-Control-Union&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-In-Context-Control-Union.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-In-Context-Control-Union.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/lora/Qwen-Image-In-Context-Control-Union.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_training/validate_lora/Qwen-Image-In-Context-Control-Union.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix"&gt;DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-Edit-Lowres-Fix.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-Edit-Lowres-Fix.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-i2L"&gt;DiffSynth-Studio/Qwen-Image-i2L&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference/Qwen-Image-i2L.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/qwen_image/model_inference_low_vram/Qwen-Image-i2L.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;FLUX.1: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/FLUX.md"&gt;/docs/en/Model_Details/FLUX.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-dev"&gt;black-forest-labs/FLUX.1-dev&lt;/a&gt; model for inference. VRAM management is enabled, and the framework automatically adjusts model parameter loading based on available GPU memory. The model can run with as little as 8 GB of VRAM.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from diffsynth.pipelines.flux_image import FluxImagePipeline, ModelConfig

vram_config = {
    "offload_dtype": torch.float8_e4m3fn,
    "offload_device": "cpu",
    "onload_dtype": torch.float8_e4m3fn,
    "onload_device": "cpu",
    "preparing_dtype": torch.float8_e4m3fn,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = FluxImagePipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="flux1-dev.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder/model.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="text_encoder_2/*.safetensors", **vram_config),
        ModelConfig(model_id="black-forest-labs/FLUX.1-dev", origin_file_pattern="ae.safetensors", **vram_config),
    ],
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 1,
)
prompt = "CG, masterpiece, best quality, solo, long hair, wavy hair, silver hair, blue eyes, blue dress, medium breasts, dress, underwater, air bubble, floating hair, refraction, portrait. The girl's flowing silver hair shimmers with every color of the rainbow and cascades down, merging with the floating flora around her."
image = pipe(prompt=prompt, seed=0)
image.save("image.jpg")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Model Lineage&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;graph LR;
    FLUX.1-Series--&amp;gt;black-forest-labs/FLUX.1-dev;
    FLUX.1-Series--&amp;gt;black-forest-labs/FLUX.1-Krea-dev;
    FLUX.1-Series--&amp;gt;black-forest-labs/FLUX.1-Kontext-dev;
    black-forest-labs/FLUX.1-dev--&amp;gt;FLUX.1-dev-ControlNet-Series;
    FLUX.1-dev-ControlNet-Series--&amp;gt;alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta;
    FLUX.1-dev-ControlNet-Series--&amp;gt;InstantX/FLUX.1-dev-Controlnet-Union-alpha;
    FLUX.1-dev-ControlNet-Series--&amp;gt;jasperai/Flux.1-dev-Controlnet-Upscaler;
    black-forest-labs/FLUX.1-dev--&amp;gt;InstantX/FLUX.1-dev-IP-Adapter;
    black-forest-labs/FLUX.1-dev--&amp;gt;ByteDance/InfiniteYou;
    black-forest-labs/FLUX.1-dev--&amp;gt;DiffSynth-Studio/Eligen;
    black-forest-labs/FLUX.1-dev--&amp;gt;DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev;
    black-forest-labs/FLUX.1-dev--&amp;gt;DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev;
    black-forest-labs/FLUX.1-dev--&amp;gt;ostris/Flex.2-preview;
    black-forest-labs/FLUX.1-dev--&amp;gt;stepfun-ai/Step1X-Edit;
    Qwen/Qwen2.5-VL-7B-Instruct--&amp;gt;stepfun-ai/Step1X-Edit;
    black-forest-labs/FLUX.1-dev--&amp;gt;DiffSynth-Studio/Nexus-GenV2;
    Qwen/Qwen2.5-VL-7B-Instruct--&amp;gt;DiffSynth-Studio/Nexus-GenV2;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for FLUX.1 is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/"&gt;/examples/flux/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Extra Args&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Low-VRAM Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Full Training Validation&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-dev"&gt;black-forest-labs/FLUX.1-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev"&gt;black-forest-labs/FLUX.1-Krea-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-Krea-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-Krea-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-Krea-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Kontext-dev"&gt;black-forest-labs/FLUX.1-Kontext-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;kontext_images&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-Kontext-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-Kontext-dev.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-Kontext-dev.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta"&gt;alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Inpainting-Beta.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Inpainting-Beta.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Inpainting-Beta.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/InstantX/FLUX.1-dev-Controlnet-Union-alpha"&gt;InstantX/FLUX.1-dev-Controlnet-Union-alpha&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Union-alpha.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Union-alpha.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Union-alpha.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/jasperai/Flux.1-dev-Controlnet-Upscaler"&gt;jasperai/Flux.1-dev-Controlnet-Upscaler&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-Controlnet-Upscaler.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-Controlnet-Upscaler.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-Controlnet-Upscaler.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/InstantX/FLUX.1-dev-IP-Adapter"&gt;InstantX/FLUX.1-dev-IP-Adapter&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;ipadapter_images&lt;/code&gt;, &lt;code&gt;ipadapter_scale&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-IP-Adapter.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-IP-Adapter.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-IP-Adapter.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/ByteDance/InfiniteYou"&gt;ByteDance/InfiniteYou&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;infinityou_id_image&lt;/code&gt;, &lt;code&gt;infinityou_guidance&lt;/code&gt;, &lt;code&gt;controlnet_inputs&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-InfiniteYou.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-InfiniteYou.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-InfiniteYou.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Eligen"&gt;DiffSynth-Studio/Eligen&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;eligen_entity_prompts&lt;/code&gt;, &lt;code&gt;eligen_entity_masks&lt;/code&gt;, &lt;code&gt;eligen_enable_on_negative&lt;/code&gt;, &lt;code&gt;eligen_enable_inpaint&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLUX.1-dev-EliGen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLUX.1-dev-EliGen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev"&gt;DiffSynth-Studio/LoRA-Encoder-FLUX.1-Dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_encoder_inputs&lt;/code&gt;, &lt;code&gt;lora_encoder_scale&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-LoRA-Encoder.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLUX.1-dev-LoRA-Encoder.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLUX.1-dev-LoRA-Encoder.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLUX.1-dev-LoRA-Encoder.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev"&gt;DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/stepfun-ai/Step1X-Edit"&gt;stepfun-ai/Step1X-Edit&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;step1x_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/Step1X-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/Step1X-Edit.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/Step1X-Edit.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/ostris/Flex.2-preview"&gt;ostris/Flex.2-preview&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;flex_inpaint_image&lt;/code&gt;, &lt;code&gt;flex_inpaint_mask&lt;/code&gt;, &lt;code&gt;flex_control_image&lt;/code&gt;, &lt;code&gt;flex_control_strength&lt;/code&gt;, &lt;code&gt;flex_control_stop&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/FLEX.2-preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/FLEX.2-preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/FLEX.2-preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2"&gt;DiffSynth-Studio/Nexus-GenV2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;nexus_gen_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/Nexus-Gen-Editing.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference_low_vram/Nexus-Gen-Editing.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/full/Nexus-Gen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_full/Nexus-Gen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/lora/Nexus-Gen.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_training/validate_lora/Nexus-Gen.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Video Synthesis&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/1d66ae74-3b02-40a9-acc3-ea95fc039314"&gt;https://github.com/user-attachments/assets/1d66ae74-3b02-40a9-acc3-ea95fc039314&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Wan: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/docs/en/Model_Details/Wan.md"&gt;/docs/en/Model_Details/Wan.md&lt;/a&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Quick Start&lt;/summary&gt; 
 &lt;p&gt;Running the following code will quickly load the &lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B"&gt;Wan-AI/Wan2.1-T2V-1.3B&lt;/a&gt; model for inference. VRAM management is enabled, and the framework automatically adjusts model parameter loading based on available GPU memory. The model can run with as little as 8 GB of VRAM.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from diffsynth.utils.data import save_video, VideoData
from diffsynth.pipelines.wan_video import WanVideoPipeline, ModelConfig

vram_config = {
    "offload_dtype": "disk",
    "offload_device": "disk",
    "onload_dtype": torch.bfloat16,
    "onload_device": "cpu",
    "preparing_dtype": torch.bfloat16,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}
pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="diffusion_pytorch_model*.safetensors", **vram_config),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", **vram_config),
        ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="Wan2.1_VAE.pth", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="google/umt5-xxl/"),
    vram_limit=torch.cuda.mem_get_info("cuda")[1] / (1024 ** 3) - 2,
)

video = pipe(
    prompt="Á∫™ÂÆûÊëÑÂΩ±È£éÊ†ºÁîªÈù¢Ôºå‰∏ÄÂè™Ê¥ªÊ≥ºÁöÑÂ∞èÁãóÂú®ÁªøËåµËåµÁöÑËçâÂú∞‰∏äËøÖÈÄüÂ•îË∑ë„ÄÇÂ∞èÁãóÊØõËâ≤Ê£ïÈªÑÔºå‰∏§Âè™ËÄ≥ÊúµÁ´ãËµ∑ÔºåÁ•ûÊÉÖ‰∏ìÊ≥®ËÄåÊ¨¢Âø´„ÄÇÈò≥ÂÖâÊ¥íÂú®ÂÆÉË∫´‰∏äÔºå‰ΩøÂæóÊØõÂèëÁúã‰∏äÂéªÊ†ºÂ§ñÊüîËΩØËÄåÈó™‰∫Æ„ÄÇËÉåÊôØÊòØ‰∏ÄÁâáÂºÄÈòîÁöÑËçâÂú∞ÔºåÂÅ∂Â∞îÁÇπÁºÄÁùÄÂá†ÊúµÈáéËä±ÔºåËøúÂ§ÑÈöêÁ∫¶ÂèØËßÅËìùÂ§©ÂíåÂá†ÁâáÁôΩ‰∫ë„ÄÇÈÄèËßÜÊÑüÈ≤úÊòéÔºåÊçïÊçâÂ∞èÁãóÂ•îË∑ëÊó∂ÁöÑÂä®ÊÑüÂíåÂõõÂë®ËçâÂú∞ÁöÑÁîüÊú∫„ÄÇ‰∏≠ÊôØ‰æßÈù¢ÁßªÂä®ËßÜËßí„ÄÇ",
    negative_prompt="Ëâ≤Ë∞ÉËâ≥‰∏ΩÔºåËøáÊõùÔºåÈùôÊÄÅÔºåÁªÜËäÇÊ®°Á≥ä‰∏çÊ∏ÖÔºåÂ≠óÂπïÔºåÈ£éÊ†ºÔºå‰ΩúÂìÅÔºåÁîª‰ΩúÔºåÁîªÈù¢ÔºåÈùôÊ≠¢ÔºåÊï¥‰ΩìÂèëÁÅ∞ÔºåÊúÄÂ∑ÆË¥®ÈáèÔºå‰ΩéË¥®ÈáèÔºåJPEGÂéãÁº©ÊÆãÁïôÔºå‰∏ëÈôãÁöÑÔºåÊÆãÁº∫ÁöÑÔºåÂ§ö‰ΩôÁöÑÊâãÊåáÔºåÁîªÂæó‰∏çÂ•ΩÁöÑÊâãÈÉ®ÔºåÁîªÂæó‰∏çÂ•ΩÁöÑËÑ∏ÈÉ®ÔºåÁï∏ÂΩ¢ÁöÑÔºåÊØÅÂÆπÁöÑÔºåÂΩ¢ÊÄÅÁï∏ÂΩ¢ÁöÑËÇ¢‰ΩìÔºåÊâãÊåáËûçÂêàÔºåÈùôÊ≠¢‰∏çÂä®ÁöÑÁîªÈù¢ÔºåÊùÇ‰π±ÁöÑËÉåÊôØÔºå‰∏âÊù°ËÖøÔºåËÉåÊôØ‰∫∫ÂæàÂ§öÔºåÂÄíÁùÄËµ∞",
    seed=0, tiled=True,
)
save_video(video, "video.mp4", fps=15, quality=5)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Model Lineage&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;graph LR;
    Wan-Series--&amp;gt;Wan2.1-Series;
    Wan-Series--&amp;gt;Wan2.2-Series;
    Wan2.1-Series--&amp;gt;Wan-AI/Wan2.1-T2V-1.3B;
    Wan2.1-Series--&amp;gt;Wan-AI/Wan2.1-T2V-14B;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.1-I2V-14B-480P;
    Wan-AI/Wan2.1-I2V-14B-480P--&amp;gt;Wan-AI/Wan2.1-I2V-14B-720P;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.1-FLF2V-14B-720P;
    Wan-AI/Wan2.1-T2V-1.3B--&amp;gt;iic/VACE-Wan2.1-1.3B-Preview;
    iic/VACE-Wan2.1-1.3B-Preview--&amp;gt;Wan-AI/Wan2.1-VACE-1.3B;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.1-VACE-14B;
    Wan-AI/Wan2.1-T2V-1.3B--&amp;gt;Wan2.1-Fun-1.3B-Series;
    Wan2.1-Fun-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-1.3B-InP;
    Wan2.1-Fun-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-1.3B-Control;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan2.1-Fun-14B-Series;
    Wan2.1-Fun-14B-Series--&amp;gt;PAI/Wan2.1-Fun-14B-InP;
    Wan2.1-Fun-14B-Series--&amp;gt;PAI/Wan2.1-Fun-14B-Control;
    Wan-AI/Wan2.1-T2V-1.3B--&amp;gt;Wan2.1-Fun-V1.1-1.3B-Series;
    Wan2.1-Fun-V1.1-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control;
    Wan2.1-Fun-V1.1-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-1.3B-InP;
    Wan2.1-Fun-V1.1-1.3B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan2.1-Fun-V1.1-14B-Series;
    Wan2.1-Fun-V1.1-14B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-14B-Control;
    Wan2.1-Fun-V1.1-14B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-14B-InP;
    Wan2.1-Fun-V1.1-14B-Series--&amp;gt;PAI/Wan2.1-Fun-V1.1-14B-Control-Camera;
    Wan-AI/Wan2.1-T2V-1.3B--&amp;gt;DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;krea/krea-realtime-video;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;meituan-longcat/LongCat-Video;
    Wan-AI/Wan2.1-I2V-14B-720P--&amp;gt;ByteDance/Video-As-Prompt-Wan2.1-14B;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.2-Animate-14B;
    Wan-AI/Wan2.1-T2V-14B--&amp;gt;Wan-AI/Wan2.2-S2V-14B;
    Wan2.2-Series--&amp;gt;Wan-AI/Wan2.2-T2V-A14B;
    Wan2.2-Series--&amp;gt;Wan-AI/Wan2.2-I2V-A14B;
    Wan2.2-Series--&amp;gt;Wan-AI/Wan2.2-TI2V-5B;
    Wan-AI/Wan2.2-T2V-A14B--&amp;gt;Wan2.2-Fun-Series;
    Wan2.2-Fun-Series--&amp;gt;PAI/Wan2.2-VACE-Fun-A14B;
    Wan2.2-Fun-Series--&amp;gt;PAI/Wan2.2-Fun-A14B-InP;
    Wan2.2-Fun-Series--&amp;gt;PAI/Wan2.2-Fun-A14B-Control;
    Wan2.2-Fun-Series--&amp;gt;PAI/Wan2.2-Fun-A14B-Control-Camera;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Examples&lt;/summary&gt; 
 &lt;p&gt;Example code for Wan is available at: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/"&gt;/examples/wanvideo/&lt;/a&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Extra Args&lt;/th&gt; 
    &lt;th&gt;Inference&lt;/th&gt; 
    &lt;th&gt;Full Training&lt;/th&gt; 
    &lt;th&gt;Full Training Validation&lt;/th&gt; 
    &lt;th&gt;LoRA Training&lt;/th&gt; 
    &lt;th&gt;LoRA Training Validation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B"&gt;Wan-AI/Wan2.1-T2V-1.3B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-T2V-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-T2V-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-T2V-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-T2V-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B"&gt;Wan-AI/Wan2.1-T2V-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-T2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-T2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-T2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-T2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-T2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P"&gt;Wan-AI/Wan2.1-I2V-14B-480P&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-I2V-14B-480P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-I2V-14B-480P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-480P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-480P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-480P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P"&gt;Wan-AI/Wan2.1-I2V-14B-720P&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-I2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-I2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-I2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-I2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-I2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P"&gt;Wan-AI/Wan2.1-FLF2V-14B-720P&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-FLF2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-FLF2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-FLF2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-FLF2V-14B-720P.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-FLF2V-14B-720P.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/iic/VACE-Wan2.1-1.3B-Preview"&gt;iic/VACE-Wan2.1-1.3B-Preview&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-VACE-1.3B-Preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B-Preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B-Preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B-Preview.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B-Preview.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-1.3B"&gt;Wan-AI/Wan2.1-VACE-1.3B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-VACE-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-VACE-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-VACE-1.3B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-1.3B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.1-VACE-14B"&gt;Wan-AI/Wan2.1-VACE-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-VACE-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-VACE-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-VACE-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-VACE-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-VACE-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-InP"&gt;PAI/Wan2.1-Fun-1.3B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-1.3B-Control"&gt;PAI/Wan2.1-Fun-1.3B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-InP"&gt;PAI/Wan2.1-Fun-14B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-Control"&gt;PAI/Wan2.1-Fun-14B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control"&gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;, &lt;code&gt;reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control"&gt;PAI/Wan2.1-Fun-V1.1-14B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;, &lt;code&gt;reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-InP"&gt;PAI/Wan2.1-Fun-V1.1-1.3B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-InP"&gt;PAI/Wan2.1-Fun-V1.1-14B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera"&gt;PAI/Wan2.1-Fun-V1.1-1.3B-Control-Camera&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_camera_video&lt;/code&gt;, &lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-1.3B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.1-Fun-V1.1-14B-Control-Camera"&gt;PAI/Wan2.1-Fun-V1.1-14B-Control-Camera&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_camera_video&lt;/code&gt;, &lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-Fun-V1.1-14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-Fun-V1.1-14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-Fun-V1.1-14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-Fun-V1.1-14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-Fun-V1.1-14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1"&gt;DiffSynth-Studio/Wan2.1-1.3b-speedcontrol-v1&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;motion_bucket_id&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.1-1.3b-speedcontrol-v1.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.1-1.3b-speedcontrol-v1.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.1-1.3b-speedcontrol-v1.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.1-1.3b-speedcontrol-v1.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.1-1.3b-speedcontrol-v1.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/krea/krea-realtime-video"&gt;krea/krea-realtime-video&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/krea-realtime-video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/krea-realtime-video.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/krea-realtime-video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/krea-realtime-video.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/krea-realtime-video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/meituan-longcat/LongCat-Video"&gt;meituan-longcat/LongCat-Video&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;longcat_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/LongCat-Video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/LongCat-Video.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/LongCat-Video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/LongCat-Video.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/LongCat-Video.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/ByteDance/Video-As-Prompt-Wan2.1-14B"&gt;ByteDance/Video-As-Prompt-Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vap_video&lt;/code&gt;, &lt;code&gt;vap_prompt&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Video-As-Prompt-Wan2.1-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Video-As-Prompt-Wan2.1-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Video-As-Prompt-Wan2.1-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Video-As-Prompt-Wan2.1-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Video-As-Prompt-Wan2.1-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.2-T2V-A14B"&gt;Wan-AI/Wan2.2-T2V-A14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-T2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-T2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-T2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-T2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-T2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B"&gt;Wan-AI/Wan2.2-I2V-A14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-I2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-I2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-I2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-I2V-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-I2V-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/Wan-AI/Wan2.2-TI2V-5B"&gt;Wan-AI/Wan2.2-TI2V-5B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-TI2V-5B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-TI2V-5B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-TI2V-5B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-TI2V-5B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-TI2V-5B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Wan-AI/Wan2.2-Animate-14B"&gt;Wan-AI/Wan2.2-Animate-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;animate_pose_video&lt;/code&gt;, &lt;code&gt;animate_face_video&lt;/code&gt;, &lt;code&gt;animate_inpaint_video&lt;/code&gt;, &lt;code&gt;animate_mask_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-Animate-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-Animate-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-Animate-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-Animate-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-Animate-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/Wan-AI/Wan2.2-S2V-14B"&gt;Wan-AI/Wan2.2-S2V-14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;input_audio&lt;/code&gt;, &lt;code&gt;audio_sample_rate&lt;/code&gt;, &lt;code&gt;s2v_pose_video&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-S2V-14B_multi_clips.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-S2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-S2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-S2V-14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-S2V-14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/PAI/Wan2.2-VACE-Fun-A14B"&gt;PAI/Wan2.2-VACE-Fun-A14B&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;vace_control_video&lt;/code&gt;, &lt;code&gt;vace_reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-VACE-Fun-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-VACE-Fun-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-VACE-Fun-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-VACE-Fun-A14B.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-VACE-Fun-A14B.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.2-Fun-A14B-InP"&gt;PAI/Wan2.2-Fun-A14B-InP&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;input_image&lt;/code&gt;, &lt;code&gt;end_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-Fun-A14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-Fun-A14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-Fun-A14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-Fun-A14B-InP.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-Fun-A14B-InP.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.2-Fun-A14B-Control"&gt;PAI/Wan2.2-Fun-A14B-Control&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_video&lt;/code&gt;, &lt;code&gt;reference_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-Fun-A14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-Fun-A14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-Fun-A14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-Fun-A14B-Control.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-Fun-A14B-Control.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/PAI/Wan2.2-Fun-A14B-Control-Camera"&gt;PAI/Wan2.2-Fun-A14B-Control-Camera&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;control_camera_video&lt;/code&gt;, &lt;code&gt;input_image&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_inference/Wan2.2-Fun-A14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/full/Wan2.2-Fun-A14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_full/Wan2.2-Fun-A14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/lora/Wan2.2-Fun-A14B-Control-Camera.sh"&gt;code&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/wanvideo/model_training/validate_lora/Wan2.2-Fun-A14B-Control-Camera.py"&gt;code&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;Innovative Achievements&lt;/h2&gt; 
&lt;p&gt;DiffSynth-Studio is not just an engineered model framework, but also an incubator for innovative achievements.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;AttriCtrl: Attribute Intensity Control for Image Generation Models&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.02151"&gt;AttriCtrl: Fine-Grained Control of Aesthetic Attribute Intensity in Diffusion Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-AttriCtrl.py"&gt;/examples/flux/model_inference/FLUX.1-dev-AttriCtrl.py&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev"&gt;ModelScope&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;brightness scale = 0.1&lt;/th&gt; 
    &lt;th&gt;brightness scale = 0.3&lt;/th&gt; 
    &lt;th&gt;brightness scale = 0.5&lt;/th&gt; 
    &lt;th&gt;brightness scale = 0.7&lt;/th&gt; 
    &lt;th&gt;brightness scale = 0.9&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.1.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.3.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.5.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.7.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/AttriCtrl-FLUX.1-Dev/resolve/master/assets/brightness/value_control_0.9.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;AutoLoRA: Automated LoRA Retrieval and Fusion&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2508.02107"&gt;AutoLoRA: Automatic LoRA Retrieval and Fine-Grained Gated Fusion for Text-to-Image Generation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py"&gt;/examples/flux/model_inference/FLUX.1-dev-LoRA-Fusion.py&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev"&gt;ModelScope&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://modelscope.cn/models/cancel13/cxsk"&gt;LoRA 1&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://modelscope.cn/models/wy413928499/xuancai2"&gt;LoRA 2&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;LoRA 3&lt;/a&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;a href="https://modelscope.cn/models/hongyanbujian/JPL"&gt;LoRA 4&lt;/a&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/cancel13/cxsk"&gt;LoRA 1&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_0.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_1.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_3.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/wy413928499/xuancai2"&gt;LoRA 2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_1.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_1.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_3.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;LoRA 3&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_2_2.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_2_3.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://modelscope.cn/models/hongyanbujian/JPL"&gt;LoRA 4&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_0_3.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_1_3.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_2_3.jpg" alt="" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://www.modelscope.cn/models/DiffSynth-Studio/LoRAFusion-preview-FLUX.1-dev/resolve/master/assets/car/image_3_3.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Nexus-Gen: Unified Architecture for Image Understanding, Generation, and Editing&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detailed Page: &lt;a href="https://github.com/modelscope/Nexus-Gen"&gt;https://github.com/modelscope/Nexus-Gen&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/pdf/2504.21356"&gt;Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/Nexus-GenV2"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset"&gt;ModelScope Dataset&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen"&gt;ModelScope Nexus-Gen Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;img src="https://github.com/modelscope/Nexus-Gen/raw/main/assets/illustrations/gen_edit.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ArtAug: Aesthetic Enhancement for Image Generation Models&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detailed Page: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ArtAug/"&gt;./examples/ArtAug/&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2412.12888"&gt;ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/aigc/imageGeneration?tab=advanced&amp;amp;versionId=7228&amp;amp;modelType=LoRA&amp;amp;sdVersion=FLUX_1&amp;amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0"&gt;ModelScope AIGC Tab&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;FLUX.1-dev&lt;/th&gt; 
    &lt;th&gt;FLUX.1-dev + ArtAug LoRA&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/e1d5c505-b423-45fe-be01-25c2758f5417" alt="image_1_base" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/335908e3-d0bd-41c2-9d99-d10528a2d719" alt="image_1_enhance" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;EliGen: Precise Image Partition Control&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2501.01097"&gt;EliGen: Entity-Level Controlled Image Generation with Regional Attention&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: &lt;a href="https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/flux/model_inference/FLUX.1-dev-EliGen.py"&gt;/examples/flux/model_inference/FLUX.1-dev-EliGen.py&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://www.modelscope.cn/models/DiffSynth-Studio/Eligen"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/modelscope/EliGen"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Online Experience: &lt;a href="https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen"&gt;ModelScope EliGen Studio&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dataset: &lt;a href="https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet"&gt;EliGen Train Set&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Entity Control Region&lt;/th&gt; 
    &lt;th&gt;Generated Image&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/1c6d9445-5022-4d91-ad2e-dc05321883d1" alt="eligen_example_2_mask_0" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/86739945-cb07-4a49-b3b3-3bb65c90d14f" alt="eligen_example_2_0" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ExVideo: Extended Training for Video Generation Models&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Project Page: &lt;a href="https://ecnu-cilab.github.io/ExVideoProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2406.14130"&gt;ExVideo: Extending Video Diffusion Models via Parameter-Efficient Post-Tuning&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: Please refer to the &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/ExVideo"&gt;older version&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Model: &lt;a href="https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;ModelScope&lt;/a&gt;, &lt;a href="https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc"&gt;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Diffutoon: High-Resolution Anime-Style Video Rendering&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Project Page: &lt;a href="https://ecnu-cilab.github.io/DiffutoonProjectPage/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2401.16224"&gt;Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: Please refer to the &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/Diffutoon"&gt;older version&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd"&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DiffSynth: The Original Version of This Project&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Project Page: &lt;a href="https://ecnu-cilab.github.io/DiffSynth.github.io/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2308.03463"&gt;DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sample Code: Please refer to the &lt;a href="https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/diffsynth"&gt;older version&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea"&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt;</description>
    </item>
    
  </channel>
</rss>