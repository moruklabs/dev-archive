<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Tue, 02 Sep 2025 01:31:18 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>JetBrains/koog</title>
      <link>https://github.com/JetBrains/koog</link>
      <description>&lt;p&gt;Koog is the official Kotlin framework for building and running robust, scalable and production-ready AI agents across all platforms – from backend services to Android and iOS, JVM, and even in-browser environments. Koog is based on our AI products expertise and provides proven solutions for complex LLM and AI problems&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Koog&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://kotlinlang.org/docs/components-stability.html"&gt;&lt;img src="https://kotl.in/badges/alpha.svg?sanitize=true" alt="Kotlin Alpha" /&gt;&lt;/a&gt; &lt;a href="https://search.maven.org/artifact/ai.koog/koog-agents"&gt;&lt;img src="https://img.shields.io/maven-central/v/ai.koog/koog-agents" alt="Maven Central" /&gt;&lt;/a&gt; &lt;a href="https://github.com/JetBrains#jetbrains-on-github"&gt;&lt;img src="https://jb.gg/badges/incubator.svg?sanitize=true" alt="JetBrains incubator project" /&gt;&lt;/a&gt; &lt;a href="http://kotlinlang.org"&gt;&lt;img src="https://img.shields.io/badge/kotlin-2.1-blue.svg?logo=kotlin" alt="Kotlin" /&gt;&lt;/a&gt; &lt;a href="https://github.com/JetBrains/koog/actions?query=branch%3Amain"&gt;&lt;img src="https://img.shields.io/github/checks-status/JetBrains/koog/main" alt="CI status" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/JetBrains/koog/develop/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/JetBrains/koog" alt="GitHub license" /&gt;&lt;/a&gt; &lt;a href="https://docs.koog.ai"&gt;&lt;img src="https://img.shields.io/badge/documentation-blue" alt="docs" /&gt;&lt;/a&gt; &lt;a href="https://kotlinlang.slack.com/messages/koog-agentic-framework/"&gt;&lt;img src="https://img.shields.io/badge/chat-slack-green.svg?logo=slack" alt="Slack channel" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Koog is a Kotlin-based framework designed to build and run AI agents entirely in idiomatic Kotlin. It lets you create agents that can interact with tools, handle complex workflows, and communicate with users.&lt;/p&gt; 
&lt;h3&gt;Key features&lt;/h3&gt; 
&lt;p&gt;Key features of Koog include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pure Kotlin implementation&lt;/strong&gt;: Build AI agents entirely in natural and idiomatic Kotlin.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP integration&lt;/strong&gt;: Connect to Model Context Protocol for enhanced model management.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Embedding capabilities&lt;/strong&gt;: Use vector embeddings for semantic search and knowledge retrieval.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom tool creation&lt;/strong&gt;: Extend your agents with tools that access external systems and APIs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ready-to-use components&lt;/strong&gt;: Speed up development with pre-built solutions for common AI engineering challenges.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent history compression&lt;/strong&gt;: Optimize token usage while maintaining conversation context using various pre-built strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Powerful Streaming API&lt;/strong&gt;: Process responses in real-time with streaming support and parallel tool calls.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Persistent agent memory&lt;/strong&gt;: Enable knowledge retention across sessions and even different agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive tracing&lt;/strong&gt;: Debug and monitor agent execution with detailed and configurable tracing.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible graph workflows&lt;/strong&gt;: Design complex agent behaviors using intuitive graph-based workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular feature system&lt;/strong&gt;: Customize agent capabilities through a composable architecture.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable architecture&lt;/strong&gt;: Handle workloads from simple chatbots to enterprise applications.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiplatform&lt;/strong&gt;: Run agents on JVM, JS, WasmJS, iOS targets with Kotlin Multiplatform.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Available LLM providers and platforms&lt;/h3&gt; 
&lt;p&gt;The LLM providers and platforms whose LLMs you can use to power your agent capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Google&lt;/li&gt; 
 &lt;li&gt;OpenAI&lt;/li&gt; 
 &lt;li&gt;Anthropic&lt;/li&gt; 
 &lt;li&gt;OpenRouter&lt;/li&gt; 
 &lt;li&gt;Ollama&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quickstart example&lt;/h3&gt; 
&lt;p&gt;To help you get started with AI agents, here is a quick example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-kotlin"&gt;fun main() = runBlocking {
    // Before you run the example, assign a corresponding API key as an environment variable.
   val apiKey = System.getenv("OPENAI_API_KEY") // or Anthropic, Google, OpenRouter, etc.

   val agent = AIAgent(
      executor = simpleOpenAIExecutor(apiKey), // or Anthropic, Google, OpenRouter, etc.
      systemPrompt = "You are a helpful assistant. Answer user questions concisely.",
      llmModel = OpenAIModels.Chat.GPT4o
   )

   val result = agent.run("Hello! How can you help me?")
   println(result)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using in your projects&lt;/h2&gt; 
&lt;h3&gt;Supported targets&lt;/h3&gt; 
&lt;p&gt;Currently, the framework supports the JVM, JS, WasmJS and iOS targets.&lt;/p&gt; 
&lt;p&gt;On JVM, JDK 17 or higher is required to use the framework.&lt;/p&gt; 
&lt;p&gt;Please check the &lt;a href="https://raw.githubusercontent.com/JetBrains/koog/develop/gradle/libs.versions.toml"&gt;libs.versions.toml&lt;/a&gt; to know more about the Koog dependencies.&lt;/p&gt; 
&lt;h3&gt;Gradle (Kotlin DSL)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Add dependencies to the &lt;code&gt;build.gradle.kts&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dependencies {
    implementation("ai.koog:koog-agents:0.4.1")
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have &lt;code&gt;mavenCentral()&lt;/code&gt; in the list of repositories.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Gradle (Groovy)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Add dependencies to the &lt;code&gt;build.gradle&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dependencies {
    implementation 'ai.koog:koog-agents:0.4.1'
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have &lt;code&gt;mavenCentral()&lt;/code&gt; in the list of repositories.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Maven&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Add dependencies to the &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;ai.koog&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;koog-agents-jvm&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.4.1&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have &lt;code&gt;mavenCentral&lt;/code&gt; in the list of repositories.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Read the &lt;a href="https://raw.githubusercontent.com/JetBrains/koog/develop/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;This project and the corresponding community are governed by the &lt;a href="https://github.com/jetbrains#code-of-conduct"&gt;JetBrains Open Source and Community Code of Conduct&lt;/a&gt;. Please make sure you read it.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Koog is licensed under the &lt;a href="https://raw.githubusercontent.com/JetBrains/koog/develop/LICENSE.txt"&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Please feel free to ask any questions in our official Slack channel (&lt;a href="https://kotlinlang.slack.com/messages/koog-agentic-framework/"&gt;link&lt;/a&gt;) and to use &lt;a href="https://youtrack.jetbrains.com/issues/KG"&gt;Koog official YouTrack project&lt;/a&gt; for filing feature requests and bug reports.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QuentinFuxa/WhisperLiveKit</title>
      <link>https://github.com/QuentinFuxa/WhisperLiveKit</link>
      <description>&lt;p&gt;Real-time &amp; local speech-to-text, translation, and speaker diarization. With server &amp; web UI.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;WhisperLiveKit&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png" alt="WhisperLiveKit Demo" width="730" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&lt;b&gt;Real-time, Fully Local Speech-to-Text with Speaker Identification&lt;/b&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="PyPI Version" src="https://img.shields.io/pypi/v/whisperlivekit?color=g" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/whisperlivekit"&gt;&lt;img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/whisperlivekit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=installations" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="Python Versions" src="https://img.shields.io/badge/python-3.9--3.13-dark_green" /&gt;&lt;/a&gt; &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/badge/License-MIT/Dual Licensed-dark_green" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Real-time speech transcription directly to your browser, with a ready-to-use backend+server and a simple frontend. ✨&lt;/p&gt; 
&lt;h4&gt;Powered by Leading Research:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/SimulStreaming"&gt;SimulStreaming&lt;/a&gt; (SOTA 2025) - Ultra-low latency transcription with AlignAtt policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;WhisperStreaming&lt;/a&gt; (SOTA 2023) - Low latency transcription with LocalAgreement policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2507.18446"&gt;Streaming Sortformer&lt;/a&gt; (SOTA 2025) - Advanced real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/juanmc2005/diart"&gt;Diart&lt;/a&gt; (SOTA 2021) - Real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; (2024) - Enterprise-grade Voice Activity Detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Why not just run a simple Whisper model on every audio batch?&lt;/strong&gt; Whisper is designed for complete utterances, not real-time chunks. Processing small segments loses context, cuts off words mid-syllable, and produces poor transcription. WhisperLiveKit uses state-of-the-art simultaneous speech research for intelligent buffering and incremental processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Architecture&lt;/h3&gt; 
&lt;img alt="Architecture" src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png" /&gt; 
&lt;p&gt;&lt;em&gt;The backend supports multiple concurrent users. Voice Activity Detection reduces overhead when no voice is detected.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Installation &amp;amp; Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install whisperlivekit
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;FFmpeg is required&lt;/strong&gt; and must be installed before using WhisperLiveKit&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;OS&lt;/th&gt; 
    &lt;th&gt;How to install&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Ubuntu/Debian&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;sudo apt install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MacOS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;brew install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Windows&lt;/td&gt; 
    &lt;td&gt;Download .exe from &lt;a href="https://ffmpeg.org/download.html"&gt;https://ffmpeg.org/download.html&lt;/a&gt; and add to PATH&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the transcription server:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;whisperlivekit-server --model base --language en
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open your browser&lt;/strong&gt; and navigate to &lt;code&gt;http://localhost:8000&lt;/code&gt;. Start speaking and watch your words appear in real-time!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;See &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/li&gt; 
  &lt;li&gt;For HTTPS requirements, see the &lt;strong&gt;Parameters&lt;/strong&gt; section for SSL configuration options.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;pip install&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speaker diarization with Sortformer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speaker diarization with Diart&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Original Whisper backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Improved timestamps backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper-timestamped&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Apple Silicon optimization backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mlx-whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI API backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;strong&gt;Parameters &amp;amp; Configuration&lt;/strong&gt; below on how to use them.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Command-line Interface&lt;/strong&gt;: Start the transcription server with various options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use better model than default (small)
whisperlivekit-server --model large-v3

# Advanced configuration with diarization and language
whisperlivekit-server --host 0.0.0.0 --port 8000 --model medium --diarization --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Python API Integration&lt;/strong&gt;: Check &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/basic_server.py"&gt;basic_server&lt;/a&gt; for a more complete example of how to use the functions and classes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from whisperlivekit import TranscriptionEngine, AudioProcessor, parse_args
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
import asyncio

transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    transcription_engine = TranscriptionEngine(model="medium", diarization=True, lan="en")
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket: WebSocket, results_generator):
    async for response in results_generator:
        await websocket.send_json(response)
    await websocket.send_json({"type": "ready_to_stop"})

@app.websocket("/asr")
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # Create a new AudioProcessor for each connection, passing the shared engine
    audio_processor = AudioProcessor(transcription_engine=transcription_engine)    
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(handle_websocket_results(websocket, results_generator))
    await websocket.accept()
    while True:
        message = await websocket.receive_bytes()
        await audio_processor.process_audio(message)        
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Frontend Implementation&lt;/strong&gt;: The package includes an HTML/JavaScript implementation &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/web/live_transcription.html"&gt;here&lt;/a&gt;. You can also import it using &lt;code&gt;from whisperlivekit import get_inline_ui_html&lt;/code&gt; &amp;amp; &lt;code&gt;page = get_inline_ui_html()&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Parameters &amp;amp; Configuration&lt;/h2&gt; 
&lt;p&gt;An important list of parameters can be changed. But what &lt;em&gt;should&lt;/em&gt; you change?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;--model&lt;/code&gt; size. List and recommandations &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/available_models.md"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--language&lt;/code&gt;. List &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;here&lt;/a&gt;. If you use &lt;code&gt;auto&lt;/code&gt;, the model attempts to detect the language automatically, but it tends to bias towards English.&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--backend&lt;/code&gt; ? you can switch to &lt;code&gt;--backend faster-whisper&lt;/code&gt; if &lt;code&gt;simulstreaming&lt;/code&gt; does not work correctly or if you prefer to avoid the dual-license requirements.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--warmup-file&lt;/code&gt;, if you have one&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;, &lt;code&gt;--port&lt;/code&gt;, &lt;code&gt;--ssl-certfile&lt;/code&gt;, &lt;code&gt;--ssl-keyfile&lt;/code&gt;, if you set up a server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--diarization&lt;/code&gt;, if you want to use it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The rest I don't recommend. But below are your options.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Parameter&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whisper model size.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source language code or &lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--task&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt; or &lt;code&gt;translate&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Processing backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;simulstreaming&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--min-chunk-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio chunk size (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vac&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Controller&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vad&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--warmup-file&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio file path for model warmup&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jfk.wav&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server host address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-certfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL certificate file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-keyfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL private key file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;WhisperStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--confidence-validation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use confidence scores for faster validation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--buffer_trimming&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Buffer trimming strategy (&lt;code&gt;sentence&lt;/code&gt; or &lt;code&gt;segment&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;segment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SimulStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--frame-threshold&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AlignAtt frame threshold (lower = faster, higher = more accurate)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--beams&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Number of beams for beam search (1 = greedy decoding)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--decoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Force decoder type (&lt;code&gt;beam&lt;/code&gt; or &lt;code&gt;greedy&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-max-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum audio buffer length (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;30.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-min-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio length to process (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--cif-ckpt-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to CIF model for word boundary detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--never-fire&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Never truncate incomplete words&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for the model&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--static-init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Static prompt that doesn't scroll&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--max-context-tokens&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum context tokens&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Direct path to .pt model file. Download it if not found&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;./base.pt&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--preloaded-model-count&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Optional. Number of models to preload in memory to speed up loading (set up to the expected number of concurrent users)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Diarization options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable speaker identification&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt; or &lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--segmentation-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart segmentation model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--embedding-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart embedding model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;speechbrain/spkrec-ecapa-voxceleb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For diarization using Diart, you need access to pyannote.audio models:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation-3.0"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/embedding"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/embedding&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;Login with HuggingFace: &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🚀 Deployment Guide&lt;/h3&gt; 
&lt;p&gt;To deploy WhisperLiveKit in production:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Server Setup&lt;/strong&gt;: Install production ASGI server &amp;amp; launch with multiple workers&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install uvicorn gunicorn
gunicorn -k uvicorn.workers.UvicornWorker -w 4 your_app:app
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Host your customized version of the &lt;code&gt;html&lt;/code&gt; example &amp;amp; ensure WebSocket connection points correctly&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nginx Configuration&lt;/strong&gt; (recommended for production):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-nginx"&gt;server {
   listen 80;
   server_name your-domain.com;
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
}}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTPS Support&lt;/strong&gt;: For secure deployments, use "wss://" instead of "ws://" in WebSocket URL&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🐋 Docker&lt;/h2&gt; 
&lt;p&gt;Deploy the application easily using Docker with GPU or CPU support.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker installed on your system&lt;/li&gt; 
 &lt;li&gt;For GPU support: NVIDIA Docker runtime installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;With GPU acceleration (recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t wlk .
docker run --gpus all -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CPU only:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f Dockerfile.cpu -t wlk .
docker run -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Custom configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with custom model and language
docker run --gpus all -p 8000:8000 --name wlk wlk --model large-v3 --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Memory Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Large models&lt;/strong&gt;: Ensure your Docker runtime has sufficient memory allocated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--build-arg&lt;/code&gt; Options: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;EXTRAS="whisper-timestamped"&lt;/code&gt; - Add extras to the image's installation (no spaces). Remember to set necessary container options!&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_PRECACHE_DIR="./.cache/"&lt;/code&gt; - Pre-load a model cache for faster first-time start&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_TKN_FILE="./token"&lt;/code&gt; - Add your Hugging Face Hub access token to download gated models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔮 Use Cases&lt;/h2&gt; 
&lt;p&gt;Capture discussions in real-time for meeting transcription, help hearing-impaired users follow conversations through accessibility tools, transcribe podcasts or videos automatically for content creation, transcribe support calls with speaker identification for customer service...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lllyasviel/Fooocus</title>
      <link>https://github.com/lllyasviel/Fooocus</link>
      <description>&lt;p&gt;Focus on prompting and generating&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/483fb86d-c9a2-4c20-997c-46dafc124f25" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;Fooocus&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#download"&gt;&amp;gt;&amp;gt;&amp;gt; Click Here to Install Fooocus &amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Fooocus is an image generating software (based on &lt;a href="https://www.gradio.app/"&gt;Gradio&lt;/a&gt; &lt;a href="https://github.com/gradio-app/gradio"&gt;&lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Fooocus presents a rethinking of image generator designs. The software is offline, open source, and free, while at the same time, similar to many online image generators like Midjourney, the manual tweaking is not needed, and users only need to focus on the prompts and images. Fooocus has also simplified the installation: between pressing "download" and generating the first image, the number of needed mouse clicks is strictly limited to less than 3. Minimal GPU memory requirement is 4GB (Nvidia).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Recently many fake websites exist on Google when you search “fooocus”. Do not trust those – here is the only official source of Fooocus.&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;Project Status: Limited Long-Term Support (LTS) with Bug Fixes Only&lt;/h1&gt; 
&lt;p&gt;The Fooocus project, built entirely on the &lt;strong&gt;Stable Diffusion XL&lt;/strong&gt; architecture, is now in a state of limited long-term support (LTS) with bug fixes only. As the existing functionalities are considered as nearly free of programmartic issues (Thanks to &lt;a href="https://github.com/mashb1t"&gt;mashb1t&lt;/a&gt;'s huge efforts), future updates will focus exclusively on addressing any bugs that may arise.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There are no current plans to migrate to or incorporate newer model architectures.&lt;/strong&gt; However, this may change during time with the development of open-source community. For example, if the community converge to one single dominant method for image generation (which may really happen in half or one years given the current status), Fooocus may also migrate to that exact method.&lt;/p&gt; 
&lt;p&gt;For those interested in utilizing newer models such as &lt;strong&gt;Flux&lt;/strong&gt;, we recommend exploring alternative platforms such as &lt;a href="https://github.com/lllyasviel/stable-diffusion-webui-forge"&gt;WebUI Forge&lt;/a&gt; (also from us), &lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI/SwarmUI&lt;/a&gt;. Additionally, several &lt;a href="https://github.com/lllyasviel/Fooocus?tab=readme-ov-file#forks"&gt;excellent forks of Fooocus&lt;/a&gt; are available for experimentation.&lt;/p&gt; 
&lt;p&gt;Again, recently many fake websites exist on Google when you search “fooocus”. Do &lt;strong&gt;NOT&lt;/strong&gt; get Fooocus from those websites – this page is the only official source of Fooocus. We never have any website like such as “fooocus.com”, “fooocus.net”, “fooocus.co”, “fooocus.ai”, “fooocus.org”, “fooocus.pro”, “fooocus.one”. Those websites are ALL FAKE. &lt;strong&gt;They have ABSOLUTLY no relationship to us. Fooocus is a 100% non-commercial offline open-source software.&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;p&gt;Below is a quick list using Midjourney's examples:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Midjourney&lt;/th&gt; 
   &lt;th&gt;Fooocus&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-quality text-to-image without needing much prompt engineering or parameter tuning. &lt;br /&gt; (Unknown method)&lt;/td&gt; 
   &lt;td&gt;High-quality text-to-image without needing much prompt engineering or parameter tuning. &lt;br /&gt; (Fooocus has an offline GPT-2 based prompt processing engine and lots of sampling improvements so that results are always beautiful, no matter if your prompt is as short as “house in garden” or as long as 1000 words)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;V1 V2 V3 V4&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Upscale or Variation -&amp;gt; Vary (Subtle) / Vary (Strong)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;U1 U2 U3 U4&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Upscale or Variation -&amp;gt; Upscale (1.5x) / Upscale (2x)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inpaint / Up / Down / Left / Right (Pan)&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Inpaint or Outpaint -&amp;gt; Inpaint / Up / Down / Left / Right &lt;br /&gt; (Fooocus uses its own inpaint algorithm and inpaint models so that results are more satisfying than all other software that uses standard SDXL inpaint method/model)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Image Prompt&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Image Prompt &lt;br /&gt; (Fooocus uses its own image prompt algorithm so that result quality and prompt understanding are more satisfying than all other software that uses standard SDXL methods like standard IP-Adapters or Revisions)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--style&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Style&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--stylize&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Advanced -&amp;gt; Guidance&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--niji&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/discussions/679"&gt;Multiple launchers: "run.bat", "run_anime.bat", and "run_realistic.bat".&lt;/a&gt; &lt;br /&gt; Fooocus support SDXL models on Civitai &lt;br /&gt; (You can google search “Civitai” if you do not know about it)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--quality&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Quality&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--repeat&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Image Number&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi Prompts (::)&lt;/td&gt; 
   &lt;td&gt;Just use multiple lines of prompts&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Prompt Weights&lt;/td&gt; 
   &lt;td&gt;You can use " I am (happy:1.5)". &lt;br /&gt; Fooocus uses A1111's reweighting algorithm so that results are better than ComfyUI if users directly copy prompts from Civitai. (Because if prompts are written in ComfyUI's reweighting, users are less likely to copy prompt texts as they prefer dragging files) &lt;br /&gt; To use embedding, you can use "(embedding:file_name:1.1)"&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--no&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Negative Prompt&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;--ar&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Aspect Ratios&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;InsightFace&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Image Prompt -&amp;gt; Advanced -&amp;gt; FaceSwap&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Describe&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Describe&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Below is a quick list using LeonardoAI's examples:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;LeonardoAI&lt;/th&gt; 
   &lt;th&gt;Fooocus&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Prompt Magic&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Style -&amp;gt; Fooocus V2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Advanced Sampler Parameters (like Contrast/Sharpness/etc)&lt;/td&gt; 
   &lt;td&gt;Advanced -&amp;gt; Advanced -&amp;gt; Sampling Sharpness / etc&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;User-friendly ControlNets&lt;/td&gt; 
   &lt;td&gt;Input Image -&amp;gt; Image Prompt -&amp;gt; Advanced&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Also, &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/117"&gt;click here to browse the advanced features.&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Download&lt;/h1&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;You can directly download Fooocus with:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/releases/download/v2.5.0/Fooocus_win64_2-5-0.7z"&gt;&amp;gt;&amp;gt;&amp;gt; Click here to download &amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;After you download the file, please uncompress it and then run the "run.bat".&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/c49269c4-c274-4893-b368-047c401cc58c" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;The first time you launch the software, it will automatically download models:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It will download &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#models"&gt;default models&lt;/a&gt; to the folder "Fooocus\models\checkpoints" given different presets. You can download them in advance if you do not want automatic download.&lt;/li&gt; 
 &lt;li&gt;Note that if you use inpaint, at the first time you inpaint an image, it will download &lt;a href="https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/inpaint_v26.fooocus.patch"&gt;Fooocus's own inpaint control model from here&lt;/a&gt; as the file "Fooocus\models\inpaint\inpaint_v26.fooocus.patch" (the size of this file is 1.28GB).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After Fooocus 2.1.60, you will also have &lt;code&gt;run_anime.bat&lt;/code&gt; and &lt;code&gt;run_realistic.bat&lt;/code&gt;. They are different model presets (and require different models, but they will be automatically downloaded). &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/679"&gt;Check here for more details&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;After Fooocus 2.3.0 you can also switch presets directly in the browser. Keep in mind to add these arguments if you want to change the default behavior:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;--disable-preset-selection&lt;/code&gt; to disable preset selection in the browser.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;--always-download-new-model&lt;/code&gt; to download missing models on preset switch. Default is fallback to &lt;code&gt;previous_default_models&lt;/code&gt; defined in the corresponding preset, also see terminal output.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/d386f817-4bd7-490c-ad89-c1e228c23447" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;If you already have these files, you can copy them to the above locations to speed up installation.&lt;/p&gt; 
&lt;p&gt;Note that if you see &lt;strong&gt;"MetadataIncompleteBuffer" or "PytorchStreamReader"&lt;/strong&gt;, then your model files are corrupted. Please download models again.&lt;/p&gt; 
&lt;p&gt;Below is a test on a relatively low-end laptop with &lt;strong&gt;16GB System RAM&lt;/strong&gt; and &lt;strong&gt;6GB VRAM&lt;/strong&gt; (Nvidia 3060 laptop). The speed on this machine is about 1.35 seconds per iteration. Pretty impressive – nowadays laptops with 3060 are usually at very acceptable price.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/938737a5-b105-4f19-b051-81356cb7c495" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;Besides, recently many other software report that Nvidia driver above 532 is sometimes 10x slower than Nvidia driver 531. If your generation time is very long, consider download &lt;a href="https://www.nvidia.com/download/driverResults.aspx/199991/en-us/"&gt;Nvidia Driver 531 Laptop&lt;/a&gt; or &lt;a href="https://www.nvidia.com/download/driverResults.aspx/199990/en-us/"&gt;Nvidia Driver 531 Desktop&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note that the minimal requirement is &lt;strong&gt;4GB Nvidia GPU memory (4GB VRAM)&lt;/strong&gt; and &lt;strong&gt;8GB system memory (8GB RAM)&lt;/strong&gt;. This requires using Microsoft’s Virtual Swap technique, which is automatically enabled by your Windows installation in most cases, so you often do not need to do anything about it. However, if you are not sure, or if you manually turned it off (would anyone really do that?), or &lt;strong&gt;if you see any "RuntimeError: CPUAllocator"&lt;/strong&gt;, you can enable it here:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click here to see the image instructions. &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://github.com/lllyasviel/Fooocus/assets/19834515/2a06b130-fe9b-4504-94f1-2763be4476e9" alt="image" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;And make sure that you have at least 40GB free space on each drive if you still see "RuntimeError: CPUAllocator" !&lt;/strong&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;Please open an issue if you use similar devices but still cannot achieve acceptable performances.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#minimal-requirement"&gt;minimal requirement&lt;/a&gt; for different platforms is different.&lt;/p&gt; 
&lt;p&gt;See also the common problems and troubleshoots &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/troubleshoot.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Colab&lt;/h3&gt; 
&lt;p&gt;(Last tested - 2024 Aug 12 by &lt;a href="https://github.com/mashb1t"&gt;mashb1t&lt;/a&gt;)&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Colab&lt;/th&gt; 
   &lt;th&gt;Info&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/fooocus_colab.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Fooocus Official&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;In Colab, you can modify the last line to &lt;code&gt;!python entry_with_update.py --share --always-high-vram&lt;/code&gt; or &lt;code&gt;!python entry_with_update.py --share --always-high-vram --preset anime&lt;/code&gt; or &lt;code&gt;!python entry_with_update.py --share --always-high-vram --preset realistic&lt;/code&gt; for Fooocus Default/Anime/Realistic Edition.&lt;/p&gt; 
&lt;p&gt;You can also change the preset in the UI. Please be aware that this may lead to timeouts after 60 seconds. If this is the case, please wait until the download has finished, change the preset to initial and back to the one you've selected or reload the page.&lt;/p&gt; 
&lt;p&gt;Note that this Colab will disable refiner by default because Colab free's resources are relatively limited (and some "big" features like image prompt may cause free-tier Colab to disconnect). We make sure that basic text-to-image is always working on free-tier Colab.&lt;/p&gt; 
&lt;p&gt;Using &lt;code&gt;--always-high-vram&lt;/code&gt; shifts resource allocation from RAM to VRAM and achieves the overall best balance between performance, flexibility and stability on the default T4 instance. Please find more information &lt;a href="https://github.com/lllyasviel/Fooocus/pull/1710#issuecomment-1989185346"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thanks to &lt;a href="https://github.com/camenduru"&gt;camenduru&lt;/a&gt; for the template!&lt;/p&gt; 
&lt;h3&gt;Linux (Using Anaconda)&lt;/h3&gt; 
&lt;p&gt;If you want to use Anaconda/Miniconda, you can&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git
cd Fooocus
conda env create -f environment.yaml
conda activate fooocus
pip install -r requirements_versions.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then download the models: download &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#models"&gt;default models&lt;/a&gt; to the folder "Fooocus\models\checkpoints". &lt;strong&gt;Or let Fooocus automatically download the models&lt;/strong&gt; using the launcher:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda activate fooocus
python entry_with_update.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you want to open a remote port, use&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda activate fooocus
python entry_with_update.py --listen
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Linux (Using Python Venv)&lt;/h3&gt; 
&lt;p&gt;Your Linux needs to have &lt;strong&gt;Python 3.10&lt;/strong&gt; installed, and let's say your Python can be called with the command &lt;strong&gt;python3&lt;/strong&gt; with your venv system working; you can&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git
cd Fooocus
python3 -m venv fooocus_env
source fooocus_env/bin/activate
pip install -r requirements_versions.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the above sections for model downloads. You can launch the software with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;source fooocus_env/bin/activate
python entry_with_update.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you want to open a remote port, use&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;source fooocus_env/bin/activate
python entry_with_update.py --listen
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Linux (Using native system Python)&lt;/h3&gt; 
&lt;p&gt;If you know what you are doing, and your Linux already has &lt;strong&gt;Python 3.10&lt;/strong&gt; installed, and your Python can be called with the command &lt;strong&gt;python3&lt;/strong&gt; (and Pip with &lt;strong&gt;pip3&lt;/strong&gt;), you can&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git
cd Fooocus
pip3 install -r requirements_versions.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the above sections for model downloads. You can launch the software with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 entry_with_update.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, if you want to open a remote port, use&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 entry_with_update.py --listen
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Linux (AMD GPUs)&lt;/h3&gt; 
&lt;p&gt;Note that the &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#minimal-requirement"&gt;minimal requirement&lt;/a&gt; for different platforms is different.&lt;/p&gt; 
&lt;p&gt;Same with the above instructions. You need to change torch to the AMD version&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip uninstall torch torchvision torchaudio torchtext functorch xformers 
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;AMD is not intensively tested, however. The AMD support is in beta.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Windows (AMD GPUs)&lt;/h3&gt; 
&lt;p&gt;Note that the &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#minimal-requirement"&gt;minimal requirement&lt;/a&gt; for different platforms is different.&lt;/p&gt; 
&lt;p&gt;Same with Windows. Download the software and edit the content of &lt;code&gt;run.bat&lt;/code&gt; as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.\python_embeded\python.exe -m pip uninstall torch torchvision torchaudio torchtext functorch xformers -y
.\python_embeded\python.exe -m pip install torch-directml
.\python_embeded\python.exe -s Fooocus\entry_with_update.py --directml
pause
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the &lt;code&gt;run.bat&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;AMD is not intensively tested, however. The AMD support is in beta.&lt;/p&gt; 
&lt;p&gt;For AMD, use &lt;code&gt;.\python_embeded\python.exe Fooocus\entry_with_update.py --directml --preset anime&lt;/code&gt; or &lt;code&gt;.\python_embeded\python.exe Fooocus\entry_with_update.py --directml --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;p&gt;Note that the &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#minimal-requirement"&gt;minimal requirement&lt;/a&gt; for different platforms is different.&lt;/p&gt; 
&lt;p&gt;Mac is not intensively tested. Below is an unofficial guideline for using Mac. You can discuss problems &lt;a href="https://github.com/lllyasviel/Fooocus/pull/129"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can install Fooocus on Apple Mac silicon (M1 or M2) with macOS 'Catalina' or a newer version. Fooocus runs on Apple silicon computers via &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt; MPS device acceleration. Mac Silicon computers don't come with a dedicated graphics card, resulting in significantly longer image processing times compared to computers with dedicated graphics cards.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the conda package manager and pytorch nightly. Read the &lt;a href="https://developer.apple.com/metal/pytorch/"&gt;Accelerated PyTorch training on Mac&lt;/a&gt; Apple Developer guide for instructions. Make sure pytorch recognizes your MPS device.&lt;/li&gt; 
 &lt;li&gt;Open the macOS Terminal app and clone this repository with &lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Change to the new Fooocus directory, &lt;code&gt;cd Fooocus&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Create a new conda environment, &lt;code&gt;conda env create -f environment.yaml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Activate your new conda environment, &lt;code&gt;conda activate fooocus&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install the packages required by Fooocus, &lt;code&gt;pip install -r requirements_versions.txt&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Launch Fooocus by running &lt;code&gt;python entry_with_update.py&lt;/code&gt;. (Some Mac M2 users may need &lt;code&gt;python entry_with_update.py --disable-offload-from-vram&lt;/code&gt; to speed up model loading/unloading.) The first time you run Fooocus, it will automatically download the Stable Diffusion SDXL models and will take a significant amount of time, depending on your internet connection.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Use &lt;code&gt;python entry_with_update.py --preset anime&lt;/code&gt; or &lt;code&gt;python entry_with_update.py --preset realistic&lt;/code&gt; for Fooocus Anime/Realistic Edition.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/docker.md"&gt;docker.md&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Download Previous Version&lt;/h3&gt; 
&lt;p&gt;See the guidelines &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/1405"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Minimal Requirement&lt;/h2&gt; 
&lt;p&gt;Below is the minimal requirement for running Fooocus locally. If your device capability is lower than this spec, you may not be able to use Fooocus locally. (Please let us know, in any case, if your device capability is lower but Fooocus still works.)&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;th&gt;GPU&lt;/th&gt; 
   &lt;th&gt;Minimal GPU Memory&lt;/th&gt; 
   &lt;th&gt;Minimal System Memory&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/troubleshoot.md"&gt;System Swap&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia RTX 4XXX&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;fastest&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia RTX 3XXX&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;usually faster than RTX 2XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia RTX 2XXX&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;usually faster than GTX 1XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia GTX 1XXX&lt;/td&gt; 
   &lt;td&gt;8GB (* 6GB uncertain)&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;only marginally faster than CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia GTX 9XX&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;faster or slower than CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux&lt;/td&gt; 
   &lt;td&gt;Nvidia GTX &amp;lt; 9XX&lt;/td&gt; 
   &lt;td&gt;Not supported&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
   &lt;td&gt;/&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;AMD GPU&lt;/td&gt; 
   &lt;td&gt;8GB (updated 2023 Dec 30)&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;via DirectML (* ROCm is on hold), about 3x slower than Nvidia RTX 3XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;AMD GPU&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;via ROCm, about 1.5x slower than Nvidia RTX 3XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mac&lt;/td&gt; 
   &lt;td&gt;M1/M2 MPS&lt;/td&gt; 
   &lt;td&gt;Shared&lt;/td&gt; 
   &lt;td&gt;Shared&lt;/td&gt; 
   &lt;td&gt;Shared&lt;/td&gt; 
   &lt;td&gt;about 9x slower than Nvidia RTX 3XXX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows/Linux/Mac&lt;/td&gt; 
   &lt;td&gt;only use CPU&lt;/td&gt; 
   &lt;td&gt;0GB&lt;/td&gt; 
   &lt;td&gt;32GB&lt;/td&gt; 
   &lt;td&gt;Required&lt;/td&gt; 
   &lt;td&gt;about 17x slower than Nvidia RTX 3XXX&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* AMD GPU ROCm (on hold): The AMD is still working on supporting ROCm on Windows.&lt;/p&gt; 
&lt;p&gt;* Nvidia GTX 1XXX 6GB uncertain: Some people report 6GB success on GTX 10XX, but some other people report failure cases.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Note that Fooocus is only for extremely high quality image generating. We will not support smaller models to reduce the requirement and sacrifice result quality.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Troubleshoot&lt;/h2&gt; 
&lt;p&gt;See the common problems &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/troubleshoot.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Default Models&lt;/h2&gt; 
&lt;p&gt;&lt;a name="models"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Given different goals, the default models and configs of Fooocus are different:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;Linux args&lt;/th&gt; 
   &lt;th&gt;Main Model&lt;/th&gt; 
   &lt;th&gt;Refiner&lt;/th&gt; 
   &lt;th&gt;Config&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;General&lt;/td&gt; 
   &lt;td&gt;run.bat&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;juggernautXL_v8Rundiffusion&lt;/td&gt; 
   &lt;td&gt;not used&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/raw/main/presets/default.json"&gt;here&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Realistic&lt;/td&gt; 
   &lt;td&gt;run_realistic.bat&lt;/td&gt; 
   &lt;td&gt;--preset realistic&lt;/td&gt; 
   &lt;td&gt;realisticStockPhoto_v20&lt;/td&gt; 
   &lt;td&gt;not used&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/raw/main/presets/realistic.json"&gt;here&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anime&lt;/td&gt; 
   &lt;td&gt;run_anime.bat&lt;/td&gt; 
   &lt;td&gt;--preset anime&lt;/td&gt; 
   &lt;td&gt;animaPencilXL_v500&lt;/td&gt; 
   &lt;td&gt;not used&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/raw/main/presets/anime.json"&gt;here&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Note that the download is &lt;strong&gt;automatic&lt;/strong&gt; - you do not need to do anything if the internet connection is okay. However, you can download them manually if you (or move them from somewhere else) have your own preparation.&lt;/p&gt; 
&lt;h2&gt;UI Access and Authentication&lt;/h2&gt; 
&lt;p&gt;In addition to running on localhost, Fooocus can also expose its UI in two ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Local UI listener: use &lt;code&gt;--listen&lt;/code&gt; (specify port e.g. with &lt;code&gt;--port 8888&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;API access: use &lt;code&gt;--share&lt;/code&gt; (registers an endpoint at &lt;code&gt;.gradio.live&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In both ways the access is unauthenticated by default. You can add basic authentication by creating a file called &lt;code&gt;auth.json&lt;/code&gt; in the main directory, which contains a list of JSON objects with the keys &lt;code&gt;user&lt;/code&gt; and &lt;code&gt;pass&lt;/code&gt; (see example in &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/auth-example.json"&gt;auth-example.json&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;List of "Hidden" Tricks&lt;/h2&gt; 
&lt;p&gt;&lt;a name="tech_list"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see a list of tricks. Those are based on SDXL and are not very up-to-date with latest models.&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;GPT2-based &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/117#raw"&gt;prompt expansion as a dynamic style "Fooocus V2".&lt;/a&gt; (similar to Midjourney's hidden pre-processing and "raw" mode, or the LeonardoAI's Prompt Magic).&lt;/li&gt; 
  &lt;li&gt;Native refiner swap inside one single k-sampler. The advantage is that the refiner model can now reuse the base model's momentum (or ODE's history parameters) collected from k-sampling to achieve more coherent sampling. In Automatic1111's high-res fix and ComfyUI's node system, the base model and refiner use two independent k-samplers, which means the momentum is largely wasted, and the sampling continuity is broken. Fooocus uses its own advanced k-diffusion sampling that ensures seamless, native, and continuous swap in a refiner setup. (Update Aug 13: Actually, I discussed this with Automatic1111 several days ago, and it seems that the “native refiner swap inside one single k-sampler” is &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/12371"&gt;merged&lt;/a&gt; into the dev branch of webui. Great!)&lt;/li&gt; 
  &lt;li&gt;Negative ADM guidance. Because the highest resolution level of XL Base does not have cross attentions, the positive and negative signals for XL's highest resolution level cannot receive enough contrasts during the CFG sampling, causing the results to look a bit plastic or overly smooth in certain cases. Fortunately, since the XL's highest resolution level is still conditioned on image aspect ratios (ADM), we can modify the adm on the positive/negative side to compensate for the lack of CFG contrast in the highest resolution level. (Update Aug 16, the IOS App &lt;a href="https://apps.apple.com/us/app/draw-things-ai-generation/id6444050820"&gt;Draw Things&lt;/a&gt; will support Negative ADM Guidance. Great!)&lt;/li&gt; 
  &lt;li&gt;We implemented a carefully tuned variation of Section 5.1 of &lt;a href="https://arxiv.org/pdf/2210.00939.pdf"&gt;"Improving Sample Quality of Diffusion Models Using Self-Attention Guidance"&lt;/a&gt;. The weight is set to very low, but this is Fooocus's final guarantee to make sure that the XL will never yield an overly smooth or plastic appearance (examples &lt;a href="https://github.com/lllyasviel/Fooocus/discussions/117#sharpness"&gt;here&lt;/a&gt;). This can almost eliminate all cases for which XL still occasionally produces overly smooth results, even with negative ADM guidance. (Update 2023 Aug 18, the Gaussian kernel of SAG is changed to an anisotropic kernel for better structure preservation and fewer artifacts.)&lt;/li&gt; 
  &lt;li&gt;We modified the style templates a bit and added the "cinematic-default".&lt;/li&gt; 
  &lt;li&gt;We tested the "sd_xl_offset_example-lora_1.0.safetensors" and it seems that when the lora weight is below 0.5, the results are always better than XL without lora.&lt;/li&gt; 
  &lt;li&gt;The parameters of samplers are carefully tuned.&lt;/li&gt; 
  &lt;li&gt;Because XL uses positional encoding for generation resolution, images generated by several fixed resolutions look a bit better than those from arbitrary resolutions (because the positional encoding is not very good at handling int numbers that are unseen during training). This suggests that the resolutions in UI may be hard coded for best results.&lt;/li&gt; 
  &lt;li&gt;Separated prompts for two different text encoders seem unnecessary. Separated prompts for the base model and refiner may work, but the effects are random, and we refrain from implementing this.&lt;/li&gt; 
  &lt;li&gt;The DPM family seems well-suited for XL since XL sometimes generates overly smooth texture, but the DPM family sometimes generates overly dense detail in texture. Their joint effect looks neutral and appealing to human perception.&lt;/li&gt; 
  &lt;li&gt;A carefully designed system for balancing multiple styles as well as prompt expansion.&lt;/li&gt; 
  &lt;li&gt;Using automatic1111's method to normalize prompt emphasizing. This significantly improves results when users directly copy prompts from civitai.&lt;/li&gt; 
  &lt;li&gt;The joint swap system of the refiner now also supports img2img and upscale in a seamless way.&lt;/li&gt; 
  &lt;li&gt;CFG Scale and TSNR correction (tuned for SDXL) when CFG is bigger than 10.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;Customization&lt;/h2&gt; 
&lt;p&gt;After the first time you run Fooocus, a config file will be generated at &lt;code&gt;Fooocus\config.txt&lt;/code&gt;. This file can be edited to change the model path or default parameters.&lt;/p&gt; 
&lt;p&gt;For example, an edited &lt;code&gt;Fooocus\config.txt&lt;/code&gt; (this file will be generated after the first launch) may look like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "path_checkpoints": "D:\\Fooocus\\models\\checkpoints",
    "path_loras": "D:\\Fooocus\\models\\loras",
    "path_embeddings": "D:\\Fooocus\\models\\embeddings",
    "path_vae_approx": "D:\\Fooocus\\models\\vae_approx",
    "path_upscale_models": "D:\\Fooocus\\models\\upscale_models",
    "path_inpaint": "D:\\Fooocus\\models\\inpaint",
    "path_controlnet": "D:\\Fooocus\\models\\controlnet",
    "path_clip_vision": "D:\\Fooocus\\models\\clip_vision",
    "path_fooocus_expansion": "D:\\Fooocus\\models\\prompt_expansion\\fooocus_expansion",
    "path_outputs": "D:\\Fooocus\\outputs",
    "default_model": "realisticStockPhoto_v10.safetensors",
    "default_refiner": "",
    "default_loras": [["lora_filename_1.safetensors", 0.5], ["lora_filename_2.safetensors", 0.5]],
    "default_cfg_scale": 3.0,
    "default_sampler": "dpmpp_2m",
    "default_scheduler": "karras",
    "default_negative_prompt": "low quality",
    "default_positive_prompt": "",
    "default_styles": [
        "Fooocus V2",
        "Fooocus Photograph",
        "Fooocus Negative"
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Many other keys, formats, and examples are in &lt;code&gt;Fooocus\config_modification_tutorial.txt&lt;/code&gt; (this file will be generated after the first launch).&lt;/p&gt; 
&lt;p&gt;Consider twice before you really change the config. If you find yourself breaking things, just delete &lt;code&gt;Fooocus\config.txt&lt;/code&gt;. Fooocus will go back to default.&lt;/p&gt; 
&lt;p&gt;A safer way is just to try "run_anime.bat" or "run_realistic.bat" - they should already be good enough for different tasks.&lt;/p&gt; 
&lt;p&gt;&lt;del&gt;Note that &lt;code&gt;user_path_config.txt&lt;/code&gt; is deprecated and will be removed soon.&lt;/del&gt; (Edit: it is already removed.)&lt;/p&gt; 
&lt;h3&gt;All CMD Flags&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;entry_with_update.py  [-h] [--listen [IP]] [--port PORT]
                      [--disable-header-check [ORIGIN]]
                      [--web-upload-size WEB_UPLOAD_SIZE]
                      [--hf-mirror HF_MIRROR]
                      [--external-working-path PATH [PATH ...]]
                      [--output-path OUTPUT_PATH]
                      [--temp-path TEMP_PATH] [--cache-path CACHE_PATH]
                      [--in-browser] [--disable-in-browser]
                      [--gpu-device-id DEVICE_ID]
                      [--async-cuda-allocation | --disable-async-cuda-allocation]
                      [--disable-attention-upcast]
                      [--all-in-fp32 | --all-in-fp16]
                      [--unet-in-bf16 | --unet-in-fp16 | --unet-in-fp8-e4m3fn | --unet-in-fp8-e5m2]
                      [--vae-in-fp16 | --vae-in-fp32 | --vae-in-bf16]
                      [--vae-in-cpu]
                      [--clip-in-fp8-e4m3fn | --clip-in-fp8-e5m2 | --clip-in-fp16 | --clip-in-fp32]
                      [--directml [DIRECTML_DEVICE]]
                      [--disable-ipex-hijack]
                      [--preview-option [none,auto,fast,taesd]]
                      [--attention-split | --attention-quad | --attention-pytorch]
                      [--disable-xformers]
                      [--always-gpu | --always-high-vram | --always-normal-vram | --always-low-vram | --always-no-vram | --always-cpu [CPU_NUM_THREADS]]
                      [--always-offload-from-vram]
                      [--pytorch-deterministic] [--disable-server-log]
                      [--debug-mode] [--is-windows-embedded-python]
                      [--disable-server-info] [--multi-user] [--share]
                      [--preset PRESET] [--disable-preset-selection]
                      [--language LANGUAGE]
                      [--disable-offload-from-vram] [--theme THEME]
                      [--disable-image-log] [--disable-analytics]
                      [--disable-metadata] [--disable-preset-download]
                      [--disable-enhance-output-sorting]
                      [--enable-auto-describe-image]
                      [--always-download-new-model]
                      [--rebuild-hash-cache [CPU_NUM_THREADS]]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Inline Prompt Features&lt;/h2&gt; 
&lt;h3&gt;Wildcards&lt;/h3&gt; 
&lt;p&gt;Example prompt: &lt;code&gt;__color__ flower&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Processed for positive and negative prompt.&lt;/p&gt; 
&lt;p&gt;Selects a random wildcard from a predefined list of options, in this case the &lt;code&gt;wildcards/color.txt&lt;/code&gt; file. The wildcard will be replaced with a random color (randomness based on seed). You can also disable randomness and process a wildcard file from top to bottom by enabling the checkbox &lt;code&gt;Read wildcards in order&lt;/code&gt; in Developer Debug Mode.&lt;/p&gt; 
&lt;p&gt;Wildcards can be nested and combined, and multiple wildcards can be used in the same prompt (example see &lt;code&gt;wildcards/color_flower.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;h3&gt;Array Processing&lt;/h3&gt; 
&lt;p&gt;Example prompt: &lt;code&gt;[[red, green, blue]] flower&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Processed only for positive prompt.&lt;/p&gt; 
&lt;p&gt;Processes the array from left to right, generating a separate image for each element in the array. In this case 3 images would be generated, one for each color. Increase the image number to 3 to generate all 3 variants.&lt;/p&gt; 
&lt;p&gt;Arrays can not be nested, but multiple arrays can be used in the same prompt. Does support inline LoRAs as array elements!&lt;/p&gt; 
&lt;h3&gt;Inline LoRAs&lt;/h3&gt; 
&lt;p&gt;Example prompt: &lt;code&gt;flower &amp;lt;lora:sunflowers:1.2&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Processed only for positive prompt.&lt;/p&gt; 
&lt;p&gt;Applies a LoRA to the prompt. The LoRA file must be located in the &lt;code&gt;models/loras&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Advanced Features&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/lllyasviel/Fooocus/discussions/117"&gt;Click here to browse the advanced features.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Forks&lt;/h2&gt; 
&lt;p&gt;Below are some Forks to Fooocus:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Fooocus' forks&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fenneishi/Fooocus-Control"&gt;fenneishi/Fooocus-Control&lt;/a&gt; &lt;br /&gt;&lt;a href="https://github.com/runew0lf/RuinedFooocus"&gt;runew0lf/RuinedFooocus&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/MoonRide303/Fooocus-MRE"&gt;MoonRide303/Fooocus-MRE&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/mashb1t/Fooocus"&gt;mashb1t/Fooocus&lt;/a&gt; &lt;br /&gt; and so on ...&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;p&gt;Many thanks to &lt;a href="https://github.com/twri"&gt;twri&lt;/a&gt; and &lt;a href="https://github.com/3Diva"&gt;3Diva&lt;/a&gt; and &lt;a href="https://github.com/K3nt3L"&gt;Marc K3nt3L&lt;/a&gt; for creating additional SDXL styles available in Fooocus.&lt;/p&gt; 
&lt;p&gt;The project starts from a mixture of &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui"&gt;Stable Diffusion WebUI&lt;/a&gt; and &lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI&lt;/a&gt; codebases.&lt;/p&gt; 
&lt;p&gt;Also, thanks &lt;a href="https://github.com/daswer123"&gt;daswer123&lt;/a&gt; for contributing the Canvas Zoom!&lt;/p&gt; 
&lt;h2&gt;Update Log&lt;/h2&gt; 
&lt;p&gt;The log is &lt;a href="https://raw.githubusercontent.com/lllyasviel/Fooocus/main/update_log.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Localization/Translation/I18N&lt;/h2&gt; 
&lt;p&gt;You can put json files in the &lt;code&gt;language&lt;/code&gt; folder to translate the user interface.&lt;/p&gt; 
&lt;p&gt;For example, below is the content of &lt;code&gt;Fooocus/language/example.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "Generate": "生成",
  "Input Image": "入力画像",
  "Advanced": "고급",
  "SAI 3D Model": "SAI 3D Modèle"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you add &lt;code&gt;--language example&lt;/code&gt; arg, Fooocus will read &lt;code&gt;Fooocus/language/example.json&lt;/code&gt; to translate the UI.&lt;/p&gt; 
&lt;p&gt;For example, you can edit the ending line of Windows &lt;code&gt;run.bat&lt;/code&gt; as&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.\python_embeded\python.exe -s Fooocus\entry_with_update.py --language example
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or &lt;code&gt;run_anime.bat&lt;/code&gt; as&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.\python_embeded\python.exe -s Fooocus\entry_with_update.py --language example --preset anime
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or &lt;code&gt;run_realistic.bat&lt;/code&gt; as&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.\python_embeded\python.exe -s Fooocus\entry_with_update.py --language example --preset realistic
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For practical translation, you may create your own file like &lt;code&gt;Fooocus/language/jp.json&lt;/code&gt; or &lt;code&gt;Fooocus/language/cn.json&lt;/code&gt; and then use flag &lt;code&gt;--language jp&lt;/code&gt; or &lt;code&gt;--language cn&lt;/code&gt;. Apparently, these files do not exist now. &lt;strong&gt;We need your help to create these files!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note that if no &lt;code&gt;--language&lt;/code&gt; is given and at the same time &lt;code&gt;Fooocus/language/default.json&lt;/code&gt; exists, Fooocus will always load &lt;code&gt;Fooocus/language/default.json&lt;/code&gt; for translation. By default, the file &lt;code&gt;Fooocus/language/default.json&lt;/code&gt; does not exist.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>laramies/theHarvester</title>
      <link>https://github.com/laramies/theHarvester</link>
      <description>&lt;p&gt;E-mails, subdomains and names Harvester - OSINT&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/laramies/theHarvester/raw/master/theHarvester-logo.webp" alt="theHarvester" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/laramies/theHarvester/workflows/TheHarvester%20Python%20CI/badge.svg?sanitize=true" alt="TheHarvester CI" /&gt; &lt;img src="https://github.com/laramies/theHarvester/workflows/TheHarvester%20Docker%20Image%20CI/badge.svg?sanitize=true" alt="TheHarvester Docker Image CI" /&gt; &lt;a href="https://inventory.raw.pm/"&gt;&lt;img src="https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_flat_without_logo.svg?sanitize=true" alt="Rawsec's CyberSecurity Inventory" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;theHarvester is a simple to use, yet powerful tool designed to be used during the reconnaissance stage of a red team assessment or penetration test. It performs open source intelligence (OSINT) gathering to help determine a domain's external threat landscape. The tool gathers names, emails, IPs, subdomains, and URLs by using multiple public resources that include:&lt;/p&gt; 
&lt;h2&gt;Install and dependencies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.12 or higher.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/laramies/theHarvester/wiki/Installation"&gt;https://github.com/laramies/theHarvester/wiki/Installation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Install uv:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/laramies/theHarvester
cd theHarvester
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies and create a virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run theHarvester:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run theHarvester
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To install development dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --extra dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run linting and formatting:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run ruff check
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run ruff format
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Passive modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;baidu: Baidu search engine (&lt;a href="https://www.baidu.com"&gt;https://www.baidu.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bevigil: CloudSEK BeVigil scans mobile application for OSINT assets (&lt;a href="https://bevigil.com/osint-api"&gt;https://bevigil.com/osint-api&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;brave: Brave search engine - now uses official Brave Search API (&lt;a href="https://api-dashboard.search.brave.com"&gt;https://api-dashboard.search.brave.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bufferoverun: Fast domain name lookups for TLS certificates in IPv4 space (&lt;a href="https://tls.bufferover.run"&gt;https://tls.bufferover.run&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;builtwith: Find out what websites are built with (&lt;a href="https://builtwith.com"&gt;https://builtwith.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;censys: Uses certificates searches to enumerate subdomains and gather emails (&lt;a href="https://censys.io"&gt;https://censys.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;certspotter: Cert Spotter monitors Certificate Transparency logs (&lt;a href="https://sslmate.com/certspotter"&gt;https://sslmate.com/certspotter&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;criminalip: Specialized Cyber Threat Intelligence (CTI) search engine (&lt;a href="https://www.criminalip.io"&gt;https://www.criminalip.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;crtsh: Comodo Certificate search (&lt;a href="https://crt.sh"&gt;https://crt.sh&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dehashed: Take your data security to the next level is (&lt;a href="https://dehashed.com"&gt;https://dehashed.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dnsdumpster: Domain research tool that can discover hosts related to a domain (&lt;a href="https://dnsdumpster.com"&gt;https://dnsdumpster.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;duckduckgo: DuckDuckGo search engine (&lt;a href="https://duckduckgo.com"&gt;https://duckduckgo.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;fullhunt: Next-generation attack surface security platform (&lt;a href="https://fullhunt.io"&gt;https://fullhunt.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;github-code: GitHub code search engine (&lt;a href="https://www.github.com"&gt;https://www.github.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hackertarget: Online vulnerability scanners and network intelligence to help organizations (&lt;a href="https://hackertarget.com"&gt;https://hackertarget.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;haveibeenpwned: Check if your email address is in a data breach (&lt;a href="https://haveibeenpwned.com"&gt;https://haveibeenpwned.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hunter: Hunter search engine (&lt;a href="https://hunter.io"&gt;https://hunter.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hunterhow: Internet search engines for security researchers (&lt;a href="https://hunter.how"&gt;https://hunter.how&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;intelx: Intelx search engine (&lt;a href="https://intelx.io"&gt;https://intelx.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;leaklookup: Data breach search engine (&lt;a href="https://leak-lookup.com"&gt;https://leak-lookup.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;netlas: A Shodan or Censys competitor (&lt;a href="https://app.netlas.io"&gt;https://app.netlas.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;onyphe: Cyber defense search engine (&lt;a href="https://www.onyphe.io"&gt;https://www.onyphe.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;otx: AlienVault open threat exchange (&lt;a href="https://otx.alienvault.com"&gt;https://otx.alienvault.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;pentesttools: Cloud-based toolkit for offensive security testing, focused on web applications and network penetration testing (&lt;a href="https://pentest-tools.com"&gt;https://pentest-tools.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;projecdiscovery: Actively collects and maintains internet-wide assets data, to enhance research and analyse changes around DNS for better insights (&lt;a href="https://chaos.projectdiscovery.io"&gt;https://chaos.projectdiscovery.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rapiddns: DNS query tool which make querying subdomains or sites of a same IP easy (&lt;a href="https://rapiddns.io"&gt;https://rapiddns.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rocketreach: Access real-time verified personal/professional emails, phone numbers, and social media links (&lt;a href="https://rocketreach.co"&gt;https://rocketreach.co&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;securityscorecard: helps TPRM and SOC teams detect, prioritize, and remediate vendor risk across their entire supplier ecosystem at scale (&lt;a href="https://securityscorecard.com"&gt;https://securityscorecard.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;securityTrails: Security Trails search engine, the world's largest repository of historical DNS data (&lt;a href="https://securitytrails.com"&gt;https://securitytrails.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;-s, --shodan: Shodan search engine will search for ports and banners from discovered hosts (&lt;a href="https://shodan.io"&gt;https://shodan.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;subdomaincenter: A subdomain finder tool used to find subdomains of a given domain (&lt;a href="https://www.subdomain.center"&gt;https://www.subdomain.center&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;subdomainfinderc99: A subdomain finder is a tool used to find the subdomains of a given domain (&lt;a href="https://subdomainfinder.c99.nl"&gt;https://subdomainfinder.c99.nl&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;threatminer: Data mining for threat intelligence (&lt;a href="https://www.threatminer.org"&gt;https://www.threatminer.org&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;tomba: Tomba search engine (&lt;a href="https://tomba.io"&gt;https://tomba.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;urlscan: A sandbox for the web that is a URL and website scanner (&lt;a href="https://urlscan.io"&gt;https://urlscan.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;venacus: Venacus search engine (&lt;a href="https://venacus.com"&gt;https://venacus.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;virustotal: Domain search (&lt;a href="https://www.virustotal.com"&gt;https://www.virustotal.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;whoisxml: Subdomain search (&lt;a href="https://subdomains.whoisxmlapi.com/api/pricing"&gt;https://subdomains.whoisxmlapi.com/api/pricing&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;yahoo: Yahoo search engine (&lt;a href="https://www.yahoo.com"&gt;https://www.yahoo.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;zoomeye: China's version of Shodan (&lt;a href="https://www.zoomeye.org"&gt;https://www.zoomeye.org&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Active modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;DNS brute force: dictionary brute force enumeration&lt;/li&gt; 
 &lt;li&gt;Screenshots: Take screenshots of subdomains that were found&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Modules that require an API key&lt;/h2&gt; 
&lt;p&gt;Documentation to setup API keys can be found at - &lt;a href="https://github.com/laramies/theHarvester/wiki/Installation#api-keys"&gt;https://github.com/laramies/theHarvester/wiki/Installation#api-keys&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;bevigil - 50 free queries/month, 1k queries/month $50&lt;/li&gt; 
 &lt;li&gt;brave - Free plan available, Pro plans for higher limits&lt;/li&gt; 
 &lt;li&gt;bufferoverun - 100 free queries/month, 10k/month $25&lt;/li&gt; 
 &lt;li&gt;builtwith - 50 free queries ever, $2950/yr&lt;/li&gt; 
 &lt;li&gt;censys - 500 credits $100&lt;/li&gt; 
 &lt;li&gt;criminalip - 100 free queries/month, 700k/month $59&lt;/li&gt; 
 &lt;li&gt;dehashed - 500 credts $15, 5k credits $150&lt;/li&gt; 
 &lt;li&gt;dnsdumpster - 50 free querries/day, $49&lt;/li&gt; 
 &lt;li&gt;fullhunt - 50 free queries, 200 queries $29/month, 500 queries $59/month&lt;/li&gt; 
 &lt;li&gt;github-code&lt;/li&gt; 
 &lt;li&gt;haveibeenpwned - 10 email searches/min $4.50, 50 email searches/min $22&lt;/li&gt; 
 &lt;li&gt;hunter - 50 credits/month free, 12k credits/yr $34&lt;/li&gt; 
 &lt;li&gt;hunterhow - 10k free API results per 30 days, 50k API results per 30 days $10&lt;/li&gt; 
 &lt;li&gt;intelx&lt;/li&gt; 
 &lt;li&gt;leaklookup - 20 credits $10, 50 credits $20, 140 credits $50, 300 credits $100&lt;/li&gt; 
 &lt;li&gt;netlas - 50 free requests/day, 1k requests $49, 10k requests $249&lt;/li&gt; 
 &lt;li&gt;onyphe - 10M results/month $587&lt;/li&gt; 
 &lt;li&gt;pentesttools - 5 assets netsec $95/month, 5 assets webnetsec $140/month&lt;/li&gt; 
 &lt;li&gt;projecdiscovery - requires work email. Free monthly discovery and vulnerability scans on sign-up email domain, enterprise $&lt;/li&gt; 
 &lt;li&gt;rocketreach - 100 email lookups/month $48, 250 email lookups/month $108&lt;/li&gt; 
 &lt;li&gt;securityscorecard&lt;/li&gt; 
 &lt;li&gt;securityTrails - 50 free queries/month, 20k queries/month $500&lt;/li&gt; 
 &lt;li&gt;shodan - Freelancer $69 month, Small Business $359 month&lt;/li&gt; 
 &lt;li&gt;tomba - 25 searches/month free, 1k searches/month $39, 5k searches/month $89&lt;/li&gt; 
 &lt;li&gt;venacus - 1 search/day free, 10 searches/day $12, 30 searches/day $36&lt;/li&gt; 
 &lt;li&gt;whoisxml - 2k queries $50, 5k queries $105&lt;/li&gt; 
 &lt;li&gt;zoomeye - 5 results/day free, 30/results/day $190/yr&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Comments, bugs, and requests&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/laramies"&gt;&lt;img src="https://img.shields.io/twitter/follow/laramies.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Christian Martorella @laramies &lt;a href="mailto:cmartorella@edge-security.com"&gt;cmartorella@edge-security.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/NotoriousRebel1"&gt;&lt;img src="https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Matthew Brown @NotoriousRebel1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/jay_townsend1"&gt;&lt;img src="https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Jay "L1ghtn1ng" Townsend @jay_townsend1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Main contributors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/NotoriousRebel1"&gt;&lt;img src="https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Matthew Brown @NotoriousRebel1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/jay_townsend1"&gt;&lt;img src="https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Jay "L1ghtn1ng" Townsend @jay_townsend1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/discoverscripts"&gt;&lt;img src="https://img.shields.io/twitter/follow/discoverscripts.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Lee Baird @discoverscripts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Matherly - Shodan project&lt;/li&gt; 
 &lt;li&gt;Ahmed Aboul Ela - subdomain names dictionaries (big and small)&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>humanlayer/humanlayer</title>
      <link>https://github.com/humanlayer/humanlayer</link>
      <description>&lt;p&gt;HumanLayer enables AI agents to communicate with humans in tool-based and async workflows. Guarantee human oversight of high-stakes function calls with approval workflows across slack, email and more. Bring your LLM and Framework of choice and start giving your AI agents safe access to the world. Agentic Workflows, human in the loop, tool calling&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/humanlayer/humanlayer/main/docs/images/wordmark-light.svg?sanitize=true" alt="Wordmark Logo of HumanLayer" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;🚧 &lt;strong&gt;HumanLayer&lt;/strong&gt; is undergoing some changes...stay tuned! 🚧&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://humanlayer.dev/code"&gt;HumanLayer Code&lt;/a&gt; | &lt;a href="https://humanlayer.dev/discord"&gt;Discord&lt;/a&gt; | &lt;a href="https://github.com/humanlayer/humanlayer/releases"&gt;Release&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/humanlayer/humanlayer"&gt;&lt;img src="https://img.shields.io/github/stars/humanlayer/humanlayer" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2"&gt;&lt;img src="https://img.shields.io/badge/License-Apache-green.svg?sanitize=true" alt="License: Apache-2" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;img referrerpolicy="no-referrer-when-downgrade" src="https://static.scarf.sh/a.png?x-pxid=fcfc0926-d841-47fb-b8a6-6aba3a6c3228" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#why-humanlayer"&gt;Why HumanLayer?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why HumanLayer?&lt;/h2&gt; 
&lt;p&gt;Functions and tools are a key part of &lt;a href="https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance"&gt;Agentic Workflows&lt;/a&gt;. They enable LLMs to interact meaningfully with the outside world and automate broad scopes of impactful work. Correct and accurate function calling is essential for AI agents that do meaningful things like book appointments, interact with customers, manage billing information, write+execute code, and more.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://louis-dupont.medium.com/transforming-software-interactions-with-tool-calling-and-llms-dc39185247e9"&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8rEqjGZs_e6dibWeaqaQg.png" alt="Tool Calling Loop from Louis Dupont" /&gt;&lt;/a&gt; &lt;em&gt;From &lt;a href="https://louis-dupont.medium.com/transforming-software-interactions-with-tool-calling-and-llms-dc39185247e9"&gt;https://louis-dupont.medium.com/transforming-software-interactions-with-tool-calling-and-llms-dc39185247e9&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;However&lt;/strong&gt;, the most useful functions we can give to an LLM are also the most risky. We can all imagine the value of an AI Database Administrator that constantly tunes and refactors our SQL database, but most teams wouldn't give an LLM access to run arbitrary SQL statements against a production database (heck, we mostly don't even let humans do that). That is:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;
  &lt;blockquote&gt;
   Even with state-of-the-art agentic reasoning and prompt routing, LLMs are not sufficiently reliable to be given access to high-stakes functions without human oversight
  &lt;/blockquote&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;To better define what is meant by "high stakes", some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Low Stakes&lt;/strong&gt;: Read Access to public data (e.g. search wikipedia, access public APIs and DataSets)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Low Stakes&lt;/strong&gt;: Communicate with agent author (e.g. an engineer might empower an agent to send them a private Slack message with updates on progress)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Medium Stakes&lt;/strong&gt;: Read Access to Private Data (e.g. read emails, access calendars, query a CRM)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Medium Stakes&lt;/strong&gt;: Communicate with strict rules (e.g. sending based on a specific sequence of hard-coded email templates)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Stakes&lt;/strong&gt;: Communicate on my Behalf or on behalf of my Company (e.g. send emails, post to slack, publish social/blog content)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Stakes&lt;/strong&gt;: Write Access to Private Data (e.g. update CRM records, modify feature toggles, update billing information)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt;
 &lt;img style="width: 600px" alt="Image showing the levels of function stakes stacked on top of one another" src="https://raw.githubusercontent.com/humanlayer/humanlayer/main/docs/images/function_stakes.png" /&gt;
&lt;/div&gt; 
&lt;p&gt;The high stakes functions are the ones that are the most valuable and promise the most impact in automating away human workflows. But they are also the ones where "90% accuracy" is not acceptable. Reliability is further impacted by today's LLMs' tendency to hallucinate or craft low-quality text that is clearly AI generated. The sooner teams can get Agents reliably and safely calling these tools with high-quality inputs, the sooner they can reap massive benefits.&lt;/p&gt; 
&lt;p&gt;HumanLayer provides a set of tools to &lt;em&gt;deterministically&lt;/em&gt; guarantee human oversight of high stakes function calls. Even if the LLM makes a mistake or hallucinates, HumanLayer is baked into the tool/function itself, guaranteeing a human in the loop.&lt;/p&gt; 
&lt;div align="center"&gt;
 &lt;img style="width: 400px" alt="HumanLayer @require_approval decorator wrapping the Commnicate on my behalf function" src="https://raw.githubusercontent.com/humanlayer/humanlayer/main/docs/images/humanlayer_require_approval.png" /&gt;
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;
  &lt;blockquote&gt;
    HumanLayer provides a set of tools to *deterministically* guarantee human oversight of high stakes function calls 
  &lt;/blockquote&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;h3&gt;The Future: Autonomous Agents and the "Outer Loop"&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Read More: &lt;a href="https://theouterloop.substack.com/p/openais-realtime-api-is-a-step-towards"&gt;OpenAI's RealTime API is a step towards outer-loop agents&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Between &lt;code&gt;require_approval&lt;/code&gt; and &lt;code&gt;human_as_tool&lt;/code&gt;, HumanLayer is built to empower the next generation of AI agents - Autonomous Agents, but it's just a piece of the puzzle. To clarify "next generation", we can summarize briefly the history of LLM applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gen 1&lt;/strong&gt;: Chat - human-initiated question / response interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gen 2&lt;/strong&gt;: Agentic Assistants - frameworks drive prompt routing, tool calling, chain of thought, and context window management to get much more reliability and functionality. Most workflows are initiated by humans in single-shot "here's a task, go do it" or rolling chat interfaces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gen 3&lt;/strong&gt;: Autonomous Agents - no longer human initiated, agents will live in the "outer loop" driving toward their goals using various tools and functions. Human/Agent communication is Agent-initiated rather than human-initiated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/humanlayer/humanlayer/main/docs/images/gen-2-gen-3-agents.png" alt="gen2 vs gen 3 agents" /&gt;&lt;/p&gt; 
&lt;p&gt;Gen 3 autonomous agents will need ways to consult humans for input on various tasks. In order for these agents to perform actual useful work, they'll need human oversight for sensitive operations.&lt;/p&gt; 
&lt;p&gt;These agents will require ways to contact one or more humans across various channels including chat, email, sms, and more.&lt;/p&gt; 
&lt;p&gt;While early versions of these agents may technically be "human initiated" in that they get kicked off on a regular schedule by e.g. a cron or similar, the best ones will be managing their own scheduling and costs. This will require toolkits for inspecting costs and something akin to &lt;code&gt;sleep_until&lt;/code&gt;. They'll need to run in orchestration frameworks that can durably serialize and resume agent workflows across tool calls that might not return for hours or days. These frameworks will need to support context window management by a "manager LLM" and enable agents to fork sub-chains to handle specialized tasks and roles.&lt;/p&gt; 
&lt;p&gt;Example use cases for these outer loop agents include &lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/examples/langchain/04-human_as_tool_linkedin.py"&gt;the linkedin inbox assistant&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/examples/langchain/05-approvals_and_humans_composite.py"&gt;the customer onboarding assistant&lt;/a&gt;, but that's really just scratching the surface.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The HumanLayer SDK and docs are open-source and we welcome contributions in the form of issues, documentation, pull requests, and more. See &lt;a href="https://raw.githubusercontent.com/humanlayer/humanlayer/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fun Stuff&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#humanlayer/humanlayer&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=humanlayer/humanlayer&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development Conventions&lt;/h2&gt; 
&lt;h3&gt;TODO Annotations&lt;/h3&gt; 
&lt;p&gt;We use a priority-based TODO annotation system throughout the codebase:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;TODO(0)&lt;/code&gt;: Critical - never merge&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TODO(1)&lt;/code&gt;: High - architectural flaws, major bugs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TODO(2)&lt;/code&gt;: Medium - minor bugs, missing features&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TODO(3)&lt;/code&gt;: Low - polish, tests, documentation&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TODO(4)&lt;/code&gt;: Questions/investigations needed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PERF&lt;/code&gt;: Performance optimization opportunities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The HumanLayer SDK and CodeLayer sources in this repo are licensed under the Apache 2 License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ashishpatel26/500-AI-Agents-Projects</title>
      <link>https://github.com/ashishpatel26/500-AI-Agents-Projects</link>
      <description>&lt;p&gt;The 500 AI Agents Projects is a curated collection of AI agent use cases across various industries. It showcases practical applications and provides links to open-source projects for implementation, illustrating how AI agents are transforming sectors such as healthcare, finance, education, retail, and more.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🌟 500+ AI Agent Projects / UseCases&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/ashishpatel26/500-AI-Agents-Projects"&gt;&lt;img src="https://img.shields.io/badge/500--AI--Agents--Projects-UseCase-2ea44f?logo=https%3A%2F%2Fstatic-00.iconduck.com%2Fassets.00%2Frobot-emoji-2048x2044-kay057lt.png&amp;amp;logoColor=2ea44f" alt="500-AI-Agents-Projects - UseCase" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/images/AIAgentUseCase.jpg" alt="img" /&gt;&lt;/p&gt; 
&lt;p&gt;A curated collection of AI agent use cases across industries, showcasing practical applications and linking to open-source projects for implementation. Explore how AI agents are transforming industries like healthcare, finance, education, and more! 🤖✨&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📋 Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#-industry-usecase-mindmap"&gt;Industry Usecase&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#use-case-table"&gt;Use Case Table&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#framework-wise-usecases"&gt;Framework Wise UseCase&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#framework-name-crewai"&gt;CrewAI UseCase&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#framework-name-autogen"&gt;AutoGen UseCase&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#framework-name-agno"&gt;Agno UseCase&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#framework-name-langgraph"&gt;Langgraph UseCase&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🧠 Introduction&lt;/h2&gt; 
&lt;p&gt;Artificial Intelligence (AI) agents are revolutionizing the way industries operate. From personalized learning to financial trading bots, AI agents bring efficiency, innovation, and scalability. This repository provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A categorized list of industries where AI agents are making an impact.&lt;/li&gt; 
 &lt;li&gt;Detailed use cases with links to open-source projects for implementation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Whether you're a developer, researcher, or business enthusiast, this repository is your go-to resource for AI agent inspiration and learning.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🏭 Industry UseCase MindMap&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/images/industry_usecase1.png" alt="" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🧩 Use Case Table&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Industry&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Code Github&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;HIA (Health Insights Agent)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Healthcare&lt;/td&gt; 
   &lt;td&gt;analyses medical reports and provide health insights.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/harshhh28/hia.git"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Health Assistant&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Healthcare&lt;/td&gt; 
   &lt;td&gt;Diagnoses and monitors diseases using patient data.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ahmadvh/AI-Agents-for-Medical-Diagnostics.git"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Automated Trading Bot&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Finance&lt;/td&gt; 
   &lt;td&gt;Automates stock trading with real-time market analysis.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MingyuJ666/Stockagent.git"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Virtual AI Tutor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Education&lt;/td&gt; 
   &lt;td&gt;Provides personalized education tailored to users.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hqanhh/EduGPT.git"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;24/7 AI Chatbot&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Customer Service&lt;/td&gt; 
   &lt;td&gt;Handles customer queries around the clock.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/customer_support_agent_langgraph.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Product Recommendation Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Retail&lt;/td&gt; 
   &lt;td&gt;Suggests products based on user preferences and history.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/RecAI"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Self-Driving Delivery Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Transportation&lt;/td&gt; 
   &lt;td&gt;Optimizes routes and autonomously delivers packages.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sled-group/driVLMe"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Factory Process Monitoring Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Manufacturing&lt;/td&gt; 
   &lt;td&gt;Monitors production lines and ensures quality control.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/yuchenxia/llm4ias"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Property Pricing Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Real Estate&lt;/td&gt; 
   &lt;td&gt;Analyzes market trends to determine property prices.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/AleksNeStu/ai-real-estate-assistant"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Smart Farming Assistant&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Agriculture&lt;/td&gt; 
   &lt;td&gt;Provides insights on crop health and yield predictions.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mohammed97ashraf/LLM_Agri_Bot"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Energy Demand Forecasting Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Energy&lt;/td&gt; 
   &lt;td&gt;Predicts energy usage to optimize grid management.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/yecchen/MIRAI"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Personalization Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Entertainment&lt;/td&gt; 
   &lt;td&gt;Recommends personalized media based on preferences.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crosleythomas/MirrorGPT"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Legal Document Review Assistant&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Legal&lt;/td&gt; 
   &lt;td&gt;Automates document review and highlights key clauses.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/firica/legalai"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Recruitment Recommendation Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Human Resources&lt;/td&gt; 
   &lt;td&gt;Suggests best-fit candidates for job openings.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sentient-engineering/jobber"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Virtual Travel Assistant&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Hospitality&lt;/td&gt; 
   &lt;td&gt;Plans travel itineraries based on preferences.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/nirbar1985/ai-travel-agent"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Game Companion Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Gaming&lt;/td&gt; 
   &lt;td&gt;Enhances player experience with real-time assistance.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/onjas-buidl/LLM-agent-game"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Real-Time Threat Detection Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Cybersecurity&lt;/td&gt; 
   &lt;td&gt;Identifies potential threats and mitigates attacks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVISOsecurity/cyber-security-llm-agents"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;E-commerce Personal Shopper Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;E-commerce&lt;/td&gt; 
   &lt;td&gt;Helps customers find products they’ll love.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Hoanganhvu123/ShoppingGPT"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Logistics Optimization Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Supply Chain&lt;/td&gt; 
   &lt;td&gt;Plans efficient delivery routes and manages inventory.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/OptiGuide"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Vibe Hacking Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Cybersecurity&lt;/td&gt; 
   &lt;td&gt;Autonomous Multi-Agent Based Red Team Testing Service.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/PurpleAILAB/Decepticon"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MediSuite-Ai-Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Health insurance&lt;/td&gt; 
   &lt;td&gt;A medical ai agent that helps automating the process of hospitals / insurance claiming workflow.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MahmoudRabea13/MediSuite-Ai-Agent"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Lina-Egyptian-Medical-Chatbot&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Health insurance&lt;/td&gt; 
   &lt;td&gt;A medical ai agent that helps automating the process of hospitals / insurance claiming workflow.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MahmoudRabea13/MediSuite-Ai-Agent"&gt;&lt;img src="https://img.shields.io/badge/Code-GitHub-black?logo=github" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Framework wise Usecases&lt;/h2&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;Framework Name&lt;/strong&gt;: &lt;strong&gt;CrewAI&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Industry&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;GitHub&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📧 Email Auto Responder Flow&lt;/td&gt; 
   &lt;td&gt;🗣️ Communication&lt;/td&gt; 
   &lt;td&gt;Automates email responses based on predefined criteria to enhance communication efficiency.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/email_auto_responder_flow"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📝 Meeting Assistant Flow&lt;/td&gt; 
   &lt;td&gt;🛠️ Productivity&lt;/td&gt; 
   &lt;td&gt;Assists in organizing and managing meetings, including scheduling and agenda preparation.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/meeting_assistant_flow"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔄 Self Evaluation Loop Flow&lt;/td&gt; 
   &lt;td&gt;👥 Human Resources&lt;/td&gt; 
   &lt;td&gt;Facilitates self-assessment processes within an organization, aiding in performance reviews.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/self_evaluation_loop_flow"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📈 Lead Score Flow&lt;/td&gt; 
   &lt;td&gt;💼 Sales&lt;/td&gt; 
   &lt;td&gt;Evaluates and scores potential leads to prioritize outreach in sales strategies.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/lead-score-flow"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📊 Marketing Strategy Generator&lt;/td&gt; 
   &lt;td&gt;📢 Marketing&lt;/td&gt; 
   &lt;td&gt;Develops marketing strategies by analyzing market trends and audience data.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/marketing_strategy"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📝 Job Posting Generator&lt;/td&gt; 
   &lt;td&gt;🧑‍💼 Recruitment&lt;/td&gt; 
   &lt;td&gt;Creates job postings by analyzing job requirements, aiding in recruitment processes.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/job-posting"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔄 Recruitment Workflow&lt;/td&gt; 
   &lt;td&gt;🧑‍💼 Recruitment&lt;/td&gt; 
   &lt;td&gt;Streamlines the recruitment process by automating various tasks involved in hiring.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/recruitment"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔍 Match Profile to Positions&lt;/td&gt; 
   &lt;td&gt;🧑‍💼 Recruitment&lt;/td&gt; 
   &lt;td&gt;Matches candidate profiles to suitable job positions to enhance recruitment efficiency.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/match_profile_to_positions"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📸 Instagram Post Generator&lt;/td&gt; 
   &lt;td&gt;📱 Social Media&lt;/td&gt; 
   &lt;td&gt;Generates and schedules Instagram posts automatically, streamlining social media management.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/instagram_post"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🌐 Landing Page Generator&lt;/td&gt; 
   &lt;td&gt;💻 Web Development&lt;/td&gt; 
   &lt;td&gt;Automates the creation of landing pages for websites, facilitating web development tasks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/landing_page_generator"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🎮 Game Builder Crew&lt;/td&gt; 
   &lt;td&gt;🎮 Game Development&lt;/td&gt; 
   &lt;td&gt;Assists in the development of games by automating certain aspects of game creation.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/game-builder-crew"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;💹 Stock Analysis Tool&lt;/td&gt; 
   &lt;td&gt;💰 Finance&lt;/td&gt; 
   &lt;td&gt;Provides tools for analyzing stock market data to assist in financial decision-making.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🗺️ Trip Planner&lt;/td&gt; 
   &lt;td&gt;✈️ Travel&lt;/td&gt; 
   &lt;td&gt;Assists in planning trips by organizing itineraries and managing travel details.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🎁 Surprise Trip Planner&lt;/td&gt; 
   &lt;td&gt;✈️ Travel&lt;/td&gt; 
   &lt;td&gt;Plans surprise trips by selecting destinations and activities based on user preferences.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/surprise_trip"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📚 Write a Book with Flows&lt;/td&gt; 
   &lt;td&gt;✍️ Creative Writing&lt;/td&gt; 
   &lt;td&gt;Assists authors in writing books by providing structured workflows and writing assistance.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/write_a_book_with_flows"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🎬 Screenplay Writer&lt;/td&gt; 
   &lt;td&gt;✍️ Creative Writing&lt;/td&gt; 
   &lt;td&gt;Aids in writing screenplays by offering templates and guidance for script development.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/screenplay_writer"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;✅ Markdown Validator&lt;/td&gt; 
   &lt;td&gt;📄 Documentation&lt;/td&gt; 
   &lt;td&gt;Validates Markdown files to ensure proper formatting and adherence to standards.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/markdown_validator"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🧠 Meta Quest Knowledge&lt;/td&gt; 
   &lt;td&gt;📚 Knowledge Management&lt;/td&gt; 
   &lt;td&gt;Manages and organizes knowledge related to Meta Quest, facilitating information retrieval.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/meta_quest_knowledge"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🤖 NVIDIA Models Integration&lt;/td&gt; 
   &lt;td&gt;🤖 AI Integration&lt;/td&gt; 
   &lt;td&gt;Integrates NVIDIA AI models into workflows to enhance computational capabilities.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/nvidia_models"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🗂️ Prep for a Meeting&lt;/td&gt; 
   &lt;td&gt;🛠️ Productivity&lt;/td&gt; 
   &lt;td&gt;Assists in preparing for meetings by organizing materials and setting agendas.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/prep-for-a-meeting"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🛠️Starter Template&lt;/td&gt; 
   &lt;td&gt;🛠️ Development&lt;/td&gt; 
   &lt;td&gt;Provides a starter template for new projects to streamline the setup process.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/starter_template"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Repository-blue" alt="GitHub" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔗CrewAI + LangGraph Integration&lt;/td&gt; 
   &lt;td&gt;🤖 AI Integration&lt;/td&gt; 
   &lt;td&gt;Demonstrates integration between CrewAI and LangGraph for enhanced workflow automation.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;Framework Name&lt;/strong&gt;: &lt;strong&gt;Autogen&lt;/strong&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Code Generation, Execution, and Debugging&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Industry&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🤖 Automated Task Solving with Code Generation, Execution &amp;amp; Debugging&lt;/td&gt; 
   &lt;td&gt;💻 Software Development&lt;/td&gt; 
   &lt;td&gt;Demonstrates automated task-solving by generating, executing, and debugging code.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_auto_feedback_from_code_execution"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🧑‍💻 Automated Code Generation and Question Answering with Retrieval Augmented Agents&lt;/td&gt; 
   &lt;td&gt;💻 Software Development&lt;/td&gt; 
   &lt;td&gt;Generates code and answers questions using retrieval-augmented methods.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_RetrieveChat"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🧠 Automated Code Generation and Question Answering with Qdrant-based Retrieval&lt;/td&gt; 
   &lt;td&gt;💻 Software Development&lt;/td&gt; 
   &lt;td&gt;Utilizes Qdrant for enhanced retrieval-augmented agent performance.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_RetrieveChat_qdrant"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-Agent Collaboration (&amp;gt;3 Agents)&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤝 Automated Task Solving by Group Chat (3 members, 1 manager)&lt;/td&gt; 
   &lt;td align="left"&gt;🤝 Collaboration&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates group task-solving via multi-agent collaboration.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_groupchat"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📊 Automated Data Visualization by Group Chat (3 members, 1 manager)&lt;/td&gt; 
   &lt;td align="left"&gt;📊 Data Analysis&lt;/td&gt; 
   &lt;td align="left"&gt;Uses multi-agent collaboration to create data visualizations.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_groupchat_vis"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧩 Automated Complex Task Solving by Group Chat (6 members, 1 manager)&lt;/td&gt; 
   &lt;td align="left"&gt;🤝 Collaboration&lt;/td&gt; 
   &lt;td align="left"&gt;Solves complex tasks collaboratively with a larger group of agents.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_groupchat_research"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧑‍💻 Automated Task Solving with Coding &amp;amp; Planning Agents&lt;/td&gt; 
   &lt;td align="left"&gt;🛠️ Planning &amp;amp; Development&lt;/td&gt; 
   &lt;td align="left"&gt;Combines coding and planning agents for solving tasks effectively.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_planning.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📐 Automated Task Solving with Transition Paths Specified in a Graph&lt;/td&gt; 
   &lt;td align="left"&gt;🤝 Collaboration&lt;/td&gt; 
   &lt;td align="left"&gt;Uses predefined transition paths in a graph for solving tasks.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat_finite_state_machine"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Running a Group Chat as an Inner-Monologue via the SocietyOfMindAgent&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 Cognitive Sciences&lt;/td&gt; 
   &lt;td align="left"&gt;Simulates inner-monologue for problem-solving using group chats.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_society_of_mind"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔧 Running a Group Chat with Custom Speaker Selection Function&lt;/td&gt; 
   &lt;td align="left"&gt;🤝 Collaboration&lt;/td&gt; 
   &lt;td align="left"&gt;Implements a custom function for speaker selection in group chats.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_groupchat_customized"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Sequential Multi-Agent Chats&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔄 Solving Multiple Tasks in a Sequence of Chats Initiated by a Single Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🔄 Workflow Automation&lt;/td&gt; 
   &lt;td align="left"&gt;Automates sequential task-solving with a single initiating agent.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_multi_task_chats"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;⏳ Async-solving Multiple Tasks in a Sequence of Chats Initiated by a Single Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🔄 Workflow Automation&lt;/td&gt; 
   &lt;td align="left"&gt;Handles asynchronous task-solving in a sequence of chats initiated by one agent.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_multi_task_async_chats"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤝 Solving Multiple Tasks in a Sequence of Chats Initiated by Different Agents&lt;/td&gt; 
   &lt;td align="left"&gt;🔄 Workflow Automation&lt;/td&gt; 
   &lt;td align="left"&gt;Facilitates sequential task-solving with different agents initiating each chat.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchats_sequential_chats"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Nested Chats&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Solving Complex Tasks with Nested Chats&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 Problem Solving&lt;/td&gt; 
   &lt;td align="left"&gt;Uses nested chats to solve hierarchical and complex problems.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_nestedchat"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔄 Solving Complex Tasks with A Sequence of Nested Chats&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 Problem Solving&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates sequential task-solving using nested chats.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_nested_sequential_chats"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🏭 OptiGuide for Solving a Supply Chain Optimization Problem with Nested Chats&lt;/td&gt; 
   &lt;td align="left"&gt;🏭 Supply Chain Optimization&lt;/td&gt; 
   &lt;td align="left"&gt;Showcases how to solve supply chain optimization problems using nested chats, a coding agent, and a safeguard agent.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_nestedchat_optiguide"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;♟️ Conversational Chess with Nested Chats and Tool Use&lt;/td&gt; 
   &lt;td align="left"&gt;🎮 Gaming&lt;/td&gt; 
   &lt;td align="left"&gt;Explores the use of nested chats for playing conversational chess with integrated tools.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_nested_chats_chess"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Application&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔄 Automated Continual Learning from New Data&lt;/td&gt; 
   &lt;td align="left"&gt;📊 Machine Learning&lt;/td&gt; 
   &lt;td align="left"&gt;Continuously learns from new data inputs for adaptive AI.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_stream.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🏭 OptiGuide - Coding, Tool Using, Safeguarding &amp;amp; Question Answering for Supply Chain Optimization&lt;/td&gt; 
   &lt;td align="left"&gt;🏭 Supply Chain Optimization&lt;/td&gt; 
   &lt;td align="left"&gt;Highlights a solution combining coding, tool use, and safeguarding for supply chain optimization.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_nestedchat_optiguide"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤖 AutoAnny - A Discord bot built using AutoGen&lt;/td&gt; 
   &lt;td align="left"&gt;💬 Communication Tools&lt;/td&gt; 
   &lt;td align="left"&gt;Showcases the development of a Discord bot using AutoGen for enhanced interaction.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/tree/main/samples/apps/auto-anny"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🌐 Web Search: Solve Tasks Requiring Web Info&lt;/td&gt; 
   &lt;td align="left"&gt;🔍 Information Retrieval&lt;/td&gt; 
   &lt;td align="left"&gt;Searches the web to gather information required for completing tasks.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_web_info.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔧 Use Provided Tools as Functions&lt;/td&gt; 
   &lt;td align="left"&gt;🛠️ Tool Integration&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates how to use pre-provided tools as callable functions in AutoGen.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_function_call_currency_calculator"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔗 Use Tools via Sync and Async Function Calling&lt;/td&gt; 
   &lt;td align="left"&gt;🛠️ Tool Integration&lt;/td&gt; 
   &lt;td align="left"&gt;Illustrates synchronous and asynchronous tool usage within AutoGen workflows.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_function_call_async"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧩 Task Solving with Langchain Provided Tools as Functions&lt;/td&gt; 
   &lt;td align="left"&gt;🔍 Language Processing&lt;/td&gt; 
   &lt;td align="left"&gt;Leverages Langchain tools for task-solving within AutoGen.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_langchain.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📚 RAG: Group Chat with Retrieval Augmented Generation&lt;/td&gt; 
   &lt;td align="left"&gt;🤝 Collaboration&lt;/td&gt; 
   &lt;td align="left"&gt;Enables group chat with Retrieval Augmented Generation (RAG) to support information sharing.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_groupchat_RAG"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;⚙️ Function Inception: Update/Remove Functions During Conversations&lt;/td&gt; 
   &lt;td align="left"&gt;🔧 Development Tools&lt;/td&gt; 
   &lt;td align="left"&gt;Allows AutoGen agents to modify their functions dynamically during conversations.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_inception_function.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔊 Agent Chat with Whisper&lt;/td&gt; 
   &lt;td align="left"&gt;🎙️ Audio Processing&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates AI agent capabilities for transcription and translation using Whisper.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_video_transcript_translate_with_whisper"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📏 Constrained Responses via Guidance&lt;/td&gt; 
   &lt;td align="left"&gt;💡 Natural Language Processing&lt;/td&gt; 
   &lt;td align="left"&gt;Shows how to use guidance to constrain responses generated by agents.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_guidance.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🌍 Browse the Web with Agents&lt;/td&gt; 
   &lt;td align="left"&gt;🌐 Information Retrieval&lt;/td&gt; 
   &lt;td align="left"&gt;Explains how to configure agents to browse and retrieve information from the web.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_surfer.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📊 SQL: Natural Language Text to SQL Query Using Spider Benchmark&lt;/td&gt; 
   &lt;td align="left"&gt;💾 Database Management&lt;/td&gt; 
   &lt;td align="left"&gt;Converts natural language inputs into SQL queries using the Spider benchmark.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_sql_spider.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🕸️ Web Scraping with Apify&lt;/td&gt; 
   &lt;td align="left"&gt;🌐 Data Gathering&lt;/td&gt; 
   &lt;td align="left"&gt;Illustrates web scraping techniques with Apify using AutoGen.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_webscraping_with_apify"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🕷️ Web Crawling: Crawl Entire Domain with Spider API&lt;/td&gt; 
   &lt;td align="left"&gt;🌐 Data Gathering&lt;/td&gt; 
   &lt;td align="left"&gt;Explains how to crawl entire domains using the Spider API.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_webcrawling_with_spider"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;💻 Write a Software App Task by Task with Specially Designed Functions&lt;/td&gt; 
   &lt;td align="left"&gt;💻 Software Development&lt;/td&gt; 
   &lt;td align="left"&gt;Builds a software application step-by-step using designed functions.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_function_call_code_writing.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Human Development&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;💬 Simple Example in ChatGPT Style&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 Conversational AI&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates a simple conversational example in the style of ChatGPT.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/samples/simple_chat.py"&gt;&lt;img src="https://img.shields.io/badge/View-Example-blue?logo=openai" alt="Example" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤖 Auto Code Generation, Execution, Debugging and Human Feedback&lt;/td&gt; 
   &lt;td align="left"&gt;💻 Software Development&lt;/td&gt; 
   &lt;td align="left"&gt;Showcases code generation, execution, debugging with human feedback integrated into the workflow.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_human_feedback.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;👥 Automated Task Solving with GPT-4 + Multiple Human Users&lt;/td&gt; 
   &lt;td align="left"&gt;🤝 Collaboration&lt;/td&gt; 
   &lt;td align="left"&gt;Enables task solving with multiple human users collaborating with GPT-4.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_two_users.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔄 Agent Chat with Async Human Inputs&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 Conversational AI&lt;/td&gt; 
   &lt;td align="left"&gt;Supports asynchronous human input during agent conversations.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/Async_human_input.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Agent Teaching and Learning&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📘 Teach Agents New Skills &amp;amp; Reuse via Automated Chat&lt;/td&gt; 
   &lt;td align="left"&gt;🎓 Education &amp;amp; Training&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates teaching new skills to agents and enabling their reuse in automated chats.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_teaching"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Teach Agents New Facts, User Preferences and Skills Beyond Coding&lt;/td&gt; 
   &lt;td align="left"&gt;🎓 Education &amp;amp; Training&lt;/td&gt; 
   &lt;td align="left"&gt;Shows how to teach agents new facts, user preferences, and non-coding skills.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_teachability"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤖 Teach OpenAI Assistants Through GPTAssistantAgent&lt;/td&gt; 
   &lt;td align="left"&gt;💻 AI Assistant Development&lt;/td&gt; 
   &lt;td align="left"&gt;Illustrates how to enhance OpenAI assistants' capabilities using GPTAssistantAgent.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_teachable_oai_assistants.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔄 Agent Optimizer: Train Agents in an Agentic Way&lt;/td&gt; 
   &lt;td align="left"&gt;🛠️ Optimization&lt;/td&gt; 
   &lt;td align="left"&gt;Explains how to train agents effectively in an agentic manner using the Agent Optimizer.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_agentoptimizer.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-Agent Chat with OpenAI Assistants in the loop&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🌟 Hello-World Chat with OpenAI Assistant in AutoGen&lt;/td&gt; 
   &lt;td align="left"&gt;🤖 Conversational AI&lt;/td&gt; 
   &lt;td align="left"&gt;A basic example of chatting with OpenAI Assistant using AutoGen.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_oai_assistant_twoagents_basic.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔧 Chat with OpenAI Assistant using Function Call&lt;/td&gt; 
   &lt;td align="left"&gt;🔧 Development Tools&lt;/td&gt; 
   &lt;td align="left"&gt;Illustrates how to use function calls with OpenAI Assistant in chats.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_oai_assistant_function_call.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Chat with OpenAI Assistant with Code Interpreter&lt;/td&gt; 
   &lt;td align="left"&gt;💻 Software Development&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates the use of OpenAI Assistant as a code interpreter in chats.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_oai_code_interpreter.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔍 Chat with OpenAI Assistant with Retrieval Augmentation&lt;/td&gt; 
   &lt;td align="left"&gt;📚 Information Retrieval&lt;/td&gt; 
   &lt;td align="left"&gt;Enables retrieval-augmented conversations with OpenAI Assistant.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_oai_assistant_retrieval.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤝 OpenAI Assistant in a Group Chat&lt;/td&gt; 
   &lt;td align="left"&gt;🤝 Collaboration&lt;/td&gt; 
   &lt;td align="left"&gt;Shows how OpenAI Assistant can collaborate with other agents in a group chat.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_oai_assistant_groupchat.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🛠️ GPTAssistantAgent based Multi-Agent Tool Use&lt;/td&gt; 
   &lt;td align="left"&gt;🔧 Development Tools&lt;/td&gt; 
   &lt;td align="left"&gt;Explains how to use GPTAssistantAgent for multi-agent tool usage.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/gpt_assistant_agent_function_call.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Non-OpenAI Models&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;♟️ Conversational Chess using Non-OpenAI Models&lt;/td&gt; 
   &lt;td align="left"&gt;🎮 Gaming&lt;/td&gt; 
   &lt;td align="left"&gt;Explores conversational chess implemented with non-OpenAI models.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_nested_chats_chess_altmodels"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Multimodal Agent&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🎨 Multimodal Agent Chat with DALLE and GPT-4V&lt;/td&gt; 
   &lt;td align="left"&gt;🖼️ Multimedia AI&lt;/td&gt; 
   &lt;td align="left"&gt;Combines DALLE and GPT-4V for multimodal agent communication.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_dalle_and_gpt4v.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🖌️ Multimodal Agent Chat with Llava&lt;/td&gt; 
   &lt;td align="left"&gt;📷 Image Processing&lt;/td&gt; 
   &lt;td align="left"&gt;Uses Llava for enabling multimodal agent conversations with image processing.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_lmm_llava.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🖼️ Multimodal Agent Chat with GPT-4V&lt;/td&gt; 
   &lt;td align="left"&gt;🖼️ Multimedia AI&lt;/td&gt; 
   &lt;td align="left"&gt;Leverages GPT-4V for visual and conversational interactions in multimodal agents.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_lmm_gpt-4v.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Long Context Handling&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📜 Long Context Handling as A Capability&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI Capability&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates techniques for handling long context effectively within AI workflows.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_transform_messages"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Evaluation and Assessment&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📊 AgentEval: A Multi-Agent System for Assessing Utility of LLM-Powered Applications&lt;/td&gt; 
   &lt;td align="left"&gt;📈 Performance Evaluation&lt;/td&gt; 
   &lt;td align="left"&gt;Introduces AgentEval for evaluating and assessing the performance of LLM-based applications.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agenteval_cq_math.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Automatic Agent Building&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🏗️ Automatically Build Multi-agent System with AgentBuilder&lt;/td&gt; 
   &lt;td align="left"&gt;🤖 AI Development&lt;/td&gt; 
   &lt;td align="left"&gt;Explains how to automatically build multi-agent systems using the AgentBuilder tool.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/autobuild_basic.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📚 Automatically Build Multi-agent System from Agent Library&lt;/td&gt; 
   &lt;td align="left"&gt;🤖 AI Development&lt;/td&gt; 
   &lt;td align="left"&gt;Shows how to construct multi-agent systems by leveraging a pre-defined agent library.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/autobuild_agent_library.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Observability&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📊 Track LLM Calls, Tool Usage, Actions and Errors using AgentOps&lt;/td&gt; 
   &lt;td align="left"&gt;📈 Monitoring &amp;amp; Analytics&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates how to monitor LLM interactions, tool usage, and errors using AgentOps.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_agentops.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Enhanced Inferences&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔗 API Unification&lt;/td&gt; 
   &lt;td align="left"&gt;🔧 API Management&lt;/td&gt; 
   &lt;td align="left"&gt;Explains how to unify API usage with documentation and code examples.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference/#api-unification"&gt;&lt;img src="https://img.shields.io/badge/View-Documentation-blue?logo=readthedocs" alt="Documentation" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;⚙️ Utility Functions to Help Managing API Configurations Effectively&lt;/td&gt; 
   &lt;td align="left"&gt;🔧 API Management&lt;/td&gt; 
   &lt;td align="left"&gt;Demonstrates utility functions to manage API configurations more effectively.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://microsoft.github.io/autogen/0.2/docs/topics/llm_configuration"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;💰 Cost Calculation&lt;/td&gt; 
   &lt;td align="left"&gt;📈 Cost Management&lt;/td&gt; 
   &lt;td align="left"&gt;Introduces methods for tracking token usage and estimating costs for LLM interactions.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/agentchat_cost_token_tracking.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;⚡ Optimize for Code Generation&lt;/td&gt; 
   &lt;td align="left"&gt;📊 Optimization&lt;/td&gt; 
   &lt;td align="left"&gt;Highlights cost-effective optimization techniques for improving code generation with LLMs.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/oai_completion.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📐 Optimize for Math&lt;/td&gt; 
   &lt;td align="left"&gt;📊 Optimization&lt;/td&gt; 
   &lt;td align="left"&gt;Explains techniques to optimize LLM performance for solving mathematical problems.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/autogen/raw/0.2/notebook/oai_chatgpt_gpt4.ipynb"&gt;&lt;img src="https://img.shields.io/badge/View-Notebook-blue?logo=jupyter" alt="Notebook" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;Framework Name&lt;/strong&gt;: &lt;strong&gt;Agno&lt;/strong&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;UseCase&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤖 Support Agent&lt;/td&gt; 
   &lt;td align="left"&gt;💻 Software Development / AI / Framework Support&lt;/td&gt; 
   &lt;td align="left"&gt;The Agno Support Agent helps developers with the Agno framework by providing real-time answers, explanations, and code examples.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/agno_support_agent.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🎥 YouTube Agent&lt;/td&gt; 
   &lt;td align="left"&gt;📺 Media &amp;amp; Content&lt;/td&gt; 
   &lt;td align="left"&gt;An intelligent agent that analyzes YouTube videos by generating detailed summaries, timestamps, themes, and content breakdowns using AI tools.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/youtube_agent.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📊 Finance Agent&lt;/td&gt; 
   &lt;td align="left"&gt;💼 Finance&lt;/td&gt; 
   &lt;td align="left"&gt;An advanced AI-powered market analyst that delivers real-time stock market insights, analyst recommendations, financial deep-dives, and sector-specific trends. Supports prompts for detailed analysis of companies like AAPL, TSLA, NVDA, etc.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/thinking_finance_agent.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📚 Study Partner&lt;/td&gt; 
   &lt;td align="left"&gt;🎓 Education&lt;/td&gt; 
   &lt;td align="left"&gt;Assists users in learning by finding resources, answering questions, and creating study plans.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/study_partner.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🛍️ Shopping Partner Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🏬 E-commerce&lt;/td&gt; 
   &lt;td align="left"&gt;A product recommender agent that helps users find matching products based on preferences from trusted platforms like Amazon, Flipkart, etc.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/shopping_partner.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🎓 Research Scholar Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 Education / Research&lt;/td&gt; 
   &lt;td align="left"&gt;An AI-powered academic assistant that performs advanced academic searches, analyzes recent publications, synthesizes findings across disciplines, and writes well-structured academic reports with proper citations.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/research_agent_exa.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Research Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🗞️ Media &amp;amp; Journalism&lt;/td&gt; 
   &lt;td align="left"&gt;A research agent that combines web search and professional journalistic writing. It performs deep investigations and produces NYT-style reports.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/research_agent.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🍳 Recipe Creator&lt;/td&gt; 
   &lt;td align="left"&gt;🍽️ Food &amp;amp; Culinary&lt;/td&gt; 
   &lt;td align="left"&gt;An AI-powered recipe recommendation agent that provides personalized recipes based on ingredients, preferences, and time constraints.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/recipe_creator.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🗞️ Finance Agent&lt;/td&gt; 
   &lt;td align="left"&gt;💼 Finance&lt;/td&gt; 
   &lt;td align="left"&gt;A powerful financial analyst agent combining real-time stock data, analyst insights, company fundamentals, and market news. Ideal for analyzing companies like Apple, Tesla, NVIDIA, and sectors like semiconductors or automotive.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/finance_agent.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Financial Reasoning Agent&lt;/td&gt; 
   &lt;td align="left"&gt;📈 Finance&lt;/td&gt; 
   &lt;td align="left"&gt;Uses a Claude-3.5 Sonnet-based agent to analyze stocks like NVDA using tools for reasoning and Yahoo Finance data.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/reasoning_finance_agent.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤖 Readme Generator Agent&lt;/td&gt; 
   &lt;td align="left"&gt;💻 Software Dev&lt;/td&gt; 
   &lt;td align="left"&gt;Agent generates high-quality READMEs for GitHub repositories using repo metadata.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/readme_generator.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🎬 Movie Recommendation Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🎥 Entertainment&lt;/td&gt; 
   &lt;td align="left"&gt;An intelligent agent that gives personalized movie recommendations using Exa and GPT-4o, analyzing genres, themes, and latest ratings.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/movie_recommedation.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔍 Media Trend Analysis Agent&lt;/td&gt; 
   &lt;td align="left"&gt;📰 Media &amp;amp; News&lt;/td&gt; 
   &lt;td align="left"&gt;Analyzes emerging trends, patterns, and influencers from digital platforms using AI-powered agents and scraping.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/media_trend_analysis_agent.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;⚖️ Legal Document Analysis Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🏛️ Legal Tech&lt;/td&gt; 
   &lt;td align="left"&gt;An AI agent that analyzes legal documents from PDF URLs and provides legal insights based on a knowledge base using vector embeddings and GPT-4o.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/legal_consultant.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤔 DeepKnowledge&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 Research&lt;/td&gt; 
   &lt;td align="left"&gt;This agent performs iterative searches through its knowledge base, breaking down complex queries into sub-questions and synthesizing comprehensive answers. It uses Agno docs for demonstration and is designed for deep reasoning and exploration.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/deep_knowledge.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;📚 Book Recommendation Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 Publishing &amp;amp; Media&lt;/td&gt; 
   &lt;td align="left"&gt;An intelligent agent that provides personalized book suggestions using literary data, reader preferences, reviews, and release info.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/book_recommendation.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🏠 MCP Airbnb Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🛎️ Hospitality&lt;/td&gt; 
   &lt;td align="left"&gt;Create an AI Agent using MCP and Llama 4 to search Airbnb listings with filters like workspace &amp;amp; transport proximity.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/airbnb_mcp.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤖 Assist Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI Framework&lt;/td&gt; 
   &lt;td align="left"&gt;An AI agent using GPT-4o to answer questions about the Agno framework with hybrid search and embedded knowledge.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/agno-agi/agno/raw/main/cookbook/examples/agents/agno_assist.py"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;Framework Name&lt;/strong&gt;: &lt;strong&gt;Langgraph&lt;/strong&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;UseCase&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Use Case&lt;/th&gt; 
   &lt;th align="left"&gt;Industry&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤖 Chatbot Simulation Evaluation&lt;/td&gt; 
   &lt;td align="left"&gt;💻 💬 AI / Quality Assurance&lt;/td&gt; 
   &lt;td align="left"&gt;Simulate user interactions to evaluate chatbot performance, ensuring robustness and reliability in real-world scenarios.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/chatbot-simulation-evaluation/agent-simulation-evaluation.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Information Gathering via Prompting&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Research &amp;amp; Development&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to design a LangGraph workflow that utilizes prompting techniques to gather information effectively. It showcases how to structure prompts and manage the flow of information to build intelligent agents.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/chatbots/information-gather-prompting.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Code Assistant with LangGraph&lt;/td&gt; 
   &lt;td align="left"&gt;💻 Software Development&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to build a resilient code assistant using LangGraph. It guides you through creating a graph-based agent that can handle code generation, error checking, and iterative refinement, ensuring robust and accurate coding assistance.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/code_assistant/langgraph_code_assistant.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧑‍💼 Customer Support Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🧑‍💼 Customer Support Agent&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to build a customer support agent using LangGraph. It guides you through creating a graph-based agent that can handle customer inquiries, providing automated support and enhancing user experience.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/customer-support/customer-support.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🔁 Extraction with Retries&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Data Extraction&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to implement retry mechanisms in LangGraph workflows, ensuring robust data extraction processes that can handle transient errors and improve reliability.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/extraction/retries.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Multi-Agent Workflow&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Workflow Orchestration&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to build a multi-agent system using LangGraph's agent supervisor. It guides you through creating a supervisor agent that orchestrates multiple specialized agents, managing task delegation and communication flow.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/multi_agent/agent_supervisor.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Hierarchical Agent Teams&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Workflow Orchestration&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to build a hierarchical agent system using LangGraph. It guides you through creating a top-level supervisor agent that delegates tasks to specialized sub-agents, enabling complex workflows with clear task delegation and communication.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/multi_agent/hierarchical_agent_teams.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤝 Multi-Agent Collaboration&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Workflow Orchestration&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to implement multi-agent collaboration using LangGraph. It guides you through creating multiple specialized agents that work together to accomplish a complex task, showcasing the power of agent collaboration in AI workflows.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/multi_agent/multi-agent-collaboration.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Plan-and-Execute Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Workflow Orchestration&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to build a "Plan-and-Execute" style agent using LangGraph. It guides you through creating an agent that first generates a multi-step plan and then executes each step sequentially, revisiting and modifying the plan as necessary. This approach is inspired by the Plan-and-Solve paper and the Baby-AGI project, aiming to enhance long-term planning and task execution in AI workflows.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/plan-and-execute/plan-and-execute.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 SQL Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Database Interaction&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to build an agent that can answer questions about a SQL database. The agent fetches available tables, determines relevance to the question, retrieves schemas, generates a query, checks for errors, executes it, and formulates a response.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/sql-agent.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Reflection Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Workflow Orchestration&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to build a reflection agent using LangGraph. It guides you through creating an agent that can critique and revise its own outputs, enhancing the quality and reliability of generated content.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/reflection/reflection.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 Reflexion Agent&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Workflow Orchestration&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to build a reflexion agent using LangGraph. It guides you through creating an agent that can reflect on its actions and outcomes, enabling iterative improvement and more accurate decision-making in complex workflows.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/reflexion/reflexion.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;LangGraph Agentic RAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 &lt;strong&gt;Adaptive RAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Information Retrieval&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to build an Adaptive RAG system using LangGraph. It guides you through creating a dynamic retrieval process that adjusts based on query complexity, enhancing the efficiency and accuracy of information retrieval.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/rag/langgraph_adaptive_rag.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 &lt;strong&gt;Adaptive RAG (Local)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Information Retrieval&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial focuses on implementing Adaptive RAG with local models, allowing for offline retrieval and generation, which is crucial for environments with limited internet access or privacy concerns.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/rag/langgraph_adaptive_rag_local.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤖 &lt;strong&gt;Agentic RAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;🤖 AI / Intelligent Agents&lt;/td&gt; 
   &lt;td align="left"&gt;Learn to build an Agentic RAG system where an agent determines the best retrieval strategy before generating a response, improving the relevance and accuracy of answers.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/rag/langgraph_agentic_rag.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🤖 &lt;strong&gt;Agentic RAG (Local)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;🤖 AI / Intelligent Agents&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial extends Agentic RAG to local environments, enabling the use of local models and data sources for retrieval and generation tasks.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/rag/langgraph_agentic_rag_local.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 &lt;strong&gt;Corrective RAG (CRAG)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Information Retrieval&lt;/td&gt; 
   &lt;td align="left"&gt;Implement a Corrective RAG system that evaluates and refines retrieved documents before passing them to the generator, ensuring higher-quality outputs.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/rag/langgraph_crag.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 &lt;strong&gt;Corrective RAG (Local)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Information Retrieval&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial focuses on building a Corrective RAG system using local resources, allowing for offline document evaluation and refinement processes.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/rag/langgraph_crag_local.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 &lt;strong&gt;Self-RAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Information Retrieval&lt;/td&gt; 
   &lt;td align="left"&gt;Learn to implement Self-RAG, where the system reflects on its responses and retrieves additional information if necessary, enhancing the accuracy and relevance of generated content.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/rag/langgraph_self_rag.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;🧠 &lt;strong&gt;Self-RAG (Local)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;🧠 AI / Information Retrieval&lt;/td&gt; 
   &lt;td align="left"&gt;This tutorial demonstrates how to implement Self-RAG using local models and data sources, enabling offline reflection and retrieval processes.&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/langchain-ai/langgraph/raw/main/docs/docs/tutorials/rag/langgraph_self_rag_local.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=AI+Agent+Code&amp;amp;message=Python&amp;amp;color=%23244cd1" alt="AI Agent Code - Python" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🤝 Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! 🎉 Here's how you can help:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository.&lt;/li&gt; 
 &lt;li&gt;Add a new use case or improve an existing one.&lt;/li&gt; 
 &lt;li&gt;Submit a pull request with your changes.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please follow our &lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📜 License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/ashishpatel26/500-AI-Agents-Projects/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 Let's Build Together!&lt;/h2&gt; 
&lt;p&gt;Feel free to share this repository with your network and star ⭐ it if you find it useful. Let’s collaborate to create the ultimate resource for AI agent use cases!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>denizsafak/abogen</title>
      <link>https://github.com/denizsafak/abogen</link>
      <description>&lt;p&gt;Generate audiobooks from EPUBs, PDFs and text with synchronized captions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;abogen &lt;img width="40px" title="abogen icon" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico" align="right" style="padding-left: 10px; padding-top:5px;" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/denizsafak/abogen/actions"&gt;&lt;img src="https://github.com/denizsafak/abogen/actions/workflows/test_pip.yml/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/denizsafak/abogen" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/abogen/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen PyPi Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/os-windows%20%7C%20linux%20%7C%20macos%20-blue" alt="Operating Systems" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-maroon.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Abogen is a powerful text-to-speech conversion tool that makes it easy to turn ePub, PDF, or text files into high-quality audio with matching subtitles in seconds. Use it for audiobooks, voiceovers for Instagram, YouTube, TikTok, or any project that needs natural-sounding text-to-speech, using &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img title="Abogen Main" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png" width="380" /&gt; &lt;img title="Abogen Processing" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png" width="380" /&gt;&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/094ba3df-7d66-494a-bc31-0e4b41d0b865"&gt;https://github.com/user-attachments/assets/094ba3df-7d66-494a-bc31-0e4b41d0b865&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This demo was generated in just 5&amp;nbsp;seconds, producing ∼1&amp;nbsp;minute of audio with perfectly synced subtitles. To create a similar video, see &lt;a href="https://github.com/denizsafak/abogen/tree/main/demo"&gt;the demo guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to install?&lt;/code&gt; &lt;a href="https://pypi.org/project/abogen/" target="_blank"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen Compatible PyPi Python Versions" align="right" style="margin-top:6px;" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Go to &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng latest release&lt;/a&gt; download and run the *.msi file.&lt;/p&gt; 
&lt;h4&gt;OPTION 1: Install using script&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download&lt;/a&gt; the repository&lt;/li&gt; 
 &lt;li&gt;Extract the ZIP file&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;WINDOWS_INSTALL.bat&lt;/code&gt; by double-clicking it&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This method handles everything automatically - installing all dependencies including CUDA in a self-contained environment without requiring a separate Python installation. (You still need to install &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng&lt;/a&gt;.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You don't need to install Python separately. The script will install Python automatically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;OPTION 2: Install using pip&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a virtual environment (optional)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python -m venv venv
venv\Scripts\activate

# For NVIDIA GPUs:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# For AMD GPUs:
# Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.

# Install abogen
pip install abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
brew install espeak-ng

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For Silicon Mac (M1, M2 etc.)
# After installing abogen, we need to install Kokoro's development version which includes MPS support.
pip3 install git+https://github.com/hexgrad/kokoro.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
sudo apt install espeak-ng # Ubuntu/Debian
sudo pacman -S espeak-ng # Arch Linux
sudo dnf install espeak-ng # Fedora

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For NVIDIA GPUs:
# Already supported, no need to install CUDA separately.

# For AMD GPUs:
# After installing abogen, we need to uninstall the existing torch package
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get &lt;code&gt;WARNING: The script abogen-cli is installed in '/home/username/.local/bin' which is not on PATH.&lt;/code&gt; error, run the following command to add it to your PATH:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;echo "export PATH=\"/home/$USER/.local/bin:\$PATH\"" &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get "No matching distribution found" error, try installing it on supported Python (3.10 to 3.12). You can use &lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt; to manage multiple Python versions easily in Linux. Watch this &lt;a href="https://www.youtube.com/watch?v=MVyb-nI4KyI"&gt;video&lt;/a&gt; by NetworkChuck for a quick guide.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/hg000125"&gt;@hg000125&lt;/a&gt; for his contribution in &lt;a href="https://github.com/denizsafak/abogen/issues/23"&gt;#23&lt;/a&gt;. AMD GPU support is possible thanks to his work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to run?&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you installed using pip, you can simply run the following command to start Abogen:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you installed using the Windows installer &lt;code&gt;(WINDOWS_INSTALL.bat)&lt;/code&gt;, It should have created a shortcut in the same folder, or your desktop. You can run it from there. If you lost the shortcut, Abogen is located in &lt;code&gt;python_embedded/Scripts/abogen.exe&lt;/code&gt;. You can run it from there directly.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to use?&lt;/code&gt;&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Drag and drop any ePub, PDF, or text file (or use the built-in text editor)&lt;/li&gt; 
 &lt;li&gt;Configure the settings: 
  &lt;ul&gt; 
   &lt;li&gt;Set speech speed&lt;/li&gt; 
   &lt;li&gt;Select a voice (or create a custom voice using voice mixer)&lt;/li&gt; 
   &lt;li&gt;Select subtitle generation style (by sentence, word, etc.)&lt;/li&gt; 
   &lt;li&gt;Select output format&lt;/li&gt; 
   &lt;li&gt;Select where to save the output&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Hit Start&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;code&gt;In action&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen in action" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif" /&gt; 
&lt;p&gt;Here’s Abogen in action: in this demo, it processes ∼3,000 characters of text in just 11 seconds and turns it into 3 minutes and 28 seconds of audio, and I have a low-end &lt;strong&gt;RTX&amp;nbsp;2060&amp;nbsp;Mobile laptop GPU&lt;/strong&gt;. Your results may vary depending on your hardware.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Configuration&lt;/code&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Input Box&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Drag and drop &lt;code&gt;ePub&lt;/code&gt;, &lt;code&gt;PDF&lt;/code&gt;, or &lt;code&gt;.TXT&lt;/code&gt; files (or use built-in text editor)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Queue options&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Add multiple files to a queue and process them in batch, with individual settings for each file. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#queue-mode"&gt;Queue mode&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Adjust speech rate from &lt;code&gt;0.1x&lt;/code&gt; to &lt;code&gt;2.0x&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Select Voice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;First letter of the language code (e.g., &lt;code&gt;a&lt;/code&gt; for American English, &lt;code&gt;b&lt;/code&gt; for British English, etc.), second letter is for &lt;code&gt;m&lt;/code&gt; for male and &lt;code&gt;f&lt;/code&gt; for female.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice mixer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create custom voices by mixing different voice models with a profile system. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#voice-mixer"&gt;Voice Mixer&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice preview&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Listen to the selected voice before processing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Generate subtitles&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Disabled&lt;/code&gt;, &lt;code&gt;Sentence&lt;/code&gt;, &lt;code&gt;Sentence + Comma&lt;/code&gt;, &lt;code&gt;Sentence + Highlighting&lt;/code&gt;, &lt;code&gt;1 word&lt;/code&gt;, &lt;code&gt;2 words&lt;/code&gt;, &lt;code&gt;3 words&lt;/code&gt;, etc. (Represents the number of words in each subtitle entry)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output voice format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.WAV&lt;/code&gt;, &lt;code&gt;.FLAC&lt;/code&gt;, &lt;code&gt;.MP3&lt;/code&gt;, &lt;code&gt;.OPUS (best compression)&lt;/code&gt; and &lt;code&gt;M4B (with chapters)&lt;/code&gt; (Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for chapter support in PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output subtitle format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the subtitle format as &lt;code&gt;SRT (standard)&lt;/code&gt;, &lt;code&gt;ASS (wide)&lt;/code&gt;, &lt;code&gt;ASS (narrow)&lt;/code&gt;, &lt;code&gt;ASS (centered wide)&lt;/code&gt;, or &lt;code&gt;ASS (centered narrow)&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Replace single newlines with spaces&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Replaces single newlines with spaces in the text. This is useful for texts that have imaginary line breaks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save location&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Save next to input file&lt;/code&gt;, &lt;code&gt;Save to desktop&lt;/code&gt;, or &lt;code&gt;Choose output folder&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Book handler options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chapter Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Select specific &lt;code&gt;chapters&lt;/code&gt; from ePUBs or &lt;code&gt;chapters + pages&lt;/code&gt; from PDFs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save each chapter separately&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save each chapter in e-books as a separate audio file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create a merged version&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create a single audio file that combines all chapters. (If &lt;code&gt;Save each chapter separately&lt;/code&gt; is disabled, this option will be the default behavior.)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save in a project folder with metadata&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save the converted items in a project folder with available metadata files.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Menu options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Theme&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Change the application's theme using &lt;code&gt;System&lt;/code&gt;, &lt;code&gt;Light&lt;/code&gt;, or &lt;code&gt;Dark&lt;/code&gt; options.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max words per subtitle&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of words per subtitle entry.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max lines in log window&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of lines to display in the log window.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Separate chapters audio format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the audio format for separate chapters as &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;flac&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, or &lt;code&gt;opus&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create desktop shortcut&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Creates a shortcut on your desktop for easy access.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open config directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the directory where the configuration file is stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open cache directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the cache directory where converted text files are stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Clear cache files&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Deletes cache files created during the conversion or preview.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Check for updates at startup&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automatically checks for updates when the program starts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Disable Kokoro's internet access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Prevents Kokoro from downloading models or voices from HuggingFace Hub, useful for offline use.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Reset to default settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Resets all settings to their default values.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/robmckinnon"&gt;@robmckinnon&lt;/a&gt; for adding Sentence + Highlighting feature in PR &lt;a href="https://github.com/denizsafak/abogen/pull/65"&gt;#65&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;Voice Mixer&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen Voice Mixer" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png" /&gt; 
&lt;p&gt;With voice mixer, you can create custom voices by mixing different voice models. You can adjust the weight of each voice and save your custom voice as a profile for future use. The voice mixer allows you to create unique and personalized voices. (Huge thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for making this possible through his contributions in &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Queue Mode&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen queue mode" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png" /&gt; 
&lt;p&gt;Abogen supports &lt;strong&gt;queue mode&lt;/strong&gt;, allowing you to add multiple files to a processing queue. This is useful if you want to convert several files in one batch.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can add text files (&lt;code&gt;.txt&lt;/code&gt;) directly using the &lt;strong&gt;Add files&lt;/strong&gt; button in the Queue Manager. To add PDF or EPUB files, use the input box in the main window and click the &lt;strong&gt;Add to Queue&lt;/strong&gt; button.&lt;/li&gt; 
 &lt;li&gt;Each file in the queue keeps the configuration settings that were active when it was added. Changing the main window configuration afterward does &lt;strong&gt;not&lt;/strong&gt; affect files already in the queue.&lt;/li&gt; 
 &lt;li&gt;You can view each file's configuration by hovering over them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Abogen will process each item in the queue automatically, saving outputs as configured.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for adding queue mode in PR &lt;a href="https://github.com/denizsafak/abogen/pull/35"&gt;#35&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;About Chapter Markers&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;When you process ePUB or PDF files, Abogen converts them into text files stored in your cache directory. When you click "Edit," you're actually modifying these converted text files. In these text files, you'll notice tags that look like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Chapter Title&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These are chapter markers. They are automatically added when you process ePUB or PDF files, based on the chapters you select. They serve an important purpose:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Allow you to split the text into separate audio files for each chapter&lt;/li&gt; 
 &lt;li&gt;Save time by letting you reprocess only specific chapters if errors occur, rather than the entire file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can manually add these markers to plain text files for the same benefits. Simply include them in your text like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Introduction&amp;gt;&amp;gt;
This is the beginning of my text...  

&amp;lt;&amp;lt;CHAPTER_MARKER:Main Content&amp;gt;&amp;gt; 
Here's another part...  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When you process the text file, Abogen will detect these markers automatically and ask if you want to save each chapter separately and create a merged version.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png" alt="Abogen Chapter Marker" /&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;About Metadata Tags&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Similar to chapter markers, it is possible to add metadata tags for &lt;code&gt;M4B&lt;/code&gt; files. This is useful for audiobook players that support metadata, allowing you to add information like title, author, year, etc. Abogen automatically adds these tags when you process ePUB or PDF files, but you can also add them manually to your text files. Add metadata tags &lt;strong&gt;at the beginning of your text file&lt;/strong&gt; like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;METADATA_TITLE:Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ARTIST:Author&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM:Album Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_YEAR:Year&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM_ARTIST:Album Artist&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_COMPOSER:Narrator&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_GENRE:Audiobook&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Supported Languages&lt;/code&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;# 🇺🇸 'a' =&amp;gt; American English, 🇬🇧 'b' =&amp;gt; British English
# 🇪🇸 'e' =&amp;gt; Spanish es
# 🇫🇷 'f' =&amp;gt; French fr-fr
# 🇮🇳 'h' =&amp;gt; Hindi hi
# 🇮🇹 'i' =&amp;gt; Italian it
# 🇯🇵 'j' =&amp;gt; Japanese: pip install misaki[ja]
# 🇧🇷 'p' =&amp;gt; Brazilian Portuguese pt-br
# 🇨🇳 'z' =&amp;gt; Mandarin Chinese: pip install misaki[zh]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a complete list of supported languages and voices, refer to Kokoro's &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md"&gt;VOICES.md&lt;/a&gt;. To listen to sample audio outputs, see &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md"&gt;SAMPLES.md&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Japanese audio may require additional configuration. Please check &lt;a href="https://github.com/denizsafak/abogen/issues/56"&gt;#56&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;MPV Config&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I highly recommend using &lt;a href="https://mpv.io/installation/"&gt;MPV&lt;/a&gt; to play your audio files, as it supports displaying subtitles even without a video track. Here's my &lt;code&gt;mpv.conf&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Docker Guide&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you want to run Abogen in a Docker container:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download the repository&lt;/a&gt; and extract, or clone it using git.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;code&gt;abogen&lt;/code&gt; folder. You should see &lt;code&gt;Dockerfile&lt;/code&gt; there.&lt;/li&gt; 
 &lt;li&gt;Open your termminal in that directory and run the following commands:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build the Docker image:
docker build --progress plain -t abogen .

# Note that building the image may take a while.
# After building is complete, run the Docker container:

# Windows
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# Linux
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# MacOS
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 abogen

# We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Abogen launches automatically inside the container.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can access it via a web browser at &lt;a href="http://localhost:5800"&gt;http://localhost:5800&lt;/a&gt; or connect to it using a VNC client at &lt;code&gt;localhost:5900&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;You can use &lt;code&gt;/shared&lt;/code&gt; directory to share files between your host and the container.&lt;/li&gt; 
 &lt;li&gt;For later use, start it with &lt;code&gt;docker start abogen&lt;/code&gt; and stop it with &lt;code&gt;docker stop abogen&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Known issues:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Audio preview is not working inside container (ALSA error).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Open cache directory&lt;/code&gt; and &lt;code&gt;Open configuration directory&lt;/code&gt; options in settings not working. (Tried pcmanfm, did not work with Abogen).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(Special thanks to &lt;a href="https://www.reddit.com/user/geo38/"&gt;@geo38&lt;/a&gt; from Reddit, who provided the Dockerfile and instructions in &lt;a href="https://www.reddit.com/r/selfhosted/comments/1k8x1yo/comment/mpe0bz8/"&gt;this comment&lt;/a&gt;.)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Similar Projects&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Abogen is a standalone project, but it is inspired by and shares some similarities with other projects. Here are a few:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/santinic/audiblez"&gt;audiblez&lt;/a&gt;: Generate audiobooks from e-books. &lt;strong&gt;(Has CLI and GUI support)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/plusuncold/autiobooks"&gt;autiobooks&lt;/a&gt;: Automatically convert epubs to audiobooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mateogon/pdf-narrator"&gt;pdf-narrator&lt;/a&gt;: Convert your PDFs and EPUBs into audiobooks effortlessly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/p0n1/epub_to_audiobook"&gt;epub_to_audiobook&lt;/a&gt;: EPUB to audiobook converter, optimized for Audiobookshelf&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;: Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Roadmap&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add OCR scan feature for PDF files using docling/teserract.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add chapter metadata for .m4a files. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/9"&gt;#9&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for different languages in GUI.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add voice formula feature that enables mixing different voice models. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/1"&gt;#1&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for kokoro-onnx (If it's necessary).&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add dark mode.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Troubleshooting&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you encounter any issues while running Abogen, try launching it from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;abogen-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start Abogen in command-line mode and display detailed error messages. Please open a new issue on the &lt;a href="https://github.com/denizsafak/abogen/issues"&gt;Issues&lt;/a&gt; page with the error message and a description of your problem.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Contributing&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I welcome contributions! If you have ideas for new features, improvements, or bug fixes, please fork the repository and submit a pull request.&lt;/p&gt; 
&lt;h3&gt;For developers and contributors&lt;/h3&gt; 
&lt;p&gt;If you'd like to modify the code and contribute to development, you can &lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;download the repository&lt;/a&gt;, extract it and run the following commands to build &lt;strong&gt;or&lt;/strong&gt; install the package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Go to the directory where you extracted the repository and run:
pip install -e .      # Installs the package in editable mode
pip install build     # Install the build package
python -m build       # Builds the package in dist folder (optional)
abogen                # Opens the GUI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Feel free to explore the code and make any changes you like.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Credits&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Abogen uses &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; for its high-quality, natural-sounding text-to-speech synthesis. Huge thanks to the Kokoro team for making this possible.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/wojiushixiaobai"&gt;@wojiushixiaobai&lt;/a&gt; for &lt;a href="https://github.com/wojiushixiaobai/Python-Embed-Win64"&gt;Embedded Python&lt;/a&gt; packages. These modified packages include pip pre-installed, enabling Abogen to function as a standalone application without requiring users to separately install Python in Windows.&lt;/li&gt; 
 &lt;li&gt;Thanks to creators of &lt;a href="https://github.com/aerkalov/ebooklib"&gt;EbookLib&lt;/a&gt;, a Python library for reading and writing ePub files, which is used for extracting text from ePub files.&lt;/li&gt; 
 &lt;li&gt;Special thanks to the &lt;a href="https://www.riverbankcomputing.com/software/pyqt/"&gt;PyQt&lt;/a&gt; team for providing the cross-platform GUI toolkit that powers Abogen's interface.&lt;/li&gt; 
 &lt;li&gt;Icons: &lt;a href="https://icons8.com/icon/aRiu1GGi6Aoe/usa"&gt;US&lt;/a&gt;, &lt;a href="https://icons8.com/icon/t3NE3BsOAQwq/great-britain"&gt;Great Britain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/ly7tzANRt33n/spain"&gt;Spain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/3muzEmi4dpD5/france"&gt;France&lt;/a&gt;, &lt;a href="https://icons8.com/icon/esGVrxg9VCJ1/india"&gt;India&lt;/a&gt;, &lt;a href="https://icons8.com/icon/PW8KZnP7qXzO/italy"&gt;Italy&lt;/a&gt;, &lt;a href="https://icons8.com/icon/McQbrq9qaQye/japan"&gt;Japan&lt;/a&gt;, &lt;a href="https://icons8.com/icon/zHmH8HpOmM90/brazil"&gt;Brazil&lt;/a&gt;, &lt;a href="https://icons8.com/icon/Ej50Oe3crXwF/china"&gt;China&lt;/a&gt;, &lt;a href="https://icons8.com/icon/uI49hxbpxTkp/female"&gt;Female&lt;/a&gt;, &lt;a href="https://icons8.com/icon/12351/male"&gt;Male&lt;/a&gt;, &lt;a href="https://icons8.com/icon/21698/adjust"&gt;Adjust&lt;/a&gt; and &lt;a href="https://icons8.com/icon/GskSeVoroQ7u/voice-id"&gt;Voice Id&lt;/a&gt; icons by &lt;a href="https://icons8.com/"&gt;Icons8&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;License&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;This project is available under the MIT License - see the &lt;a href="https://github.com/denizsafak/abogen/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details. &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; is licensed under &lt;a href="https://github.com/hexgrad/kokoro/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; which allows commercial use, modification, distribution, and private use.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Subtitle generation currently works only for English. This is because Kokoro provides timestamp tokens only for English text. If you want subtitles in other languages, please request this feature in the &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro project&lt;/a&gt;. For more technical details, see &lt;a href="https://github.com/hexgrad/kokoro/raw/6d87f4ae7abc2d14dbc4b3ef2e5f19852e861ac2/kokoro/pipeline.py#L383"&gt;this line&lt;/a&gt; in the Kokoro's code.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tags: audiobook, kokoro, text-to-speech, TTS, audiobook generator, audiobooks, text to speech, audiobook maker, audiobook creator, audiobook generator, voice-synthesis, text to audio, text to audio converter, text to speech converter, text to speech generator, text to speech software, text to speech app, epub to audio, pdf to audio, content-creation, media-generation&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>dockur/windows</title>
      <link>https://github.com/dockur/windows</link>
      <description>&lt;p&gt;Windows inside a Docker container.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Windows&lt;br /&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://github.com/dockur/windows"&gt;&lt;img src="https://github.com/dockur/windows/raw/master/.github/logo.png" title="Logo" style="max-width:100%;" width="128" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;p&gt;&lt;a href="https://github.com/dockur/windows/"&gt;&lt;img src="https://github.com/dockur/windows/actions/workflows/build.yml/badge.svg?sanitize=true" alt="Build" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dockurr/windows/tags"&gt;&lt;img src="https://img.shields.io/docker/v/dockurr/windows/latest?arch=amd64&amp;amp;sort=semver&amp;amp;color=066da5" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dockurr/windows/tags"&gt;&lt;img src="https://img.shields.io/docker/image-size/dockurr/windows/latest?color=066da5&amp;amp;label=size" alt="Size" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dockur/windows/pkgs/container/windows"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fdockur%2Fwindows%2Fwindows.json&amp;amp;query=%24.downloads&amp;amp;logo=github&amp;amp;style=flat&amp;amp;color=066da5&amp;amp;label=pulls" alt="Package" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dockurr/windows/"&gt;&lt;img src="https://img.shields.io/docker/pulls/dockurr/windows.svg?style=flat&amp;amp;label=pulls&amp;amp;logo=docker" alt="Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt;&lt;/h1&gt; 
&lt;p&gt;Windows inside a Docker container.&lt;/p&gt; 
&lt;h2&gt;Features ✨&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ISO downloader&lt;/li&gt; 
 &lt;li&gt;KVM acceleration&lt;/li&gt; 
 &lt;li&gt;Web-based viewer&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Video 📺&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=xhGYobuG508"&gt;&lt;img src="https://img.youtube.com/vi/xhGYobuG508/0.jpg" alt="Youtube" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage 🐳&lt;/h2&gt; 
&lt;h5&gt;Via Docker Compose:&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  windows:
    image: dockurr/windows
    container_name: windows
    environment:
      VERSION: "11"
    devices:
      - /dev/kvm
      - /dev/net/tun
    cap_add:
      - NET_ADMIN
    ports:
      - 8006:8006
      - 3389:3389/tcp
      - 3389:3389/udp
    volumes:
      - ./windows:/storage
    restart: always
    stop_grace_period: 2m
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Via Docker CLI:&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --rm --name windows -p 8006:8006 --device=/dev/kvm --device=/dev/net/tun --cap-add NET_ADMIN -v "${PWD:-.}/windows:/storage" --stop-timeout 120 dockurr/windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Via Kubernetes:&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;kubectl apply -f https://raw.githubusercontent.com/dockur/windows/refs/heads/master/kubernetes.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Via Github Codespaces:&lt;/h5&gt; 
&lt;p&gt;&lt;a href="https://codespaces.new/dockur/windows"&gt;&lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" alt="Open in GitHub Codespaces" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;FAQ 💬&lt;/h2&gt; 
&lt;h3&gt;How do I use it?&lt;/h3&gt; 
&lt;p&gt;Very simple! These are the steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Start the container and connect to &lt;a href="http://127.0.0.1:8006/"&gt;port 8006&lt;/a&gt; using your web browser.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Sit back and relax while the magic happens, the whole installation will be performed fully automatic.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Once you see the desktop, your Windows installation is ready for use.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Enjoy your brand new machine, and don't forget to star this repo!&lt;/p&gt; 
&lt;h3&gt;How do I select the Windows version?&lt;/h3&gt; 
&lt;p&gt;By default, Windows 11 Pro will be installed. But you can add the &lt;code&gt;VERSION&lt;/code&gt; environment variable to your compose file, in order to specify an alternative Windows version to be downloaded:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  VERSION: "11"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Select from the values below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Version&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Size&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;11&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 11 Pro&lt;/td&gt; 
   &lt;td&gt;5.4 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;11l&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 11 LTSC&lt;/td&gt; 
   &lt;td&gt;4.7 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;11e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 11 Enterprise&lt;/td&gt; 
   &lt;td&gt;4.0 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 10 Pro&lt;/td&gt; 
   &lt;td&gt;5.7 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;10l&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 10 LTSC&lt;/td&gt; 
   &lt;td&gt;4.6 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;10e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 10 Enterprise&lt;/td&gt; 
   &lt;td&gt;5.2 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;8e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 8.1 Enterprise&lt;/td&gt; 
   &lt;td&gt;3.7 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;7u&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 7 Ultimate&lt;/td&gt; 
   &lt;td&gt;3.1 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;vu&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Vista Ultimate&lt;/td&gt; 
   &lt;td&gt;3.0 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;xp&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows XP Professional&lt;/td&gt; 
   &lt;td&gt;0.6 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2k&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 2000 Professional&lt;/td&gt; 
   &lt;td&gt;0.4 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2025&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2025&lt;/td&gt; 
   &lt;td&gt;5.6 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2022&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2022&lt;/td&gt; 
   &lt;td&gt;4.7 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2019&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2019&lt;/td&gt; 
   &lt;td&gt;5.3 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2016&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2016&lt;/td&gt; 
   &lt;td&gt;6.5 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2012&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2012&lt;/td&gt; 
   &lt;td&gt;4.3 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2008&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2008&lt;/td&gt; 
   &lt;td&gt;3.0 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2003&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2003&lt;/td&gt; 
   &lt;td&gt;0.6 GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] To install ARM64 versions of Windows use &lt;a href="https://github.com/dockur/windows-arm/"&gt;dockur/windows-arm&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;How do I change the storage location?&lt;/h3&gt; 
&lt;p&gt;To change the storage location, include the following bind mount in your compose file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;volumes:
  - ./windows:/storage
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace the example path &lt;code&gt;./windows&lt;/code&gt; with the desired storage folder or named volume.&lt;/p&gt; 
&lt;h3&gt;How do I change the size of the disk?&lt;/h3&gt; 
&lt;p&gt;To expand the default size of 64 GB, add the &lt;code&gt;DISK_SIZE&lt;/code&gt; setting to your compose file and set it to your preferred capacity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  DISK_SIZE: "256G"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] This can also be used to resize the existing disk to a larger capacity without any data loss.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;How do I share files with the host?&lt;/h3&gt; 
&lt;p&gt;Open 'File Explorer' and click on the 'Network' section, you will see a computer called &lt;code&gt;host.lan&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Double-click it and it will show a folder called &lt;code&gt;Data&lt;/code&gt;, which can be bound to any folder on your host via the compose file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;volumes:
  -  ./example:/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The example folder &lt;code&gt;./example&lt;/code&gt; will be available as &lt;code&gt; \\host.lan\Data&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] You can map this path to a drive letter in Windows, for easier access.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;How do I change the amount of CPU or RAM?&lt;/h3&gt; 
&lt;p&gt;By default, the container will be allowed to use a maximum of 2 CPU cores and 4 GB of RAM.&lt;/p&gt; 
&lt;p&gt;If you want to adjust this, you can specify the desired amount using the following environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  RAM_SIZE: "8G"
  CPU_CORES: "4"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I configure the username and password?&lt;/h3&gt; 
&lt;p&gt;By default, a user called &lt;code&gt;Docker&lt;/code&gt; is created during installation and its password is &lt;code&gt;admin&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to use different credentials, you can configure them in your compose file (only before installation):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  USERNAME: "bill"
  PASSWORD: "gates"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I select the Windows language?&lt;/h3&gt; 
&lt;p&gt;By default, the English version of Windows will be downloaded.&lt;/p&gt; 
&lt;p&gt;But before installation you can add the &lt;code&gt;LANGUAGE&lt;/code&gt; environment variable to your compose file, in order to specify an alternative language:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  LANGUAGE: "French"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can choose between: 🇦🇪 Arabic, 🇧🇬 Bulgarian, 🇨🇳 Chinese, 🇭🇷 Croatian, 🇨🇿 Czech, 🇩🇰 Danish, 🇳🇱 Dutch, 🇬🇧 English, 🇪🇪 Estonian, 🇫🇮 Finnish, 🇫🇷 French, 🇩🇪 German, 🇬🇷 Greek, 🇮🇱 Hebrew, 🇭🇺 Hungarian, 🇮🇹 Italian, 🇯🇵 Japanese, 🇰🇷 Korean, 🇱🇻 Latvian, 🇱🇹 Lithuanian, 🇳🇴 Norwegian, 🇵🇱 Polish, 🇵🇹 Portuguese, 🇷🇴 Romanian, 🇷🇺 Russian, 🇷🇸 Serbian, 🇸🇰 Slovak, 🇸🇮 Slovenian, 🇪🇸 Spanish, 🇸🇪 Swedish, 🇹🇭 Thai, 🇹🇷 Turkish and 🇺🇦 Ukrainian.&lt;/p&gt; 
&lt;h3&gt;How do I select the keyboard layout?&lt;/h3&gt; 
&lt;p&gt;If you want to use a keyboard layout or locale that is not the default for your selected language, you can add &lt;code&gt;KEYBOARD&lt;/code&gt; and &lt;code&gt;REGION&lt;/code&gt; variables like this (before installation):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  REGION: "en-US"
  KEYBOARD: "en-US"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I select the edition?&lt;/h3&gt; 
&lt;p&gt;Windows Server offers a minimalistic Core edition without a GUI. To select those non-standard editions, you can add a &lt;code&gt;EDITION&lt;/code&gt; variable like this (before installation):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  EDITION: "core"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I install a custom image?&lt;/h3&gt; 
&lt;p&gt;In order to download an unsupported ISO image, specify its URL in the &lt;code&gt;VERSION&lt;/code&gt; environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  VERSION: "https://example.com/win.iso"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can also skip the download and use a local file instead, by binding it in your compose file in this way:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;volumes:
  - ./example.iso:/boot.iso
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace the example path &lt;code&gt;./example.iso&lt;/code&gt; with the filename of your desired ISO file. The value of &lt;code&gt;VERSION&lt;/code&gt; will be ignored in this case.&lt;/p&gt; 
&lt;h3&gt;How do I run a script after installation?&lt;/h3&gt; 
&lt;p&gt;To run your own script after installation, you can create a file called &lt;code&gt;install.bat&lt;/code&gt; and place it in a folder together with any additional files it needs (software to be installed for example).&lt;/p&gt; 
&lt;p&gt;Then bind that folder in your compose file like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;volumes:
  -  ./example:/oem
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The example folder &lt;code&gt;./example&lt;/code&gt; will be copied to &lt;code&gt;C:\OEM&lt;/code&gt; and the containing &lt;code&gt;install.bat&lt;/code&gt; will be executed during the last step of the automatic installation.&lt;/p&gt; 
&lt;h3&gt;How do I perform a manual installation?&lt;/h3&gt; 
&lt;p&gt;It's recommended to stick to the automatic installation, as it adjusts various settings to prevent common issues when running Windows inside a virtual environment.&lt;/p&gt; 
&lt;p&gt;However, if you insist on performing the installation manually at your own risk, add the following environment variable to your compose file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  MANUAL: "Y"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I connect using RDP?&lt;/h3&gt; 
&lt;p&gt;The web-viewer is mainly meant to be used during installation, as its picture quality is low, and it has no audio or clipboard for example.&lt;/p&gt; 
&lt;p&gt;So for a better experience you can connect using any Microsoft Remote Desktop client to the IP of the container, using the username &lt;code&gt;Docker&lt;/code&gt; and password &lt;code&gt;admin&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;There is a RDP client for &lt;a href="https://play.google.com/store/apps/details?id=com.microsoft.rdc.androidx"&gt;Android&lt;/a&gt; available from the Play Store and one for &lt;a href="https://apps.apple.com/nl/app/microsoft-remote-desktop/id714464092?l=en-GB"&gt;iOS&lt;/a&gt; in the Apple Store. For Linux you can use &lt;a href="https://www.freerdp.com/"&gt;FreeRDP&lt;/a&gt; and on Windows just type &lt;code&gt;mstsc&lt;/code&gt; in the search box.&lt;/p&gt; 
&lt;h3&gt;How do I assign an individual IP address to the container?&lt;/h3&gt; 
&lt;p&gt;By default, the container uses bridge networking, which shares the IP address with the host.&lt;/p&gt; 
&lt;p&gt;If you want to assign an individual IP address to the container, you can create a macvlan network as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker network create -d macvlan \
    --subnet=192.168.0.0/24 \
    --gateway=192.168.0.1 \
    --ip-range=192.168.0.100/28 \
    -o parent=eth0 vlan
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Be sure to modify these values to match your local subnet.&lt;/p&gt; 
&lt;p&gt;Once you have created the network, change your compose file to look as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  windows:
    container_name: windows
    ..&amp;lt;snip&amp;gt;..
    networks:
      vlan:
        ipv4_address: 192.168.0.100

networks:
  vlan:
    external: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An added benefit of this approach is that you won't have to perform any port mapping anymore, since all ports will be exposed by default.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; This IP address won't be accessible from the Docker host due to the design of macvlan, which doesn't permit communication between the two. If this is a concern, you need to create a &lt;a href="https://blog.oddbit.com/post/2018-03-12-using-docker-macvlan-networks/#host-access"&gt;second macvlan&lt;/a&gt; as a workaround.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;How can Windows acquire an IP address from my router?&lt;/h3&gt; 
&lt;p&gt;After configuring the container for &lt;a href="https://raw.githubusercontent.com/dockur/windows/master/#how-do-i-assign-an-individual-ip-address-to-the-container"&gt;macvlan&lt;/a&gt;, it is possible for Windows to become part of your home network by requesting an IP from your router, just like a real PC.&lt;/p&gt; 
&lt;p&gt;To enable this mode, in which the container and Windows will have separate IP addresses, add the following lines to your compose file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  DHCP: "Y"
devices:
  - /dev/vhost-net
device_cgroup_rules:
  - 'c *:* rwm'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I add multiple disks?&lt;/h3&gt; 
&lt;p&gt;To create additional disks, modify your compose file like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  DISK2_SIZE: "32G"
  DISK3_SIZE: "64G"
volumes:
  - ./example2:/storage2
  - ./example3:/storage3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I pass-through a disk?&lt;/h3&gt; 
&lt;p&gt;It is possible to pass-through disk devices or partitions directly by adding them to your compose file in this way:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;devices:
  - /dev/sdb:/disk1
  - /dev/sdc1:/disk2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;/disk1&lt;/code&gt; if you want it to become your main drive (which will be formatted during installation), and use &lt;code&gt;/disk2&lt;/code&gt; and higher to add them as secondary drives (which will stay untouched).&lt;/p&gt; 
&lt;h3&gt;How do I pass-through a USB device?&lt;/h3&gt; 
&lt;p&gt;To pass-through a USB device, first lookup its vendor and product id via the &lt;code&gt;lsusb&lt;/code&gt; command, then add them to your compose file like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;environment:
  ARGUMENTS: "-device usb-host,vendorid=0x1234,productid=0x1234"
devices:
  - /dev/bus/usb
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the device is a USB disk drive, please wait until after the installation is fully completed before connecting it. Otherwise the installation may fail, as the order of the disks can get rearranged.&lt;/p&gt; 
&lt;h3&gt;How do I verify if my system supports KVM?&lt;/h3&gt; 
&lt;p&gt;First check if your software is compatible using this chart:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Product&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Linux&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Win11&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Win10&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docker CLI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docker Desktop&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Podman CLI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Podman Desktop&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;After that you can run the following commands in Linux to check your system:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install cpu-checker
sudo kvm-ok
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you receive an error from &lt;code&gt;kvm-ok&lt;/code&gt; indicating that KVM cannot be used, please check whether:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;the virtualization extensions (&lt;code&gt;Intel VT-x&lt;/code&gt; or &lt;code&gt;AMD SVM&lt;/code&gt;) are enabled in your BIOS.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;you enabled "nested virtualization" if you are running the container inside a virtual machine.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;you are not using a cloud provider, as most of them do not allow nested virtualization for their VPS's.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you did not receive any error from &lt;code&gt;kvm-ok&lt;/code&gt; but the container still complains about a missing KVM device, it could help to add &lt;code&gt;privileged: true&lt;/code&gt; to your compose file (or &lt;code&gt;sudo&lt;/code&gt; to your &lt;code&gt;docker&lt;/code&gt; command) to rule out any permission issue.&lt;/p&gt; 
&lt;h3&gt;How do I run macOS in a container?&lt;/h3&gt; 
&lt;p&gt;You can use &lt;a href="https://github.com/dockur/macos"&gt;dockur/macos&lt;/a&gt; for that. It shares many of the same features, except for the automatic installation.&lt;/p&gt; 
&lt;h3&gt;How do I run a Linux desktop in a container?&lt;/h3&gt; 
&lt;p&gt;You can use &lt;a href="https://github.com/qemus/qemu"&gt;qemus/qemu&lt;/a&gt; in that case.&lt;/p&gt; 
&lt;h3&gt;Is this project legal?&lt;/h3&gt; 
&lt;p&gt;Yes, this project contains only open-source code and does not distribute any copyrighted material. Any product keys found in the code are just generic placeholders provided by Microsoft for trial purposes. So under all applicable laws, this project will be considered legal.&lt;/p&gt; 
&lt;h2&gt;Stars 🌟&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/dockur/windows"&gt;&lt;img src="https://starchart.cc/dockur/windows.svg?variant=adaptive" alt="Stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer ⚖️&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;The product names, logos, brands, and other trademarks referred to within this project are the property of their respective trademark holders. This project is not affiliated, sponsored, or endorsed by Microsoft Corporation.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>bevyengine/bevy</title>
      <link>https://github.com/bevyengine/bevy</link>
      <description>&lt;p&gt;A refreshingly simple data-driven game engine built in Rust&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href="https://bevy.org"&gt;&lt;img src="https://raw.githubusercontent.com/bevyengine/bevy/main/assets/branding/bevy_logo_light_dark_and_dimmed.svg?sanitize=true" alt="Bevy" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/bevyengine/bevy#license"&gt;&lt;img src="https://img.shields.io/badge/license-MIT%2FApache-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/bevy"&gt;&lt;img src="https://img.shields.io/crates/v/bevy.svg?sanitize=true" alt="Crates.io" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/bevy"&gt;&lt;img src="https://img.shields.io/crates/d/bevy.svg?sanitize=true" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://docs.rs/bevy/latest/bevy/"&gt;&lt;img src="https://docs.rs/bevy/badge.svg?sanitize=true" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://github.com/bevyengine/bevy/actions"&gt;&lt;img src="https://github.com/bevyengine/bevy/workflows/CI/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/bevy"&gt;&lt;img src="https://img.shields.io/discord/691052431525675048.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Bevy?&lt;/h2&gt; 
&lt;p&gt;Bevy is a refreshingly simple data-driven game engine built in Rust. It is free and open-source forever!&lt;/p&gt; 
&lt;h2&gt;WARNING&lt;/h2&gt; 
&lt;p&gt;Bevy is still in the early stages of development. Important features are missing. Documentation is sparse. A new version of Bevy containing breaking changes to the API is released &lt;a href="https://bevy.org/news/bevy-0-6/#the-train-release-schedule"&gt;approximately once every 3 months&lt;/a&gt;. We provide &lt;a href="https://bevy.org/learn/migration-guides/"&gt;migration guides&lt;/a&gt;, but we can't guarantee migrations will always be easy. Use only if you are willing to work in this environment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;MSRV:&lt;/strong&gt; Bevy relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally close to "the latest stable release" of Rust.&lt;/p&gt; 
&lt;h2&gt;Design Goals&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Capable&lt;/strong&gt;: Offer a complete 2D and 3D feature set&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: Easy for newbies to pick up, but infinitely flexible for power users&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Focused&lt;/strong&gt;: Data-oriented architecture using the Entity Component System paradigm&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular&lt;/strong&gt;: Use only what you need. Replace what you don't like&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: App logic should run quickly, and when possible, in parallel&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Productive&lt;/strong&gt;: Changes should compile quickly ... waiting isn't fun&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bevy.org"&gt;Features&lt;/a&gt;:&lt;/strong&gt; A quick overview of Bevy's features.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bevy.org/news/"&gt;News&lt;/a&gt;&lt;/strong&gt;: A development blog that covers our progress, plans and shiny new features.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Docs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bevy.org/learn/quick-start/introduction"&gt;Quick Start Guide&lt;/a&gt;:&lt;/strong&gt; Bevy's official Quick Start Guide. The best place to start learning Bevy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.rs/bevy"&gt;Bevy Rust API Docs&lt;/a&gt;:&lt;/strong&gt; Bevy's Rust API docs, which are automatically generated from the doc comments in this repo.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/bevyengine/bevy/tree/latest/examples"&gt;Official Examples&lt;/a&gt;:&lt;/strong&gt; Bevy's dedicated, runnable examples, which are great for digging into specific concepts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bevy.org/assets/#learning"&gt;Community-Made Learning Resources&lt;/a&gt;&lt;/strong&gt;: More tutorials, documentation, and examples made by the Bevy community.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Before contributing or participating in discussions with the community, you should familiarize yourself with our &lt;a href="https://raw.githubusercontent.com/bevyengine/bevy/main/CODE_OF_CONDUCT.md"&gt;&lt;strong&gt;Code of Conduct&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://discord.gg/bevy"&gt;Discord&lt;/a&gt;:&lt;/strong&gt; Bevy's official discord server.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://reddit.com/r/bevy"&gt;Reddit&lt;/a&gt;:&lt;/strong&gt; Bevy's official subreddit.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/bevyengine/bevy/discussions"&gt;GitHub Discussions&lt;/a&gt;:&lt;/strong&gt; The best place for questions about Bevy, answered right here!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://bevy.org/assets/"&gt;Bevy Assets&lt;/a&gt;:&lt;/strong&gt; A collection of awesome Bevy projects, tools, plugins and learning materials.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;If you'd like to help build Bevy, check out the &lt;strong&gt;&lt;a href="https://bevy.org/learn/contribute/introduction"&gt;Contributor's Guide&lt;/a&gt;&lt;/strong&gt;. For simple problems, feel free to &lt;a href="https://github.com/bevyengine/bevy/issues"&gt;open an issue&lt;/a&gt; or &lt;a href="https://github.com/bevyengine/bevy/pulls"&gt;PR&lt;/a&gt; and tackle it yourself!&lt;/p&gt; 
&lt;p&gt;For more complex architecture decisions and experimental mad science, please open an &lt;a href="https://github.com/bevyengine/rfcs"&gt;RFC&lt;/a&gt; (Request For Comments) so we can brainstorm together effectively!&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;We recommend checking out the &lt;a href="https://bevy.org/learn/quick-start/introduction"&gt;Quick Start Guide&lt;/a&gt; for a brief introduction.&lt;/p&gt; 
&lt;p&gt;Follow the &lt;a href="https://bevy.org/learn/quick-start/getting-started/setup"&gt;Setup guide&lt;/a&gt; to ensure your development environment is set up correctly. Once set up, you can quickly try out the &lt;a href="https://github.com/bevyengine/bevy/tree/latest/examples"&gt;examples&lt;/a&gt; by cloning this repo and running the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Switch to the correct version (latest release, default is main development branch)
git checkout latest
# Runs the "breakout" example
cargo run --example breakout
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To draw a window with standard functionality enabled, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use bevy::prelude::*;

fn main() {
    App::new()
        .add_plugins(DefaultPlugins)
        .run();
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fast Compiles&lt;/h3&gt; 
&lt;p&gt;Bevy can be built just fine using default configuration on stable Rust. However for really fast iterative compiles, you should enable the "fast compiles" setup by &lt;a href="https://bevy.org/learn/quick-start/getting-started/setup"&gt;following the instructions here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/bevyengine/bevy/main/docs/cargo_features.md"&gt;Bevy Cargo Features&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;This &lt;a href="https://raw.githubusercontent.com/bevyengine/bevy/main/docs/cargo_features.md"&gt;list&lt;/a&gt; outlines the different cargo features supported by Bevy. These allow you to customize the Bevy feature set for your use-case.&lt;/p&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;p&gt;Bevy is the result of the hard work of many people. A huge thanks to all Bevy contributors, the many open source projects that have come before us, the &lt;a href="https://arewegameyet.rs/"&gt;Rust gamedev ecosystem&lt;/a&gt;, and the many libraries we build on.&lt;/p&gt; 
&lt;p&gt;A huge thanks to Bevy's &lt;a href="https://bevy.org"&gt;generous sponsors&lt;/a&gt;. Bevy will always be free and open source, but it isn't free to make. Please consider &lt;a href="https://bevy.org/donate/"&gt;sponsoring our work&lt;/a&gt; if you like what we're building.&lt;/p&gt; 
&lt;!-- This next line need to stay exactly as is. It is required for BrowserStack sponsorship. --&gt; 
&lt;p&gt;This project is tested with BrowserStack.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Bevy is free, open source and permissively licensed! Except where noted (below and/or in individual files), all code in this repository is dual-licensed under either:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MIT License (&lt;a href="https://raw.githubusercontent.com/bevyengine/bevy/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; or &lt;a href="http://opensource.org/licenses/MIT"&gt;http://opensource.org/licenses/MIT&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Apache License, Version 2.0 (&lt;a href="https://raw.githubusercontent.com/bevyengine/bevy/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href="http://www.apache.org/licenses/LICENSE-2.0"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;at your option. This means you can select the license you prefer! This dual-licensing approach is the de-facto standard in the Rust ecosystem and there are &lt;a href="https://github.com/bevyengine/bevy/issues/2373"&gt;very good reasons&lt;/a&gt; to include both.&lt;/p&gt; 
&lt;p&gt;Some of the engine's code carries additional copyright notices and license terms due to their external origins. These are generally BSD-like, but exact details vary by crate: If the README of a crate contains a 'License' header (or similar), the additional copyright notices and license terms applicable to that crate will be listed. The above licensing requirement still applies to contributions to those crates, and sections of those crates will carry those license terms. The &lt;a href="https://doc.rust-lang.org/cargo/reference/manifest.html#the-license-and-license-file-fields"&gt;license&lt;/a&gt; field of each crate will also reflect this.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/bevyengine/bevy/main/assets"&gt;assets&lt;/a&gt; included in this repository (for our &lt;a href="https://raw.githubusercontent.com/bevyengine/bevy/main/examples/README.md"&gt;examples&lt;/a&gt;) typically fall under different open licenses. These will not be included in your game (unless copied in by you), and they are not distributed in the published bevy crates. See &lt;a href="https://raw.githubusercontent.com/bevyengine/bevy/main/CREDITS.md"&gt;CREDITS.md&lt;/a&gt; for the details of the licenses of those files.&lt;/p&gt; 
&lt;h3&gt;Your contributions&lt;/h3&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zakirullin/cognitive-load</title>
      <link>https://github.com/zakirullin/cognitive-load</link>
      <description>&lt;p&gt;🧠 Cognitive Load is what matters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cognitive Load is what matters&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://minds.md/zakirullin/cognitive"&gt;Readable version&lt;/a&gt; | &lt;a href="https://github.com/zakirullin/cognitive-load/raw/main/README.zh-cn.md"&gt;Chinese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/README.ko.md"&gt;Korean&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/README.tr.md"&gt;Turkish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/README.ja.md"&gt;Japanese&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;It is a living document, last update: &lt;strong&gt;September 2025&lt;/strong&gt;. Your contributions are welcome!&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;There are so many buzzwords and best practices out there, but most of them have failed. We need something more fundamental, something that can't be wrong.&lt;/p&gt; 
&lt;p&gt;Sometimes we feel confusion going through the code. Confusion costs time and money. Confusion is caused by high &lt;em&gt;cognitive load&lt;/em&gt;. It's not some fancy abstract concept, but rather &lt;strong&gt;a fundamental human constraint&lt;/strong&gt;. It's not imagined, it's there and we can feel it.&lt;/p&gt; 
&lt;p&gt;Since we spend far more time reading and understanding code than writing it, we should constantly ask ourselves whether we are embedding excessive cognitive load into our code.&lt;/p&gt; 
&lt;h2&gt;Cognitive load&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Cognitive load is how much a developer needs to think in order to complete a task.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;When reading code, you put things like values of variables, control flow logic and call sequences into your head. The average person can hold roughly &lt;a href="https://github.com/zakirullin/cognitive-load/issues/16"&gt;four such chunks&lt;/a&gt; in working memory. Once the cognitive load reaches this threshold, it becomes much harder to understand things.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Let's say we have been asked to make some fixes to a completely unfamiliar project. We were told that a really smart developer had contributed to it. Lots of cool architectures, fancy libraries and trendy technologies were used. In other words, &lt;strong&gt;the author had created a high cognitive load for us.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/img/cognitiveloadv6.png" alt="Cognitive load" width="750" /&gt; 
&lt;/div&gt; 
&lt;p&gt;We should reduce the cognitive load in our projects as much as possible.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Cognitive load and interruptions&lt;/b&gt;&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/img/interruption.jpeg" /&gt;
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We are going to use "cognitive load" in an informal sense; sometimes it lines up with the specific scientific concept of Cognitive Load, but we don't know enough about where it does and doesn't match.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Types of cognitive load&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Intrinsic&lt;/strong&gt; - caused by the inherent difficulty of a task. It can't be reduced, it's at the very heart of software development.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Extraneous&lt;/strong&gt; - created by the way the information is presented. Caused by factors not directly relevant to the task, such as smart author's quirks. Can be greatly reduced. We will focus on this type of cognitive load.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/img/smartauthorv14thanksmari.png" alt="Intrinsic vs Extraneous" width="600" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Let's jump straight to the concrete practical examples of extraneous cognitive load.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;We will refer to the level of cognitive load as follows:&lt;br /&gt; &lt;code&gt;🧠&lt;/code&gt;: fresh working memory, zero cognitive load&lt;br /&gt; &lt;code&gt;🧠++&lt;/code&gt;: two facts in our working memory, cognitive load increased&lt;br /&gt; &lt;code&gt;🤯&lt;/code&gt;: cognitive overload, more than 4 facts&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Our brain is much more complex and unexplored, but we can go with this simplistic model.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Complex conditionals&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;if val &amp;gt; someConstant // 🧠+
    &amp;amp;&amp;amp; (condition2 || condition3) // 🧠+++, prev cond should be true, one of c2 or c3 has be true
    &amp;amp;&amp;amp; (condition4 &amp;amp;&amp;amp; !condition5) { // 🤯, we are messed up by this point
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Introduce intermediate variables with meaningful names:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;isValid = val &amp;gt; someConstant
isAllowed = condition2 || condition3
isSecure = condition4 &amp;amp;&amp;amp; !condition5 
// 🧠, we don't need to remember the conditions, there are descriptive variables
if isValid &amp;amp;&amp;amp; isAllowed &amp;amp;&amp;amp; isSecure {
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Nested ifs&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;if isValid { // 🧠+, okay nested code applies to valid input only
    if isSecure { // 🧠++, we do stuff for valid and secure input only
        stuff // 🧠+++
    }
} 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compare it with the early returns:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;if !isValid
    return
 
if !isSecure
    return

// 🧠, we don't really care about earlier returns, if we are here then all good

stuff // 🧠+
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We can focus on the happy path only, thus freeing our working memory from all sorts of preconditions.&lt;/p&gt; 
&lt;h2&gt;Inheritance nightmare&lt;/h2&gt; 
&lt;p&gt;We are asked to change a few things for our admin users: &lt;code&gt;🧠&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;AdminController extends UserController extends GuestController extends BaseController&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Ohh, part of the functionality is in &lt;code&gt;BaseController&lt;/code&gt;, let's have a look: &lt;code&gt;🧠+&lt;/code&gt;&lt;br /&gt; Basic role mechanics got introduced in &lt;code&gt;GuestController&lt;/code&gt;: &lt;code&gt;🧠++&lt;/code&gt;&lt;br /&gt; Things got partially altered in &lt;code&gt;UserController&lt;/code&gt;: &lt;code&gt;🧠+++&lt;/code&gt;&lt;br /&gt; Finally we are here, &lt;code&gt;AdminController&lt;/code&gt;, let's code stuff! &lt;code&gt;🧠++++&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Oh, wait, there's &lt;code&gt;SuperuserController&lt;/code&gt; which extends &lt;code&gt;AdminController&lt;/code&gt;. By modifying &lt;code&gt;AdminController&lt;/code&gt; we can break things in the inherited class, so let's dive in &lt;code&gt;SuperuserController&lt;/code&gt; first: &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Prefer composition over inheritance. We won't go into detail - there's &lt;a href="https://www.youtube.com/watch?v=hxGOiiR9ZKg"&gt;plenty of material&lt;/a&gt; out there.&lt;/p&gt; 
&lt;h2&gt;Too many small methods, classes or modules&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Method, class and module are interchangeable in this context&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Mantras like "methods should be shorter than 15 lines of code" or "classes should be small" turned out to be somewhat wrong.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deep module&lt;/strong&gt; - simple interface, complex functionality&lt;br /&gt; &lt;strong&gt;Shallow module&lt;/strong&gt; - interface is relatively complex to the small functionality it provides&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/img/deepmodulev8.png" alt="Deep module" width="700" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Having too many shallow modules can make it difficult to understand the project. &lt;strong&gt;Not only do we have to keep in mind each module responsibilities, but also all their interactions&lt;/strong&gt;. To understand the purpose of a shallow module, we first need to look at the functionality of all the related modules. Jumping between such shallow components is mentally exhausting, &lt;a target="_blank" href="https://blog.separateconcerns.com/2023-09-11-linear-code.html"&gt;linear thinking&lt;/a&gt; is more natural to us humans.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Information hiding is paramount, and we don't hide as much complexity in shallow modules.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;I have two pet projects, both of them are somewhat 5K lines of code. The first one has 80 shallow classes, whereas the second one has only 7 deep classes. I haven't been maintaining any of these projects for one year and a half.&lt;/p&gt; 
&lt;p&gt;Once I came back, I realised that it was extremely difficult to untangle all the interactions between those 80 classes in the first project. I would have to rebuild an enormous amount of cognitive load before I could start coding. On the other hand, I was able to grasp the second project quickly, because it had only a few deep classes with a simple interface.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The best components are those that provide powerful functionality yet have a simple interface.&lt;br /&gt; &lt;strong&gt;John K. Ousterhout&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The interface of the UNIX I/O is very simple. It has only five basic calls:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;open(path, flags, permissions)
read(fd, buffer, count)
write(fd, buffer, count)
lseek(fd, offset, referencePosition)
close(fd)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A modern implementation of this interface has &lt;strong&gt;hundreds of thousands of lines of code&lt;/strong&gt;. Lots of complexity is hidden under the hood. Yet it is easy to use due to its simple interface.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This deep module example is taken from the book &lt;a href="https://web.stanford.edu/~ouster/cgi-bin/book.php"&gt;A Philosophy of Software Design&lt;/a&gt; by John K. Ousterhout. Not only does this book cover the very essence of complexity in software development, but it also has the greatest interpretation of Parnas' influential paper &lt;a href="https://www.win.tue.nl/~wstomv/edu/2ip30/references/criteria_for_modularization.pdf"&gt;On the Criteria To Be Used in Decomposing Systems into Modules&lt;/a&gt;. Both are essential reads. Other related readings: &lt;a href="https://github.com/johnousterhout/aposd-vs-clean-code"&gt;A Philosophy of Software Design vs Clean Code&lt;/a&gt;, &lt;a href="https://qntm.org/clean"&gt;It's probably time to stop recommending Clean Code&lt;/a&gt;, &lt;a href="https://copyconstruct.medium.com/small-functions-considered-harmful-91035d316c29"&gt;Small Functions considered Harmful&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;P.S. If you think we are rooting for bloated God objects with too many responsibilities, you got it wrong.&lt;/p&gt; 
&lt;h2&gt;Responsible for one thing&lt;/h2&gt; 
&lt;p&gt;All too often, we end up creating lots of shallow modules, following some vague "a module should be responsible for one, and only one, thing" principle. What is this blurry one thing? Instantiating an object is one thing, right? So &lt;a href="https://minds.md/benji/frameworks"&gt;MetricsProviderFactoryFactory&lt;/a&gt; seems to be just fine. &lt;strong&gt;The names and interfaces of such classes tend to be more mentally taxing than their entire implementations, what kind of abstraction is that?&lt;/strong&gt; Something went wrong.&lt;/p&gt; 
&lt;p&gt;We make changes to our systems to satisfy our users and stakeholders. We are responsible to them.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A module should be responsible to one, and only one, user or stakeholder.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;This is what this Single Responsibility Principle is all about. Simply put, if we introduce a bug in one place, and then two different business people come to complain, we've violated the principle. It has nothing to do with the number of things we do in our module.&lt;/p&gt; 
&lt;p&gt;But even now, this rule can do more harm than good. This principle can be understood in as many different ways as there are individuals. A better approach would be to look at how much cognitive load it all creates. It's mentally demanding to remember that change in one place can trigger a chain of reactions across different business streams. And that's about it, no fancy terms to learn.&lt;/p&gt; 
&lt;h2&gt;Too many shallow microservices&lt;/h2&gt; 
&lt;p&gt;This shallow-deep module principle is scale-agnostic, and we can apply it to microservices architecture. Too many shallow microservices won't do any good - the industry is heading towards somewhat "macroservices", i.e., services that are not so shallow (=deep). One of the worst and hardest to fix phenomena is so-called distributed monolith, which is often the result of this overly granular shallow separation.&lt;/p&gt; 
&lt;p&gt;I once consulted a startup where a team of five developers introduced 17(!) microservices. They were 10 months behind schedule and appeared nowhere close to the public release. Every new requirement led to changes in 4+ microservices. It took an enormous amount of time to reproduce and debug an issue in such a distributed system. Both time to market and cognitive load were unacceptably high. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Is this the right way to approach the uncertainty of a new system? It's enormously difficult to elicit the right logical boundaries in the beginning. The key is to make decisions as late as you can responsibly wait, because that is when you have the most information at hand. By introducing a network layer up front, we make our design decisions hard to revert right from the start. The team's only justification was: "The FAANG companies proved microservices architecture to be effective". &lt;em&gt;Hello, you got to stop dreaming big.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Tanenbaum%E2%80%93Torvalds_debate"&gt;Tanenbaum-Torvalds debate&lt;/a&gt; argued that Linux's monolithic design was flawed and obsolete, and that a microkernel architecture should be used instead. Indeed, the microkernel design seemed to be superior "from a theoretical and aesthetical" point of view. On the practical side of things - three decades on, microkernel-based GNU Hurd is still in development, and monolithic Linux is everywhere. This page is powered by Linux, your smart teapot is powered by Linux. By monolithic Linux.&lt;/p&gt; 
&lt;p&gt;A well-crafted monolith with truly isolated modules is often much more flexible than a bunch of microservices. It also requires far less cognitive effort to maintain. It's only when the need for separate deployments becomes crucial, such as scaling the development team, that you should consider adding a network layer between the modules, future microservices.&lt;/p&gt; 
&lt;h2&gt;Feature-rich languages&lt;/h2&gt; 
&lt;p&gt;We feel excited when new features got released in our favourite language. We spend some time learning these features, we build code upon them.&lt;/p&gt; 
&lt;p&gt;If there are lots of features, we may spend half an hour playing with a few lines of code, to use one or another feature. And it's kind of a waste of time. But what's worse, &lt;strong&gt;when you come back later, you would have to recreate that thought process!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;You not only have to understand this complicated program, you have to understand why a programmer decided this was the way to approach a problem from the features that are available.&lt;/strong&gt; &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;These statements are made by none other than Rob Pike.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Reduce cognitive load by limiting the number of choices.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Language features are OK, as long as they are orthogonal to each other.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Thoughts from an engineer with 20 years of C++ experience ⭐️&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; I was looking at my RSS reader the other day and noticed that I have somewhat three hundred unread articles under the "C++" tag. I haven't read a single article about the language since last summer, and I feel great!
 &lt;br /&gt;
 &lt;br /&gt; I've been using C++ for 20 years for now, that's almost two-thirds of my life. Most of my experience lies in dealing with the darkest corners of the language (such as undefined behaviours of all sorts). It's not a reusable experience, and it's kind of creepy to throw it all away now.
 &lt;br /&gt;
 &lt;br /&gt; Like, can you imagine, the token 
 &lt;code&gt;||&lt;/code&gt; has a different meaning in 
 &lt;code&gt;requires ((!P&amp;lt;T&amp;gt; || !Q&amp;lt;T&amp;gt;))&lt;/code&gt; and in 
 &lt;code&gt;requires (!(P&amp;lt;T&amp;gt; || Q&amp;lt;T&amp;gt;))&lt;/code&gt;. The first is the constraint disjunction, the second is the good-old logical OR operator, and they behave differently.
 &lt;br /&gt;
 &lt;br /&gt; You can't allocate space for a trivial type and just 
 &lt;code&gt;memcpy&lt;/code&gt; a set of bytes there without extra effort - that won't start the lifetime of an object. This was the case before C++20. It was fixed in C++20, but the cognitive load of the language has only increased.
 &lt;br /&gt;
 &lt;br /&gt; Cognitive load is constantly growing, even though things got fixed. I should know what was fixed, when it was fixed, and what it was like before. I am a professional after all. Sure, C++ is good at legacy support, which also means that you 
 &lt;b&gt;will face&lt;/b&gt; that legacy. For example, last month a colleague of mine asked me about some behaviour in C++03. 
 &lt;code&gt;🤯&lt;/code&gt;
 &lt;br /&gt;
 &lt;br /&gt; There were 20 ways of initialization. Uniform initialization syntax has been added. Now we have 21 ways of initialization. By the way, does anyone remember the rules for selecting constructors from the initializer list? Something about implicit conversion with the least loss of information, 
 &lt;i&gt;but if&lt;/i&gt; the value is known statically, then... 
 &lt;code&gt;🤯&lt;/code&gt;
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;b&gt;This increased cognitive load is not caused by a business task at hand. It is not an intrinsic complexity of the domain. It is just there due to historical reasons&lt;/b&gt; (
 &lt;i&gt;extraneous cognitive load&lt;/i&gt;).
 &lt;br /&gt;
 &lt;br /&gt; I had to come up with some rules. Like, if that line of code is not as obvious and I have to remember the standard, I better not write it that way. The standard is somewhat 1500 pages long, by the way.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;b&gt;By no means I am trying to blame C++.&lt;/b&gt; I love the language. It's just that I am tired now.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;Thanks to &lt;a href="https://0xd34df00d.me" target="_blank"&gt;0xd34df00d&lt;/a&gt; for writing.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Business logic and HTTP status codes&lt;/h2&gt; 
&lt;p&gt;On the backend we return:&lt;br /&gt; &lt;code&gt;401&lt;/code&gt; for expired jwt token&lt;br /&gt; &lt;code&gt;403&lt;/code&gt; for not enough access&lt;br /&gt; &lt;code&gt;418&lt;/code&gt; for banned users&lt;/p&gt; 
&lt;p&gt;The engineers on the frontend use backend API to implement login functionality. They would have to temporarily create the following cognitive load in their brains:&lt;br /&gt; &lt;code&gt;401&lt;/code&gt; is for expired jwt token // &lt;code&gt;🧠+&lt;/code&gt;, ok just temporary remember it&lt;br /&gt; &lt;code&gt;403&lt;/code&gt; is for not enough access // &lt;code&gt;🧠++&lt;/code&gt;&lt;br /&gt; &lt;code&gt;418&lt;/code&gt; is for banned users // &lt;code&gt;🧠+++&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Frontend developers would (hopefully) introduce some kind &lt;code&gt;numeric status -&amp;gt; meaning&lt;/code&gt; dictionary on their side, so that subsequent generations of contributors wouldn't have to recreate this mapping in their brains.&lt;/p&gt; 
&lt;p&gt;Then QA engineers come into play: "Hey, I got &lt;code&gt;403&lt;/code&gt; status, is that expired token or not enough access?" &lt;strong&gt;QA engineers can't jump straight to testing, because first they have to recreate the cognitive load that the engineers on the backend once created.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Why hold this custom mapping in our working memory? It's better to abstract away your business details from the HTTP transfer protocol, and return self-descriptive codes directly in the response body:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "code": "jwt_has_expired"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Cognitive load on the frontend side: &lt;code&gt;🧠&lt;/code&gt; (fresh, no facts are held in mind)&lt;br /&gt; Cognitive load on the QA side: &lt;code&gt;🧠&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;The same rule applies to all sorts of numeric statuses (in the database or wherever) - &lt;strong&gt;prefer self-describing strings&lt;/strong&gt;. We are not in the era of 640K computers to optimise for memory.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;People spend time arguing between &lt;code&gt;401&lt;/code&gt; and &lt;code&gt;403&lt;/code&gt;, making decisions based on their own mental models. New developers are coming in, and they need to recreate that thought process. You may have documented the "whys" (ADRs) for your code, helping newcomers to understand the decisions made. But in the end it just doesn't make any sense. We can separate errors into either user-related or server-related, but apart from that, things are kind of blurry.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;P.S. It's often mentally taxing to distinguish between "authentication" and "authorization". We can use simpler terms like &lt;a href="https://ntietz.com/blog/lets-say-instead-of-auth/"&gt;"login" and "permissions"&lt;/a&gt; to reduce the cognitive load.&lt;/p&gt; 
&lt;h2&gt;Abusing DRY principle&lt;/h2&gt; 
&lt;p&gt;Do not repeat yourself - that is one of the first principles you are taught as a software engineer. It is so deeply embedded in ourselves that we can not stand the fact of a few extra lines of code. Although in general a good and fundamental rule, when overused it leads to the cognitive load we can not handle.&lt;/p&gt; 
&lt;p&gt;Nowadays, everyone builds software based on logically separated components. Often those are distributed among multiple codebases representing separate services. When you strive to eliminate any repetition, you might end up creating tight coupling between unrelated components. As a result changes in one part may have unintended consequences in other seemingly unrelated areas. It can also hinder the ability to replace or modify individual components without impacting the entire system. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;In fact, the same problem arises even within a single module. You might extract common functionality too early, based on perceived similarities that might not actually exist in the long run. This can result in unnecessary abstractions that are difficult to modify or extend.&lt;/p&gt; 
&lt;p&gt;Rob Pike once said:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A little copying is better than a little dependency.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We are tempted to not reinvent the wheel so strong that we are ready to import large, heavy libraries to use a small function that we could easily write by ourselves.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;All your dependencies are your code.&lt;/strong&gt; Going through 10+ levels of stack trace of some imported library and figuring out what went wrong (&lt;em&gt;because things go wrong&lt;/em&gt;) is painful.&lt;/p&gt; 
&lt;h2&gt;Tight coupling with a framework&lt;/h2&gt; 
&lt;p&gt;There's a lot of "magic" in frameworks. By relying too heavily on a framework, &lt;strong&gt;we force all upcoming developers to learn that "magic" first&lt;/strong&gt;. It can take months. Even though frameworks enable us to launch MVPs in a matter of days, in the long run they tend to add unnecessary complexity and cognitive load.&lt;/p&gt; 
&lt;p&gt;Worse yet, at some point frameworks can become a significant constraint when faced with a new requirement that just doesn't fit the architecture. From here onwards people end up forking a framework and maintaining their own custom version. Imagine the amount of cognitive load a newcomer would have to build (i.e. learn this custom framework) in order to deliver any value. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;By no means do we advocate to invent everything from scratch!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We can write code in a somewhat framework-agnostic way. The business logic should not reside within a framework; rather, it should use the framework's components. Put a framework outside of your core logic. Use the framework in a library-like fashion. This would allow new contributors to add value from day one, without the need of going through debris of framework-related complexity first.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href="https://minds.md/benji/frameworks"&gt;Why I Hate Frameworks&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Layered architecture&lt;/h2&gt; 
&lt;p&gt;There is a certain engineering excitement about all this stuff.&lt;/p&gt; 
&lt;p&gt;I myself was a passionate advocate of Hexagonal/Onion Architecture for years. I used it here and there and encouraged other teams to do so. The complexity of our projects went up, the sheer number of files alone had doubled. It felt like we were writing a lot of glue code. On ever changing requirements we had to make changes across multiple layers of abstractions, it all became tedious. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Abstraction is supposed to hide complexity, here it just adds &lt;a href="https://fhur.me/posts/2024/thats-not-an-abstraction"&gt;indirection&lt;/a&gt;. Jumping from call to call to read along and figure out what goes wrong and what is missing is a vital requirement to quickly solve a problem. With this architecture’s layer uncoupling it requires an exponential factor of extra, often disjointed, traces to get to the point where the failure occurs. Every such trace takes space in our limited working memory. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This architecture was something that made intuitive sense at first, but every time we tried applying it to projects it made a lot more harm than good. In the end, we gave it all up in favour of the good old dependency inversion principle. &lt;strong&gt;No port/adapter terms to learn, no unnecessary layers of horizontal abstractions, no extraneous cognitive load.&lt;/strong&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Coding principles and experience&lt;/b&gt;&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/img/complexity.png" /&gt;
 &lt;br /&gt; 
 &lt;a href="https://twitter.com/flaviocopes"&gt;@flaviocopes&lt;/a&gt; 
&lt;/details&gt; 
&lt;p&gt;If you think that such layering will allow you to quickly replace a database or other dependencies, you're mistaken. Changing the storage causes lots of problems, and believe us, having some abstractions for the data access layer is the least of your worries. At best, abstractions can save somewhat 10% of your migration time (if any), the real pain is in data model incompatibilities, communication protocols, distributed systems challenges, and &lt;a href="https://www.hyrumslaw.com"&gt;implicit interfaces&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;With a sufficient number of users of an API,&lt;br /&gt; it does not matter what you promise in the contract:&lt;br /&gt; all observable behaviors of your system&lt;br /&gt; will be depended on by somebody.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We did a storage migration, and that took us about 10 months. The old system was single-threaded, so the exposed events were sequential. All our systems depended on that observed behaviour. This behavior was not part of the API contract, it was not reflected in the code. A new distributed storage didn't have that guarantee - the events came out-of-order. We spent only a few hours coding a new storage adapter, thanks to an abstraction. &lt;strong&gt;We spent the next 10 months on dealing with out-of-order events and other challenges.&lt;/strong&gt; It's now funny to say that abstractions helps us replace components quickly.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;So, why pay the price of high cognitive load for such a layered architecture, if it doesn't pay off in the future?&lt;/strong&gt; Plus, in most cases, that future of replacing some core component never happens.&lt;/p&gt; 
&lt;p&gt;These architectures are not fundamental, they are just subjective, biased consequences of more fundamental principles. Why rely on those subjective interpretations? Follow the fundamental rules instead: dependency inversion principle, single source of truth, cognitive load and information hiding. Your business logic should not depend on low-level modules like database, UI or framework. We should be able to write tests for our core logic without worrying about the infrastructure, and that's it. &lt;a href="https://github.com/zakirullin/cognitive-load/discussions/24"&gt;Discuss&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Do not add layers of abstractions for the sake of an architecture. Add them whenever you need an extension point that is justified for practical reasons.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://blog.jooq.org/why-you-should-not-implement-layered-architecture"&gt;Layers of abstraction aren't free of charge&lt;/a&gt;, they are to be held in our limited working memory&lt;/strong&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/img/layers.png" alt="Layers" width="400" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Domain-driven design&lt;/h2&gt; 
&lt;p&gt;Domain-driven design has some great points, although it is often misinterpreted. People say, "We write code in DDD", which is a bit strange, because DDD is more about the problem space rather than the solution space.&lt;/p&gt; 
&lt;p&gt;Ubiquitous language, domain, bounded context, aggregate, event storming are all about problem space. They are meant to help us learn the insights about the domain and extract the boundaries. DDD enables developers, domain experts and business people to communicate effectively using a single, unified language. Rather than focusing on these problem space aspects of DDD, we tend to emphasise particular folder structures, services, repositories, and other solution space techniques.&lt;/p&gt; 
&lt;p&gt;Chances are that the way we interpret DDD is likely to be unique and subjective. And if we build code upon this understanding, i.e., if we create a lot of extraneous cognitive load - future developers are doomed. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Team Topologies provides a much better, easier to understand framework that helps us split the cognitive load across teams. Engineers tend to develop somewhat similar mental models after learning about Team Topologies. DDD, on the other hand, seems to be creating 10 different mental models for 10 different readers. Instead of being common ground, it becomes a battleground for unnecessary debates.&lt;/p&gt; 
&lt;h2&gt;Cognitive load in familiar projects&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The problem is that &lt;strong&gt;familiarity is not the same as simplicity&lt;/strong&gt;. They &lt;em&gt;feel&lt;/em&gt; the same — that same ease of moving through a space without much mental effort — but for very different reasons. Every “clever” (read: “self-indulgent”) and non-idiomatic trick you use incurs a learning penalty for everyone else. Once they have done that learning, then they will find working with the code less difficult. So it is hard to recognise how to simplify code that you are already familiar with. This is why I try to get “the new kid” to critique the code before they get too institutionalised!&lt;/p&gt; 
 &lt;p&gt;It is likely that the previous author(s) created this huge mess one tiny increment at a time, not all at once. So you are the first person who has ever had to try to make sense of it all at once.&lt;/p&gt; 
 &lt;p&gt;In my class I describe a sprawling SQL stored procedure we were looking at one day, with hundreds of lines of conditionals in a huge WHERE clause. Someone asked how anyone could have let it get this bad. I told them: “When there are only 2 or 3 conditionals, adding another one doesn’t make any difference. By the time there are 20 or 30 conditionals, adding another one doesn’t make any difference!”&lt;/p&gt; 
 &lt;p&gt;There is no “simplifying force” acting on the code base other than deliberate choices that you make. Simplifying takes effort, and people are too often in a hurry.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Thanks to &lt;a href="https://dannorth.net"&gt;Dan North&lt;/a&gt; for his comment&lt;/em&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you've internalized the mental models of the project into your long-term memory, you won't experience a high cognitive load.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/img/mentalmodelsv15.png" alt="Mental models" width="700" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The more mental models there are to learn, the longer it takes for a new developer to deliver value.&lt;/p&gt; 
&lt;p&gt;Once you onboard new people on your project, try to measure the amount of confusion they have (pair programming may help). If they're confused for more than ~40 minutes in a row - you've got things to improve in your code.&lt;/p&gt; 
&lt;p&gt;If you keep the cognitive load low, people can contribute to your codebase within the first few hours of joining your company.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Our architecture is a standard CRUD app architecture, &lt;a href="https://danluu.com/simple-architectures/"&gt;a Python monolith on top of Postgres&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;How Instagram scaled to 14 million users with &lt;a href="https://read.engineerscodex.com/p/how-instagram-scaled-to-14-million"&gt;only 3 engineers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The companies where we were like ”woah, these folks are &lt;a href="https://kenkantzer.com/learnings-from-5-years-of-tech-startup-code-audits/"&gt;smart as hell&lt;/a&gt;” for the most part failed&lt;/li&gt; 
 &lt;li&gt;One function that wires up the entire system. If you want to know how the system works - &lt;a href="https://www.infoq.com/presentations/8-lines-code-refactoring"&gt;go read it&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These architectures are quite boring and easy to understand. Anyone can grasp them without much mental effort.&lt;/p&gt; 
&lt;p&gt;Involve junior developers in architecture reviews. They will help you to identify the mentally demanding areas.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Maintaining software is hard&lt;/strong&gt;, things break and we would need every bit of mental effort we can save.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Imagine for a moment that what we inferred in the second chapter isn’t actually true. If that’s the case, then the conclusion we just negated, along with the conclusions in the previous chapter that we had accepted as valid, might not be correct either. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Do you feel it? Not only do you have to jump all over the article to get the meaning (shallow modules!), but the paragraph in general is difficult to understand. We have just created an unnecessary cognitive load in your head. &lt;strong&gt;Do not do this to your colleagues.&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/zakirullin/cognitive-load/main/img/smartauthorv14thanksmari.png" alt="Smart author" width="600" /&gt; 
&lt;/div&gt; 
&lt;p&gt;We should reduce any cognitive load above and beyond what is intrinsic to the work we do.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/zakirullin/"&gt;LinkedIn&lt;/a&gt;, &lt;a href="https://twitter.com/zakirullin"&gt;X&lt;/a&gt;, &lt;a href="https://github.com/zakirullin"&gt;GitHub&lt;/a&gt;, artemzr(аt)g-yоu-knоw-com&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Comments&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Rob Pike&lt;/strong&gt;&lt;br /&gt;Nice article.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://x.com/karpathy/status/1872038630405054853" target="_blank"&gt;Andrej Karpathy&lt;/a&gt;&lt;/strong&gt; &lt;i&gt;(ChatGPT, Tesla)&lt;/i&gt;&lt;br /&gt;Nice post on software engineering. Probably the most true, least practiced viewpoint.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://x.com/elonmusk/status/1872346903792566655" target="_blank"&gt;Elon Musk&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;True.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7277757844970520576/" target="_blank"&gt;Addy Osmani&lt;/a&gt;&lt;/strong&gt; &lt;i&gt;(Chrome, the most complex software system in the world)&lt;/i&gt;&lt;br /&gt;I've seen countless projects where smart developers created impressive architectures using the latest design patterns and microservices. But when new team members tried to make changes, they spent weeks just trying to understand how everything fits together. The cognitive load was so high that productivity plummeted and bugs multiplied.&lt;/p&gt; 
 &lt;p&gt;The irony? Many of these complexity-inducing patterns were implemented in the name of "clean code."&lt;/p&gt; 
 &lt;p&gt;What really matters is reducing unnecessary cognitive burden. Sometimes this means fewer, deeper modules instead of many shallow ones. Sometimes it means keeping related logic together instead of splitting it into tiny functions.&lt;/p&gt; 
 &lt;p&gt;And sometimes it means choosing boring, straightforward solutions over clever ones. The best code isn't the most elegant or sophisticated - it's the code that future developers (including yourself) can understand quickly.&lt;/p&gt; 
 &lt;p&gt;Your article really resonates with the challenges we face in browser development. You're absolutely right about modern browsers being among the most complex software systems. Managing that complexity in Chromium is a constant challenge that aligns perfectly with many of the points you made about cognitive load.&lt;/p&gt; 
 &lt;p&gt;One way we try to handle this in Chromium is through careful component isolation and well-defined interfaces between subsystems (like rendering, networking, JavaScript execution, etc.). Similar to your deep modules example with Unix I/O - we aim for powerful functionality behind relatively simple interfaces. For instance, our rendering pipeline handles incredible complexity (layout, compositing, GPU acceleration) but developers can interact with it through clear abstraction layers.&lt;/p&gt; 
 &lt;p&gt;Your points about avoiding unnecessary abstractions really hit home too. In browser development, we constantly balance between making the codebase approachable for new contributors while handling the inherent complexity of web standards and compatibility. &lt;/p&gt; 
 &lt;p&gt;Sometimes the simplest solution is the best one, even in a complex system.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://x.com/antirez" target="_blank"&gt;antirez&lt;/a&gt;&lt;/strong&gt; &lt;i&gt;(Redis)&lt;/i&gt;&lt;br /&gt;Totally agree about it :) Also, what I believe is missing from mentioned "A Philosophy of Software Design" is the concept of "design sacrifice". That is, sometimes you sacrifice something and get back simplicity, or performances, or both. I apply this idea continuously, but often is not understood.&lt;/p&gt; 
 &lt;p&gt;A good example is the fact that I always refused to have hash items expires. This is a design sacrifice because if you have certain attributes only in the top-level items (the keys themselves), the design is simpler, values will just be objects. When Redis got hash expires, it was a nice feature but required (indeed) many changes to many parts, raising the complexity.&lt;/p&gt; 
 &lt;p&gt;Another example is what I'm doing right now, Vector Sets, the new Redis data type. I decided that Redis would not be the source of truth about vectors, but that it can just take an approximate version of them, so I was able to do on-insert normalization, quantization without trying to retain the large floats vector on disk, and so forth. May vector DBs don't sacrifice the fact of remembering what the user put inside (the full precision vector).&lt;/p&gt; 
 &lt;p&gt;These are just two random examples, but I apply this idea everywhere. Now the thing is: of course one must sacrifice the right things. Often, there are 5% features that account for a very large amount of complexity: that is a good thing to kill :D&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://working-for-the-future.medium.com/about" target="_blank"&gt;A developer from the internet&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;You would not hire me... I sell myself on my track record of released enterprise projects.&lt;/p&gt; 
 &lt;p&gt;I worked with a guy that could speak design patterns. I could never speak that way, though I was one of the few that could well understand him. The managers loved him and he could dominate any development conversation. The people working around him said he left a trail of destruction behind him. I was told that I was the first person that could understand his projects. Maintainability matters. I care most about TCO. For some firms, that's what matters.&lt;/p&gt; 
 &lt;p&gt;I logged into Github after not being there for a while and for some reason it took me to an article in a repository by someone that seemed random. I was thinking "what is this" and had some trouble getting to my home page, so I read it. I didn't really register it at the time, but it was amazing. Every developer should read it. It largely said that almost everything we've been told about programming best practices leads to excessive "cognitive load", meaning our minds are getting kicked by the intellectual demands. I've known this for a while, especially with the demands of cloud, security and DevOps.&lt;/p&gt; 
 &lt;p&gt;I also liked it because it described practices I have done for decades, but never much admit to because they are not popular... I write really complicated stuff and need all the help I can get.&lt;/p&gt; 
 &lt;p&gt;Consider, if I'm right, it popped up because the Github folks, very smart people, though that developers should see it. I agree.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=45074248" target="_blank"&gt;Comments on Hacker News&lt;/a&gt; (&lt;a href="https://news.ycombinator.com/item?id=42489645" target="_blank"&gt;2&lt;/a&gt;)&lt;/p&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>juspay/hyperswitch</title>
      <link>https://github.com/juspay/hyperswitch</link>
      <description>&lt;p&gt;An open source payments switch written in Rust to make payments fast, reliable and affordable&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-logo-dark.svg#gh-dark-mode-only" alt="Hyperswitch-Logo" width="40%" /&gt; &lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-logo-light.svg#gh-light-mode-only" alt="Hyperswitch-Logo" width="40%" /&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Composable Open-Source Payments Infrastructure&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/gifs/quickstart.gif" alt="Quickstart demo" /&gt; &lt;/p&gt; 
&lt;!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} --&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/juspay/hyperswitch/actions?query=workflow%3ACI+branch%3Amain"&gt; &lt;img src="https://github.com/juspay/hyperswitch/workflows/CI-push/badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://github.com/juspay/hyperswitch/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/juspay/hyperswitch" /&gt; &lt;/a&gt; &lt;a href="https://github.com/juspay/hyperswitch/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/badge/Made_in-Rust-orange" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/company/hyperswitch/"&gt; &lt;img src="https://img.shields.io/badge/follow-hyperswitch-blue?logo=linkedin&amp;amp;labelColor=grey" /&gt; &lt;/a&gt; &lt;a href="https://x.com/hyperswitchio"&gt; &lt;img src="https://img.shields.io/badge/follow-%40hyperswitchio-white?logo=x&amp;amp;labelColor=grey" /&gt; &lt;/a&gt; &lt;a href="https://inviter.co/hyperswitch-slack"&gt; &lt;img src="https://img.shields.io/badge/chat-on_slack-blue?logo=slack&amp;amp;labelColor=grey&amp;amp;color=%233f0e40" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;📁 Table of Contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#-what-can-i-do-with-hyperswitch"&gt;What Can I Do with Hyperswitch?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#-quickstart-local-setup"&gt;Quickstart (Local Setup)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#cloud-deployment"&gt;Cloud Deployment&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#hosted-sandbox-no-setup-required"&gt;Hosted Sandbox (No Setup Required)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#-why-hyperswitch"&gt;Why Hyperswitch?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#architectural-overview"&gt;Architectural Overview&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#our-vision"&gt;Our Vision&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#community--contributions"&gt;Community &amp;amp; Contributions&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#feature-requests--bugs"&gt;Feature Requests &amp;amp; Bugs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#versioning"&gt;Versioning&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#copyright-and-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#team-behind-hyperswitch"&gt;Team Behind Hyperswitch&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;summary&gt;&lt;h2&gt; What Can I Do with Hyperswitch?&lt;/h2&gt;&lt;/summary&gt; 
&lt;p&gt;Hyperswitch offers a modular, open-source payments infrastructure designed for flexibility and control. Apart from our Payment Suite offering, this solution allows businesses to pick and integrate only the modules they need on top of their existing payment stack — without unnecessary complexity or vendor lock-in.&lt;/p&gt; 
&lt;p&gt;Each module is independent and purpose-built to optimize different aspects of payment processing.&lt;/p&gt; 
&lt;h3&gt; Learn More About The Payment Modules &lt;/h3&gt; 
&lt;details&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cost Observability&lt;/strong&gt;&lt;br /&gt; Advanced observability tools to audit, monitor, and optimize your payment costs. Detect hidden fees, downgrades, and penalties with self-serve dashboards and actionable insights.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/ai-powered-cost-observability"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Revenue Recovery&lt;/strong&gt;&lt;br /&gt; Combat passive churn with intelligent retry strategies tuned by card bin, region, method, and more. Offers fine-grained control over retry algorithms, penalty budgets, and recovery transparency.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/revenue-recovery"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vault&lt;/strong&gt;&lt;br /&gt; A PCI-compliant vault service to store cards, tokens, wallets, and bank credentials. Provides a unified, secure, and reusable store of customer-linked payment methods.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/vault"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Intelligent Routing&lt;/strong&gt;&lt;br /&gt; Route each transaction to the PSP with the highest predicted auth rate. Reduce retries, avoid downtime, and minimize latency while maximizing first attempt success.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/intelligent-routing"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconciliation&lt;/strong&gt;&lt;br /&gt; Automate 2-way and 3-way reconciliation with backdated support, staggered scheduling, and customizable outputs. Reduces manual ops effort and increases audit confidence.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/reconciliation"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternate Payment Methods&lt;/strong&gt;&lt;br /&gt; Drop-in widgets for PayPal, Apple Pay, Google Pay, Samsung Pay, Pay by Bank, and BNPL providers like Klarna. Maximizes conversions with seamless one-click checkout.&lt;br /&gt; &lt;em&gt;&lt;a href="https://docs.hyperswitch.io/about-hyperswitch/payments-modules/enable-alternate-payment-method-widgets"&gt;Read more&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt; Local Setup via Docker &lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# One-click local setup

git clone --depth 1 --branch latest https://github.com/juspay/hyperswitch

cd hyperswitch

scripts/setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;This script: &lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Detects Docker/Podman&lt;/li&gt; 
  &lt;li&gt;Offers multiple deployment profiles: 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Standard&lt;/strong&gt;: App server + Control Center&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Full&lt;/strong&gt;: Includes monitoring + schedulers&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Minimal&lt;/strong&gt;: Standalone App server&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Provides access links when done&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;If you need further help, check out our &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/overview/unified-local-setup-using-docker"&gt;video tutorial&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;👉 After setup, &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/using-hyperswitch-control-center#add-a-payment-processor"&gt;configure a connector&lt;/a&gt; and &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/account-setup/test-a-payment"&gt;test a payment&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Hosted Sandbox (No Setup Required)&lt;/h3&gt; 
&lt;p&gt;Hyperswitch offers a fully hosted sandbox environment that requires no setup. You can explore the Control Center, configure payment connectors, and test payments directly from the UI.&lt;/p&gt; 
&lt;a href="https://app.hyperswitch.io"&gt; &lt;img src="https://github.com/juspay/hyperswitch/raw/main/docs/imgs/try-the-sandbox.png?raw=true" height="35" /&gt; &lt;/a&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt; What you can do in the Hosted Sandbox&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Access the full Control Center&lt;/li&gt; 
  &lt;li&gt;Configure payment connectors&lt;/li&gt; 
  &lt;li&gt;View logs, routing rules, and retry strategies&lt;/li&gt; 
  &lt;li&gt;Try payments directly from the UI&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;strong&gt;Cloud Deployment&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;You can deploy to AWS, GCP, or Azure using Helm or CDK scripts. Fastest path:&lt;/p&gt; 
&lt;p&gt;Click to deploy via AWS:&lt;/p&gt; 
&lt;a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=HyperswitchBootstarp&amp;amp;templateURL=https://hyperswitch-synth.s3.eu-central-1.amazonaws.com/hs-starter-config.yaml"&gt; &lt;img src="https://github.com/juspay/hyperswitch/raw/main/docs/imgs/aws_button.png?raw=true" height="35" /&gt; &lt;/a&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Cloud Deployment Instructions&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Click the AWS deployment button above to launch the stack.&lt;/li&gt; 
  &lt;li&gt;Follow the guided steps in the AWS Console (approx. 30–45 mins).&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;✅ This setup provisions Hyperswitch on your cloud account using CloudFormation.&lt;/p&gt; 
 &lt;p&gt;📘 For full instructions and Helm-based deployments, check out the&lt;br /&gt; &lt;a href="https://docs.hyperswitch.io/hyperswitch-open-source/deploy-on-kubernetes-using-helm"&gt;Cloud Install Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#architectural-overview"&gt; &lt;h2 id="architectural-overview"&gt;Architectural Overview&lt;/h2&gt; &lt;/a&gt; 
&lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/features.png" /&gt; 
&lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/non-functional-features.png" /&gt; 
&lt;img src="https://raw.githubusercontent.com/juspay/hyperswitch/main/docs/imgs/hyperswitch-architecture-v1.png" /&gt; 
&lt;h2&gt;Why Hyperswitch?&lt;/h2&gt; 
&lt;p&gt;Hyperswitch is a commercial open-source payments stack purpose-built for scale, flexibility, and developer experience. Designed with a modular architecture, Hyperswitch lets you pick only the components you need—whether it’s routing, retries, vaulting, or observability—without vendor lock-in or bloated integrations.&lt;/p&gt; 
&lt;p&gt;Built in Rust for performance and reliability, Hyperswitch supports global payment methods (cards, wallets, BNPL, UPI, Pay by Bank), exposes smart routing and retry logic, and provides a visual workflow builder in the Control Center. Whether you're integrating a full payment suite or augmenting an existing stack with a single module, Hyperswitch meets you where you are.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;“Linux for Payments”&lt;/strong&gt; — Hyperswitch is a well-architected reference for teams who want to own their payments stack.&lt;/p&gt; 
&lt;p&gt;We believe in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Embracing Payment Diversity:&lt;/strong&gt; Innovation comes from enabling choice—across payment methods, processors, and flows.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Open Source by Default:&lt;/strong&gt; Transparency drives trust and builds better, reusable software.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Community-Driven Development:&lt;/strong&gt; Our roadmap is shaped by real-world use cases and contributors.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Systems-Level Engineering:&lt;/strong&gt; We hold ourselves to a high bar for reliability, security, and performance.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Maximizing Value Creation:&lt;/strong&gt; For developers, customers, and partners alike.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt; Community-Driven, Enterprise-Tested:&lt;/strong&gt; Hyperswitch is built in the open with real-world feedback from developers and contributors, and maintained by Juspay, the team powering payment infrastructure for 400+ leading enterprises worldwide.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributors from around the world to help build Hyperswitch. Whether you're fixing bugs, improving documentation, or adding new features, your help is appreciated.&lt;/p&gt; 
&lt;p&gt;Please read our &lt;a href="https://github.com/juspay/hyperswitch/raw/main/docs/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;Join the conversation on &lt;a href="https://inviter.co/hyperswitch-slack"&gt;Slack&lt;/a&gt; or explore open issues on &lt;a href="https://github.com/juspay/hyperswitch/issues"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#feature-requests"&gt; &lt;h2 id="feature-requests"&gt;Feature requests &amp;amp; Bugs&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;For new product features, enhancements, roadmap discussions, or to share queries and ideas, visit our &lt;a href="https://github.com/juspay/hyperswitch/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For reporting a bug, please read the issue guidelines and search for &lt;a href="https://github.com/juspay/hyperswitch/issues"&gt;existing and closed issues&lt;/a&gt;. If your problem or idea is not addressed yet, please &lt;a href="https://github.com/juspay/hyperswitch/issues/new/choose"&gt;open a new issue&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#versioning"&gt; &lt;h2 id="versioning"&gt;Versioning&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;Check the &lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#copyright-and-license"&gt; &lt;h2 id="copyright-and-license"&gt;Copyright and License&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;This product is licensed under the &lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://raw.githubusercontent.com/juspay/hyperswitch/main/#team-behind-hyperswitch"&gt; &lt;h2 id="team-behind-hyperswitch"&gt;Team behind Hyperswitch&lt;/h2&gt; &lt;/a&gt; 
&lt;p&gt;The core team of 150+ engineers building Hyperswitch. Keep up the great work! 🥂&lt;/p&gt; 
&lt;a href="https://github.com/juspay/hyperswitch/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=juspay/hyperswitch" alt="Contributors" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/MiniCPM-V</title>
      <link>https://github.com/OpenBMB/MiniCPM-V</link>
      <description>&lt;p&gt;MiniCPM-V 4.5: A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm_v_and_minicpm_o_title.png" width="500em" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README_zh.md"&gt;中文&lt;/a&gt; | English&lt;/strong&gt;&lt;/p&gt; 
 &lt;span style="display: inline-flex; align-items: center; margin-right: 2px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/wechat.png" alt="WeChat" style="margin-right: 4px;" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/wechat.md" target="_blank"&gt; WeChat&lt;/a&gt; &amp;nbsp;| &lt;/span&gt; &amp;nbsp; 
 &lt;span style="display: inline-flex; align-items: center; margin-left: -8px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/discord.png" alt="Discord" style="margin-right: 4px;" /&gt; &lt;a href="https://discord.gg/rftuRMbqzf" target="_blank"&gt; Discord&lt;/a&gt; &amp;nbsp; &lt;/span&gt; 
 &lt;p align="center"&gt; MiniCPM-V 4.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;🤗&lt;/a&gt; &lt;a href="http://101.126.42.235:30910/"&gt;🤖&lt;/a&gt; | MiniCPM-o 2.6 &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;🤗&lt;/a&gt; &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt; 🤖&lt;/a&gt; | &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-Cookbook"&gt;🍳 Cookbook&lt;/a&gt; | 📄 Technical Report (Coming Soon) &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V&lt;/strong&gt; is a series of efficient end-side multimodal LLMs (MLLMs), which accept images, videos and text as inputs and deliver high-quality text outputs. &lt;strong&gt;MiniCPM-o&lt;/strong&gt; additionally takes audio as inputs and provides high-quality speech outputs in an end-to-end fashion. Since February 2024, we have released 7 versions of the model, aiming to achieve &lt;strong&gt;strong performance and efficient deployment&lt;/strong&gt;. The most notable models in the series currently include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt;: 🔥🔥🔥 The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, this model &lt;strong&gt;outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B&lt;/strong&gt; in vision-language capabilities, making it the most performant on-device multimodal model in the open-source community. This version brings &lt;strong&gt;new features including efficient high-FPS and long video understanding (up to 96x compression rate for video tokens), controllable hybrid fast/deep thinking, strong handwritten OCR and complex table/document parsing&lt;/strong&gt;. It also advances MiniCPM-V's popular features such as trustworthy behavior, multilingual support and end-side deployability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt;: ⭐️⭐️⭐️ The most capable model in the MiniCPM-o series. With a total of 8B parameters, this end-to-end model &lt;strong&gt;achieves comparable performance to GPT-4o-202405 in vision, speech, and multimodal live streaming&lt;/strong&gt;, making it one of the most versatile and performant models in the open-source community. For the new voice mode, MiniCPM-o 2.6 &lt;strong&gt;supports bilingual real-time speech conversation with configurable voices&lt;/strong&gt;, and also allows for fun capabilities such as emotion/speed/style control, end-to-end voice cloning, role play, etc. Due to its superior token density, MiniCPM-o 2.6 can for the first time &lt;strong&gt;support multimodal live streaming on end-side devices&lt;/strong&gt; such as iPad.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;News 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;h4&gt;📌 Pinned&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;[2025.09.01] ⭐️⭐️⭐️ MiniCPM-V 4.5 has been officially supported by &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15575"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm/pull/23586"&gt;vLLM&lt;/a&gt;, and &lt;a href="https://github.com/hiyouga/LLaMA-Factory/pull/9022"&gt;LLaMA-Factory&lt;/a&gt;. You are welcome to use it directly through these official channels! Support for additional frameworks such as &lt;a href="https://github.com/ollama/ollama/pull/12078"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/sgl-project/sglang/pull/9610"&gt;SGLang&lt;/a&gt; is actively in progress.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.26] 🔥🔥🔥 We open-source MiniCPM-V 4.5, which outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B. It advances popular capabilities of MiniCPM-V, and brings useful new features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.01] ⭐️⭐️⭐️ We open-sourced the &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;! It provides comprehensive guides for diverse user scenarios, paired with our new &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;Docs Site&lt;/a&gt; for smoother onboarding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.06.20] ⭐️⭐️⭐️ Our official &lt;a href="https://ollama.com/openbmb"&gt;Ollama repository&lt;/a&gt; is released. Try our latest models with &lt;a href="https://ollama.com/openbmb/minicpm-o2.6"&gt;one click&lt;/a&gt;！&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.03.01] 🚀🚀🚀 RLAIF-V, the alignment technique of MiniCPM-o, is accepted by CVPR 2025 Highlights！The &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;code&lt;/a&gt;, &lt;a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset"&gt;dataset&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2405.17220"&gt;paper&lt;/a&gt; are open-sourced!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.24] 📢📢📢 MiniCPM-o 2.6 technical report is released! See &lt;a href="https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] 📢 &lt;strong&gt;ATTENTION!&lt;/strong&gt; We are currently working on merging MiniCPM-o 2.6 into the official repositories of llama.cpp, Ollama, and vllm. Until the merge is complete, please USE OUR LOCAL FORKS of &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;Ollama&lt;/a&gt;, and &lt;a href="https://github.com/OpenBMB/MiniCPM-o?tab=readme-ov-file#efficient-inference-with-llamacpp-ollama-vllm"&gt;vllm&lt;/a&gt;. &lt;strong&gt;Using the official repositories before the merge may lead to unexpected issues&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] ⭐️⭐️⭐️ MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.17] We have updated the usage of MiniCPM-o 2.6 int4 quantization version and resolved the model initialization error. Click &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;here&lt;/a&gt; and try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.13] 🔥🔥🔥 We open-source MiniCPM-o 2.6, which matches GPT-4o-202405 on vision, speech and multimodal live streaming. It advances popular capabilities of MiniCPM-V 2.6, and supports various new fun features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.17] 🚀🚀🚀 MiniCPM-V 2.6 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.06] 🔥🔥🔥 We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See &lt;a href="https://arxiv.org/abs/2408.01800"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.05.23] 🔥🔥🔥 MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradio’s official account, is available &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;here&lt;/a&gt;. Come and try it out!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more news.&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;[2025.08.02] 🚀🚀🚀 We open-source MiniCPM-V 4.0, which outperforms GPT-4.1-mini-20250414 in image understanding. It advances popular features of MiniCPM-V 2.6, and largely improves the efficiency. We also open-source the iOS App on iPhone and iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2025.01.23] 💡💡💡 MiniCPM-o 2.6 is now supported by &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything&lt;/a&gt;, a framework by PKU-Alignment Team for aligning any-to-any modality large models with human intentions. It supports DPO and SFT fine-tuning on both vision and audio. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.15] We now also support multi-image SFT. For more details, please refer to the &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune"&gt;document&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.14] MiniCPM-V 2.6 now also supports &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.10] 🚀🚀🚀 MiniCPM-Llama3-V 2.5 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model's layers across multiple GPUs. For more details, check this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] 🚀🚀🚀 MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and Ollama! Please pull the latest code &lt;strong&gt;of our provided forks&lt;/strong&gt; (&lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-v2.5/examples/minicpmv/README.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5"&gt;Ollama&lt;/a&gt;). GGUF models in various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main"&gt;here&lt;/a&gt;. MiniCPM-Llama3-V 2.5 series is &lt;strong&gt;not supported by the official repositories yet&lt;/strong&gt;, and we are working hard to merge PRs. Please stay tuned!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] 💫 We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.24] We release the MiniCPM-Llama3-V 2.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;gguf&lt;/a&gt;, which supports &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-llamacpp"&gt;llama.cpp&lt;/a&gt; inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.23] 🔍 We've released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmark evaluations, multilingual capabilities, and inference efficiency 🌟📊🌍🚀. Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/compare_with_phi-3_vision.md"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#deployment-on-mobile-phone"&gt;efficient inference&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;simple fine-tuning&lt;/a&gt;. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.17] MiniCPM-V-2.0 supports deploying &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#webui-demo"&gt;WebUI Demo&lt;/a&gt; now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.15] MiniCPM-V-2.0 now also supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href="https://rank.opencompass.org.cn/leaderboard-multimodal"&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href="https://openbmb.vercel.app/minicpm-v-2"&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.14] MiniCPM-V now supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework. Thanks to &lt;a href="https://github.com/Jintao-Huang"&gt;Jintao&lt;/a&gt; for the contribution！&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.01] MiniCPM-V can now be deployed on Mac!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v-45"&gt;MiniCPM-V 4.5&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-efficiency"&gt;Inference Efficiency&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-o-26"&gt;MiniCPM-o 2.6&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v--o-cookbook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio-"&gt;Chat with Our Demo on Gradio 🤗&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference"&gt;Inference&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#model-zoo"&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multi-turn-conversation"&gt;Multi-turn Conversation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-multiple-images"&gt;Chat with Multiple Images&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#in-context-few-shot-learning"&gt;In-context Few-shot Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-video"&gt;Chat with Video&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#speech-and-audio-mode"&gt;Speech and Audio Mode&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multimodal-live-streaming"&gt;Multimodal Live Streaming&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-multiple-gpus"&gt;Inference on Multiple GPUs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-mac"&gt;Inference on Mac&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#fine-tuning"&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#awesome-work-using-minicpm-v--minicpm-o"&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#faqs"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#limitations"&gt;Limitations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;MiniCPM-V 4.5&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt; is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;State-of-the-art Vision-Language Capability.&lt;/strong&gt; MiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B&lt;/strong&gt; for vision-language capabilities, making it the most performant MLLM under 30B parameters.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🎬 &lt;strong&gt;Efficient High-FPS and Long Video Understanding.&lt;/strong&gt; Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can perceive significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high-FPS (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;⚙️ &lt;strong&gt;Controllable Hybrid Fast/Deep Thinking.&lt;/strong&gt; MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💪 &lt;strong&gt;Strong OCR, Document Parsing and Others.&lt;/strong&gt; Based on &lt;a href="https://arxiv.org/pdf/2403.11703"&gt;LLaVA-UHD&lt;/a&gt; architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x fewer visual tokens than most MLLMs. The model achieves &lt;strong&gt;leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5&lt;/strong&gt;. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o-latest on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; in more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💫 &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-V 4.5 can be easily used in various ways: (1) &lt;a href="https://github.com/tc-mb/llama.cpp/raw/Support-MiniCPM-V-4.5/docs/multimodal/minicpmv4.5.md"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/ollama/tree/MIniCPM-V"&gt;ollama&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;int4&lt;/a&gt;, &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/AutoAWQ"&gt;AWQ&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://github.com/tc-mb/sglang/tree/main"&gt;SGLang&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://github.com/tc-mb/transformers/tree/main"&gt;Transformers&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, (6) optimized &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;local iOS app&lt;/a&gt; on iPhone and iPad, and (7) online web demo on &lt;a href="http://101.126.42.235:30910/"&gt;server&lt;/a&gt;. See our &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;Cookbook&lt;/a&gt; for full usage!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Techniques 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-v-4dot5-framework.png" , width="100%" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Architechture: Unified 3D-Resampler for High-density Video Compression.&lt;/strong&gt; MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96× compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high-FPS video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pre-training: Unified Learning for OCR and Knowledge from Documents.&lt;/strong&gt; Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe that the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Post-training: Hybrid Fast/Deep Thinking with Multimodal RL.&lt;/strong&gt; MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; and &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;, it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar_minicpm_v45.png" , width="60%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv_4_5_evaluation_result.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Inference Efficiency&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;OpenCompass&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score ↑&lt;/th&gt; 
    &lt;th&gt;Total Inference Time ↓&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;76.6&lt;/td&gt; 
    &lt;td&gt;17.5h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiMo-VL-7B-RL&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;76.4&lt;/td&gt; 
    &lt;td&gt;11h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;77.0&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;7.5h&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Video-MME&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score ↑&lt;/th&gt; 
    &lt;th&gt;Total Inference Time ↓&lt;/th&gt; 
    &lt;th&gt;GPU Mem ↓&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2.5-VL-7B-Instruct&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;3h&lt;/td&gt; 
    &lt;td&gt;60G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;73.6&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;2.63h&lt;/td&gt; 
    &lt;td&gt;32G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;73.5&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;0.26h&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;28G&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;Both Video-MME and OpenCompass were evaluated using 8×A100 GPUs for inference. The reported inference time of Video-MME includes full model-side computation, and excludes the external cost of video frame extraction (dependent on specific frame extraction tools) for fair comparison.&lt;/p&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=Cn23FujYMMU"&gt;&lt;img src="./assets/minicpmv4_5/MiniCPM-V 4.5-8.26_img.jpeg" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case1.png" alt="en_case1" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case2.png" alt="en_case2" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case3.jpeg" alt="en_case3" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more cases.&lt;/summary&gt; 
 &lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/zh_extra.jpeg" alt="zh_extra" style="margin-bottom: 5px;" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;p&gt;We deploy MiniCPM-V 4.5 on iPad M4 with &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;iOS demo&lt;/a&gt;. The demo video is the raw screen recording without edition.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_cot.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_travel.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;table align="center"&gt;   
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-o 2.6&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt; is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for real-time speech conversation and multimodal live streaming. Notable features of MiniCPM-o 2.6 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;Leading Visual Capability.&lt;/strong&gt; MiniCPM-o 2.6 achieves an average score of 70.2 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-202405, Gemini 1.5 Pro, and Claude 3.5 Sonnet&lt;/strong&gt; for single image understanding. It also &lt;strong&gt;outperforms GPT-4V and Claude 3.5 Sonnet&lt;/strong&gt; in multi-image and video understanding, and shows promising in-context learning capability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🎙 &lt;strong&gt;State-of-the-art Speech Capability.&lt;/strong&gt; MiniCPM-o 2.6 supports &lt;strong&gt;bilingual real-time speech conversation with configurable voices&lt;/strong&gt; in English and Chinese. It &lt;strong&gt;outperforms GPT-4o-realtime on audio understanding tasks&lt;/strong&gt; such as ASR and STT translation, and shows &lt;strong&gt;state-of-the-art performance on speech conversation in both semantic and acoustic evaluations in the open-source community&lt;/strong&gt;. It also allows for fun features such as emotion/speed/style control, end-to-end voice cloning, role play, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🎬 &lt;strong&gt;Strong Multimodal Live Streaming Capability.&lt;/strong&gt; As a new feature, MiniCPM-o 2.6 can &lt;strong&gt;accept continuous video and audio streams independent of user queries, and support real-time speech interaction&lt;/strong&gt;. It &lt;strong&gt;outperforms GPT-4o-202408 and Claude 3.5 Sonnet and shows state-of-the-art performance in the open-source community on StreamingBench&lt;/strong&gt;, a comprehensive benchmark for real-time video understanding, omni-source (video &amp;amp; audio) understanding, and multimodal contextual understanding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💪 &lt;strong&gt;Strong OCR Capability and Others.&lt;/strong&gt; Advancing popular visual capabilities from MiniCPM-V series, MiniCPM-o 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves &lt;strong&gt;state-of-the-art performance on OCRBench for models under 25B, surpassing proprietary models such as GPT-4o-202405&lt;/strong&gt;. Based on the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o and Claude 3.5 Sonnet on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; on more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🚀 &lt;strong&gt;Superior Efficiency.&lt;/strong&gt; In addition to its friendly size, MiniCPM-o 2.6 also shows &lt;strong&gt;state-of-the-art token density&lt;/strong&gt; (i.e., the number of pixels encoded into each visual token). &lt;strong&gt;It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models&lt;/strong&gt;. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-o 2.6 can efficiently support &lt;strong&gt;multimodal live streaming&lt;/strong&gt; on end-side devices such as iPads.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💫 &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-o 2.6 can be easily used in various ways: (1) &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;int4&lt;/a&gt; and &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;GGUF&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, and (6) online web demo on &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;server&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Architecture.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end Omni-modal Architecture.&lt;/strong&gt; Different modality encoders/decoders are connected and trained in an &lt;strong&gt;end-to-end&lt;/strong&gt; fashion to fully exploit rich multimodal knowledge. The model is trained in a fully end-to-end manner with only CE loss.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Omni-modal Live Streaming Mechanism.&lt;/strong&gt; (1) We change the offline modality encoder/decoders into online ones for &lt;strong&gt;streaming inputs/outputs.&lt;/strong&gt; (2) We devise a &lt;strong&gt;time-division multiplexing (TDM) mechanism&lt;/strong&gt; for omni-modality streaming processing in the LLM backbone. It divides parallel omni-modality streams into sequential info within small periodic time slices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Speech Modeling Design.&lt;/strong&gt; We devise a multimodal system prompt, including traditional text system prompt, and &lt;strong&gt;a new audio system prompt to determine the assistant voice&lt;/strong&gt;. This enables flexible voice configurations in inference time, and also facilitates end-to-end voice cloning and description-based voice creation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-o-26-framework-v2.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar.jpg" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view visual understanding results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Image Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;Token Density&lt;sup&gt;+&lt;/sup&gt;&lt;/th&gt; 
     &lt;th&gt;OpenCompass&lt;/th&gt; 
     &lt;th&gt;OCRBench&lt;/th&gt; 
     &lt;th&gt;MathVista mini&lt;/th&gt; 
     &lt;th&gt;ChartQA&lt;/th&gt; 
     &lt;th&gt;MMVet&lt;/th&gt; 
     &lt;th&gt;MMStar&lt;/th&gt; 
     &lt;th&gt;MME&lt;/th&gt; 
     &lt;th&gt;MMB1.1 test&lt;/th&gt; 
     &lt;th&gt;AI2D&lt;/th&gt; 
     &lt;th&gt;MMMU val&lt;/th&gt; 
     &lt;th&gt;HallusionBench&lt;/th&gt; 
     &lt;th&gt;TextVQA val&lt;/th&gt; 
     &lt;th&gt;DocVQA test&lt;/th&gt; 
     &lt;th&gt;MathVerse mini&lt;/th&gt; 
     &lt;th&gt;MathVision&lt;/th&gt; 
     &lt;th&gt;MMHal Score&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;69.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;736&lt;/td&gt; 
     &lt;td&gt;61.3&lt;/td&gt; 
     &lt;td&gt;85.7&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;2328.7&lt;/td&gt; 
     &lt;td&gt;82.2&lt;/td&gt; 
     &lt;td&gt;84.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;92.8&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;50.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;30.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;3.6&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Claude3.5-Sonnet&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;750&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;788&lt;/td&gt; 
     &lt;td&gt;61.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;90.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;66.0&lt;/td&gt; 
     &lt;td&gt;62.2&lt;/td&gt; 
     &lt;td&gt;1920.0&lt;/td&gt; 
     &lt;td&gt;78.5&lt;/td&gt; 
     &lt;td&gt;80.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;65.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;49.9&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;95.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;64.4&lt;/td&gt; 
     &lt;td&gt;754&lt;/td&gt; 
     &lt;td&gt;57.7&lt;/td&gt; 
     &lt;td&gt;81.3&lt;/td&gt; 
     &lt;td&gt;64.0&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;2110.6&lt;/td&gt; 
     &lt;td&gt;73.9&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;45.6&lt;/td&gt; 
     &lt;td&gt;73.5&lt;/td&gt; 
     &lt;td&gt;86.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;19.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-mini-20240718&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;64.1&lt;/td&gt; 
     &lt;td&gt;785&lt;/td&gt; 
     &lt;td&gt;52.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;66.9&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2003.4&lt;/td&gt; 
     &lt;td&gt;76.0&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;46.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Cambrian-34B&lt;/td&gt; 
     &lt;td&gt;34B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1820&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.3&lt;/td&gt; 
     &lt;td&gt;591&lt;/td&gt; 
     &lt;td&gt;50.3&lt;/td&gt; 
     &lt;td&gt;75.6&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;54.2&lt;/td&gt; 
     &lt;td&gt;2049.9&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;79.5&lt;/td&gt; 
     &lt;td&gt;50.4&lt;/td&gt; 
     &lt;td&gt;41.6&lt;/td&gt; 
     &lt;td&gt;76.7&lt;/td&gt; 
     &lt;td&gt;75.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4V-9B&lt;/td&gt; 
     &lt;td&gt;13B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;776&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;58.0&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2018.8&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;71.2&lt;/td&gt; 
     &lt;td&gt;46.9&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Pixtral-12B&lt;/td&gt; 
     &lt;td&gt;12B&lt;/td&gt; 
     &lt;td&gt;256&lt;/td&gt; 
     &lt;td&gt;61.0&lt;/td&gt; 
     &lt;td&gt;685&lt;/td&gt; 
     &lt;td&gt;56.9&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;58.5&lt;/td&gt; 
     &lt;td&gt;54.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;72.7&lt;/td&gt; 
     &lt;td&gt;79.0&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;47.0&lt;/td&gt; 
     &lt;td&gt;75.7&lt;/td&gt; 
     &lt;td&gt;90.7&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;63.3&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;66.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;52.7&lt;/td&gt; 
     &lt;td&gt;60.2&lt;/td&gt; 
     &lt;td&gt;2328.1&lt;/td&gt; 
     &lt;td&gt;76.8&lt;/td&gt; 
     &lt;td&gt;79.2&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;44.6&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;DeepSeek-VL2-27B (4B)&lt;/td&gt; 
     &lt;td&gt;27B&lt;/td&gt; 
     &lt;td&gt;672&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;809&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;86.0&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;61.9&lt;/td&gt; 
     &lt;td&gt;2253.0&lt;/td&gt; 
     &lt;td&gt;81.2&lt;/td&gt; 
     &lt;td&gt;83.8&lt;/td&gt; 
     &lt;td&gt;54.0&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;84.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;93.3&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;67.1&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;866&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.2&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;62.0&lt;/td&gt; 
     &lt;td&gt;60.7&lt;/td&gt; 
     &lt;td&gt;2326.0&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;54.1&lt;/td&gt; 
     &lt;td&gt;50.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;84.3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;94.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;31.9&lt;/td&gt; 
     &lt;td&gt;16.3&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;182&lt;/td&gt; 
     &lt;td&gt;68.1&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;67.5&lt;/td&gt; 
     &lt;td&gt;83.7&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;65.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;2261.0&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;85.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;56.8&lt;/td&gt; 
     &lt;td&gt;49.0&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;91.3&lt;/td&gt; 
     &lt;td&gt;39.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.5&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;706&lt;/td&gt; 
     &lt;td&gt;68.3&lt;/td&gt; 
     &lt;td&gt;822&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;2344.0&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;83.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.5&lt;/td&gt; 
     &lt;td&gt;56.0&lt;/td&gt; 
     &lt;td&gt;50.1&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;93.0&lt;/td&gt; 
     &lt;td&gt;39.5&lt;/td&gt; 
     &lt;td&gt;19.7&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;65.2&lt;/td&gt; 
     &lt;td&gt;852*&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;79.4&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;57.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2348.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;78.0&lt;/td&gt; 
     &lt;td&gt;82.1&lt;/td&gt; 
     &lt;td&gt;49.8*&lt;/td&gt; 
     &lt;td&gt;48.1*&lt;/td&gt; 
     &lt;td&gt;80.1&lt;/td&gt; 
     &lt;td&gt;90.8&lt;/td&gt; 
     &lt;td&gt;25.7&lt;/td&gt; 
     &lt;td&gt;18.3&lt;/td&gt; 
     &lt;td&gt;3.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;70.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;897*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;86.9*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;67.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2372.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;50.4*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;51.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;82.0&lt;/td&gt; 
     &lt;td&gt;93.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;41.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;23.1*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set. 
 &lt;p&gt;&lt;sup&gt;+&lt;/sup&gt; Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.&lt;/p&gt; 
 &lt;p&gt;Note: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-image and Video Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;BLINK val&lt;/th&gt; 
     &lt;th&gt;Mantis Eval&lt;/th&gt; 
     &lt;th&gt;MIRB&lt;/th&gt; 
     &lt;th&gt;Video-MME (wo / w subs)&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;68.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9/77.2&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT4V&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;54.6&lt;/td&gt; 
     &lt;td&gt;62.7&lt;/td&gt; 
     &lt;td&gt;53.1&lt;/td&gt; 
     &lt;td&gt;59.9/63.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;56.1/58.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-NeXT-Interleave 14B&lt;/td&gt; 
     &lt;td&gt;14B&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;30.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;55.4&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;77.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;66.2/69.5&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MANTIS 8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;49.1&lt;/td&gt; 
     &lt;td&gt;59.5&lt;/td&gt; 
     &lt;td&gt;34.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;69.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67.6*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.3/69.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;67.7&lt;/td&gt; 
     &lt;td&gt;52.5&lt;/td&gt; 
     &lt;td&gt;64.2/66.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.0&lt;/td&gt; 
     &lt;td&gt;69.1&lt;/td&gt; 
     &lt;td&gt;53.8&lt;/td&gt; 
     &lt;td&gt;60.9/63.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;56.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;71.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;58.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;63.9/67.9&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view audio understanding and speech conversation results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Audio Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (zh)&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (en)&lt;/th&gt; 
     &lt;th colspan="2"&gt;AST&lt;/th&gt; 
     &lt;th&gt;Emotion&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th colspan="3"&gt;CER↓&lt;/th&gt; 
     &lt;th colspan="3"&gt;WER↓&lt;/th&gt; 
     &lt;th colspan="2"&gt;BLEU↑&lt;/th&gt; 
     &lt;th&gt;ACC↑&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th&gt;AISHELL-1&lt;/th&gt; 
     &lt;th&gt;Fleurs zh&lt;/th&gt; 
     &lt;th&gt;WenetSpeech test-net&lt;/th&gt; 
     &lt;th&gt;LibriSpeech test-clean&lt;/th&gt; 
     &lt;th&gt;GigaSpeech&lt;/th&gt; 
     &lt;th&gt;TED-LIUM&lt;/th&gt; 
     &lt;th&gt;CoVoST en2zh&lt;/th&gt; 
     &lt;th&gt;CoVoST zh2en&lt;/th&gt; 
     &lt;th&gt;MELD emotion&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.3*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;28.9*&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;12.9*&lt;/td&gt; 
     &lt;td&gt;4.8*&lt;/td&gt; 
     &lt;td&gt;37.1*&lt;/td&gt; 
     &lt;td&gt;15.7*&lt;/td&gt; 
     &lt;td&gt;33.2*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;4.5*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;14.3*&lt;/td&gt; 
     &lt;td&gt;2.9*&lt;/td&gt; 
     &lt;td&gt;10.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;47.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;22.6*&lt;/td&gt; 
     &lt;td&gt;48.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;45.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;24.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B-Instruct&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;6.9*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;10.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;3.1*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;9.7&lt;/u&gt;*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;39.5*&lt;/td&gt; 
     &lt;td&gt;22.9*&lt;/td&gt; 
     &lt;td&gt;17.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.16&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;8.4&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice-Base&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;6.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;8.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;48.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;27.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;52.4&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Speech Generation&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="9"&gt;SpeechQA&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th colspan="3"&gt;ACC↑&lt;/th&gt; 
     &lt;th&gt;G-Eval (10 point)↑&lt;/th&gt; 
     &lt;th&gt;Semantic ELO score↑&lt;/th&gt; 
     &lt;th&gt;Acoustic ELO score↑&lt;/th&gt; 
     &lt;th&gt;Overall ELO score↑&lt;/th&gt; 
     &lt;th&gt;UTMOS↑&lt;/th&gt; 
     &lt;th&gt;ASR-WER↓&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th&gt;Speech Llama Q.&lt;/th&gt; 
     &lt;th&gt;Speech Web Q.&lt;/th&gt; 
     &lt;th&gt;Speech Trivia QA&lt;/th&gt; 
     &lt;th&gt;Speech AlpacaEval&lt;/th&gt; 
     &lt;th colspan="5"&gt;AudioArena&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;7.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1157&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1203&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1200&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;50.0&lt;/td&gt; 
     &lt;td&gt;32.0&lt;/td&gt; 
     &lt;td&gt;36.4&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;999&lt;/td&gt; 
     &lt;td&gt;1147&lt;/td&gt; 
     &lt;td&gt;1035&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;4.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;11.7&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Llama-Omni&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;22.9&lt;/td&gt; 
     &lt;td&gt;10.7&lt;/td&gt; 
     &lt;td&gt;3.9&lt;/td&gt; 
     &lt;td&gt;960&lt;/td&gt; 
     &lt;td&gt;878&lt;/td&gt; 
     &lt;td&gt;897&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
     &lt;td&gt;24.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;46.7&lt;/td&gt; 
     &lt;td&gt;28.1&lt;/td&gt; 
     &lt;td&gt;23.3&lt;/td&gt; 
     &lt;td&gt;2.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Moshi&lt;/td&gt; 
     &lt;td&gt;7B&lt;/td&gt; 
     &lt;td&gt;43.7&lt;/td&gt; 
     &lt;td&gt;23.8&lt;/td&gt; 
     &lt;td&gt;16.7&lt;/td&gt; 
     &lt;td&gt;2.4&lt;/td&gt; 
     &lt;td&gt;871&lt;/td&gt; 
     &lt;td&gt;808&lt;/td&gt; 
     &lt;td&gt;875&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;8.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Mini-Omni&lt;/td&gt; 
     &lt;td&gt;1B&lt;/td&gt; 
     &lt;td&gt;22.0&lt;/td&gt; 
     &lt;td&gt;12.8&lt;/td&gt; 
     &lt;td&gt;6.9&lt;/td&gt; 
     &lt;td&gt;2.5&lt;/td&gt; 
     &lt;td&gt;926&lt;/td&gt; 
     &lt;td&gt;803&lt;/td&gt; 
     &lt;td&gt;865&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;10.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;61.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1088&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1163&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1131&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;9.8&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; All results are from AudioEvals, and the evaluation methods along with further details can be found in 
 &lt;a href="https://github.com/OpenBMB/UltraEval-Audio" target="_blank"&gt;AudioEvals&lt;/a&gt;.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;End-to-end Voice Cloning&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th colspan="2"&gt;Voice cloning&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;SIMO↑&lt;/th&gt; 
     &lt;th&gt;SIMO↑&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-zh&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-en&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;F5-TTS&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;76&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;CosyVoice&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;75&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;FireRedTTS&lt;/td&gt; 
     &lt;td&gt;63&lt;/td&gt; 
     &lt;td&gt;46&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;57&lt;/td&gt; 
     &lt;td&gt;47&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view multimodal live streaming results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Multimodal Live Streaming&lt;/strong&gt;: results on StreamingBench&lt;/p&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Real-Time Video Understanding&lt;/th&gt; 
    &lt;th&gt;Omni-Source Understanding&lt;/th&gt; 
    &lt;th&gt;Contextual Understanding&lt;/th&gt; 
    &lt;th&gt;Overall&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td colspan="7" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;77.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.1&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GPT-4o-202408&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.5&lt;/td&gt; 
    &lt;td&gt;51.0&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;48.0&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;64.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Claude-3.5-Sonnet&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.0&lt;/td&gt; 
    &lt;td&gt;41.4&lt;/td&gt; 
    &lt;td&gt;37.8&lt;/td&gt; 
    &lt;td&gt;59.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="9" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VILA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;37.5&lt;/td&gt; 
    &lt;td&gt;26.7&lt;/td&gt; 
    &lt;td&gt;49.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LongVA&lt;/td&gt; 
    &lt;td&gt;7B&lt;/td&gt; 
    &lt;td&gt;63.1&lt;/td&gt; 
    &lt;td&gt;35.9&lt;/td&gt; 
    &lt;td&gt;30.2&lt;/td&gt; 
    &lt;td&gt;50.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-Next-Video-34B&lt;/td&gt; 
    &lt;td&gt;34B&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;41.7&lt;/td&gt; 
    &lt;td&gt;34.3&lt;/td&gt; 
    &lt;td&gt;56.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;71.2&lt;/td&gt; 
    &lt;td&gt;40.7&lt;/td&gt; 
    &lt;td&gt;33.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternVL2-8B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.1&lt;/td&gt; 
    &lt;td&gt;42.7&lt;/td&gt; 
    &lt;td&gt;34.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.9&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;35.8&lt;/td&gt; 
    &lt;td&gt;57.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-OneVision-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;74.3&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;31.0&lt;/td&gt; 
    &lt;td&gt;58.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternLM-XC2.5-OL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;75.4&lt;/td&gt; 
    &lt;td&gt;46.2&lt;/td&gt; 
    &lt;td&gt;33.6&lt;/td&gt; 
    &lt;td&gt;60.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;72.4&lt;/td&gt; 
    &lt;td&gt;40.2&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;53.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;38.5&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;66.0&lt;/u&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We deploy MiniCPM-o 2.6 on end devices. The demo video is the raw-speed recording on an iPad Pro and a Web demo.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=vRIMbxJzStY&amp;amp;t=2s"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/2dot6_o_demo_video_img.png" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_math_intersect.png" alt="math" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_diagram_train_NN.png" alt="diagram" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_multi-image_bike.png" alt="bike" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Legacy Models 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Introduction and Guidance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v4_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2dot6_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-Llama3-V 2.5&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_llama3_v2dot5.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 1.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v1.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;OmniLMM-12B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/omnilmm_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/h2&gt; 
&lt;p&gt;Discover comprehensive, ready-to-deploy solutions for the MiniCPM-V and MiniCPM-o model series in our structured &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;cookbook&lt;/a&gt;, which empowers developers to rapidly implement multimodal AI applications with integrated vision, speech, and live-streaming capabilities. Key features include:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Easy Usage Documentation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our comprehensive &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;documentation website&lt;/a&gt; presents every recipe in a clear, well-organized manner. All features are displayed at a glance, making it easy for you to quickly find exactly what you need.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Broad User Spectrum&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We support a wide range of users, from individuals to enterprises and researchers.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Individuals&lt;/strong&gt;: Enjoy effortless inference using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/ollama/minicpm-v4_ollama.md"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/llama.cpp/minicpm-v4_llamacpp.md"&gt;Llama.cpp&lt;/a&gt; with minimal setup.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprises&lt;/strong&gt;: Achieve high-throughput, scalable performance with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/vllm/minicpm-v4_vllm.md"&gt;vLLM&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/sglang/MiniCPM-v4_sglang.md"&gt;SGLang&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Researchers&lt;/strong&gt;: Leverage advanced frameworks including &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_full.md"&gt;Transformers&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_llamafactory.md"&gt;LLaMA-Factory&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/swift.md"&gt;SWIFT&lt;/a&gt;, and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/align_anything.md"&gt;Align-anything&lt;/a&gt; to enable flexible model development and cutting-edge experimentation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Versatile Deployment Scenarios&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our ecosystem delivers optimal solution for a variety of hardware environments and deployment demands.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Web demo&lt;/strong&gt;: Launch interactive multimodal AI web demo with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/README.md"&gt;FastAPI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quantized deployment&lt;/strong&gt;: Maximize efficiency and minimize resource consumption using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/gguf/minicpm-v4_gguf_quantize.md"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/bnb/minicpm-v4_bnb_quantize.md"&gt;BNB&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;End devices&lt;/strong&gt;: Bring powerful AI experiences to &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/ios_demo/ios.md"&gt;iPhone and iPad&lt;/a&gt;, supporting offline and privacy-sensitive applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Chat with Our Demo on Gradio 🤗&lt;/h2&gt; 
&lt;p&gt;We provide online and local demos powered by Hugging Face Gradio &lt;a href="https://github.com/gradio-app/gradio"&gt;&lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;, the most popular model deployment framework nowadays. It supports streaming outputs, progress bars, queuing, alerts, and other useful features.&lt;/p&gt; 
&lt;h3&gt;Online Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;Click here to try out the online demo of &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;MiniCPM-o 2.6&lt;/a&gt; | &lt;a href="http://120.92.209.146:8887/"&gt;MiniCPM-V 2.6&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;MiniCPM-Llama3-V 2.5&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;MiniCPM-V 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Local WebUI Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily build your own local WebUI demo using the following commands.&lt;/p&gt; 
&lt;p&gt;Please ensure that &lt;code&gt;transformers==4.44.2&lt;/code&gt; is installed, as other versions may have compatibility issues.&lt;/p&gt; 
&lt;p&gt;If you are using an older version of PyTorch, you might encounter this issue &lt;code&gt;"weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16'&lt;/code&gt;, Please add &lt;code&gt;self.minicpmo_model.tts.float()&lt;/code&gt; during the model initialization.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For real-time voice/video call demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;launch model server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/model_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;launch web server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Make sure Node and PNPM is installed.
sudo apt-get update
sudo apt-get install nodejs npm
npm install -g pnpm


cd web_demos/minicpm-o_2.6/web_server
# create ssl cert for https, https is required to request camera and microphone permissions.
bash ./make_ssl_cert.sh  # output key.pem and cert.pem

pnpm install  # install requirements
pnpm run dev  # start server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;https://localhost:8088/&lt;/code&gt; in browser and enjoy the real-time voice/video call.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For chatbot demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/chatbot_web_demo_o2.6.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;http://localhost:8000/&lt;/code&gt; in browser and enjoy the vision mode chatbot.&lt;/p&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;h3&gt;Model Zoo&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Device&lt;/th&gt; 
   &lt;th align="center"&gt;Memory&lt;/th&gt; 
   &lt;th align="left"&gt;         Description&lt;/th&gt; 
   &lt;th align="center"&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, strong end-side multimodal performance for single image, multi-image and video understanding.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 AWQ&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-AWQ"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-AWQ"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, achieving GPT-4o level performance for vision, speech and multimodal live streaming on end-side devices.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Multi-turn Conversation&lt;/h3&gt; 
&lt;p&gt;If you wish to enable long-thinking mode, provide the argument &lt;code&gt;enable_thinking=True&lt;/code&gt; to the chat function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to the following codes to run.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/show_demo.jpg" width="500px" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

torch.manual_seed(100)

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True) # or openbmb/MiniCPM-o-2_6

image = Image.open('./assets/minicpmo2_6/show_demo.jpg').convert('RGB')

enable_thinking=False # If `enable_thinking=True`, the long-thinking mode is enabled.

# First round chat 
question = "What is the landform in the picture?"
msgs = [{'role': 'user', 'content': [image, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    enable_thinking=enable_thinking
)
print(answer)

# Second round chat, pass history context of multi-turn conversation
msgs.append({"role": "assistant", "content": [answer]})
msgs.append({"role": "user", "content": ["What should I pay attention to when traveling here?"]})

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will get the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# round1
The landform in the picture is karst topography. Karst landscapes are characterized by distinctive, jagged limestone hills or mountains with steep, irregular peaks and deep valleys—exactly what you see here These unique formations result from the dissolution of soluble rocks like limestone over millions of years through water erosion.

This scene closely resembles the famous karst landscape of Guilin and Yangshuo in China’s Guangxi Province. The area features dramatic, pointed limestone peaks rising dramatically above serene rivers and lush green forests, creating a breathtaking and iconic natural beauty that attracts millions of visitors each year for its picturesque views.

# round2
When traveling to a karst landscape like this, here are some important tips:

1. Wear comfortable shoes: The terrain can be uneven and hilly.
2. Bring water and snacks for energy during hikes or boat rides.
3. Protect yourself from the sun with sunscreen, hats, and sunglasses—especially since you’ll likely spend time outdoors exploring scenic spots.
4. Respect local customs and nature regulations by not littering or disturbing wildlife.

By following these guidelines, you'll have a safe and enjoyable trip while appreciating the stunning natural beauty of places such as Guilin’s karst mountains.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Chat with Multiple Images&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with multiple images input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

image1 = Image.open('image1.jpg').convert('RGB')
image2 = Image.open('image2.jpg').convert('RGB')
question = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'

msgs = [{'role': 'user', 'content': [image1, image2, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;In-context Few-shot Learning&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with few-shot input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

question = "production date" 
image1 = Image.open('example1.jpg').convert('RGB')
answer1 = "2023.08.04"
image2 = Image.open('example2.jpg').convert('RGB')
answer2 = "2007.04.24"
image_test = Image.open('test.jpg').convert('RGB')

msgs = [
    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},
    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},
    {'role': 'user', 'content': [image_test, question]}
]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Chat with Video&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 by with video input and 3D-Resampler. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;## The 3d-resampler compresses multiple frames into 64 tokens by introducing temporal_ids. 
# To achieve this, you need to organize your video data into two corresponding sequences: 
#   frames: List[Image]
#   temporal_ids: List[List[Int]].

import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from decord import VideoReader, cpu    # pip install decord
from scipy.spatial import cKDTree
import numpy as np
import math

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

MAX_NUM_FRAMES=180 # Indicates the maximum number of frames received after the videos are packed. The actual maximum number of valid frames is MAX_NUM_FRAMES * MAX_NUM_PACKING.
MAX_NUM_PACKING=3  # indicates the maximum packing number of video frames. valid range: 1-6
TIME_SCALE = 0.1 

def map_to_nearest_scale(values, scale):
    tree = cKDTree(np.asarray(scale)[:, None])
    _, indices = tree.query(np.asarray(values)[:, None])
    return np.asarray(scale)[indices]


def group_array(arr, size):
    return [arr[i:i+size] for i in range(0, len(arr), size)]

def encode_video(video_path, choose_fps=3, force_packing=None):
    def uniform_sample(l, n):
        gap = len(l) / n
        idxs = [int(i * gap + gap / 2) for i in range(n)]
        return [l[i] for i in idxs]
    vr = VideoReader(video_path, ctx=cpu(0))
    fps = vr.get_avg_fps()
    video_duration = len(vr) / fps
        
    if choose_fps * int(video_duration) &amp;lt;= MAX_NUM_FRAMES:
        packing_nums = 1
        choose_frames = round(min(choose_fps, round(fps)) * min(MAX_NUM_FRAMES, video_duration))
        
    else:
        packing_nums = math.ceil(video_duration * choose_fps / MAX_NUM_FRAMES)
        if packing_nums &amp;lt;= MAX_NUM_PACKING:
            choose_frames = round(video_duration * choose_fps)
        else:
            choose_frames = round(MAX_NUM_FRAMES * MAX_NUM_PACKING)
            packing_nums = MAX_NUM_PACKING

    frame_idx = [i for i in range(0, len(vr))]      
    frame_idx =  np.array(uniform_sample(frame_idx, choose_frames))

    if force_packing:
        packing_nums = min(force_packing, MAX_NUM_PACKING)
    
    print(video_path, ' duration:', video_duration)
    print(f'get video frames={len(frame_idx)}, packing_nums={packing_nums}')
    
    frames = vr.get_batch(frame_idx).asnumpy()

    frame_idx_ts = frame_idx / fps
    scale = np.arange(0, video_duration, TIME_SCALE)

    frame_ts_id = map_to_nearest_scale(frame_idx_ts, scale) / TIME_SCALE
    frame_ts_id = frame_ts_id.astype(np.int32)

    assert len(frames) == len(frame_ts_id)

    frames = [Image.fromarray(v.astype('uint8')).convert('RGB') for v in frames]
    frame_ts_id_group = group_array(frame_ts_id, packing_nums)
    
    return frames, frame_ts_id_group


video_path="video_test.mp4"
fps = 5 # fps for video
force_packing = None # You can set force_packing to ensure that 3D packing is forcibly enabled; otherwise, encode_video will dynamically set the packing quantity based on the duration.
frames, frame_ts_id_group = encode_video(video_path, fps, force_packing=force_packing)

question = "Describe the video"
msgs = [
    {'role': 'user', 'content': frames + [question]}, 
]


answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    use_image_id=False,
    max_slice_nums=1,
    temporal_ids=frame_ts_id_group
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Speech and Audio Mode&lt;/h4&gt; 
&lt;p&gt;Model initialization&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
import librosa
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()
model.tts.float()
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Mimick 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;Mimick&lt;/code&gt; task reflects a model's end-to-end speech modeling capability. The model takes audio input, and outputs an ASR transcription and subsequently reconstructs the original audio with high similarity. The higher the similarity between the reconstructed audio and the original audio, the stronger the model's foundational capability in end-to-end speech modeling.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mimick_prompt = "Please repeat each user's speech, including voice style and speech content."
audio_input, _ = librosa.load('./assets/input_examples/Trump_WEF_2018_10s.mp3', sr=16000, mono=True) # load the audio to be mimicked

# `./assets/input_examples/fast-pace.wav`, 
# `./assets/input_examples/chi-english-1.wav` 
# `./assets/input_examples/exciting-emotion.wav` 
# for different aspects of speech-centric features.

msgs = [{'role': 'user', 'content': [mimick_prompt, audio_input]}]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    temperature=0.3,
    generate_audio=True,
    output_audio_path='output_mimick.wav', # save the tts result to output_audio_path
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;General Speech Conversation with Configurable Voices 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;A general usage scenario of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is role-playing a specific character based on the audio prompt. It will mimic the voice of the character to some extent and act like the character in text, including language style. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; sounds &lt;strong&gt;more natural and human-like&lt;/strong&gt;. Self-defined audio prompts can be used to customize the voice of the character in an end-to-end manner.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_roleplay', language='en')

# round one
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Speech Conversation as an AI Assistant 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;An enhanced feature of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is to act as an AI assistant, but only with limited choice of voices. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is &lt;strong&gt;less human-like and more like a voice assistant&lt;/strong&gt;. In this mode, the model is more instruction-following. For demo, you are suggested to use &lt;code&gt;assistant_female_voice&lt;/code&gt;, &lt;code&gt;assistant_male_voice&lt;/code&gt;, and &lt;code&gt;assistant_default_female_voice&lt;/code&gt;. Other voices may work but not as stable as the default voices.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please note that, &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt; are more stable but sounds like robots, while &lt;code&gt;assistant_default_female_voice&lt;/code&gt; is more human-alike but not stable, its voice often changes in multiple turns. We suggest you to try stable voices &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/assistant_female_voice.wav', sr=16000, mono=True) # or use `./assets/input_examples/assistant_male_voice.wav`
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_assistant', language='en') 
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]} # load the user's audio question

# round one
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Instruction-to-Speech 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do Instruction-to-Speech, aka &lt;strong&gt;Voice Creation&lt;/strong&gt;. You can describe a voice in detail, and the model will generate a voice that matches the description. For more Instruction-to-Speech sample instructions, you can refer to &lt;a href="https://voxinstruct.github.io/VoxInstruct/"&gt;https://voxinstruct.github.io/VoxInstruct/&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;instruction = 'Speak like a male charming superstar, radiating confidence and style in every word.'

msgs = [{'role': 'user', 'content': [instruction]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_creation.wav',
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Voice Cloning 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do zero-shot text-to-speech, aka &lt;strong&gt;Voice Cloning&lt;/strong&gt;. With this mode, model will act like a TTS model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='voice_cloning', language='en')
text_prompt = f"Please read the text below."
user_question = {'role': 'user', 'content': [text_prompt, "content that you want to read"]}

msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_cloning.wav',
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Addressing Various Audio Understanding Tasks 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also be used to address various audio understanding tasks, such as ASR, speaker analysis, general audio captioning, and sound scene tagging.&lt;/p&gt; 
&lt;p&gt;For audio-to-text tasks, you can use the following prompts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ASR with ZH(same as AST en2zh): &lt;code&gt;请仔细听这段音频片段，并将其内容逐字记录。&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ASR with EN(same as AST zh2en): &lt;code&gt;Please listen to the audio snippet carefully and transcribe the content.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Speaker Analysis: &lt;code&gt;Based on the speaker's content, speculate on their gender, condition, age range, and health status.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Audio Caption: &lt;code&gt;Summarize the main content of the audio.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Sound Scene Tagging: &lt;code&gt;Utilize one keyword to convey the audio's content or the associated scene.&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;task_prompt = "Please listen to the audio snippet carefully and transcribe the content." + "\n" # can change to other prompts.
audio_input, _ = librosa.load('./assets/input_examples/audio_understanding.mp3', sr=16000, mono=True) # load the audio to be captioned

msgs = [{'role': 'user', 'content': [task_prompt, audio_input]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_audio_understanding.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Multimodal Live Streaming&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with chat inference. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import math
import numpy as np
from PIL import Image
from moviepy.editor import VideoFileClip
import tempfile
import librosa
import soundfile as sf
import torch
from transformers import AutoModel, AutoTokenizer

def get_video_chunk_content(video_path, flatten=True):
    video = VideoFileClip(video_path)
    print('video_duration:', video.duration)
    
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as temp_audio_file:
        temp_audio_file_path = temp_audio_file.name
        video.audio.write_audiofile(temp_audio_file_path, codec="pcm_s16le", fps=16000)
        audio_np, sr = librosa.load(temp_audio_file_path, sr=16000, mono=True)
    num_units = math.ceil(video.duration)
    
    # 1 frame + 1s audio chunk
    contents= []
    for i in range(num_units):
        frame = video.get_frame(i+1)
        image = Image.fromarray((frame).astype(np.uint8))
        audio = audio_np[sr*i:sr*(i+1)]
        if flatten:
            contents.extend(["&amp;lt;unit&amp;gt;", image, audio])
        else:
            contents.append(["&amp;lt;unit&amp;gt;", image, audio])
    
    return contents


model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16)
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()

# If you are using an older version of PyTorch, you might encounter this issue "weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16', Please convert the TTS to float32 type.
# model.tts.float()

# https://huggingface.co/openbmb/MiniCPM-o-2_6/blob/main/assets/Skiing.mp4
video_path="assets/Skiing.mp4"
sys_msg = model.get_sys_prompt(mode='omni', language='en')
# if use voice clone prompt, please set ref_audio
# ref_audio_path = '/path/to/ref_audio'
# ref_audio, _ = librosa.load(ref_audio_path, sr=16000, mono=True)
# sys_msg = model.get_sys_prompt(ref_audio=ref_audio, mode='omni', language='en')

contents = get_video_chunk_content(video_path)
msg = {"role":"user", "content": contents}
msgs = [sys_msg, msg]

# please set generate_audio=True and output_audio_path to save the tts result
generate_audio = True
output_audio_path = 'output.wav'

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.5,
    max_new_tokens=4096,
    omni_input=True, # please set omni_input=True when omni inference
    use_tts_template=True,
    generate_audio=generate_audio,
    output_audio_path=output_audio_path,
    max_slice_nums=1,
    use_image_id=False,
    return_dict=True
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with streaming inference. &lt;/summary&gt; 
 &lt;p&gt;Note: The streaming inference has a slight performance degradation because the audio encoding is not global.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# a new conversation need reset session first, it will reset the kv-cache
model.reset_session()

contents = get_video_chunk_content(video_path, flatten=False)
session_id = '123'
generate_audio = True

# 1. prefill system prompt
res = model.streaming_prefill(
    session_id=session_id,
    msgs=[sys_msg], 
    tokenizer=tokenizer
)

# 2. prefill video/audio chunks
for content in contents:
    msgs = [{"role":"user", "content": content}]
    res = model.streaming_prefill(
        session_id=session_id,
        msgs=msgs, 
        tokenizer=tokenizer
    )

# 3. generate
res = model.streaming_generate(
    session_id=session_id,
    tokenizer=tokenizer,
    temperature=0.5,
    generate_audio=generate_audio
)

audios = []
text = ""

if generate_audio:
    for r in res:
        audio_wav = r.audio_wav
        sampling_rate = r.sampling_rate
        txt = r.text

        audios.append(audio_wav)
        text += txt
        
    res = np.concatenate(audios)
    sf.write("output.wav", res, samplerate=sampling_rate)
    print("text:", text)
    print("audio saved to output.wav")
else:
    for r in res:
        text += r['text']
    print("text:", text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Inference on Multiple GPUs&lt;/h3&gt; 
&lt;p&gt;You can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs (12 GB or 16 GB) by distributing the model's layers across multiple GPUs. Please refer to this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;tutorial&lt;/a&gt; for detailed instructions on how to load the model and inference using multiple low VRAM GPUs.&lt;/p&gt; 
&lt;h3&gt;Inference on Mac&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view an example, to run MiniCPM-Llama3-V 2.5 on 💻 Mac with MPS (Apple silicon or AMD GPUs). &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# test.py  Need more than 16GB memory.
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, low_cpu_mem_usage=True)
model = model.to(device='mps')

tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)
model.eval()

image = Image.open('./assets/hk_OCR.jpg').convert('RGB')
question = 'Where is this photo taken?'
msgs = [{'role': 'user', 'content': question}]

answer, context, _ = model.chat(
    image=image,
    msgs=msgs,
    context=None,
    tokenizer=tokenizer,
    sampling=True
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run with command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/llama.cpp/tree/minicpmv-main/examples/llava/README-minicpmv2.6.md"&gt;our fork of llama.cpp&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environment：iPad Pro + M4).&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;our fork of Ollama&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environment：iPad Pro + M4).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; vLLM now officially supports MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0. And you can use our fork to run MiniCPM-o 2.6 for now. Click to see. &lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install vLLM(&amp;gt;=0.7.1):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Run Example:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/vision_language.html"&gt;Vision Language&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/audio_language.html"&gt;Audio Language&lt;/a&gt; &lt;/li&gt;
 &lt;/ul&gt;
&lt;/details&gt;   
&lt;h2&gt;Fine-tuning&lt;/h2&gt; 
&lt;h3&gt;Simple Fine-tuning 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support simple fine-tuning with Hugging Face for MiniCPM-o 2.6, MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;Reference Document&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;With Align-Anything 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 by PKU-Alignment Team (both vision and audio, SFT and DPO) with the &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything framework&lt;/a&gt;. Align-Anything is a scalable framework that aims to align any-modality large models with human intentions, open-sourcing the &lt;a href="https://huggingface.co/datasets/PKU-Alignment/align-anything"&gt;datasets, models and benchmarks&lt;/a&gt;. Benefiting from its concise and modular design, it supports 30+ open-source benchmarks, 40+ models and algorithms including SFT, SimPO, RLHF, &lt;em&gt;etc&lt;/em&gt;. It also provides 30+ directly runnable scripts, making it suitable for beginners to quickly get started.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://github.com/PKU-Alignment/align-anything/tree/main/scripts"&gt;MiniCPM-o 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With LLaMA-Factory 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 and MiniCPM-V 2.6 with the LLaMA-Factory framework. LLaMA-Factory provides a solution for flexibly customizing the fine-tuning (Lora/Full/Qlora) of 200+ LLMs without the need for coding through the built-in web UI LLaMABoard. It supports various training methods like sft/ppo/dpo/kto and advanced algorithms like Galore/BAdam/LLaMA-Pro/Pissa/LongLoRA.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;MiniCPM-o 2.6 | MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With the SWIFT Framework 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We now support MiniCPM-V series fine-tuning with the SWIFT framework. SWIFT supports training, inference, evaluation and deployment of nearly 200 LLMs and MLLMs . It supports the lightweight training solutions provided by PEFT and a complete Adapters Library including techniques such as NEFTune, LoRA+ and LLaMA-PRO.&lt;/p&gt; 
&lt;p&gt;Best Practices：&lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 1.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 2.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CatchTheTornado/text-extract-api"&gt;text-extract-api&lt;/a&gt;: Document extraction API using OCRs and Ollama supported models &lt;img src="https://img.shields.io/github/stars/CatchTheTornado/text-extract-api" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heshengtao/comfyui_LLM_party"&gt;comfyui_LLM_party&lt;/a&gt;: Build LLM workflows and integrate into existing image workflows &lt;img src="https://img.shields.io/github/stars/heshengtao/comfyui_LLM_party" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;Ollama-OCR&lt;/a&gt;: OCR package uses vlms through Ollama to extract text from images and PDF &lt;img src="https://img.shields.io/github/stars/imanoop7/Ollama-OCR" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MixLabPro/comfyui-mixlab-nodes"&gt;comfyui-mixlab-nodes&lt;/a&gt;: ComfyUI node suite supports Workflow-to-APP、GPT&amp;amp;3D and more &lt;img src="https://img.shields.io/github/stars/MixLabPro/comfyui-mixlab-nodes" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HumanAIGC-Engineering/OpenAvatarChat"&gt;OpenAvatarChat&lt;/a&gt;: Interactive digital human conversation implementation on single PC &lt;img src="https://img.shields.io/github/stars/HumanAIGC-Engineering/OpenAvatarChat" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arkohut/pensieve"&gt;pensieve&lt;/a&gt;: A privacy-focused passive recording project by recording screen content &lt;img src="https://img.shields.io/github/stars/arkohut/pensieve" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/icereed/paperless-gpt"&gt;paperless-gpt&lt;/a&gt;: Use LLMs to handle paperless-ngx, AI-powered titles, tags and OCR &lt;img src="https://img.shields.io/github/stars/icereed/paperless-gpt" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kimjammer/Neuro"&gt;Neuro&lt;/a&gt;: A recreation of Neuro-Sama, but running on local models on consumer hardware &lt;img src="https://img.shields.io/github/stars/kimjammer/Neuro" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQs&lt;/h2&gt; 
&lt;p&gt;Click here to view the &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/faqs.md"&gt;FAQs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Limitations&lt;/h2&gt; 
&lt;p&gt;As an experimental trial, we find MiniCPM-o 2.6 has notable limitations worth further investigation and improvement.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Unstable speech output.&lt;/strong&gt; The speech generation can be flawed with noisy backgrounds and unmeaningful sounds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Repeated response.&lt;/strong&gt; The model tends to repeat its response when encountering similar consecutive user queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-latency on Web Demo.&lt;/strong&gt; Users may experience unusual high-latency when using web demo hosted on overseas servers. We recommend deploying the demo locally or with good network connections.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model License 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;This repository is released under the &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; License.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The usage of MiniCPM-o/V model weights must strictly follow &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md"&gt;MiniCPM Model License.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The models and weights of MiniCPM are completely free for academic research. after filling out a &lt;a href="https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g"&gt;"questionnaire"&lt;/a&gt; for registration, are also available for free commercial use.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Statement 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;As MLLMs, MiniCPM-o/V models generate content by learning a large number of multimodal corpora, but they cannot comprehend, express personal opinions, or make value judgements. Anything generated by MiniCPM-o/V models does not represent the views and positions of the model developers&lt;/p&gt; 
&lt;p&gt;We will not be liable for any problems arising from the use of MiniCPM-o/V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination, or misuse of the model.&lt;/p&gt; 
&lt;h2&gt;Institutions 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png" width="28px" /&gt; &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png" width="28px" /&gt; &lt;a href="https://modelbest.cn/"&gt;ModelBest&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🌟 Star History 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;!-- &lt;table align="center"&gt;
    &lt;p align="center"&gt;
      &lt;img src="assets/star_history.svg"/&gt;
    &lt;/p&gt;
&lt;/table&gt; --&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;amp;type=Date&amp;amp;theme=dark
    " /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;amp;type=Date
    " /&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h2&gt;Key Techniques and Other Multimodal Projects 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;👏 Welcome to explore key techniques of MiniCPM-o/V and other multimodal projects of our team:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/OpenBMB/VisCPM/tree/main"&gt;VisCPM&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLHF-V"&gt;RLHF-V&lt;/a&gt; | &lt;a href="https://github.com/thunlp/LLaVA-UHD"&gt;LLaVA-UHD&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;If you find our model/code/paper helpful, please consider citing our papers 📝 and staring us ⭐️！&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@article{yao2024minicpm,
  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;img width="1200" alt="cb-big2" src="https://github.com/user-attachments/assets/bd8c5f03-e91d-4ee5-b680-57355da204d1" /&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;&lt;img src="https://img.shields.io/badge/listen-demo_samples-blue" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;&lt;img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://podonos.com/resembleai/chatterbox"&gt;&lt;img src="https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;&lt;img src="https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with ♥️ by &lt;a href="https://resemble.ai" target="_blank"&gt;&lt;img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We're excited to introduce Chatterbox, &lt;a href="https://resemble.ai"&gt;Resemble AI's&lt;/a&gt; first production-grade open source TTS model. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.&lt;/p&gt; 
&lt;p&gt;Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life. It's also the first open source TTS model to support &lt;strong&gt;emotion exaggeration control&lt;/strong&gt;, a powerful feature that makes your voices stand out. Try it now on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;Hugging Face Gradio app.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href="https://resemble.ai"&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms—ideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;h1&gt;Key Details&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;SoTA zeroshot TTS&lt;/li&gt; 
 &lt;li&gt;0.5B Llama backbone&lt;/li&gt; 
 &lt;li&gt;Unique exaggeration/intensity control&lt;/li&gt; 
 &lt;li&gt;Ultra-stable with alignment-informed inference&lt;/li&gt; 
 &lt;li&gt;Trained on 0.5M hours of cleaned data&lt;/li&gt; 
 &lt;li&gt;Watermarked outputs&lt;/li&gt; 
 &lt;li&gt;Easy voice conversion script&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://podonos.com/resembleai/chatterbox"&gt;Outperforms ElevenLabs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Tips&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debain 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torchaudio as ta
from chatterbox.tts import ChatterboxTTS

model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-1.wav", wav, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h1&gt;Supported Lanugage&lt;/h1&gt; 
&lt;p&gt;Currenlty only English.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yl4579/HiFTNet"&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xingchensong/S3Tokenizer"&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h1&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Resemble AI's Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Official Discord&lt;/h1&gt; 
&lt;p&gt;👋 Join us on &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;Discord&lt;/a&gt; and let's build something awesome together!&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>activepieces/activepieces</title>
      <link>https://github.com/activepieces/activepieces</link>
      <description>&lt;p&gt;AI Agents &amp; MCPs &amp; AI Workflow Automation • (280+ MCP servers for AI agents) • AI Automation / AI Agent with MCPs • AI Workflows &amp; AI Agents • MCPs for AI Agents&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a target="_blank" href="https://activepieces.com"&gt; &lt;img align="center" alt="Activepieces" src="https://github.com/activepieces/activepieces/assets/1812998/76c97441-c285-4480-bc75-30a0c73ed340" style="width:100%;" /&gt; &lt;/a&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/LICENSE" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-green?style=for-the-badge" /&gt;&lt;/a&gt;&amp;nbsp;&lt;img src="https://img.shields.io/github/commit-activity/w/activepieces/activepieces/main?style=for-the-badge" /&gt;&amp;nbsp;&lt;a href="https://discord.gg/2jUXBKDdP8"&gt;&lt;img src="https://img.shields.io/discord/966798490984382485?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; An open source replacement for Zapier &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.activepieces.com/docs" target="_blank"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;🌪️&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://www.activepieces.com/docs/developers/overview" target="_blank"&gt;&lt;b&gt;Create a Piece&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;🖉&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://www.activepieces.com/docs/install/overview" target="_blank"&gt;&lt;b&gt;Deploy&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;🔥&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://discord.gg/yvxF5k5AUb" target="_blank"&gt; &lt;b&gt;Join Discord&lt;/b&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h1&gt;🤯 Welcome to Activepieces&lt;/h1&gt; 
&lt;p&gt;All-in-one AI automation designed to be &lt;strong&gt;extensible&lt;/strong&gt; through a &lt;strong&gt;type-safe&lt;/strong&gt; pieces framework written in &lt;strong&gt;TypeScript&lt;/strong&gt;. When you contribute pieces to Activepieces they become automatically available as MCP servers that you can use with LLMs through Claude Desktop, Cursor or Windsurf!&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h2&gt;🔥 Why Activepieces is Different:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;💖 Loved by Everyone&lt;/strong&gt;: Intuitive interface and great experience for both technical and non-technical users with a quick learning curve.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/activepieces/activepieces/main/docs/resources/templates.gif" /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🌐 Open Ecosystem:&lt;/strong&gt; All pieces are open source and available on npmjs.com, &lt;strong&gt;60% of the pieces are contributed by the community&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🛠️ Largest open source MCP toolkit&lt;/strong&gt;: All our pieces (280+) are available as MCP that you can use with LLMs on Claude Desktop, Cursor or Windsurf.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🛠️ Pieces are written in Typescript&lt;/strong&gt;: Pieces are npm packages in TypeScript, offering full customization with the best developer experience, including &lt;strong&gt;hot reloading&lt;/strong&gt; for &lt;strong&gt;local&lt;/strong&gt; piece development on your machine. 😎&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/activepieces/activepieces/main/docs/resources/create-action.png" alt="" /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🤖 AI-First&lt;/strong&gt;: Native AI pieces let you experiment with various providers, or create your own agents using our AI SDK, and there is Copilot to help you build flows inside the builder.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🏢 Enterprise-Ready&lt;/strong&gt;: Developers set up the tools, and anyone in the organization can use the no-code builder. Full customization from branding to control.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔒 Secure by Design&lt;/strong&gt;: Self-hosted and network-gapped for maximum security and control over your data.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🧠 Human in the Loop&lt;/strong&gt;: Delay execution for a period of time or require approval. These are just pieces built on top of the piece framework, and you can build many pieces like that. 🎨&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;💻 Human Input Interfaces&lt;/strong&gt;: Built-in support for human input triggers like "Chat Interface" 💬 and "Form Interface" 📝&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🛠️ Builder Features:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Loops&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Branches&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Auto Retries&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; HTTP&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Code with &lt;strong&gt;NPM&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ASK AI in Code Piece (Non technical user can clean data without knowing to code)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Flows are fully versioned.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Languages Translations&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Customizable Templates&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 200+ Pieces, check &lt;a href="https://www.activepieces.com/pieces"&gt;https://www.activepieces.com/pieces&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;We release updates frequently. Check the product changelog for the latest features.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;🔌 Create Your Own Piece&lt;/h2&gt; 
&lt;p&gt;Activepieces supports integrations with Google Sheets, OpenAI, Discord, RSS, and over 200 other services. &lt;a href="https://www.activepieces.com/pieces"&gt;Check out the full list of supported integrations&lt;/a&gt;, which is constantly expanding thanks to our community's contributions.&lt;/p&gt; 
&lt;p&gt;As an &lt;strong&gt;open ecosystem&lt;/strong&gt;, all integration source code is accessible in our repository. These integrations are versioned and &lt;a href="https://www.npmjs.com/search?q=%40activepieces"&gt;published&lt;/a&gt; directly to npmjs.com upon contribution.&lt;/p&gt; 
&lt;p&gt;You can easily create your own integration using our TypeScript framework. For detailed instructions, please refer to our &lt;a href="https://www.activepieces.com/docs/contributing/overview"&gt;Contributor's Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Activepieces' Community Edition is released as open source under the &lt;a href="https://github.com/activepieces/activepieces/raw/main/LICENSE"&gt;MIT license&lt;/a&gt; and enterprise features are released under &lt;a href="https://github.com/activepieces/activepieces/raw/main/packages/ee/LICENSE"&gt;Commercial License&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Read more about the feature comparison here &lt;a href="https://www.activepieces.com/docs/about/editions"&gt;https://www.activepieces.com/docs/about/editions&lt;/a&gt; &lt;br /&gt; &lt;br /&gt;&lt;/p&gt; 
&lt;h1&gt;💭 Join Our Community&lt;/h1&gt; 
&lt;a href="https://discord.gg/2jUXBKDdP8" target="_blank"&gt; &lt;img src="https://discordapp.com/api/guilds/966798490984382485/widget.png?style=banner3" alt="" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h1&gt;🌐 Contributions&lt;/h1&gt; 
&lt;p&gt;We welcome contributions big or small and in different directions. The best way to do this is to check this &lt;a href="https://www.activepieces.com/docs/contributing/building-pieces/create-action"&gt;document&lt;/a&gt; and we are always up to talk on &lt;a href="https://discord.gg/2jUXBKDdP8"&gt;our Discord Server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;📚 Translations&lt;/h2&gt; 
&lt;p&gt;Not into coding but still interested in contributing? Come join our &lt;a href="https://discord.gg/2jUXBKDdP8"&gt;Discord&lt;/a&gt; and visit &lt;a href="https://www.activepieces.com/docs/about/i18n"&gt;https://www.activepieces.com/docs/about/i18n&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?color=blue&amp;amp;label=fr&amp;amp;style=for-the-badge&amp;amp;logo=crowdin&amp;amp;query=%24.progress%5B?(@.data.languageId==%27fr%27)%5D.data.translationProgress&amp;amp;url=https%3A%2F%2Fbadges.awesome-crowdin.com%2Fstats-16093902-626364-update.json" alt="fr translation" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?color=blue&amp;amp;label=it&amp;amp;style=for-the-badge&amp;amp;logo=crowdin&amp;amp;query=%24.progress%5B?(@.data.languageId==%27it%27)%5D.data.translationProgress&amp;amp;url=https%3A%2F%2Fbadges.awesome-crowdin.com%2Fstats-16093902-626364-update.json" alt="it translation" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?color=blue&amp;amp;label=de&amp;amp;style=for-the-badge&amp;amp;logo=crowdin&amp;amp;query=%24.progress%5B?(@.data.languageId==%27de%27)%5D.data.translationProgress&amp;amp;url=https%3A%2F%2Fbadges.awesome-crowdin.com%2Fstats-16093902-626364-update.json" alt="de translation" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?color=blue&amp;amp;label=ja&amp;amp;style=for-the-badge&amp;amp;logo=crowdin&amp;amp;query=%24.progress%5B?(@.data.languageId==%27ja%27)%5D.data.translationProgress&amp;amp;url=https%3A%2F%2Fbadges.awesome-crowdin.com%2Fstats-16093902-626364-update.json" alt="ja translation" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?color=blue&amp;amp;label=pt-BR&amp;amp;style=for-the-badge&amp;amp;logo=crowdin&amp;amp;query=%24.progress%5B?(@.data.languageId==%27pt-BR%27)%5D.data.translationProgress&amp;amp;url=https%3A%2F%2Fbadges.awesome-crowdin.com%2Fstats-16093902-626364-update.json" alt="pt-BR translation" /&gt;&lt;/p&gt; 
&lt;h2&gt;🦫 Contributors&lt;/h2&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/ShahedAlMashni"&gt;&lt;img src="https://avatars.githubusercontent.com/u/41443850?v=4?s=100" width="100px;" alt="ShahedAlMashni" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ShahedAlMashni&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-ShahedAlMashni" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/AbdulTheActivePiecer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/106555838?v=4?s=100" width="100px;" alt="AbdulTheActivePiecer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AbdulTheActivePiecer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#maintenance-AbdulTheActivePiecer" title="Maintenance"&gt;🚧&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/khaledmashaly"&gt;&lt;img src="https://avatars.githubusercontent.com/u/61781545?v=4?s=100" width="100px;" alt="Khaled Mashaly" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Khaled Mashaly&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#maintenance-khaledmashaly" title="Maintenance"&gt;🚧&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/abuaboud"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1812998?v=4?s=100" width="100px;" alt="Mohammed Abu Aboud" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mohammed Abu Aboud&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#maintenance-abuaboud" title="Maintenance"&gt;🚧&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://aboudzein.github.io"&gt;&lt;img src="https://avatars.githubusercontent.com/u/12976630?v=4?s=100" width="100px;" alt="Abdulrahman Zeineddin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abdulrahman Zeineddin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-aboudzein" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/creed983"&gt;&lt;img src="https://avatars.githubusercontent.com/u/62152944?v=4?s=100" width="100px;" alt="ahmad jaber" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ahmad jaber&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-creed983" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/ashrafsamhouri"&gt;&lt;img src="https://avatars.githubusercontent.com/u/97393596?v=4?s=100" width="100px;" alt="ashrafsamhouri" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ashrafsamhouri&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-ashrafsamhouri" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://steercampaign.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/12627658?v=4?s=100" width="100px;" alt="Mohammad Abu Musa" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mohammad Abu Musa&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#projectManagement-mabumusa1" title="Project Management"&gt;📆&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/kanarelo"&gt;&lt;img src="https://avatars.githubusercontent.com/u/393261?v=4?s=100" width="100px;" alt="Mukewa Wekalao" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mukewa Wekalao&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-kanarelo" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://osamahaikal.me/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/72370395?v=4?s=100" width="100px;" alt="Osama Abdallah Essa Haikal" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Osama Abdallah Essa Haikal&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-OsamaHaikal" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/M-Arman"&gt;&lt;img src="https://avatars.githubusercontent.com/u/54455592?v=4?s=100" width="100px;" alt="Arman" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Arman&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#security-M-Arman" title="Security"&gt;🛡️&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/oskarkraemer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/42745862?v=4?s=100" width="100px;" alt="Oskar Krämer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Oskar Krämer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=oskarkraemer" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://thibpat.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/494686?v=4?s=100" width="100px;" alt="Thibaut Patel" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Thibaut Patel&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#ideas-tpatel" title="Ideas, Planning, &amp;amp; Feedback"&gt;🤔&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-tpatel" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Applesaucesomer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/18318905?v=4?s=100" width="100px;" alt="Applesaucesomer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Applesaucesomer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#ideas-Applesaucesomer" title="Ideas, Planning, &amp;amp; Feedback"&gt;🤔&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/crazyTweek"&gt;&lt;img src="https://avatars.githubusercontent.com/u/6828237?v=4?s=100" width="100px;" alt="crazyTweek" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;crazyTweek&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#ideas-crazyTweek" title="Ideas, Planning, &amp;amp; Feedback"&gt;🤔&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://linkedin.com/in/muhammad-tabaza"&gt;&lt;img src="https://avatars.githubusercontent.com/u/23503983?v=4?s=100" width="100px;" alt="Muhammad Tabaza" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Muhammad Tabaza&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-m-tabaza" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://shaypunter.co.uk"&gt;&lt;img src="https://avatars.githubusercontent.com/u/18310437?v=4?s=100" width="100px;" alt="Shay Punter" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shay Punter&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=ShayPunter" title="Documentation"&gt;📖&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-ShayPunter" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/abaza738"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50132270?v=4?s=100" width="100px;" alt="abaza738" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;abaza738&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-abaza738" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/jonaboe"&gt;&lt;img src="https://avatars.githubusercontent.com/u/51358680?v=4?s=100" width="100px;" alt="Jona Boeddinghaus" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jona Boeddinghaus&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-jonaboe" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/fomojola"&gt;&lt;img src="https://avatars.githubusercontent.com/u/264253?v=4?s=100" width="100px;" alt="fomojola" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;fomojola&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=fomojola" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/astorozhevsky"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11055414?v=4?s=100" width="100px;" alt="Alexander Storozhevsky" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alexander Storozhevsky&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=astorozhevsky" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/J0LGER"&gt;&lt;img src="https://avatars.githubusercontent.com/u/54769522?v=4?s=100" width="100px;" alt="J0LGER" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;J0LGER&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#security-J0LGER" title="Security"&gt;🛡️&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://about.me/veverkap"&gt;&lt;img src="https://avatars.githubusercontent.com/u/22348?v=4?s=100" width="100px;" alt="Patrick Veverka" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Patrick Veverka&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Aveverkap" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://berksmbl.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/10000339?v=4?s=100" width="100px;" alt="Berk Sümbül" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Berk Sümbül&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=berksmbl" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Willianwg"&gt;&lt;img src="https://avatars.githubusercontent.com/u/51550522?v=4?s=100" width="100px;" alt="Willian Guedes" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Willian Guedes&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Willianwg" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/abdullahranginwala"&gt;&lt;img src="https://avatars.githubusercontent.com/u/19731056?v=4?s=100" width="100px;" alt="Abdullah Ranginwala" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abdullah Ranginwala&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=abdullahranginwala" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/dentych"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2256372?v=4?s=100" width="100px;" alt="Dennis Tychsen" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Dennis Tychsen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-dentych" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/MyWay"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1765284?v=4?s=100" width="100px;" alt="MyWay" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;MyWay&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-MyWay" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/bibhuty-did-this"&gt;&lt;img src="https://avatars.githubusercontent.com/u/28416188?v=4?s=100" width="100px;" alt="Bibhuti Bhusan Panda" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bibhuti Bhusan Panda&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-bibhuty-did-this" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/tarunsamanta2k20"&gt;&lt;img src="https://avatars.githubusercontent.com/u/55488549?v=4?s=100" width="100px;" alt="Tarun Samanta" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tarun Samanta&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Atarunsamanta2k20" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.linkedin.com/in/herman-kudria-10868b207/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/9007211?v=4?s=100" width="100px;" alt="Herman Kudria" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Herman Kudria&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-HKudria" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://nulldev.imagefoo.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/66683380?v=4?s=100" width="100px;" alt="[NULL] Dev" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;[NULL] Dev&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Abdallah-Alwarawreh" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/JanHolger"&gt;&lt;img src="https://avatars.githubusercontent.com/u/25184957?v=4?s=100" width="100px;" alt="Jan Bebendorf" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jan Bebendorf&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-JanHolger" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://blog.nileshtrivedi.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/19304?v=4?s=100" width="100px;" alt="Nilesh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Nilesh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-nileshtrivedi" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://certopus.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/40790016?v=4?s=100" width="100px;" alt="Vraj Gohil" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vraj Gohil&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-VrajGohil" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/BastienMe"&gt;&lt;img src="https://avatars.githubusercontent.com/u/71411115?v=4?s=100" width="100px;" alt="BastienMe" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;BastienMe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-BastienMe" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://blog.fosketts.net"&gt;&lt;img src="https://avatars.githubusercontent.com/u/8627862?v=4?s=100" width="100px;" alt="Stephen Foskett" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Stephen Foskett&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=SFoskett" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://ganapati.fr"&gt;&lt;img src="https://avatars.githubusercontent.com/u/15729117?v=4?s=100" width="100px;" alt="Nathan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Nathan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=asuri0n" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.n-soft.pl"&gt;&lt;img src="https://avatars.githubusercontent.com/u/4056319?v=4?s=100" width="100px;" alt="Marcin Natanek" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marcin Natanek&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-mnatanek" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://all-tech-plus.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/23551912?v=4?s=100" width="100px;" alt="Mark van Bellen" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mark van Bellen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-buttonsbond" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://guzguz.fr"&gt;&lt;img src="https://avatars.githubusercontent.com/u/13715916?v=4?s=100" width="100px;" alt="Olivier Guzzi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Olivier Guzzi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-olivierguzzi" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Ozak93"&gt;&lt;img src="https://avatars.githubusercontent.com/u/31257994?v=4?s=100" width="100px;" alt="Osama Zakarneh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Osama Zakarneh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Ozak93" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/phestvik"&gt;&lt;img src="https://avatars.githubusercontent.com/u/88210985?v=4?s=100" width="100px;" alt="phestvik" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;phestvik&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#ideas-phestvik" title="Ideas, Planning, &amp;amp; Feedback"&gt;🤔&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://website-portfolio-bucket.s3-website-ap-northeast-1.amazonaws.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/113296626?v=4?s=100" width="100px;" alt="Rajdeep Pal" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rajdeep Pal&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=Rajdeep1311" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://www.tepote.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/40870?v=4?s=100" width="100px;" alt="Camilo Usuga" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Camilo Usuga&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-camilou" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/kishanprmr"&gt;&lt;img src="https://avatars.githubusercontent.com/u/135701940?v=4?s=100" width="100px;" alt="Kishan Parmar" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kishan Parmar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=kishanprmr" title="Documentation"&gt;📖&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-kishanprmr" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/BBND"&gt;&lt;img src="https://avatars.githubusercontent.com/u/42919338?v=4?s=100" width="100px;" alt="BBND" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;BBND&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-BBND" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/haseebrehmanpc"&gt;&lt;img src="https://avatars.githubusercontent.com/u/37938986?v=4?s=100" width="100px;" alt="Haseeb Rehman" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Haseeb Rehman&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-haseebrehmanpc" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.linkedin.com/in/ritagorokhod/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/60586879?v=4?s=100" width="100px;" alt="Rita Gorokhod" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rita Gorokhod&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-rita-gorokhod" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/facferreira"&gt;&lt;img src="https://avatars.githubusercontent.com/u/487349?v=4?s=100" width="100px;" alt="Fábio Ferreira" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fábio Ferreira&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-facferreira" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://buffetitservices.ch"&gt;&lt;img src="https://avatars.githubusercontent.com/u/73933252?v=4?s=100" width="100px;" alt="Florin Buffet" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Florin Buffet&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=FlorinBuffet" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Owlcept"&gt;&lt;img src="https://avatars.githubusercontent.com/u/67299472?v=4?s=100" width="100px;" alt="Drew Lewis" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Drew Lewis&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Owlcept" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://bendersej.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/10613140?v=4?s=100" width="100px;" alt="Benjamin André-Micolon" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Benjamin André-Micolon&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-bendersej" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/DGurskij"&gt;&lt;img src="https://avatars.githubusercontent.com/u/26856659?v=4?s=100" width="100px;" alt="Denis Gurskij" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Denis Gurskij&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-DGurskij" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://neferlopez.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11466949?v=4?s=100" width="100px;" alt="Nefer Lopez" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Nefer Lopez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=thatguynef" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/fardeenpanjwani-codeglo"&gt;&lt;img src="https://avatars.githubusercontent.com/u/141914308?v=4?s=100" width="100px;" alt="fardeenpanjwani-codeglo" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;fardeenpanjwani-codeglo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=fardeenpanjwani-codeglo" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/landonmoir"&gt;&lt;img src="https://avatars.githubusercontent.com/u/29764668?v=4?s=100" width="100px;" alt="Landon Moir" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Landon Moir&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-landonmoir" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://lightspeed-it.nl/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/22002313?v=4?s=100" width="100px;" alt="Diego Nijboer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Diego Nijboer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-lldiegon" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://ductan.me/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/24206229?v=4?s=100" width="100px;" alt="Tân Một Nắng" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tân Một Nắng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-tanoggy" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://geteduca.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/838788?v=4?s=100" width="100px;" alt="Gavin Foley" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Gavin Foley&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=GFoley83" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://dtrautwein.eu"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11836793?v=4?s=100" width="100px;" alt="Dennis Trautwein" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Dennis Trautwein&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Adennis-tra" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/inspiredclick"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1548613?v=4?s=100" width="100px;" alt="Andrew Rosenblatt" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrew Rosenblatt&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Ainspiredclick" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/w95"&gt;&lt;img src="https://avatars.githubusercontent.com/u/6433752?v=4?s=100" width="100px;" alt="rika" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;rika&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-w95" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/cyrilselasi"&gt;&lt;img src="https://avatars.githubusercontent.com/u/7190330?v=4?s=100" width="100px;" alt="Cyril Selasi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Cyril Selasi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-cyrilselasi" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://nijfranck.github.io"&gt;&lt;img src="https://avatars.githubusercontent.com/u/9940307?v=4?s=100" width="100px;" alt="Franck Nijimbere" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Franck Nijimbere&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-nijfranck" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/alerdenisov"&gt;&lt;img src="https://avatars.githubusercontent.com/u/3899837?v=4?s=100" width="100px;" alt="Aleksandr Denisov" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aleksandr Denisov&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-alerdenisov" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/rbnswartz"&gt;&lt;img src="https://avatars.githubusercontent.com/u/724704?v=4?s=100" width="100px;" alt="Reuben Swartz" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Reuben Swartz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=rbnswartz" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://lupianezjose.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/4380557?v=4?s=100" width="100px;" alt="joselupianez" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;joselupianez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-joselupianez" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://www.zidoary.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/24081860?v=4?s=100" width="100px;" alt="Awais Manzoor" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Awais Manzoor&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Aawais000" title="Bug reports"&gt;🐛&lt;/a&gt; &lt;a href="https://github.com/activepieces/activepieces/commits?author=awais000" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/andchir"&gt;&lt;img src="https://avatars.githubusercontent.com/u/6392311?v=4?s=100" width="100px;" alt="Andrei" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrei&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Aandchir" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/derbbre"&gt;&lt;img src="https://avatars.githubusercontent.com/u/281843?v=4?s=100" width="100px;" alt="derbbre" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;derbbre&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=derbbre" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/maor-rozenfeld"&gt;&lt;img src="https://avatars.githubusercontent.com/u/49363375?v=4?s=100" width="100px;" alt="Maor Rozenfeld" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Maor Rozenfeld&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=maor-rozenfeld" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/miqh"&gt;&lt;img src="https://avatars.githubusercontent.com/u/43751307?v=4?s=100" width="100px;" alt="Michael Huynh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Michael Huynh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=miqh" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/fdundjer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/17405319?v=4?s=100" width="100px;" alt="Filip Dunđer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Filip Dunđer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=fdundjer" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://donthorp.net"&gt;&lt;img src="https://avatars.githubusercontent.com/u/8629?v=4?s=100" width="100px;" alt="Don Thorp" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Don Thorp&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=donthorp" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://joeworkman.net"&gt;&lt;img src="https://avatars.githubusercontent.com/u/225628?v=4?s=100" width="100px;" alt="Joe Workman" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Joe Workman&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-joeworkman" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Autumnlight02"&gt;&lt;img src="https://avatars.githubusercontent.com/u/68244453?v=4?s=100" width="100px;" alt="Aykut Akgün" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aykut Akgün&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=Autumnlight02" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/yann120"&gt;&lt;img src="https://avatars.githubusercontent.com/u/10012140?v=4?s=100" width="100px;" alt="Yann Petitjean" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yann Petitjean&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-yann120" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt; &lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Ayann120" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/pfernandez98"&gt;&lt;img src="https://avatars.githubusercontent.com/u/54374282?v=4?s=100" width="100px;" alt="pfernandez98" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;pfernandez98&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-pfernandez98" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://denieler.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2836281?v=4?s=100" width="100px;" alt="Daniel O." /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Daniel O.&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-denieler" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://myh.tw"&gt;&lt;img src="https://avatars.githubusercontent.com/u/12458706?v=4?s=100" width="100px;" alt="Meng-Yuan Huang" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Meng-Yuan Huang&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=MrMYHuang" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/bigfluffycookie"&gt;&lt;img src="https://avatars.githubusercontent.com/u/54935347?v=4?s=100" width="100px;" alt="Leyla" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Leyla&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Abigfluffycookie" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://i-nithin.netlify.app/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/97078688?v=4?s=100" width="100px;" alt="i-nithin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;i-nithin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-i-nithin" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://lawrenceli.me"&gt;&lt;img src="https://avatars.githubusercontent.com/u/24540598?v=4?s=100" width="100px;" alt="la3rence" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;la3rence&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-la3rence" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://dennisrongo.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/51771021?v=4?s=100" width="100px;" alt="Dennis Rongo" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Dennis Rongo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Adennisrongo" title="Bug reports"&gt;🐛&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-dennisrongo" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/kartikmehta8"&gt;&lt;img src="https://avatars.githubusercontent.com/u/77505989?v=4?s=100" width="100px;" alt="Kartik Mehta" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kartik Mehta&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=kartikmehta8" title="Documentation"&gt;📖&lt;/a&gt; &lt;a href="https://github.com/activepieces/activepieces/commits?author=kartikmehta8" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://zakher.me"&gt;&lt;img src="https://avatars.githubusercontent.com/u/46135573?v=4?s=100" width="100px;" alt="Zakher Masri" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zakher Masri&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=zaaakher" title="Documentation"&gt;📖&lt;/a&gt; &lt;a href="https://github.com/activepieces/activepieces/commits?author=zaaakher" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/AbdullahBitar"&gt;&lt;img src="https://avatars.githubusercontent.com/u/122645579?v=4?s=100" width="100px;" alt="AbdullahBitar" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AbdullahBitar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-AbdullahBitar" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/mariomeyer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/867650?v=4?s=100" width="100px;" alt="Mario Meyer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mario Meyer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-mariomeyer" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/karimkhaleel"&gt;&lt;img src="https://avatars.githubusercontent.com/u/94621779?v=4?s=100" width="100px;" alt="Karim Khaleel" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Karim Khaleel&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-karimkhaleel" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/CPonchet"&gt;&lt;img src="https://avatars.githubusercontent.com/u/40756925?v=4?s=100" width="100px;" alt="CPonchet" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;CPonchet&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3ACPonchet" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/AdamSelene"&gt;&lt;img src="https://avatars.githubusercontent.com/u/79495?v=4?s=100" width="100px;" alt="Olivier Sambourg" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Olivier Sambourg&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-AdamSelene" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Verlich"&gt;&lt;img src="https://avatars.githubusercontent.com/u/30838131?v=4?s=100" width="100px;" alt="Ahmad(Ed)" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ahmad(Ed)&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Verlich" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/leenmashni"&gt;&lt;img src="https://avatars.githubusercontent.com/u/102361544?v=4?s=100" width="100px;" alt="leenmashni" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;leenmashni&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-leenmashni" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/AliasKingsWorth"&gt;&lt;img src="https://avatars.githubusercontent.com/u/47811610?v=4?s=100" width="100px;" alt="M Abdul Rauf" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;M Abdul Rauf&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=AliasKingsWorth" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/vbarrier"&gt;&lt;img src="https://avatars.githubusercontent.com/u/446808?v=4?s=100" width="100px;" alt="Vincent Barrier" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vincent Barrier&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-vbarrier" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://johnmark.dev"&gt;&lt;img src="https://avatars.githubusercontent.com/u/65794951?v=4?s=100" width="100px;" alt="John" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;John&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=jmgb27" title="Code"&gt;💻&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-jmgb27" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://joost.blog/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/487629?v=4?s=100" width="100px;" alt="Joost de Valk" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Joost de Valk&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-jdevalk" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.linkedin.com/in/nyamkamunhjin/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/44439626?v=4?s=100" width="100px;" alt="MJ" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;MJ&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-nyamkamunhjin" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/shravankshenoy"&gt;&lt;img src="https://avatars.githubusercontent.com/u/29670290?v=4?s=100" width="100px;" alt="ShravanShenoy" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ShravanShenoy&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=shravankshenoy" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://jonkristian.no"&gt;&lt;img src="https://avatars.githubusercontent.com/u/13219?v=4?s=100" width="100px;" alt="Jon Kristian" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jon Kristian&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=jonkristian" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/cr0fters"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1754858?v=4?s=100" width="100px;" alt="cr0fters" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;cr0fters&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Acr0fters" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://bibek-timsina.com.np/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/29589003?v=4?s=100" width="100px;" alt="Bibek Timsina" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bibek Timsina&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Abimsina" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/szepeviktor/debian-server-tools/raw/master/CV.md"&gt;&lt;img src="https://avatars.githubusercontent.com/u/952007?v=4?s=100" width="100px;" alt="Viktor Szépe" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Viktor Szépe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=szepeviktor" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/rendyt1"&gt;&lt;img src="https://avatars.githubusercontent.com/u/38492810?v=4?s=100" width="100px;" alt="Rendy Tan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rendy Tan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=rendyt1" title="Documentation"&gt;📖&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-rendyt1" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://islamaf.github.io"&gt;&lt;img src="https://avatars.githubusercontent.com/u/44944648?v=4?s=100" width="100px;" alt="Islam Abdelfattah" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Islam Abdelfattah&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Aislamaf" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/uniqueeest"&gt;&lt;img src="https://avatars.githubusercontent.com/u/123538138?v=4?s=100" width="100px;" alt="Yoonjae Choi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yoonjae Choi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=uniqueeest" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://javix64.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/58471170?v=4?s=100" width="100px;" alt="Javier HM" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Javier HM&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-javix64" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://farag.tech"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50884619?v=4?s=100" width="100px;" alt="Mohamed Hassan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mohamed Hassan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3AMohamedHassan499" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.coasy.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/17610709?v=4?s=100" width="100px;" alt="Christian Schab" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Christian Schab&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-christian-schab" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.gamespecifications.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/37847256?v=4?s=100" width="100px;" alt="Pratik Kinage" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Pratik Kinage&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-thirstycode" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/LevwTech"&gt;&lt;img src="https://avatars.githubusercontent.com/u/69399787?v=4?s=100" width="100px;" alt="Abdelrahman Mostafa " /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abdelrahman Mostafa &lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-LevwTech" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/HamzaZagha"&gt;&lt;img src="https://avatars.githubusercontent.com/u/45468866?v=4?s=100" width="100px;" alt="Hamza Zagha" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Hamza Zagha&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3AHamzaZagha" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://founderblocks.io/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/88160672?v=4?s=100" width="100px;" alt="Lasse Schuirmann" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Lasse Schuirmann&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-founderblocks-sils" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://about.me/cyril_duchon_doris"&gt;&lt;img src="https://avatars.githubusercontent.com/u/7388889?v=4?s=100" width="100px;" alt="Cyril Duchon-Doris" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Cyril Duchon-Doris&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Startouf" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Javiink"&gt;&lt;img src="https://avatars.githubusercontent.com/u/43996484?v=4?s=100" width="100px;" alt="Javiink" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Javiink&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Javiink" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/hharchani"&gt;&lt;img src="https://avatars.githubusercontent.com/u/6430611?v=4?s=100" width="100px;" alt="Harshit Harchani" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Harshit Harchani&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-hharchani" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/MrAkber"&gt;&lt;img src="https://avatars.githubusercontent.com/u/170118042?v=4?s=100" width="100px;" alt="MrAkber" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;MrAkber&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=MrAkber" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/marek-slavicek"&gt;&lt;img src="https://avatars.githubusercontent.com/u/136325104?v=4?s=100" width="100px;" alt="marek-slavicek" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;marek-slavicek&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-marek-slavicek" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/hugh-codes"&gt;&lt;img src="https://avatars.githubusercontent.com/u/166336705?v=4?s=100" width="100px;" alt="hugh-codes" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;hugh-codes&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-hugh-codes" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/alewis001"&gt;&lt;img src="https://avatars.githubusercontent.com/u/3482446?v=4?s=100" width="100px;" alt="Alex Lewis" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alex Lewis&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Aalewis001" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://yual.in"&gt;&lt;img src="https://avatars.githubusercontent.com/u/21105863?v=4?s=100" width="100px;" alt="Yuanlin Lin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yuanlin Lin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=yuaanlin" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://klo.dev"&gt;&lt;img src="https://avatars.githubusercontent.com/u/96867907?v=4?s=100" width="100px;" alt="Ala Shiban" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ala Shiban&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=AlaShibanAtKlo" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://hamedsh.medium.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/6043214?v=4?s=100" width="100px;" alt="hamsh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;hamsh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=hamedsh" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.anne-mariel.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/77142075?v=4?s=100" width="100px;" alt="Anne Mariel Catapang" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Anne Mariel Catapang&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-AnneMariel95" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://hi.carlogino.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/19299524?v=4?s=100" width="100px;" alt="Carlo Gino Catapang" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Carlo Gino Catapang&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-codegino" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/drona2938"&gt;&lt;img src="https://avatars.githubusercontent.com/u/34496554?v=4?s=100" width="100px;" alt="Aditya Rathore" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aditya Rathore&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-drona2938" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/coderbob2"&gt;&lt;img src="https://avatars.githubusercontent.com/u/47177246?v=4?s=100" width="100px;" alt="coderbob2" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;coderbob2&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-coderbob2" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://raamyy.netlify.app"&gt;&lt;img src="https://avatars.githubusercontent.com/u/29176293?v=4?s=100" width="100px;" alt="Ramy Gamal" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ramy Gamal&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Raamyy" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://alexandrudanpop.dev/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/15979292?v=4?s=100" width="100px;" alt="Alexandru-Dan Pop" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alexandru-Dan Pop&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=alexandrudanpop" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Trayshmhirk"&gt;&lt;img src="https://avatars.githubusercontent.com/u/112286458?v=4?s=100" width="100px;" alt="Frank Micheal " /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Frank Micheal &lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Trayshmhirk" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/emmanuel-ferdman"&gt;&lt;img src="https://avatars.githubusercontent.com/u/35470921?v=4?s=100" width="100px;" alt="Emmanuel Ferdman" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emmanuel Ferdman&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=emmanuel-ferdman" title="Documentation"&gt;📖&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/sany2407"&gt;&lt;img src="https://avatars.githubusercontent.com/u/179091674?v=4?s=100" width="100px;" alt="Sany A" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sany A&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-sany2407" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://swimburger.net"&gt;&lt;img src="https://avatars.githubusercontent.com/u/3382717?v=4?s=100" width="100px;" alt="Niels Swimberghe" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Niels Swimberghe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3ASwimburger" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/lostinbug"&gt;&lt;img src="https://avatars.githubusercontent.com/u/157452389?v=4?s=100" width="100px;" alt="lostinbug" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;lostinbug&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-lostinbug" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/gushkool"&gt;&lt;img src="https://avatars.githubusercontent.com/u/64713308?v=4?s=100" width="100px;" alt="gushkool" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;gushkool&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-gushkool" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://www.linkedin.com/in/omarsayed"&gt;&lt;img src="https://avatars.githubusercontent.com/u/3813045?v=4?s=100" width="100px;" alt="Omar Sayed" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Omar Sayed&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-OmarSayed" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/rSnapkoOpenOps"&gt;&lt;img src="https://avatars.githubusercontent.com/u/179845343?v=4?s=100" width="100px;" alt="rSnapkoOpenOps" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;rSnapkoOpenOps&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3ArSnapkoOpenOps" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/ahronshor"&gt;&lt;img src="https://avatars.githubusercontent.com/u/25138831?v=4?s=100" width="100px;" alt="ahronshor" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ahronshor&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-ahronshor" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/cezudas"&gt;&lt;img src="https://avatars.githubusercontent.com/u/3786138?v=4?s=100" width="100px;" alt="Cezar" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Cezar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Acezudas" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/geekyme-fsmk"&gt;&lt;img src="https://avatars.githubusercontent.com/u/100678833?v=4?s=100" width="100px;" alt="Shawn Lim" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shawn Lim&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-geekyme-fsmk" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://shawn.storyline.io/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/977460?v=4?s=100" width="100px;" alt="Shawn Lim" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shawn Lim&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-geekyme" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/pavloDeshko"&gt;&lt;img src="https://avatars.githubusercontent.com/u/27104046?v=4?s=100" width="100px;" alt="pavloDeshko" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;pavloDeshko&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3ApavloDeshko" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/liuhuapiaoyuan"&gt;&lt;img src="https://avatars.githubusercontent.com/u/8020726?v=4?s=100" width="100px;" alt="abc" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;abc&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=liuhuapiaoyuan" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/manojkum-d"&gt;&lt;img src="https://avatars.githubusercontent.com/u/141437046?v=4?s=100" width="100px;" alt="manoj kumar d" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;manoj kumar d&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-manojkum-d" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/felifluid"&gt;&lt;img src="https://avatars.githubusercontent.com/u/59516203?v=4?s=100" width="100px;" alt="Feli" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Feli&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-felifluid" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/mordonez"&gt;&lt;img src="https://avatars.githubusercontent.com/u/293837?v=4?s=100" width="100px;" alt="Miguel" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Miguel&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-mordonez" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/dev-instasent"&gt;&lt;img src="https://avatars.githubusercontent.com/u/116744368?v=4?s=100" width="100px;" alt="Instasent DEV" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Instasent DEV&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-dev-instasent" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/matthieu-lombard"&gt;&lt;img src="https://avatars.githubusercontent.com/u/33624489?v=4?s=100" width="100px;" alt="Matthieu Lombard" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Matthieu Lombard&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-matthieu-lombard" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/beyondlevi"&gt;&lt;img src="https://avatars.githubusercontent.com/u/57486338?v=4?s=100" width="100px;" alt="beyondlevi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;beyondlevi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-beyondlevi" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://rafal.fyi"&gt;&lt;img src="https://avatars.githubusercontent.com/u/10667346?v=4?s=100" width="100px;" alt="Rafal Zawadzki" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rafal Zawadzki&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-rafalzawadzki" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.pdfmonkey.io/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/119303?v=4?s=100" width="100px;" alt="Simon Courtois" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Simon Courtois&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-simonc" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/alegria-solutions"&gt;&lt;img src="https://avatars.githubusercontent.com/u/124846022?v=4?s=100" width="100px;" alt="alegria-solutions" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;alegria-solutions&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-alegria-solutions" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/D-Rowe-FS"&gt;&lt;img src="https://avatars.githubusercontent.com/u/142934784?v=4?s=100" width="100px;" alt="D-Rowe-FS" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;D-Rowe-FS&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-D-Rowe-FS" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/ChineseHamberger"&gt;&lt;img src="https://avatars.githubusercontent.com/u/101547635?v=4?s=100" width="100px;" alt="张晟杰" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;张晟杰&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-ChineseHamberger" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://codesign.rf.gd"&gt;&lt;img src="https://avatars.githubusercontent.com/u/72438085?v=4?s=100" width="100px;" alt="Ashot" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ashot&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-AshotZaqoyan" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/amrabuaza"&gt;&lt;img src="https://avatars.githubusercontent.com/u/30035105?v=4?s=100" width="100px;" alt="Amr Abu Aza" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Amr Abu Aza&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-amrabuaza" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://johng.io"&gt;&lt;img src="https://avatars.githubusercontent.com/u/9030780?v=4?s=100" width="100px;" alt="John Goodliff" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;John Goodliff&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-jerboa88" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/DiwashDev"&gt;&lt;img src="https://avatars.githubusercontent.com/u/182864159?v=4?s=100" width="100px;" alt="Diwash Dev" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Diwash Dev&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-DiwashDev" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.seven.io"&gt;&lt;img src="https://avatars.githubusercontent.com/u/12965261?v=4?s=100" width="100px;" alt="André" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;André&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-matthiez" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/loudotdigital"&gt;&lt;img src="https://avatars.githubusercontent.com/u/7611772?v=4?s=100" width="100px;" alt="Lou | Digital Marketing" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Lou | Digital Marketing&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-loudotdigital" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/maarteNNNN"&gt;&lt;img src="https://avatars.githubusercontent.com/u/14275291?v=4?s=100" width="100px;" alt="Maarten Coppens" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Maarten Coppens&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-maarteNNNN" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/mahmuthamet"&gt;&lt;img src="https://avatars.githubusercontent.com/u/90776946?v=4?s=100" width="100px;" alt="Mahmoud Hamed" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mahmoud Hamed&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-mahmuthamet" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://dammaretz.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/14098167?v=4?s=100" width="100px;" alt="Theo Dammaretz" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Theo Dammaretz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Blightwidow" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/s31w4n"&gt;&lt;img src="https://avatars.githubusercontent.com/u/63353528?v=4?s=100" width="100px;" alt="s31w4n" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;s31w4n&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=s31w4n" title="Documentation"&gt;📖&lt;/a&gt; &lt;a href="https://github.com/activepieces/activepieces/commits?author=s31w4n" title="Code"&gt;💻&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-s31w4n" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://kallabot.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/94991678?v=4?s=100" width="100px;" alt="Abdul Rahman" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abdul Rahman&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-abdulrahmanmajid" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/coat"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1661?v=4?s=100" width="100px;" alt="Kent Smith" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kent Smith&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-coat" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/ArvindEnvoy"&gt;&lt;img src="https://avatars.githubusercontent.com/u/25014185?v=4?s=100" width="100px;" alt="Arvind Ramesh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Arvind Ramesh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=ArvindEnvoy" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/valentin-mourtialon"&gt;&lt;img src="https://avatars.githubusercontent.com/u/88686764?v=4?s=100" width="100px;" alt="valentin-mourtialon" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;valentin-mourtialon&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-valentin-mourtialon" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/psgpsg16"&gt;&lt;img src="https://avatars.githubusercontent.com/u/188385621?v=4?s=100" width="100px;" alt="psgpsg16" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;psgpsg16&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-psgpsg16" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/mariiawidrpay"&gt;&lt;img src="https://avatars.githubusercontent.com/u/110456120?v=4?s=100" width="100px;" alt="Mariia Shyn" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mariia Shyn&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-mariiawidrpay" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/joshuaheslin"&gt;&lt;img src="https://avatars.githubusercontent.com/u/48037470?v=4?s=100" width="100px;" alt="Joshua Heslin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Joshua Heslin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-joshuaheslin" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/ahmad-swanblocks"&gt;&lt;img src="https://avatars.githubusercontent.com/u/165162455?v=4?s=100" width="100px;" alt="Ahmad" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ahmad&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-ahmad-swanblocks" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/danielpoonwj"&gt;&lt;img src="https://avatars.githubusercontent.com/u/17039704?v=4?s=100" width="100px;" alt="Daniel Poon" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Daniel Poon&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=danielpoonwj" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Kevinyu-alan"&gt;&lt;img src="https://avatars.githubusercontent.com/u/198612963?v=4?s=100" width="100px;" alt="Kévin Yu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kévin Yu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Kevinyu-alan" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/flex-yeongeun"&gt;&lt;img src="https://avatars.githubusercontent.com/u/186537288?v=4?s=100" width="100px;" alt="노영은" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;노영은&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-flex-yeongeun" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/reemayoush"&gt;&lt;img src="https://avatars.githubusercontent.com/u/168414383?v=4?s=100" width="100px;" alt="reemayoush" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;reemayoush&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-reemayoush" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/briceflaceliere"&gt;&lt;img src="https://avatars.githubusercontent.com/u/5811531?v=4?s=100" width="100px;" alt="Brice" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Brice&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#security-briceflaceliere" title="Security"&gt;🛡️&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://mg-wunna.github.io/mg-wunna/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/63114419?v=4?s=100" width="100px;" alt="Mg Wunna" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mg Wunna&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-mg-wunna" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.linkedin.com/in/harikrishnanum/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/61736905?v=4?s=100" width="100px;" alt="Harikrishnan U M" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Harikrishnan U M&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/issues?q=author%3Ahakrsh" title="Bug reports"&gt;🐛&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/perrine-pullicino-alan"&gt;&lt;img src="https://avatars.githubusercontent.com/u/143406842?v=4?s=100" width="100px;" alt="perrine-pullicino-alan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;perrine-pullicino-alan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-perrine-pullicino-alan" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://kaovilai.pw"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11228024?v=4?s=100" width="100px;" alt="Tiger Kaovilai" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tiger Kaovilai&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=kaovilai" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/CarefulGuru"&gt;&lt;img src="https://avatars.githubusercontent.com/u/141072854?v=4?s=100" width="100px;" alt="CarefulGuru" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;CarefulGuru&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-CarefulGuru" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/AnkitSharmaOnGithub"&gt;&lt;img src="https://avatars.githubusercontent.com/u/53289186?v=4?s=100" width="100px;" alt="Ankit Kumar Sharma" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ankit Kumar Sharma&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-AnkitSharmaOnGithub" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/nhnansari"&gt;&lt;img src="https://avatars.githubusercontent.com/u/116841234?v=4?s=100" width="100px;" alt="Naeem Hassan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Naeem Hassan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-nhnansari" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://timpetricola.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/674084?v=4?s=100" width="100px;" alt="Tim Petricola" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tim Petricola&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=TimPetricola" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/amaan-ai20"&gt;&lt;img src="https://avatars.githubusercontent.com/u/188329978?v=4?s=100" width="100px;" alt="Amaan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Amaan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-amaan-ai20" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.marcllopartriera.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1257083?v=4?s=100" width="100px;" alt="Marc Llopart" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marc Llopart&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-mllopart" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/onyedikachi-david"&gt;&lt;img src="https://avatars.githubusercontent.com/u/51977119?v=4?s=100" width="100px;" alt="David Anyatonwu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;David Anyatonwu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-onyedikachi-david" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://huzef.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/62795688?v=4?s=100" width="100px;" alt="neo773" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;neo773&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-neo773" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Daniel-Klippa"&gt;&lt;img src="https://avatars.githubusercontent.com/u/207180643?v=4?s=100" width="100px;" alt="Daniel-Klippa" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Daniel-Klippa&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=Daniel-Klippa" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/krushnarout"&gt;&lt;img src="https://avatars.githubusercontent.com/u/129386740?v=4?s=100" width="100px;" alt="Krushna Kanta Rout" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Krushna Kanta Rout&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-krushnarout" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.snimesh.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/12984120?v=4?s=100" width="100px;" alt="Nimesh Solanki" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Nimesh Solanki&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=nish17" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/rimjhimyadav"&gt;&lt;img src="https://avatars.githubusercontent.com/u/187646079?v=4?s=100" width="100px;" alt="Rimjhim Yadav" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rimjhim Yadav&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-rimjhimyadav" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/gs03-dev"&gt;&lt;img src="https://avatars.githubusercontent.com/u/70076620?v=4?s=100" width="100px;" alt="gs03" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;gs03&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-gs03-dev" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Kunal-Darekar"&gt;&lt;img src="https://avatars.githubusercontent.com/u/150500530?v=4?s=100" width="100px;" alt="Kunal Darekar" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kunal Darekar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Kunal-Darekar" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Sanket6652"&gt;&lt;img src="https://avatars.githubusercontent.com/u/119039046?v=4?s=100" width="100px;" alt="Sanket6652" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sanket6652&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Sanket6652" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Ani-4x"&gt;&lt;img src="https://avatars.githubusercontent.com/u/174266491?v=4?s=100" width="100px;" alt="Animesh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Animesh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Ani-4x" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://tarvent.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/13419128?v=4?s=100" width="100px;" alt="Derek Johnson" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Derek Johnson&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-DerekJDev" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/mamiefurax"&gt;&lt;img src="https://avatars.githubusercontent.com/u/3955802?v=4?s=100" width="100px;" alt="MamieFurax" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;MamieFurax&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-mamiefurax" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/jadhavharshh"&gt;&lt;img src="https://avatars.githubusercontent.com/u/182950168?v=4?s=100" width="100px;" alt="Harsh Jadhav" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Harsh Jadhav&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=jadhavharshh" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://twitter.com/lucaslima_souza"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1576846?v=4?s=100" width="100px;" alt="Lucas Lima de Souza" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Lucas Lima de Souza&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-lucaslimasouza" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/BenjaminAlan"&gt;&lt;img src="https://avatars.githubusercontent.com/u/42831606?v=4?s=100" width="100px;" alt="Benjamin Benouarka" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Benjamin Benouarka&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=BenjaminAlan" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.linkedin.com/in/chris-wu/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/4491213?v=4?s=100" width="100px;" alt="Chris Wu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Chris Wu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=cjwooo" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/prasanna2000-max"&gt;&lt;img src="https://avatars.githubusercontent.com/u/61037314?v=4?s=100" width="100px;" alt="Prasanna R" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Prasanna R&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=prasanna2000-max" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/AdminCraftHD"&gt;&lt;img src="https://avatars.githubusercontent.com/u/33310274?v=4?s=100" width="100px;" alt="AdminCraftHD" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AdminCraftHD&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=AdminCraftHD" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://tumrabertworld.vercel.app/resume"&gt;&lt;img src="https://avatars.githubusercontent.com/u/38305310?v=4?s=100" width="100px;" alt="Tanakit Phentun" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tanakit Phentun&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=tumrabert" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/marapper"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1397054?v=4?s=100" width="100px;" alt="Sergey Bondar" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sergey Bondar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-marapper" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://yusufcirak.net"&gt;&lt;img src="https://avatars.githubusercontent.com/u/81169996?v=4?s=100" width="100px;" alt="Yusuf Çırak" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yusuf Çırak&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-yusuf-cirak" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://ezhil.dev"&gt;&lt;img src="https://avatars.githubusercontent.com/u/103899034?v=4?s=100" width="100px;" alt="Ezhil Shanmugham" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ezhil Shanmugham&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-ezhil56x" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/sunorains"&gt;&lt;img src="https://avatars.githubusercontent.com/u/211304820?v=4?s=100" width="100px;" alt="Anderson" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Anderson&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-sunorains" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/calladodan"&gt;&lt;img src="https://avatars.githubusercontent.com/u/7246416?v=4?s=100" width="100px;" alt="Daniel Callado" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Daniel Callado&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-calladodan" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/sparkybug"&gt;&lt;img src="https://avatars.githubusercontent.com/u/52334088?v=4?s=100" width="100px;" alt="Ukaegbu Osinachi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ukaegbu Osinachi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-sparkybug" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://mavrick-portfolio.vercel.app/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/146999057?v=4?s=100" width="100px;" alt="Rishi Mondal" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rishi Mondal&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-MAVRICK-1" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Cloudieunnie"&gt;&lt;img src="https://avatars.githubusercontent.com/u/178718590?v=4?s=100" width="100px;" alt="Cloudieunnie" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Cloudieunnie&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=Cloudieunnie" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/balwant1707"&gt;&lt;img src="https://avatars.githubusercontent.com/u/17769387?v=4?s=100" width="100px;" alt="Balwant Singh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Balwant Singh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-balwant1707" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/ravikiranvm"&gt;&lt;img src="https://avatars.githubusercontent.com/u/36404296?v=4?s=100" width="100px;" alt="Ravi Kiran Vallamkonda" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ravi Kiran Vallamkonda&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=ravikiranvm" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/owuzo"&gt;&lt;img src="https://avatars.githubusercontent.com/u/173556464?v=4?s=100" width="100px;" alt="Owuzo Joy" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Owuzo Joy&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-owuzo" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/helenedurand-smet"&gt;&lt;img src="https://avatars.githubusercontent.com/u/206527847?v=4?s=100" width="100px;" alt="helenedurand-smet" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;helenedurand-smet&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://github.com/activepieces/activepieces/commits?author=helenedurand-smet" title="Code"&gt;💻&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/nuvex-dev"&gt;&lt;img src="https://avatars.githubusercontent.com/u/181783827?v=4?s=100" width="100px;" alt="Nuvex" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Nuvex&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-nuvex-dev" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Zebi15"&gt;&lt;img src="https://avatars.githubusercontent.com/u/80625145?v=4?s=100" width="100px;" alt="Apostol Eusebiu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Apostol Eusebiu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-Zebi15" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/fortunamide"&gt;&lt;img src="https://avatars.githubusercontent.com/u/122938302?v=4?s=100" width="100px;" alt="Fortunate" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fortunate&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-fortunamide" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://jaachiwrites.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/173014495?v=4?s=100" width="100px;" alt="Jaachịmmá Anyatọnwụ" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jaachịmmá Anyatọnwụ&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-thejaachi" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://luizmainart.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/55499897?v=4?s=100" width="100px;" alt="Luiz D. M. Mainart" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Luiz D. M. Mainart&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/activepieces/activepieces/main/#plugin-LuizDMM" title="Plugin/utility libraries"&gt;🔌&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;p&gt;This project follows the &lt;a href="https://allcontributors.org"&gt;all-contributors&lt;/a&gt; specification. Contributions of any kind are welcome!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>