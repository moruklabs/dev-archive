<rss version="2.0">
  <channel>
    <title>GitHub Rust Daily Trending</title>
    <description>Daily Trending of Rust in GitHub</description>
    <pubDate>Wed, 17 Sep 2025 02:32:19 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>GyulyVGC/sniffnet</title>
      <link>https://github.com/GyulyVGC/sniffnet</link>
      <description>&lt;p&gt;Comfortably monitor your Internet traffic 🕵️‍♂️&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="" title="Sniffnet" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/header_repository.png" width="95%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/#download"&gt;&lt;img alt="" title="Download" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/download.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GyulyVGC/sniffnet/raw/main/ROADMAP.md"&gt;&lt;img alt="" title="Roadmap" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/roadmap.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://sniffnet.net"&gt;&lt;img alt="" title="Website" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/website.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GyulyVGC/sniffnet/wiki"&gt;&lt;img alt="" title="Wiki" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/wiki.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Application to comfortably monitor your Internet traffic.&lt;br /&gt; Cross-platform. Intuitive. Reliable.&lt;/p&gt; 
 &lt;p&gt;Translated in:&lt;br /&gt; 🇨🇳 🇩🇪 🇫🇷 🇷🇺 🇵🇹 🇪🇦 🇮🇹 🇵🇱 &lt;a href="https://github.com/GyulyVGC/sniffnet/issues/60"&gt;+&amp;nbsp;15&amp;nbsp;more&amp;nbsp;languages&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt; 
 &lt;picture&gt; 
  &lt;img alt="" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/hr.png" width="100%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img alt="" title="Overview page" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/overview.png" width="95%" /&gt; 
 &lt;img alt="" title="Inspect page" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/inspect.png" width="47%" /&gt; 
 &lt;img alt="" title="Notifications page" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/notifications.png" width="47%" /&gt; 
 &lt;img alt="" title="Custom theme" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/deep_cosmos.png" width="47%" /&gt; 
 &lt;img alt="" title="Thumbnail mode" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/thumbnail.png" width="47%" /&gt; 
&lt;/div&gt; 
&lt;p&gt; 
 &lt;picture&gt; 
  &lt;img alt="" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/hr.png" width="100%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h2&gt;&lt;em&gt;Support Sniffnet's development&lt;/em&gt; 💖&lt;/h2&gt; 
&lt;p&gt;&lt;i&gt;Sniffnet is completely free, open-source software which needs lots of effort and time to develop and maintain.&lt;/i&gt;&lt;/p&gt; 
&lt;p&gt;&lt;i&gt;If you appreciate Sniffnet, &lt;a href="https://github.com/sponsors/GyulyVGC"&gt;consider sponsoring&lt;/a&gt;: your support will enable a constant growth with &lt;a href="https://github.com/GyulyVGC/sniffnet/raw/main/ROADMAP.md"&gt;new features and functionalities&lt;/a&gt;.&lt;br /&gt; Do you want to help the project in an alternative way? You can also head to the &lt;a href="https://grindhouse.dev/collections/sniffnet"&gt;official store&lt;/a&gt; and put your hands on some cool merchandise!&lt;/i&gt;&lt;/p&gt; 
&lt;p&gt;&lt;i&gt;A special mention goes to these awesome organizations and folks who are sponsoring Sniffnet:&lt;/i&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/github" title="GitHub"&gt;&lt;img src="https://avatars.githubusercontent.com/github?v=4" width="60px" alt="GitHub" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://nlnet.nl" title="NLnet"&gt;&lt;img src="https://nlnet.nl/logo/logo.svg?sanitize=true" width="60px" alt="NLnet" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://ipinfo.io" title="IPinfo"&gt;&lt;img src="https://avatars.githubusercontent.com/ipinfo?v=4" width="60px" alt="IPinfo" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/Cthulu201" title="Cthulu201"&gt;&lt;img src="https://avatars.githubusercontent.com/Cthulu201?v=4" width="60px" alt="Cthulu201" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/0x0177b11f" title="Tiansheng Li"&gt;&lt;img src="https://avatars.githubusercontent.com/0x0177b11f?v=4" width="60px" alt="Tiansheng Li" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/ZEROF" title="ZEROF"&gt;&lt;img src="https://avatars.githubusercontent.com/ZEROF?v=4" width="60px" alt="ZEROF" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://www.janwalter.org/" title="Jan Walter"&gt;&lt;img src="https://avatars.githubusercontent.com/wahn?v=4" width="60px" alt="Jan Walter" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Download&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; 
    &lt;picture&gt;
     &lt;img alt="Windows" title="Windows" height="85px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/windows.svg?sanitize=true" /&gt;
    &lt;/picture&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_Windows_64-bit.msi"&gt;64-bit&lt;/a&gt; | &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_Windows_32-bit.msi"&gt;32-bit&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; 
    &lt;picture&gt;
     &lt;img alt="macOS" title="macOS" height="85px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/macos.svg?sanitize=true" /&gt;
    &lt;/picture&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_macOS_Intel.dmg"&gt;Intel&lt;/a&gt; | &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_macOS_AppleSilicon.dmg"&gt;Apple silicon&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; 
    &lt;picture&gt;
     &lt;img alt="Linux" title="Linux" height="85px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/linux.svg?sanitize=true" /&gt;
    &lt;/picture&gt; &lt;/td&gt; 
   &lt;td&gt; AppImage: &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxAppImage_amd64.AppImage"&gt;amd64&lt;/a&gt; | &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxAppImage_arm64.AppImage"&gt;arm64&lt;/a&gt; | &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxAppImage_i386.AppImage"&gt;i386&lt;/a&gt; | &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxAppImage_armhf.AppImage"&gt;armhf&lt;/a&gt; &lt;br /&gt; DEB: &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_amd64.deb"&gt;amd64&lt;/a&gt; | &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_arm64.deb"&gt;arm64&lt;/a&gt; | &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_i386.deb"&gt;i386&lt;/a&gt; | &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_armhf.deb"&gt;armhf&lt;/a&gt; &lt;br /&gt; RPM: &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxRPM_x86_64.rpm"&gt;x86_64&lt;/a&gt; | &lt;a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxRPM_aarch64.rpm"&gt;aarch64&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;Links in the table above will download the latest version of Sniffnet directly from &lt;a href="https://github.com/GyulyVGC/sniffnet/releases"&gt;GitHub releases&lt;/a&gt;. &lt;br /&gt; Not what you're looking for? Check out &lt;a href="https://github.com/GyulyVGC/sniffnet/wiki/Alternative-installation-methods"&gt;alternative installation methods&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Remember to also install the &lt;a href="https://github.com/GyulyVGC/sniffnet/wiki/Required-dependencies"&gt;required dependencies&lt;/a&gt; for your operating system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;💻 choose a &lt;strong&gt;network adapter&lt;/strong&gt; of your PC to inspect&lt;/li&gt; 
 &lt;li&gt;🏷️ select a set of &lt;strong&gt;filters&lt;/strong&gt; to apply to the observed traffic&lt;/li&gt; 
 &lt;li&gt;📖 view overall &lt;strong&gt;statistics&lt;/strong&gt; about your Internet traffic&lt;/li&gt; 
 &lt;li&gt;📈 view &lt;strong&gt;real-time charts&lt;/strong&gt; about traffic intensity&lt;/li&gt; 
 &lt;li&gt;📌 keep an eye on your network even when the application is &lt;strong&gt;minimized&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;📁 &lt;strong&gt;import&lt;/strong&gt; and &lt;strong&gt;export&lt;/strong&gt; comprehensive capture reports as &lt;strong&gt;PCAP files&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;🔎 identify &lt;strong&gt;6000+ upper layer services&lt;/strong&gt;, protocols, trojans, and worms&lt;/li&gt; 
 &lt;li&gt;🌐 find out &lt;strong&gt;domain name&lt;/strong&gt; and &lt;strong&gt;ASN&lt;/strong&gt; of the hosts you are exchanging traffic with&lt;/li&gt; 
 &lt;li&gt;🏠 identify connections in your &lt;strong&gt;local network&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;🌍 discover the &lt;strong&gt;geographical location&lt;/strong&gt; of remote hosts&lt;/li&gt; 
 &lt;li&gt;⭐ save your &lt;strong&gt;favorite&lt;/strong&gt; network hosts&lt;/li&gt; 
 &lt;li&gt;🕵️‍♂️ search and &lt;strong&gt;inspect&lt;/strong&gt; each of your network connections in real time&lt;/li&gt; 
 &lt;li&gt;🔉 set custom &lt;strong&gt;notifications&lt;/strong&gt; to inform you when defined network events occur&lt;/li&gt; 
 &lt;li&gt;🎨 choose the &lt;strong&gt;style&lt;/strong&gt; that fits you the most, including custom themes support&lt;/li&gt; 
 &lt;li&gt;...and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;User manual&lt;/h2&gt; 
&lt;p&gt;Do you want to &lt;strong&gt;learn more&lt;/strong&gt;? &lt;br /&gt; Check out the &lt;a href="https://github.com/GyulyVGC/sniffnet/wiki"&gt;&lt;strong&gt;Sniffnet Wiki&lt;/strong&gt;&lt;/a&gt;, a comprehensive manual to help you thoroughly master the application from a basic setup to the most advanced functionalities. &lt;br /&gt; The Wiki includes step-by-step guides, tips, examples of usage, and answers to frequent questions.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/GyulyVGC/sniffnet/wiki"&gt; &lt;img alt="" title="Sniffnet Wiki" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/logos/wiki/wikilogo.svg?sanitize=true" width="300px" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;See details&lt;/summary&gt; 
 &lt;h3&gt;Missing dependencies&lt;/h3&gt; 
 &lt;p&gt;Most of the errors that may arise are likely due to your system missing dependencies required to correctly analyze a network adapter. &lt;br /&gt; Check the &lt;a href="https://github.com/GyulyVGC/sniffnet/wiki/Required-dependencies"&gt;required dependencies page&lt;/a&gt; for instructions on how to proceed depending on your operating system.&lt;/p&gt; 
 &lt;h3&gt;Rendering problems&lt;/h3&gt; 
 &lt;p&gt;In some circumstances, especially if you are running on an old architecture or your graphical drivers are not updated, the &lt;code&gt;wgpu&lt;/code&gt; default renderer used by &lt;a href="https://github.com/iced-rs/iced"&gt;iced&lt;/a&gt; may manifest bugs (the interface glitches, color gradients are unsupported, or some icons are completely black). &lt;br /&gt; In these cases you can set an environment variable to switch to the &lt;code&gt;tiny-skia&lt;/code&gt; renderer, a CPU-only software renderer that should work properly on every environment:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;ICED_BACKEND=tiny-skia
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;em&gt;&lt;strong&gt;In any case, don't hesitate to &lt;a href="https://github.com/GyulyVGC/sniffnet/issues/new/choose"&gt;open an issue&lt;/a&gt;, and I will do my best to help you!&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt; 
&lt;/details&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A big shout-out to &lt;a href="https://github.com/GyulyVGC/sniffnet/raw/main/CONTRIBUTORS.md"&gt;all the contributors&lt;/a&gt; of Sniffnet!&lt;/li&gt; 
 &lt;li&gt;The graphical user interface has been realized with &lt;a href="https://github.com/iced-rs/iced"&gt;iced&lt;/a&gt;, a cross-platform GUI library for Rust focused on simplicity and type-safety&lt;/li&gt; 
 &lt;li&gt;IP geolocation and ASN data are provided by &lt;a href="https://www.maxmind.com"&gt;MaxMind&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Free code signing for Windows Installer is provided by &lt;a href="https://about.signpath.io/"&gt;SignPath.io&lt;/a&gt;, certificate by &lt;a href="https://signpath.org/"&gt;SignPath Foundation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ads.fund/token/0xadfc251f8ef00ceaeca2b5c1882dabe5db0833df"&gt;Sniffnet&lt;/a&gt; project is supported by ADS.FUND&lt;/li&gt; 
 &lt;li&gt;Last but not least, thanks to &lt;a href="https://github.com/GyulyVGC/sniffnet/stargazers"&gt;every single stargazer&lt;/a&gt;: all forms of support made it possible to keep improving Sniffnet!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Stay in the loop&lt;/h2&gt; 
&lt;p&gt;Wait... there's more!&lt;br /&gt;Sniffnet is rapidly evolving, and new features are added on a regular basis.&lt;br /&gt; Follow the &lt;a href="https://sniffnet.net/news"&gt;&lt;b&gt;news&lt;/b&gt;&lt;/a&gt; and Sniffnet socials to never miss an update.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://bsky.app/profile/sniffnet.net"&gt;&lt;img width="48" height="48" alt="Bluesky" title="Bluesky" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/bluesky.svg?sanitize=true" /&gt;&lt;/a&gt;&amp;nbsp; 
 &lt;a href="https://www.linkedin.com/company/sniffnet"&gt;&lt;img width="48" height="48" alt="LinkedIn" title="LinkedIn" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/linkedin.svg?sanitize=true" /&gt;&lt;/a&gt;&amp;nbsp; 
 &lt;a href="https://mastodon.social/@sniffnet"&gt;&lt;img width="48" height="48" alt="Mastodon" title="Mastodon" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/mastodon.svg?sanitize=true" /&gt;&lt;/a&gt;&amp;nbsp; 
 &lt;a href="https://t.me/sniffnet"&gt;&lt;img width="48" height="48" alt="Telegram" title="Telegram" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/telegram.svg?sanitize=true" /&gt;&lt;/a&gt;&amp;nbsp; 
 &lt;a href="https://x.com/sniffnet"&gt;&lt;img width="48" height="48" alt="Twitter / X" title="Twitter / X" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/x.svg?sanitize=true" /&gt;&lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>tree-sitter/tree-sitter</title>
      <link>https://github.com/tree-sitter/tree-sitter</link>
      <description>&lt;p&gt;An incremental parsing system for programming tools&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tree-sitter&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://zenodo.org/badge/latestdoi/14164618"&gt;&lt;img src="https://zenodo.org/badge/14164618.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/w7nTvsVJhm"&gt;&lt;img src="https://img.shields.io/discord/1063097320771698699?logo=discord&amp;amp;label=discord" alt="discord" /&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/#tree-sitter-chat:matrix.org"&gt;&lt;img src="https://img.shields.io/matrix/tree-sitter-chat%3Amatrix.org?logo=matrix&amp;amp;label=matrix" alt="matrix" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Tree-sitter is a parser generator tool and an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited. Tree-sitter aims to be:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt; enough to parse any programming language&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt; enough to parse on every keystroke in a text editor&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Robust&lt;/strong&gt; enough to provide useful results even in the presence of syntax errors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency-free&lt;/strong&gt; so that the runtime library (which is written in pure C) can be embedded in any application&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://tree-sitter.github.io"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tree-sitter/tree-sitter/master/lib/binding_rust/README.md"&gt;Rust binding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tree-sitter/tree-sitter/master/lib/binding_web/README.md"&gt;Wasm binding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tree-sitter/tree-sitter/master/crates/cli/README.md"&gt;Command-line interface&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>YaLTeR/niri</title>
      <link>https://github.com/YaLTeR/niri</link>
      <description>&lt;p&gt;A scrollable-tiling Wayland compositor.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;&lt;img alt="niri" src="https://github.com/user-attachments/assets/07d05cd0-d5dc-4a28-9a35-51bae8f119a0" /&gt;&lt;/h1&gt; 
&lt;p align="center"&gt;A scrollable-tiling Wayland compositor.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://matrix.to/#/#niri:matrix.org"&gt;&lt;img alt="Matrix" src="https://img.shields.io/badge/matrix-%23niri-blue?logo=matrix" /&gt;&lt;/a&gt; &lt;a href="https://github.com/YaLTeR/niri/raw/main/LICENSE"&gt;&lt;img alt="GitHub License" src="https://img.shields.io/github/license/YaLTeR/niri" /&gt;&lt;/a&gt; &lt;a href="https://github.com/YaLTeR/niri/releases"&gt;&lt;img alt="GitHub Release" src="https://img.shields.io/github/v/release/YaLTeR/niri?logo=github" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://yalter.github.io/niri/Getting-Started.html"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://yalter.github.io/niri/Configuration%3A-Introduction.html"&gt;Configuration&lt;/a&gt; | &lt;a href="https://github.com/YaLTeR/niri/discussions/325"&gt;Setup&amp;nbsp;Showcase&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/535e6530-2f44-4b84-a883-1240a3eee6e9" alt="niri with a few windows open" /&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;Windows are arranged in columns on an infinite strip going to the right. Opening a new window never causes existing windows to resize.&lt;/p&gt; 
&lt;p&gt;Every monitor has its own separate window strip. Windows can never "overflow" onto an adjacent monitor.&lt;/p&gt; 
&lt;p&gt;Workspaces are dynamic and arranged vertically. Every monitor has an independent set of workspaces, and there's always one empty workspace present all the way down.&lt;/p&gt; 
&lt;p&gt;The workspace arrangement is preserved across disconnecting and connecting monitors where it makes sense. When a monitor disconnects, its workspaces will move to another monitor, but upon reconnection they will move back to the original monitor.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Built from the ground up for scrollable tiling&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yalter.github.io/niri/Workspaces.html"&gt;Dynamic workspaces&lt;/a&gt; like in GNOME&lt;/li&gt; 
 &lt;li&gt;An &lt;a href="https://github.com/user-attachments/assets/379a5d1f-acdb-4c11-b36c-e85fd91f0995"&gt;Overview&lt;/a&gt; that zooms out workspaces and windows&lt;/li&gt; 
 &lt;li&gt;Built-in screenshot UI&lt;/li&gt; 
 &lt;li&gt;Monitor and window screencasting through xdg-desktop-portal-gnome 
  &lt;ul&gt; 
   &lt;li&gt;You can &lt;a href="https://yalter.github.io/niri/Configuration%3A-Window-Rules.html#block-out-from"&gt;block out&lt;/a&gt; sensitive windows from screencasts&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://yalter.github.io/niri/Screencasting.html#dynamic-screencast-target"&gt;Dynamic cast target&lt;/a&gt; that can change what it shows on the go&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/YaLTeR/niri/assets/1794388/946a910e-9bec-4cd1-a923-4a9421707515"&gt;Touchpad&lt;/a&gt; and &lt;a href="https://github.com/YaLTeR/niri/assets/1794388/8464e65d-4bf2-44fa-8c8e-5883355bd000"&gt;mouse&lt;/a&gt; gestures&lt;/li&gt; 
 &lt;li&gt;Group windows into &lt;a href="https://yalter.github.io/niri/Tabs.html"&gt;tabs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Configurable layout: gaps, borders, struts, window sizes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yalter.github.io/niri/Configuration%3A-Layout.html#gradients"&gt;Gradient borders&lt;/a&gt; with Oklab and Oklch support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/YaLTeR/niri/assets/1794388/ce178da2-af9e-4c51-876f-8709c241d95e"&gt;Animations&lt;/a&gt; with support for &lt;a href="https://github.com/YaLTeR/niri/assets/1794388/27a238d6-0a22-4692-b794-30dc7a626fad"&gt;custom shaders&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Live-reloading config&lt;/li&gt; 
 &lt;li&gt;Works with &lt;a href="https://yalter.github.io/niri/Accessibility.html"&gt;screen readers&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Video Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/YaLTeR/niri/assets/1794388/bce834b0-f205-434e-a027-b373495f9729"&gt;https://github.com/YaLTeR/niri/assets/1794388/bce834b0-f205-434e-a027-b373495f9729&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Also check out this video from Brodie Robertson that showcases a lot of the niri functionality: &lt;a href="https://youtu.be/DeYx2exm04M"&gt;Niri Is My New Favorite Wayland Compositor&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;Niri is stable for day-to-day use and does most things expected of a Wayland compositor. Many people are daily-driving niri, and are happy to help in our &lt;a href="https://matrix.to/#/#niri:matrix.org"&gt;Matrix channel&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Give it a try! Follow the instructions on the &lt;a href="https://yalter.github.io/niri/Getting-Started.html"&gt;Getting Started&lt;/a&gt; page. Have your &lt;a href="https://github.com/Alexays/Waybar"&gt;waybar&lt;/a&gt;s and &lt;a href="https://codeberg.org/dnkl/fuzzel"&gt;fuzzel&lt;/a&gt;s ready: niri is not a complete desktop environment. Also check out &lt;a href="https://github.com/Vortriz/awesome-niri"&gt;awesome-niri&lt;/a&gt;, a list of niri-related links and projects.&lt;/p&gt; 
&lt;p&gt;Here are some points you may have questions about:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-monitor&lt;/strong&gt;: yes, a core part of the design from the very start. Mixed DPI works.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fractional scaling&lt;/strong&gt;: yes, plus all niri UI stays pixel-perfect.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NVIDIA&lt;/strong&gt;: seems to work fine.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Floating windows&lt;/strong&gt;: yes, starting from niri 25.01.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Input devices&lt;/strong&gt;: niri supports tablets, touchpads, and touchscreens. You can map the tablet to a specific monitor, or use &lt;a href="https://opentabletdriver.net/"&gt;OpenTabletDriver&lt;/a&gt;. We have touchpad gestures, but no touchscreen gestures yet.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wlr protocols&lt;/strong&gt;: yes, we have most of the important ones like layer-shell, gamma-control, screencopy. You can check on &lt;a href="https://wayland.app"&gt;wayland.app&lt;/a&gt; at the bottom of each protocol's page.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: while I run niri on beefy machines, I try to stay conscious of performance. I've seen someone use it fine on an Eee&amp;nbsp;PC&amp;nbsp;900 from&amp;nbsp;2008, of all things.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Xwayland&lt;/strong&gt;: &lt;a href="https://yalter.github.io/niri/Xwayland.html#using-xwayland-satellite"&gt;integrated&lt;/a&gt; via xwayland-satellite starting from niri 25.08.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Media&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/Kmz8ODolnDg?list=PLRdS-n5seLRqrmWDQY4KDqtRMfIwU0U3T"&gt;niri: Making a Wayland compositor in Rust&lt;/a&gt; · &lt;em&gt;December 2024&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;My talk from the 2024 Moscow RustCon about niri, and how I do randomized property testing and profiling, and measure input latency. The talk is in Russian, but I prepared full English subtitles that you can find in YouTube's subtitle language selector.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.trommelspeicher.de/podcast/special_the_developer_behind_niri"&gt;An interview with Ivan, the developer behind Niri&lt;/a&gt; · &lt;em&gt;June 2025&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;An interview by a German tech podcast Das Triumvirat (in English). We talk about niri development and history, and my experience building and maintaining niri.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://lwn.net/Articles/1025866/"&gt;A tour of the niri scrolling-tiling Wayland compositor&lt;/a&gt; · &lt;em&gt;July 2025&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;An LWN article with a nice overview and introduction to niri.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you'd like to help with niri, there are plenty of both coding- and non-coding-related ways to do so. See &lt;a href="https://github.com/YaLTeR/niri/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for an overview.&lt;/p&gt; 
&lt;h2&gt;Inspiration&lt;/h2&gt; 
&lt;p&gt;Niri is heavily inspired by &lt;a href="https://github.com/paperwm/PaperWM"&gt;PaperWM&lt;/a&gt; which implements scrollable tiling on top of GNOME Shell.&lt;/p&gt; 
&lt;p&gt;One of the reasons that prompted me to try writing my own compositor is being able to properly separate the monitors. Being a GNOME Shell extension, PaperWM has to work against Shell's global window coordinate space to prevent windows from overflowing.&lt;/p&gt; 
&lt;h2&gt;Tile Scrollably Elsewhere&lt;/h2&gt; 
&lt;p&gt;Here are some other projects which implement a similar workflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/paperwm/PaperWM"&gt;PaperWM&lt;/a&gt;: scrollable tiling on top of GNOME Shell.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/peterfajdiga/karousel"&gt;karousel&lt;/a&gt;: scrollable tiling on top of KDE.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dawsers/scroll"&gt;scroll&lt;/a&gt; and &lt;a href="https://spwhitton.name/tech/code/papersway/"&gt;papersway&lt;/a&gt;: scrollable tiling on top of sway/i3.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hyprwm/hyprland-plugins/tree/main/hyprscrolling"&gt;hyprscrolling&lt;/a&gt; and &lt;a href="https://gitlab.com/magus/hyprslidr"&gt;hyprslidr&lt;/a&gt;: scrollable tiling on top of Hyprland.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mogenson/PaperWM.spoon"&gt;PaperWM.spoon&lt;/a&gt;: scrollable tiling on top of macOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;Our main communication channel is a Matrix chat, feel free to join and ask a question: &lt;a href="https://matrix.to/#/#niri:matrix.org"&gt;https://matrix.to/#/#niri:matrix.org&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We also have a community Discord server: &lt;a href="https://discord.gg/vT8Sfjy7sx"&gt;https://discord.gg/vT8Sfjy7sx&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ankitects/anki</title>
      <link>https://github.com/ankitects/anki</link>
      <description>&lt;p&gt;Anki is a smart spaced repetition flashcard program&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anki®&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://buildkite.com/ankitects/anki-ci"&gt;&lt;img src="https://badge.buildkite.com/c9edf020a4aec976f9835e54751cc5409d843adbb66d043bd3.svg?branch=main" alt="Build status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repo contains the source code for the computer version of &lt;a href="https://apps.ankiweb.net"&gt;Anki&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;About&lt;/h1&gt; 
&lt;p&gt;Anki is a spaced repetition program. Please see the &lt;a href="https://apps.ankiweb.net"&gt;website&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;h3&gt;Anki Betas&lt;/h3&gt; 
&lt;p&gt;If you'd like to try development builds of Anki but don't feel comfortable building the code, please see &lt;a href="https://betas.ankiweb.net/"&gt;Anki betas&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Developing&lt;/h3&gt; 
&lt;p&gt;For more information on building and developing, please see &lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/docs/development.md"&gt;Development&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;Want to contribute to Anki? Check out the &lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/docs/contributing.md"&gt;Contribution Guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Anki Contributors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/CONTRIBUTORS"&gt;CONTRIBUTORS&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Anki's license: &lt;a href="https://raw.githubusercontent.com/ankitects/anki/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>huggingface/candle</title>
      <link>https://github.com/huggingface/candle</link>
      <description>&lt;p&gt;Minimalist ML framework for Rust&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;candle&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/hugging-face-879548962464493619"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/hugging-face-879548962464493619" alt="discord server" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/candle-core"&gt;&lt;img src="https://img.shields.io/crates/v/candle-core.svg?sanitize=true" alt="Latest version" /&gt;&lt;/a&gt; &lt;a href="https://docs.rs/candle-core"&gt;&lt;img src="https://docs.rs/candle-core/badge.svg?sanitize=true" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/candle/raw/main/LICENSE-MIT"&gt;&lt;img src="https://img.shields.io/github/license/base-org/node?color=blue" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/candle/raw/main/LICENSE-APACHE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue?style=flat-square" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use. Try our online demos: &lt;a href="https://huggingface.co/spaces/lmz/candle-whisper"&gt;whisper&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/lmz/candle-llama2"&gt;LLaMA2&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm"&gt;T5&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/lmz/candle-yolo"&gt;yolo&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/radames/candle-segment-anything-wasm"&gt;Segment Anything&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;p&gt;Make sure that you have &lt;a href="https://github.com/huggingface/candle/tree/main/candle-core"&gt;&lt;code&gt;candle-core&lt;/code&gt;&lt;/a&gt; correctly installed as described in &lt;a href="https://huggingface.github.io/candle/guide/installation.html"&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Let's see how to run a simple matrix multiplication. Write the following to your &lt;code&gt;myapp/src/main.rs&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use candle_core::{Device, Tensor};

fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    let device = Device::Cpu;

    let a = Tensor::randn(0f32, 1., (2, 3), &amp;amp;device)?;
    let b = Tensor::randn(0f32, 1., (3, 4), &amp;amp;device)?;

    let c = a.matmul(&amp;amp;b)?;
    println!("{c}");
    Ok(())
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;cargo run&lt;/code&gt; should display a tensor of shape &lt;code&gt;Tensor[[2, 4], f32]&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Having installed &lt;code&gt;candle&lt;/code&gt; with Cuda support, simply define the &lt;code&gt;device&lt;/code&gt; to be on GPU:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-diff"&gt;- let device = Device::Cpu;
+ let device = Device::new_cuda(0)?;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more advanced examples, please have a look at the following section.&lt;/p&gt; 
&lt;h2&gt;Check out our examples&lt;/h2&gt; 
&lt;p&gt;These online demos run entirely in your browser:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/lmz/candle-yolo"&gt;yolo&lt;/a&gt;: pose estimation and object recognition.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/lmz/candle-whisper"&gt;whisper&lt;/a&gt;: speech recognition.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/lmz/candle-llama2"&gt;LLaMA2&lt;/a&gt;: text generation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm"&gt;T5&lt;/a&gt;: text generation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm"&gt;Phi-1.5, and Phi-2&lt;/a&gt;: text generation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/radames/candle-segment-anything-wasm"&gt;Segment Anything Model&lt;/a&gt;: Image segmentation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/radames/Candle-BLIP-Image-Captioning"&gt;BLIP&lt;/a&gt;: image captioning.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also provide some command line based examples using state of the art models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/llama/"&gt;LLaMA v1, v2, and v3&lt;/a&gt;: general LLM, includes the SOLAR-10.7B variant.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/falcon/"&gt;Falcon&lt;/a&gt;: general LLM.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/codegeex4-9b/"&gt;Codegeex4&lt;/a&gt;: Code completion, code interpreter, web search, function calling, repository-level&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/glm4/"&gt;GLM4&lt;/a&gt;: Open Multilingual Multimodal Chat LMs by THUDM&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/gemma/"&gt;Gemma v1 and v2&lt;/a&gt;: 2b and 7b+/9b general LLMs from Google Deepmind.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/recurrent-gemma/"&gt;RecurrentGemma&lt;/a&gt;: 2b and 7b Griffin based models from Google that mix attention with a RNN like state.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/phi/"&gt;Phi-1, Phi-1.5, Phi-2, and Phi-3&lt;/a&gt;: 1.3b, 2.7b, and 3.8b general LLMs with performance on par with 7b models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/stable-lm/"&gt;StableLM-3B-4E1T&lt;/a&gt;: a 3b general LLM pre-trained on 1T tokens of English and code datasets. Also supports StableLM-2, a 1.6b LLM trained on 2T tokens, as well as the code variants.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/mamba/"&gt;Mamba&lt;/a&gt;: an inference only implementation of the Mamba state space model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/mistral/"&gt;Mistral7b-v0.1&lt;/a&gt;: a 7b general LLM with better performance than all publicly available 13b models as of 2023-09-28.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/mixtral/"&gt;Mixtral8x7b-v0.1&lt;/a&gt;: a sparse mixture of experts 8x7b general LLM with better performance than a Llama 2 70B model with much faster inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/bigcode/"&gt;StarCoder&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/starcoder2/"&gt;StarCoder2&lt;/a&gt;: LLM specialized to code generation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/qwen/"&gt;Qwen1.5&lt;/a&gt;: Bilingual (English/Chinese) LLMs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/rwkv/"&gt;RWKV v5 and v6&lt;/a&gt;: An RNN with transformer level LLM performance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/replit-code/"&gt;Replit-code-v1.5&lt;/a&gt;: a 3.3b LLM specialized for code completion.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/yi/"&gt;Yi-6B / Yi-34B&lt;/a&gt;: two bilingual (English/Chinese) general LLMs with 6b and 34b parameters.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/quantized/"&gt;Quantized LLaMA&lt;/a&gt;: quantized version of the LLaMA model using the same quantization techniques as &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://github.com/huggingface/candle/raw/main/candle-examples/examples/quantized/assets/aoc.gif" width="600" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/stable-diffusion/"&gt;Stable Diffusion&lt;/a&gt;: text to image generative model, support for the 1.5, 2.1, SDXL 1.0 and Turbo versions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://github.com/huggingface/candle/raw/main/candle-examples/examples/stable-diffusion/assets/stable-diffusion-xl.jpg" width="200" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/wuerstchen/"&gt;Wuerstchen&lt;/a&gt;: another text to image generative model.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://github.com/huggingface/candle/raw/main/candle-examples/examples/wuerstchen/assets/cat.jpg" width="200" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/yolo-v3/"&gt;yolo-v3&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/yolo-v8/"&gt;yolo-v8&lt;/a&gt;: object detection and pose estimation models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.od.jpg" width="200" /&gt;&lt;img src="https://github.com/huggingface/candle/raw/main/candle-examples/examples/yolo-v8/assets/bike.pose.jpg" width="200" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/segment-anything/"&gt;segment-anything&lt;/a&gt;: image segmentation model with prompt.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://github.com/huggingface/candle/raw/main/candle-examples/examples/segment-anything/assets/sam_merged.jpg" width="200" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/segformer/"&gt;SegFormer&lt;/a&gt;: transformer based semantic segmentation model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/whisper/"&gt;Whisper&lt;/a&gt;: speech recognition model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/encodec/"&gt;EnCodec&lt;/a&gt;: high-quality audio compression model using residual vector quantization.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/metavoice/"&gt;MetaVoice&lt;/a&gt;: foundational model for text-to-speech.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/parler-tts/"&gt;Parler-TTS&lt;/a&gt;: large text-to-speech model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/t5"&gt;T5&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/bert/"&gt;Bert&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/jina-bert/"&gt;JinaBert&lt;/a&gt; : useful for sentence embeddings.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/dinov2/"&gt;DINOv2&lt;/a&gt;: computer vision model trained using self-supervision (can be used for imagenet classification, depth evaluation, segmentation).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/vgg/"&gt;VGG&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/repvgg"&gt;RepVGG&lt;/a&gt;: computer vision models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/blip/"&gt;BLIP&lt;/a&gt;: image to text model, can be used to generate captions for an image.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/clip/"&gt;CLIP&lt;/a&gt;: multi-model vision and language model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/trocr/"&gt;TrOCR&lt;/a&gt;: a transformer OCR model, with dedicated submodels for hand-writing and printed recognition.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/marian-mt/"&gt;Marian-MT&lt;/a&gt;: neural machine translation model, generates the translated text from the input text.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/moondream/"&gt;Moondream&lt;/a&gt;: tiny computer-vision model that can answer real-world questions about images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run them using commands like:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cargo run --example quantized --release
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In order to use &lt;strong&gt;CUDA&lt;/strong&gt; add &lt;code&gt;--features cuda&lt;/code&gt; to the example command line. If you have cuDNN installed, use &lt;code&gt;--features cudnn&lt;/code&gt; for even more speedups.&lt;/p&gt; 
&lt;p&gt;There are also some wasm examples for whisper and &lt;a href="https://github.com/karpathy/llama2.c"&gt;llama2.c&lt;/a&gt;. You can either build them with &lt;code&gt;trunk&lt;/code&gt; or try them online: &lt;a href="https://huggingface.co/spaces/lmz/candle-whisper"&gt;whisper&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/lmz/candle-llama2"&gt;llama2&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/radames/Candle-T5-Generation-Wasm"&gt;T5&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/radames/Candle-Phi-1.5-Wasm"&gt;Phi-1.5, and Phi-2&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/radames/candle-segment-anything-wasm"&gt;Segment Anything Model&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For LLaMA2, run the following command to retrieve the weight files and start a test server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd candle-wasm-examples/llama2-c
wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/model.bin
wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/tokenizer.json
trunk serve --release --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then head over to &lt;a href="http://localhost:8081/"&gt;http://localhost:8081/&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- ANCHOR: useful_libraries ---&gt; 
&lt;h2&gt;Useful External Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ToluClassics/candle-tutorial"&gt;&lt;code&gt;candle-tutorial&lt;/code&gt;&lt;/a&gt;: A very detailed tutorial showing how to convert a PyTorch model to Candle.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EricLBuehler/candle-lora"&gt;&lt;code&gt;candle-lora&lt;/code&gt;&lt;/a&gt;: Efficient and ergonomic LoRA implementation for Candle. &lt;code&gt;candle-lora&lt;/code&gt; has&lt;br /&gt; out-of-the-box LoRA support for many models from Candle, which can be found &lt;a href="https://github.com/EricLBuehler/candle-lora/tree/master/candle-lora-transformers/examples"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KGrewal1/optimisers"&gt;&lt;code&gt;optimisers&lt;/code&gt;&lt;/a&gt;: A collection of optimisers including SGD with momentum, AdaGrad, AdaDelta, AdaMax, NAdam, RAdam, and RMSprop.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EricLBuehler/candle-vllm"&gt;&lt;code&gt;candle-vllm&lt;/code&gt;&lt;/a&gt;: Efficient platform for inference and serving local LLMs including an OpenAI compatible API server.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mokeyish/candle-ext"&gt;&lt;code&gt;candle-ext&lt;/code&gt;&lt;/a&gt;: An extension library to Candle that provides PyTorch functions not currently available in Candle.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vishpat/candle-coursera-ml"&gt;&lt;code&gt;candle-coursera-ml&lt;/code&gt;&lt;/a&gt;: Implementation of ML algorithms from Coursera's &lt;a href="https://www.coursera.org/specializations/machine-learning-introduction"&gt;Machine Learning Specialization&lt;/a&gt; course.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/floneum/floneum/tree/master/interfaces/kalosm"&gt;&lt;code&gt;kalosm&lt;/code&gt;&lt;/a&gt;: A multi-modal meta-framework in Rust for interfacing with local pre-trained models with support for controlled generation, custom samplers, in-memory vector databases, audio transcription, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EricLBuehler/candle-sampling"&gt;&lt;code&gt;candle-sampling&lt;/code&gt;&lt;/a&gt;: Sampling techniques for Candle.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jeroenvlek/gpt-from-scratch-rs"&gt;&lt;code&gt;gpt-from-scratch-rs&lt;/code&gt;&lt;/a&gt;: A port of Andrej Karpathy's &lt;em&gt;Let's build GPT&lt;/em&gt; tutorial on YouTube showcasing the Candle API on a toy problem.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tomsanbear/candle-einops"&gt;&lt;code&gt;candle-einops&lt;/code&gt;&lt;/a&gt;: A pure rust implementation of the python &lt;a href="https://github.com/arogozhnikov/einops"&gt;einops&lt;/a&gt; library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/atoma-network/atoma-infer"&gt;&lt;code&gt;atoma-infer&lt;/code&gt;&lt;/a&gt;: A Rust library for fast inference at scale, leveraging FlashAttention2 for efficient attention computation, PagedAttention for efficient KV-cache memory management, and multi-GPU support. It is OpenAI api compatible.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nerdai/llms-from-scratch-rs"&gt;&lt;code&gt;llms-from-scratch-rs&lt;/code&gt;&lt;/a&gt;: A comprehensive Rust translation of the code from Sebastian Raschka's Build an LLM from Scratch book.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you have an addition to this list, please submit a pull request.&lt;/p&gt; 
&lt;!-- ANCHOR_END: useful_libraries ---&gt; 
&lt;!-- ANCHOR: features ---&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Simple syntax, looks and feels like PyTorch. 
  &lt;ul&gt; 
   &lt;li&gt;Model training.&lt;/li&gt; 
   &lt;li&gt;Embed user-defined ops/kernels, such as &lt;a href="https://github.com/huggingface/candle/raw/89ba005962495f2bfbda286e185e9c3c7f5300a3/candle-flash-attn/src/lib.rs#L152"&gt;flash-attention v2&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Backends. 
  &lt;ul&gt; 
   &lt;li&gt;Optimized CPU backend with optional MKL support for x86 and Accelerate for macs.&lt;/li&gt; 
   &lt;li&gt;CUDA backend for efficiently running on GPUs, multiple GPU distribution via NCCL.&lt;/li&gt; 
   &lt;li&gt;WASM support, run your models in a browser.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Included models. 
  &lt;ul&gt; 
   &lt;li&gt;Language Models. 
    &lt;ul&gt; 
     &lt;li&gt;LLaMA v1, v2, and v3 with variants such as SOLAR-10.7B.&lt;/li&gt; 
     &lt;li&gt;Falcon.&lt;/li&gt; 
     &lt;li&gt;StarCoder, StarCoder2.&lt;/li&gt; 
     &lt;li&gt;Phi 1, 1.5, 2, and 3.&lt;/li&gt; 
     &lt;li&gt;Mamba, Minimal Mamba&lt;/li&gt; 
     &lt;li&gt;Gemma v1 2b and 7b+, v2 2b and 9b.&lt;/li&gt; 
     &lt;li&gt;Mistral 7b v0.1.&lt;/li&gt; 
     &lt;li&gt;Mixtral 8x7b v0.1.&lt;/li&gt; 
     &lt;li&gt;StableLM-3B-4E1T, StableLM-2-1.6B, Stable-Code-3B.&lt;/li&gt; 
     &lt;li&gt;Replit-code-v1.5-3B.&lt;/li&gt; 
     &lt;li&gt;Bert.&lt;/li&gt; 
     &lt;li&gt;Yi-6B and Yi-34B.&lt;/li&gt; 
     &lt;li&gt;Qwen1.5, Qwen1.5 MoE.&lt;/li&gt; 
     &lt;li&gt;RWKV v5 and v6.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Quantized LLMs. 
    &lt;ul&gt; 
     &lt;li&gt;Llama 7b, 13b, 70b, as well as the chat and code variants.&lt;/li&gt; 
     &lt;li&gt;Mistral 7b, and 7b instruct.&lt;/li&gt; 
     &lt;li&gt;Mixtral 8x7b.&lt;/li&gt; 
     &lt;li&gt;Zephyr 7b a and b (Mistral-7b based).&lt;/li&gt; 
     &lt;li&gt;OpenChat 3.5 (Mistral-7b based).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Text to text. 
    &lt;ul&gt; 
     &lt;li&gt;T5 and its variants: FlanT5, UL2, MADLAD400 (translation), CoEdit (Grammar correction).&lt;/li&gt; 
     &lt;li&gt;Marian MT (Machine Translation).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Text to image. 
    &lt;ul&gt; 
     &lt;li&gt;Stable Diffusion v1.5, v2.1, XL v1.0.&lt;/li&gt; 
     &lt;li&gt;Wurstchen v2.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Image to text. 
    &lt;ul&gt; 
     &lt;li&gt;BLIP.&lt;/li&gt; 
     &lt;li&gt;TrOCR.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Audio. 
    &lt;ul&gt; 
     &lt;li&gt;Whisper, multi-lingual speech-to-text.&lt;/li&gt; 
     &lt;li&gt;EnCodec, audio compression model.&lt;/li&gt; 
     &lt;li&gt;MetaVoice-1B, text-to-speech model.&lt;/li&gt; 
     &lt;li&gt;Parler-TTS, text-to-speech model.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Computer Vision Models. 
    &lt;ul&gt; 
     &lt;li&gt;DINOv2, ConvMixer, EfficientNet, ResNet, ViT, VGG, RepVGG, ConvNeXT, ConvNeXTv2, MobileOne, EfficientVit (MSRA), MobileNetv4, Hiera, FastViT.&lt;/li&gt; 
     &lt;li&gt;yolo-v3, yolo-v8.&lt;/li&gt; 
     &lt;li&gt;Segment-Anything Model (SAM).&lt;/li&gt; 
     &lt;li&gt;SegFormer.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;File formats: load models from safetensors, npz, ggml, or PyTorch files.&lt;/li&gt; 
 &lt;li&gt;Serverless (on CPU), small and fast deployments.&lt;/li&gt; 
 &lt;li&gt;Quantization support using the llama.cpp quantized types.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- ANCHOR_END: features ---&gt; 
&lt;h2&gt;How to use&lt;/h2&gt; 
&lt;!-- ANCHOR: cheatsheet ---&gt; 
&lt;p&gt;Cheatsheet:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Using PyTorch&lt;/th&gt; 
   &lt;th&gt;Using Candle&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Creation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;torch.Tensor([[1, 2], [3, 4]])&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Tensor::new(&amp;amp;[[1f32, 2.], [3., 4.]], &amp;amp;Device::Cpu)?&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Creation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;torch.zeros((2, 2))&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Tensor::zeros((2, 2), DType::F32, &amp;amp;Device::Cpu)?&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Indexing&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;tensor[:, :4]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;tensor.i((.., ..4))?&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operations&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;tensor.view((2, 2))&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;tensor.reshape((2, 2))?&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operations&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;a.matmul(b)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;a.matmul(&amp;amp;b)?&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Arithmetic&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;a + b&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;amp;a + &amp;amp;b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Device&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;tensor.to(device="cuda")&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;tensor.to_device(&amp;amp;Device::new_cuda(0)?)?&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dtype&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;tensor.to(dtype=torch.float16)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;tensor.to_dtype(&amp;amp;DType::F16)?&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Saving&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;torch.save({"A": A}, "model.bin")&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;candle::safetensors::save(&amp;amp;HashMap::from([("A", A)]), "model.safetensors")?&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Loading&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;weights = torch.load("model.bin")&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;candle::safetensors::load("model.safetensors", &amp;amp;device)&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- ANCHOR_END: cheatsheet ---&gt; 
&lt;h2&gt;Structure&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-core"&gt;candle-core&lt;/a&gt;: Core ops, devices, and &lt;code&gt;Tensor&lt;/code&gt; struct definition&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-nn/"&gt;candle-nn&lt;/a&gt;: Tools to build real models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/"&gt;candle-examples&lt;/a&gt;: Examples of using the library in realistic settings&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-kernels/"&gt;candle-kernels&lt;/a&gt;: CUDA custom kernels&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-datasets/"&gt;candle-datasets&lt;/a&gt;: Datasets and data loaders.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-transformers"&gt;candle-transformers&lt;/a&gt;: transformers-related utilities.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-flash-attn"&gt;candle-flash-attn&lt;/a&gt;: Flash attention v2 layer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/huggingface/candle/main/candle-onnx/"&gt;candle-onnx&lt;/a&gt;: ONNX model evaluation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;Why should I use Candle?&lt;/h3&gt; 
&lt;!-- ANCHOR: goals ---&gt; 
&lt;p&gt;Candle's core goal is to &lt;em&gt;make serverless inference possible&lt;/em&gt;. Full machine learning frameworks like PyTorch are very large, which makes creating instances on a cluster slow. Candle allows deployment of lightweight binaries.&lt;/p&gt; 
&lt;p&gt;Secondly, Candle lets you &lt;em&gt;remove Python&lt;/em&gt; from production workloads. Python overhead can seriously hurt performance, and the &lt;a href="https://www.backblaze.com/blog/the-python-gil-past-present-and-future/"&gt;GIL&lt;/a&gt; is a notorious source of headaches.&lt;/p&gt; 
&lt;p&gt;Finally, Rust is cool! A lot of the HF ecosystem already has Rust crates, like &lt;a href="https://github.com/huggingface/safetensors"&gt;safetensors&lt;/a&gt; and &lt;a href="https://github.com/huggingface/tokenizers"&gt;tokenizers&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- ANCHOR_END: goals ---&gt; 
&lt;h3&gt;Other ML frameworks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/coreylowman/dfdx"&gt;dfdx&lt;/a&gt; is a formidable crate, with shapes being included in types. This prevents a lot of headaches by getting the compiler to complain about shape mismatches right off the bat. However, we found that some features still require nightly, and writing code can be a bit daunting for non rust experts.&lt;/p&gt; &lt;p&gt;We're leveraging and contributing to other core crates for the runtime so hopefully both crates can benefit from each other.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/burn-rs/burn"&gt;burn&lt;/a&gt; is a general crate that can leverage multiple backends so you can choose the best engine for your workload.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/LaurentMazare/tch-rs.git"&gt;tch-rs&lt;/a&gt; Bindings to the torch library in Rust. Extremely versatile, but they bring in the entire torch library into the runtime. The main contributor of &lt;code&gt;tch-rs&lt;/code&gt; is also involved in the development of &lt;code&gt;candle&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Common Errors&lt;/h3&gt; 
&lt;h4&gt;Missing symbols when compiling with the mkl feature.&lt;/h4&gt; 
&lt;p&gt;If you get some missing symbols when compiling binaries/tests using the mkl or accelerate features, e.g. for mkl you get:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  = note: /usr/bin/ld: (....o): in function `blas::sgemm':
          .../blas-0.22.0/src/lib.rs:1944: undefined reference to `sgemm_' collect2: error: ld returned 1 exit status

  = note: some `extern` functions couldn't be found; some native libraries may need to be installed or have their path specified
  = note: use the `-l` flag to specify native libraries to link
  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or for accelerate:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Undefined symbols for architecture arm64:
            "_dgemm_", referenced from:
                candle_core::accelerate::dgemm::h1b71a038552bcabe in libcandle_core...
            "_sgemm_", referenced from:
                candle_core::accelerate::sgemm::h2cf21c592cba3c47 in libcandle_core...
          ld: symbol(s) not found for architecture arm64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is likely due to a missing linker flag that was needed to enable the mkl library. You can try adding the following for mkl at the top of your binary:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;extern crate intel_mkl_src;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or for accelerate:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;extern crate accelerate_src;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Cannot run the LLaMA examples: access to source requires login credentials&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;Error: request error: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/tokenizer.json: status code 401
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is likely because you're not permissioned for the LLaMA-v2 model. To fix this, you have to register on the huggingface-hub, accept the &lt;a href="https://huggingface.co/meta-llama/Llama-2-7b-hf"&gt;LLaMA-v2 model conditions&lt;/a&gt;, and set up your authentication token. See issue &lt;a href="https://github.com/huggingface/candle/issues/350"&gt;#350&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h4&gt;Missing cute/cutlass headers when compiling flash-attn&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;  In file included from kernels/flash_fwd_launch_template.h:11:0,
                   from kernels/flash_fwd_hdim224_fp16_sm80.cu:5:
  kernels/flash_fwd_kernel.h:8:10: fatal error: cute/algorithm/copy.hpp: No such file or directory
   #include &amp;lt;cute/algorithm/copy.hpp&amp;gt;
            ^~~~~~~~~~~~~~~~~~~~~~~~~
  compilation terminated.
  Error: nvcc error while compiling:
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/cutlass"&gt;cutlass&lt;/a&gt; is provided as a git submodule so you may want to run the following command to check it in properly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git submodule update --init
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Compiling with flash-attention fails&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;/usr/include/c++/11/bits/std_function.h:530:146: error: parameter packs not expanded with ‘...’:
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is a bug in gcc-11 triggered by the Cuda compiler. To fix this, install a different, supported gcc version - for example gcc-10, and specify the path to the compiler in the NVCC_CCBIN environment variable.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;env NVCC_CCBIN=/usr/lib/gcc/x86_64-linux-gnu/10 cargo ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Linking error on windows when running rustdoc or mdbook tests&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;Couldn't compile the test.
---- .\candle-book\src\inference\hub.md - Using_the_hub::Using_in_a_real_model_ (line 50) stdout ----
error: linking with `link.exe` failed: exit code: 1181
//very long chain of linking
 = note: LINK : fatal error LNK1181: cannot open input file 'windows.0.48.5.lib'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Make sure you link all native libraries that might be located outside a project target, e.g., to run mdbook tests, you should run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mdbook test candle-book -L .\target\debug\deps\ `
-L native=$env:USERPROFILE\.cargo\registry\src\index.crates.io-6f17d22bba15001f\windows_x86_64_msvc-0.42.2\lib `
-L native=$env:USERPROFILE\.cargo\registry\src\index.crates.io-6f17d22bba15001f\windows_x86_64_msvc-0.48.5\lib
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extremely slow model load time with WSL&lt;/h4&gt; 
&lt;p&gt;This may be caused by the models being loaded from &lt;code&gt;/mnt/c&lt;/code&gt;, more details on &lt;a href="https://stackoverflow.com/questions/68972448/why-is-wsl-extremely-slow-when-compared-with-native-windows-npm-yarn-processing"&gt;stackoverflow&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Tracking down errors&lt;/h4&gt; 
&lt;p&gt;You can set &lt;code&gt;RUST_BACKTRACE=1&lt;/code&gt; to be provided with backtraces when a candle error is generated.&lt;/p&gt; 
&lt;h4&gt;CudaRC error&lt;/h4&gt; 
&lt;p&gt;If you encounter an error like this one &lt;code&gt;called &lt;/code&gt;Result::unwrap()&lt;code&gt;on an&lt;/code&gt;Err&lt;code&gt; value: LoadLibraryExW { source: Os { code: 126, kind: Uncategorized, message: "The specified module could not be found." } }&lt;/code&gt; on windows. To fix copy and rename these 3 files (make sure they are in path). The paths depend on your cuda version. &lt;code&gt;c:\Windows\System32\nvcuda.dll&lt;/code&gt; -&amp;gt; &lt;code&gt;cuda.dll&lt;/code&gt; &lt;code&gt;c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin\cublas64_12.dll&lt;/code&gt; -&amp;gt; &lt;code&gt;cublas.dll&lt;/code&gt; &lt;code&gt;c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin\curand64_10.dll&lt;/code&gt; -&amp;gt; &lt;code&gt;curand.dll&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>embassy-rs/embassy</title>
      <link>https://github.com/embassy-rs/embassy</link>
      <description>&lt;p&gt;Modern embedded framework, using Rust and async.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Embassy&lt;/h1&gt; 
&lt;p&gt;Embassy is the next-generation framework for embedded applications. Write safe, correct, and energy-efficient embedded code faster, using the Rust programming language, its async facilities, and the Embassy libraries.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://embassy.dev/book/index.html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.embassy.dev/"&gt;API reference&lt;/a&gt; - &lt;a href="https://embassy.dev/"&gt;Website&lt;/a&gt; - &lt;a href="https://matrix.to/#/#embassy-rs:matrix.org"&gt;Chat&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Rust + async ❤️ embedded&lt;/h2&gt; 
&lt;p&gt;The Rust programming language is blazingly fast and memory-efficient, with no runtime, garbage collector, or OS. It catches a wide variety of bugs at compile time, thanks to its full memory- and thread-safety, and expressive type system.&lt;/p&gt; 
&lt;p&gt;Rust's &lt;a href="https://rust-lang.github.io/async-book/"&gt;async/await&lt;/a&gt; allows for unprecedentedly easy and efficient multitasking in embedded systems. Tasks get transformed at compile time into state machines that get run cooperatively. It requires no dynamic memory allocation and runs on a single stack, so no per-task stack size tuning is required. It obsoletes the need for a traditional RTOS with kernel context switching, and is &lt;a href="https://tweedegolf.nl/en/blog/65/async-rust-vs-rtos-showdown"&gt;faster and smaller than one!&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Batteries included&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hardware Abstraction Layers&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;HALs implement safe, idiomatic Rust APIs to use the hardware capabilities, so raw register manipulation is not needed. The Embassy project maintains HALs for select hardware, but you can still use HALs from other projects with Embassy.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.embassy.dev/embassy-stm32/"&gt;embassy-stm32&lt;/a&gt;, for all STM32 microcontroller families.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.embassy.dev/embassy-nrf/"&gt;embassy-nrf&lt;/a&gt;, for the Nordic Semiconductor nRF52, nRF53, nRF54 and nRF91 series.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.embassy.dev/embassy-rp/"&gt;embassy-rp&lt;/a&gt;, for the Raspberry Pi RP2040 and RP23xx microcontrollers.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.embassy.dev/embassy-mspm0/"&gt;embassy-mspm0&lt;/a&gt;, for the Texas Instruments MSPM0 microcontrollers.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/esp-rs"&gt;esp-rs&lt;/a&gt;, for the Espressif Systems ESP32 series of chips. 
    &lt;ul&gt; 
     &lt;li&gt;Embassy HAL support for Espressif chips, as well as Async Wi-Fi, Bluetooth, and ESP-NOW, is being developed in the &lt;a href="https://github.com/esp-rs/esp-hal"&gt;esp-rs/esp-hal&lt;/a&gt; repository.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/ch32-rs/ch32-hal"&gt;ch32-hal&lt;/a&gt;, for the WCH 32-bit RISC-V(CH32V) series of chips.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/AlexCharlton/mpfs-hal"&gt;mpfs-hal&lt;/a&gt;, for the Microchip PolarFire SoC.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/py32-rs/py32-hal"&gt;py32-hal&lt;/a&gt;, for the Puya Semiconductor PY32 series of microcontrollers.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Time that Just Works&lt;/strong&gt; - No more messing with hardware timers. &lt;a href="https://docs.embassy.dev/embassy-time"&gt;embassy_time&lt;/a&gt; provides Instant, Duration, and Timer types that are globally available and never overflow.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Real-time ready&lt;/strong&gt; - Tasks on the same async executor run cooperatively, but you can create multiple executors with different priorities so that higher priority tasks preempt lower priority ones. See the &lt;a href="https://github.com/embassy-rs/embassy/raw/master/examples/nrf52840/src/bin/multiprio.rs"&gt;example&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Low-power ready&lt;/strong&gt; - Easily build devices with years of battery life. The async executor automatically puts the core to sleep when there's no work to do. Tasks are woken by interrupts, there is no busy-loop polling while waiting.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Networking&lt;/strong&gt; - The &lt;a href="https://docs.embassy.dev/embassy-net/"&gt;embassy-net&lt;/a&gt; network stack implements extensive networking functionality, including Ethernet, IP, TCP, UDP, ICMP, and DHCP. Async drastically simplifies managing timeouts and serving multiple connections concurrently.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bluetooth&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The &lt;a href="https://github.com/embassy-rs/trouble"&gt;trouble&lt;/a&gt; crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the &lt;a href="https://github.com/embassy-rs/bt-hci"&gt;bt-hci&lt;/a&gt; traits (currently &lt;code&gt;nRF52&lt;/code&gt;, &lt;code&gt;rp2040&lt;/code&gt;, &lt;code&gt;rp23xx&lt;/code&gt; and &lt;code&gt;esp32&lt;/code&gt; and &lt;code&gt;serial&lt;/code&gt; controllers are supported).&lt;/li&gt; 
   &lt;li&gt;The &lt;a href="https://github.com/embassy-rs/nrf-softdevice"&gt;nrf-softdevice&lt;/a&gt; crate provides Bluetooth Low Energy 4.x and 5.x support for nRF52 microcontrollers.&lt;/li&gt; 
   &lt;li&gt;The &lt;a href="https://github.com/embassy-rs/embassy/tree/main/embassy-stm32-wpan"&gt;embassy-stm32-wpan&lt;/a&gt; crate provides Bluetooth Low Energy 5.x support for stm32wb microcontrollers.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LoRa&lt;/strong&gt; - The &lt;a href="https://github.com/lora-rs/lora-rs"&gt;lora-rs&lt;/a&gt; project provides an async LoRa and LoRaWAN stack that works well on Embassy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;USB&lt;/strong&gt; - &lt;a href="https://docs.embassy.dev/embassy-usb/"&gt;embassy-usb&lt;/a&gt; implements a device-side USB stack. Implementations for common classes such as USB serial (CDC ACM) and USB HID are available, and a rich builder API allows building your own.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bootloader and DFU&lt;/strong&gt; - &lt;a href="https://github.com/embassy-rs/embassy/tree/master/embassy-boot"&gt;embassy-boot&lt;/a&gt; is a lightweight bootloader supporting firmware application upgrades in a power-fail-safe way, with trial boots and rollbacks.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sneak peek&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-rust,ignore"&gt;use defmt::info;
use embassy_executor::Spawner;
use embassy_time::{Duration, Timer};
use embassy_nrf::gpio::{AnyPin, Input, Level, Output, OutputDrive, Pin, Pull};
use embassy_nrf::{Peri, Peripherals};

// Declare async tasks
#[embassy_executor::task]
async fn blink(pin: Peri&amp;lt;'static, AnyPin&amp;gt;) {
    let mut led = Output::new(pin, Level::Low, OutputDrive::Standard);

    loop {
        // Timekeeping is globally available, no need to mess with hardware timers.
        led.set_high();
        Timer::after_millis(150).await;
        led.set_low();
        Timer::after_millis(150).await;
    }
}

// Main is itself an async task as well.
#[embassy_executor::main]
async fn main(spawner: Spawner) {
    let p = embassy_nrf::init(Default::default());

    // Spawned tasks run in the background, concurrently.
    spawner.spawn(blink(p.P0_13.into()).unwrap());

    let mut button = Input::new(p.P0_11, Pull::Up);
    loop {
        // Asynchronously wait for GPIO events, allowing other tasks
        // to run, or the core to sleep.
        button.wait_for_low().await;
        info!("Button pressed!");
        button.wait_for_high().await;
        info!("Button released!");
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Examples are found in the &lt;code&gt;examples/&lt;/code&gt; folder separated by the chip manufacturer they are designed to run on. For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;examples/nrf52840&lt;/code&gt; run on the &lt;code&gt;nrf52840-dk&lt;/code&gt; board (PCA10056) but should be easily adaptable to other nRF52 chips and boards.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/nrf5340&lt;/code&gt; run on the &lt;code&gt;nrf5340-dk&lt;/code&gt; board (PCA10095).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/stm32xx&lt;/code&gt; for the various STM32 families.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/rp&lt;/code&gt; are for the RP2040 chip.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/std&lt;/code&gt; are designed to run locally on your PC.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Running examples&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;code&gt;probe-rs&lt;/code&gt; following the instructions at &lt;a href="https://probe.rs"&gt;https://probe.rs&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Change directory to the sample's base directory. For example:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd examples/nrf52840
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;Cargo.toml&lt;/code&gt; sets the right feature for the name of the chip you are programming. If this name is incorrect, the example may fail to run or immediately crash after being programmed.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;.cargo/config.toml&lt;/code&gt; contains the name of the chip you are programming.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the example&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo run --release --bin blinky
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more help getting started, see &lt;a href="https://github.com/embassy-rs/embassy/wiki/Getting-Started"&gt;Getting Started&lt;/a&gt; and &lt;a href="https://github.com/embassy-rs/embassy/wiki/Running-the-Examples"&gt;Running the Examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Developing Embassy with Rust Analyzer-based editors&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://rust-analyzer.github.io/"&gt;Rust Analyzer&lt;/a&gt; is used by &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; and others. Given the multiple targets that Embassy serves, there is no Cargo workspace file. Instead, the Rust Analyzer must be told of the target project to work with. In the case of Visual Studio Code, please refer to the &lt;code&gt;.vscode/settings.json&lt;/code&gt; file's &lt;code&gt;rust-analyzer.linkedProjects&lt;/code&gt;setting.&lt;/p&gt; 
&lt;h2&gt;Minimum supported Rust version (MSRV)&lt;/h2&gt; 
&lt;p&gt;Embassy is guaranteed to compile on stable Rust 1.75 and up. It &lt;em&gt;might&lt;/em&gt; compile with older versions, but that may change in any new patch release.&lt;/p&gt; 
&lt;h2&gt;Why the name?&lt;/h2&gt; 
&lt;p&gt;EMBedded ASYnc! :)&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Embassy is licensed under either of&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Apache License, Version 2.0 (&lt;a href="https://raw.githubusercontent.com/embassy-rs/embassy/main/LICENSE-APACHE"&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href="http://www.apache.org/licenses/LICENSE-2.0"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;MIT license (&lt;a href="https://raw.githubusercontent.com/embassy-rs/embassy/main/LICENSE-MIT"&gt;LICENSE-MIT&lt;/a&gt; or &lt;a href="http://opensource.org/licenses/MIT"&gt;http://opensource.org/licenses/MIT&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;at your option.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ArthurBrussee/brush</title>
      <link>https://github.com/ArthurBrussee/brush</link>
      <description>&lt;p&gt;3D Reconstruction for all&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Brush&lt;/h1&gt; 
&lt;p&gt;
 &lt;video src="https://github.com/user-attachments/assets/5756967a-846c-44cf-bde9-3ca4c86f1a4d"&gt;
  A video showing various Brush features and scenes
 &lt;/video&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt; Massive thanks to &lt;a href="https://www.youtube.com/@gradeeterna"&gt;@GradeEterna&lt;/a&gt; for the beautiful scenes &lt;/i&gt; &lt;/p&gt; 
&lt;p&gt;Brush is a 3D reconstruction engine using &lt;a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/"&gt;Gaussian splatting&lt;/a&gt;. It works on a wide range of systems: &lt;strong&gt;macOS/windows/linux&lt;/strong&gt;, &lt;strong&gt;AMD/Nvidia/Intel&lt;/strong&gt; cards, &lt;strong&gt;Android&lt;/strong&gt;, and in a &lt;strong&gt;browser&lt;/strong&gt;. To achieve this, it uses WebGPU compatible tech and the &lt;a href="https://github.com/tracel-ai/burn"&gt;Burn&lt;/a&gt; machine learning framework.&lt;/p&gt; 
&lt;p&gt;Machine learning for real time rendering has tons of potential, but most ML tools don't work well with it: Rendering requires realtime interactivity, usually involve dynamic shapes &amp;amp; computations, don't run on most platforms, and it can be cumbersome to ship apps with large CUDA deps. Brush on the other hand produces simple dependency free binaries, runs on nearly all devices, without any setup.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://arthurbrussee.github.io/brush-demo"&gt;&lt;strong&gt;Try the web demo&lt;/strong&gt; &lt;img src="https://cdn-icons-png.flaticon.com/256/888/888846.png" alt="chrome logo" width="24" /&gt; &lt;/a&gt; &lt;em&gt;NOTE: Only works on Chrome and Edge. Firefox and Safari are hopefully supported soon)&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/TbxJST2BbC"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/TbxJST2BbC" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;p&gt;Brush takes in COLMAP data or datasets in the Nerfstudio format. Training is fully supported natively, on mobile, and in a browser. While training you can interact with the scene and see the training dynamics live, and compare the current rendering to input views as the training progresses.&lt;/p&gt; 
&lt;p&gt;It also supports masking images:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Images with transparency. This will force the final splat to match the transparency of the input.&lt;/li&gt; 
 &lt;li&gt;A folder of images called 'masks'. This ignores parts of the image that are masked out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Viewer&lt;/h2&gt; 
&lt;p&gt;Brush also works well as a splat viewer, including on the web. It can load .ply &amp;amp; .compressed.ply files. You can stream in data from a URL (for a web app, simply append &lt;code&gt;?url=&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Brush also can load .zip of splat files to display them as an animation, or a special ply that includes delta frames (see &lt;a href="https://cat-4d.github.io/"&gt;cat-4D&lt;/a&gt; and &lt;a href="https://felixtaubner.github.io/cap4d/"&gt;Cap4D&lt;/a&gt;!).&lt;/p&gt; 
&lt;h2&gt;CLI&lt;/h2&gt; 
&lt;p&gt;Brush can be used as a CLI. Run &lt;code&gt;brush --help&lt;/code&gt; to get an overview. Every CLI command can work with &lt;code&gt;--with-viewer&lt;/code&gt; which also opens the UI, for easy debugging.&lt;/p&gt; 
&lt;h2&gt;Rerun&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f679fec0-935d-4dd2-87e1-c301db9cdc2c"&gt;https://github.com/user-attachments/assets/f679fec0-935d-4dd2-87e1-c301db9cdc2c&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;While training, additional data can be visualized with the excellent &lt;a href="https://rerun.io/"&gt;rerun&lt;/a&gt;. To install rerun on your machine, please follow their &lt;a href="https://rerun.io/docs/getting-started/installing-viewer"&gt;instructions&lt;/a&gt;. Open the ./brush_blueprint.rbl in the viewer for best results.&lt;/p&gt; 
&lt;h2&gt;Building Brush&lt;/h2&gt; 
&lt;p&gt;First install rust 1.88+. You can run tests with &lt;code&gt;cargo test --all&lt;/code&gt;. Brush uses the wonderful &lt;a href="https://rerun.io/"&gt;rerun&lt;/a&gt; for additional visualizations while training, run &lt;code&gt;cargo install rerun-cli&lt;/code&gt; if you want to use it.&lt;/p&gt; 
&lt;h3&gt;Windows/macOS/Linux&lt;/h3&gt; 
&lt;p&gt;Simply &lt;code&gt;cargo run&lt;/code&gt; or &lt;code&gt;cargo run --release&lt;/code&gt; from the workspace root. Brush can also be used as a CLI, run &lt;code&gt;cargo run --release -- --help&lt;/code&gt; to use the CLI directly from source. See the notes about the CLI in the features section.&lt;/p&gt; 
&lt;h3&gt;Web&lt;/h3&gt; 
&lt;p&gt;Brush can be compiled to WASM. Run &lt;code&gt;npm run dev&lt;/code&gt; to start the demo website using Next.js, see the brush_nextjs directory.&lt;/p&gt; 
&lt;p&gt;Brush uses &lt;a href="https://rustwasm.github.io/wasm-bindgen/introduction.html"&gt;&lt;code&gt;wasm-pack&lt;/code&gt;&lt;/a&gt; to build the WASM bundle. You can also use it without a bundler, see &lt;a href="hhttps://rustwasm.github.io/wasm-bindgen/examples/without-a-bundler.html"&gt;wasm-pack's documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;WebGPU is still an upcoming standard, and as such, only Chrome 134+ on Windows and macOS is currently supported.&lt;/p&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;p&gt;As a one time setup, make sure you have the Android SDK &amp;amp; NDK installed.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check if ANDROID_NDK_HOME and ANDROID_HOME are set&lt;/li&gt; 
 &lt;li&gt;Add the Android target to rust &lt;code&gt;rustup target add aarch64-linux-android&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install cargo-ndk to manage building a lib &lt;code&gt;cargo install cargo-ndk&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each time you change the rust code, run&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;cargo ndk -t arm64-v8a -o crates/brush-app/app/src/main/jniLibs/ build&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Nb: Nb, for best performance, build in release mode. This is separate from the Android Studio app build configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cargo ndk -t arm64-v8a -o crates/brush-app/app/src/main/jniLibs/ build --release&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can now either run the project from Android Studio (Android Studio does NOT build the rust code), or run it from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./gradlew build
./gradlew installDebug
adb shell am start -n com.splats.app/.MainActivity
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also open this folder as a project in Android Studio and run things from there. Nb: Running in Android Studio does &lt;em&gt;not&lt;/em&gt; rebuild the rust code automatically.&lt;/p&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;Rendering and training are generally faster than gsplat. You can run benchmarks of some of the kernels using &lt;code&gt;cargo bench&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/nerfstudio-project/gsplat"&gt;&lt;strong&gt;gSplat&lt;/strong&gt;&lt;/a&gt;, for their reference version of the kernels&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Peter Hedman, George Kopanas &amp;amp; Bernhard Kerbl&lt;/strong&gt;, for the many discussions &amp;amp; pointers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The Burn team&lt;/strong&gt;, for help &amp;amp; improvements to Burn along the way&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Raph Levien&lt;/strong&gt;, for the &lt;a href="https://github.com/googlefonts/compute-shader-101/pull/31"&gt;original version&lt;/a&gt; of the GPU radix sort.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;GradeEterna&lt;/strong&gt;, for feedback and their scenes.&lt;/p&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;This is &lt;em&gt;not&lt;/em&gt; an official Google product. This repository is a forked public version of &lt;a href="https://github.com/google-research/google-research/tree/master/brush_splat"&gt;the google-research repository&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>InfinitiBit/graphbit</title>
      <link>https://github.com/InfinitiBit/graphbit</link>
      <description>&lt;p&gt;GraphBit is the world’s first enterprise-grade Agentic AI framework, built on a Rust core with a Python wrapper for unmatched speed, security, and scalability. It enables reliable multi-agent workflows with minimal CPU and memory usage, making it production-ready for real-world enterprise environments.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;GraphBit - High Performance Agentic Framework&lt;/h1&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/InfinitiBit/graphbit/main/assets/logo(circle).png" width="160px" alt="Logo" /&gt; &lt;/p&gt; 
 &lt;!-- Added placeholders for links, fill it up when the corresponding links are available. --&gt; 
 &lt;p align="center"&gt; &lt;a href="https://graphbit.ai/"&gt;Website&lt;/a&gt; | &lt;a href="https://docs.graphbit.ai/"&gt;Docs&lt;/a&gt; | &lt;a href="https://discord.com/invite/huVJwkyu"&gt;Discord&lt;/a&gt; &lt;br /&gt;&lt;br /&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/InfinitiBit/graphbit/actions/workflows/update-docs.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/InfinitiBit/graphbit/update-docs.yml?branch=main" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/InfinitiBit/graphbit/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://www.rust-lang.org"&gt;&lt;img src="https://img.shields.io/badge/rust-1.70+-blue.svg?sanitize=true" alt="Rust Version" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org"&gt;&lt;img src="https://img.shields.io/badge/python-3.10--3.13-blue.svg?sanitize=true" alt="Python Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Type-Safe AI Agent Workflows with Rust Performance&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Graphbit is an &lt;strong&gt;industry-grade agentic AI framework&lt;/strong&gt; built for developers and AI teams that demand stability, scalability, and low resource usage.&lt;/p&gt; 
&lt;p&gt;Written in &lt;strong&gt;Rust&lt;/strong&gt; for maximum performance and safety, it delivers up to &lt;strong&gt;68× lower CPU usage&lt;/strong&gt; and &lt;strong&gt;140× lower memory&lt;/strong&gt; footprint than certain leading alternatives while consistently using far fewer resources than the rest, all while maintaining comparable throughput and execution speed. See &lt;a href="https://raw.githubusercontent.com/InfinitiBit/graphbit/main/benchmarks/report/framework-benchmark-report.md"&gt;benchmarks&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;Designed to run &lt;strong&gt;multi-agent workflows in parallel&lt;/strong&gt;, Graphbit persists memory across steps, recovers from failures, and ensures &lt;strong&gt;100% task success&lt;/strong&gt; under load. Its lightweight, resource-efficient architecture enables deployment in both &lt;strong&gt;high-scale enterprise environments&lt;/strong&gt; and &lt;strong&gt;low-resource edge scenarios&lt;/strong&gt;. With built-in observability and concurrency support, Graphbit eliminates the bottlenecks that slow decision-making and erode ROI.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tool Selection&lt;/strong&gt; - LLMs intelligently select tools based on descriptions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type Safety&lt;/strong&gt; - Strong typing throughout the execution pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliability&lt;/strong&gt; - Circuit breakers, retry policies, and error handling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-LLM Support&lt;/strong&gt; - OpenAI, Anthropic, DeepSeek, Ollama&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Resource Management&lt;/strong&gt; - Concurrency controls and memory optimization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Observability&lt;/strong&gt; - Built-in metrics and execution tracing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Clone the repository&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/InfinitiBit/graphbit.git
cd graphbit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install Rust&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/macOS&lt;/strong&gt;:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh &amp;amp;&amp;amp; source $HOME/.cargo/env`
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Download &amp;amp; run &lt;a href="https://win.rustup.rs/x86_64"&gt;rustup-init.exe&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Install Poetry&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set up poetry environment, then install dependencies&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry env use python3.11   # Supports from python 3.10 to 3.13
source $(poetry env info --path)/bin/activate
poetry install --no-root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Build from source&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo build --release
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Build Python bindings&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd python/
maturin develop --release
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Environment Setup&lt;/h3&gt; 
&lt;p&gt;First, set up your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_openai_api_key_here
export ANTHROPIC_API_KEY=your_anthropic_api_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Security Note&lt;/strong&gt;: Never commit API keys to version control. Always use environment variables or secure secret management.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os

from graphbit import LlmConfig, Executor, Workflow, Node, tool

# Initialize and configure
config = LlmConfig.openai(os.getenv("OPENAI_API_KEY"), "gpt-4o-mini")

# Create executor
executor = Executor(config)

# Create tools with clear descriptions for LLM selection
@tool(description="Get current weather information for any city")
def get_weather(location: str) -&amp;gt; dict:
    return {"location": location, "temperature": 22, "condition": "sunny"}

@tool(description="Perform mathematical calculations and return results")
def calculate(expression: str) -&amp;gt; str:
    return f"Result: {eval(expression)}"

# Build workflow
workflow = Workflow("Analysis Pipeline")

# Create agent nodes
smart_agent = Node.agent(
    name="Smart Agent",
    prompt="What's the weather in Paris and calculate 15 + 27?",
    system_prompt="You are an assistant skilled in weather lookup and math calculations. Use tools to answer queries accurately.",
    tools=[get_weather, calculate]
)

processor = Node.agent(
    name="Data Processor",
    prompt="Process the results obtained from Smart Agent.",
    system_prompt="""You process and organize results from other agents.

    - Summarize and clarify key points
    - Structure your output for easy reading
    - Focus on actionable insights
    """
)

# Connect and execute
id1 = workflow.add_node(smart_agent)
id2 = workflow.add_node(processor)
workflow.connect(id1, id2)

result = executor.execute(workflow)
print(f"Workflow completed: {result.is_success()}")
print("\nSmart Agent Output: \n", result.get_node_output("Smart Agent"))
print("\nData Processor Output: \n", result.get_node_output("Data Processor"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;High-Level Architecture&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/InfinitiBit/graphbit/main/assets/architecture.svg?sanitize=true" height="250" alt="GraphBit Architecture" /&gt; &lt;/p&gt; 
&lt;p&gt;Three-tier design for reliability and performance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Rust Core&lt;/strong&gt; - Workflow engine, agents, and LLM providers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Orchestration Layer&lt;/strong&gt; - Project management and execution&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python API&lt;/strong&gt; - PyO3 bindings with async support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Python API Integrations&lt;/h2&gt; 
&lt;p&gt;GraphBit provides a rich Python API for building and integrating agentic workflows, including executors, nodes, LLM clients, and embeddings. For the complete list of classes, methods, and usage examples, see the &lt;a href="https://raw.githubusercontent.com/InfinitiBit/graphbit/main/docs/api-reference/python-api.md"&gt;Python API Reference&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing to GraphBit&lt;/h2&gt; 
&lt;p&gt;We welcome contributions. To get started, please see the &lt;a href="https://raw.githubusercontent.com/InfinitiBit/graphbit/main/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; file for development setup and guidelines.&lt;/p&gt; 
&lt;p&gt;GraphBit is built by a wonderful community of researchers and engineers.&lt;/p&gt; 
&lt;a href="https://github.com/InfinitiBit/graphbit/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=InfinitiBit/graphbit&amp;amp;columns=10" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>aptos-labs/aptos-core</title>
      <link>https://github.com/aptos-labs/aptos-core</link>
      <description>&lt;p&gt;Aptos is a layer 1 blockchain built to support the widespread use of blockchain through better technology and user experience.&lt;/p&gt;&lt;hr&gt;&lt;a href="https://aptos.dev"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/aptos-labs/aptos-core/main/.assets/aptos_banner.png" alt="Aptos Banner" /&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aptos-labs/aptos-core/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache-green.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aptos-labs/aptos-core/actions/workflows/lint-test.yaml"&gt;&lt;img src="https://github.com/aptos-labs/aptos-core/actions/workflows/lint-test.yaml/badge.svg?sanitize=true" alt="Lint+Test" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/aptos-labs/aptos-core"&gt;&lt;img src="https://codecov.io/gh/aptos-labs/aptos-core/branch/main/graph/badge.svg?token=X01RKXSGDE" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/aptosnetwork"&gt;&lt;img src="https://img.shields.io/discord/945856774056083548?style=flat-square" alt="Discord chat" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Aptos is a layer 1 blockchain bringing a paradigm shift to Web3 through better technology and user experience. Built with Move to create a home for developers building next-gen applications.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aptosfoundation.org/"&gt;Aptos Foundation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aptos.dev"&gt;Aptos Developer Network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aptos.dev/guides/system-integrators-guide"&gt;Guide - Integrate with the Aptos Blockchain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aptos.dev/tutorials"&gt;Tutorials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Follow us on &lt;a href="https://twitter.com/Aptos"&gt;Twitter&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join us on the &lt;a href="https://discord.gg/aptosnetwork"&gt;Aptos Discord&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;You can learn more about contributing to the Aptos project by reading our &lt;a href="https://github.com/aptos-labs/aptos-core/raw/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; and by viewing our &lt;a href="https://github.com/aptos-labs/aptos-core/raw/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Aptos Core is licensed under &lt;a href="https://github.com/aptos-labs/aptos-core/raw/main/LICENSE"&gt;Apache 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/codex</title>
      <link>https://github.com/openai/codex</link>
      <description>&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;OpenAI Codex CLI&lt;/h1&gt; 
&lt;p align="center"&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer. &lt;br /&gt; &lt;br /&gt;If you want Codex in your code editor (VS Code, Cursor, Windsurf), &lt;a href="https://developers.openai.com/codex/ide"&gt;install in your IDE&lt;/a&gt; &lt;br /&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, go to &lt;a href="https://chatgpt.com/codex"&gt;chatgpt.com/codex&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png" alt="Codex CLI splash" width="80%" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; 
&lt;p&gt;Install globally with your preferred package manager. If you use npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @openai/codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you use Homebrew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;brew install codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can also go to the &lt;a href="https://github.com/openai/codex/releases/latest"&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; 
 &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS 
   &lt;ul&gt; 
    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Linux 
   &lt;ul&gt; 
    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-login.png" alt="Codex CLI login" width="80%" /&gt; &lt;/p&gt; 
&lt;p&gt;Run &lt;code&gt;codex&lt;/code&gt; and select &lt;strong&gt;Sign in with ChatGPT&lt;/strong&gt;. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan. &lt;a href="https://help.openai.com/en/articles/11369540-codex-in-chatgpt"&gt;Learn more about what's included in your ChatGPT plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also use Codex with an API key, but this requires &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#usage-based-billing-alternative-use-an-openai-api-key"&gt;additional setup&lt;/a&gt;. If you previously used an API key for usage-based billing, see the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#migrating-from-usage-based-billing-api-key"&gt;migration steps&lt;/a&gt;. If you're having trouble with login, please comment on &lt;a href="https://github.com/openai/codex/issues/1243"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Model Context Protocol (MCP)&lt;/h3&gt; 
&lt;p&gt;Codex CLI supports &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#model-context-protocol-mcp"&gt;MCP servers&lt;/a&gt;. Enable by adding an &lt;code&gt;mcp_servers&lt;/code&gt; section to your &lt;code&gt;~/.codex/config.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Codex CLI supports a rich set of configuration options, with preferences stored in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. For full configuration options, see &lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/config.md"&gt;Configuration&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Docs &amp;amp; FAQ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md"&gt;&lt;strong&gt;Getting started&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#cli-usage"&gt;CLI usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#running-with-a-prompt-as-input"&gt;Running with a prompt as input&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#example-prompts"&gt;Example prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#memory-with-agentsmd"&gt;Memory with AGENTS.md&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/config.md"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/sandbox.md"&gt;&lt;strong&gt;Sandbox &amp;amp; approvals&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md"&gt;&lt;strong&gt;Authentication&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#forcing-a-specific-auth-method-advanced"&gt;Auth methods&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#connecting-on-a-headless-machine"&gt;Login on a "Headless" machine&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md"&gt;&lt;strong&gt;Advanced&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#non-interactive--ci-mode"&gt;Non-interactive / CI mode&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#tracing--verbose-logging"&gt;Tracing / verbose logging&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/zdr.md"&gt;&lt;strong&gt;Zero data retention (ZDR)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/contributing.md"&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md"&gt;&lt;strong&gt;Install &amp;amp; build&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md#system-requirements"&gt;System Requirements&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md#dotslash"&gt;DotSlash&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/install.md#build-from-source"&gt;Build from source&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/faq.md"&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/docs/open-source-fund.md"&gt;&lt;strong&gt;Open source fund&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vectordotdev/vector</title>
      <link>https://github.com/vectordotdev/vector</link>
      <description>&lt;p&gt;A high-performance observability data pipeline.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/vectordotdev/vector/actions/workflows/nightly.yml"&gt;&lt;img src="https://github.com/vectordotdev/vector/actions/workflows/nightly.yml/badge.svg?sanitize=true" alt="Nightly" /&gt;&lt;/a&gt; &lt;a href="https://github.com/vectordotdev/vector/actions/workflows/e2e.yml"&gt;&lt;img src="https://github.com/vectordotdev/vector/actions/workflows/e2e.yml/badge.svg?sanitize=true" alt="E2E Test Suite" /&gt;&lt;/a&gt; &lt;a href="https://github.com/vectordotdev/vector/actions/workflows/component_features.yml"&gt;&lt;img src="https://github.com/vectordotdev/vector/actions/workflows/component_features.yml/badge.svg?sanitize=true" alt="Component Features" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/vectordotdev/vector/master/website/static/img/diagram.svg?sanitize=true" alt="Vector" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt; &lt;a href="https://vector.dev/docs/setup/quickstart/"&gt;Quickstart&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://vector.dev/docs/"&gt;Docs&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://vector.dev/guides/"&gt;Guides&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://vector.dev/components/"&gt;Integrations&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://chat.vector.dev"&gt;Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://vector.dev/releases/latest/download/"&gt;Download&lt;/a&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp; &lt;a href="https://rust-doc.vector.dev/"&gt;Rust Crate Docs&lt;/a&gt; &lt;/strong&gt; &lt;/p&gt; 
&lt;h2&gt;What is Vector?&lt;/h2&gt; 
&lt;p&gt;Vector is a high-performance, end-to-end (agent &amp;amp; aggregator) observability data pipeline that puts you in control of your observability data. &lt;a href="https://vector.dev/docs/reference/configuration/sources/"&gt;Collect&lt;/a&gt;, &lt;a href="https://vector.dev/docs/reference/configuration/transforms/"&gt;transform&lt;/a&gt;, and &lt;a href="https://vector.dev/docs/reference/configuration/sinks/"&gt;route&lt;/a&gt; all your logs and metrics to any vendors you want today and any other vendors you may want tomorrow. Vector enables dramatic cost reduction, novel data enrichment, and data security where you need it, not where it is most convenient for your vendors. Additionally, it is open source and up to 10x faster than every alternative in the space.&lt;/p&gt; 
&lt;p&gt;To get started, follow our &lt;a href="https://vector.dev/docs/setup/quickstart/"&gt;&lt;strong&gt;quickstart guide&lt;/strong&gt;&lt;/a&gt; or &lt;a href="https://vector.dev/docs/setup/installation/"&gt;&lt;strong&gt;install Vector&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Vector is maintained by Datadog's &lt;a href="https://opensource.datadoghq.com/about/#the-community-open-source-engineering-team"&gt;Community Open Source Engineering team&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Principles&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt; - Built in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, Vector's primary design goal is reliability.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end&lt;/strong&gt; - Deploys as an &lt;a href="https://vector.dev/docs/setup/deployment/roles/#agent"&gt;agent&lt;/a&gt; or &lt;a href="https://vector.dev/docs/setup/deployment/roles/#aggregator"&gt;aggregator&lt;/a&gt;. Vector is a complete platform.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified&lt;/strong&gt; - &lt;a href="https://vector.dev/docs/architecture/data-model/log/"&gt;Logs&lt;/a&gt;, &lt;a href="https://vector.dev/docs/architecture/data-model/metric/"&gt;metrics&lt;/a&gt; (beta), and traces (coming soon). One tool for all of your data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Use cases&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reduce total observability costs.&lt;/li&gt; 
 &lt;li&gt;Transition vendors without disrupting workflows.&lt;/li&gt; 
 &lt;li&gt;Enhance data quality and improve insights.&lt;/li&gt; 
 &lt;li&gt;Consolidate agents and eliminate agent fatigue.&lt;/li&gt; 
 &lt;li&gt;Improve overall observability performance and reliability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vector is relied on by startups and enterprises like &lt;strong&gt;Atlassian&lt;/strong&gt;, &lt;strong&gt;T-Mobile&lt;/strong&gt;, &lt;strong&gt;Comcast&lt;/strong&gt;, &lt;strong&gt;Zendesk&lt;/strong&gt;, &lt;strong&gt;Discord&lt;/strong&gt;, &lt;strong&gt;Fastly&lt;/strong&gt;, &lt;strong&gt;CVS&lt;/strong&gt;, &lt;strong&gt;Trivago&lt;/strong&gt;, &lt;strong&gt;Tuple&lt;/strong&gt;, &lt;strong&gt;Douban&lt;/strong&gt;, &lt;strong&gt;Visa&lt;/strong&gt;, &lt;strong&gt;Mambu&lt;/strong&gt;, &lt;strong&gt;Blockfi&lt;/strong&gt;, &lt;strong&gt;Claranet&lt;/strong&gt;, &lt;strong&gt;Instacart&lt;/strong&gt;, &lt;strong&gt;Forcepoint&lt;/strong&gt;, and &lt;a href="https://github.com/vectordotdev/vector/issues/790"&gt;many more&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Vector is &lt;strong&gt;downloaded over 100,000 times per day&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Vector's largest user &lt;strong&gt;processes over 500TB daily&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Vector has &lt;strong&gt;over 500 contributors&lt;/strong&gt; and growing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;All user documentation is available at &lt;strong&gt;&lt;a href="https://vector.dev/docs"&gt;vector.dev/docs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Other Resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://calendar.vector.dev"&gt;&lt;strong&gt;Vector Calendar&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Policies&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/CODE_OF_CONDUCT.md"&gt;&lt;strong&gt;Code of Conduct&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/PRIVACY.md"&gt;&lt;strong&gt;Privacy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/RELEASES.md"&gt;&lt;strong&gt;Releases&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/raw/master/VERSIONING.md"&gt;&lt;strong&gt;Versioning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/vectordotdev/vector/security/policy"&gt;&lt;strong&gt;Security&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Comparisons&lt;/h2&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;p&gt;The following performance tests demonstrate baseline performance between common protocols with the exception of the Regex Parsing test.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Test&lt;/th&gt; 
   &lt;th&gt;Vector&lt;/th&gt; 
   &lt;th&gt;Filebeat&lt;/th&gt; 
   &lt;th&gt;FluentBit&lt;/th&gt; 
   &lt;th&gt;FluentD&lt;/th&gt; 
   &lt;th&gt;Logstash&lt;/th&gt; 
   &lt;th&gt;SplunkUF&lt;/th&gt; 
   &lt;th&gt;SplunkHF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_blackhole_performance"&gt;TCP to Blackhole&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;86mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;64.4mib/s&lt;/td&gt; 
   &lt;td&gt;27.7mib/s&lt;/td&gt; 
   &lt;td&gt;40.6mib/s&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_to_tcp_performance"&gt;File to TCP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;76.7mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;7.8mib/s&lt;/td&gt; 
   &lt;td&gt;35mib/s&lt;/td&gt; 
   &lt;td&gt;26.1mib/s&lt;/td&gt; 
   &lt;td&gt;3.1mib/s&lt;/td&gt; 
   &lt;td&gt;40.1mib/s&lt;/td&gt; 
   &lt;td&gt;39mib/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/regex_parsing_performance"&gt;Regex Parsing&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;13.2mib/s&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;20.5mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;2.6mib/s&lt;/td&gt; 
   &lt;td&gt;4.6mib/s&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;7.8mib/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_http_performance"&gt;TCP to HTTP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;26.7mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;19.6mib/s&lt;/td&gt; 
   &lt;td&gt;&amp;lt;1mib/s&lt;/td&gt; 
   &lt;td&gt;2.7mib/s&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
   &lt;td&gt;n/a&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/tcp_to_tcp_performance"&gt;TCP to TCP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;69.9mib/s&lt;/td&gt; 
   &lt;td&gt;5mib/s&lt;/td&gt; 
   &lt;td&gt;67.1mib/s&lt;/td&gt; 
   &lt;td&gt;3.9mib/s&lt;/td&gt; 
   &lt;td&gt;10mib/s&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;&lt;strong&gt;70.4mib/s&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td&gt;7.6mib/s&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To learn more about our performance tests, please see the &lt;a href="https://github.com/vectordotdev/vector-test-harness/"&gt;Vector test harness&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Correctness&lt;/h3&gt; 
&lt;p&gt;The following correctness tests are not exhaustive, but they demonstrate fundamental differences in quality and attention to detail:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Test&lt;/th&gt; 
   &lt;th&gt;Vector&lt;/th&gt; 
   &lt;th&gt;Filebeat&lt;/th&gt; 
   &lt;th&gt;FluentBit&lt;/th&gt; 
   &lt;th&gt;FluentD&lt;/th&gt; 
   &lt;th&gt;Logstash&lt;/th&gt; 
   &lt;th&gt;Splunk UF&lt;/th&gt; 
   &lt;th&gt;Splunk HF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/disk_buffer_persistence_correctness"&gt;Disk Buffer Persistence&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_create_correctness"&gt;File Rotate (create)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_rotate_truncate_correctness"&gt;File Rotate (copytruncate)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/file_truncate_correctness"&gt;File Truncation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/sighup_correctness"&gt;Process (SIGHUP)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vectordotdev/vector-test-harness/tree/master/cases/wrapped_json_correctness"&gt;JSON (wrapped)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To learn more about our correctness tests, please see the &lt;a href="https://github.com/vectordotdev/vector-test-harness/"&gt;Vector test harness&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;p&gt;Vector is an end-to-end, unified, open data platform.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Vector&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;Beats&lt;/th&gt; 
   &lt;th&gt;Fluentbit&lt;/th&gt; 
   &lt;th&gt;Fluentd&lt;/th&gt; 
   &lt;th&gt;Logstash&lt;/th&gt; 
   &lt;th&gt;Splunk UF&lt;/th&gt; 
   &lt;th&gt;Splunk HF&lt;/th&gt; 
   &lt;th&gt;Telegraf&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;End-to-end&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Agent&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Aggregator&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Unified&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Logs&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Metrics&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;⚠&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Traces&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Open-source&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vendor-neutral&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Reliability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Memory-safe&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Delivery guarantees&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi-core&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;✓&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
   &lt;td&gt;✓&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;⚠ = Not interoperable, metrics are represented as structured logs&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; Developed with ❤️ by &lt;strong&gt;&lt;a href="https://datadoghq.com"&gt;Datadog&lt;/a&gt;&lt;/strong&gt; - &lt;a href="https://github.com/vectordotdev/vector/security/policy"&gt;Security Policy&lt;/a&gt; - &lt;a href="https://github.com/vectordotdev/vector/raw/master/PRIVACY.md"&gt;Privacy Policy&lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>0xPlaygrounds/rig</title>
      <link>https://github.com/0xPlaygrounds/rig</link>
      <description>&lt;p&gt;⚙️🦀 Build modular and scalable LLM Applications in Rust&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="img/rig-playgrounds-dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="img/rig-playgrounds-light.svg" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/img/rig-playgrounds-light.svg?sanitize=true" style="width: 40%; height: 40%;" alt="Rig logo" /&gt; 
 &lt;/picture&gt; &lt;br /&gt; &lt;a href="https://docs.rig.rs"&gt;&lt;img src="https://img.shields.io/badge/📖 docs-rig.rs-dca282.svg" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://docs.rs/rig-core/latest/rig/"&gt;&lt;img src="https://img.shields.io/badge/docs-API Reference-dca282.svg" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://crates.io/crates/rig-core"&gt;&lt;img src="https://img.shields.io/crates/v/rig-core.svg?color=dca282" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://crates.io/crates/rig-core"&gt;&lt;img src="https://img.shields.io/crates/d/rig-core.svg?color=dca282" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://discord.gg/playgrounds"&gt;&lt;img src="https://img.shields.io/discord/511303648119226382?color=%236d82cc&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/0xPlaygrounds/rig"&gt;&lt;img src="https://img.shields.io/github/stars/0xPlaygrounds/rig?style=social" alt="stars - rig" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/built_with-Rust-dca282.svg?logo=rust" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://twitter.com/ryzomeai"&gt;&lt;img src="https://img.shields.io/twitter/follow/ryzomeai" /&gt;&lt;/a&gt; &amp;nbsp; &lt;br /&gt; &lt;/p&gt; &amp;nbsp; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://docs.rig.rs"&gt;📑 Docs&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://rig.rs"&gt;🌐 Website&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://github.com/0xPlaygrounds/rig/issues/new"&gt;🤝 Contribute&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://docs.rig.rs/guides"&gt;✍🏽 Blogs&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;✨ If you would like to help spread the word about Rig, please consider starring the repo!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Here be dragons! As we plan to ship a torrent of features in the following months, future updates &lt;strong&gt;will&lt;/strong&gt; contain &lt;strong&gt;breaking changes&lt;/strong&gt;. With Rig evolving, we'll annotate changes and highlight migration paths as we encounter them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/#table-of-contents"&gt;Table of contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/#what-is-rig"&gt;What is Rig?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/#high-level-features"&gt;High-level features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/#who-is-using-rig-in-production"&gt;Who's using Rig in production?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/#get-started"&gt;Get Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/#simple-example"&gt;Simple example:&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/#integrations"&gt;Integrations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What is Rig?&lt;/h2&gt; 
&lt;p&gt;Rig is a Rust library for building scalable, modular, and ergonomic &lt;strong&gt;LLM-powered&lt;/strong&gt; applications.&lt;/p&gt; 
&lt;p&gt;More information about this crate can be found in the &lt;a href="https://docs.rig.rs"&gt;official&lt;/a&gt; &amp;amp; &lt;a href="https://docs.rs/rig-core/latest/rig/"&gt;crate&lt;/a&gt; (API Reference) documentations.&lt;/p&gt; 
&lt;h2&gt;High-level features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Full support for LLM completion and embedding workflows&lt;/li&gt; 
 &lt;li&gt;Simple but powerful common abstractions over LLM providers (e.g. OpenAI, Cohere) and vector stores (e.g. MongoDB, SQlite, in-memory)&lt;/li&gt; 
 &lt;li&gt;Integrate LLMs in your app with minimal boilerplate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Who is using Rig in production?&lt;/h2&gt; 
&lt;p&gt;Below is a non-exhaustive list of companies and people who are using Rig in production:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/firstbatchxyz/dkn-compute-node"&gt;Dria Compute Node&lt;/a&gt; - a node that serves computation results within the Dria Knowledge Network&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelcontextprotocol/rust-sdk"&gt;The MCP Rust SDK&lt;/a&gt; - the official Model Context Protocol Rust SDK. Has an example for usage with Rig.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/buger/probe"&gt;Probe&lt;/a&gt; - an AI-friendly, fully local semantic code search tool.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NethermindEth/nine"&gt;NINE&lt;/a&gt; - Neural Interconnected Nodes Engine, by &lt;a href="https://www.nethermind.io/"&gt;Nethermind.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/0xPlaygrounds/rig-onchain-kit"&gt;rig-onchain-kit&lt;/a&gt; - the Rig Onchain Kit. Intended to make interactions between Solana/EVM and Rig much easier to implement.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/linera-io/linera-protocol"&gt;Linera Protocol&lt;/a&gt; - Decentralized blockchain infrastructure designed for highly scalable, secure, low-latency Web3 applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/piotrostr/listen"&gt;Listen&lt;/a&gt; - A framework aiming to become the go-to framework for AI portfolio management agents. Powers &lt;a href="https://app.listen-rs.com/"&gt;the Listen app.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Are you also using Rig in production? &lt;a href="https://www.github.com/0xPlaygrounds/rig/issues"&gt;Open an issue&lt;/a&gt; to have your name added!&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo add rig-core
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Simple example:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use rig::{completion::Prompt, providers::openai};

#[tokio::main]
async fn main() {
    // Create OpenAI client and model
    // This requires the `OPENAI_API_KEY` environment variable to be set.
    let openai_client = openai::Client::from_env();

    let gpt4 = openai_client.agent("gpt-4").build();

    // Prompt the model and print its response
    let response = gpt4
        .prompt("Who are you?")
        .await
        .expect("Failed to prompt GPT-4");

    println!("GPT-4: {response}");
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note using &lt;code&gt;#[tokio::main]&lt;/code&gt; requires you enable tokio's &lt;code&gt;macros&lt;/code&gt; and &lt;code&gt;rt-multi-thread&lt;/code&gt; features or just &lt;code&gt;full&lt;/code&gt; to enable all features (&lt;code&gt;cargo add tokio --features macros,rt-multi-thread&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;You can find more examples each crate's &lt;code&gt;examples&lt;/code&gt; (ie. &lt;a href="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/rig-core/examples"&gt;&lt;code&gt;rig-core/examples&lt;/code&gt;&lt;/a&gt;) directory. More detailed use cases walkthroughs are regularly published on our &lt;a href="https://dev.to/0thtachi"&gt;Dev.to Blog&lt;/a&gt; and added to Rig's official documentation &lt;a href="http://docs.rig.rs"&gt;(docs.rig.rs)&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Supported Integrations&lt;/h2&gt; 
&lt;p&gt;Vector stores are available as separate companion-crates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MongoDB: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-mongodb"&gt;&lt;code&gt;rig-mongodb&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;LanceDB: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-lancedb"&gt;&lt;code&gt;rig-lancedb&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Neo4j: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-neo4j"&gt;&lt;code&gt;rig-neo4j&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Qdrant: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-qdrant"&gt;&lt;code&gt;rig-qdrant&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SQLite: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-sqlite"&gt;&lt;code&gt;rig-sqlite&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SurrealDB: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-surrealdb"&gt;&lt;code&gt;rig-surrealdb&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Milvus: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-milvus"&gt;&lt;code&gt;rig-milvus&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ScyllaDB: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-scylladb"&gt;&lt;code&gt;rig-scylladb&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AWS S3Vectors: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-s3vectors"&gt;&lt;code&gt;rig-s3vectors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following providers are available as separate companion-crates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fastembed: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-fastembed"&gt;&lt;code&gt;rig-fastembed&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Eternal AI: &lt;a href="https://github.com/0xPlaygrounds/rig/tree/main/rig-eternalai"&gt;&lt;code&gt;rig-eternalai&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;br /&gt; &lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/0xPlaygrounds/rig/main/img/built-by-playgrounds.svg?sanitize=true" alt="Build by Playgrounds" width="30%" /&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>cloud-hypervisor/cloud-hypervisor</title>
      <link>https://github.com/cloud-hypervisor/cloud-hypervisor</link>
      <description>&lt;p&gt;A Virtual Machine Monitor for modern Cloud workloads. Features include CPU, memory and device hotplug, support for running Windows and Linux guests, device offload with vhost-user and a minimal compact footprint. Written in Rust with a strong focus on security.&lt;/p&gt;&lt;hr&gt;&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#1-what-is-cloud-hypervisor"&gt;1. What is Cloud Hypervisor?&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#objectives"&gt;Objectives&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#high-level"&gt;High Level&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#architectures"&gt;Architectures&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#guest-os"&gt;Guest OS&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#2-getting-started"&gt;2. Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#host-os"&gt;Host OS&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#use-pre-built-binaries"&gt;Use Pre-built Binaries&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#packages"&gt;Packages&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#building-from-source"&gt;Building from Source&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#booting-linux"&gt;Booting Linux&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#firmware-booting"&gt;Firmware Booting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#custom-kernel-and-disk-image"&gt;Custom Kernel and Disk Image&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#building-your-kernel"&gt;Building your Kernel&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#disk-image"&gt;Disk image&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#booting-the-guest-vm"&gt;Booting the guest VM&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#3-status"&gt;3. Status&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#hot-plug"&gt;Hot Plug&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#device-model"&gt;Device Model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#4-relationship-with-rust-vmm-project"&gt;4. Relationship with &lt;em&gt;Rust VMM&lt;/em&gt; Project&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#differences-with-firecracker-and-crosvm"&gt;Differences with Firecracker and crosvm&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#5-community"&gt;5. Community&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#contribute"&gt;Contribute&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#slack"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#mailing-list"&gt;Mailing list&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/#security-issues"&gt;Security issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;1. What is Cloud Hypervisor?&lt;/h1&gt; 
&lt;p&gt;Cloud Hypervisor is an open source Virtual Machine Monitor (VMM) that runs on top of the &lt;a href="https://www.kernel.org/doc/Documentation/virtual/kvm/api.txt"&gt;KVM&lt;/a&gt; hypervisor and the Microsoft Hypervisor (MSHV).&lt;/p&gt; 
&lt;p&gt;The project focuses on running modern, &lt;em&gt;Cloud Workloads&lt;/em&gt;, on specific, common, hardware architectures. In this case &lt;em&gt;Cloud Workloads&lt;/em&gt; refers to those that are run by customers inside a Cloud Service Provider. This means modern operating systems with most I/O handled by paravirtualised devices (e.g. &lt;em&gt;virtio&lt;/em&gt;), no requirement for legacy devices, and 64-bit CPUs.&lt;/p&gt; 
&lt;p&gt;Cloud Hypervisor is implemented in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; and is based on the &lt;a href="https://github.com/rust-vmm"&gt;Rust VMM&lt;/a&gt; crates.&lt;/p&gt; 
&lt;h2&gt;Objectives&lt;/h2&gt; 
&lt;h3&gt;High Level&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Runs on KVM or MSHV&lt;/li&gt; 
 &lt;li&gt;Minimal emulation&lt;/li&gt; 
 &lt;li&gt;Low latency&lt;/li&gt; 
 &lt;li&gt;Low memory footprint&lt;/li&gt; 
 &lt;li&gt;Low complexity&lt;/li&gt; 
 &lt;li&gt;High performance&lt;/li&gt; 
 &lt;li&gt;Small attack surface&lt;/li&gt; 
 &lt;li&gt;64-bit support only&lt;/li&gt; 
 &lt;li&gt;CPU, memory, PCI hotplug&lt;/li&gt; 
 &lt;li&gt;Machine to machine migration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Architectures&lt;/h3&gt; 
&lt;p&gt;Cloud Hypervisor supports the &lt;code&gt;x86-64&lt;/code&gt;, &lt;code&gt;AArch64&lt;/code&gt; and &lt;code&gt;riscv64&lt;/code&gt; architectures, with functionality varying across these platforms. The functionality differences between &lt;code&gt;x86-64&lt;/code&gt; and &lt;code&gt;AArch64&lt;/code&gt; are documented in &lt;a href="https://github.com/cloud-hypervisor/cloud-hypervisor/issues/1125"&gt;#1125&lt;/a&gt;. The &lt;code&gt;riscv64&lt;/code&gt; architecture support is experimental and offers limited functionality. For more details and instructions, please refer to &lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/docs/riscv.md"&gt;riscv documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Guest OS&lt;/h3&gt; 
&lt;p&gt;Cloud Hypervisor supports &lt;code&gt;64-bit Linux&lt;/code&gt; and Windows 10/Windows Server 2019.&lt;/p&gt; 
&lt;h1&gt;2. Getting Started&lt;/h1&gt; 
&lt;p&gt;The following sections describe how to build and run Cloud Hypervisor.&lt;/p&gt; 
&lt;h2&gt;Prerequisites for AArch64&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;AArch64 servers (recommended) or development boards equipped with the GICv3 interrupt controller.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Host OS&lt;/h2&gt; 
&lt;p&gt;For required KVM functionality and adequate performance the recommended host kernel version is 5.13. The majority of the CI currently tests with kernel version 5.15.&lt;/p&gt; 
&lt;h2&gt;Use Pre-built Binaries&lt;/h2&gt; 
&lt;p&gt;The recommended approach to getting started with Cloud Hypervisor is by using a pre-built binary. Binaries are available for the &lt;a href="https://github.com/cloud-hypervisor/cloud-hypervisor/releases/latest"&gt;latest release&lt;/a&gt;. Use &lt;code&gt;cloud-hypervisor-static&lt;/code&gt; for &lt;code&gt;x86-64&lt;/code&gt; or &lt;code&gt;cloud-hypervisor-static-aarch64&lt;/code&gt; for &lt;code&gt;AArch64&lt;/code&gt; platform.&lt;/p&gt; 
&lt;h2&gt;Packages&lt;/h2&gt; 
&lt;p&gt;For convenience, packages are also available targeting some popular Linux distributions. This is thanks to the &lt;a href="https://build.opensuse.org"&gt;Open Build Service&lt;/a&gt;. The &lt;a href="https://github.com/cloud-hypervisor/obs-packaging"&gt;OBS README&lt;/a&gt; explains how to enable the repository in a supported Linux distribution and install Cloud Hypervisor and accompanying packages. Please report any packaging issues in the &lt;a href="https://github.com/cloud-hypervisor/obs-packaging"&gt;obs-packaging&lt;/a&gt; repository.&lt;/p&gt; 
&lt;h2&gt;Building from Source&lt;/h2&gt; 
&lt;p&gt;Please see the &lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/docs/building.md"&gt;instructions for building from source&lt;/a&gt; if you do not wish to use the pre-built binaries.&lt;/p&gt; 
&lt;h2&gt;Booting Linux&lt;/h2&gt; 
&lt;p&gt;Cloud Hypervisor supports direct kernel boot (the x86-64 kernel requires the kernel built with PVH support or a bzImage) or booting via a firmware (either &lt;a href="https://github.com/cloud-hypervisor/rust-hypervisor-firmware"&gt;Rust Hypervisor Firmware&lt;/a&gt; or an edk2 UEFI firmware called &lt;code&gt;CLOUDHV&lt;/code&gt; / &lt;code&gt;CLOUDHV_EFI&lt;/code&gt;.)&lt;/p&gt; 
&lt;p&gt;Binary builds of the firmware files are available for the latest release of &lt;a href="https://github.com/cloud-hypervisor/rust-hypervisor-firmware/releases/latest"&gt;Rust Hypervisor Firmware&lt;/a&gt; and &lt;a href="https://github.com/cloud-hypervisor/edk2/releases/latest"&gt;our edk2 repository&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The choice of firmware depends on your guest OS choice; some experimentation may be required.&lt;/p&gt; 
&lt;h3&gt;Firmware Booting&lt;/h3&gt; 
&lt;p&gt;Cloud Hypervisor supports booting disk images containing all needed components to run cloud workloads, a.k.a. cloud images.&lt;/p&gt; 
&lt;p&gt;The following sample commands will download an Ubuntu Cloud image, converting it into a format that Cloud Hypervisor can use and a firmware to boot the image with.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img
$ qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw
$ wget https://github.com/cloud-hypervisor/rust-hypervisor-firmware/releases/download/0.4.2/hypervisor-fw
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Ubuntu cloud images do not ship with a default password so it necessary to use a &lt;code&gt;cloud-init&lt;/code&gt; disk image to customise the image on the first boot. A basic &lt;code&gt;cloud-init&lt;/code&gt; image is generated by this &lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/scripts/create-cloud-init.sh"&gt;script&lt;/a&gt;. This seeds the image with a default username/password of &lt;code&gt;cloud/cloud123&lt;/code&gt;. It is only necessary to add this disk image on the first boot. Script also assigns default IP address using &lt;code&gt;test_data/cloud-init/ubuntu/local/network-config&lt;/code&gt; details with &lt;code&gt;--net "mac=12:34:56:78:90:ab,tap="&lt;/code&gt; option. Then the matching mac address interface will be enabled as per &lt;code&gt;network-config&lt;/code&gt; details.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo setcap cap_net_admin+ep ./cloud-hypervisor
$ ./create-cloud-init.sh
$ ./cloud-hypervisor \
	--firmware ./hypervisor-fw \
	--disk path=focal-server-cloudimg-amd64.raw path=/tmp/ubuntu-cloudinit.img \
	--cpus boot=4 \
	--memory size=1024M \
	--net "tap=,mac=,ip=,mask="
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If access to the firmware messages or interaction with the boot loader (e.g. GRUB) is required then it necessary to switch to the serial console instead of &lt;code&gt;virtio-console&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ ./cloud-hypervisor \
	--kernel ./hypervisor-fw \
	--disk path=focal-server-cloudimg-amd64.raw path=/tmp/ubuntu-cloudinit.img \
	--cpus boot=4 \
	--memory size=1024M \
	--net "tap=,mac=,ip=,mask=" \
	--serial tty \
	--console off
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Booting: &lt;code&gt;--firmware&lt;/code&gt; vs &lt;code&gt;--kernel&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;The following scenarios are supported by Cloud Hypervisor to bootstrap a VM, i.e., to load a payload/bootitem(s):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Provide firmware&lt;/li&gt; 
 &lt;li&gt;Provide kernel [+ cmdline]\ [+ initrd]&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please note that our Cloud Hypervisor firmware (&lt;code&gt;hypervisor-fw&lt;/code&gt;) has a Xen PVH boot entry, therefore it can also be booted via the &lt;code&gt;--kernel&lt;/code&gt; parameter, as seen in some examples.&lt;/p&gt; 
&lt;h3&gt;Custom Kernel and Disk Image&lt;/h3&gt; 
&lt;h4&gt;Building your Kernel&lt;/h4&gt; 
&lt;p&gt;Cloud Hypervisor also supports direct kernel boot. For x86-64, a &lt;code&gt;vmlinux&lt;/code&gt; ELF kernel (compiled with PVH support) or a regular bzImage are supported. In order to support development there is a custom branch; however provided the required options are enabled any recent kernel will suffice.&lt;/p&gt; 
&lt;p&gt;To build the kernel:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Clone the Cloud Hypervisor Linux branch
$ git clone --depth 1 https://github.com/cloud-hypervisor/linux.git -b ch-6.12.8 linux-cloud-hypervisor
$ pushd linux-cloud-hypervisor
$ make ch_defconfig
# Do native build of the x86-64 kernel
$ KCFLAGS="-Wa,-mx86-used-note=no" make bzImage -j `nproc`
# Do native build of the AArch64 kernel
$ make -j `nproc`
$ popd
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For x86-64, the &lt;code&gt;vmlinux&lt;/code&gt; kernel image will then be located at &lt;code&gt;linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin&lt;/code&gt;. For AArch64, the &lt;code&gt;Image&lt;/code&gt; kernel image will then be located at &lt;code&gt;linux-cloud-hypervisor/arch/arm64/boot/Image&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Disk image&lt;/h4&gt; 
&lt;p&gt;For the disk image the same Ubuntu image as before can be used. This contains an &lt;code&gt;ext4&lt;/code&gt; root filesystem.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img # x86-64
$ wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-arm64.img # AArch64
$ qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw # x86-64
$ qemu-img convert -p -f qcow2 -O raw focal-server-cloudimg-arm64.img focal-server-cloudimg-arm64.raw # AArch64
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Booting the guest VM&lt;/h4&gt; 
&lt;p&gt;These sample commands boot the disk image using the custom kernel whilst also supplying the desired kernel command line.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;x86-64&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo setcap cap_net_admin+ep ./cloud-hypervisor
$ ./create-cloud-init.sh
$ ./cloud-hypervisor \
	--kernel ./linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin \
	--disk path=focal-server-cloudimg-amd64.raw path=/tmp/ubuntu-cloudinit.img \
	--cmdline "console=hvc0 root=/dev/vda1 rw" \
	--cpus boot=4 \
	--memory size=1024M \
	--net "tap=,mac=,ip=,mask="
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;AArch64&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ sudo setcap cap_net_admin+ep ./cloud-hypervisor
$ ./create-cloud-init.sh
$ ./cloud-hypervisor \
	--kernel ./linux-cloud-hypervisor/arch/arm64/boot/Image \
	--disk path=focal-server-cloudimg-arm64.raw path=/tmp/ubuntu-cloudinit.img \
	--cmdline "console=hvc0 root=/dev/vda1 rw" \
	--cpus boot=4 \
	--memory size=1024M \
	--net "tap=,mac=,ip=,mask="
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If earlier kernel messages are required the serial console should be used instead of &lt;code&gt;virtio-console&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;x86-64&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ ./cloud-hypervisor \
	--kernel ./linux-cloud-hypervisor/arch/x86/boot/compressed/vmlinux.bin \
	--console off \
	--serial tty \
	--disk path=focal-server-cloudimg-amd64.raw \
	--cmdline "console=ttyS0 root=/dev/vda1 rw" \
	--cpus boot=4 \
	--memory size=1024M \
	--net "tap=,mac=,ip=,mask="
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;AArch64&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ ./cloud-hypervisor \
	--kernel ./linux-cloud-hypervisor/arch/arm64/boot/Image \
	--console off \
	--serial tty \
	--disk path=focal-server-cloudimg-arm64.raw \
	--cmdline "console=ttyAMA0 root=/dev/vda1 rw" \
	--cpus boot=4 \
	--memory size=1024M \
	--net "tap=,mac=,ip=,mask="
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;3. Status&lt;/h1&gt; 
&lt;p&gt;Cloud Hypervisor is under active development. The following stability guarantees are currently made:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;The API (including command line options) will not be removed or changed in a breaking way without a minimum of 2 major releases notice. Where possible warnings will be given about the use of deprecated functionality and the deprecations will be documented in the release notes.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Point releases will be made between individual releases where there are substantial bug fixes or security issues that need to be fixed. These point releases will only include bug fixes.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Currently the following items are &lt;strong&gt;not&lt;/strong&gt; guaranteed across updates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Snapshot/restore is not supported across different versions&lt;/li&gt; 
 &lt;li&gt;Live migration is not supported across different versions&lt;/li&gt; 
 &lt;li&gt;The following features are considered experimental and may change substantially between releases: TDX, vfio-user, vDPA.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Further details can be found in the &lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/docs/releases.md"&gt;release documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;As of 2023-01-03, the following cloud images are supported:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://cloud-images.ubuntu.com/focal/current/"&gt;Ubuntu Focal&lt;/a&gt; (focal-server-cloudimg-{amd64,arm64}.img)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud-images.ubuntu.com/jammy/current/"&gt;Ubuntu Jammy&lt;/a&gt; (jammy-server-cloudimg-{amd64,arm64}.img)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud-images.ubuntu.com/noble/current/"&gt;Ubuntu Noble&lt;/a&gt; (noble-server-cloudimg-{amd64,arm64}.img)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://archives.fedoraproject.org/pub/archive/fedora/linux/releases/36/Cloud/"&gt;Fedora 36&lt;/a&gt; (&lt;a href="https://archives.fedoraproject.org/pub/archive/fedora/linux/releases/36/Cloud/x86_64/images/"&gt;Fedora-Cloud-Base-36-1.5.x86_64.raw.xz&lt;/a&gt; / &lt;a href="https://archives.fedoraproject.org/pub/archive/fedora/linux/releases/36/Cloud/aarch64/images/"&gt;Fedora-Cloud-Base-36-1.5.aarch64.raw.xz&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Direct kernel boot to userspace should work with a rootfs from most distributions although you may need to enable exotic filesystem types in the reference kernel configuration (e.g. XFS or btrfs.)&lt;/p&gt; 
&lt;h2&gt;Hot Plug&lt;/h2&gt; 
&lt;p&gt;Cloud Hypervisor supports hotplug of CPUs, passthrough devices (VFIO), &lt;code&gt;virtio-{net,block,pmem,fs,vsock}&lt;/code&gt; and memory resizing. This &lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/docs/hotplug.md"&gt;document&lt;/a&gt; details how to add devices to a running VM.&lt;/p&gt; 
&lt;h2&gt;Device Model&lt;/h2&gt; 
&lt;p&gt;Details of the device model can be found in this &lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/docs/device_model.md"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;The project roadmap is tracked through a &lt;a href="https://github.com/orgs/cloud-hypervisor/projects/6"&gt;GitHub project&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;4. Relationship with &lt;em&gt;Rust VMM&lt;/em&gt; Project&lt;/h1&gt; 
&lt;p&gt;In order to satisfy the design goal of having a high-performance, security-focused hypervisor the decision was made to use the &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; programming language. The language's strong focus on memory and thread safety makes it an ideal candidate for implementing VMMs.&lt;/p&gt; 
&lt;p&gt;Instead of implementing the VMM components from scratch, Cloud Hypervisor is importing the &lt;a href="https://github.com/rust-vmm"&gt;Rust VMM&lt;/a&gt; crates, and sharing code and architecture together with other VMMs like e.g. Amazon's &lt;a href="https://firecracker-microvm.github.io/"&gt;Firecracker&lt;/a&gt; and Google's &lt;a href="https://chromium.googlesource.com/chromiumos/platform/crosvm/"&gt;crosvm&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Cloud Hypervisor embraces the &lt;em&gt;Rust VMM&lt;/em&gt; project's goals, which is to be able to share and re-use as many virtualization crates as possible.&lt;/p&gt; 
&lt;h2&gt;Differences with Firecracker and crosvm&lt;/h2&gt; 
&lt;p&gt;A large part of the Cloud Hypervisor code is based on either the Firecracker or the crosvm project's implementations. Both of these are VMMs written in Rust with a focus on safety and security, like Cloud Hypervisor.&lt;/p&gt; 
&lt;p&gt;The goal of the Cloud Hypervisor project differs from the aforementioned projects in that it aims to be a general purpose VMM for &lt;em&gt;Cloud Workloads&lt;/em&gt; and not limited to container/serverless or client workloads.&lt;/p&gt; 
&lt;p&gt;The Cloud Hypervisor community thanks the communities of both the Firecracker and crosvm projects for their excellent work.&lt;/p&gt; 
&lt;h1&gt;5. Community&lt;/h1&gt; 
&lt;p&gt;The Cloud Hypervisor project follows the governance, and community guidelines described in the &lt;a href="https://github.com/cloud-hypervisor/community"&gt;Community&lt;/a&gt; repository.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;The project strongly believes in building a global, diverse and collaborative community around the Cloud Hypervisor project. Anyone who is interested in &lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/CONTRIBUTING.md"&gt;contributing&lt;/a&gt; to the project is welcome to participate.&lt;/p&gt; 
&lt;p&gt;Contributing to a open source project like Cloud Hypervisor covers a lot more than just sending code. Testing, documentation, pull request reviews, bug reports, feature requests, project improvement suggestions, etc, are all equal and welcome means of contribution. See the &lt;a href="https://raw.githubusercontent.com/cloud-hypervisor/cloud-hypervisor/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; document for more details.&lt;/p&gt; 
&lt;h2&gt;Slack&lt;/h2&gt; 
&lt;p&gt;Get an &lt;a href="https://join.slack.com/t/cloud-hypervisor/shared_invite/enQtNjY3MTE3MDkwNDQ4LWQ1MTA1ZDVmODkwMWQ1MTRhYzk4ZGNlN2UwNTI3ZmFlODU0OTcwOWZjMTkwZDExYWE3YjFmNzgzY2FmNDAyMjI"&gt;invite to our Slack channel&lt;/a&gt;, &lt;a href="https://cloud-hypervisor.slack.com/"&gt;join us on Slack&lt;/a&gt;, and &lt;a href="https://cloud-hypervisor.slack.com/archives/C04R5DUQVBN"&gt;participate in our community activities&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Mailing list&lt;/h2&gt; 
&lt;p&gt;Please report bugs using the &lt;a href="https://github.com/cloud-hypervisor/cloud-hypervisor/issues"&gt;GitHub issue tracker&lt;/a&gt; but for broader community discussions you may use our &lt;a href="https://lists.cloudhypervisor.org/g/dev/"&gt;mailing list&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Security issues&lt;/h2&gt; 
&lt;p&gt;Please contact the maintainers listed in the MAINTAINERS.md file with security issues.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>gitbutlerapp/gitbutler</title>
      <link>https://github.com/gitbutlerapp/gitbutler</link>
      <description>&lt;p&gt;The GitButler version control client, backed by Git, powered by Tauri/Rust/Svelte&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img align="center" width="128px" src="https://raw.githubusercontent.com/gitbutlerapp/gitbutler/master/crates/gitbutler-tauri/icons/128x128@2x.png" /&gt; 
 &lt;h1 align="center"&gt;&lt;b&gt;GitButler&lt;/b&gt;&lt;/h1&gt; 
 &lt;p align="center"&gt; Git branch management tool, built from the ground up for modern workflows &lt;br /&gt; &lt;a href="https://gitbutler.com"&gt;&lt;strong&gt;gitbutler.com »&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;b&gt;Download for &lt;/b&gt; macOS (&lt;a href="https://app.gitbutler.com/downloads/release/darwin/aarch64/dmg"&gt;Apple Silicon&lt;/a&gt; | &lt;a href="https://app.gitbutler.com/downloads/release/darwin/x86_64/dmg"&gt;Intel&lt;/a&gt;) · Linux (&lt;a href="https://app.gitbutler.com/downloads/release/linux/x86_64/gz"&gt;AppImage&lt;/a&gt; | &lt;a href="https://app.gitbutler.com/downloads/release/linux/x86_64/deb"&gt;deb&lt;/a&gt;) · Windows (&lt;a href="https://app.gitbutler.com/downloads/release/windows/x86_64/msi"&gt;msi&lt;/a&gt;) &lt;br /&gt; &lt;br /&gt; (Unstable Nightly releases can be found &lt;a href="https://app.gitbutler.com/downloads"&gt;here&lt;/a&gt;) &lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bf9bdb33-a979-47a0-b2b2-8fff5ea53afb" alt="gitbutler_client" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/gitbutlerapp/gitbutler/actions/workflows/push.yaml"&gt;&lt;img src="https://github.com/gitbutlerapp/gitbutler/actions/workflows/push.yaml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://gitbutler.com/"&gt;&lt;img src="https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D" alt="BADGE" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=gitbutler"&gt;&lt;img src="https://img.shields.io/badge/Twitter-black?logo=x&amp;amp;logoColor=white" alt="TWEET" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/MmFkmaJ42D"&gt;&lt;img src="https://img.shields.io/discord/1060193121130000425?label=Discord&amp;amp;color=5865F2" alt="DISCORD" /&gt;&lt;/a&gt; &lt;a href="https://www.instagram.com/gitbutler/"&gt;&lt;img src="https://img.shields.io/badge/Instagram-E4405F?logo=instagram&amp;amp;logoColor=white" alt="INSTA" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/@gitbutlerapp"&gt;&lt;img src="https://img.shields.io/youtube/channel/subscribers/UCEwkZIHGqsTGYvX8wgD0LoQ" alt="YOUTUBE" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/gitbutlerapp/gitbutler"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="DEEPWIKI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/fb23382bcf57c609832661874d3019a43555d6ae.svg?sanitize=true" alt="Alt" title="Repobeats analytics for GitButler" /&gt;&lt;/p&gt; 
&lt;p&gt;GitButler is a git client that lets you work on multiple branches at the same time. It allows you to quickly organize file changes into separate branches while still having them applied to your working directory. You can then push branches individually to your remote, or directly create pull requests.&lt;/p&gt; 
&lt;p&gt;In a nutshell, it's a more flexible version of &lt;code&gt;git add -p&lt;/code&gt; and &lt;code&gt;git rebase -i&lt;/code&gt;, allowing you to efficiently multitask across branches.&lt;/p&gt; 
&lt;h2&gt;How Does It Work?&lt;/h2&gt; 
&lt;p&gt;GitButler keeps track of uncommitted changes in a layer on top of Git. Changes to files or parts of files can be grouped into what we call virtual branches. Whenever you are happy with the contents of a virtual branch, you can push it to a remote. GitButler makes sure that the state of other virtual branches is kept separate.&lt;/p&gt; 
&lt;h2&gt;How Do GB's Virtual Branches Differ From Git Branches?&lt;/h2&gt; 
&lt;p&gt;The branches that we know and love in Git are separate universes, and switching between them is a full context switch. GitButler allows you to work with multiple branches in parallel in the same working directory. This effectively means having the content of multiple branches available at the same time.&lt;/p&gt; 
&lt;p&gt;GitButler is aware of changes before they are committed. This allows it to keep a record of which virtual branch each individual diff belongs to. Effectively, this means that you can separate out individual branches with their content at any time to push them to a remote or to unapply them from your working directory.&lt;/p&gt; 
&lt;p&gt;And finally, while in Git it is preferable that you create your desired branch ahead of time, using GitButler you can move changes between virtual branches at any point during development.&lt;/p&gt; 
&lt;h2&gt;Why GitButler?&lt;/h2&gt; 
&lt;p&gt;We love Git. Our own &lt;a href="https://github.com/schacon"&gt;@schacon&lt;/a&gt; has even published the &lt;a href="https://git-scm.com/book/en/v2"&gt;Pro Git&lt;/a&gt; book. At the same time, Git's user interface hasn't been fundamentally changed for 15 years. While it was written for Linux kernel devs sending patches to each other over mailing lists, most developers today have different workflows and needs.&lt;/p&gt; 
&lt;p&gt;Instead of trying to fit the semantics of the Git CLI into a graphical interface, we are starting with the developer workflow and mapping it back to Git.&lt;/p&gt; 
&lt;h2&gt;Tech&lt;/h2&gt; 
&lt;p&gt;GitButler is a &lt;a href="https://tauri.app/"&gt;Tauri&lt;/a&gt;-based application. Its UI is written in &lt;a href="https://svelte.dev/"&gt;Svelte&lt;/a&gt; using &lt;a href="https://www.typescriptlang.org"&gt;TypeScript&lt;/a&gt; and its backend is written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Main Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Virtual Branches&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Organize work on multiple branches simultaneously, rather than constantly switching branches&lt;/li&gt; 
   &lt;li&gt;Automatically create new branches when needed&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Commit Management&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Undo, Amend and Squash commits by dragging and dropping&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Undo Timeline&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Logs all operations and changes and allows you to easily undo or revert any operation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Integration&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Authenticate to GitHub to open Pull Requests, list branches and statuses and more&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy SSH Key Management&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;GitButler can generate an SSH key to upload to GitHub automatically&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Tooling&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Automatically write commit messages based on your work in progress&lt;/li&gt; 
   &lt;li&gt;Automatically create descriptive branch names&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Commit Signing&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Easy commit signing with GPG or SSH&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Example Uses&lt;/h2&gt; 
&lt;h3&gt;Fixing a Bug While Working on a Feature&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Say that while developing a feature, you encounter a bug that you wish to fix. It's often desirable that you ship the fix as a separate contribution (Pull request).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Using Git you can stash your changes and switch to another branch, where you can commit, and push your fix.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;With GitButler&lt;/em&gt; you simply assign your fix to a separate virtual branch, which you can individually push (or directly create a PR). An additional benefit is that you can retain the fix in your working directory while waiting for CI and/or code review.&lt;/p&gt; 
&lt;h3&gt;Trying Someone Else's Branch Together With My Work in Progress&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Say you want to test a branch from someone else for the purpose of code review.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Using Git trying out someone else's branch is a full context switch away from your own work. &lt;em&gt;With GitButler&lt;/em&gt; you can apply and unapply (add / remove) any remote branch directly into your working directory.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;You can find our end user documentation at: &lt;a href="https://docs.gitbutler.com"&gt;https://docs.gitbutler.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Bugs and Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a bug or feature request, feel free to open an &lt;a href="https://github.com/gitbutlerapp/gitbutler/issues/new"&gt;issue&lt;/a&gt;, or &lt;a href="https://discord.gg/MmFkmaJ42D"&gt;join our Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;AI Commit Message Generation&lt;/h2&gt; 
&lt;p&gt;Commit message generation is an opt-in feature. You can enable it while adding your repository for the first time or later in the project settings.&lt;/p&gt; 
&lt;p&gt;Currently, GitButler uses OpenAI's API for diff summarization, which means that if enabled, code diffs would be sent to OpenAI's servers.&lt;/p&gt; 
&lt;p&gt;Our goal is to make this feature more modular such that in the future you can modify the prompt as well as plug a different LLM endpoints (including local ones).&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;So you want to help out? Please check out the &lt;a href="https://raw.githubusercontent.com/gitbutlerapp/gitbutler/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;If you want to skip right to getting the code to actually compile, take a look at the &lt;a href="https://raw.githubusercontent.com/gitbutlerapp/gitbutler/master/DEVELOPMENT.md"&gt;DEVELOPMENT.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Want to show your support? Add a GitButler badge to your project's README:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-md"&gt;[![GitButler](https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D)](https://gitbutler.com/)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://gitbutler.com/"&gt;&lt;img src="https://img.shields.io/badge/GitButler-%23B9F4F2?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCAzOSAyOCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTI1LjIxNDUgMTIuMTk5N0wyLjg3MTA3IDEuMzg5MTJDMS41NDI5NSAwLjc0NjUzMiAwIDEuNzE0MDYgMCAzLjE4OTQ3VjI0LjgxMDVDMCAyNi4yODU5IDEuNTQyOTUgMjcuMjUzNSAyLjg3MTA3IDI2LjYxMDlMMjUuMjE0NSAxNS44MDAzQzI2LjcxOTcgMTUuMDcyMSAyNi43MTk3IDEyLjkyNzkgMjUuMjE0NSAxMi4xOTk3WiIgZmlsbD0iYmxhY2siLz4KPHBhdGggZD0iTTEzLjc4NTUgMTIuMTk5N0wzNi4xMjg5IDEuMzg5MTJDMzcuNDU3MSAwLjc0NjUzMiAzOSAxLjcxNDA2IDM5IDMuMTg5NDdWMjQuODEwNUMzOSAyNi4yODU5IDM3LjQ1NzEgMjcuMjUzNSAzNi4xMjg5IDI2LjYxMDlMMTMuNzg1NSAxNS44MDAzQzEyLjI4MDMgMTUuMDcyMSAxMi4yODAzIDEyLjkyNzkgMTMuNzg1NSAxMi4xOTk3WiIgZmlsbD0idXJsKCNwYWludDBfcmFkaWFsXzMxMF8xMjkpIi8%2BCjxkZWZzPgo8cmFkaWFsR3JhZGllbnQgaWQ9InBhaW50MF9yYWRpYWxfMzEwXzEyOSIgY3g9IjAiIGN5PSIwIiByPSIxIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSIgZ3JhZGllbnRUcmFuc2Zvcm09InRyYW5zbGF0ZSgxNi41NzAxIDE0KSBzY2FsZSgxOS44NjQxIDE5LjgzODMpIj4KPHN0b3Agb2Zmc2V0PSIwLjMwMTA1NiIgc3RvcC1vcGFjaXR5PSIwIi8%2BCjxzdG9wIG9mZnNldD0iMSIvPgo8L3JhZGlhbEdyYWRpZW50Pgo8L2RlZnM%2BCjwvc3ZnPgo%3D" alt="BADGE" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ai-dynamo/dynamo</title>
      <link>https://github.com/ai-dynamo/dynamo</link>
      <description>&lt;p&gt;A Datacenter Scale Distributed Inference Serving Framework&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/images/frontpage-banner.png" alt="Dynamo banner" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ai-dynamo/dynamo/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/ai-dynamo/dynamo" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/D92uqZRjCZ"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/D92uqZRjCZ?style=flat" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/ai-dynamo/dynamo"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;| &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/dynamo/issues/762"&gt;Roadmap&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/dynamo/raw/main/docs/support_matrix.md"&gt;Support matrix&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://docs.nvidia.com/dynamo/latest/index.html"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/dynamo/tree/main/examples"&gt;Examples&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-dynamo/collections/ai-dynamo"&gt;Prebuilt containers&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://github.com/ai-dynamo/enhancements"&gt;Design Proposals&lt;/a&gt;&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://developer.nvidia.com/blog/tag/nvidia-dynamo"&gt;Blogs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;NVIDIA Dynamo&lt;/h1&gt; 
&lt;p&gt;High-throughput, low-latency inference framework designed for serving generative AI and reasoning models in multi-node distributed environments.&lt;/p&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[08/05] Deploy &lt;code&gt;openai/gpt-oss-120b&lt;/code&gt; with disaggregated serving on NVIDIA Blackwell GPUs using Dynamo &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/trtllm/gpt-oss.md"&gt;➡️ link&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;The Era of Multi-GPU, Multi-Node&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/images/frontpage-gpu-vertical.png" alt="Multi Node Multi-GPU topology" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;Large language models are quickly outgrowing the memory and compute budget of any single GPU. Tensor-parallelism solves the capacity problem by spreading each layer across many GPUs—and sometimes many servers—but it creates a new one: how do you coordinate those shards, route requests, and share KV cache fast enough to feel like one accelerator? This orchestration gap is exactly what NVIDIA Dynamo is built to close.&lt;/p&gt; 
&lt;p&gt;Dynamo is designed to be inference engine agnostic (supports TRT-LLM, vLLM, SGLang or others) and captures LLM-specific capabilities such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Disaggregated prefill &amp;amp; decode inference&lt;/strong&gt; – Maximizes GPU throughput and facilitates trade off between throughput and latency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic GPU scheduling&lt;/strong&gt; – Optimizes performance based on fluctuating demand&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM-aware request routing&lt;/strong&gt; – Eliminates unnecessary KV cache re-computation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accelerated data transfer&lt;/strong&gt; – Reduces inference response time using NIXL.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;KV cache offloading&lt;/strong&gt; – Leverages multiple memory hierarchies for higher system throughput&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/images/frontpage-architecture.png" alt="Dynamo architecture" width="600" /&gt; &lt;/p&gt; 
&lt;h2&gt;Framework Support Matrix&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;vLLM&lt;/th&gt; 
   &lt;th&gt;SGLang&lt;/th&gt; 
   &lt;th&gt;TensorRT-LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/disagg_serving.md"&gt;&lt;strong&gt;Disaggregated Serving&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/disagg_serving.md#conditional-disaggregation"&gt;&lt;strong&gt;Conditional Disaggregation&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/kv_cache_routing.md"&gt;&lt;strong&gt;KV-Aware Routing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/load_planner.md"&gt;&lt;strong&gt;Load Based Planner&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/sla_planner.md"&gt;&lt;strong&gt;SLA-Based Planner&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/architecture/kvbm_architecture.md"&gt;&lt;strong&gt;KVBM&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To learn more about each framework and their capabilities, check out each framework's README!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/vllm/README.md"&gt;vLLM&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/sglang/README.md"&gt;SGLang&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends/trtllm/README.md"&gt;TensorRT-LLM&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Built in Rust for performance and in Python for extensibility, Dynamo is fully open-source and driven by a transparent, OSS (Open Source Software) first development approach.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;The following examples require a few system level packages. Recommended to use Ubuntu 24.04 with a x86_64 CPU. See &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/support_matrix.md"&gt;docs/support_matrix.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;1. Initial setup&lt;/h2&gt; 
&lt;p&gt;The Dynamo team recommends the &lt;code&gt;uv&lt;/code&gt; Python package manager, although any way works. Install uv:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install etcd and NATS (required)&lt;/h3&gt; 
&lt;p&gt;To coordinate across a data center, Dynamo relies on etcd and NATS. To run Dynamo locally, these need to be available.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://etcd.io/"&gt;etcd&lt;/a&gt; can be run directly as &lt;code&gt;./etcd&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nats.io/"&gt;nats&lt;/a&gt; needs jetstream enabled: &lt;code&gt;nats-server -js&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To quickly setup etcd &amp;amp; NATS, you can also run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# At the root of the repository:
docker compose -f deploy/docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2. Select an engine&lt;/h2&gt; 
&lt;p&gt;We publish Python wheels specialized for each of our supported engines: vllm, sglang, trtllm, and llama.cpp. The examples that follow use SGLang; continue reading for other engines.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;uv venv venv
source venv/bin/activate
uv pip install pip

# Choose one
uv pip install "ai-dynamo[sglang]"  #replace with [vllm], [trtllm], etc.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Run Dynamo&lt;/h2&gt; 
&lt;h3&gt;Running an LLM API server&lt;/h3&gt; 
&lt;p&gt;Dynamo provides a simple way to spin up a local set of inference components including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OpenAI Compatible Frontend&lt;/strong&gt; – High performance OpenAI compatible http api server written in Rust.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Basic and Kv Aware Router&lt;/strong&gt; – Route and load balance traffic to a set of workers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Workers&lt;/strong&gt; – Set of pre-configured LLM serving engines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;# Start an OpenAI compatible HTTP server, a pre-processor (prompt templating and tokenization) and a router.
# Pass the TLS certificate and key paths to use HTTPS instead of HTTP.
python -m dynamo.frontend --http-port 8000 [--tls-cert-path cert.pem] [--tls-key-path key.pem]

# Start the SGLang engine, connecting to NATS and etcd to receive requests. You can run several of these,
# both for the same model and for multiple models. The frontend node will discover them.
python -m dynamo.sglang.worker --model deepseek-ai/DeepSeek-R1-Distill-Llama-8B --skip-tokenizer-init
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Send a Request&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl localhost:8000/v1/chat/completions   -H "Content-Type: application/json"   -d '{
    "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "messages": [
    {
        "role": "user",
        "content": "Hello, how are you?"
    }
    ],
    "stream":false,
    "max_tokens": 300
  }' | jq
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Rerun with &lt;code&gt;curl -N&lt;/code&gt; and change &lt;code&gt;stream&lt;/code&gt; in the request to &lt;code&gt;true&lt;/code&gt; to get the responses as soon as the engine issues them.&lt;/p&gt; 
&lt;h3&gt;Deploying Dynamo&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/guides/dynamo_deploy/README.md"&gt;Quickstart Guide&lt;/a&gt; to deploy on Kubernetes.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/components/backends"&gt;Backends&lt;/a&gt; to deploy various workflow configurations (e.g. SGLang with router, vLLM with disaggregated serving, etc.)&lt;/li&gt; 
 &lt;li&gt;Run some &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/examples"&gt;Examples&lt;/a&gt; to learn about building components in Dynamo and exploring various integrations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Benchmarking Dynamo&lt;/h3&gt; 
&lt;p&gt;Dynamo provides comprehensive benchmarking tools to evaluate and optimize your deployments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/benchmarks/benchmarking.md"&gt;Benchmarking Guide&lt;/a&gt;&lt;/strong&gt; – Compare deployment topologies (aggregated vs. disaggregated vs. vanilla vLLM) using GenAI-Perf&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/benchmarks/pre_deployment_profiling.md"&gt;Pre-Deployment Profiling&lt;/a&gt;&lt;/strong&gt; – Optimize configurations before deployment to meet SLA requirements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Engines&lt;/h1&gt; 
&lt;p&gt;Dynamo is designed to be inference engine agnostic. To use any engine with Dynamo, NATS and etcd need to be installed, along with a Dynamo frontend (&lt;code&gt;python -m dynamo.frontend [--interactive]&lt;/code&gt;).&lt;/p&gt; 
&lt;h2&gt;vLLM&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install ai-dynamo[vllm]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the backend/worker like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m dynamo.vllm --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;vLLM attempts to allocate enough KV cache for the full context length at startup. If that does not fit in your available memory pass &lt;code&gt;--context-length &amp;lt;value&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To specify which GPUs to use set environment variable &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;SGLang&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;# Install libnuma
apt install -y libnuma-dev

uv pip install ai-dynamo[sglang]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the backend/worker like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m dynamo.sglang.worker --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can pass any sglang flags directly to this worker, see &lt;a href="https://docs.sglang.ai/advanced_features/server_arguments.html"&gt;https://docs.sglang.ai/advanced_features/server_arguments.html&lt;/a&gt; . See there to use multiple GPUs.&lt;/p&gt; 
&lt;h2&gt;TensorRT-LLM&lt;/h2&gt; 
&lt;p&gt;It is recommended to use &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch"&gt;NGC PyTorch Container&lt;/a&gt; for running the TensorRT-LLM engine.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Ensure that you select a PyTorch container image version that matches the version of TensorRT-LLM you are using. For example, if you are using &lt;code&gt;tensorrt-llm==1.1.0rc3&lt;/code&gt;, use the PyTorch container image version &lt;code&gt;25.06&lt;/code&gt;. To find the correct PyTorch container version for your desired &lt;code&gt;tensorrt-llm&lt;/code&gt; release, visit the &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docker/Dockerfile.multi"&gt;TensorRT-LLM Dockerfile.multi&lt;/a&gt; on GitHub. Switch to the branch that matches your &lt;code&gt;tensorrt-llm&lt;/code&gt; version, and look for the &lt;code&gt;BASE_TAG&lt;/code&gt; line to identify the recommended PyTorch container tag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] Launch container with the following additional settings &lt;code&gt;--shm-size=1g --ulimit memlock=-1&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install prerequisites&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# Optional step: Only required for Blackwell and Grace Hopper
uv pip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# Required until the trtllm version is bumped to include this pinned dependency itself
uv pip install "cuda-python&amp;gt;=12,&amp;lt;13"

sudo apt-get -y install libopenmpi-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Tip] You can learn more about these prequisites and known issues with TensorRT-LLM pip based installation &lt;a href="https://nvidia.github.io/TensorRT-LLM/installation/linux.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;After installing the pre-requisites above, install Dynamo&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install ai-dynamo[trtllm]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the backend/worker like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m dynamo.trtllm --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To specify which GPUs to use set environment variable &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Developing Locally&lt;/h1&gt; 
&lt;h2&gt;1. Install libraries&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Ubuntu:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt install -y build-essential libhwloc-dev libudev-dev pkg-config libclang-dev protobuf-compiler python3-dev cmake
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://brew.sh/"&gt;Homebrew&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;# if brew is not installed on your system, install it
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developer.apple.com/xcode/"&gt;Xcode&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;brew install cmake protobuf

## Check that Metal is accessible
xcrun -sdk macosx metal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If Metal is accessible, you should see an error like &lt;code&gt;metal: error: no input files&lt;/code&gt;, which confirms it is installed correctly.&lt;/p&gt; 
&lt;h2&gt;2. Install Rust&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Create a Python virtual env:&lt;/h2&gt; 
&lt;p&gt;Follow the instructions in &lt;a href="https://docs.astral.sh/uv/#installation"&gt;uv installation&lt;/a&gt; guide to install uv if you don't have &lt;code&gt;uv&lt;/code&gt; installed. Once uv is installed, create a virtual environment and activate it.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a virtual environment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv dynamo
source dynamo/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;4. Install build tools&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install pip maturin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/PyO3/maturin"&gt;Maturin&lt;/a&gt; is the Rust&amp;lt;-&amp;gt;Python bindings build tool.&lt;/p&gt; 
&lt;h2&gt;5. Build the Rust bindings&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;cd lib/bindings/python
maturin develop --uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;6. Install the wheel&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;cd $PROJECT_ROOT
uv pip install .
# For development, use
export PYTHONPATH="${PYTHONPATH}:$(pwd)/components/frontend/src:$(pwd)/components/planner/src:$(pwd)/components/backends/vllm/src:$(pwd)/components/backends/sglang/src:$(pwd)/components/backends/trtllm/src:$(pwd)/components/backends/llama_cpp/src:$(pwd)/components/backends/mocker/src"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Editable (&lt;code&gt;-e&lt;/code&gt;) does not work because the &lt;code&gt;dynamo&lt;/code&gt; package is split over multiple directories, one per backend.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You should now be able to run &lt;code&gt;python -m dynamo.frontend&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Remember that nats and etcd must be running (see earlier).&lt;/p&gt; 
&lt;p&gt;Set the environment variable &lt;code&gt;DYN_LOG&lt;/code&gt; to adjust the logging level; for example, &lt;code&gt;export DYN_LOG=debug&lt;/code&gt;. It has the same syntax as &lt;code&gt;RUST_LOG&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you use vscode or cursor, we have a .devcontainer folder built on &lt;a href="https://code.visualstudio.com/docs/devcontainers/containers"&gt;Microsofts Extension&lt;/a&gt;. For instructions see the &lt;a href="https://raw.githubusercontent.com/ai-dynamo/dynamo/main/.devcontainer/README.md"&gt;ReadMe&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>oxc-project/oxc</title>
      <link>https://github.com/oxc-project/oxc</link>
      <description>&lt;p&gt;⚓ A collection of JavaScript tools written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="OXC Logo" src="https://cdn.jsdelivr.net/gh/oxc-project/oxc-assets/preview-universal.png" width="700" /&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/oxc-project/oxc/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="MIT licensed" /&gt;&lt;/a&gt; &lt;a href="https://github.com/oxc-project/oxc/actions/workflows/ci.yml?query=event%3Apush+branch%3Amain"&gt;&lt;img src="https://github.com/oxc-project/oxc/actions/workflows/ci.yml/badge.svg?event=push&amp;amp;branch=main" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/oxc-project/oxc"&gt;&lt;img src="https://codecov.io/gh/oxc-project/oxc/graph/badge.svg?token=FVHEH0BQLJ" alt="Code Coverage" /&gt;&lt;/a&gt; &lt;a href="https://codspeed.io/oxc-project/oxc"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="CodSpeed Badge" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/Boshen"&gt;&lt;img src="https://img.shields.io/github/sponsors/Boshen" alt="Sponsors" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/9uXCAwqQZW"&gt;&lt;img src="https://img.shields.io/discord/1079625926024900739?logo=discord&amp;amp;label=Discord" alt="Discord chat" /&gt;&lt;/a&gt; &lt;a href="https://playground.oxc.rs/"&gt;&lt;img src="https://img.shields.io/badge/Playground-blue?color=9BE4E0" alt="Playground" /&gt;&lt;/a&gt; &lt;a href="https://oxc.rs"&gt;&lt;img src="https://img.shields.io/badge/Website-blue" alt="Website" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;⚓ Oxc&lt;/h2&gt; 
&lt;p&gt;The Oxidation Compiler is a collection of high-performance tools for JavaScript and TypeScript written in Rust.&lt;/p&gt; 
&lt;p&gt;Our goal is to enable a new generation of faster, more reliable development tools by providing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: 2-100x faster than existing JavaScript tools&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliability&lt;/strong&gt;: 100% compatibility with JavaScript and TypeScript standards&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modularity&lt;/strong&gt;: Use individual tools or compose them into complete toolchains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Developer Experience&lt;/strong&gt;: Clear error messages and seamless editor integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We are building a parser, linter, formatter, transformer, minifier, resolver ... all written in Rust.&lt;/p&gt; 
&lt;p&gt;For more information, check out our documentation at &lt;a href="https://oxc.rs"&gt;oxc.rs&lt;/a&gt; and architecture guide in &lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/ARCHITECTURE.md"&gt;ARCHITECTURE.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;VoidZero Inc.&lt;/h2&gt; 
&lt;p&gt;Oxc is a project of &lt;a href="https://voidzero.dev/"&gt;VoidZero&lt;/a&gt;, see our announcement &lt;a href="https://voidzero.dev/blog"&gt;Announcing VoidZero - Next Generation Toolchain for JavaScript&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you have requirements for JavaScript tools at scale, please &lt;a href="https://forms.gle/WQgjyzYJpwurpxWKA"&gt;get in touch&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;🙋Who's using Oxc?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rolldown.rs"&gt;Rolldown&lt;/a&gt; uses the &lt;a href="https://docs.rs/oxc"&gt;oxc&lt;/a&gt; crate for parsing and transformation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://trynova.dev"&gt;Nova engine&lt;/a&gt; uses the &lt;a href="https://docs.rs/oxc"&gt;oxc&lt;/a&gt; crate for parsing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://rolldown.rs"&gt;Rolldown&lt;/a&gt;, &lt;a href="https://github.com/swc-project/swc-node"&gt;swc-node&lt;/a&gt; and &lt;a href="https://github.com/webpro-nl/knip"&gt;knip&lt;/a&gt; use the &lt;a href="https://docs.rs/oxc_resolver"&gt;oxc_resolver&lt;/a&gt; crate for module resolution.&lt;/li&gt; 
 &lt;li&gt;Projects and companies like &lt;a href="https://github.com/preactjs/preact/raw/4c20c23c16dd60f380ce9fe98afc93041a7e1562/oxlint.json"&gt;Preact&lt;/a&gt;, &lt;a href="https://oxc.rs/blog/2023-12-12-announcing-oxlint.html#_50-100-times-faster-than-eslint"&gt;Shopify&lt;/a&gt;, ByteDance and Shopee uses oxlint for linting.&lt;/li&gt; 
 &lt;li&gt;...&lt;a href="https://oxc.rs/docs/guide/projects.html"&gt;and many more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;✍️ Contribute&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidance.&lt;/p&gt; 
&lt;p&gt;Check out some of the &lt;a href="https://github.com/oxc-project/oxc/contribute"&gt;good first issues&lt;/a&gt; or ask us on &lt;a href="https://discord.gg/9uXCAwqQZW"&gt;Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are unable to contribute by code, you can still participate by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add a &lt;a href="https://github.com/oxc-project/oxc/stargazers"&gt;GitHub Star&lt;/a&gt; to the project.&lt;/li&gt; 
 &lt;li&gt;Join us on &lt;a href="https://discord.gg/9uXCAwqQZW"&gt;Discord&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/boshen_c"&gt;Follow me on X&lt;/a&gt; and post about this project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⚡️ Linter Quick Start&lt;/h2&gt; 
&lt;p&gt;The linter is ready to catch mistakes for you. It comes with 93 rules turned on by default (out of 430+ in total) and no configuration is required.&lt;/p&gt; 
&lt;p&gt;To get started, run &lt;a href="https://www.npmjs.com/package/oxlint"&gt;oxlint&lt;/a&gt; or via &lt;code&gt;npx&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx oxlint@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To give you an idea of its capabilities, here is an example from the &lt;a href="https://github.com/microsoft/vscode"&gt;vscode&lt;/a&gt; repository, which finishes linting 4800+ files in 0.7 seconds.&lt;/p&gt; 
&lt;p float="left" align="left"&gt; &lt;img src="https://cdn.jsdelivr.net/gh/oxc-project/oxc-assets/linter-screenshot.png" width="60%" /&gt; &lt;/p&gt; 
&lt;h2&gt;⚡️ Performance&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The parser aims to be the fastest Rust-based ready-for-production parser.&lt;/li&gt; 
 &lt;li&gt;The linter is more than 50 times faster than &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt;, and scales with the number of CPU cores.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p float="left" align="middle"&gt; &lt;img src="https://raw.githubusercontent.com/Boshen/bench-javascript-parser-written-in-rust/main/bar-graph.svg?sanitize=true" width="49%" /&gt; &lt;img src="https://raw.githubusercontent.com/Boshen/bench-javascript-linter/main/bar-graph.svg?sanitize=true" width="49%" /&gt; &lt;/p&gt; 
&lt;h2&gt;⌨️ Rust, Node.js and Wasm Usage&lt;/h2&gt; 
&lt;h3&gt;Rust&lt;/h3&gt; 
&lt;p&gt;Individual crates are published, you may use them to build your own JavaScript tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The umbrella crate &lt;a href="https://docs.rs/oxc"&gt;oxc&lt;/a&gt; exports all public crates from this repository.&lt;/li&gt; 
 &lt;li&gt;The AST and parser crates &lt;a href="https://docs.rs/oxc_ast"&gt;oxc_ast&lt;/a&gt; and &lt;a href="https://docs.rs/oxc_parser"&gt;oxc_parser&lt;/a&gt; are production ready.&lt;/li&gt; 
 &lt;li&gt;The resolver crate &lt;a href="https://docs.rs/oxc_resolver"&gt;oxc_resolver&lt;/a&gt; for module resolution is also production ready.&lt;/li&gt; 
 &lt;li&gt;Example usages of these crates can be found in their respective &lt;code&gt;crates/*/examples&lt;/code&gt; directory.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We have optimized Rust compilation speed to ensure developing your own Oxc-based tools remains efficient. Our &lt;a href="https://github.com/oxc-project/oxc/actions/workflows/ci.yml?query=branch%3Amain"&gt;CI runs&lt;/a&gt; complete in approximately 3 minutes.&lt;/p&gt; 
&lt;h3&gt;Node.js&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;via napi: &lt;a href="https://www.npmjs.com/package/oxc-parser"&gt;oxc-parser&lt;/a&gt;, &lt;a href="https://www.npmjs.com/package/oxc-transform"&gt;oxc-transform&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Wasm&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.npmjs.com/package/@oxc-parser/wasm"&gt;@oxc-parser/wasm&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🎯 Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-ast-and-parser"&gt;AST and Parser&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-linter"&gt;Linter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-resolver"&gt;Resolver&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-minifier"&gt;Minifier&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-formatter"&gt;Formatter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/#-transformer"&gt;Transformer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔸 AST and Parser&lt;/h3&gt; 
&lt;p&gt;Oxc maintains its own AST and parser, which is by far the fastest and most conformant JavaScript and TypeScript (including JSX and TSX) parser written in Rust.&lt;/p&gt; 
&lt;p&gt;As the parser often represents a key performance bottleneck in JavaScript tooling, any minor improvements can have a cascading effect on our downstream tools.&lt;/p&gt; 
&lt;h4&gt;🏆 Parser Performance&lt;/h4&gt; 
&lt;p&gt;Our &lt;a href="https://github.com/Boshen/bench-javascript-parser-written-in-rust"&gt;benchmark&lt;/a&gt; reveals that the Oxc parser surpasses the speed of the &lt;a href="https://swc.rs"&gt;swc&lt;/a&gt; parser by approximately 3 times and the &lt;a href="https://biomejs.dev/"&gt;Biome&lt;/a&gt; parser by 5 times.&lt;/p&gt; 
&lt;h3&gt;🔸 Linter&lt;/h3&gt; 
&lt;p&gt;The linter embraces convention over configuration, eliminating the need for extensive configuration and plugin setup. Unlike other linters like &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt;, which often require intricate configurations and plugin installations (e.g. &lt;a href="https://typescript-eslint.io"&gt;@typescript-eslint&lt;/a&gt;), our linter only requires a single command that you can immediately run on your codebase:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx oxlint@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;🏆 Linter Performance&lt;/h4&gt; 
&lt;p&gt;The linter is 50 - 100 times faster than &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt; depending on the number of rules and number of CPU cores used. It completes in less than a second for most codebases with a few hundred files and completes in a few seconds for larger monorepos. See &lt;a href="https://github.com/Boshen/bench-javascript-linter"&gt;bench-javascript-linter&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;As an upside, the binary is approximately 5MB, whereas &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt; and its associated plugin dependencies can easily exceed 100.&lt;/p&gt; 
&lt;p&gt;You may also download the linter binary from the &lt;a href="https://github.com/oxc-project/oxc/releases/latest"&gt;latest release tag&lt;/a&gt; as a standalone binary, this lets you run the linter without a Node.js installation in your CI.&lt;/p&gt; 
&lt;h3&gt;🔸 Resolver&lt;/h3&gt; 
&lt;p&gt;Module resolution plays a crucial role in JavaScript tooling, especially for tasks like multi-file analysis or bundling. However, it can often become a performance bottleneck. To address this, we developed &lt;a href="https://docs.rs/oxc_resolver"&gt;oxc_resolver&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The resolver is production-ready and is currently being used in &lt;a href="https://rolldown.rs"&gt;Rolldown&lt;/a&gt;. Usage and examples can be found in its own &lt;a href="https://github.com/oxc-project/oxc_resolver"&gt;repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;🔸 Transformer&lt;/h3&gt; 
&lt;p&gt;A transformer is responsible for turning higher versions of ECMAScript to a lower version that can be used in older browsers.&lt;/p&gt; 
&lt;p&gt;TypeScript, React, ES6 transforms are complete.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.npmjs.com/package/oxc-transform"&gt;oxc-transform&lt;/a&gt; can be used for experimentation.&lt;/p&gt; 
&lt;h3&gt;🔸 Isolated Declarations&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://devblogs.microsoft.com/typescript/announcing-typescript-5-5/#isolated-declarations"&gt;TypeScript Isolated Declarations Emit&lt;/a&gt; without using the TypeScript compiler.&lt;/p&gt; 
&lt;p&gt;Our &lt;a href="https://github.com/oxc-project/bench-transformer"&gt;benchmark&lt;/a&gt; indicates that our implementation is at least 20 times faster than the TypeScript compiler.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://www.npmjs.com/package/oxc-transform"&gt;npm package&lt;/a&gt; or &lt;a href="https://crates.io/crates/oxc_isolated_declarations"&gt;crate&lt;/a&gt; can be used for this task.&lt;/p&gt; 
&lt;h3&gt;🔸 Minifier&lt;/h3&gt; 
&lt;p&gt;JavaScript minification plays a crucial role in optimizing website performance as it reduces the amount of data sent to users, resulting in faster page loads. This holds tremendous economic value, particularly for e-commerce websites, where every second can equate to millions of dollars.&lt;/p&gt; 
&lt;p&gt;However, existing minifiers typically require a trade-off between compression quality and speed. You have to choose between the slowest for the best compression or the fastest for less compression. But what if we could develop a faster minifier without compromising on compression?&lt;/p&gt; 
&lt;p&gt;We are actively working on a prototype that aims to achieve this goal, by porting all test cases from well-known minifiers such as &lt;a href="https://github.com/google/closure-compiler"&gt;google-closure-compiler&lt;/a&gt;, &lt;a href="https://terser.org"&gt;terser&lt;/a&gt;, &lt;a href="https://esbuild.github.io/"&gt;esbuild&lt;/a&gt;, and &lt;a href="https://github.com/tdewolff/minify"&gt;tdewolff-minify&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Preliminary results indicate that we are on track to achieve our objectives. With the Oxc minifier, you can expect faster minification times without sacrificing compression quality.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/privatenumber/minification-benchmarks"&gt;minification benchmarks&lt;/a&gt; for comparisons.&lt;/p&gt; 
&lt;h3&gt;🔸 Formatter&lt;/h3&gt; 
&lt;p&gt;While &lt;a href="https://prettier.io"&gt;prettier&lt;/a&gt; has established itself as the de facto code formatter for JavaScript, there is a significant demand in the developer community for a less opinionated alternative. Recognizing this need, our ambition is to undertake research and development to create a new JavaScript formatter that offers increased flexibility and customization options.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://github.com/oxc-project/oxc/tree/main/crates/oxc_formatter"&gt;prototype&lt;/a&gt; is currently work in progress.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🧪Test Infrastructure&lt;/h2&gt; 
&lt;p&gt;In Oxc, correctness and reliability are taken extremely seriously.&lt;/p&gt; 
&lt;p&gt;We spend half of our time on strengthening the test infrastructure to prevent problems from propagating to downstream tools.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://oxc.rs/docs/learn/architecture/test.html"&gt;Test Infrastructure&lt;/a&gt; documents our test procedures:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Conformance suite on Test262, Babel, TypeScript&lt;/li&gt; 
 &lt;li&gt;Lots of fuzzing&lt;/li&gt; 
 &lt;li&gt;Linter snapshot diagnostics&lt;/li&gt; 
 &lt;li&gt;oxlint ecosystem ci&lt;/li&gt; 
 &lt;li&gt;Idempotency testing&lt;/li&gt; 
 &lt;li&gt;Code coverage&lt;/li&gt; 
 &lt;li&gt;End to end 3000 top npm packages&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📚 Learning Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My small tutorial on &lt;a href="https://oxc.rs/docs/learn/parser_in_rust/intro.html"&gt;how to write a JavaScript Parser in Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;My small article &lt;a href="https://oxc.rs/docs/learn/performance.html"&gt;Pursuit of Performance on Building a JavaScript Compiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://oxc.rs/docs/learn/references.html"&gt;And more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🤝 Credits&lt;/h2&gt; 
&lt;p&gt;This project was incubated with the assistance of these exceptional mentors and their projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://biomejs.dev/"&gt;Biome&lt;/a&gt; - &lt;a href="https://github.com/ematipico"&gt;@ematipico&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://beta.ruff.rs"&gt;Ruff&lt;/a&gt; - &lt;a href="https://github.com/charliermarsh"&gt;@charliermarsh&lt;/a&gt;, &lt;a href="https://github.com/MichaReiser"&gt;@MichaReiser&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/quick-lint/quick-lint-js"&gt;quick-lint-js&lt;/a&gt; - &lt;a href="https://github.com/strager"&gt;@strager&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://package.elm-lang.org/packages/jfmengels/elm-review/latest"&gt;elm-review&lt;/a&gt; - &lt;a href="https://github.com/jfmengels"&gt;@jfmengels&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Special thanks go to&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/domonji"&gt;@domonji&lt;/a&gt; for bootstrapping this project together, and also completing the TypeScript parser.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tongtong-lu"&gt;@tongtong-lu&lt;/a&gt; and &lt;a href="https://github.com/guan-wy"&gt;@guan-wy&lt;/a&gt; for designing the &lt;a href="https://github.com/oxc-project/oxc-assets"&gt;project logo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;❤ Who's &lt;a href="https://github.com/sponsors/Boshen"&gt;Sponsoring Oxc&lt;/a&gt;?&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/sponsors/Boshen"&gt; &lt;img src="https://raw.githubusercontent.com/Boshen/sponsors/main/sponsors.svg?sanitize=true" alt="My sponsors" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;📖 License&lt;/h2&gt; 
&lt;p&gt;Oxc is free and open-source software licensed under the &lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Oxc ports or copies code from other open source projects, their licenses are listed in &lt;a href="https://raw.githubusercontent.com/oxc-project/oxc/main/THIRD-PARTY-LICENSE"&gt;&lt;strong&gt;Third-party library licenses&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>feschber/lan-mouse</title>
      <link>https://github.com/feschber/lan-mouse</link>
      <description>&lt;p&gt;mouse &amp; keyboard sharing via LAN&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lan Mouse&lt;/h1&gt; 
&lt;p&gt;Lan Mouse is a &lt;em&gt;cross-platform&lt;/em&gt; mouse and keyboard sharing software similar to universal-control on Apple devices. It allows for using multiple PCs via a single set of mouse and keyboard. This is also known as a Software KVM switch.&lt;/p&gt; 
&lt;p&gt;Goal of this project is to be an open-source alternative to proprietary tools like &lt;a href="https://symless.com/synergy"&gt;Synergy 2/3&lt;/a&gt;, &lt;a href="https://www.sharemouse.com/de/"&gt;Share Mouse&lt;/a&gt; and other open source tools like &lt;a href="https://github.com/deskflow/deskflow"&gt;Deskflow&lt;/a&gt; or &lt;a href="https://github.com/input-leap"&gt;Input Leap&lt;/a&gt; (Synergy fork).&lt;/p&gt; 
&lt;p&gt;Focus lies on performance, ease of use and a maintainable implementation that can be expanded to support additional backends for e.g. Android, iOS, ... in the future.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;blazingly fast™&lt;/strong&gt;&lt;/em&gt; because it's written in rust.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;Now with a gtk frontend&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="/screenshots/dark.png?raw=true" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="/screenshots/light.png?raw=true" /&gt; 
 &lt;img alt="Screenshot of Lan-Mouse" srcset="/screenshots/dark.png" /&gt; 
&lt;/picture&gt; 
&lt;h2&gt;Encryption&lt;/h2&gt; 
&lt;p&gt;Lan Mouse encrypts all network traffic using the DTLS implementation provided by &lt;a href="https://github.com/webrtc-rs/webrtc"&gt;WebRTC.rs&lt;/a&gt;. There are currently no mitigations in place for timing side-channel attacks.&lt;/p&gt; 
&lt;h2&gt;OS Support&lt;/h2&gt; 
&lt;p&gt;Most current desktop environments and operating systems are fully supported, this includes&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GNOME &amp;gt;= 45&lt;/li&gt; 
 &lt;li&gt;KDE Plasma &amp;gt;= 6.1&lt;/li&gt; 
 &lt;li&gt;Most wlroots based compositors, including Sway (&amp;gt;= 1.8), Hyprland and Wayfire&lt;/li&gt; 
 &lt;li&gt;Windows&lt;/li&gt; 
 &lt;li&gt;MacOS&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Caveats / Known Issues&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;X11&lt;/strong&gt; currently only has support for input emulation, i.e. can only be used on the receiving end.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sway / wlroots&lt;/strong&gt;: Wlroots based compositors without libei support on the receiving end currently do not handle modifier events on the client side. This results in CTRL / SHIFT / ALT / SUPER keys not working with a sending device that is NOT using the &lt;code&gt;layer-shell&lt;/code&gt; backend&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Wayfire&lt;/strong&gt;: If you are using &lt;a href="https://github.com/WayfireWM/wayfire"&gt;Wayfire&lt;/a&gt;, make sure to use a recent version (must be newer than October 23rd) and &lt;strong&gt;add &lt;code&gt;shortcuts-inhibit&lt;/code&gt; to the list of plugins in your wayfire config!&lt;/strong&gt; Otherwise input capture will not work.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;: The mouse cursor will be invisible when sending input to a Windows system if there is no real mouse connected to the machine.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more detailed information about os support see &lt;a href="https://raw.githubusercontent.com/feschber/lan-mouse/main/#detailed-os-support"&gt;Detailed OS Support&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Android &amp;amp; IOS&lt;/h3&gt; 
&lt;p&gt;A proof of concept for an Android / IOS Application by &lt;a href="https://github.com/rohitsangwan01"&gt;rohitsangwan01&lt;/a&gt; can be found &lt;a href="https://github.com/rohitsangwan01/lan-mouse-mobile"&gt;here&lt;/a&gt;. It can be used as a remote control for any device supported by Lan Mouse.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Arch Linux&lt;/summary&gt; 
 &lt;p&gt;Lan Mouse can be installed from the &lt;a href="https://archlinux.org/packages/extra/x86_64/lan-mouse/"&gt;official repositories&lt;/a&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;pacman -S lan-mouse
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The prerelease version (following &lt;code&gt;main&lt;/code&gt;) is available on the AUR:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;paru -S lan-mouse-git
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Nix (OS)&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;nixpkgs: &lt;a href="https://search.nixos.org/packages?channel=unstable&amp;amp;show=lan-mouse&amp;amp;from=0&amp;amp;size=50&amp;amp;sort=relevance&amp;amp;type=packages&amp;amp;query=lan-mouse"&gt;search.nixos.org&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;flake: &lt;a href="https://raw.githubusercontent.com/feschber/lan-mouse/main/nix/README.md"&gt;README.md&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Manual Installation&lt;/summary&gt; 
 &lt;p&gt;First make sure to &lt;a href="https://raw.githubusercontent.com/feschber/lan-mouse/main/#installing-dependencies"&gt;install the necessary dependencies&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Precompiled release binaries for Windows, MacOS and Linux are available in the &lt;a href="https://github.com/feschber/lan-mouse/releases"&gt;releases section&lt;/a&gt;. For Windows, the depenedencies are included in the .zip file, for other operating systems see &lt;a href="https://raw.githubusercontent.com/feschber/lan-mouse/main/#installing-dependencies"&gt;Installing Dependencies&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Alternatively, the &lt;code&gt;lan-mouse&lt;/code&gt; binary can be compiled from source (see below).&lt;/p&gt; 
 &lt;h3&gt;Installing desktop file, app icon and firewall rules (optional)&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# install lan-mouse (replace path/to/ with the correct path)
sudo cp path/to/lan-mouse /usr/local/bin/

# install app icon
sudo mkdir -p /usr/local/share/icons/hicolor/scalable/apps
sudo cp lan-mouse-gtk/resources/de.feschber.LanMouse.svg /usr/local/share/icons/hicolor/scalable/apps

# update icon cache
gtk-update-icon-cache /usr/local/share/icons/hicolor/

# install desktop entry
sudo mkdir -p /usr/local/share/applications
sudo cp de.feschber.LanMouse.desktop /usr/local/share/applications

# when using firewalld: install firewall rule
sudo cp firewall/lan-mouse.xml /etc/firewalld/services
# -&amp;gt; enable the service in firewalld settings
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Instead of downloading from the releases, the &lt;code&gt;lan-mouse&lt;/code&gt; binary can be easily compiled via cargo or nix:&lt;/p&gt; 
 &lt;h3&gt;Compiling and installing manually:&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# compile in release mode
cargo build --release

# install lan-mouse
sudo cp target/release/lan-mouse /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Compiling and installing via cargo:&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# will end up in ~/.cargo/bin
cargo install lan-mouse
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Compiling and installing via nix:&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# you can find the executable in result/bin/lan-mouse
nix-build
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Conditional compilation&lt;/h3&gt; 
 &lt;p&gt;Support for other platforms is omitted automatically based on the active rust toolchain.&lt;/p&gt; 
 &lt;p&gt;Additionally, available backends and frontends can be configured manually via &lt;a href="https://doc.rust-lang.org/cargo/reference/features.html"&gt;cargo features&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;E.g. if only support for sway is needed, the following command produces an executable with support for only the &lt;code&gt;layer-shell&lt;/code&gt; capture backend and &lt;code&gt;wlroots&lt;/code&gt; emulation backend:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;cargo build --no-default-features --features layer_shell_capture,wlroots_emulation
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For a detailed list of available features, checkout the &lt;a href="https://raw.githubusercontent.com/feschber/lan-mouse/main/Cargo.toml"&gt;Cargo.toml&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installing Dependencies for Development / Compiling from Source&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;MacOS&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;brew install libadwaita pkg-config
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Ubuntu and derivatives&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;sudo apt install libadwaita-1-dev libgtk-4-dev libx11-dev libxtst-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Arch and derivatives&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;sudo pacman -S libadwaita gtk libx11 libxtst
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Fedora and derivatives&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;sudo dnf install libadwaita-devel libXtst-devel libX11-devel
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Nix&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;nix-shell .
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Nix (flake)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;nix develop
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Windows&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;First install &lt;a href="https://www.rust-lang.org/tools/install"&gt;Rust&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Then follow the instructions at &lt;a href="https://gtk-rs.org/gtk4-rs/stable/latest/book/installation_windows.html"&gt;gtk-rs.org&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;em&gt;TLDR:&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;Build gtk from source&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;The following commands should be run in an &lt;strong&gt;admin power shell&lt;/strong&gt; instance:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# install chocolatey
Set-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))

# install gvsbuild dependencies
choco install python git msys2 visualstudio2022-workload-vctools
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;The following commands should be run in a &lt;strong&gt;regular power shell&lt;/strong&gt; instance:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# install gvsbuild with python
python -m pip install --user pipx
python -m pipx ensurepath
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Relaunch your powershell instance so the changes in the environment are reflected.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;pipx install gvsbuild

# build gtk + libadwaita
gvsbuild build gtk4 libadwaita librsvg adwaita-icon-theme
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Make sure to add the directory&lt;/strong&gt; &lt;code&gt;C:\gtk-build\gtk\x64\release\bin&lt;/code&gt; &lt;a href="https://raw.githubusercontent.com/feschber/lan-mouse/main/(https://learn.microsoft.com/en-us/previous-versions/office/developer/sharepoint-2010/ee537574(v=office.14))"&gt;&lt;strong&gt;to the &lt;code&gt;PATH&lt;/code&gt; environment variable&lt;/strong&gt;&lt;/a&gt;. Otherwise the project will fail to build.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;To avoid building GTK from source, it is possible to disable the gtk frontend (see conditional compilation).&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Gtk Frontend&lt;/summary&gt; 
 &lt;p&gt;By default the gtk frontend will open when running &lt;code&gt;lan-mouse&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;To connect a device you want to control, simply click the &lt;code&gt;Add&lt;/code&gt; button and enter the hostname of the device.&lt;/p&gt; 
 &lt;p&gt;On the &lt;em&gt;remote&lt;/em&gt; device, authorize your &lt;em&gt;local&lt;/em&gt; device for incoming traffic using the &lt;code&gt;Authorize&lt;/code&gt; button under the "Incoming Connections" section. The fingerprint for authorization can be found under the general section of your &lt;em&gt;local&lt;/em&gt; device. It is of the form "aa:bb:cc:..."&lt;/p&gt; 
 &lt;p&gt;Authorized devices can be persisted using the configuration file (see &lt;a href="https://raw.githubusercontent.com/feschber/lan-mouse/main/#configuration"&gt;Configuration&lt;/a&gt;).&lt;/p&gt; 
 &lt;p&gt;If the device still can not be entered, make sure you have UDP port &lt;code&gt;4242&lt;/code&gt; (or the one selected) opened up in your firewall.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Command Line Interface&lt;/summary&gt; 
 &lt;p&gt;The cli interface can be accessed by passing &lt;code&gt;cli&lt;/code&gt; as a commandline argument. Use&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;lan-mouse cli help
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;to list the available commands and&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;lan-mouse cli &amp;lt;cmd&amp;gt; help
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;for information on how to use a specific command.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Daemon Mode&lt;/summary&gt; 
 &lt;p&gt;Lan Mouse can be launched in daemon mode to keep it running in the background (e.g. for use in a systemd-service).&lt;/p&gt; 
 &lt;p&gt;To do so, use the &lt;code&gt;daemon&lt;/code&gt; subcommand:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;lan-mouse daemon
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;In order to start lan-mouse with a graphical session automatically, the &lt;a href="https://raw.githubusercontent.com/feschber/lan-mouse/main/service/lan-mouse.service"&gt;systemd-service&lt;/a&gt; can be used:&lt;/p&gt; 
 &lt;p&gt;Copy the file to &lt;code&gt;~/.config/systemd/user/&lt;/code&gt; and enable the service:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;cp service/lan-mouse.service ~/.config/systemd/user
systemctl --user daemon-reload
systemctl --user enable --now lan-mouse.service
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;To automatically load clients on startup, the file &lt;code&gt;$XDG_CONFIG_HOME/lan-mouse/config.toml&lt;/code&gt; is parsed. &lt;code&gt;$XDG_CONFIG_HOME&lt;/code&gt; defaults to &lt;code&gt;~/.config/&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To create this file you can copy the following example config:&lt;/p&gt; 
&lt;h3&gt;Example config&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] key symbols in the release bind are named according to their names in &lt;a href="https://raw.githubusercontent.com/feschber/lan-mouse/main/input-event/src/scancode.rs#L176"&gt;input-event/src/scancode.rs#L172&lt;/a&gt;. This is bound to change&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# example configuration

# configure release bind
release_bind = [ "KeyA", "KeyS", "KeyD", "KeyF" ]

# optional port (defaults to 4242)
port = 4242

# list of authorized tls certificate fingerprints that
# are accepted for incoming traffic
[authorized_fingerprints]
"bc:05&lt;span&gt;🆎&lt;/span&gt;7a:a4&lt;span&gt;🇩🇪&lt;/span&gt;88:8c:2f:92:ac:bc:b8:49:b8:24:0d:44:b3:e6:a4:ef:d7:0b:6c:69:6d:77:53:0b:14:80" = "iridium"

# define a client on the right side with host name "iridium"
[[clients]]
# position (left | right | top | bottom)
position = "right"
# hostname
hostname = "iridium"
# activate this client immediately when lan-mouse is started
activate_on_startup = true
# optional list of (known) ip addresses
ips = ["192.168.178.156"]

# define a client on the left side with IP address 192.168.178.189
[[clients]]
position = "left"
# The hostname is optional: When no hostname is specified,
# at least one ip address needs to be specified.
hostname = "thorium"
# ips for ethernet and wifi
ips = ["192.168.178.189", "192.168.178.172"]
# optional port
port = 4242
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where &lt;code&gt;left&lt;/code&gt; can be either &lt;code&gt;left&lt;/code&gt;, &lt;code&gt;right&lt;/code&gt;, &lt;code&gt;top&lt;/code&gt; or &lt;code&gt;bottom&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Graphical frontend (gtk + libadwaita)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; respect xdg-config-home for config file location.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; IP Address switching&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Liveness tracking Automatically ungrab mouse when client unreachable&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Liveness tracking: Automatically release keys, when server offline&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; MacOS KeyCode Translation&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Libei Input Capture&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; MacOS Input Capture&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Windows Input Capture&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Encryption&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; X11 Input Capture&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Latency measurement and visualization&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Bandwidth usage measurement and visualization&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Clipboard support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Detailed OS Support&lt;/h2&gt; 
&lt;p&gt;In order to use a device for sending events, an &lt;strong&gt;input-capture&lt;/strong&gt; backend is required, while receiving events requires a supported &lt;strong&gt;input-emulation&lt;/strong&gt; &lt;em&gt;and&lt;/em&gt; &lt;strong&gt;input-capture&lt;/strong&gt; backend.&lt;/p&gt; 
&lt;p&gt;A suitable backend is chosen automatically based on the active desktop environment / compositor.&lt;/p&gt; 
&lt;p&gt;The following sections detail the emulation and capture backends provided by lan-mouse and their support in desktop environments / operating systems.&lt;/p&gt; 
&lt;h3&gt;Input Emulation Support&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Desktop / Backend&lt;/th&gt; 
   &lt;th&gt;wlroots&lt;/th&gt; 
   &lt;th&gt;libei&lt;/th&gt; 
   &lt;th&gt;remote-desktop portal&lt;/th&gt; 
   &lt;th&gt;windows&lt;/th&gt; 
   &lt;th&gt;macos&lt;/th&gt; 
   &lt;th&gt;x11&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wayland (wlroots)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wayland (KDE)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wayland (Gnome)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MacOS&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;wlroots&lt;/code&gt;: This backend makes use of the &lt;a href="https://wayland.app/protocols/wlr-virtual-pointer-unstable-v1"&gt;wlr-virtual-pointer-unstable-v1&lt;/a&gt; and &lt;a href="https://wayland.app/protocols/virtual-keyboard-unstable-v1"&gt;virtual-keyboard-unstable-v1&lt;/a&gt; protocols and is supported by most wlroots based compositors.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;libei&lt;/code&gt;: This backend uses &lt;a href="https://gitlab.freedesktop.org/libinput/libei"&gt;libei&lt;/a&gt; and is supported by GNOME &amp;gt;= 45 or KDE Plasma &amp;gt;= 6.1.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;xdp&lt;/code&gt;: This backend uses the &lt;a href="https://flatpak.github.io/xdg-desktop-portal/#gdbus-org.freedesktop.portal.RemoteDesktop"&gt;freedesktop remote-desktop-portal&lt;/a&gt; and is supported on GNOME and Plasma.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;x11&lt;/code&gt;: Backend for X11 sessions.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;windows&lt;/code&gt;: Backend for Windows.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;macos&lt;/code&gt;: Backend for MacOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Input Capture Support&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Desktop / Backend&lt;/th&gt; 
   &lt;th&gt;layer-shell&lt;/th&gt; 
   &lt;th&gt;libei&lt;/th&gt; 
   &lt;th&gt;windows&lt;/th&gt; 
   &lt;th&gt;macos&lt;/th&gt; 
   &lt;th&gt;x11&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wayland (wlroots)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wayland (KDE)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wayland (Gnome)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MacOS&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;WIP&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;layer-shell&lt;/code&gt;: This backend creates a single pixel wide window on the edges of Displays to capture the cursor using the &lt;a href="https://wayland.app/protocols/wlr-layer-shell-unstable-v1"&gt;layer-shell protocol&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;libei&lt;/code&gt;: This backend uses &lt;a href="https://gitlab.freedesktop.org/libinput/libei"&gt;libei&lt;/a&gt; and is supported by GNOME &amp;gt;= 45 or KDE Plasma &amp;gt;= 6.1.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;windows&lt;/code&gt;: Backend for input capture on Windows.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;macos&lt;/code&gt;: Backend for input capture on MacOS.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;x11&lt;/code&gt;: TODO (not yet supported)&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>zed-industries/zed</title>
      <link>https://github.com/zed-industries/zed</link>
      <description>&lt;p&gt;Code at the speed of thought – Zed is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zed&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://zed.dev"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/zed-industries/zed/main/assets/badge/v0.json" alt="Zed" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zed-industries/zed/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/zed-industries/zed/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Welcome to Zed, a high-performance, multiplayer code editor from the creators of &lt;a href="https://github.com/atom/atom"&gt;Atom&lt;/a&gt; and &lt;a href="https://github.com/tree-sitter/tree-sitter"&gt;Tree-sitter&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;On macOS and Linux you can &lt;a href="https://zed.dev/download"&gt;download Zed directly&lt;/a&gt; or &lt;a href="https://zed.dev/docs/linux#installing-via-a-package-manager"&gt;install Zed via your local package manager&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Other platforms are not yet available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Windows (&lt;a href="https://github.com/zed-industries/zed/issues/5394"&gt;tracking issue&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Web (&lt;a href="https://github.com/zed-industries/zed/issues/5396"&gt;tracking issue&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Developing Zed&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/docs/src/development/macos.md"&gt;Building Zed for macOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/docs/src/development/linux.md"&gt;Building Zed for Linux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/docs/src/development/windows.md"&gt;Building Zed for Windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/docs/src/development/local-collaboration.md"&gt;Running Collaboration Locally&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/zed-industries/zed/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for ways you can contribute to Zed.&lt;/p&gt; 
&lt;p&gt;Also... we're hiring! Check out our &lt;a href="https://zed.dev/jobs"&gt;jobs&lt;/a&gt; page for open roles.&lt;/p&gt; 
&lt;h3&gt;Licensing&lt;/h3&gt; 
&lt;p&gt;License information for third party dependencies must be correctly provided for CI to pass.&lt;/p&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/EmbarkStudios/cargo-about"&gt;&lt;code&gt;cargo-about&lt;/code&gt;&lt;/a&gt; to automatically comply with open source licenses. If CI is failing, check the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Is it showing a &lt;code&gt;no license specified&lt;/code&gt; error for a crate you've created? If so, add &lt;code&gt;publish = false&lt;/code&gt; under &lt;code&gt;[package]&lt;/code&gt; in your crate's Cargo.toml.&lt;/li&gt; 
 &lt;li&gt;Is the error &lt;code&gt;failed to satisfy license requirements&lt;/code&gt; for a dependency? If so, first determine what license the project has and whether this system is sufficient to comply with this license's requirements. If you're unsure, ask a lawyer. Once you've verified that this system is acceptable add the license's SPDX identifier to the &lt;code&gt;accepted&lt;/code&gt; array in &lt;code&gt;script/licenses/zed-licenses.toml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Is &lt;code&gt;cargo-about&lt;/code&gt; unable to find the license for a dependency? If so, add a clarification field at the end of &lt;code&gt;script/licenses/zed-licenses.toml&lt;/code&gt;, as specified in the &lt;a href="https://embarkstudios.github.io/cargo-about/cli/generate/config.html#crate-configuration"&gt;cargo-about book&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Schniz/fnm</title>
      <link>https://github.com/Schniz/fnm</link>
      <description>&lt;p&gt;🚀 Fast and simple Node.js version manager, built in Rust&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; Fast Node Manager (&lt;code&gt;fnm&lt;/code&gt;) &lt;img alt="Amount of downloads" src="https://img.shields.io/github/downloads/Schniz/fnm/total.svg?style=flat" /&gt; &lt;a href="https://github.com/Schniz/fnm/actions"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/Schniz/fnm/rust.yml?branch=master&amp;amp;label=workflow" alt="GitHub Actions workflow status" /&gt;&lt;/a&gt; &lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🚀 Fast and simple Node.js version manager, built in Rust&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/Schniz/fnm/master/docs/fnm.svg?sanitize=true" alt="Blazing fast!" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;🌎 Cross-platform support (macOS, Windows, Linux)&lt;/p&gt; 
&lt;p&gt;✨ Single file, easy installation, instant startup&lt;/p&gt; 
&lt;p&gt;🚀 Built with speed in mind&lt;/p&gt; 
&lt;p&gt;📂 Works with &lt;code&gt;.node-version&lt;/code&gt; and &lt;code&gt;.nvmrc&lt;/code&gt; files&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Using a script (macOS/Linux)&lt;/h3&gt; 
&lt;p&gt;For &lt;code&gt;bash&lt;/code&gt;, &lt;code&gt;zsh&lt;/code&gt; and &lt;code&gt;fish&lt;/code&gt; shells, there's an &lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/.ci/install.sh"&gt;automatic installation script&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;First ensure that &lt;code&gt;curl&lt;/code&gt; and &lt;code&gt;unzip&lt;/code&gt; are already installed on your operating system. Then execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl -fsSL https://fnm.vercel.app/install | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Upgrade&lt;/h4&gt; 
&lt;p&gt;On macOS, it is as simple as &lt;code&gt;brew upgrade fnm&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;On other operating systems, upgrading &lt;code&gt;fnm&lt;/code&gt; is almost the same as installing it. To prevent duplication in your shell config file, pass &lt;code&gt;--skip-shell&lt;/code&gt; to the install command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl -fsSL https://fnm.vercel.app/install | bash -s -- --skip-shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Parameters&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;--install-dir&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Set a custom directory for fnm to be installed. The default is &lt;code&gt;$XDG_DATA_HOME/fnm&lt;/code&gt; (if &lt;code&gt;$XDG_DATA_HOME&lt;/code&gt; is not defined it falls back to &lt;code&gt;$HOME/.local/share/fnm&lt;/code&gt; on linux and &lt;code&gt;$HOME/Library/Application Support/fnm&lt;/code&gt; on MacOS).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; On macOS, this option is only meaningful when using &lt;code&gt;--force-install&lt;/code&gt; since Homebrew is the default installation method.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;code&gt;--skip-shell&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Skip appending shell specific loader to shell config file, based on the current user shell, defined in &lt;code&gt;$SHELL&lt;/code&gt;. e.g. for Bash, &lt;code&gt;$HOME/.bashrc&lt;/code&gt;. &lt;code&gt;$HOME/.zshrc&lt;/code&gt; for Zsh. For Fish - &lt;code&gt;$HOME/.config/fish/conf.d/fnm.fish&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--force-install&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;macOS installations using the installation script are deprecated in favor of the Homebrew formula, but this forces the script to install using it anyway.&lt;/p&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl -fsSL https://fnm.vercel.app/install | bash -s -- --install-dir "./.fnm" --skip-shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manually&lt;/h3&gt; 
&lt;h4&gt;Using Homebrew (macOS/Linux)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;brew install fnm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, &lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/#shell-setup"&gt;set up your shell for fnm&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Using Winget (Windows)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;winget install Schniz.fnm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Scoop (Windows)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;scoop install fnm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, &lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/#shell-setup"&gt;set up your shell for fnm&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Using Chocolatey (Windows)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;choco install fnm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, &lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/#shell-setup"&gt;set up your shell for fnm&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Using Cargo (Linux/macOS/Windows)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo install fnm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, &lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/#shell-setup"&gt;set up your shell for fnm&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Using a release binary (Linux/macOS/Windows)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download the &lt;a href="https://github.com/Schniz/fnm/releases"&gt;latest release binary&lt;/a&gt; for your system&lt;/li&gt; 
 &lt;li&gt;Make it available globally on &lt;code&gt;PATH&lt;/code&gt; environment variable&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/#shell-setup"&gt;Set up your shell for fnm&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Removing&lt;/h3&gt; 
&lt;p&gt;To remove fnm (😢), just delete the &lt;code&gt;.fnm&lt;/code&gt; folder in your home directory. You should also edit your shell configuration to remove any references to fnm (ie. read &lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/#shell-setup"&gt;Shell Setup&lt;/a&gt;, and do the opposite).&lt;/p&gt; 
&lt;h2&gt;Completions&lt;/h2&gt; 
&lt;p&gt;fnm ships its completions with the binary:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;fnm completions --shell &amp;lt;SHELL&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where &lt;code&gt;&amp;lt;SHELL&amp;gt;&lt;/code&gt; can be one of the supported shells:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bash&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;zsh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;fish&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;powershell&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please follow your shell instructions to install them.&lt;/p&gt; 
&lt;h3&gt;Shell Setup&lt;/h3&gt; 
&lt;p&gt;Environment variables need to be setup before you can start using fnm. This is done by evaluating the output of &lt;code&gt;fnm env&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Check out the &lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/docs/configuration.md"&gt;Configuration&lt;/a&gt; section to enable highly recommended features, like automatic version switching.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Adding a &lt;code&gt;.node-version&lt;/code&gt; to your project is as simple as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ node --version
v14.18.3
$ node --version &amp;gt; .node-version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the following guides for the shell you use:&lt;/p&gt; 
&lt;h4&gt;Bash&lt;/h4&gt; 
&lt;p&gt;Add the following to your &lt;code&gt;.bashrc&lt;/code&gt; profile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;eval "$(fnm env --use-on-cd --shell bash)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Zsh&lt;/h4&gt; 
&lt;p&gt;Add the following to your &lt;code&gt;.zshrc&lt;/code&gt; profile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-zsh"&gt;eval "$(fnm env --use-on-cd --shell zsh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Fish shell&lt;/h4&gt; 
&lt;p&gt;Create &lt;code&gt;~/.config/fish/conf.d/fnm.fish&lt;/code&gt; and add this line to it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-fish"&gt;fnm env --use-on-cd --shell fish | source
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;PowerShell&lt;/h4&gt; 
&lt;p&gt;Add the following to the end of your profile file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;fnm env --use-on-cd --shell powershell | Out-String | Invoke-Expression
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For macOS/Linux, the profile is located at &lt;code&gt;~/.config/powershell/Microsoft.PowerShell_profile.ps1&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For Windows location is either: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;%userprofile%\Documents\WindowsPowerShell\Microsoft.PowerShell_profile.ps1&lt;/code&gt; Powershell 5&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;%userprofile%\Documents\PowerShell\Microsoft.PowerShell_profile.ps1&lt;/code&gt; Powershell 6+&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;To create the profile file you can run this in PowerShell: &lt;pre&gt;&lt;code class="language-powershell"&gt;if (-not (Test-Path $profile)) { New-Item $profile -Force }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;To edit your profile run this in PowerShell: &lt;pre&gt;&lt;code class="language-powershell"&gt;Invoke-Item $profile
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Windows Command Prompt aka Batch aka WinCMD&lt;/h4&gt; 
&lt;p&gt;fnm is also supported but is not entirely covered. You can set up a startup script for &lt;a href="https://superuser.com/a/144348"&gt;cmd.exe&lt;/a&gt; or &lt;a href="https://superuser.com/a/1855283"&gt;Windows Terminal&lt;/a&gt; and append the following lines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-batch"&gt;@echo off
:: for /F will launch a new instance of cmd so we create a guard to prevent an infnite loop
if not defined FNM_AUTORUN_GUARD (
    set "FNM_AUTORUN_GUARD=AutorunGuard"
    FOR /f "tokens=*" %%z IN ('fnm env --use-on-cd') DO CALL %%z
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Usage with Cmder&lt;/h4&gt; 
&lt;p&gt;Usage is very similar to the normal WinCMD install, apart for a few tweaks to allow being called from the cmder startup script. The example &lt;strong&gt;assumes&lt;/strong&gt; that the &lt;code&gt;CMDER_ROOT&lt;/code&gt; environment variable is &lt;strong&gt;set&lt;/strong&gt; to the &lt;strong&gt;root directory&lt;/strong&gt; of your Cmder installation. Then you can do something like this:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make a .cmd file to invoke it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-batch"&gt;:: %CMDER_ROOT%\bin\fnm_init.cmd
@echo off
FOR /f "tokens=*" %%z IN ('fnm env --use-on-cd') DO CALL %%z
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add it to the startup script&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-batch"&gt;:: %CMDER_ROOT%\config\user_profile.cmd
call "%CMDER_ROOT%\bin\fnm_init.cmd"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can replace &lt;code&gt;%CMDER_ROOT%&lt;/code&gt; with any other convenient path too.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/docs/configuration.md"&gt;Configuration&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/docs/configuration.md"&gt;See the available configuration options for an extended configuration documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/docs/commands.md"&gt;Usage&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Schniz/fnm/master/docs/commands.md"&gt;See the available commands for an extended usage documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;PRs welcome &lt;span&gt;🎉&lt;/span&gt;&lt;/p&gt; 
&lt;h3&gt;Developing:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Install Rust
git clone https://github.com/Schniz/fnm.git
cd fnm/
cargo build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Binary:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo run -- --help # Will behave like `fnm --help`
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Tests:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cargo test
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>typst/typst</title>
      <link>https://github.com/typst/typst</link>
      <description>&lt;p&gt;A new markup-based typesetting system that is powerful and easy to learn.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img alt="Typst" src="https://user-images.githubusercontent.com/17899797/226108480-722b770e-6313-40d7-84f2-26bebb55a281.png" /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://typst.app/docs/"&gt; &lt;img alt="Documentation" src="https://img.shields.io/website?down_message=offline&amp;amp;label=docs&amp;amp;up_color=007aff&amp;amp;up_message=online&amp;amp;url=https%3A%2F%2Ftypst.app%2Fdocs" /&gt;&lt;/a&gt; &lt;a href="https://typst.app/"&gt; &lt;img alt="Typst App" src="https://img.shields.io/website?down_message=offline&amp;amp;label=typst.app&amp;amp;up_color=239dad&amp;amp;up_message=online&amp;amp;url=https%3A%2F%2Ftypst.app" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/2uDybryKPe"&gt; &lt;img alt="Discord Server" src="https://img.shields.io/discord/1054443721975922748?color=5865F2&amp;amp;label=discord&amp;amp;labelColor=555" /&gt;&lt;/a&gt; &lt;a href="https://github.com/typst/typst/raw/main/LICENSE"&gt; &lt;img alt="Apache-2 License" src="https://img.shields.io/badge/license-Apache%202-brightgreen" /&gt;&lt;/a&gt; &lt;a href="https://typst.app/jobs/"&gt; &lt;img alt="Jobs at Typst" src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Ftypst.app%2Fassets%2Fdata%2Fshields.json&amp;amp;query=%24.jobs.text&amp;amp;label=jobs&amp;amp;color=%23A561FF&amp;amp;cacheSeconds=1800" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Typst is a new markup-based typesetting system that is designed to be as powerful as LaTeX while being much easier to learn and use. Typst has:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Built-in markup for the most common formatting tasks&lt;/li&gt; 
 &lt;li&gt;Flexible functions for everything else&lt;/li&gt; 
 &lt;li&gt;A tightly integrated scripting system&lt;/li&gt; 
 &lt;li&gt;Math typesetting, bibliography management, and more&lt;/li&gt; 
 &lt;li&gt;Fast compile times thanks to incremental compilation&lt;/li&gt; 
 &lt;li&gt;Friendly error messages in case something goes wrong&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This repository contains the Typst compiler and its CLI, which is everything you need to compile Typst documents locally. For the best writing experience, consider signing up to our &lt;a href="https://typst.app/"&gt;collaborative online editor&lt;/a&gt; for free.&lt;/p&gt; 
&lt;h2&gt;Example&lt;/h2&gt; 
&lt;p&gt;A &lt;a href="https://typst.app/docs/tutorial/"&gt;gentle introduction&lt;/a&gt; to Typst is available in our documentation. However, if you want to see the power of Typst encapsulated in one image, here it is:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img alt="Example" width="900" src="https://user-images.githubusercontent.com/17899797/228031796-ced0e452-fcee-4ae9-92da-b9287764ff25.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Let's dissect what's going on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;We use &lt;em&gt;set rules&lt;/em&gt; to configure element properties like the size of pages or the numbering of headings. By setting the page height to &lt;code&gt;auto&lt;/code&gt;, it scales to fit the content. Set rules accommodate the most common configurations. If you need full control, you can also use &lt;a href="https://typst.app/docs/reference/styling/#show-rules"&gt;show rules&lt;/a&gt; to completely redefine the appearance of an element.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We insert a heading with the &lt;code&gt;= Heading&lt;/code&gt; syntax. One equals sign creates a top level heading, two create a subheading and so on. Typst has more lightweight markup like this, see the &lt;a href="https://typst.app/docs/reference/syntax/"&gt;syntax&lt;/a&gt; reference for a full list.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://typst.app/docs/reference/math/"&gt;Mathematical equations&lt;/a&gt; are enclosed in dollar signs. By adding extra spaces around the contents of an equation, we can put it into a separate block. Multi-letter identifiers are interpreted as Typst definitions and functions unless put into quotes. This way, we don't need backslashes for things like &lt;code&gt;floor&lt;/code&gt; and &lt;code&gt;sqrt&lt;/code&gt;. And &lt;code&gt;phi.alt&lt;/code&gt; applies the &lt;code&gt;alt&lt;/code&gt; modifier to the &lt;code&gt;phi&lt;/code&gt; to select a particular symbol variant.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Now, we get to some &lt;a href="https://typst.app/docs/reference/scripting/"&gt;scripting&lt;/a&gt;. To input code into a Typst document, we can write a hash followed by an expression. We define two variables and a recursive function to compute the n-th fibonacci number. Then, we display the results in a center-aligned table. The table function takes its cells row-by-row. Therefore, we first pass the formulas &lt;code&gt;$F_1$&lt;/code&gt; to &lt;code&gt;$F_8$&lt;/code&gt; and then the computed fibonacci numbers. We apply the spreading operator (&lt;code&gt;..&lt;/code&gt;) to both because they are arrays and we want to pass the arrays' items as individual arguments.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Text version of the code example.&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-typst"&gt;#set page(width: 10cm, height: auto)
#set heading(numbering: "1.")

= Fibonacci sequence
The Fibonacci sequence is defined through the
recurrence relation $F_n = F_(n-1) + F_(n-2)$.
It can also be expressed in _closed form:_

$ F_n = round(1 / sqrt(5) phi.alt^n), quad
  phi.alt = (1 + sqrt(5)) / 2 $

#let count = 8
#let nums = range(1, count + 1)
#let fib(n) = (
  if n &amp;lt;= 2 { 1 }
  else { fib(n - 1) + fib(n - 2) }
)

The first #count numbers of the sequence are:

#align(center, table(
  columns: count,
  ..nums.map(n =&amp;gt; $F_#n$),
  ..nums.map(n =&amp;gt; str(fib(n))),
))
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Typst's CLI is available from different sources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can get sources and pre-built binaries for the latest release of Typst from the &lt;a href="https://github.com/typst/typst/releases/"&gt;releases page&lt;/a&gt;. Download the archive for your platform and place it in a directory that is in your &lt;code&gt;PATH&lt;/code&gt;. To stay up to date with future releases, you can simply run &lt;code&gt;typst update&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can install Typst through different package managers. Note that the versions in the package managers might lag behind the latest release.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Linux: 
    &lt;ul&gt; 
     &lt;li&gt;View &lt;a href="https://repology.org/project/typst/versions"&gt;Typst on Repology&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;View &lt;a href="https://snapcraft.io/typst"&gt;Typst's Snap&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;macOS: &lt;code&gt;brew install typst&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Windows: &lt;code&gt;winget install --id Typst.Typst&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you have a &lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt; toolchain installed, you can install&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;the latest released Typst version with &lt;code&gt;cargo install --locked typst-cli&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;a development version with &lt;code&gt;cargo install --git https://github.com/typst/typst --locked typst-cli&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Nix users can&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;use the &lt;code&gt;typst&lt;/code&gt; package with &lt;code&gt;nix-shell -p typst&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;build and run a development version with &lt;code&gt;nix run github:typst/typst -- --version&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Docker users can run a prebuilt image with &lt;code&gt;docker run ghcr.io/typst/typst:latest --help&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Once you have installed Typst, you can use it like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Creates `file.pdf` in working directory.
typst compile file.typ

# Creates PDF file at the desired path.
typst compile path/to/source.typ path/to/output.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also watch source files and automatically recompile on changes. This is faster than compiling from scratch each time because Typst has incremental compilation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Watches source files and recompiles on changes.
typst watch file.typ
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Typst further allows you to add custom font paths for your project and list all of the fonts it discovered:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Adds additional directories to search for fonts.
typst compile --font-path path/to/fonts file.typ

# Lists all of the discovered fonts in the system and the given directory.
typst fonts --font-path path/to/fonts

# Or via environment variable (Linux syntax).
TYPST_FONT_PATHS=path/to/fonts typst fonts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For other CLI subcommands and options, see below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prints available subcommands and options.
typst help

# Prints detailed usage of a subcommand.
typst help watch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you prefer an integrated IDE-like experience with autocompletion and instant preview, you can also check out our &lt;a href="https://typst.app/"&gt;free web app&lt;/a&gt;. Alternatively, there is a community-created language server called &lt;a href="https://myriad-dreamin.github.io/tinymist/"&gt;Tinymist&lt;/a&gt; which is integrated into various editor extensions.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;The main places where the community gathers are our &lt;a href="https://forum.typst.app/"&gt;Forum&lt;/a&gt; and our &lt;a href="https://discord.gg/2uDybryKPe"&gt;Discord server&lt;/a&gt;. The Forum is a great place to ask questions, help others, and share cool things you created with Typst. The Discord server is more suitable for quicker questions, discussions about contributing, or just to chat. We'd be happy to see you there!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://typst.app/universe/"&gt;Typst Universe&lt;/a&gt; is where the community shares templates and packages. If you want to share your own creations, you can submit them to our &lt;a href="https://github.com/typst/packages/"&gt;package repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you had a bad experience in our community, please &lt;a href="https://typst.app/contact"&gt;reach out to us&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We love to see contributions from the community. If you experience bugs, feel free to open an issue. If you would like to implement a new feature or bug fix, please follow the steps outlined in the &lt;a href="https://github.com/typst/typst/raw/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To build Typst yourself, first ensure that you have the &lt;a href="https://rustup.rs/"&gt;latest stable Rust&lt;/a&gt; installed. Then, clone this repository and build the CLI with the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/typst/typst
cd typst
cargo build --release
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The optimized binary will be stored in &lt;code&gt;target/release/&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Another good way to contribute is by &lt;a href="https://github.com/typst/packages/"&gt;sharing packages&lt;/a&gt; with the community.&lt;/p&gt; 
&lt;h2&gt;Pronunciation and Spelling&lt;/h2&gt; 
&lt;p&gt;IPA: /taɪpst/. "Ty" like in &lt;strong&gt;Ty&lt;/strong&gt;pesetting and "pst" like in Hi&lt;strong&gt;pst&lt;/strong&gt;er. When writing about Typst, capitalize its name as a proper noun, with a capital "T".&lt;/p&gt; 
&lt;h2&gt;Design Principles&lt;/h2&gt; 
&lt;p&gt;All of Typst has been designed with three key goals in mind: Power, simplicity, and performance. We think it's time for a system that matches the power of LaTeX, is easy to learn and use, all while being fast enough to realize instant preview. To achieve these goals, we follow three core design principles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Simplicity through Consistency:&lt;/strong&gt; If you know how to do one thing in Typst, you should be able to transfer that knowledge to other things. If there are multiple ways to do the same thing, one of them should be at a different level of abstraction than the other. E.g. it's okay that &lt;code&gt;= Introduction&lt;/code&gt; and &lt;code&gt;#heading[Introduction]&lt;/code&gt; do the same thing because the former is just syntax sugar for the latter.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Power through Composability:&lt;/strong&gt; There are two ways to make something flexible: Have a knob for everything or have a few knobs that you can combine in many ways. Typst is designed with the second way in mind. We provide systems that you can compose in ways we've never even thought of. TeX is also in the second category, but it's a bit low-level and therefore people use LaTeX instead. But there, we don't really have that much composability. Instead, there's a package for everything (&lt;code&gt;\usepackage{knob}&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Performance through Incrementality:&lt;/strong&gt; All Typst language features must accommodate for incremental compilation. Luckily we have &lt;a href="https://github.com/typst/comemo/"&gt;&lt;code&gt;comemo&lt;/code&gt;&lt;/a&gt;, a system for incremental compilation which does most of the hard work in the background.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We'd like to thank everyone who is supporting Typst's development, be it via &lt;a href="https://github.com/sponsors/typst/"&gt;GitHub sponsors&lt;/a&gt; or elsewhere. In particular, special thanks[^1] go to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://posit.co/blog/posit-and-typst/"&gt;Posit&lt;/a&gt; for financing a full-time compiler engineer&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nlnet.nl/"&gt;NLnet&lt;/a&gt; for supporting work on Typst via multiple grants through the &lt;a href="https://nlnet.nl/core"&gt;NGI Zero Core&lt;/a&gt; fund: 
  &lt;ul&gt; 
   &lt;li&gt;Work on &lt;a href="https://nlnet.nl/project/Typst-HTML/"&gt;HTML export&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Work on &lt;a href="https://nlnet.nl/project/Typst-Accessibility/"&gt;PDF accessibility&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.science-startups.berlin/"&gt;Science &amp;amp; Startups&lt;/a&gt; for having financed Typst development from January through June 2023 via the Berlin Startup Scholarship&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://zerodha.tech/blog/1-5-million-pdfs-in-25-minutes/"&gt;Zerodha&lt;/a&gt; for their generous one-time sponsorship&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;[^1]: This list only includes contributions for our open-source work that exceed or are expected to exceed €10K.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>leptos-rs/leptos</title>
      <link>https://github.com/leptos-rs/leptos</link>
      <description>&lt;p&gt;Build fast web applications with Rust.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source srcset="https://raw.githubusercontent.com/leptos-rs/leptos/main/docs/logos/Leptos_logo_pref_dark_RGB.svg" media="(prefers-color-scheme: dark)" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/leptos-rs/leptos/main/docs/logos/Leptos_logo_RGB.svg?sanitize=true" alt="Leptos Logo" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;&lt;a href="https://crates.io/crates/leptos"&gt;&lt;img src="https://img.shields.io/crates/v/leptos.svg?sanitize=true" alt="crates.io" /&gt;&lt;/a&gt; &lt;a href="https://docs.rs/leptos"&gt;&lt;img src="https://docs.rs/leptos/badge.svg?sanitize=true" alt="docs.rs" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/crates/msrv/leptos" alt="Crates.io MSRV" /&gt; &lt;a href="https://discord.gg/YdRAhS7eQB"&gt;&lt;img src="https://img.shields.io/discord/1031524867910148188?color=%237289DA&amp;amp;label=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/#leptos:matrix.org"&gt;&lt;img src="https://img.shields.io/badge/Matrix-leptos-grey?logo=matrix&amp;amp;labelColor=white&amp;amp;logoColor=black" alt="Matrix" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://leptos.dev"&gt;Website&lt;/a&gt; | &lt;a href="https://leptos-rs.github.io/leptos/"&gt;Book&lt;/a&gt; | &lt;a href="https://docs.rs/leptos/latest/leptos/"&gt;Docs.rs&lt;/a&gt; | &lt;a href="https://codesandbox.io/p/sandbox/leptos-rtfggt?file=%2Fsrc%2Fmain.rs%3A1%2C1"&gt;Playground&lt;/a&gt; | &lt;a href="https://discord.gg/YdRAhS7eQB"&gt;Discord&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can find a list of useful libraries and example projects at &lt;a href="https://github.com/leptos-rs/awesome-leptos"&gt;&lt;code&gt;awesome-leptos&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Leptos&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-rust"&gt;use leptos::*;

#[component]
pub fn SimpleCounter(initial_value: i32) -&amp;gt; impl IntoView {
    // create a reactive signal with the initial value
    let (value, set_value) = signal(initial_value);

    // create event handlers for our buttons
    // note that `value` and `set_value` are `Copy`, so it's super easy to move them into closures
    let clear = move |_| set_value(0);
    let decrement = move |_| set_value.update(|value| *value -= 1);
    let increment = move |_| set_value.update(|value| *value += 1);

    // create user interfaces with the declarative `view!` macro
    view! {
        &amp;lt;div&amp;gt;
            &amp;lt;button on:click=clear&amp;gt;Clear&amp;lt;/button&amp;gt;
            &amp;lt;button on:click=decrement&amp;gt;-1&amp;lt;/button&amp;gt;
            // text nodes can be quoted or unquoted
            &amp;lt;span&amp;gt;"Value: " {value} "!"&amp;lt;/span&amp;gt;
            &amp;lt;button on:click=increment&amp;gt;+1&amp;lt;/button&amp;gt;
        &amp;lt;/div&amp;gt;
    }
}

// we also support a builder syntax rather than the JSX-like `view` macro
#[component]
pub fn SimpleCounterWithBuilder(initial_value: i32) -&amp;gt; impl IntoView {
    use leptos::html::*;

    let (value, set_value) = signal(initial_value);
    let clear = move |_| set_value(0);
    let decrement = move |_| set_value.update(|value| *value -= 1);
    let increment = move |_| set_value.update(|value| *value += 1);

    // the `view` macro above expands to this builder syntax
    div().child((
        button().on(ev::click, clear).child("Clear"),
        button().on(ev::click, decrement).child("-1"),
        span().child(("Value: ", value, "!")),
        button().on(ev::click, increment).child("+1")
    ))
}

// Easy to use with Trunk (trunkrs.dev) or with a simple wasm-bindgen setup
pub fn main() {
    mount_to_body(|| view! {
        &amp;lt;SimpleCounter initial_value=3 /&amp;gt;
    })
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;About the Framework&lt;/h2&gt; 
&lt;p&gt;Leptos is a full-stack, isomorphic Rust web framework leveraging fine-grained reactivity to build declarative user interfaces.&lt;/p&gt; 
&lt;h2&gt;What does that mean?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Full-stack&lt;/strong&gt;: Leptos can be used to build apps that run in the browser (client-side rendering), on the server (server-side rendering), or by rendering HTML on the server and then adding interactivity in the browser (server-side rendering with hydration). This includes support for HTTP streaming of both data (&lt;a href="https://docs.rs/leptos/latest/leptos/struct.Resource.html"&gt;&lt;code&gt;Resource&lt;/code&gt;s&lt;/a&gt;) and HTML (out-of-order or in-order streaming of &lt;a href="https://docs.rs/leptos/latest/leptos/fn.Suspense.html"&gt;&lt;code&gt;&amp;lt;Suspense/&amp;gt;&lt;/code&gt;&lt;/a&gt; components.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Isomorphic&lt;/strong&gt;: Leptos provides primitives to write isomorphic &lt;a href="https://docs.rs/leptos_server/0.2.5/leptos_server/index.html"&gt;server functions&lt;/a&gt;, i.e., functions that can be called with the “same shape” on the client or server, but only run on the server. This means you can write your server-only logic (database requests, authentication etc.) alongside the client-side components that will consume it, and call server functions as if they were running in the browser, without needing to create and maintain a separate REST or other API.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web&lt;/strong&gt;: Leptos is built on the Web platform and Web standards. The &lt;a href="https://docs.rs/leptos_router/latest/leptos_router/"&gt;router&lt;/a&gt; is designed to use Web fundamentals (like links and forms) and build on top of them rather than trying to replace them.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: Leptos provides most of what you need to build a modern web app: a reactive system, templating library, and a router that works on both the server and client side.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fine-grained reactivity&lt;/strong&gt;: The entire framework is built from reactive primitives. This allows for extremely performant code with minimal overhead: when a reactive signal’s value changes, it can update a single text node, toggle a single class, or remove an element from the DOM without any other code running. (So, no virtual DOM overhead!)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Declarative&lt;/strong&gt;: Tell Leptos how you want the page to look, and let the framework tell the browser how to do it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn more&lt;/h2&gt; 
&lt;p&gt;Here are some resources for learning more about Leptos:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://leptos-rs.github.io/leptos/"&gt;Book&lt;/a&gt; (work in progress)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/leptos-rs/leptos/tree/main/examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.rs/leptos/latest/leptos/"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/leptos-rs/leptos/tree/main/docs/COMMON_BUGS.md"&gt;Common Bugs&lt;/a&gt; (and how to fix them!)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;cargo-leptos&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/leptos-rs/cargo-leptos"&gt;&lt;code&gt;cargo-leptos&lt;/code&gt;&lt;/a&gt; is a build tool that's designed to make it easy to build apps that run on both the client and the server, with seamless integration. The best way to get started with a real Leptos project right now is to use &lt;code&gt;cargo-leptos&lt;/code&gt; and our starter templates for &lt;a href="https://github.com/leptos-rs/start"&gt;Actix&lt;/a&gt; or &lt;a href="https://github.com/leptos-rs/start-axum"&gt;Axum&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install cargo-leptos
cargo leptos new --git https://github.com/leptos-rs/start-axum
cd [your project name]
cargo leptos watch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open browser to &lt;a href="http://localhost:3000/"&gt;http://localhost:3000/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;FAQs&lt;/h2&gt; 
&lt;h3&gt;What’s up with the name?&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Leptos&lt;/em&gt; (λεπτός) is an ancient Greek word meaning “thin, light, refined, fine-grained.” To me, a classicist and not a dog owner, it evokes the lightweight reactive system that powers the framework. I've since learned the same word is at the root of the medical term “leptospirosis,” a blood infection that affects humans and animals... My bad. No dogs were harmed in the creation of this framework.&lt;/p&gt; 
&lt;h3&gt;Is it production ready?&lt;/h3&gt; 
&lt;p&gt;People usually mean one of three things by this question.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Are the APIs stable?&lt;/strong&gt; i.e., will I have to rewrite my whole app from Leptos 0.1 to 0.2 to 0.3 to 0.4, or can I write it now and benefit from new features and updates as new versions come?&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The APIs are basically settled. We’re adding new features, but we’re very happy with where the type system and patterns have landed. I would not expect major breaking changes to your code to adapt to future releases, in terms of architecture.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;Are there bugs?&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Yes, I’m sure there are. You can see from the state of our issue tracker over time that there aren’t that &lt;em&gt;many&lt;/em&gt; bugs and they’re usually resolved pretty quickly. But for sure, there may be moments where you encounter something that requires a fix at the framework level, which may not be immediately resolved.&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;Am I a consumer or a contributor?&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This may be the big one: “production ready” implies a certain orientation to a library: that you can basically use it, without any special knowledge of its internals or ability to contribute. Everyone has this at some level in their stack: for example I (@gbj) don’t have the capacity or knowledge to contribute to something like &lt;code&gt;wasm-bindgen&lt;/code&gt; at this point: I simply rely on it to work.&lt;/p&gt; 
&lt;p&gt;There are several people in the community using Leptos right now for many websites at work, who have also become significant contributors. There may be missing features that you need, and you may end up building them! But, if you're willing to contribute a few missing pieces along the way, the framework is most definitely usable for production applications, especially given the ecosystem of libraries that have sprung up around it.&lt;/p&gt; 
&lt;h3&gt;Can I use this for native GUI?&lt;/h3&gt; 
&lt;p&gt;Sure! Obviously the &lt;code&gt;view&lt;/code&gt; macro is for generating DOM nodes but you can use the reactive system to drive any native GUI toolkit that uses the same kind of object-oriented, event-callback-based framework as the DOM pretty easily. The principles are the same:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use signals, derived signals, and memos to create your reactive system&lt;/li&gt; 
 &lt;li&gt;Create GUI widgets&lt;/li&gt; 
 &lt;li&gt;Use event listeners to update signals&lt;/li&gt; 
 &lt;li&gt;Create effects to update the UI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The 0.7 update originally set out to create a "generic rendering" approach that would allow us to reuse most of the same view logic to do all of the above. Unfortunately, this has had to be shelved for now due to difficulties encountered by the Rust compiler when building larger-scale applications with the number of generics spread throughout the codebase that this required. It's an approach I'm looking forward to exploring again in the future; feel free to reach out if you're interested in this kind of work.&lt;/p&gt; 
&lt;h3&gt;How is this different from Yew?&lt;/h3&gt; 
&lt;p&gt;Yew is the most-used library for Rust web UI development, but there are several differences between Yew and Leptos, in philosophy, approach, and performance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;VDOM vs. fine-grained:&lt;/strong&gt; Yew is built on the virtual DOM (VDOM) model: state changes cause components to re-render, generating a new virtual DOM tree. Yew diffs this against the previous VDOM, and applies those patches to the actual DOM. Component functions rerun whenever state changes. Leptos takes an entirely different approach. Components run once, creating (and returning) actual DOM nodes and setting up a reactive system to update those DOM nodes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; This has huge performance implications: Leptos is simply much faster at both creating and updating the UI than Yew is.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Server integration:&lt;/strong&gt; Yew was created in an era in which browser-rendered single-page apps (SPAs) were the dominant paradigm. While Leptos supports client-side rendering, it also focuses on integrating with the server side of your application via server functions and multiple modes of serving HTML, including out-of-order streaming.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How is this different from Dioxus?&lt;/h3&gt; 
&lt;p&gt;Like Leptos, Dioxus is a framework for building UIs using web technologies. However, there are significant differences in approach and features.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;VDOM vs. fine-grained:&lt;/strong&gt; While Dioxus has a performant virtual DOM (VDOM), it still uses coarse-grained/component-scoped reactivity: changing a stateful value reruns the component function and diffs the old UI against the new one. Leptos components use a different mental model, creating (and returning) actual DOM nodes and setting up a reactive system to update those DOM nodes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web vs. desktop priorities:&lt;/strong&gt; Dioxus uses Leptos server functions in its fullstack mode, but does not have the same &lt;code&gt;&amp;lt;Suspense&amp;gt;&lt;/code&gt;-based support for things like streaming HTML rendering, or share the same focus on holistic web performance. Leptos tends to prioritize holistic web performance (streaming HTML rendering, smaller WASM binary sizes, etc.), whereas Dioxus has an unparalleled experience when building desktop apps, because your application logic runs as a native Rust binary.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How is this different from Sycamore?&lt;/h3&gt; 
&lt;p&gt;Sycamore and Leptos are both heavily influenced by SolidJS. At this point, Leptos has a larger community and ecosystem and is more actively developed. Other differences:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Templating DSLs:&lt;/strong&gt; Sycamore uses a custom templating language for its views, while Leptos uses a JSX-like template format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;'static&lt;/code&gt; signals:&lt;/strong&gt; One of Leptos’s main innovations was the creation of &lt;code&gt;Copy + 'static&lt;/code&gt; signals, which have excellent ergonomics. Sycamore is in the process of adopting the same pattern, but this is not yet released.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Perseus vs. server functions:&lt;/strong&gt; The Perseus metaframework provides an opinionated way to build Sycamore apps that include server functionality. Leptos instead provides primitives like server functions in the core of the framework.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>mainmatter/100-exercises-to-learn-rust</title>
      <link>https://github.com/mainmatter/100-exercises-to-learn-rust</link>
      <description>&lt;p&gt;A self-paced course to learn Rust, one exercise at a time.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn Rust, one exercise at a time&lt;/h1&gt; 
&lt;p&gt;You've heard about Rust, but you never had the chance to try it out?&lt;br /&gt; This course is for you!&lt;/p&gt; 
&lt;p&gt;You'll learn Rust by solving 100 exercises.&lt;br /&gt; You'll go from knowing nothing about Rust to being able to start writing your own programs, one exercise at a time.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This course has been written by &lt;a href="https://mainmatter.com/rust-consulting/"&gt;Mainmatter&lt;/a&gt;.&lt;br /&gt; It's one of the trainings in &lt;a href="https://mainmatter.com/services/workshops/rust/"&gt;our portfolio of Rust workshops&lt;/a&gt;.&lt;br /&gt; Check out our &lt;a href="https://mainmatter.com/rust-consulting/"&gt;landing page&lt;/a&gt; if you're looking for Rust consulting or training!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Go to &lt;a href="https://rust-exercises.com"&gt;rust-exercises.com&lt;/a&gt; and follow the instructions there to get started with the course.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Rust&lt;/strong&gt; (follow instructions &lt;a href="https://www.rust-lang.org/tools/install"&gt;here&lt;/a&gt;).&lt;br /&gt; If &lt;code&gt;rustup&lt;/code&gt; is already installed on your system, run &lt;code&gt;rustup update&lt;/code&gt; (or another appropriate command depending on how you installed Rust on your system) to make sure you're running on the latest stable version.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;(Optional but recommended)&lt;/em&gt; An IDE with Rust autocompletion support. We recommend one of the following: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.jetbrains.com/rust/"&gt;RustRover&lt;/a&gt;;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://code.visualstudio.com"&gt;Visual Studio Code&lt;/a&gt; with the &lt;a href="https://marketplace.visualstudio.com/items?itemName=matklad.rust-analyzer"&gt;&lt;code&gt;rust-analyzer&lt;/code&gt;&lt;/a&gt; extension.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Solutions&lt;/h2&gt; 
&lt;p&gt;You can find the solutions to the exercises in the &lt;a href="https://github.com/mainmatter/100-exercises-to-learn-rust/tree/solutions"&gt;&lt;code&gt;solutions&lt;/code&gt; branch&lt;/a&gt; of this repository.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Copyright © 2024- Mainmatter GmbH (&lt;a href="https://mainmatter.com"&gt;https://mainmatter.com&lt;/a&gt;), released under the &lt;a href="https://creativecommons.org/licenses/by-nc/4.0/"&gt;Creative Commons Attribution-NonCommercial 4.0 International license&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>erebe/wstunnel</title>
      <link>https://github.com/erebe/wstunnel</link>
      <description>&lt;p&gt;Tunnel all your traffic over Websocket or HTTP2 - Bypass firewalls/DPI - Static binary available&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://github.com/erebe/wstunnel/raw/main/docs/logo_wstunnel.png" alt="wstunnel logo" height="400" /&gt; &lt;/p&gt; 
&lt;p align="right"&gt; &lt;a href="https://ko-fi.com/P5P4QCHMO"&gt;&lt;img src="https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#description"&gt;Description&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#demo"&gt;Demo server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#cmd"&gt;Command line&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#release"&gt;Release&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#note"&gt;Note&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#bench"&gt;Benchmark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#build"&gt;How to build&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Description &lt;a name="description"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Most of the time when you are using a public network, you are behind some kind of firewall or proxy. One of their purpose is to constrain you to only use certain kind of protocols and consult only a subset of the web. Nowadays, the most widespread protocol is http and is de facto allowed by third party equipment.&lt;/p&gt; 
&lt;p&gt;Wstunnel uses the websocket protocol which is compatible with http in order to bypass firewalls and proxies. Wstunnel allows you to tunnel whatever traffic you want and access whatever resources/site you need.&lt;/p&gt; 
&lt;p&gt;My inspiration came from &lt;a href="https://www.npmjs.com/package/wstunnel"&gt;this project&lt;/a&gt; but as I don't want to install npm and nodejs to use this tool, I remade it in &lt;del&gt;Haskell&lt;/del&gt; Rust and improved it.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;What to expect:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Easy to use&lt;/li&gt; 
 &lt;li&gt;Good error messages and debug information&lt;/li&gt; 
 &lt;li&gt;Static forward (reverse) tunneling (TCP, UDP, Unix socket, Stdio)&lt;/li&gt; 
 &lt;li&gt;Dynamic (reverse) tunneling (Socks5 proxy, HTTP proxy and Transparent Proxy)&lt;/li&gt; 
 &lt;li&gt;Support for using http proxy (when behind one) as gateway&lt;/li&gt; 
 &lt;li&gt;Support of proxy protocol&lt;/li&gt; 
 &lt;li&gt;Support for tls/https server with certificates auto-reload (with embedded self-signed certificate, or your own)&lt;/li&gt; 
 &lt;li&gt;Support of mTLS with certificates auto-reload - &lt;a href="https://github.com/erebe/wstunnel/raw/main/docs/using_mtls.md"&gt;documentation here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Support IPv6&lt;/li&gt; 
 &lt;li&gt;Support for Websocket and HTTP2 as transport protocol (websocket is more performant)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone binaries&lt;/strong&gt; (so just cp it where you want) &lt;a href="https://github.com/erebe/wstunnel/releases"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors &lt;a name="sponsors"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Part of Wstunnel development has been sponsored by&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://serviceplanet.nl"&gt; &lt;img width="200" height="100" src="https://github.com/erebe/wstunnel/raw/main/docs/logo_serviceplanet.png" alt="service planet logo" /&gt; &lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Note &lt;a name="note"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;v7.0.0 is a complete rewrite of wstunnel in Rust and is not compatible with previous version. Previous code in Haskell can be found on branch &lt;a href="https://github.com/erebe/wstunnel/tree/haskell"&gt;https://github.com/erebe/wstunnel/tree/haskell&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;What to expect from previous version:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;More throughput and less jitter due to Haskell GC. Most of you will not care, as it was performant enough already. But you can now saturate a gigabit ethernet card with a single connection&lt;/li&gt; 
 &lt;li&gt;Command line is more homogeneous/has better UX. All tunnel can be specified multiple times&lt;/li&gt; 
 &lt;li&gt;Tunnel protocol tries to look like normal traffic, to avoid being flagged&lt;/li&gt; 
 &lt;li&gt;Support of reverse tunneling&lt;/li&gt; 
 &lt;li&gt;New bug, it is a rewrite (╯'□')╯︵ ┻━┻ ¯\&lt;em&gt;(ツ)&lt;/em&gt;/¯&lt;/li&gt; 
 &lt;li&gt;Mainly for me to ease the maintenance of the project. I don't do a lot of haskell nowadays and it was harder for me to keep maintening the project over time, as I get lost in touch of the Haskell ecosystem and new release.&lt;/li&gt; 
 &lt;li&gt;Armv7 build (aka raspberry pi), as new version of GHC (Haskell compiler) dropped its support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo server &lt;a name="demo"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;If you just want to try out that you can bypass your proxy/firewall. You can give it a try with wstunnel demo server.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# In a terminal start wstunnel client
# You can set as tls-sni-override whatever domain you want. The tunnel is the only one that is going to be allowed. 
wstunnel client -L 'tcp://4443:localhost:444?proxy_protocol' -P demo --tls-sni-override=google.fr wss://49.13.58.9

# on another terminal, run curl and it should return you this greetings
curl -k https://localhost:4443
&amp;gt; Memento mori !
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Command line &lt;a name="cmd"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;Usage: wstunnel client [OPTIONS] &amp;lt;ws[s]|http[s]://wstunnel.server.com[:port]&amp;gt;

Arguments:
  &amp;lt;ws[s]|http[s]://wstunnel.server.com[:port]&amp;gt;
          Address of the wstunnel server
          You can either use websocket or http2 as transport protocol. Use websocket if you are unsure.
          Example: For websocket with TLS wss://wstunnel.example.com or without ws://wstunnel.example.com
                   For http2 with TLS https://wstunnel.example.com or without http://wstunnel.example.com
          
          *WARNING* HTTP2 as transport protocol is harder to make it works because:
            - If you are behind a (reverse) proxy/CDN they are going to buffer the whole request before forwarding it to the server
              Obviously, this is not going to work for tunneling traffic
            - if you have wstunnel behind a reverse proxy, most of them (i.e: nginx) are going to turn http2 request into http1
              This is not going to work, because http1 does not support streaming naturally
          The only way to make it works with http2 is to have wstunnel directly exposed to the internet without any reverse proxy in front of it

Options:
  -L, --local-to-remote &amp;lt;{tcp,udp,socks5,stdio,unix}://[BIND:]PORT:HOST:PORT&amp;gt;
          Listen on local and forwards traffic from remote. Can be specified multiple times
          examples:
          'tcp://1212:google.com:443'      =&amp;gt;       listen locally on tcp on port 1212 and forward to google.com on port 443
          'tcp://2:n.lan:4?proxy_protocol' =&amp;gt;       listen locally on tcp on port 2 and forward to n.lan on port 4
                                                    Send a proxy protocol header v2 when establishing connection to n.lan
          
          'udp://1212:1.1.1.1:53'          =&amp;gt;       listen locally on udp on port 1212 and forward to cloudflare dns 1.1.1.1 on port 53
          'udp://1212:1.1.1.1:53?timeout_sec=10'    timeout_sec on udp force close the tunnel after 10sec. Set it to 0 to disable the timeout [default: 30]
          
          'socks5://[::1]:1212'            =&amp;gt;       listen locally with socks5 on port 1212 and forward dynamically requested tunnel
          'socks5://[::1]:1212?login=admin&amp;amp;password=admin' =&amp;gt; listen locally with socks5 on port 1212 and only accept connection with login=admin and password=admin
          
          'http://[::1]:1212'              =&amp;gt;       start a http proxy on port 1212 and forward dynamically requested tunnel
          'http://[::1]:1212?login=admin&amp;amp;password=admin' =&amp;gt; start a http proxy on port 1212 and only accept connection with login=admin and password=admin

          'tproxy+tcp://[::1]:1212'        =&amp;gt;       listen locally on tcp on port 1212 as a *transparent proxy* and forward dynamically requested tunnel
          'tproxy+udp://[::1]:1212?timeout_sec=10'  listen locally on udp on port 1212 as a *transparent proxy* and forward dynamically requested tunnel
                                                    linux only and requires sudo/CAP_NET_ADMIN
          
          'stdio://google.com:443'         =&amp;gt;       listen for data from stdio, mainly for `ssh -o ProxyCommand="wstunnel client --log-lvl=off -L stdio://%h:%p ws://localhost:8080" my-server`
          
          'unix:///tmp/wstunnel.sock:g.com:443' =&amp;gt;  listen for data from unix socket of path /tmp/wstunnel.sock and forward to g.com:443

  -R, --remote-to-local &amp;lt;{tcp,udp,socks5,unix}://[BIND:]PORT:HOST:PORT&amp;gt;
          Listen on remote and forwards traffic from local. Can be specified multiple times. Only tcp is supported
          examples:
          'tcp://1212:google.com:443'      =&amp;gt;     listen on server for incoming tcp cnx on port 1212 and forward to google.com on port 443 from local machine
          'udp://1212:1.1.1.1:53'          =&amp;gt;     listen on server for incoming udp on port 1212 and forward to cloudflare dns 1.1.1.1 on port 53 from local machine
          'socks5://[::1]:1212'            =&amp;gt;     listen on server for incoming socks5 request on port 1212 and forward dynamically request from local machine
          'http://[::1]:1212'              =&amp;gt;     listen on server for incoming http proxy request on port 1212 and forward dynamically request from local machine (login/password is supported)
          'unix://wstunnel.sock:g.com:443' =&amp;gt;     listen on server for incoming data from unix socket of path wstunnel.sock and forward to g.com:443 from local machine

      --no-color &amp;lt;NO_COLOR&amp;gt;
          Disable color output in logs
          
          [env: NO_COLOR=]

      --socket-so-mark &amp;lt;INT&amp;gt;
          (linux only) Mark network packet with SO_MARK sockoption with the specified value.
          You need to use {root, sudo, capabilities} to run wstunnel when using this option

  -c, --connection-min-idle &amp;lt;INT&amp;gt;
          Client will maintain a pool of open connection to the server, in order to speed up the connection process.
          This option set the maximum number of connection that will be kept open.
          This is useful if you plan to create/destroy a lot of tunnel (i.e: with socks5 to navigate with a browser)
          It will avoid the latency of doing tcp + tls handshake with the server
          
          [default: 0]

      --nb-worker-threads &amp;lt;INT&amp;gt;
          *WARNING* The flag does nothing, you need to set the env variable *WARNING*
          Control the number of threads that will be used.
          By default, it is equal the number of cpus
          
          [env: TOKIO_WORKER_THREADS=]

      --log-lvl &amp;lt;LOG_LEVEL&amp;gt;
          Control the log verbosity. i.e: TRACE, DEBUG, INFO, WARN, ERROR, OFF
          for more details: https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#example-syntax
          
          [env: RUST_LOG=]
          [default: INFO]

      --tls-sni-override &amp;lt;DOMAIN_NAME&amp;gt;
          Domain name that will be used as SNI during TLS handshake
          Warning: If you are behind a CDN (i.e: Cloudflare) you must set this domain also in the http HOST header.
                   or it will be flagged as fishy and your request rejected

      --tls-sni-disable
          Disable sending SNI during TLS handshake
          Warning: Most reverse proxies rely on it

      --tls-ech-enable
          Enable ECH (encrypted sni) during TLS handshake to wstunnel server.
          Warning: Ech DNS config is not refreshed over time. It is retrieved only once at startup of the program

      --tls-verify-certificate
          Enable TLS certificate verification.
          Disabled by default. The client will happily connect to any server with self-signed certificate.

  -p, --http-proxy &amp;lt;USER:PASS@HOST:PORT&amp;gt;
          If set, will use this http proxy to connect to the server
          
          [env: HTTP_PROXY=]

      --http-proxy-login &amp;lt;LOGIN&amp;gt;
          If set, will use this login to connect to the http proxy. Override the one from --http-proxy
          
          [env: WSTUNNEL_HTTP_PROXY_LOGIN=]

      --http-proxy-password &amp;lt;PASSWORD&amp;gt;
          If set, will use this password to connect to the http proxy. Override the one from --http-proxy
          
          [env: WSTUNNEL_HTTP_PROXY_PASSWORD=]

  -P, --http-upgrade-path-prefix &amp;lt;HTTP_UPGRADE_PATH_PREFIX&amp;gt;
          Use a specific prefix that will show up in the http path during the upgrade request.
          Useful if you need to route requests server side but don't have vhosts
          
          [env: WSTUNNEL_HTTP_UPGRADE_PATH_PREFIX=]
          [default: v1]

      --http-upgrade-credentials &amp;lt;USER[:PASS]&amp;gt;
          Pass authorization header with basic auth credentials during the upgrade request.
          If you need more customization, you can use the http_headers option.

      --websocket-ping-frequency-sec &amp;lt;seconds&amp;gt;
          Frequency at which the client will send websocket ping to the server.
          
          [default: 30]

      --websocket-mask-frame
          Enable the masking of websocket frames. Default is false
          Enable this option only if you use unsecure (non TLS) websocket server, and you see some issues. Otherwise, it is just overhead.

  -H, --http-headers &amp;lt;HEADER_NAME: HEADER_VALUE&amp;gt;
          Send custom headers in the upgrade request
          Can be specified multiple time

      --http-headers-file &amp;lt;FILE_PATH&amp;gt;
          Send custom headers in the upgrade request reading them from a file.
          It overrides http_headers specified from command line.
          File is read everytime and file format must contain lines with `HEADER_NAME: HEADER_VALUE`

      --tls-certificate &amp;lt;FILE_PATH&amp;gt;
          [Optional] Certificate (pem) to present to the server when connecting over TLS (HTTPS).
          Used when the server requires clients to authenticate themselves with a certificate (i.e. mTLS).
          The certificate will be automatically reloaded if it changes

      --tls-private-key &amp;lt;FILE_PATH&amp;gt;
          [Optional] The private key for the corresponding certificate used with mTLS.
          The certificate will be automatically reloaded if it changes

      --dns-resolver &amp;lt;DNS_RESOLVER&amp;gt;
          Dns resolver to use to lookup ips of domain name. Can be specified multiple time
          Example:
           dns://1.1.1.1 for using udp
           dns+https://1.1.1.1?sni=cloudflare-dns.com for using dns over HTTPS
           dns+tls://8.8.8.8?sni=dns.google for using dns over TLS
          For Dns over HTTPS/TLS if an HTTP proxy is configured, it will be used also
          To use libc resolver, use
          system://0.0.0.0

          **WARN** On windows you may want to specify explicitly the DNS resolver to avoid excessive DNS queries
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;SERVER
Usage: wstunnel server [OPTIONS] &amp;lt;ws[s]://0.0.0.0[:port]&amp;gt;

Arguments:
  &amp;lt;ws[s]://0.0.0.0[:port]&amp;gt;
          Address of the wstunnel server to bind to
          Example: With TLS wss://0.0.0.0:8080 or without ws://[::]:8080
          
          The server is capable of detecting by itself if the request is websocket or http2. So you don't need to specify it.

Options:
      --socket-so-mark &amp;lt;INT&amp;gt;
          (linux only) Mark network packet with SO_MARK sockoption with the specified value.
          You need to use {root, sudo, capabilities} to run wstunnel when using this option

      --websocket-ping-frequency-sec &amp;lt;seconds&amp;gt;
          Frequency at which the server will send websocket ping to client.

      --no-color &amp;lt;NO_COLOR&amp;gt;
          Disable color output in logs
          
          [env: NO_COLOR=]

      --websocket-mask-frame
          Enable the masking of websocket frames. Default is false
          Enable this option only if you use unsecure (non TLS) websocket server, and you see some issues. Otherwise, it is just overhead.

      --nb-worker-threads &amp;lt;INT&amp;gt;
          *WARNING* The flag does nothing, you need to set the env variable *WARNING*
          Control the number of threads that will be used.
          By default, it is equal the number of cpus
          
          [env: TOKIO_WORKER_THREADS=]

      --restrict-to &amp;lt;DEST:PORT&amp;gt;
          Server will only accept connection from the specified tunnel information.
          Can be specified multiple time
          Example: --restrict-to "google.com:443" --restrict-to "localhost:22"

      --dns-resolver &amp;lt;DNS_RESOLVER&amp;gt;
          Dns resolver to use to lookup ips of domain name
          This option is not going to work if you use transparent proxy
          Can be specified multiple time
          Example:
           dns://1.1.1.1 for using udp
           dns+https://1.1.1.1?sni=loudflare-dns.com for using dns over HTTPS
           dns+tls://8.8.8.8?sni=dns.google for using dns over TLS
          To use libc resolver, use
          system://0.0.0.0

      --log-lvl &amp;lt;LOG_LEVEL&amp;gt;
          Control the log verbosity. i.e: TRACE, DEBUG, INFO, WARN, ERROR, OFF
          for more details: https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#example-syntax
          
          [env: RUST_LOG=]
          [default: INFO]

  -r, --restrict-http-upgrade-path-prefix &amp;lt;RESTRICT_HTTP_UPGRADE_PATH_PREFIX&amp;gt;
          Server will only accept connection from if this specific path prefix is used during websocket upgrade.
          Useful if you specify in the client a custom path prefix, and you want the server to only allow this one.
          The path prefix act as a secret to authenticate clients
          Disabled by default. Accept all path prefix. Can be specified multiple time
          
          [env: WSTUNNEL_RESTRICT_HTTP_UPGRADE_PATH_PREFIX=]

      --restrict-config &amp;lt;RESTRICT_CONFIG&amp;gt;
          Path to the location of the restriction yaml config file.
          Restriction file is automatically reloaded if it changes

      --tls-certificate &amp;lt;FILE_PATH&amp;gt;
          [Optional] Use custom certificate (pem) instead of the default embedded self-signed certificate.
          The certificate will be automatically reloaded if it changes

      --tls-private-key &amp;lt;FILE_PATH&amp;gt;
          [Optional] Use a custom tls key (pem, ec, rsa) that the server will use instead of the default embedded one
          The private key will be automatically reloaded if it changes

      --tls-client-ca-certs &amp;lt;FILE_PATH&amp;gt;
          [Optional] Enables mTLS (client authentication with certificate). Argument must be PEM file
          containing one or more certificates of CA's of which the certificate of clients needs to be signed with.
          The ca will be automatically reloaded if it changes
          
    -p, --http-proxy &amp;lt;USER:PASS@HOST:PORT&amp;gt;
          If set, will use this http proxy to connect to the client

          [env: HTTP_PROXY=]

      --http-proxy-login &amp;lt;LOGIN&amp;gt;
          If set, will use this login to connect to the http proxy. Override the one from --http-proxy

          [env: WSTUNNEL_HTTP_PROXY_LOGIN=]

      --http-proxy-password &amp;lt;PASSWORD&amp;gt;
          If set, will use this password to connect to the http proxy. Override the one from --http-proxy

          [env: WSTUNNEL_HTTP_PROXY_PASSWORD=]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Release &lt;a name="release"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Static binaries are available in &lt;a href="https://github.com/erebe/wstunnel/releases"&gt;release section&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;docker image are available at &lt;a href="https://github.com/erebe/wstunnel/pkgs/container/wstunnel"&gt;https://github.com/erebe/wstunnel/pkgs/container/wstunnel&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/erebe/wstunnel:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples &lt;a name="examples"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#syntax"&gt;Understand command line syntax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#simple"&gt;Simplest one with socks5 - Good for browsing internet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#ssh"&gt;Proxy SSH&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#corporate"&gt;Bypass a corporate proxy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#wireguard"&gt;Proxy Wireguard traffic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#android"&gt;Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#tproxy"&gt;Proxy easily any traffic with transparent proxy (linux only)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#reverse"&gt;Reverse tunneling&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#secure"&gt;How to secure access of your wstunnel server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#http2"&gt;Use HTTP2 instead of websocket for transport protocol&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/erebe/wstunnel/main/#stealth"&gt;Maximize your stealthiness/Make your traffic discrete&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Understand command line syntax &lt;a name="syntax"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Wstunnel command line mimic ssh tunnel syntax. You can take reference to &lt;a href="https://iximiuz.com/en/posts/ssh-tunnels/"&gt;this article&lt;/a&gt;, or this diagram to understand &lt;img src="https://iximiuz.com/ssh-tunnels/ssh-tunnels.png" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Simplest one &lt;a name="simple"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;On your remote host, start the wstunnel's server by typing this command in your terminal&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wstunnel server wss://[::]:8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create a websocket server listening on any interface on port 8080. On the client side use this command to forward traffic through the websocket tunnel&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wstunnel client -L socks5://127.0.0.1:8888 --connection-min-idle 5 wss://myRemoteHost:8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will create a socks5 server listening on port 8888 of the loopback interface and will forward traffic dynamically. &lt;code&gt;connection-min-idle 10&lt;/code&gt; is going an optimization to create a pool of 10 connection connected to the server, to speed-up the establishement of new tunnels.&lt;/p&gt; 
&lt;p&gt;With firefox you can setup a proxy using this tunnel, by setting in networking preferences 127.0.0.1:8888 and selecting socks5 proxy. Be sure to check the option &lt;code&gt;Proxy DNS when using SOCKS v5&lt;/code&gt; for the server to resolve DNS name and not your local machine.&lt;/p&gt; 
&lt;p&gt;or with curl&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -x socks5h://127.0.0.1:8888 http://google.com/
#Please note h after the 5, it is to avoid curl resolving DNS name locally
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;As proxy command for SSH &lt;a name="ssh"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;You can specify &lt;code&gt;stdio&lt;/code&gt; as source port on the client side if you wish to use wstunnel as part of a proxy command for ssh&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ssh -o ProxyCommand="wstunnel client --log-lvl=off -L stdio://%h:%p ws://myRemoteHost:8080" my-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;When behind a corporate proxy &lt;a name="corporate"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;An other useful example is when you want to bypass an http proxy (a corporate proxy for example) The most reliable way to do it is to use wstunnel as described below&lt;/p&gt; 
&lt;p&gt;Start your wstunnel server with tls activated&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wstunnel server wss://[::]:443 --restrict-to 127.0.0.1:22
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The server will listen on any interface using port 443 (https) and restrict traffic to be forwarded only to the ssh daemon.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Be aware that the server will use self signed certificate with weak cryptographic algorithm. It was made in order to add the least possible overhead while still being compliant with tls.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Do not rely on wstunnel to protect your privacy, if it is one of your concerns, you should only forwards traffic that is already secure by design (ie: https or vpn traffic)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Now on the client side start the client with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wstunnel client -L tcp://9999:127.0.0.1:22 -p http://mycorporateproxy:8080 wss://myRemoteHost:443
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will start a tcp server on port 9999 that will contact the corporate proxy, negotiate a tls connection with the remote host and forward traffic to the ssh daemon on the remote host.&lt;/p&gt; 
&lt;p&gt;You may now access your server from your local machine on ssh by using&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ssh -p 9999 login@127.0.0.1
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Wireguard and wstunnel &lt;a name="wireguard"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;You can find a full &lt;a href="https://community.hetzner.com/tutorials/obfuscating-wireguard-using-wstunnel"&gt;tutorial&lt;/a&gt; that explain how to setup wstunnel and wireguard at &lt;a href="https://community.hetzner.com/tutorials/obfuscating-wireguard-using-wstunnel"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For a quick explanation see below.&lt;/p&gt; 
&lt;p&gt;You have a working wireguard client configuration called &lt;code&gt;wg0.conf&lt;/code&gt;. Let's say&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[Interface]
Address = 10.200.0.2/32, fd00:cafe::2/128
PrivateKey = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=

[Peer]
PublicKey = 9iicV7Stdl/U0RH1BNf3VvlVjaa4Eus6QPEfEz6cR0c=
AllowedIPs = 0.0.0.0/0, ::/0
Endpoint = my.server.com:51820
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start wstunnel server on my.server.com like this&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;wstunnel server --restrict-to localhost:51820 wss://[::]:443
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;on your local machine start the client like this&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;wstunnel client -L 'udp://51820:localhost:51820?timeout_sec=0' wss://my.server.com:443
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;change your wireguard client config to something&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[Interface]
Address = 10.200.0.2/32, fd00:cafe::2/128
PrivateKey = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=
# Replace by a dns your server has access to
dns = 8.8.8.8
# https://github.com/nitred/nr-wg-mtu-finder to find best mtu for you
MTU = 1400 

[Peer]
PublicKey = 9iicV7Stdl/U0RH1BNf3VvlVjaa4Eus6QPEfEz6cR0c=
AllowedIPs = 0.0.0.0/0, ::/0
# Should target port where wstunnel client is listenning to
Endpoint = localhost:51820
# Should not be necessary if you enable wstunnel client websocket ping
PersistentKeepalive = 20
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add a default route to your server, as your AllowedIps are catch-all, it is to avoid the traffic looping.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo ip route add ip.of.my.server.com dev eth0 via 192.168.0.1
# replace eth0 (interface) and 192.168.0.1 (router gateway) by the one given by `ip route get ip.of.my.server.com` 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;start your wireguard, and it should be working&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo wg-quick up wg0
ping 10.200.0.1 # ping another ip of your vpn network
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;FAQ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Disable default udp tunnel timeout that will auto-close it after 30sec. &lt;code&gt;i.e: udp://1212:127.0.0.1:5201?timeout_sec=0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;If you see some throughput issue, be sure to lower the MTU of your wireguard interface (you can do it via config file) to something like 1300 or you will endup fragmenting udp packet (due to overhead of other layer) which is always causing issues&lt;/li&gt; 
 &lt;li&gt;If wstunnel cannot connect to server while wireguard is on, be sure you have added a static route via your main gateway for the ip of wstunnel server. Else if you forward all the traffic without putting a static route, you will endup looping your traffic wireguard interface -&amp;gt; wstunnel client -&amp;gt; wireguard interface&lt;/li&gt; 
 &lt;li&gt;If you have trouble making it works on windows, please check this issue &lt;a href="https://github.com/erebe/wstunnel/issues/252"&gt;https://github.com/erebe/wstunnel/issues/252&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Android &lt;a name="android"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;You can use the android binary and use termux to run it on your phone.&lt;/p&gt; 
&lt;p&gt;If you want a guide regarding how to use wstunnel on Android, you can follow this &lt;a href="https://community.hetzner.com/tutorials/obfuscating-wireguard-using-wstunnel"&gt;guide&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Transparent proxy (linux only) &lt;a name="tproxy"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Transparent proxy allows to easily proxy any program. Start wstunnel with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo wstunnel client -L 'tproxy+tcp://1080' -L 'tproxy+udp://1080' wss://my.server.com:443
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;use this project to route traffic seamlessly &lt;a href="https://github.com/NOBLES5E/cproxy"&gt;https://github.com/NOBLES5E/cproxy&lt;/a&gt;. It works with any program&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cproxy --port 1080 --mode tproxy -- curl https://google.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can even start a new shell, were all your commands will be proxyfied&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cproxy --port 1080 --mode tproxy -- bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Reverse tunneling &lt;a name="reverse"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Start wstunnel with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo wstunnel client -R 'tcp://[::]:8000:localhost:8000' wss://my.server.com:443
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In another terminal, start a simple webserver on your local machine&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m http.server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From your my.server.com machine/network you can now do&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;How to secure the access of your wstunnel server &lt;a name="secure"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Generate a secret, let's say &lt;code&gt;h3GywpDrP6gJEdZ6xbJbZZVFmvFZDCa4KcRd&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Now start you server with the following command&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wstunnel server --restrict-http-upgrade-path-prefix h3GywpDrP6gJEdZ6xbJbZZVFmvFZDCa4KcRd  wss://[::]:443 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And start your client with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wstunnel client --http-upgrade-path-prefix h3GywpDrP6gJEdZ6xbJbZZVFmvFZDCa4KcRd ... wss://myRemoteHost
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now your wstunnel server, will only accept connection if the client specify the correct path prefix during the upgrade request.&lt;/p&gt; 
&lt;p&gt;If you need more customization, you can use a config file to specify specific rules with &lt;code&gt;--restrict-config&lt;/code&gt;. You can find examples of restriction rules &lt;a href="https://github.com/erebe/wstunnel/raw/main/restrictions.yaml"&gt;there&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Use HTTP2 instead of websocket for the transport protocol &lt;a name="http2"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Use this only if websocket is blocked by your firewall/proxy. Otherwise, it is less performant than websocket.&lt;/p&gt; 
&lt;p&gt;Start your wstunnel server as usual with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wstunnel server wss://[::]:8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On the client the only difference is to specify https:// instead of wss://&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wstunnel client -L socks5://127.0.0.1:8888 https://myRemoteHost:8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt; HTTP2 as transport protocol is harder to make it works because:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you are behind a (reverse) proxy/CDN they may buffer the whole request before forwarding it to the server. Cloudflare is doing that, and obviously, this is not going to work for tunneling traffic&lt;/li&gt; 
 &lt;li&gt;if you have wstunnel behind a reverse proxy, most of them (i.e: nginx) are going to turn http2 request into http1 This is not going to work, because http1 does not support streaming naturally&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The only way to make it works with HTTP2 is to have wstunnel server directly exposed to the internet without any reverse proxy in front of it&lt;/p&gt; 
&lt;p&gt;In addition, you may also want to play with the request headers (i.e: content-length and content-type) to make it looks like normal traffic to bypass your firewall/proxy. Some firewall may not like to see request with content-length not set, or with content-type set to application/octet-stream&lt;/p&gt; 
&lt;h3&gt;Maximize your stealthiness/Make your traffic discrete &lt;a name="stealth"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use wstunnel with TLS activated (wss://) and use your own certificate 
  &lt;ul&gt; 
   &lt;li&gt;Embedded certificate is self-signed and are the same for everyone, so can be easily fingerprinted/flagged&lt;/li&gt; 
   &lt;li&gt;Use valid certificate (i.e: with Let's Encrypt), self-signed certificate are suspicious&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Use a custom http path prefix (see &lt;code&gt;--http-upgrade-path-prefix&lt;/code&gt; option) 
  &lt;ul&gt; 
   &lt;li&gt;To avoid having the same url than every other wstunnel user&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Change your tls-sni-override to a domain is known to be allowed (i.e: google.com, baidu.com, etc...) 
  &lt;ul&gt; 
   &lt;li&gt;this will not work if your wstunnel server is behind a reverse proxy (i.e: Nginx, Cloudflare, HAProxy, ...)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark &lt;a name="bench"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/erebe/wstunnel/assets/854278/6e3580b0-c4f8-449e-881e-64d1df56b0ce" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;How to Build &lt;a name="build"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Install the Rust &lt;a href="https://www.rust-lang.org/tools/install"&gt;https://www.rust-lang.org/tools/install&lt;/a&gt; or if you are a believer&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and run those commands at the root of the project&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cargo build --package wstunnel-cli
target/debug/wstunnel ...
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>