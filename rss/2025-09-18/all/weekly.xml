<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Wed, 17 Sep 2025 02:36:27 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>ItzCrazyKns/Perplexica</title>
      <link>https://github.com/ItzCrazyKns/Perplexica</link>
      <description>&lt;p&gt;Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🚀 Perplexica - An AI-powered search engine 🔎 
 &lt;!-- omit in toc --&gt;&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.warp.dev/perplexica"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/775dd593-9b5f-40f1-bf48-479faff4c27b" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="https://www.warp.dev/perplexica"&gt;Warp, the AI Devtool that lives in your terminal&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://www.warp.dev/perplexica"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/26aArMy8tT"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/26aArMy8tT?style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-screenshot.png?" alt="preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#overview"&gt;Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#preview"&gt;Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#installation"&gt;Installation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#getting-started-with-docker-recommended"&gt;Getting Started with Docker (Recommended)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#non-docker-installation"&gt;Non-Docker Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#ollama-connection-errors"&gt;Ollama Connection Errors&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#using-as-a-search-engine"&gt;Using as a Search Engine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#using-perplexicas-api"&gt;Using Perplexica's API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#expose-perplexica-to-network"&gt;Expose Perplexica to a network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#one-click-deployment"&gt;One-Click Deployment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features"&gt;Upcoming Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#support-us"&gt;Support Us&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#donations"&gt;Donations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#help-and-support"&gt;Help and Support&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Perplexica is an open-source AI-powered searching tool or an AI-powered search engine that goes deep into the internet to find answers. Inspired by Perplexity AI, it's an open-source option that not just searches the web but understands your questions. It uses advanced machine learning algorithms like similarity searching and embeddings to refine results and provides clear answers with sources cited.&lt;/p&gt; 
&lt;p&gt;Using SearxNG to stay current and fully open source, Perplexica ensures you always get the most up-to-date information without compromising your privacy.&lt;/p&gt; 
&lt;p&gt;Want to know more about its architecture and how it works? You can read it &lt;a href="https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Preview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-preview.gif" alt="video-preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Local LLMs&lt;/strong&gt;: You can utilize local LLMs such as Qwen, DeepSeek, Llama, and Mistral.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two Main Modes:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Copilot Mode:&lt;/strong&gt; (In development) Boosts search by generating different queries to find more relevant internet sources. Like normal search instead of just using the context by SearxNG, it visits the top matches and tries to find relevant sources to the user's query directly from the page.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Normal Mode:&lt;/strong&gt; Processes your query and performs a web search.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Focus Modes:&lt;/strong&gt; Special modes to better answer specific types of questions. Perplexica currently has 6 focus modes: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All Mode:&lt;/strong&gt; Searches the entire web to find the best results.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Writing Assistant Mode:&lt;/strong&gt; Helpful for writing tasks that do not require searching the web.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Academic Search Mode:&lt;/strong&gt; Finds articles and papers, ideal for academic research.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;YouTube Search Mode:&lt;/strong&gt; Finds YouTube videos based on the search query.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Wolfram Alpha Search Mode:&lt;/strong&gt; Answers queries that need calculations or data analysis using Wolfram Alpha.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Reddit Search Mode:&lt;/strong&gt; Searches Reddit for discussions and opinions related to the query.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Current Information:&lt;/strong&gt; Some search tools might give you outdated info because they use data from crawling bots and convert them into embeddings and store them in a index. Unlike them, Perplexica uses SearxNG, a metasearch engine to get the results and rerank and get the most relevant source out of it, ensuring you always get the latest information without the overhead of daily data updates.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt;: Integrate Perplexica into your existing applications and make use of its capibilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It has many more features like image and video search. Some of the planned features are mentioned in &lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features"&gt;upcoming features&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;There are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.&lt;/p&gt; 
&lt;h3&gt;Getting Started with Docker (Recommended)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure Docker is installed and running on your system.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the Perplexica repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ItzCrazyKns/Perplexica.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;After cloning, navigate to the directory containing the project files.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt;. For Docker setups, you need only fill in the following fields:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OPENAI&lt;/code&gt;: Your OpenAI API key. &lt;strong&gt;You only need to fill this if you wish to use OpenAI's models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;CUSTOM_OPENAI&lt;/code&gt;: Your OpenAI-API-compliant local server URL, model name, and API key. You should run your local server with host set to &lt;code&gt;0.0.0.0&lt;/code&gt;, take note of which port number it is running on, and then use that port number to set &lt;code&gt;API_URL = http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. You must specify the model name, such as &lt;code&gt;MODEL_NAME = "unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_XL"&lt;/code&gt;. Finally, set &lt;code&gt;API_KEY&lt;/code&gt; to the appropriate value. If you have not defined an API key, just put anything you want in-between the quotation marks: &lt;code&gt;API_KEY = "whatever-you-want-but-not-blank"&lt;/code&gt; &lt;strong&gt;You only need to configure these settings if you want to use a local OpenAI-compliant server, such as Llama.cpp's &lt;a href="https://github.com/ggml-org/llama.cpp/raw/master/tools/server/README.md"&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OLLAMA&lt;/code&gt;: Your Ollama API URL. You should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Ollama on port 11434, use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Ollama's models instead of OpenAI's&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;GROQ&lt;/code&gt;: Your Groq API key. &lt;strong&gt;You only need to fill this if you wish to use Groq's hosted models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;ANTHROPIC&lt;/code&gt;: Your Anthropic API key. &lt;strong&gt;You only need to fill this if you wish to use Anthropic models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;Gemini&lt;/code&gt;: Your Gemini API key. &lt;strong&gt;You only need to fill this if you wish to use Google's models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;DEEPSEEK&lt;/code&gt;: Your Deepseek API key. &lt;strong&gt;Only needed if you want Deepseek models.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;AIMLAPI&lt;/code&gt;: Your AI/ML API key. &lt;strong&gt;Only needed if you want to use AI/ML API models and embeddings.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can change these after starting Perplexica from the settings dialog.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;SIMILARITY_MEASURE&lt;/code&gt;: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensure you are in the directory containing the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and execute:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Wait a few minutes for the setup to complete. You can access Perplexica at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt; in your web browser.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.&lt;/p&gt; 
&lt;h3&gt;Non-Docker Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install SearXNG and allow &lt;code&gt;JSON&lt;/code&gt; format in the SearXNG settings.&lt;/li&gt; 
 &lt;li&gt;Clone the repository and rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt; in the root directory. Ensure you complete all required fields in this file.&lt;/li&gt; 
 &lt;li&gt;After populating the configuration run &lt;code&gt;npm i&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install the dependencies and then execute &lt;code&gt;npm run build&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Finally, start the app by running &lt;code&gt;npm run start&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/installation"&gt;installation documentation&lt;/a&gt; for more information like updating, etc.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;h4&gt;Local OpenAI-API-Compliant Servers&lt;/h4&gt; 
&lt;p&gt;If Perplexica tells you that you haven't configured any chat model providers, ensure that:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Your server is running on &lt;code&gt;0.0.0.0&lt;/code&gt; (not &lt;code&gt;127.0.0.1&lt;/code&gt;) and on the same port you put in the API URL.&lt;/li&gt; 
 &lt;li&gt;You have specified the correct model name loaded by your local LLM server.&lt;/li&gt; 
 &lt;li&gt;You have specified the correct API key, or if one is not defined, you have put &lt;em&gt;something&lt;/em&gt; in the API key field and not left it empty.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Ollama Connection Errors&lt;/h4&gt; 
&lt;p&gt;If you're encountering an Ollama connection error, it is likely due to the backend being unable to connect to Ollama's API. To fix this issue you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check your Ollama API URL:&lt;/strong&gt; Ensure that the API URL is correctly set in the settings menu.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Update API URL Based on OS:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; Use &lt;code&gt;http://&amp;lt;private_ip_of_host&amp;gt;:11434&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Adjust the port number if you're using a different one.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux Users - Expose Ollama to Network:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;/etc/systemd/system/ollama.service&lt;/code&gt;, you need to add &lt;code&gt;Environment="OLLAMA_HOST=0.0.0.0:11434"&lt;/code&gt;. (Change the port number if you are using a different one.) Then reload the systemd manager configuration with &lt;code&gt;systemctl daemon-reload&lt;/code&gt;, and restart Ollama by &lt;code&gt;systemctl restart ollama&lt;/code&gt;. For more information see &lt;a href="https://github.com/ollama/ollama/raw/main/docs/faq.md#setting-environment-variables-on-linux"&gt;Ollama docs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Ensure that the port (default is 11434) is not blocked by your firewall.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using as a Search Engine&lt;/h2&gt; 
&lt;p&gt;If you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser's search bar, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open your browser's settings.&lt;/li&gt; 
 &lt;li&gt;Navigate to the 'Search Engines' section.&lt;/li&gt; 
 &lt;li&gt;Add a new site search with the following URL: &lt;code&gt;http://localhost:3000/?q=%s&lt;/code&gt;. Replace &lt;code&gt;localhost&lt;/code&gt; with your IP address or domain name, and &lt;code&gt;3000&lt;/code&gt; with the port number if Perplexica is not hosted locally.&lt;/li&gt; 
 &lt;li&gt;Click the add button. Now, you can use Perplexica directly from your browser's search bar.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using Perplexica's API&lt;/h2&gt; 
&lt;p&gt;Perplexica also provides an API for developers looking to integrate its powerful search engine into their own applications. You can run searches, use multiple models and get answers to your queries.&lt;/p&gt; 
&lt;p&gt;For more details, check out the full documentation &lt;a href="https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/API/SEARCH.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Expose Perplexica to network&lt;/h2&gt; 
&lt;p&gt;Perplexica runs on Next.js and handles all API requests. It works right away on the same network and stays accessible even with port forwarding.&lt;/p&gt; 
&lt;h2&gt;One-Click Deployment&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://usw.sealos.io/?openapp=system-template%3FtemplateName%3Dperplexica"&gt;&lt;img src="https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg?sanitize=true" alt="Deploy to Sealos" /&gt;&lt;/a&gt; &lt;a href="https://repocloud.io/details/?app_id=267"&gt;&lt;img src="https://d16t0pc4846x52.cloudfront.net/deploylobe.svg?sanitize=true" alt="Deploy to RepoCloud" /&gt;&lt;/a&gt; &lt;a href="https://template.run.claw.cloud/?referralCode=U11MRQ8U9RM4&amp;amp;openapp=system-fastdeploy%3FtemplateName%3Dperplexica"&gt;&lt;img src="https://raw.githubusercontent.com/ClawCloud/Run-Template/refs/heads/main/Run-on-ClawCloud.svg?sanitize=true" alt="Run on ClawCloud" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Upcoming Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add settings page&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Adding support for local LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; History Saving features&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Introducing various Focus Modes&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Adding API support&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Adding Discover&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Finalizing Copilot Mode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;If you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.&lt;/p&gt; 
&lt;h3&gt;Donations&lt;/h3&gt; 
&lt;p&gt;We also accept donations to help sustain our project. If you would like to contribute, you can use the following options to donate. Thank you for your support!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ethereum&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Address: &lt;code&gt;0xB025a84b2F269570Eb8D4b05DEdaA41D8525B6DD&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Perplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the &lt;a href="https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file to learn more about Perplexica and how you can contribute to it.&lt;/p&gt; 
&lt;h2&gt;Help and Support&lt;/h2&gt; 
&lt;p&gt;If you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. &lt;a href="https://discord.gg/EFwsmQDgAu"&gt;Click here&lt;/a&gt; to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at &lt;code&gt;itzcrazykns&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don't forget to check back for updates and new features!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelcontextprotocol/registry</title>
      <link>https://github.com/modelcontextprotocol/registry</link>
      <description>&lt;p&gt;A community driven registry service for Model Context Protocol (MCP) servers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Registry&lt;/h1&gt; 
&lt;p&gt;The MCP registry provides MCP clients with a list of MCP servers, like an app store for MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/guides/publishing/publish-server.md"&gt;&lt;strong&gt;📤 Publish my MCP server&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://registry.modelcontextprotocol.io/docs"&gt;&lt;strong&gt;⚡️ Live API docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/explanations/ecosystem-vision.md"&gt;&lt;strong&gt;👀 Ecosystem vision&lt;/strong&gt;&lt;/a&gt; | 📖 &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs"&gt;Full documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Development Status&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;2025-09-08 update&lt;/strong&gt;: The registry has launched in preview 🎉 (&lt;a href="https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/"&gt;announcement blog post&lt;/a&gt;). While the system is now more stable, this is still a preview release and breaking changes or data resets may occur. A general availability (GA) release will follow later. We'd love your feedback in &lt;a href="https://github.com/modelcontextprotocol/registry/discussions/new?category=ideas"&gt;GitHub discussions&lt;/a&gt; or in the &lt;a href="https://discord.com/channels/1358869848138059966/1369487942862504016"&gt;#registry-dev Discord&lt;/a&gt; (&lt;a href="https://modelcontextprotocol.io/community/communication"&gt;joining details here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Current key maintainers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Adam Jones&lt;/strong&gt; (Anthropic) &lt;a href="https://github.com/domdomegg"&gt;@domdomegg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tadas Antanavicius&lt;/strong&gt; (PulseMCP) &lt;a href="https://github.com/tadasant"&gt;@tadasant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toby Padilla&lt;/strong&gt; (GitHub) &lt;a href="https://github.com/toby"&gt;@toby&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We use multiple channels for collaboration - see &lt;a href="https://modelcontextprotocol.io/community/communication"&gt;modelcontextprotocol.io/community/communication&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Often (but not always) ideas flow through this pipeline:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://modelcontextprotocol.io/community/communication"&gt;Discord&lt;/a&gt;&lt;/strong&gt; - Real-time community discussions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/discussions"&gt;Discussions&lt;/a&gt;&lt;/strong&gt; - Propose and discuss product/technical requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/issues"&gt;Issues&lt;/a&gt;&lt;/strong&gt; - Track well-scoped technical work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/pulls"&gt;Pull Requests&lt;/a&gt;&lt;/strong&gt; - Contribute work towards issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick start:&lt;/h3&gt; 
&lt;h4&gt;Pre-requisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Go 1.24.x&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;golangci-lint v2.4.0&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running the server&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start full development environment
make dev-compose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This starts the registry at &lt;a href="http://localhost:8080"&gt;&lt;code&gt;localhost:8080&lt;/code&gt;&lt;/a&gt; with PostgreSQL and seed data. It can be configured with environment variables in &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; - see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/.env.example"&gt;.env.example&lt;/a&gt; for a reference.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Local setup without Docker&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;PostgreSQL running locally&lt;/li&gt; 
  &lt;li&gt;Go 1.24.x installed&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Build and run locally
make build
make dev-local
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The service runs on &lt;a href="http://localhost:8080"&gt;&lt;code&gt;localhost:8080&lt;/code&gt;&lt;/a&gt; by default. This can be configured with environment variables in &lt;code&gt;.env&lt;/code&gt; - see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/.env.example"&gt;.env.example&lt;/a&gt; for a reference.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Running a pre-built Docker image&lt;/summary&gt; 
 &lt;p&gt;Pre-built Docker images are automatically published to GitHub Container Registry:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Run latest stable release
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:latest

# Run latest from main branch (continuous deployment)
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main

# Run specific release version
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:v1.0.0

# Run development build from main branch
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main-20250906-abc123d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Available tags:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Releases&lt;/strong&gt;: &lt;code&gt;latest&lt;/code&gt;, &lt;code&gt;v1.0.0&lt;/code&gt;, &lt;code&gt;v1.1.0&lt;/code&gt;, etc.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Continuous&lt;/strong&gt;: &lt;code&gt;main&lt;/code&gt; (latest main branch build)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: &lt;code&gt;main-&amp;lt;date&amp;gt;-&amp;lt;sha&amp;gt;&lt;/code&gt; (specific commit builds)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h4&gt;Publishing a server&lt;/h4&gt; 
&lt;p&gt;To publish a server, we've built a simple CLI. You can use it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build the latest CLI
make publisher

# Use it!
./bin/mcp-publisher --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/guides/publishing/publish-server.md"&gt;the publisher guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h4&gt;Other commands&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run lint, unit tests and integration tests
make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are also a few more helpful commands for development. Run &lt;code&gt;make help&lt;/code&gt; to learn more, or look in &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/Makefile"&gt;Makefile&lt;/a&gt;.&lt;/p&gt; 
&lt;!--
For Claude and other AI tools: Always prefer make targets over custom commands where possible.
--&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Project Structure&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;├── cmd/                     # Application entry points
│   └── publisher/           # Server publishing tool
├── data/                    # Seed data
├── deploy/                  # Deployment configuration (Pulumi)
├── docs/                    # Documentation
├── internal/                # Private application code
│   ├── api/                 # HTTP handlers and routing
│   ├── auth/                # Authentication (GitHub OAuth, JWT, namespace blocking)
│   ├── config/              # Configuration management
│   ├── database/            # Data persistence (PostgreSQL, in-memory)
│   ├── service/             # Business logic
│   ├── telemetry/           # Metrics and monitoring
│   └── validators/          # Input validation
├── pkg/                     # Public packages
│   ├── api/                 # API types and structures
│   │   └── v0/              # Version 0 API types
│   └── model/               # Data models for server.json
├── scripts/                 # Development and testing scripts
├── tests/                   # Integration tests
└── tools/                   # CLI tools and utilities
    └── validate-*.sh        # Schema validation tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Authentication&lt;/h3&gt; 
&lt;p&gt;Publishing supports multiple authentication methods:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OAuth&lt;/strong&gt; - For publishing by logging into GitHub&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OIDC&lt;/strong&gt; - For publishing from GitHub Actions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DNS verification&lt;/strong&gt; - For proving ownership of a domain and its subdomains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP verification&lt;/strong&gt; - For proving ownership of a domain&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The registry validates namespace ownership when publishing. E.g. to publish...:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;io.github.domdomegg/my-cool-mcp&lt;/code&gt; you must login to GitHub as &lt;code&gt;domdomegg&lt;/code&gt;, or be in a GitHub Action on domdomegg's repos&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;me.adamjones/my-cool-mcp&lt;/code&gt; you must prove ownership of &lt;code&gt;adamjones.me&lt;/code&gt; via DNS or HTTP challenge&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;More documentation&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs"&gt;documentation&lt;/a&gt; for more details if your question has not been answered here!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PowerShell/PowerShell</title>
      <link>https://github.com/PowerShell/PowerShell</link>
      <description>&lt;p&gt;PowerShell for every system!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/PowerShell/PowerShell/master/assets/ps_black_64.svg?sanitize=true" alt="logo" /&gt; PowerShell&lt;/h1&gt; 
&lt;p&gt;Welcome to the PowerShell GitHub Community! &lt;a href="https://learn.microsoft.com/powershell/scripting/overview"&gt;PowerShell&lt;/a&gt; is a cross-platform (Windows, Linux, and macOS) automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language, and a framework for processing cmdlets.&lt;/p&gt; 
&lt;h2&gt;Windows PowerShell vs. PowerShell 7+&lt;/h2&gt; 
&lt;p&gt;Although this repository started as a fork of the Windows PowerShell codebase, changes made in this repository are not ported back to Windows PowerShell 5.1. This also means that &lt;a href="https://github.com/PowerShell/PowerShell/issues"&gt;issues tracked here&lt;/a&gt; are only for PowerShell 7.x and higher. Windows PowerShell specific issues should be reported with the &lt;a href="https://support.microsoft.com/windows/send-feedback-to-microsoft-with-the-feedback-hub-app-f59187f8-8739-22d6-ba93-f66612949332"&gt;Feedback Hub app&lt;/a&gt;, by choosing "Apps &amp;gt; PowerShell" in the category.&lt;/p&gt; 
&lt;h2&gt;New to PowerShell?&lt;/h2&gt; 
&lt;p&gt;If you are new to PowerShell and want to learn more, we recommend reviewing the &lt;a href="https://learn.microsoft.com/powershell/scripting/learn/more-powershell-learning"&gt;getting started&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;h2&gt;Get PowerShell&lt;/h2&gt; 
&lt;p&gt;PowerShell is supported on Windows, macOS, and a variety of Linux platforms. For more information, see &lt;a href="https://learn.microsoft.com/powershell/scripting/install/installing-powershell"&gt;Installing PowerShell&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Upgrading PowerShell&lt;/h2&gt; 
&lt;p&gt;For best results when upgrading, you should use the same install method you used when you first installed PowerShell. The update method is different for each platform and install method.&lt;/p&gt; 
&lt;h2&gt;Community Dashboard&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aka.ms/PSPublicDashboard"&gt;Dashboard&lt;/a&gt; with visualizations for community contributions and project status using PowerShell, Azure, and PowerBI.&lt;/p&gt; 
&lt;p&gt;For more information on how and why we built this dashboard, check out this &lt;a href="https://devblogs.microsoft.com/powershell/powershell-open-source-community-dashboard/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Discussions&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://docs.github.com/discussions/quickstart"&gt;GitHub Discussions&lt;/a&gt; is a feature to enable free and open discussions within the community for topics that are not related to code, unlike issues.&lt;/p&gt; 
&lt;p&gt;This is an experiment we are trying in our repositories, to see if it helps move discussions out of issues so that issues remain actionable by the team or members of the community. There should be no expectation that PowerShell team members are regular participants in these discussions. Individual PowerShell team members may choose to participate in discussions, but the expectation is that community members help drive discussions so that team members can focus on issues.&lt;/p&gt; 
&lt;p&gt;Create or join a &lt;a href="https://github.com/PowerShell/PowerShell/discussions"&gt;discussion&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Chat&lt;/h2&gt; 
&lt;p&gt;Want to chat with other members of the PowerShell community?&lt;/p&gt; 
&lt;p&gt;There are dozens of topic-specific channels on our community-driven PowerShell Virtual User Group, which you can join on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://gitter.im/PowerShell/PowerShell"&gt;Gitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/PowerShell"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://web.libera.chat/#powershell"&gt;IRC&lt;/a&gt; on Libera.Chat&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/psslack"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build status of nightly builds&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Azure CI (Windows)&lt;/th&gt; 
   &lt;th align="left"&gt;Azure CI (Linux)&lt;/th&gt; 
   &lt;th align="left"&gt;Azure CI (macOS)&lt;/th&gt; 
   &lt;th align="left"&gt;CodeFactor Grade&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://powershell.visualstudio.com/PowerShell/_build?definitionId=32"&gt;&lt;img src="https://powershell.visualstudio.com/PowerShell/_apis/build/status/PowerShell-CI-Windows-daily" alt="windows-nightly-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://powershell.visualstudio.com/PowerShell/_build?definitionId=23"&gt;&lt;img src="https://powershell.visualstudio.com/PowerShell/_apis/build/status/PowerShell-CI-linux-daily?branchName=master" alt="linux-nightly-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://powershell.visualstudio.com/PowerShell/_build?definitionId=24"&gt;&lt;img src="https://powershell.visualstudio.com/PowerShell/_apis/build/status/PowerShell-CI-macos-daily?branchName=master" alt="macOS-nightly-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.codefactor.io/repository/github/powershell/powershell"&gt;&lt;img src="https://www.codefactor.io/repository/github/powershell/powershell/badge" alt="cf-image" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Developing and Contributing&lt;/h2&gt; 
&lt;p&gt;Want to contribute to PowerShell? Please start with the &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; to learn how to develop and contribute.&lt;/p&gt; 
&lt;p&gt;If you are developing .NET Core C# applications targeting PowerShell Core, &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md#where-do-i-get-the-powershell-core-sdk-package"&gt;check out our FAQ&lt;/a&gt; to learn more about the PowerShell SDK NuGet package.&lt;/p&gt; 
&lt;p&gt;Also, make sure to check out our &lt;a href="https://github.com/powershell/powershell-rfc"&gt;PowerShell-RFC repository&lt;/a&gt; for request-for-comments (RFC) documents to submit and give comments on proposed and future designs.&lt;/p&gt; 
&lt;h2&gt;Building PowerShell&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Linux&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;macOS&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/building/linux.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/building/windows-core.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/building/macos.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;If you have any problems building PowerShell, please start by consulting the developer &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/docs/FAQ.md"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Downloading the Source Code&lt;/h2&gt; 
&lt;p&gt;You can clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/PowerShell/PowerShell.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information, see &lt;a href="https://github.com/PowerShell/PowerShell/tree/master/docs/git"&gt;working with the PowerShell repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;For support, see the &lt;a href="https://github.com/PowerShell/PowerShell/tree/master/.github/SUPPORT.md"&gt;Support Section&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Legal and Licensing&lt;/h2&gt; 
&lt;p&gt;PowerShell is licensed under the &lt;a href="https://github.com/PowerShell/PowerShell/tree/master/LICENSE.txt"&gt;MIT license&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker Containers&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] The PowerShell container images are now &lt;a href="https://github.com/PowerShell/Announcements/issues/75"&gt;maintained by the .NET team&lt;/a&gt;. The containers at &lt;code&gt;mcr.microsoft.com/powershell&lt;/code&gt; are currently not maintained.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;License: By requesting and using the Container OS Image for Windows containers, you acknowledge, understand, and consent to the Supplemental License Terms available on &lt;a href="https://mcr.microsoft.com/en-us/product/powershell/tags"&gt;Microsoft Artifact Registry&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Telemetry&lt;/h3&gt; 
&lt;p&gt;Please visit our &lt;a href="https://learn.microsoft.com/powershell/module/microsoft.powershell.core/about/about_telemetry"&gt;about_Telemetry&lt;/a&gt; topic to read details about telemetry gathered by PowerShell.&lt;/p&gt; 
&lt;h2&gt;Governance&lt;/h2&gt; 
&lt;p&gt;The governance policy for the PowerShell project is described the &lt;a href="https://github.com/PowerShell/PowerShell/raw/master/docs/community/governance.md"&gt;PowerShell Governance&lt;/a&gt; document.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Please see our &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; before participating in this project.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/SECURITY.md"&gt;Security Policy&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;For any security issues, please see our &lt;a href="https://raw.githubusercontent.com/PowerShell/PowerShell/master/.github/SECURITY.md"&gt;Security Policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Vanilagy/mediabunny</title>
      <link>https://github.com/Vanilagy/mediabunny</link>
      <description>&lt;p&gt;Pure TypeScript media toolkit for reading, writing, and converting video and audio files, directly in the browser.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mediabunny - JavaScript media toolkit&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.npmjs.com/package/mediabunny"&gt;&lt;img src="https://img.shields.io/npm/v/mediabunny" alt="" /&gt;&lt;/a&gt; &lt;a href="https://bundlephobia.com/package/mediabunny"&gt;&lt;img src="https://img.shields.io/bundlephobia/minzip/mediabunny" alt="" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/mediabunny"&gt;&lt;img src="https://img.shields.io/npm/dm/mediabunny" alt="" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/hmpkyYuS4U"&gt;&lt;img src="https://img.shields.io/discord/1390044844285497344?logo=discord&amp;amp;label=Discord" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/Vanilagy/mediabunny/main/docs/public/mediabunny-logo.svg?sanitize=true" width="180" height="180" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Mediabunny is a JavaScript library for reading, writing, and converting media files (like MP4, WebM, MP3), directly in the browser. It aims to be a complete toolkit for high-performance media operations on the web. It's written from scratch in pure TypeScript, has zero dependencies, is very performant, and is extremely tree-shakable, meaning you only include what you use. You can think of it a bit like &lt;a href="https://ffmpeg.org/"&gt;FFmpeg&lt;/a&gt;, but built from the ground up for the web.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://mediabunny.dev"&gt;Documentation&lt;/a&gt; | &lt;a href="https://mediabunny.dev/examples"&gt;Examples&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Vanilagy/mediabunny/main/#sponsoring"&gt;Sponsoring&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Vanilagy/mediabunny/main/#license"&gt;License&lt;/a&gt; | &lt;a href="https://discord.gg/hmpkyYuS4U"&gt;Discord&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Gold sponsors&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://remotion.dev/" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/Vanilagy/mediabunny/main/docs/public/sponsors/remotion.png" width="60" height="60" alt="Remotion" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
 &lt;a href="https://www.gling.ai/" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/Vanilagy/mediabunny/main/docs/public/sponsors/gling.svg?sanitize=true" width="60" height="60" alt="Gling AI" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
 &lt;a href="https://diffusion.studio/" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/Vanilagy/mediabunny/main/docs/public/sponsors/diffusionstudio.png" width="60" height="60" alt="Diffusion Studio" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
 &lt;a href="https://kino.ai/" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/Vanilagy/mediabunny/main/docs/public/sponsors/kino.jpg" width="60" height="60" alt="Kino" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Bronze sponsors&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.reactvideoeditor.com/" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/Vanilagy/mediabunny/main/docs/public/sponsors/rve.svg?sanitize=true" width="40" height="40" alt="React Video Editor" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/Vanilagy"&gt;Sponsor Mediabunny's development&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Core features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Wide format support&lt;/strong&gt;: Read and write MP4, MOV, WebM, MKV, WAVE, MP3, Ogg, ADTS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in encoding &amp;amp; decoding&lt;/strong&gt;: Supports 25+ video, audio, and subtitle codecs, hardware-accelerated using the WebCodecs API&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High precision&lt;/strong&gt;: Fine-grained, microsecond-accurate reading and writing operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversion API&lt;/strong&gt;: Easy-to-use API with features such as transmuxing, transcoding, resizing, rotation, cropping, resampling, trimming, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Streaming I/O&lt;/strong&gt;: Handle reading &amp;amp; writing files of any size with memory-efficient streaming&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tree-shakable&lt;/strong&gt;: Only bundle what you use (as small as 5 kB gzipped)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero dependencies&lt;/strong&gt;: Implemented in highly performant TypeScript&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt;: Works in browsers and Node.js&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://mediabunny.dev/guide/introduction#features"&gt;See full feature list&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Install it via npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install mediabunny
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, include it directly with a script tag using one of the &lt;a href="https://github.com/Vanilagy/mediabunny/releases"&gt;builds&lt;/a&gt;. Doing so exposes a global &lt;code&gt;Mediabunny&lt;/code&gt; object.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;script src="mediabunny.cjs"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Requires any JavaScript environment that can run ECMAScript 2021 or later. Mediabunny is expected to be run in modern browsers. For types, TypeScript 5.7 or later is required.&lt;/p&gt; 
&lt;h3&gt;Read file metadata&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { Input, ALL_FORMATS, BlobSource } from 'mediabunny';

const input = new Input({
    source: new BlobSource(file), // Reading from disk
    formats: ALL_FORMATS,
});

const duration = await input.computeDuration(); // in seconds
const videoTrack = await input.getPrimaryVideoTrack();
const audioTrack = await input.getPrimaryAudioTrack();
const { displayWidth, displayHeight, rotation } = videoTrack;
const { sampleRate, numberOfChannels } = audioTrack;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Create new media files&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { Output, Mp4OutputFormat, BufferTarget, CanvasSource, QUALITY_HIGH } from 'mediabunny';

const output = new Output({
    format: new Mp4OutputFormat(),
    target: new BufferTarget(), // Writing to memory
});

// Add a video track backed by a canvas element
const videoSource = new CanvasSource(canvas, {
    codec: 'avc',
    bitrate: QUALITY_HIGH,
});
output.addVideoTrack(videoSource);

await output.start();
// Add frames...
await output.finalize();

const buffer = output.target.buffer; // Final MP4 file
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert files&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { Input, Output, Conversion, ALL_FORMATS, BlobSource, WebMOutputFormat } from 'mediabunny';

const input = new Input({
    source: new BlobSource(file),
    formats: ALL_FORMATS,
});

const output = new Output({
    format: new WebMOutputFormat(), // Convert to WebM
    target: new BufferTarget(),
});

const conversion = await Conversion.init({ input, output });
await conversion.execute();
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://mediabunny.dev/guide/quick-start"&gt;See more code snippets&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit the &lt;a href="https://mediabunny.dev/guide/introduction"&gt;Docs&lt;/a&gt; for comprehensive guides, examples and API documentation.&lt;/p&gt; 
&lt;h2&gt;Sponsoring&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://mediabunny.dev/#sponsors"&gt;See all sponsors&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Mediabunny is an open-source project released under the &lt;a href="https://choosealicense.com/licenses/mpl-2.0/" target="_blank"&gt;MPL-2.0&lt;/a&gt; and is therefore free to use for any purpose, including closed-source commercial use. A permissive license is essential for a foundational library like this to truly thrive. That said, this project requires an immense amount of work and care to maintain and expand. This is made possible by the generous financial backing of the sponsors of this project.&lt;/p&gt; 
&lt;p&gt;If you have derived considerable value from this project, please consider &lt;a href="https://github.com/sponsors/Vanilagy"&gt;sponsoring it&lt;/a&gt; or providing a one-time donation. Thank you! 🩷&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://choosealicense.com/licenses/mpl-2.0/"&gt;Mozilla Public License 2.0&lt;/a&gt;. This is a very permissive weak copyleft license, not much different from the MIT License, allowing you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use Mediabunny for any purpose, commercial or non-commercial, without royalties&lt;/li&gt; 
 &lt;li&gt;Use Mediabunny in open- and closed-source projects&lt;/li&gt; 
 &lt;li&gt;Freely distribute projects built with Mediabunny&lt;/li&gt; 
 &lt;li&gt;Inspect and modify Mediabunny's source code&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;However, you have the following obligation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you modify Mediabunny's licensed source code (e.g. in a fork) and then distribute it, you must publicly publish your modifications under the Mozilla Public License 2.0.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This ensures that library usage remains permissive for everybody, while any improvements to Mediabunny remain in the open, benefiting everyone.&lt;/p&gt; 
&lt;p&gt;You are not allowed to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Remove the license and copyright headers from any Mediabunny source file&lt;/li&gt; 
 &lt;li&gt;Claim the "Mediabunny" trademark&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And finally, Mediabunny - like any other library - comes with no warranty of any kind and is not liable for any direct or indirect damages.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This is not legal advice. Refer to the full text of the &lt;a href="https://choosealicense.com/licenses/mpl-2.0/"&gt;Mozilla Public License 2.0&lt;/a&gt; for the binding license agreement.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Implementation &amp;amp; development&lt;/h2&gt; 
&lt;p&gt;Mediabunny is implemented from scratch in pure TypeScript with zero dependencies. At its core, the library is a collection of multiplexers and demultiplexers (one for every container format), which are then connected together via abstractions around the WebCodecs API. The logic is heavily pipelined and lazy, keeping performance high and memory usage low. If this stuff interests you, refer to the &lt;a href="https://mediabunny.dev/guide/introduction#technical-overview"&gt;Technical overview&lt;/a&gt; for more.&lt;/p&gt; 
&lt;p&gt;For development, clone this repository and install it using a modern version of Node.js and npm. The build system uses TypeScript, esbuild, API Extractor, Vite, and VitePress.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install # Install dependencies
npm run watch # Build bundles on watch mode

npm run build # Production build with type definitions

npm run check # Type checking
npm run lint # ESLint

npm run docs:generate # Generates API docs
npm run docs:dev # Start docs development server
npm run dev # Start examples development server

npm run docs:build # Build docs and examples
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/AutoAgent</title>
      <link>https://github.com/HKUDS/AutoAgent</link>
      <description>&lt;p&gt;"AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/AutoAgent_logo.svg?sanitize=true" alt="Logo" width="200" /&gt; 
 &lt;h1 align="center"&gt;AutoAgent: Fully-Automated &amp;amp; Zero-Code&lt;br /&gt; LLM Agent Framework &lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://autoagent-ai.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white" alt="Credits" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/metachain-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Slack community" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/jQJdXyDB"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Discord community" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/HKUDS/AutoAgent/raw/main/assets/autoagent-wechat.jpg"&gt;&lt;img src="https://img.shields.io/badge/Wechat-Join%20Us-green?logo=wechat&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Wechat community" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://autoagent-ai.github.io/docs"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2502.05957"&gt;&lt;img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Paper" /&gt;&lt;/a&gt; 
 &lt;a href="https://gaia-benchmark-leaderboard.hf.space/"&gt;&lt;img src="https://img.shields.io/badge/GAIA%20Benchmark-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge" alt="Evaluation Benchmark Score" /&gt;&lt;/a&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13954" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13954" alt="HKUDS%2FAutoAgent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to AutoAgent! AutoAgent is a &lt;strong&gt;Fully-Automated&lt;/strong&gt; and highly &lt;strong&gt;Self-Developing&lt;/strong&gt; framework that enables users to create and deploy LLM agents through &lt;strong&gt;Natural Language Alone&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;✨Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🏆 Top Performers on the GAIA Benchmark &lt;br /&gt;AutoAgent has delivering comparable performance to many &lt;strong&gt;Deep Research Agents&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;✨ Agent and Workflow Create with Ease &lt;br /&gt;AutoAgent leverages natural language to effortlessly build ready-to-use &lt;strong&gt;tools&lt;/strong&gt;, &lt;strong&gt;agents&lt;/strong&gt; and &lt;strong&gt;workflows&lt;/strong&gt; - no coding required.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;📚 Agentic-RAG with Native Self-Managing Vector Database &lt;br /&gt;AutoAgent equipped with a native self-managing vector database, outperforms industry-leading solutions like &lt;strong&gt;LangChain&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🌐 Universal LLM Support &lt;br /&gt;AutoAgent seamlessly integrates with &lt;strong&gt;A Wide Range&lt;/strong&gt; of LLMs (e.g., OpenAI, Anthropic, Deepseek, vLLM, Grok, Huggingface ...)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🔀 Flexible Interaction &lt;br /&gt;Benefit from support for both &lt;strong&gt;function-calling&lt;/strong&gt; and &lt;strong&gt;ReAct&lt;/strong&gt; interaction modes.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🤖 Dynamic, Extensible, Lightweight &lt;br /&gt;AutoAgent is your &lt;strong&gt;Personal AI Assistant&lt;/strong&gt;, designed to be dynamic, extensible, customized, and lightweight.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;🚀 Unlock the Future of LLM Agents. Try 🔥AutoAgent🔥 Now!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AutoAgentnew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/autoagent-intro.svg?sanitize=true" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Quick Overview of AutoAgent.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;h2&gt;🔥 News&lt;/h2&gt; 
&lt;div class="scrollable"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Feb 17]&lt;/strong&gt;: &amp;nbsp;🎉🎉We've updated and released AutoAgent v0.2.0 (formerly known as MetaChain). Detailed changes include: 1) fix the bug of different LLM providers from issues; 2) add automatic installation of AutoAgent in the container environment according to issues; 3) add more easy-to-use commands for the CLI mode. 4) Rename the project to AutoAgent for better understanding.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Feb 10]&lt;/strong&gt;: &amp;nbsp;🎉🎉We've released &lt;b&gt;MetaChain!&lt;/b&gt;, including framework, evaluation codes and CLI mode! Check our &lt;a href="https://arxiv.org/abs/2502.05957"&gt;paper&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;span id="table-of-contents"&gt;&lt;/span&gt; 
&lt;h2&gt;📑 Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#features"&gt;✨ Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#news"&gt;🔥 News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#how-to-use"&gt;🔍 How to Use AutoAgent&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#user-mode"&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA 🏆 Open Deep Research)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#agent-editor"&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#workflow-editor"&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#quick-start"&gt;⚡ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#api-keys-setup"&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#start-with-cli-mode"&gt;Start with CLI Mode&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#todo"&gt;☑️ Todo List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#reproduce"&gt;🔬 How To Reproduce the Results in the Paper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#documentation"&gt;📖 Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#community"&gt;🤝 Join the Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#acknowledgements"&gt;🙏 Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#cite"&gt;🌟 Cite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="how-to-use"&gt;&lt;/span&gt; 
&lt;h2&gt;🔍 How to Use AutoAgent&lt;/h2&gt; 
&lt;span id="user-mode"&gt;&lt;/span&gt; 
&lt;h3&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA 🏆 Open Deep Research)&lt;/h3&gt; 
&lt;p&gt;AutoAgent have an out-of-the-box multi-agent system, which you could choose &lt;code&gt;user mode&lt;/code&gt; in the start page to use it. This multi-agent system is a general AI assistant, having the same functionality with &lt;strong&gt;OpenAI's Deep Research&lt;/strong&gt; and the comparable performance with it in &lt;a href="https://gaia-benchmark-leaderboard.hf.space/"&gt;GAIA&lt;/a&gt; benchmark.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🚀 &lt;strong&gt;High Performance&lt;/strong&gt;: Matches Deep Research using Claude 3.5 rather than OpenAI's o3 model.&lt;/li&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;Model Flexibility&lt;/strong&gt;: Compatible with any LLM (including Deepseek-R1, Grok, Gemini, etc.)&lt;/li&gt; 
 &lt;li&gt;💰 &lt;strong&gt;Cost-Effective&lt;/strong&gt;: Open-source alternative to Deep Research's $200/month subscription&lt;/li&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;User-Friendly&lt;/strong&gt;: Easy-to-deploy CLI interface for seamless interaction&lt;/li&gt; 
 &lt;li&gt;📁 &lt;strong&gt;File Support&lt;/strong&gt;: Handles file uploads for enhanced data interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;video width="80%" controls&gt; 
  &lt;source src="./assets/video_v1_compressed.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; 
 &lt;p&gt;&lt;em&gt;🎥 Deep Research (aka User Mode)&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;span id="agent-editor"&gt;&lt;/span&gt; 
&lt;h3&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/h3&gt; 
&lt;p&gt;The most distinctive feature of AutoAgent is its natural language customization capability. Unlike other agent frameworks, AutoAgent allows you to create tools, agents, and workflows using natural language alone. Simply choose &lt;code&gt;agent editor&lt;/code&gt; or &lt;code&gt;workflow editor&lt;/code&gt; mode to start your journey of building agents through conversations.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;agent editor&lt;/code&gt; as shown in the following figure.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/1-requirement.png" alt="requirement" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what kind of agent you want to create.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/2-profiling.png" alt="profiling" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Automated agent profiling.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/3-profiles.png" alt="profiles" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Output the agent profiles.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/4-tools.png" alt="tools" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired tools.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/5-task.png" alt="task" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what do you want to complete with the agent. (Optional)&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/6-output-next.png" alt="output" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired agent(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="workflow-editor"&gt;&lt;/span&gt; 
&lt;h3&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/h3&gt; 
&lt;p&gt;You can also create the agent workflows using natural language description with the &lt;code&gt;workflow editor&lt;/code&gt; mode, as shown in the following figure. (Tips: this mode does not support tool creation temporarily.)&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/1-requirement.png" alt="requirement" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what kind of workflow you want to create.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/2-profiling.png" alt="profiling" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Automated workflow profiling.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/3-profiles.png" alt="profiles" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Output the workflow profiles.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/4-task.png" alt="task" width="66%" /&gt; &lt;br /&gt; &lt;em&gt;Input what do you want to complete with the workflow. (Optional)&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/5-output-next.png" alt="output" width="66%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired workflow(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="quick-start"&gt;&lt;/span&gt; 
&lt;h2&gt;⚡ Quick Start&lt;/h2&gt; 
&lt;span id="installation"&gt;&lt;/span&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;AutoAgent Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/AutoAgent.git
cd AutoAgent
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Installation&lt;/h4&gt; 
&lt;p&gt;We use Docker to containerize the agent-interactive environment. So please install &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; first. You don't need to manually pull the pre-built image, because we have let Auto-Deep-Research &lt;strong&gt;automatically pull the pre-built image based on your architecture of your machine&lt;/strong&gt;.&lt;/p&gt; 
&lt;span id="api-keys-setup"&gt;&lt;/span&gt; 
&lt;h3&gt;API Keys Setup&lt;/h3&gt; 
&lt;p&gt;Create an environment variable file, just like &lt;code&gt;.env.template&lt;/code&gt;, and set the API keys for the LLMs you want to use. Not every LLM API Key is required, use what you need.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Required Github Tokens of your own
GITHUB_AI_TOKEN=

# Optional API Keys
OPENAI_API_KEY=
DEEPSEEK_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
HUGGINGFACE_API_KEY=
GROQ_API_KEY=
XAI_API_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="start-with-cli-mode"&gt;&lt;/span&gt; 
&lt;h3&gt;Start with CLI Mode&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[🚨 &lt;strong&gt;News&lt;/strong&gt;: ] We have updated a more easy-to-use command to start the CLI mode and fix the bug of different LLM providers from issues. You can follow the following steps to start the CLI mode with different LLM providers with much less configuration.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Command Options:&lt;/h4&gt; 
&lt;p&gt;You can run &lt;code&gt;auto main&lt;/code&gt; to start full part of AutoAgent, including &lt;code&gt;user mode&lt;/code&gt;, &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt;. Btw, you can also run &lt;code&gt;auto deep-research&lt;/code&gt; to start more lightweight &lt;code&gt;user mode&lt;/code&gt;, just like the &lt;a href="https://github.com/HKUDS/Auto-Deep-Research"&gt;Auto-Deep-Research&lt;/a&gt; project. Some configuration of this command is shown below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--container_name&lt;/code&gt;: Name of the Docker container (default: 'deepresearch')&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;: Port for the container (default: 12346)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;COMPLETION_MODEL&lt;/code&gt;: Specify the LLM model to use, you should follow the name of &lt;a href="https://github.com/BerriAI/litellm"&gt;Litellm&lt;/a&gt; to set the model name. (Default: &lt;code&gt;claude-3-5-sonnet-20241022&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DEBUG&lt;/code&gt;: Enable debug mode for detailed logs (default: False)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;API_BASE_URL&lt;/code&gt;: The base URL for the LLM provider (default: None)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;FN_CALL&lt;/code&gt;: Enable function calling (default: None). Most of time, you could ignore this option because we have already set the default value based on the model name.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;git_clone&lt;/code&gt;: Clone the AutoAgent repository to the local environment (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: True)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;test_pull_name&lt;/code&gt;: The name of the test pull. (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: 'autoagent_mirror')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;More details about &lt;code&gt;git_clone&lt;/code&gt; and &lt;code&gt;test_pull_name&lt;/code&gt;]&lt;/h4&gt; 
&lt;p&gt;In the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, we should clone a mirror of the AutoAgent repository to the local agent-interactive environment and let our &lt;strong&gt;AutoAgent&lt;/strong&gt; automatically update the AutoAgent itself, such as creating new tools, agents and workflows. So if you want to use the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, you should set the &lt;code&gt;git_clone&lt;/code&gt; to True and set the &lt;code&gt;test_pull_name&lt;/code&gt; to 'autoagent_mirror' or other branches.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;auto main&lt;/code&gt; with different LLM Providers&lt;/h4&gt; 
&lt;p&gt;Then I will show you how to use the full part of AutoAgent with the &lt;code&gt;auto main&lt;/code&gt; command and different LLM providers. If you want to use the &lt;code&gt;auto deep-research&lt;/code&gt; command, you can refer to the &lt;a href="https://github.com/HKUDS/Auto-Deep-Research"&gt;Auto-Deep-Research&lt;/a&gt; project for more details.&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ANTHROPIC_API_KEY=your_anthropic_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;auto main # default model is claude-3-5-sonnet-20241022
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_openai_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=gpt-4o auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Mistral&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;MISTRAL_API_KEY=your_mistral_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=mistral/mistral-large-2407 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Gemini - Google AI Studio&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GEMINI_API_KEY=your_gemini_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=gemini/gemini-2.0-flash auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Huggingface&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;HUGGINGFACE_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;HUGGINGFACE_API_KEY=your_huggingface_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=huggingface/meta-llama/Llama-3.3-70B-Instruct auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Groq&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GROQ_API_KEY=your_groq_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=groq/deepseek-r1-distill-llama-70b auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenAI-Compatible Endpoints (e.g., Grok)&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_api_key_for_openai_compatible_endpoints
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=openai/grok-2-latest API_BASE_URL=https://api.x.ai/v1 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenRouter (e.g., DeepSeek-R1)&lt;/h5&gt; 
&lt;p&gt;We recommend using OpenRouter as LLM provider of DeepSeek-R1 temporarily. Because official API of DeepSeek-R1 can not be used efficiently.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENROUTER_API_KEY=your_openrouter_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=openrouter/deepseek/deepseek-r1 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;DeepSeek&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;DEEPSEEK_API_KEY=your_deepseek_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=deepseek/deepseek-chat auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After the CLI mode is started, you can see the start page of AutoAgent:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AutoAgentnew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/cover.png" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Start Page of AutoAgent.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;h3&gt;Tips&lt;/h3&gt; 
&lt;h4&gt;Import browser cookies to browser environment&lt;/h4&gt; 
&lt;p&gt;You can import the browser cookies to the browser environment to let the agent better access some specific websites. For more details, please refer to the &lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/AutoAgent/environment/cookie_json/README.md"&gt;cookies&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h4&gt;Add your own API keys for third-party Tool Platforms&lt;/h4&gt; 
&lt;p&gt;If you want to create tools from the third-party tool platforms, such as RapidAPI, you should subscribe tools from the platform and add your own API keys by running &lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/process_tool_docs.py"&gt;process_tool_docs.py&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python process_tool_docs.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More features coming soon! 🚀 &lt;strong&gt;Web GUI interface&lt;/strong&gt; under development.&lt;/p&gt; 
&lt;span id="todo"&gt;&lt;/span&gt; 
&lt;h2&gt;☑️ Todo List&lt;/h2&gt; 
&lt;p&gt;AutoAgent is continuously evolving! Here's what's coming:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📊 &lt;strong&gt;More Benchmarks&lt;/strong&gt;: Expanding evaluations to &lt;strong&gt;SWE-bench&lt;/strong&gt;, &lt;strong&gt;WebArena&lt;/strong&gt;, and more&lt;/li&gt; 
 &lt;li&gt;🖥️ &lt;strong&gt;GUI Agent&lt;/strong&gt;: Supporting &lt;em&gt;Computer-Use&lt;/em&gt; agents with GUI interaction&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Tool Platforms&lt;/strong&gt;: Integration with more platforms like &lt;strong&gt;Composio&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;🏗️ &lt;strong&gt;Code Sandboxes&lt;/strong&gt;: Supporting additional environments like &lt;strong&gt;E2B&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;🎨 &lt;strong&gt;Web Interface&lt;/strong&gt;: Developing comprehensive GUI for better user experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Have ideas or suggestions? Feel free to open an issue! Stay tuned for more exciting updates! 🚀&lt;/p&gt; 
&lt;span id="reproduce"&gt;&lt;/span&gt; 
&lt;h2&gt;🔬 How To Reproduce the Results in the Paper&lt;/h2&gt; 
&lt;h3&gt;GAIA Benchmark&lt;/h3&gt; 
&lt;p&gt;For the GAIA benchmark, you can run the following command to run the inference.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/gaia/scripts/run_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the evaluation, you can run the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; python evaluation/gaia/get_score.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Agentic-RAG&lt;/h3&gt; 
&lt;p&gt;For the Agentic-RAG task, you can run the following command to run the inference.&lt;/p&gt; 
&lt;p&gt;Step1. Turn to &lt;a href="https://huggingface.co/datasets/yixuantt/MultiHopRAG"&gt;this page&lt;/a&gt; and download it. Save them to your datapath.&lt;/p&gt; 
&lt;p&gt;Step2. Run the following command to run the inference.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/multihoprag/scripts/run_rag.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Step3. The result will be saved in the &lt;code&gt;evaluation/multihoprag/result.json&lt;/code&gt;.&lt;/p&gt; 
&lt;span id="documentation"&gt;&lt;/span&gt; 
&lt;h2&gt;📖 Documentation&lt;/h2&gt; 
&lt;p&gt;A more detailed documentation is coming soon 🚀, and we will update in the &lt;a href="https://AutoAgent-ai.github.io/docs"&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; 
&lt;span id="community"&gt;&lt;/span&gt; 
&lt;h2&gt;🤝 Join the Community&lt;/h2&gt; 
&lt;p&gt;We want to build a community for AutoAgent, and we welcome everyone to join us. You can join our community by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/AutoAgent-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ"&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/z68KRvwB"&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/issues"&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we're working on, or add your own ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="acknowledgements"&gt;&lt;/span&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/stargazers"&gt;&lt;img src="https://reporoster.com/stars/HKUDS/AutoAgent" alt="Stargazers repo roster for @HKUDS/AutoAgent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/network/members"&gt;&lt;img src="https://reporoster.com/forks/HKUDS/AutoAgent" alt="Forkers repo roster for @HKUDS/AutoAgent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#HKUDS/AutoAgent&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=HKUDS/AutoAgent&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;🙏 Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Rome wasn't built in a day. AutoAgent stands on the shoulders of giants, and we are deeply grateful for the outstanding work that came before us. Our framework architecture draws inspiration from &lt;a href="https://github.com/openai/swarm"&gt;OpenAI Swarm&lt;/a&gt;, while our user mode's three-agent design benefits from &lt;a href="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one"&gt;Magentic-one&lt;/a&gt;'s insights. We've also learned from &lt;a href="https://github.com/All-Hands-AI/OpenHands"&gt;OpenHands&lt;/a&gt; for documentation structure and many other excellent projects for agent-environment interaction design, among others. We express our sincere gratitude and respect to all these pioneering works that have been instrumental in shaping AutoAgent.&lt;/p&gt; 
&lt;span id="cite"&gt;&lt;/span&gt; 
&lt;h2&gt;🌟 Cite&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;@misc{AutoAgent,
      title={{AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents}},
      author={Jiabin Tang, Tianyu Fan, Chao Huang},
      year={2025},
      eprint={202502.05957},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.05957},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>vercel/examples</title>
      <link>https://github.com/vercel/examples</link>
      <description>&lt;p&gt;Enjoy our curated collection of examples and solutions. Use these patterns to build your own robust and scalable applications.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://vercel.com"&gt; &lt;img src="https://assets.vercel.com/image/upload/v1588805858/repositories/vercel/logo.png" height="96" /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;h3 align="center"&gt;&lt;a href="https://vercel.com"&gt;Vercel Examples&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://vercel.com"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/vercel/examples/main/solutions"&gt;Solutions&lt;/a&gt; – Demos, reference architecture, and best practices&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/vercel/examples/main/starter"&gt;Starter&lt;/a&gt; – Functional applications which can act as a starting point&lt;/li&gt; 
 &lt;li&gt;And more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Vercel Templates&lt;/h2&gt; 
&lt;p&gt;Multiple examples are being featured in &lt;a href="https://vercel.com/templates"&gt;Vercel's Templates&lt;/a&gt;, visit that page for more advanced filtering options.&lt;/p&gt; 
&lt;h3&gt;For Vercelians&lt;/h3&gt; 
&lt;p&gt;Examples that have front matter metadata will create a new Draft template in &lt;a href="https://app.contentful.com"&gt;Contentful&lt;/a&gt;, for more steps on how to publish a template, read &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/internal/publishing-templates.md"&gt;Publishing Templates&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Adding a new example&lt;/h2&gt; 
&lt;p&gt;To quickly start contributing with a new example, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm i
pnpm new-example
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the script above isn't used, make sure the example complies with the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It must have a &lt;code&gt;.gitignore&lt;/code&gt; similar to &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example/.gitignore"&gt;plop-templates/example/.gitignore&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;It must have a &lt;code&gt;package.json&lt;/code&gt; similar to &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example/package.json"&gt;plop-templates/example/package.json&lt;/a&gt; (usage of Next.js is optional). The license should be &lt;code&gt;MIT&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;It must have a &lt;code&gt;README.md&lt;/code&gt; similar to &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example/README.md"&gt;plop-templates/example/README.md&lt;/a&gt;. The example has to be able to include a demo URL (the Vercel team will deploy it!) and if it requires environment variables, it must have a &lt;code&gt;.env.example&lt;/code&gt; file and instructions on how to set them up. Take &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/edge-middleware/bot-protection-datadome/README.md"&gt;bot-protection-datadome&lt;/a&gt; as an example. 
  &lt;ul&gt; 
   &lt;li&gt;To customize the Vercel Deploy Button take a look at the &lt;a href="https://vercel.com/docs/deploy-button"&gt;docs&lt;/a&gt;, useful if the deployment has required environment variables.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If using Next.js, it must have a &lt;code&gt;.eslintrc.json&lt;/code&gt; similar to &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example/.eslintrc.json"&gt;plop-templates/example/.eslintrc.json&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All Next.js examples should be using the same styling and layout provided by &lt;code&gt;@vercel/examples-ui&lt;/code&gt;, its usage can be seen in the &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/plop-templates/example"&gt;plop template&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Adding a template&lt;/h3&gt; 
&lt;p&gt;If you would like the example to be featured in &lt;a href="https://vercel.com/templates"&gt;vercel.com/templates&lt;/a&gt; then also add the front matter metadata to the top of the readme, like in &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/edge-middleware/bot-protection-datadome/README.md"&gt;bot-protection-datadome&lt;/a&gt;. To know all the possible values for each metadata take a look at &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/internal/fields.json"&gt;&lt;code&gt;internal/fields.json&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to add related templates to your template, copy the &lt;code&gt;slug&lt;/code&gt; from the other template into the &lt;code&gt;relatedTemplates&lt;/code&gt; field, for example for &lt;a href="https://vercel.com/templates/next.js/monorepo-turborepo"&gt;vercel.com/templates/next.js/monorepo-turborepo&lt;/a&gt; the slug is &lt;code&gt;monorepo-turborepo&lt;/code&gt;, as written in &lt;a href="https://raw.githubusercontent.com/vercel/examples/main/solutions/monorepo/README.md"&gt;solutions/monorepo/README.md&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;The pre-commit hook&lt;/h3&gt; 
&lt;p&gt;We use &lt;a href="https://typicode.github.io/husky/#/"&gt;Husky&lt;/a&gt; to manage the pre-commit &lt;a href="https://git-scm.com/docs/githooks"&gt;Git hook&lt;/a&gt; in this repo. Husky configures hooks automatically during install, so you don't need to do anything special to get them working, but if it fails to install, you can run the following command to install it manually:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm run prepare
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Code changes automatically go through Prettier and ESLint when you make a commit, &lt;strong&gt;please do not skip these steps&lt;/strong&gt; unless they're broken and in that case let us known by creating an issue.&lt;/p&gt; 
&lt;h2&gt;Read the Docs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://vercel.com/docs"&gt;Vercel Docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nextjs.org/docs"&gt;Next.js Docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you have any questions or suggestions about the docs, feel free to &lt;a href="https://github.com/vercel/examples/discussions"&gt;open a discussion&lt;/a&gt;, or &lt;a href="https://github.com/vercel/examples/pulls"&gt;submit a PR&lt;/a&gt; with your suggestions!&lt;/p&gt; 
&lt;h2&gt;Provide Feedback&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vercel/examples/discussions"&gt;Start a Discussion&lt;/a&gt; with a question, piece of feedback, or idea you want to share with the team.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vercel/examples/issues"&gt;Open an Issue&lt;/a&gt; if you believe you've encountered a bug that you want to flag for the team.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>ntdevlabs/tiny11builder</title>
      <link>https://github.com/ntdevlabs/tiny11builder</link>
      <description>&lt;p&gt;Scripts to build a trimmed-down Windows 11 image.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tiny11builder&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Scripts to build a trimmed-down Windows 11 image - now in &lt;strong&gt;PowerShell&lt;/strong&gt;!&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction :&lt;/h2&gt; 
&lt;p&gt;Tiny11 builder, now completely overhauled. &lt;br /&gt; After more than a year (for which I am so sorry) of no updates, tiny11 builder is now a much more complete and flexible solution - one script fits all. Also, it is a steppingstone for an even more fleshed-out solution.&lt;/p&gt; 
&lt;p&gt;You can now use it on ANY Windows 11 release (not just a specific build), as well as ANY language or architecture. This is made possible thanks to the much-improved scripting capabilities of PowerShell, compared to the older Batch release.&lt;/p&gt; 
&lt;p&gt;This is a script created to automate the build of a streamlined Windows 11 image, similar to tiny10. The script has also been updated to use DISM's recovery compression, resulting in a much smaller final ISO size, and no utilities from external sources. The only other executable included is &lt;strong&gt;oscdimg.exe&lt;/strong&gt;, which is provided in the Windows ADK and it is used to create bootable ISO images. Also included is an unattended answer file, which is used to bypass the Microsoft Account on OOBE and to deploy the image with the &lt;code&gt;/compact&lt;/code&gt; flag. It's open-source, &lt;strong&gt;so feel free to add or remove anything you want!&lt;/strong&gt; Feedback is also much appreciated.&lt;/p&gt; 
&lt;p&gt;Also, for the very first time, &lt;strong&gt;introducing tiny11 core builder&lt;/strong&gt;! A more powerful script, designed for a quick and dirty development testbed. Just the bare minimum, none of the fluff. This script generates a significantly reduced Windows 11 image. However, &lt;strong&gt;it's not suitable for regular use due to its lack of serviceability - you can't add languages, updates, or features post-creation&lt;/strong&gt;. tiny11 Core is not a full Windows 11 substitute but a rapid testing or development tool, potentially useful for VM environments.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⚠️ Script versions:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;tiny11maker.ps1&lt;/strong&gt; : The regular script, which removes a lot of bloat but keeps the system serviceable. You can add languages, updates, and features post-creation. This is the recommended script for regular use.&lt;/li&gt; 
 &lt;li&gt;⚠️ &lt;strong&gt;tiny11coremaker.ps1&lt;/strong&gt; : The core script, which removes even more bloat but also removes the ability to service the image. You cannot add languages, updates, or features post-creation. This is recommended for quick testing or development use.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Instructions:&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download Windows 11 from the &lt;a href="https://www.microsoft.com/software-download/windows11"&gt;Microsoft website&lt;/a&gt; or &lt;a href="https://github.com/pbatard/rufus"&gt;Rufus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mount the downloaded ISO image using Windows Explorer.&lt;/li&gt; 
 &lt;li&gt;Open &lt;strong&gt;PowerShell 5.1&lt;/strong&gt; as Administrator.&lt;/li&gt; 
 &lt;li&gt;Change the script execution policy :&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;Set-ExecutionPolicy Bypass -Scope Process
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Using &lt;code&gt;-Scope Process&lt;/code&gt; you keep your original policy intact as this change only lasts for the current PowerShell session.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;Start the script :&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;C:/path/to/your/tiny11/script.ps1 -ISO &amp;lt;letter&amp;gt; -SCRATCH &amp;lt;letter&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can see of the script by running the &lt;code&gt;get-help&lt;/code&gt; command.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;Select the drive letter where the image is mounted (only the letter, no colon (:))&lt;/li&gt; 
 &lt;li&gt;Select the SKU that you want the image to be based.&lt;/li&gt; 
 &lt;li&gt;Sit back and relax :)&lt;/li&gt; 
 &lt;li&gt;When the image is completed, you will see it in the folder where the script was extracted, with the name tiny11.iso&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;What is removed:&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tiny11maker&lt;/th&gt; 
   &lt;th&gt;Tiny11coremaker&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Clipchamp&lt;/li&gt; 
     &lt;li&gt;News&lt;/li&gt; 
     &lt;li&gt;Weather&lt;/li&gt; 
     &lt;li&gt;Xbox&lt;/li&gt; 
     &lt;li&gt;GetHelp&lt;/li&gt; 
     &lt;li&gt;GetStarted&lt;/li&gt; 
     &lt;li&gt;Office Hub&lt;/li&gt; 
     &lt;li&gt;Solitaire&lt;/li&gt; 
     &lt;li&gt;PeopleApp&lt;/li&gt; 
     &lt;li&gt;PowerAutomate&lt;/li&gt; 
     &lt;li&gt;ToDo&lt;/li&gt; 
     &lt;li&gt;Alarms&lt;/li&gt; 
     &lt;li&gt;Mail and Calendar&lt;/li&gt; 
     &lt;li&gt;Feedback Hub&lt;/li&gt; 
     &lt;li&gt;Maps&lt;/li&gt; 
     &lt;li&gt;Sound Recorder&lt;/li&gt; 
     &lt;li&gt;Your Phone&lt;/li&gt; 
     &lt;li&gt;Media Player&lt;/li&gt; 
     &lt;li&gt;QuickAssist&lt;/li&gt; 
     &lt;li&gt;Internet Explorer&lt;/li&gt; 
     &lt;li&gt;Tablet PC Math&lt;/li&gt; 
     &lt;li&gt;Edge&lt;/li&gt; 
     &lt;li&gt;OneDrive&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;all from regular tiny +&lt;/li&gt; 
     &lt;li&gt;Windows Component Store (WinSxS)&lt;/li&gt; 
     &lt;li&gt;Windows Defender (only disabled, can be enabled back if needed)&lt;/li&gt; 
     &lt;li&gt;Windows Update (wouldn't work without WinSxS, enabling it would put the system in a state of failure)&lt;/li&gt; 
     &lt;li&gt;WinRE&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Keep in mind that &lt;strong&gt;you cannot add back features in tiny11 core&lt;/strong&gt;! &lt;br /&gt; You will be asked during image creation if you want to enable .net 3.5 support!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Known issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Although Edge is removed, there are some remnants in the Settings, but the app in itself is deleted.&lt;/li&gt; 
 &lt;li&gt;You might have to update Winget before being able to install any apps, using Microsoft Store.&lt;/li&gt; 
 &lt;li&gt;Outlook and Dev Home might reappear after some time. This is an ongoing battle, though the latest script update tries to prevent this more aggressively.&lt;/li&gt; 
 &lt;li&gt;If you are using this script on arm64, you might see a glimpse of an error while running the script. This is caused by the fact that the arm64 image doesn't have OneDriveSetup.exe included in the System32 folder.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Features to be implemented:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;del&gt;disabling telemetry&lt;/del&gt; (Implemented in the 04-29-24 release!)&lt;/li&gt; 
 &lt;li&gt;&lt;del&gt;more ad suppression&lt;/del&gt; (Partially implemented in the 09-06-25 release!)&lt;/li&gt; 
 &lt;li&gt;improved language and arch detection&lt;/li&gt; 
 &lt;li&gt;more flexibility in what to keep and what to delete&lt;/li&gt; 
 &lt;li&gt;maybe a GUI???&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And that's pretty much it for now!&lt;/p&gt; 
&lt;h2&gt;❤️ Support the Project&lt;/h2&gt; 
&lt;p&gt;If this project has helped you, please consider showing your support! A small donation helps me dedicate more time to projects like this. Thank you!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="http://patreon.com/ntdev"&gt;Patreon&lt;/a&gt; | &lt;a href="http://paypal.me/ntdev2"&gt;PayPal&lt;/a&gt; | &lt;a href="http://ko-fi.com/ntdev"&gt;Ko-fi&lt;/a&gt;&lt;/strong&gt; Thanks for trying it and let me know how you like it!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>heroui-inc/heroui</title>
      <link>https://github.com/heroui-inc/heroui</link>
      <description>&lt;p&gt;🚀 Beautiful, fast and modern React UI library. (Previously NextUI)&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://heroui.com"&gt; &lt;img width="20%" src="https://raw.githubusercontent.com/heroui-inc/heroui/main/apps/docs/public/isotipo.png" alt="heorui" /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a href="https://heroui.com"&gt;HeroUI&lt;/a&gt;&lt;/h1&gt;
&lt;a href="https://heroui.com"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/heroui-inc/heroui/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/npm/l/@heroui/react?style=flat" alt="License" /&gt; &lt;/a&gt; &lt;a href="https://codecov.io/gh/jrgarciadev/nextui"&gt; &lt;img src="https://codecov.io/gh/jrgarciadev/nextui/branch/main/graph/badge.svg?token=QJF2QKR5N4" alt="codecov badge" /&gt; &lt;/a&gt; 
 &lt;!-- &lt;a href="https://github.com/heroui-inc/heroui/actions/workflows/main.yaml"&gt;
    &lt;img src="https://github.com/heroui-inc/heroui/actions/workflows/main.yaml/badge.svg" alt="CI/CD heroui"&gt;
  &lt;/a&gt; --&gt; &lt;a href="https://www.npmjs.com/package/@heroui/react"&gt; &lt;img src="https://img.shields.io/npm/dm/@heroui/react.svg?style=flat-round" alt="npm downloads" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a aria-label="heroui learn" href="https://heroui.com/learn"&gt;&lt;/a&gt;&lt;a href="https://heroui.com/guide"&gt;https://heroui.com/guide&lt;/a&gt; to get started with HeroUI.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://heroui.com/docs"&gt;https://heroui.com/docs&lt;/a&gt; to view the full documentation.&lt;/p&gt; 
&lt;h2&gt;Storybook&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://storybook.heroui.com/"&gt;https://storybook.heroui.com&lt;/a&gt; to view the storybook for all components.&lt;/p&gt; 
&lt;h2&gt;Canary Release&lt;/h2&gt; 
&lt;p&gt;Canary versions are available after every merge into &lt;code&gt;canary&lt;/code&gt; branch. You can install the packages with the tag &lt;code&gt;canary&lt;/code&gt; in npm to use the latest changes before the next production release.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://canary.heroui.com/docs"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://canary-sb.heroui.com"&gt;Storybook&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;We're excited to see the community adopt HeroUI, raise issues, and provide feedback. Whether it's a feature request, bug report, or a project to showcase, please get involved!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/9b6yyZKmH4"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/hero_ui"&gt;X&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heroui-inc/heroui/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are always welcome!&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/heroui-inc/heroui/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for ways to get started.&lt;/p&gt; 
&lt;p&gt;Please adhere to this project's &lt;a href="https://github.com/heroui-inc/heroui/raw/main/CODE_OF_CONDUCT.md"&gt;CODE_OF_CONDUCT&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://choosealicense.com/licenses/mit/"&gt;MIT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/material-design-icons</title>
      <link>https://github.com/google/material-design-icons</link>
      <description>&lt;p&gt;Material Design icons by Google (Material Symbols)&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Material Symbols / Material Icons&lt;/h2&gt; 
&lt;p&gt;These are two different official icon sets from Google, using the same underlying designs. Material Symbols is the current set, introduced in April 2022, built on variable font technology. Material Icons is the classic set, but no longer updated. More details below.&lt;/p&gt; 
&lt;p&gt;The icons can be browsed in a more user-friendly way at &lt;a href="https://fonts.google.com/icons"&gt;https://fonts.google.com/icons&lt;/a&gt;. Use the popdown menu near top left to choose between the two sets; Material Symbols is the default.&lt;/p&gt; 
&lt;p&gt;The icons are designed under the &lt;a href="https://material.io/guidelines/"&gt;material design guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Icon Requests&lt;/h2&gt; 
&lt;p&gt;We’d love to support your icon needs! Please submit your request here on GitHub as an issue.&lt;/p&gt; 
&lt;p&gt;Please note that Google Fonts does not accept user submissions of finished icon designs! There are fairly strict guidelines for Material icons, plus Google has upstream source files from which this repo is generated. Therefore, Google does not accept pull requests for icon files (whether new icon suggestions, or fixes for existing icons). Concepts are appreciated—just don’t design SVGs and submit them via pull request.&lt;/p&gt; 
&lt;p&gt;However, users are perfectly welcome to point at outside files or images as examples—for the kind of thing they want, but they won’t just be taken “as is.” This works especially well if you have multiple examples for a single icon, to help us understand the “essence”&amp;nbsp;of the idea.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For example, there is a fairly universal conceptual logo/icon for “agender,” so if you were proposing Google add an agender icon in the Material style, either mentioning that, or pointing at &lt;a href="https://www.google.com/search?q=agender+icon"&gt;https://www.google.com/search?q=agender+icon&lt;/a&gt; would be a helpful tip.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Third-party logos&lt;/h3&gt; 
&lt;p&gt;Currently, Google does not include 3rd-party logos among the Material Symbols or Material Icons due to legal reasons. Some 3rd-party logos that were included in the past have since been removed.&lt;/p&gt; 
&lt;h2&gt;npm Packages&lt;/h2&gt; 
&lt;p&gt;Google does not currently maintain the npm package for this repo, past v3 (2016). However, user @marella is hosting the following. He tells us these are automatically updated and published using GitHub Actions. Note: Google does &lt;strong&gt;not&lt;/strong&gt; monitor or vet these packages.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/marella/material-symbols/tree/main/material-symbols#readme"&gt;material-symbols&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/material-symbols"&gt;&lt;img src="https://img.shields.io/npm/v/material-symbols" alt="npm" /&gt;&lt;/a&gt; &lt;a href="https://packagephobia.com/result?p=material-symbols"&gt;&lt;img src="https://packagephobia.com/badge?p=material-symbols" alt="install size" /&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only WOFF2 variable fonts and CSS for Material Symbols&lt;/li&gt; 
 &lt;li&gt;Includes outlined, rounded, and sharp icons and all variations of fill, weight, grade, and optical size&lt;/li&gt; 
 &lt;li&gt;Supports Sass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://github.com/marella/material-icons#readme"&gt;material-icons&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/material-icons"&gt;&lt;img src="https://img.shields.io/npm/v/material-icons" alt="npm" /&gt;&lt;/a&gt; &lt;a href="https://packagephobia.com/result?p=material-icons"&gt;&lt;img src="https://packagephobia.com/badge?p=material-icons" alt="install size" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/material-icons"&gt;&lt;img src="https://img.shields.io/npm/dm/material-icons" alt="Downloads" /&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only WOFF2, WOFF fonts and CSS&lt;/li&gt; 
 &lt;li&gt;Includes outlined, round, sharp and two-tone icons&lt;/li&gt; 
 &lt;li&gt;Supports Sass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://github.com/marella/material-design-icons/tree/main/font#readme"&gt;@material-design-icons/font&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/@material-design-icons/font"&gt;&lt;img src="https://img.shields.io/npm/v/@material-design-icons/font" alt="npm (scoped)" /&gt;&lt;/a&gt; &lt;a href="https://packagephobia.com/result?p=@material-design-icons/font"&gt;&lt;img src="https://packagephobia.com/badge?p=@material-design-icons/font" alt="install size" /&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only WOFF2 fonts and CSS&lt;/li&gt; 
 &lt;li&gt;Lighter version of &lt;code&gt;material-icons&lt;/code&gt; package&lt;/li&gt; 
 &lt;li&gt;Doesn't support &lt;a href="https://caniuse.com/woff2"&gt;older browsers&lt;/a&gt; such as Internet Explorer because of dropping WOFF (v1)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href="https://github.com/marella/material-design-icons/tree/main/svg#readme"&gt;@material-design-icons/svg&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/@material-design-icons/svg"&gt;&lt;img src="https://img.shields.io/npm/v/@material-design-icons/svg" alt="npm (scoped)" /&gt;&lt;/a&gt; &lt;a href="https://packagephobia.com/result?p=@material-design-icons/svg"&gt;&lt;img src="https://packagephobia.com/badge?p=@material-design-icons/svg" alt="install size" /&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only SVGs&lt;/li&gt; 
 &lt;li&gt;Optimizes SVGs using SVGO&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Material Symbols&lt;/h2&gt; 
&lt;p&gt;These newer icons can be browsed in a more user-friendly way at &lt;a href="https://fonts.google.com/icons"&gt;https://fonts.google.com/icons&lt;/a&gt;. Use the popdown menu near top left to choose between the two sets; Material Symbols is the default.&lt;/p&gt; 
&lt;p&gt;These icons were built/designed as variable fonts first (based on the 24 px designs from Material Icons). There are three separate Material Symbols variable fonts, which also have static icons available (but those do not have all the variations available, as that would be hundreds of styles):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Outlined&lt;/li&gt; 
 &lt;li&gt;Rounded&lt;/li&gt; 
 &lt;li&gt;Sharp&lt;/li&gt; 
 &lt;li&gt;Note that although there is no separate Filled font, the Fill axis allows access to filled styles, in all three fonts. It can also be manipulated for an animated fill effect, to indicate user selection.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each of the fonts has these design axes, which can be varied in CSS, or in many more modern design apps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optical Size (opsz) from 20 to 48 px. The default is 24.&lt;/li&gt; 
 &lt;li&gt;Weight from 100 (Thin) to 700 (Bold). Regular is 400.&lt;/li&gt; 
 &lt;li&gt;Grade from -50 to 200. The default is 0 (zero). -50 is suggested for reversed contrast (e.g. white icons on black background)&lt;/li&gt; 
 &lt;li&gt;Fill from 0 to 100. The default is 0 (zero).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following directories in this repo contain specifically Material Symbols (not Material Icons) content:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;symbols&lt;/li&gt; 
 &lt;li&gt;variablefont&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;What is currently &lt;em&gt;not&lt;/em&gt; available in Material Symbols?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;only the 20 and 24 px versions are designed with perfect pixel-grid alignment&lt;/li&gt; 
 &lt;li&gt;the only pre-made fonts are the variable fonts&lt;/li&gt; 
 &lt;li&gt;there are no two-tone icons&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Material Icons&lt;/h2&gt; 
&lt;p&gt;The icons can be browsed in a more user-friendly way at &lt;a href="https://fonts.google.com/icons?icon.set=Material+Icons"&gt;https://fonts.google.com/icons?icon.set=Material+Icons&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;These classic icons are available in five distinct styles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Outlined&lt;/li&gt; 
 &lt;li&gt;Filled (the font version is just called Material Icons, as this is the oldest style)&lt;/li&gt; 
 &lt;li&gt;Rounded&lt;/li&gt; 
 &lt;li&gt;Sharp&lt;/li&gt; 
 &lt;li&gt;Two tone&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following directories in this repo contain specifically Material Icons (not Material Symbols) content:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;android&lt;/li&gt; 
 &lt;li&gt;font&lt;/li&gt; 
 &lt;li&gt;ios&lt;/li&gt; 
 &lt;li&gt;png&lt;/li&gt; 
 &lt;li&gt;src&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;What is currently &lt;em&gt;not&lt;/em&gt; available in Material Icons?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;variable fonts&lt;/li&gt; 
 &lt;li&gt;weights other than Regular&lt;/li&gt; 
 &lt;li&gt;grades other than Regular&lt;/li&gt; 
 &lt;li&gt;a means to animate Fill transitions&lt;/li&gt; 
 &lt;li&gt;new icons (since updates were halted in 2022)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Material Icons update history&lt;/h2&gt; 
&lt;h3&gt;4.0.0 Update&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2020 Aug 31&lt;/li&gt; 
 &lt;li&gt;Restructured repository, updated assets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3.0.1 Update&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2016 Sep 01&lt;/li&gt; 
 &lt;li&gt;Changed license in package.json.&lt;/li&gt; 
 &lt;li&gt;Added missing device symbol sprites.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3.0.0 Update&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2016 Aug 25&lt;/li&gt; 
 &lt;li&gt;License change to Apache 2.0!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2.0&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2016 May 28&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Read the &lt;a href="https://developers.google.com/fonts/docs/material_icons"&gt;developer guide&lt;/a&gt; on how to use the material design icons in your project.&lt;/p&gt; 
&lt;h3&gt;Using a font&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;font&lt;/code&gt; and &lt;code&gt;variablefont&lt;/code&gt; folders contain pre-generated font files that can be included in a project. This is especially convenient for the web; however, it is generally better to link to the web font hosted on Google Fonts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;link href="https://fonts.googleapis.com/css2?family=Material+Icons"
      rel="stylesheet"&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;link href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined"
      rel="stylesheet"&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more on &lt;a href="https://developers.google.com/fonts/docs/material_symbols/"&gt;Material Symbols&lt;/a&gt; or &lt;a href="https://developers.google.com/fonts/docs/material_icons/"&gt;Material Icons&lt;/a&gt; in the Google Fonts developer guide.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;We have made these icons available for you to incorporate into your products under the &lt;a href="https://www.apache.org/licenses/LICENSE-2.0.txt"&gt;Apache License Version 2.0&lt;/a&gt;. Feel free to remix and re-share these icons and documentation in your products. We'd love attribution in your app's &lt;em&gt;about&lt;/em&gt; screen, but it's not required.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ZuodaoTech/everyone-can-use-english</title>
      <link>https://github.com/ZuodaoTech/everyone-can-use-english</link>
      <description>&lt;p&gt;人人都能用英语&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/assets/icon.png" alt="Clash" width="128" /&gt; 
&lt;/div&gt; 
&lt;h3 align="center"&gt; AI 是当今世界上最好的外语老师，Enjoy 做 AI 最好的助教。 &lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/deploy-1000h.yml"&gt;&lt;img src="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/deploy-1000h.yml/badge.svg?sanitize=true" alt="Deploy 1000h website" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/test-enjoy-app.yml"&gt;&lt;img src="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/test-enjoy-app.yml/badge.svg?sanitize=true" alt="Test Enjoy App" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/release-enjoy-app.yml"&gt;&lt;img src="https://github.com/ZuodaoTech/everyone-can-use-english/actions/workflows/release-enjoy-app.yml/badge.svg?sanitize=true" alt="Release Enjoy App" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fenjoy.bot%2Fapi%2Fconfig%2Fapp_version&amp;amp;query=%24.version&amp;amp;label=Latest&amp;amp;link=https%3A%2F%2F1000h.org%2Fenjoy-app%2Finstall.html" alt="Latest Version" /&gt; &lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fenjoy.bot%2Fapi%2Fbadges%2Frecordings" alt="Recording Duration" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;网页版&lt;/h2&gt; 
&lt;p&gt;Enjoy 网页版已经上线，可访问 &lt;a href="https://enjoy.bot"&gt;https://enjoy.bot&lt;/a&gt; 直接使用。&lt;/p&gt; 
&lt;div align="center" style="display:flex;overflow:auto;gap:10px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-audios.jpg" alt="Audios" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-add-audio.jpg" alt="Add Audio" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-audio-shadow.jpg" alt="Shadow" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-audio-assessment.jpg" alt="Assessment" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-new-chat.jpg" alt="New Chat" width="300" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/web-chat.jpg" alt="Chat" width="300" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;桌面版安装及使用&lt;/h2&gt; 
&lt;p&gt;下载及使用相关说明，请参阅 &lt;a href="https://1000h.org/enjoy-app/"&gt;文档&lt;/a&gt;。&lt;/p&gt; 
&lt;h2&gt;预览&lt;/h2&gt; 
&lt;div align="center" style="display:flex;overflow:auto;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/home.png" alt="Home" width="800" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/shadow.png" alt="Home" width="800" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/assessment.png" alt="Home" width="800" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/document.png" alt="Home" width="800" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/enjoy/snapshots/chat.png" alt="Home" width="800" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;桌面版开发&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;yarn install
yarn enjoy:start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;相关阅读&lt;/h2&gt; 
&lt;h3&gt;一千小时（2024）&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/intro.html"&gt;简要说明&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/training-tasks/kick-off.html"&gt;训练任务&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/sounds-of-american-english/0-intro.html"&gt;语音塑造&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/in-the-brain/01-inifinite.html"&gt;大脑内部&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://1000h.org/self-training/00-intro.html"&gt;自我训练&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;人人都能用英语（2010）&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/README.md"&gt;简介&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter1.md"&gt;第一章：起点&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter2.md"&gt;第二章：口语&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter3.md"&gt;第三章：语音&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter4.md"&gt;第四章：朗读&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter5.md"&gt;第五章：词典&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter6.md"&gt;第六章：语法&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter7.md"&gt;第七章：精读&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/chapter8.md"&gt;第八章：叮嘱&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ZuodaoTech/everyone-can-use-english/main/book/end.md"&gt;后记&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;常见问题&lt;/h2&gt; 
&lt;p&gt;请查询 &lt;a href="https://1000h.org/enjoy-app/faq.html"&gt;文档 FAQ&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>twitter/the-algorithm</title>
      <link>https://github.com/twitter/the-algorithm</link>
      <description>&lt;p&gt;Source code for the X Recommendation Algorithm&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;X's Recommendation Algorithm&lt;/h1&gt; 
&lt;p&gt;X's Recommendation Algorithm is a set of services and jobs that are responsible for serving feeds of posts and other content across all X product surfaces (e.g. For You Timeline, Search, Explore, Notifications). For an introduction to how the algorithm works, please refer to our &lt;a href="https://blog.x.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm"&gt;engineering blog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Product surfaces at X are built on a shared set of data, models, and software frameworks. The shared components included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Data&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/tweetypie/server/README.md"&gt;tweetypie&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Core service that handles the reading and writing of post data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/unified_user_actions/README.md"&gt;unified-user-actions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time stream of user actions on X.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/user-signal-service/README.md"&gt;user-signal-service&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Centralized platform to retrieve explicit (e.g. likes, replies) and implicit (e.g. profile visits, tweet clicks) user signals.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Model&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/simclusters_v2/README.md"&gt;SimClusters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Community detection and sparse embeddings into those communities.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/twitter/the-algorithm-ml/raw/main/projects/twhin/README.md"&gt;TwHIN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Dense knowledge graph embeddings for Users and Posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/trust_and_safety_models/README.md"&gt;trust-and-safety-models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Models for detecting NSFW or abusive content.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/interaction_graph/README.md"&gt;real-graph&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model to predict the likelihood of an X User interacting with another User.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/graph/batch/job/tweepcred/README"&gt;tweepcred&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Page-Rank algorithm for calculating X User reputation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/recos-injector/README.md"&gt;recos-injector&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Streaming event processor for building input streams for &lt;a href="https://github.com/twitter/GraphJet"&gt;GraphJet&lt;/a&gt; based services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/graph-feature-service/README.md"&gt;graph-feature-service&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Serves graph features for a directed pair of users (e.g. how many of User A's following liked posts from User B).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/topic-social-proof/README.md"&gt;topic-social-proof&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Identifies topics related to individual posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/representation-scorer/README.md"&gt;representation-scorer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Compute scores between pairs of entities (Users, Posts, etc.) using embedding similarity.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Software framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/navi/README.md"&gt;navi&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;High performance, machine learning model serving written in Rust.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/product-mixer/README.md"&gt;product-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Software framework for building feeds of content.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/timelines/data_processing/ml_util/aggregation_framework/README.md"&gt;timelines-aggregation-framework&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Framework for generating aggregate features in batch or real time.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/representation-manager/README.md"&gt;representation-manager&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Service to retrieve embeddings (i.e. SimClusers and TwHIN).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/twml/README.md"&gt;twml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Legacy machine learning framework built on TensorFlow v1.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The product surfaces currently included in this repository are the For You Timeline and Recommended Notifications.&lt;/p&gt; 
&lt;h3&gt;For You Timeline&lt;/h3&gt; 
&lt;p&gt;The diagram below illustrates how major services and jobs interconnect to construct a For You Timeline.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/twitter/the-algorithm/main/docs/system-diagram.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;The core components of the For You Timeline included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Candidate Source&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/java/com/twitter/search/README.md"&gt;search-index&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Find and rank In-Network posts. ~50% of posts come from this candidate source.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/tweet-mixer"&gt;tweet-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Coordination layer for fetching Out-of-Network tweet candidates from underlying compute services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/recos/user_tweet_entity_graph/README.md"&gt;user-tweet-entity-graph&lt;/a&gt; (UTEG)&lt;/td&gt; 
   &lt;td&gt;Maintains an in memory User to Post interaction graph, and finds candidates based on traversals of this graph. This is built on the &lt;a href="https://github.com/twitter/GraphJet"&gt;GraphJet&lt;/a&gt; framework. Several other GraphJet based features and candidate sources are located &lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/recos"&gt;here&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/follow-recommendations-service/README.md"&gt;follow-recommendation-service&lt;/a&gt; (FRS)&lt;/td&gt; 
   &lt;td&gt;Provides Users with recommendations for accounts to follow, and posts from those accounts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/src/python/twitter/deepbird/projects/timelines/scripts/models/earlybird/README.md"&gt;light-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light Ranker model used by search index (Earlybird) to rank posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/twitter/the-algorithm-ml/raw/main/projects/home/recap/README.md"&gt;heavy-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Neural network for ranking candidate posts. One of the main signals used to select timeline posts post candidate sourcing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Post mixing &amp;amp; filtering&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/home-mixer/README.md"&gt;home-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main service used to construct and serve the Home Timeline. Built on &lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/product-mixer/README.md"&gt;product-mixer&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/visibilitylib/README.md"&gt;visibility-filters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Responsible for filtering X content to support legal compliance, improve product quality, increase user trust, protect revenue through the use of hard-filtering, visible product treatments, and coarse-grained downranking.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/timelineranker/README.md"&gt;timelineranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Legacy service which provides relevance-scored posts from the Earlybird Search Index and UTEG service.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Recommended Notifications&lt;/h3&gt; 
&lt;p&gt;The core components of Recommended Notifications included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Service&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/README.md"&gt;pushservice&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main recommendation service at X used to surface recommendations to our users via notifications.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/src/main/python/models/light_ranking/README.md"&gt;pushservice-light-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light Ranker model used by pushservice to rank posts. Bridges candidate generation and heavy ranking by pre-selecting highly-relevant candidates from the initial huge candidate pool.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/src/main/python/models/heavy_ranking/README.md"&gt;pushservice-heavy-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-task learning model to predict the probabilities that the target users will open and engage with the sent notifications.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Build and test code&lt;/h2&gt; 
&lt;p&gt;We include Bazel BUILD files for most components, but not a top-level BUILD or WORKSPACE file. We plan to add a more complete build and test system in the future.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We invite the community to submit GitHub issues and pull requests for suggestions on improving the recommendation algorithm. We are working on tools to manage these suggestions and sync changes to our internal repository. Any security concerns or issues should be routed to our official &lt;a href="https://hackerone.com/x"&gt;bug bounty program&lt;/a&gt; through HackerOne. We hope to benefit from the collective intelligence and expertise of the global community in helping us identify issues and suggest improvements, ultimately leading to a better X.&lt;/p&gt; 
&lt;p&gt;Read our blog on the open source initiative &lt;a href="https://blog.x.com/en_us/topics/company/2023/a-new-era-of-transparency-for-twitter"&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trueadm/ripple</title>
      <link>https://github.com/trueadm/ripple</link>
      <description>&lt;p&gt;the elegant TypeScript UI framework&lt;/p&gt;&lt;hr&gt;&lt;a href="https://ripplejs.com"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="assets/ripple-dark.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/trueadm/ripple/main/assets/ripple-light.png" alt="Ripple - the elegant TypeScript UI framework" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;&lt;a href="https://github.com/trueadm/ripple/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/trueadm/ripple/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/JBF2ySrh2W"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Server-7289da?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://stackblitz.com/github/trueadm/ripple/tree/main/templates/basic"&gt;&lt;img src="https://developer.stackblitz.com/img/open_in_stackblitz_small.svg?sanitize=true" alt="Open in StackBlitz" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;What is Ripple?&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Currently, this project is still in early development, and should not be used in production.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Ripple is a TypeScript UI framework that takes the best parts of React, Solid and Svelte and combines them into one package.&lt;/p&gt; 
&lt;p&gt;I wrote Ripple as a love letter for frontend web – and this is largely a project that I built in less than a week, so it's very raw.&lt;/p&gt; 
&lt;p&gt;Personally, I (&lt;a href="https://github.com/trueadm"&gt;@trueadm&lt;/a&gt;) have been involved in some truly amazing frontend frameworks along their journeys – from &lt;a href="https://github.com/infernojs/inferno"&gt;Inferno&lt;/a&gt;, where it all began, to &lt;a href="https://github.com/facebook/react"&gt;React&lt;/a&gt; and the journey of React Hooks, to creating &lt;a href="https://github.com/facebook/lexical"&gt;Lexical&lt;/a&gt;, to &lt;a href="https://github.com/sveltejs/svelte"&gt;Svelte 5&lt;/a&gt; and its new compiler and signal-based reactivity runtime. Along that journey, I collected ideas, and intriguing thoughts that may or may not pay off. Given my time between roles, I decided it was the best opportunity to try them out, and for open source to see what I was cooking.&lt;/p&gt; 
&lt;p&gt;Ripple was designed to be a JS/TS-first framework, rather than HTML-first. Ripple modules have their own &lt;code&gt;.ripple&lt;/code&gt; extension, and these modules fully support TypeScript. By introducing a new extension, it allows Ripple to invent its own superset language, which plays really nicely with TypeScript and JSX, but with a few interesting touches. In my experience, this has led to better DX not only for humans, but also for LLMs.&lt;/p&gt; 
&lt;p&gt;Right now, there will be plenty of bugs, things just won't work either and you'll find TODOs everywhere. At this stage, Ripple is more of an early alpha version of something that &lt;em&gt;might&lt;/em&gt; be, rather than something you should try and adopt. If anything, maybe some of the ideas can be shared and incubated back into other frameworks. There's also a lot of similarities with Svelte 5, and that's not by accident; that's because of my recent time working on Svelte 5.&lt;/p&gt; 
&lt;p&gt;If you'd like to know more, join the &lt;a href="https://discord.gg/JBF2ySrh2W"&gt;Ripple Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reactive State Management&lt;/strong&gt;: Built-in reactivity with &lt;code&gt;$&lt;/code&gt; prefixed variables and object properties&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Component-Based Architecture&lt;/strong&gt;: Clean, reusable components with props and children&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JSX-like Syntax&lt;/strong&gt;: Familiar templating with Ripple-specific enhancements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Fine-grain rendering, with industry-leading performance and memory usage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TypeScript Support&lt;/strong&gt;: Full TypeScript integration with type checking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VSCode Integration&lt;/strong&gt;: Rich editor support with diagnostics, syntax highlighting, and IntelliSense&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prettier Support&lt;/strong&gt;: Full Prettier formatting support for &lt;code&gt;.ripple&lt;/code&gt; modules&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Missing Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SSR&lt;/strong&gt;: Ripple is currently an SPA only, this is because I haven't gotten around to it&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Types&lt;/strong&gt;: The codebase is very raw with limited types; we're getting around to it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Try Ripple&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We're working hard on getting an online playground available. Watch this space!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can try Ripple now by using our basic Vite template either via &lt;a href="https://stackblitz.com/github/trueadm/ripple/tree/main/templates/basic"&gt;StackBlitz&lt;/a&gt;, or by running these commands in your terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx degit trueadm/ripple/templates/basic my-app
cd my-app
npm i # or yarn or pnpm
npm run dev # or yarn or pnpm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;VSCode Extension&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://marketplace.visualstudio.com/items?itemName=ripplejs.ripple-vscode-plugin"&gt;Ripple VSCode extension&lt;/a&gt; provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Syntax Highlighting&lt;/strong&gt; for &lt;code&gt;.ripple&lt;/code&gt; files&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Diagnostics&lt;/strong&gt; for compilation errors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TypeScript Integration&lt;/strong&gt; for type checking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IntelliSense&lt;/strong&gt; for autocompletion&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can find the extension on the VS Code Marketplace as &lt;a href="https://marketplace.visualstudio.com/items?itemName=ripplejs.ripple-vscode-plugin"&gt;&lt;code&gt;Ripple for VS Code&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also &lt;a href="https://github.com/trueadm/ripple/raw/refs/heads/main/packages/ripple-vscode-plugin/published/ripple-vscode-plugin.vsix"&gt;manually install the extension&lt;/a&gt; &lt;code&gt;.vsix&lt;/code&gt; that have been manually packaged.&lt;/p&gt; 
&lt;h3&gt;Mounting your app&lt;/h3&gt; 
&lt;p&gt;You can use the &lt;code&gt;mount&lt;/code&gt; API from the &lt;code&gt;ripple&lt;/code&gt; package to render your Ripple component, using the &lt;code&gt;target&lt;/code&gt; option to specify what DOM element you want to render the component.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;// index.ts
import { mount } from 'ripple';
import { App } from '/App.ripple';

mount(App, {
  props: {
    title: 'Hello world!',
  },
  target: document.getElementById('root'),
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Key Concepts&lt;/h2&gt; 
&lt;h3&gt;Components&lt;/h3&gt; 
&lt;p&gt;Define reusable components with the &lt;code&gt;component&lt;/code&gt; keyword. These are similar to functions in that they have &lt;code&gt;props&lt;/code&gt;, but crucially, they allow for a JSX-like syntax to be defined alongside standard TypeScript. That means you do not &lt;em&gt;return JSX&lt;/em&gt; like in other frameworks, but you instead use it like a JavaScript statement, as shown:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Button(props: { text: string, onClick: () =&amp;gt; void }) {
  &amp;lt;button onClick={props.onClick}&amp;gt;
    {props.text}
  &amp;lt;/button&amp;gt;
}

// Usage
export component App() {
  &amp;lt;Button text="Click me" onClick={() =&amp;gt; console.log("Clicked!")} /&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ripple's templating language also supports shorthands and object spreads too:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-svelte"&gt;// you can do a normal prop
&amp;lt;div onClick={onClick}&amp;gt;{text}&amp;lt;/div&amp;gt;

// or using the shorthand prop
&amp;lt;div {onClick}&amp;gt;{text}&amp;lt;/div&amp;gt;

// and you can spread props
&amp;lt;div {...properties}&amp;gt;{text}&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reactive Variables&lt;/h3&gt; 
&lt;p&gt;Variables prefixed with &lt;code&gt;$&lt;/code&gt; are automatically reactive:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;let $name = 'World';
let $count = 0;

// Updates automatically trigger re-renders
$count++;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Object properties prefixed with &lt;code&gt;$&lt;/code&gt; are also automatically reactive:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;let counter = { $current: 0 };

// Updates automatically trigger re-renders
counter.$current++;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Derived values are simply &lt;code&gt;$&lt;/code&gt; variables that combined different parts of state:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ts"&gt;let $count = 0;
let $double = $count * 2;
let $quadruple = $double * 2;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That means &lt;code&gt;$count&lt;/code&gt; itself might be derived if it were to reference another reactive property. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Counter({ $startingCount }) {
  let $count = $startingCount;
  let $double = $count * 2;
  let $quadruple = $double * 2;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now given &lt;code&gt;$startingCount&lt;/code&gt; is reactive, it would mean that &lt;code&gt;$count&lt;/code&gt; might reset each time an incoming change to &lt;code&gt;$startingCount&lt;/code&gt; occurs. That might not be desirable, so Ripple provides a way to &lt;code&gt;untrack&lt;/code&gt; reactivity in those cases:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { untrack } from 'ripple';

component Counter({ $startingCount }) {
  let $count = untrack(() =&amp;gt; $startingCount);
  let $double = $count * 2;
  let $quadruple = $double * 2;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now &lt;code&gt;$count&lt;/code&gt; will only reactively create its value on initialization.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: you cannot define reactive variables in module/global scope, they have to be created on access from an active component&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Transporting Reactivity&lt;/h4&gt; 
&lt;p&gt;Ripple doesn't constrain reactivity to components only. Reactivity can be used inside other functions (and classes in the future) and be composed in a way to improve expressivity and co-location.&lt;/p&gt; 
&lt;p&gt;Ripple provides a very nice way to transport reactivity between boundaries so that it's persisted – using objects and arrays. Here's an example using arrays to transport reactivity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { effect } from 'ripple';

function createDouble([ $count ]) {
  const $double = $count * 2;

  effect(() =&amp;gt; {
    console.log('Count:', $count)
  });

  return [ $double ];
}

export component App() {
  let $count = 0;

  const [ $double ] = createDouble([ $count ]);

  &amp;lt;div&amp;gt;{'Double: ' + $double}&amp;lt;/div&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; { $count++; }}&amp;gt;{'Increment'}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can do the same with objects too:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { effect } from 'ripple';

function createDouble({ $count }) {
  const $double = $count * 2;

  effect(() =&amp;gt; {
    console.log('Count:', $count)
  });

  return { $double };
}

export component App() {
  let $count = 0;
  const { $double } = createDouble({ $count });

  &amp;lt;div&amp;gt;{'Double: ' + $double}&amp;lt;/div&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; { $count++; }}&amp;gt;{'Increment'}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Just remember, reactive state must be connected to a component and it can't be global or created within the top-level of a module – because then Ripple won't be able to link it to your component tree.&lt;/p&gt; 
&lt;h4&gt;Reactive Arrays&lt;/h4&gt; 
&lt;p&gt;Just like, objects, you can use the &lt;code&gt;$&lt;/code&gt; prefix in an array literal to specify that the field is reactive.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;let $first = 0;
let $second = 0;
const arr = [$first, $second];

const $total = arr.reduce((a, b) =&amp;gt; a + b, 0);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Like shown in the above example, you can compose normal arrays with reactivity and pass them through props or boundaries.&lt;/p&gt; 
&lt;p&gt;However, if you need the entire array to be fully reactive, including when new elements get added, you should use the reactive array that Ripple provides.&lt;/p&gt; 
&lt;p&gt;You'll need to import the &lt;code&gt;RippleArray&lt;/code&gt; class from Ripple. It extends the standard JS &lt;code&gt;Array&lt;/code&gt; class, and supports all of its methods and properties.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { RippleArray } from 'ripple';

// using the new constructor
const arr = new RippleArray(1, 2, 3);

// using static from method
const arr = RippleArray.from([1, 2, 3]);

// using static of method
const arr = RippleArray.of(1, 2, 3);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;RippleArray&lt;/code&gt; is a reactive array, and that means you can access properties normally using numeric index. However, accessing the &lt;code&gt;length&lt;/code&gt; property of a &lt;code&gt;RippleArray&lt;/code&gt; will be not be reactive, instead you should use &lt;code&gt;$length&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Reactive Set&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;RippleSet&lt;/code&gt; extends the standard JS &lt;code&gt;Set&lt;/code&gt; class, and supports all of its methods and properties. However, accessing the &lt;code&gt;size&lt;/code&gt; property of a &lt;code&gt;RippleSet&lt;/code&gt; will be not be reactive, instead you should use &lt;code&gt;$size&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { RippleSet } from 'ripple';

const set = new RippleSet([1, 2, 3]);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RippleSet's reactive methods or properties can be used directly or assigned to reactive variables.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { RippleSet } from 'ripple';

export component App() {
  const set = new RippleSet([1, 2, 3]);

  // direct usage
  &amp;lt;p&amp;gt;{"Direct usage: set contains 2: "}{set.has(2)}&amp;lt;/p&amp;gt;

  // reactive assignment with prefixed `$`
  let $has = set.has(2);
  &amp;lt;p&amp;gt;{"Assigned usage: set contains 2: "}{$has}&amp;lt;/p&amp;gt;

  &amp;lt;button onClick={() =&amp;gt; set.delete(2)}&amp;gt;{"Delete 2"}&amp;lt;/button&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; set.add(2)}&amp;gt;{"Add 2"}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Reactive Map&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;RippleMap&lt;/code&gt; extends the standard JS &lt;code&gt;Map&lt;/code&gt; class, and supports all of its methods and properties. However, accessing the &lt;code&gt;size&lt;/code&gt; property of a &lt;code&gt;RippleMap&lt;/code&gt; will be not be reactive, instead you should use &lt;code&gt;$size&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { RippleMap } from 'ripple';

const map = new RippleMap([[1,1], [2,2], [3,3], [4,4]]);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RippleMap's reactive methods or properties can be used directly or assigned to reactive variables.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { RippleMap } from 'ripple';

export component App() {
  const map = new RippleMap([[1,1], [2,2], [3,3], [4,4]]);

  // direct usage
  &amp;lt;p&amp;gt;{"Direct usage: map has an item with key 2: "}{map.has(2)}&amp;lt;/p&amp;gt;

  // reactive assignment with prefixed `$`
  let $has = map.has(2);
  &amp;lt;p&amp;gt;{"Assigned usage: map has an item with key 2: "}{$has}&amp;lt;/p&amp;gt;

  &amp;lt;button onClick={() =&amp;gt; map.delete(2)}&amp;gt;{"Delete item with key 2"}&amp;lt;/button&amp;gt;
  &amp;lt;button onClick={() =&amp;gt; map.set(2, 2)}&amp;gt;{"Add key 2 with value 2"}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Effects&lt;/h3&gt; 
&lt;p&gt;When dealing with reactive state, you might want to be able to create side-effects based upon changes that happen upon updates. To do this, you can use &lt;code&gt;effect&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { effect } from 'ripple';

export component App() {
  let $count = 0;

  effect(() =&amp;gt; {
    console.log($count);
  });

  &amp;lt;button onClick={() =&amp;gt; $count++}&amp;gt;{'Increment'}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Control flow&lt;/h3&gt; 
&lt;p&gt;The JSX-like syntax might take some time to get used to if you're coming from another framework. For one, templating in Ripple can only occur &lt;em&gt;inside&lt;/em&gt; a &lt;code&gt;component&lt;/code&gt; body – you can't create JSX inside functions, or assign it to variables as an expression.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;&amp;lt;div&amp;gt;
  // you can create variables inside the template!
  const str = "hello world";

  console.log(str); // and function calls too!

  debugger; // you can put breakpoints anywhere to help debugging!

  {str}
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that strings inside the template need to be inside &lt;code&gt;{"string"}&lt;/code&gt;, you can't do &lt;code&gt;&amp;lt;div&amp;gt;hello&amp;lt;/div&amp;gt;&lt;/code&gt; as Ripple has no idea if &lt;code&gt;hello&lt;/code&gt; is a string or maybe some JavaScript code that needs evaluating, so just ensure you wrap them in curly braces. This shouldn't be an issue in the real-world anyway, as you'll likely use an i18n library that means using JavaScript expressions regardless.&lt;/p&gt; 
&lt;h3&gt;If statements&lt;/h3&gt; 
&lt;p&gt;If blocks work seamlessly with Ripple's templating language, you can put them inside the JSX-like statements, making control-flow far easier to read and reason with.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Truthy({ x }) {
  &amp;lt;div&amp;gt;
    if (x) {
      &amp;lt;span&amp;gt;{'x is truthy'}&amp;lt;/span&amp;gt;
    } else {
      &amp;lt;span&amp;gt;{'x is falsy'}&amp;lt;/span&amp;gt;
    }
  &amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For statements&lt;/h3&gt; 
&lt;p&gt;You can render collections using a &lt;code&gt;for...of&lt;/code&gt; block, and you don't need to specify a &lt;code&gt;key&lt;/code&gt; prop like other frameworks.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component ListView({ title, items }) {
  &amp;lt;h2&amp;gt;{title}&amp;lt;/h2&amp;gt;
  &amp;lt;ul&amp;gt;
    for (const item of items) {
      &amp;lt;li&amp;gt;{item.text}&amp;lt;/li&amp;gt;
    }
  &amp;lt;/ul&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use Ripple's reactive arrays to easily compose contents of an array.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { RippleArray } from 'ripple';

component Numbers() {
  const items = new RippleArray(1, 2, 3);

  for (const item of items) {
    &amp;lt;div&amp;gt;{item}&amp;lt;/div&amp;gt;
  }

  &amp;lt;button onClick={() =&amp;gt; items.push(`Item ${items.$length + 1}`)}&amp;gt;{"Add Item"}&amp;lt;/button&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Clicking the &lt;code&gt;&amp;lt;button&amp;gt;&lt;/code&gt; will create a new item, note that &lt;code&gt;items&lt;/code&gt; is not &lt;code&gt;$&lt;/code&gt; prefixed, because it's not reactive, but rather its properties are instead.&lt;/p&gt; 
&lt;h3&gt;Try statements&lt;/h3&gt; 
&lt;p&gt;Try blocks work to build the foundation for &lt;strong&gt;error boundaries&lt;/strong&gt;, when the runtime encounters an error in the &lt;code&gt;try&lt;/code&gt; block, you can easily render a fallback in the &lt;code&gt;catch&lt;/code&gt; block.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { reportError } from 'some-library';

component ErrorBoundary() {
  &amp;lt;div&amp;gt;
    try {
      &amp;lt;ComponentThatFails /&amp;gt;
    } catch (e) {
      reportError(e);

      &amp;lt;div&amp;gt;{'An error occurred! ' + e.message}&amp;lt;/div&amp;gt;
    }
  &amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Props&lt;/h3&gt; 
&lt;p&gt;If you want a prop to be reactive, you should also give it a &lt;code&gt;$&lt;/code&gt; prefix.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Button(props: { $text: string, onClick: () =&amp;gt; void }) {
  &amp;lt;button onClick={props.onClick}&amp;gt;
    {props.$text}
  &amp;lt;/button&amp;gt;
}

// Usage
&amp;lt;Button $text={some_text} onClick={() =&amp;gt; console.log("Clicked!")} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This also applies to DOM elements, if you want an attribute or property to be reactive, it needs to have a &lt;code&gt;$&lt;/code&gt; prefix.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-tsx"&gt;&amp;lt;div $class={props.$someClass} $id={$someId}&amp;gt;
  {$someText}
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Otherwise changes to the attribute or property will not be reactively updated.&lt;/p&gt; 
&lt;h3&gt;Children&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;$children&lt;/code&gt; prop and then use it in the form of &lt;code&gt;&amp;lt;$children /&amp;gt;&lt;/code&gt; for component composition.&lt;/p&gt; 
&lt;p&gt;When you pass in children to a component, it gets implicitly passed as the &lt;code&gt;$children&lt;/code&gt; prop, in the form of a component.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import type { Component } from 'ripple';

component Card(props: { $children: Component }) {
  &amp;lt;div class="card"&amp;gt;
    &amp;lt;props.$children /&amp;gt;
  &amp;lt;/div&amp;gt;
}

// Usage
&amp;lt;Card&amp;gt;
  &amp;lt;p&amp;gt;{"Card content here"}&amp;lt;/p&amp;gt;
&amp;lt;/Card&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You could also explicitly write the same code as shown:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import type { Component } from 'ripple';

component Card(props: { $children: Component }) {
  &amp;lt;div class="card"&amp;gt;
    &amp;lt;props.$children /&amp;gt;
  &amp;lt;/div&amp;gt;
}

// Usage with explicit component
&amp;lt;Card&amp;gt;
  component $children() {
    &amp;lt;p&amp;gt;{"Card content here"}&amp;lt;/p&amp;gt;
  }
&amp;lt;/Card&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessor Props&lt;/h3&gt; 
&lt;p&gt;When working with props on composite components (&lt;code&gt;&amp;lt;Foo&amp;gt;&lt;/code&gt; rather than &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt;), it can sometimes be difficult to debug why a certain value is a certain way. JavaScript gives us a way to do this on objects using the &lt;code&gt;get&lt;/code&gt; syntax:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;let name = 'Bob';

const object = {
  get name() {
    // I can easily debug when this property gets
    // access and track it easily
    console.log(name);
    return name;
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;So Ripple provides similar capabilities when working with composite components in a template, specifically using &lt;code&gt;$prop:={}&lt;/code&gt; rather than the typical &lt;code&gt;$prop={}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In fact, when you use an accessor, you must pass a function, and the prop must be &lt;code&gt;$&lt;/code&gt; prefixed, as Ripple considers accessor props as reactive:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

const getName = () =&amp;gt; {
  // I can easily debug when this property gets
  // access and track it easily
  console.log(name);
  return $name;
};

&amp;lt;Person $name:={getName} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also inline the function too:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

&amp;lt;Person $name:={() =&amp;gt; {
  // I can easily debug when this property gets
  // access and track it easily
  console.log(name);
  return $name;
}} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Furthermore, just like property accessors in JavaScript, Ripple provides a way of capturing the &lt;code&gt;set&lt;/code&gt; too, enabling two-way data-flow on composite component props. You just need to provide a second function after the first, separated using a comma:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

const getName = () =&amp;gt; {
  return $name;
}

const setName = (newName) =&amp;gt; {
  $name = newName;
}

&amp;lt;Person $name:={getName, setName} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or an inlined version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;let $name = 'Bob';

&amp;lt;Person $name:={() =&amp;gt; $name, (newName) =&amp;gt; $name = $newName} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now changes in the &lt;code&gt;Person&lt;/code&gt; to its &lt;code&gt;props&lt;/code&gt; will propagate to its parent component:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component Person(props) {
  const updateName = (newName) =&amp;gt; {
    props.$name = newName;
  }

  &amp;lt;NameInput onChange={updateName}&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Refs&lt;/h3&gt; 
&lt;p&gt;Ripple provides a consistent way to capture the underlying DOM element – refs. Specifically, using the syntax &lt;code&gt;{ref fn}&lt;/code&gt; where &lt;code&gt;fn&lt;/code&gt; is a function that captures the DOM element. If you're familiar with other frameworks, then this is identical to &lt;code&gt;{@attach fn}&lt;/code&gt; in Svelte 5 and somewhat similar to &lt;code&gt;ref&lt;/code&gt; in React. The hook function will receive the reference to the underlying DOM element.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;export component App() {
  let $node;

  const divRef = (node) =&amp;gt; {
    $node = node;
    console.log("mounted", node);

    return () =&amp;gt; {
      $node = undefined;
      console.log("unmounted", node);
    };
  };

  &amp;lt;div {ref divRef}&amp;gt;{"Hello world"}&amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also create &lt;code&gt;{ref}&lt;/code&gt; functions inline.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;export component App() {
  let $node;

  &amp;lt;div {ref (node) =&amp;gt; {
    $node = node;
    return () =&amp;gt; $node = null;
  }}&amp;gt;{"Hello world"}&amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use function factories to define properties, these are functions that return functions that do the same thing. However, you can use this pattern to pass reactive properties.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { fadeIn } from 'some-library';

export component App({ $ms }) {
  &amp;lt;div {ref fadeIn({ $ms })}&amp;gt;{"Hello world"}&amp;lt;/div&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Lastly, you can use refs on composite components.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;&amp;lt;Image {ref (node) =&amp;gt; console.log(node)} {...props} /&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When passing refs to composite components (rather than HTML elements) as shown above, they will be passed a &lt;code&gt;Symbol&lt;/code&gt; property, as they are not named. This still means that it can be spread to HTML template elements later on and still work.&lt;/p&gt; 
&lt;h4&gt;createRefKey&lt;/h4&gt; 
&lt;p&gt;Creates a unique object key that will be recognised as a ref when the object is spread onto an element. This allows programmatic assignment of refs without relying directly on the &lt;code&gt;{ref ...}&lt;/code&gt; template syntax.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { createRefKey } from 'ripple';

export component App() {
  let $value = '';

  const props = {
    id: "example",
    $value,
    [createRefKey()]: (node) =&amp;gt; {
      const removeListener = node.addEventListener('input', (e) =&amp;gt; $value = e.target.value);

      return () =&amp;gt; {
        removeListener();
      }
    }
  };

  // applied to an element
  &amp;lt;input type="text" {...props} /&amp;gt;

  // with composite component
  &amp;lt;Input {...props} /&amp;gt;
}

component Input({ id, $value, ...rest }) {
  &amp;lt;input type="text" {id} {$value} {...rest} /&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Event Props&lt;/h3&gt; 
&lt;p&gt;Like React, events are props that start with &lt;code&gt;on&lt;/code&gt; and then continue with an uppercase character, such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;onClick&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerMove&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerDown&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onKeyDown&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For &lt;code&gt;capture&lt;/code&gt; phase events, just add &lt;code&gt;Capture&lt;/code&gt; to the end of the prop name:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;onClickCapture&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerMoveCapture&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onPointerDownCapture&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;onKeyDownCapture&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Some events are automatically delegated where possible by Ripple to improve runtime performance.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Styling&lt;/h3&gt; 
&lt;p&gt;Ripple supports native CSS styling that is localized to the given component using the &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; element.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;component MyComponent() {
  &amp;lt;div class="container"&amp;gt;&amp;lt;h1&amp;gt;{'Hello World'}&amp;lt;/h1&amp;gt;&amp;lt;/div&amp;gt;

  &amp;lt;style&amp;gt;
    .container {
      background: blue;
      padding: 1rem;
    }

    h1 {
      color: white;
      font-size: 2rem;
    }
  &amp;lt;/style&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: the &lt;code&gt;&amp;lt;style&amp;gt;&lt;/code&gt; element must be top-level within a &lt;code&gt;component&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Context&lt;/h3&gt; 
&lt;p&gt;Ripple has the concept of &lt;code&gt;context&lt;/code&gt; where a value or reactive object can be shared through the component tree – like in other frameworks. This all happens from the &lt;code&gt;createContext&lt;/code&gt; function that is imported from &lt;code&gt;ripple&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;When you create a context, you can &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;set&lt;/code&gt; the values, but this must happen within the component. Using them outside will result in an error being thrown.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;import { createContext } from 'ripple';

const MyContext = createContext(null);

component Child() {
  // Context is read in the Child component
  const value = MyContext.get(MyContext);

  // value is "Hello from context!"
  console.log(value);
}

component Parent() {
  const value = MyContext.get(MyContext);

  // Context is read in the Parent component, but hasn't yet
  // been set, so we fallback to the initial context value.
  // So the value is `null`
  console.log(value);

  // Context is set in the Parent component
  MyContext.set("Hello from context!");

  &amp;lt;Child /&amp;gt;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are happy for your interest in contributing. Please see our &lt;a href="https://raw.githubusercontent.com/trueadm/ripple/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/trueadm/ripple/main/LICENSE"&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Physical-Intelligence/openpi</title>
      <link>https://github.com/Physical-Intelligence/openpi</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openpi&lt;/h1&gt; 
&lt;p&gt;openpi holds open-source models and packages for robotics, published by the &lt;a href="https://www.physicalintelligence.company/"&gt;Physical Intelligence team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, this repo contains three types of models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;π₀ model&lt;/a&gt;, a flow-based vision-language-action model (VLA).&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;π₀-FAST model&lt;/a&gt;, an autoregressive VLA, based on the FAST action tokenizer.&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;π₀.₅ model&lt;/a&gt;, an upgraded version of π₀ with better open-world generalization trained with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;. Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For all models, we provide &lt;em&gt;base model&lt;/em&gt; checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.&lt;/p&gt; 
&lt;p&gt;This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://droid-dataset.github.io/"&gt;DROID&lt;/a&gt;, and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Sept 2025] We released PyTorch support in openpi.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025]: We have added an &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md#data-filtering"&gt;improved idle filter&lt;/a&gt; for DROID training.&lt;/li&gt; 
 &lt;li&gt;[Jun 2025]: We have added &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md"&gt;instructions&lt;/a&gt; for using &lt;code&gt;openpi&lt;/code&gt; to train VLAs on the full &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;. This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring &lt;code&gt;fsdp_devices&lt;/code&gt; in the training config. Please also note that the current training script does not yet support multi-node training.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mode&lt;/th&gt; 
   &lt;th&gt;Memory Required&lt;/th&gt; 
   &lt;th&gt;Example GPU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 8 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (LoRA)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 22.5 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (Full)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 70 GB&lt;/td&gt; 
   &lt;td&gt;A100 (80GB) / H100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;When cloning this repo, make sure to update submodules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage Python dependencies. See the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installation instructions&lt;/a&gt; to set it up. Once uv is installed, run the following to set up the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: &lt;code&gt;GIT_LFS_SKIP_SMUDGE=1&lt;/code&gt; is needed to pull LeRobot as a dependency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/docker.md"&gt;Docker Setup&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Model Checkpoints&lt;/h2&gt; 
&lt;h3&gt;Base Models&lt;/h3&gt; 
&lt;p&gt;We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;π₀ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base autoregressive &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;π₀-FAST model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;π₀.₅ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-Tuned Models&lt;/h3&gt; 
&lt;p&gt;We also provide "expert" checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST-DROID&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$-FAST model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-DROID&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-towel&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can fold diverse towels 0-shot on ALOHA robot platforms&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_towel&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-tupperware&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can unpack food from a tupperware container&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_tupperware&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-pen-uncap&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on public &lt;a href="https://dit-policy.github.io/"&gt;ALOHA&lt;/a&gt; data: can uncap a pen&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-LIBERO&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned for the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO&lt;/a&gt; benchmark: gets state-of-the-art performance (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_libero&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-DROID&lt;/td&gt; 
   &lt;td&gt;Inference / Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt; with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;: fast inference and good language-following&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, checkpoints are automatically downloaded from &lt;code&gt;gs://openpi-assets&lt;/code&gt; and are cached in &lt;code&gt;~/.cache/openpi&lt;/code&gt; when needed. You can overwrite the download path by setting the &lt;code&gt;OPENPI_DATA_HOME&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Running Inference for a Pre-Trained Model&lt;/h2&gt; 
&lt;p&gt;Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = download.maybe_download("gs://openpi-assets/checkpoints/pi05_droid")

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    "observation/exterior_image_1_left": ...,
    "observation/wrist_image_left": ...,
    ...
    "prompt": "pick up the fork"
}
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also test this out in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/inference.ipynb"&gt;example notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README.md"&gt;DROID&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real/README.md"&gt;ALOHA&lt;/a&gt; robots.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Remote Inference&lt;/strong&gt;: We provide &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;examples and code&lt;/a&gt; for running inference of our models &lt;strong&gt;remotely&lt;/strong&gt;: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test inference without a robot&lt;/strong&gt;: We provide a &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;script&lt;/a&gt; for testing inference without a robot. This script will generate a random observation and run inference with the model. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fine-Tuning Base Models on Your Own Data&lt;/h2&gt; 
&lt;p&gt;We will fine-tune the $\pi_{0.5}$ model on the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO dataset&lt;/a&gt; as a running example for how to fine-tune a base model on your own data. We will explain three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Convert your data to a LeRobot dataset (which we use for training)&lt;/li&gt; 
 &lt;li&gt;Defining training configs and running training&lt;/li&gt; 
 &lt;li&gt;Spinning up a policy server and running inference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. Convert your data to a LeRobot dataset&lt;/h3&gt; 
&lt;p&gt;We provide a minimal example script for converting LIBERO data to a LeRobot dataset in &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/convert_libero_data_to_lerobot.py"&gt;&lt;code&gt;examples/libero/convert_libero_data_to_lerobot.py&lt;/code&gt;&lt;/a&gt;. You can easily modify it to convert your own data! You can download the raw LIBERO dataset from &lt;a href="https://huggingface.co/datasets/openvla/modified_libero_rlds"&gt;here&lt;/a&gt;, and run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.&lt;/p&gt; 
&lt;h3&gt;2. Defining training configs and running training&lt;/h3&gt; 
&lt;p&gt;To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/policies/libero_policy.py"&gt;&lt;code&gt;LiberoInputs&lt;/code&gt; and &lt;code&gt;LiberoOutputs&lt;/code&gt;&lt;/a&gt;: Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;LeRobotLiberoDataConfig&lt;/code&gt;&lt;/a&gt;: Defines how to process raw LIBERO data from LeRobot dataset for training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;TrainConfig&lt;/code&gt;&lt;/a&gt;: Defines fine-tuning hyperparameters, data config, and weight loader.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We provide example fine-tuning configs for &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;π₀&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;π₀-FAST&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;π₀.₅&lt;/a&gt; on LIBERO data.&lt;/p&gt; 
&lt;p&gt;Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/compute_norm_stats.py --config-name pi05_libero
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now we can kick off training with the following command (the &lt;code&gt;--overwrite&lt;/code&gt; flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command will log training progress to the console and save checkpoints to the &lt;code&gt;checkpoints&lt;/code&gt; directory. You can also monitor training progress on the Weights &amp;amp; Biases dashboard. For maximally using the GPU memory, set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We provide functionality for &lt;em&gt;reloading&lt;/em&gt; normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/norm_stats.md"&gt;norm_stats.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;3. Spinning up a policy server and running inference&lt;/h3&gt; 
&lt;p&gt;Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.&lt;/p&gt; 
&lt;p&gt;For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;remote inference docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;More Examples&lt;/h3&gt; 
&lt;p&gt;We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_sim"&gt;ALOHA Simulator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real"&gt;ALOHA Real&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/ur5"&gt;UR5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PyTorch Support&lt;/h2&gt; 
&lt;p&gt;openpi now provides PyTorch implementations of π₀ and π₀.₅ models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The π₀-FAST model&lt;/li&gt; 
 &lt;li&gt;Mixed precision training&lt;/li&gt; 
 &lt;li&gt;FSDP (fully-sharded data parallelism) training&lt;/li&gt; 
 &lt;li&gt;LoRA (low-rank adaptation) training&lt;/li&gt; 
 &lt;li&gt;EMA (exponential moving average) weights during training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have the latest version of all dependencies installed: &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double check that you have transformers 4.53.2 installed: &lt;code&gt;uv pip show transformers&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Apply the transformers library patches:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run &lt;code&gt;uv cache clean transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Converting JAX Models to PyTorch&lt;/h3&gt; 
&lt;p&gt;To convert a JAX model checkpoint to PyTorch format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &amp;lt;config name&amp;gt; \
    --output_path /path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Inference with PyTorch&lt;/h3&gt; 
&lt;p&gt;The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = "/path/to/converted/pytorch/checkpoint"

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Policy Server with PyTorch&lt;/h3&gt; 
&lt;p&gt;The policy server works identically with PyTorch models - just point to the converted checkpoint directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Finetuning with PyTorch&lt;/h3&gt; 
&lt;p&gt;To finetune a model in PyTorch:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Convert the JAX base model to PyTorch format:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --config_name &amp;lt;config name&amp;gt; \
    --checkpoint_dir /path/to/jax/base/model \
    --output_path /path/to/pytorch/base/model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the converted PyTorch model path in your config using &lt;code&gt;pytorch_weight_path&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch training using one of these modes:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training:
uv run scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&amp;lt;num_gpus&amp;gt; scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&amp;lt;num_nodes&amp;gt; \
    --nproc_per_node=&amp;lt;gpus_per_node&amp;gt; \
    --node_rank=&amp;lt;rank_of_node&amp;gt; \
    --master_addr=&amp;lt;master_ip&amp;gt; \
    --master_port=&amp;lt;port&amp;gt; \
    scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name=&amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Precision Settings&lt;/h3&gt; 
&lt;p&gt;JAX and PyTorch implementations handle precision as follows:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JAX:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: most weights and computations in bfloat16, with a few computations in float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting &lt;code&gt;dtype&lt;/code&gt; to float32 in the config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: supports either full bfloat16 (default) or full float32. You can change it by setting &lt;code&gt;pytorch_training_precision&lt;/code&gt; in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With torch.compile, inference speed is comparable between JAX and PyTorch.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can't find a solution, please file an issue on the repo (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/CONTRIBUTING.md"&gt;here&lt;/a&gt; for guidelines).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Issue&lt;/th&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uv sync&lt;/code&gt; fails with dependency conflicts&lt;/td&gt; 
   &lt;td&gt;Try removing the virtual environment directory (&lt;code&gt;rm -rf .venv&lt;/code&gt;) and running &lt;code&gt;uv sync&lt;/code&gt; again. If issues persist, check that you have the latest version of &lt;code&gt;uv&lt;/code&gt; installed (&lt;code&gt;uv self update&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training runs out of GPU memory&lt;/td&gt; 
   &lt;td&gt;Make sure you set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; (or higher) before running training to allow JAX to use more GPU memory. You can also use &lt;code&gt;--fsdp-devices &amp;lt;n&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;n&amp;gt;&lt;/code&gt; is your number of GPUs, to enable &lt;a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/"&gt;fully-sharded data parallelism&lt;/a&gt;, which reduces memory usage in exchange for slower training (the amount of slowdown depends on your particular setup). If you are still running out of memory, you may way to consider disabling EMA.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Policy server connection errors&lt;/td&gt; 
   &lt;td&gt;Check that the server is running and listening on the expected port. Verify network connectivity and firewall settings between client and server.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Missing norm stats error when training&lt;/td&gt; 
   &lt;td&gt;Run &lt;code&gt;scripts/compute_norm_stats.py&lt;/code&gt; with your config name before starting training.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset download fails&lt;/td&gt; 
   &lt;td&gt;Check your internet connection. For HuggingFace datasets, ensure you're logged in (&lt;code&gt;huggingface-cli login&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA/GPU errors&lt;/td&gt; 
   &lt;td&gt;Verify NVIDIA drivers are installed correctly. For Docker, ensure nvidia-container-toolkit is installed. Check GPU compatibility. You do NOT need CUDA libraries installed at a system level --- they will be installed via uv. You may even want to try &lt;em&gt;uninstalling&lt;/em&gt; system CUDA libraries if you run into CUDA issues, since system libraries can sometimes cause conflicts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Import errors when running examples&lt;/td&gt; 
   &lt;td&gt;Make sure you've installed all dependencies with &lt;code&gt;uv sync&lt;/code&gt;. Some examples may have additional requirements listed in their READMEs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Action dimensions mismatch&lt;/td&gt; 
   &lt;td&gt;Verify your data processing transforms match the expected input/output dimensions of your robot. Check the action space definitions in your policy classes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diverging training loss&lt;/td&gt; 
   &lt;td&gt;Check the &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, and &lt;code&gt;std&lt;/code&gt; values in &lt;code&gt;norm_stats.json&lt;/code&gt; for your dataset. Certain dimensions that are rarely used can end up with very small &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; values, leading to huge states and actions after normalization. You can manually adjust the norm stats as a workaround.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>microsoft/ai-agents-for-beginners</title>
      <link>https://github.com/microsoft/ai-agents-for-beginners</link>
      <description>&lt;p&gt;12 Lessons to Get Started Building AI Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Agents for Beginners - A Course&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/images/repo-thumbnailv2.png" alt="Generative AI For Beginners" /&gt;&lt;/p&gt; 
&lt;h2&gt;A course teaching everything you need to know to start building AI Agents&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/ai-agents-for-beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg?sanitize=true" alt="GitHub license" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg?sanitize=true" alt="GitHub contributors" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg?sanitize=true" alt="GitHub issues" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg?sanitize=true" alt="GitHub pull-requests" /&gt;&lt;/a&gt; &lt;a href="http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;🌐 Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fr/README.md"&gt;French&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/es/README.md"&gt;Spanish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/de/README.md"&gt;German&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ru/README.md"&gt;Russian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ar/README.md"&gt;Arabic&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fa/README.md"&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ur/README.md"&gt;Urdu&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/zh/README.md"&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/mo/README.md"&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hk/README.md"&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tw/README.md"&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ja/README.md"&gt;Japanese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ko/README.md"&gt;Korean&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hi/README.md"&gt;Hindi&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/bn/README.md"&gt;Bengali&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/mr/README.md"&gt;Marathi&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ne/README.md"&gt;Nepali&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pa/README.md"&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pt/README.md"&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/br/README.md"&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/it/README.md"&gt;Italian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pl/README.md"&gt;Polish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tr/README.md"&gt;Turkish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/el/README.md"&gt;Greek&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/th/README.md"&gt;Thai&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sv/README.md"&gt;Swedish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/da/README.md"&gt;Danish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/no/README.md"&gt;Norwegian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fi/README.md"&gt;Finnish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/nl/README.md"&gt;Dutch&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/he/README.md"&gt;Hebrew&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/vi/README.md"&gt;Vietnamese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/id/README.md"&gt;Indonesian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ms/README.md"&gt;Malay&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tl/README.md"&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sw/README.md"&gt;Swahili&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hu/README.md"&gt;Hungarian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/cs/README.md"&gt;Czech&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sk/README.md"&gt;Slovak&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ro/README.md"&gt;Romanian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/bg/README.md"&gt;Bulgarian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sr/README.md"&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hr/README.md"&gt;Croatian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sl/README.md"&gt;Slovenian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/uk/README.md"&gt;Ukrainian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/my/README.md"&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If you wish to have additional translations languages supported are listed &lt;a href="https://github.com/Azure/co-op-translator/raw/main/getting_started/supported-languages.md"&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Watch" alt="GitHub watchers" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Fork" alt="GitHub forks" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/kzRShWzttr"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/kzRShWzttr" alt="Azure AI Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🌱 Getting Started&lt;/h2&gt; 
&lt;p&gt;This course has lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!&lt;/p&gt; 
&lt;p&gt;There is multi-language support for this course. Go to our &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/#-multi-language-support"&gt;available languages here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If this is your first time building with Generative AI models, check out our &lt;a href="https://aka.ms/genai-beginners"&gt;Generative AI For Beginners&lt;/a&gt; course, which includes 21 lessons on building with GenAI.&lt;/p&gt; 
&lt;p&gt;Don't forget to &lt;a href="https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst"&gt;star (🌟) this repo&lt;/a&gt; and &lt;a href="https://github.com/microsoft/ai-agents-for-beginners/fork"&gt;fork this repo&lt;/a&gt; to run the code.&lt;/p&gt; 
&lt;h3&gt;Meet Other Learners, Get Your Questions Answered&lt;/h3&gt; 
&lt;p&gt;If you get stuck or have any questions about building AI Agents, join our dedicated Discord Channel in the &lt;a href="https://aka.ms/ai-agents/discord"&gt;Azure AI Foundry Community Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;What You Need&lt;/h3&gt; 
&lt;p&gt;Each lesson in this course includes code examples, which can be found in the code_samples folder. You can &lt;a href="https://github.com/microsoft/ai-agents-for-beginners/fork"&gt;fork this repo&lt;/a&gt; to create your own copy.&lt;/p&gt; 
&lt;p&gt;The code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents-beginners/github-models"&gt;Github Models&lt;/a&gt; - Free / Limited&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents-beginners/ai-foundry"&gt;Azure AI Foundry&lt;/a&gt; - Azure Account Required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This course also uses the following AI Agent frameworks and services from Microsoft:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents-beginners/ai-agent-service"&gt;Azure AI Agent Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents-beginners/semantic-kernel"&gt;Semantic Kernel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-agents/autogen"&gt;AutoGen&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more information on running the code for this course, go to the &lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/00-course-setup/README.md"&gt;Course Setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🙏 Want to help?&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? &lt;a href="https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst"&gt;Raise an issue&lt;/a&gt; or &lt;a href="https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst"&gt;Create a pull request&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📂 Each lesson includes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A written lesson located in the README and a short video&lt;/li&gt; 
 &lt;li&gt;Python code samples supporting Azure AI Foundry and Github Models (Free)&lt;/li&gt; 
 &lt;li&gt;Links to extra resources to continue your learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🗃️ Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Lesson&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Text &amp;amp; Code&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Extra Learning&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Intro to AI Agents and Agent Use Cases&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/01-intro-to-ai-agents/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/3zgm60bXmQk?si=z8QygFvYQv-9WtO1"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Exploring AI Agentic Frameworks&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/02-explore-agentic-frameworks/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Understanding AI Agentic Design Patterns&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/03-agentic-design-patterns/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tool Use Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/04-tool-use/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Agentic RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/05-agentic-rag/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Building Trustworthy AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/06-building-trustworthy-agents/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planning Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/07-planning-design/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi-Agent Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/08-multi-agent/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Metacognition Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/09-metacognition/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AI Agents in Production&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/10-ai-agents-production/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Using Agentic Protocols (MCP, A2A and NLWeb)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/11-agentic-protocols/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/X-Dh9R3Opn8"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Context Engineering for AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/12-context-engineering/README.md"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/F5zqRV7gEag"&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Managing Agentic Memory&lt;/td&gt; 
   &lt;td&gt;Coming - September 11th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Evaluating AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - September 18th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Building Computer Use Agents (CUA)&lt;/td&gt; 
   &lt;td&gt;Coming - September 25th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deploying Scalable Agents&lt;/td&gt; 
   &lt;td&gt;Coming - September 25th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Creating Local AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - October 3rd&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Securing AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - October 10th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;🎒 Other Courses&lt;/h2&gt; 
&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst"&gt;&lt;strong&gt;NEW&lt;/strong&gt; Model Context Protocol (MCP) For Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners using Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst"&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst"&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst"&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung"&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst"&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst"&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst"&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst"&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst"&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst"&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🌟 Community Thanks&lt;/h2&gt; 
&lt;p&gt;Thanks to &lt;a href="https://www.linkedin.com/in/shivam2003/"&gt;Shivam Goyal&lt;/a&gt; for contributing important code samples demonstrating Agentic RAG.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos is subject to those third-parties' policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>grpc/grpc-go</title>
      <link>https://github.com/grpc/grpc-go</link>
      <description>&lt;p&gt;The Go language implementation of gRPC. HTTP/2 based RPC&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gRPC-Go&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pkg.go.dev/google.golang.org/grpc"&gt;&lt;img src="https://pkg.go.dev/badge/google.golang.org/grpc" alt="GoDoc" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/grpc/grpc-go"&gt;&lt;img src="https://goreportcard.com/badge/grpc/grpc-go" alt="GoReportCard" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/grpc/grpc-go"&gt;&lt;img src="https://codecov.io/gh/grpc/grpc-go/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://golang.org"&gt;Go&lt;/a&gt; implementation of &lt;a href="https://grpc.io"&gt;gRPC&lt;/a&gt;: A high performance, open source, general RPC framework that puts mobile and HTTP/2 first. For more information see the &lt;a href="https://grpc.io/docs/languages/go"&gt;Go gRPC docs&lt;/a&gt;, or jump directly into the &lt;a href="https://grpc.io/docs/languages/go/quickstart"&gt;quick start&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://golang.org"&gt;Go&lt;/a&gt;&lt;/strong&gt;: any one of the &lt;strong&gt;two latest major&lt;/strong&gt; &lt;a href="https://golang.org/doc/devel/release.html"&gt;releases&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Simply add the following import to your code, and then &lt;code&gt;go [build|run|test]&lt;/code&gt; will automatically fetch the necessary dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-go"&gt;import "google.golang.org/grpc"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you are trying to access &lt;code&gt;grpc-go&lt;/code&gt; from &lt;strong&gt;China&lt;/strong&gt;, see the &lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/#FAQ"&gt;FAQ&lt;/a&gt; below.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Learn more&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://grpc.io/docs/languages/go"&gt;Go gRPC docs&lt;/a&gt;, which include a &lt;a href="https://grpc.io/docs/languages/go/quickstart"&gt;quick start&lt;/a&gt; and &lt;a href="https://pkg.go.dev/google.golang.org/grpc"&gt;API reference&lt;/a&gt; among other resources&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/Documentation"&gt;Low-level technical docs&lt;/a&gt; from this repository&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://performance-dot-grpc-testing.appspot.com/explore?dashboard=5180705743044608"&gt;Performance benchmark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/grpc/grpc-go/master/CONTRIBUTING.md"&gt;Contribution guidelines&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;I/O Timeout Errors&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;golang.org&lt;/code&gt; domain may be blocked from some countries. &lt;code&gt;go get&lt;/code&gt; usually produces an error like the following when this happens:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ go get -u google.golang.org/grpc
package google.golang.org/grpc: unrecognized import path "google.golang.org/grpc" (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To build Go code, there are several options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Set up a VPN and access google.golang.org through that.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;With Go module support: it is possible to use the &lt;code&gt;replace&lt;/code&gt; feature of &lt;code&gt;go mod&lt;/code&gt; to create aliases for golang.org packages. In your project's directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;go mod edit -replace=google.golang.org/grpc=github.com/grpc/grpc-go@latest
go mod tidy
go mod vendor
go build -mod=vendor
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Again, this will need to be done for all transitive dependencies hosted on golang.org as well. For details, refer to &lt;a href="https://github.com/golang/go/issues/28652"&gt;golang/go issue #28652&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Compiling error, undefined: grpc.SupportPackageIsVersion&lt;/h3&gt; 
&lt;p&gt;Please update to the latest version of gRPC-Go using &lt;code&gt;go get google.golang.org/grpc&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;How to turn on logging&lt;/h3&gt; 
&lt;p&gt;The default logger is controlled by environment variables. Turn everything on like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ export GRPC_GO_LOG_VERBOSITY_LEVEL=99
$ export GRPC_GO_LOG_SEVERITY_LEVEL=info
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;The RPC failed with error &lt;code&gt;"code = Unavailable desc = transport is closing"&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;This error means the connection the RPC is using was closed, and there are many possible reasons, including:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;mis-configured transport credentials, connection failed on handshaking&lt;/li&gt; 
 &lt;li&gt;bytes disrupted, possibly by a proxy in between&lt;/li&gt; 
 &lt;li&gt;server shutdown&lt;/li&gt; 
 &lt;li&gt;Keepalive parameters caused connection shutdown, for example if you have configured your server to terminate connections regularly to &lt;a href="https://github.com/grpc/grpc-go/issues/3170#issuecomment-552517779"&gt;trigger DNS lookups&lt;/a&gt;. If this is the case, you may want to increase your &lt;a href="https://pkg.go.dev/google.golang.org/grpc/keepalive?tab=doc#ServerParameters"&gt;MaxConnectionAgeGrace&lt;/a&gt;, to allow longer RPC calls to finish.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;It can be tricky to debug this because the error happens on the client side but the root cause of the connection being closed is on the server side. Turn on logging on &lt;strong&gt;both client and server&lt;/strong&gt;, and see if there are any transport errors.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Zie619/n8n-workflows</title>
      <link>https://github.com/Zie619/n8n-workflows</link>
      <description>&lt;p&gt;all of the workflows of n8n i could find (also from the site itself)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;⚡ N8N Workflow Collection &amp;amp; Documentation&lt;/h1&gt; 
&lt;p&gt;A professionally organized collection of &lt;strong&gt;2,053 n8n workflows&lt;/strong&gt; with a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;⚠️ IMPORTANT NOTICE (Aug 14, 2025):&lt;/strong&gt; Repository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see &lt;a href="https://github.com/Zie619/n8n-workflows/issues"&gt;Issue #X&lt;/a&gt; for instructions on syncing your copy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support My Work&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/zie619"&gt;&lt;img src="https://img.shields.io/badge/-Buy%20Me%20a%20Coffee-ffdd00?logo=buy-me-a-coffee&amp;amp;logoColor=black&amp;amp;style=flat" alt="Buy Me a Coffee" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you'd like to say thanks, consider buying me a coffee—your support helps me keep improving this project!&lt;/p&gt; 
&lt;h2&gt;🚀 &lt;strong&gt;NEW: High-Performance Documentation System&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Experience 100x performance improvement over traditional documentation!&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Quick Start - Fast Documentation System&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
pip install -r requirements.txt

# Start the fast API server
python run.py

# Open in browser
http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;⚡ &lt;strong&gt;Sub-100ms response times&lt;/strong&gt; with SQLite FTS5 search&lt;/li&gt; 
 &lt;li&gt;🔍 &lt;strong&gt;Instant full-text search&lt;/strong&gt; with advanced filtering&lt;/li&gt; 
 &lt;li&gt;📱 &lt;strong&gt;Responsive design&lt;/strong&gt; - works perfectly on mobile&lt;/li&gt; 
 &lt;li&gt;🌙 &lt;strong&gt;Dark/light themes&lt;/strong&gt; with system preference detection&lt;/li&gt; 
 &lt;li&gt;📊 &lt;strong&gt;Live statistics&lt;/strong&gt; - 365 unique integrations, 29,445 total nodes&lt;/li&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;Smart categorization&lt;/strong&gt; by trigger type and complexity&lt;/li&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;Use case categorization&lt;/strong&gt; by service name mapped to categories&lt;/li&gt; 
 &lt;li&gt;📄 &lt;strong&gt;On-demand JSON viewing&lt;/strong&gt; and download&lt;/li&gt; 
 &lt;li&gt;🔗 &lt;strong&gt;Mermaid diagram generation&lt;/strong&gt; for workflow visualization&lt;/li&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;Real-time workflow naming&lt;/strong&gt; with intelligent formatting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Metric&lt;/th&gt; 
   &lt;th&gt;Old System&lt;/th&gt; 
   &lt;th&gt;New System&lt;/th&gt; 
   &lt;th&gt;Improvement&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;File Size&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;71MB HTML&lt;/td&gt; 
   &lt;td&gt;&amp;lt;100KB&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;700x smaller&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Load Time&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;10+ seconds&lt;/td&gt; 
   &lt;td&gt;&amp;lt;1 second&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;10x faster&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Search&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Client-side only&lt;/td&gt; 
   &lt;td&gt;Full-text with FTS5&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Instant&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;~2GB RAM&lt;/td&gt; 
   &lt;td&gt;&amp;lt;50MB RAM&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;40x less&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mobile Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Poor&lt;/td&gt; 
   &lt;td&gt;Excellent&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Fully responsive&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📂 Repository Organization&lt;/h2&gt; 
&lt;h3&gt;Workflow Collection&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,053 workflows&lt;/strong&gt; with meaningful, searchable names&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; across popular platforms&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;29,445 total nodes&lt;/strong&gt; with professional categorization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality assurance&lt;/strong&gt; - All workflows analyzed and categorized&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Naming System ✨&lt;/h3&gt; 
&lt;p&gt;Our intelligent naming system converts technical filenames into readable titles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Before&lt;/strong&gt;: &lt;code&gt;2051_Telegram_Webhook_Automation_Webhook.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;After&lt;/strong&gt;: &lt;code&gt;Telegram Webhook Automation&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; with smart capitalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic integration detection&lt;/strong&gt; from node analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Use Case Category ✨&lt;/h3&gt; 
&lt;p&gt;The search interface includes a dropdown filter that lets you browse 2,000+ workflows by category.&lt;/p&gt; 
&lt;p&gt;The system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.&lt;/p&gt; 
&lt;h3&gt;How Categorization Works&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the categorization script&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python create_categories.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Service Name Recognition&lt;/strong&gt; The script analyzes each workflow JSON filename to identify recognized service names (e.g., "Twilio", "Slack", "Gmail", etc.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Category Mapping&lt;/strong&gt; Each recognized service name is matched to its corresponding category using the definitions in &lt;code&gt;context/def_categories.json&lt;/code&gt;. For example:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Twilio → Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Gmail → Communication &amp;amp; Messaging&lt;/li&gt; 
   &lt;li&gt;Airtable → Data Processing &amp;amp; Analysis&lt;/li&gt; 
   &lt;li&gt;Salesforce → CRM &amp;amp; Sales&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Search Categories Generation&lt;/strong&gt; The script produces a &lt;code&gt;search_categories.json&lt;/code&gt; file that contains the categorized workflow data&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Filter Interface&lt;/strong&gt; Users can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Available Categories&lt;/h3&gt; 
&lt;p&gt;The categorization system includes the following main categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Agent Development&lt;/li&gt; 
 &lt;li&gt;Business Process Automation&lt;/li&gt; 
 &lt;li&gt;Cloud Storage &amp;amp; File Management&lt;/li&gt; 
 &lt;li&gt;Communication &amp;amp; Messaging&lt;/li&gt; 
 &lt;li&gt;Creative Content &amp;amp; Video Automation&lt;/li&gt; 
 &lt;li&gt;Creative Design Automation&lt;/li&gt; 
 &lt;li&gt;CRM &amp;amp; Sales&lt;/li&gt; 
 &lt;li&gt;Data Processing &amp;amp; Analysis&lt;/li&gt; 
 &lt;li&gt;E-commerce &amp;amp; Retail&lt;/li&gt; 
 &lt;li&gt;Financial &amp;amp; Accounting&lt;/li&gt; 
 &lt;li&gt;Marketing &amp;amp; Advertising Automation&lt;/li&gt; 
 &lt;li&gt;Project Management&lt;/li&gt; 
 &lt;li&gt;Social Media Management&lt;/li&gt; 
 &lt;li&gt;Technical Infrastructure &amp;amp; DevOps&lt;/li&gt; 
 &lt;li&gt;Web Scraping &amp;amp; Data Extraction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contribute Categories&lt;/h3&gt; 
&lt;p&gt;You can help expand the categorization by adding more service-to-category mappings (e.g., Twilio → Communication &amp;amp; Messaging) in context/defs_categories.json.&lt;/p&gt; 
&lt;p&gt;Many workflow JSON files are conveniently named with the service name, often separated by underscores (_).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🛠 Usage Instructions&lt;/h2&gt; 
&lt;h3&gt;Option 1: Modern Fast System (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install Python dependencies
pip install -r requirements.txt

# Start the documentation server
python run.py

# Browse workflows at http://localhost:8000
# - Instant search across 2,053 workflows
# - Professional responsive interface
# - Real-time workflow statistics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Development Mode&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start with auto-reload for development
python run.py --dev

# Or specify custom host/port
python run.py --host 0.0.0.0 --port 3000

# Force database reindexing
python run.py --reindex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Import Workflows into n8n&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use the Python importer (recommended)
python import_workflows.py

# Or manually import individual workflows:
# 1. Open your n8n Editor UI
# 2. Click menu (☰) → Import workflow
# 3. Choose any .json file from the workflows/ folder
# 4. Update credentials/webhook URLs before running
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📊 Workflow Statistics&lt;/h2&gt; 
&lt;h3&gt;Current Collection Stats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Total Workflows&lt;/strong&gt;: 2,053 automation workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Active Workflows&lt;/strong&gt;: 215 (10.5% active rate)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Total Nodes&lt;/strong&gt;: 29,445 (avg 14.3 nodes per workflow)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unique Integrations&lt;/strong&gt;: 365 different services and APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: SQLite with FTS5 full-text search&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Trigger Distribution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Complex&lt;/strong&gt;: 831 workflows (40.5%) - Multi-trigger systems&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Webhook&lt;/strong&gt;: 519 workflows (25.3%) - API-triggered automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual&lt;/strong&gt;: 477 workflows (23.2%) - User-initiated workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scheduled&lt;/strong&gt;: 226 workflows (11.0%) - Time-based executions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complexity Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Low (≤5 nodes)&lt;/strong&gt;: ~35% - Simple automations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Medium (6-15 nodes)&lt;/strong&gt;: ~45% - Standard workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High (16+ nodes)&lt;/strong&gt;: ~20% - Complex enterprise systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Popular Integrations&lt;/h3&gt; 
&lt;p&gt;Top services by usage frequency:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Storage&lt;/strong&gt;: Google Drive, Google Sheets, Dropbox&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Databases&lt;/strong&gt;: PostgreSQL, MySQL, MongoDB, Airtable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI/ML&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: HTTP Request, Webhook, GraphQL&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🔍 Advanced Search Features&lt;/h2&gt; 
&lt;h3&gt;Smart Search Categories&lt;/h3&gt; 
&lt;p&gt;Our system automatically categorizes workflows into 12 service categories:&lt;/p&gt; 
&lt;h4&gt;Available Categories:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;messaging&lt;/strong&gt;: Telegram, Discord, Slack, WhatsApp, Teams&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ai_ml&lt;/strong&gt;: OpenAI, Anthropic, Hugging Face&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;database&lt;/strong&gt;: PostgreSQL, MySQL, MongoDB, Redis, Airtable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;email&lt;/strong&gt;: Gmail, Mailjet, Outlook, SMTP/IMAP&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;cloud_storage&lt;/strong&gt;: Google Drive, Google Docs, Dropbox, OneDrive&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;project_management&lt;/strong&gt;: Jira, GitHub, GitLab, Trello, Asana&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;social_media&lt;/strong&gt;: LinkedIn, Twitter/X, Facebook, Instagram&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ecommerce&lt;/strong&gt;: Shopify, Stripe, PayPal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;analytics&lt;/strong&gt;: Google Analytics, Mixpanel&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;calendar_tasks&lt;/strong&gt;: Google Calendar, Cal.com, Calendly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;forms&lt;/strong&gt;: Typeform, Google Forms, Form Triggers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;development&lt;/strong&gt;: Webhook, HTTP Request, GraphQL, SSE&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;API Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Search workflows by text
curl "http://localhost:8000/api/workflows?q=telegram+automation"

# Filter by trigger type and complexity
curl "http://localhost:8000/api/workflows?trigger=Webhook&amp;amp;complexity=high"

# Find all messaging workflows
curl "http://localhost:8000/api/workflows/category/messaging"

# Get database statistics
curl "http://localhost:8000/api/stats"

# Browse available categories
curl "http://localhost:8000/api/categories"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🏗 Technical Architecture&lt;/h2&gt; 
&lt;h3&gt;Modern Stack&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SQLite Database&lt;/strong&gt; - FTS5 full-text search with 365 indexed integrations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FastAPI Backend&lt;/strong&gt; - RESTful API with automatic OpenAPI documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responsive Frontend&lt;/strong&gt; - Modern HTML5 with embedded CSS/JavaScript&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Analysis&lt;/strong&gt; - Automatic workflow categorization and naming&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Change Detection&lt;/strong&gt; - MD5 hashing for efficient re-indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt; - Non-blocking workflow analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compressed Responses&lt;/strong&gt; - Gzip middleware for optimal speed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Error Handling&lt;/strong&gt; - Graceful degradation and comprehensive logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile Optimization&lt;/strong&gt; - Touch-friendly interface design&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Database Performance&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sql"&gt;-- Optimized schema for lightning-fast queries
CREATE TABLE workflows (
    id INTEGER PRIMARY KEY,
    filename TEXT UNIQUE,
    name TEXT,
    active BOOLEAN,
    trigger_type TEXT,
    complexity TEXT,
    node_count INTEGER,
    integrations TEXT,  -- JSON array of 365 unique services
    description TEXT,
    file_hash TEXT,     -- MD5 for change detection
    analyzed_at TIMESTAMP
);

-- Full-text search with ranking
CREATE VIRTUAL TABLE workflows_fts USING fts5(
    filename, name, description, integrations, tags,
    content='workflows', content_rowid='id'
);
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🔧 Setup &amp;amp; Requirements&lt;/h2&gt; 
&lt;h3&gt;System Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.7+&lt;/strong&gt; - For running the documentation system&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modern Browser&lt;/strong&gt; - Chrome, Firefox, Safari, Edge&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;50MB Storage&lt;/strong&gt; - For SQLite database and indexes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Instance&lt;/strong&gt; - For importing and running workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone &amp;lt;repo-url&amp;gt;
cd n8n-workflows

# Install dependencies
pip install -r requirements.txt

# Start documentation server
python run.py

# Access at http://localhost:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or .venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run with auto-reload for development
python api_server.py --reload

# Force database reindexing
python workflow_db.py --index --force
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📋 Naming Convention&lt;/h2&gt; 
&lt;h3&gt;Intelligent Formatting System&lt;/h3&gt; 
&lt;p&gt;Our system automatically converts technical filenames to user-friendly names:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Automatic transformations:
2051_Telegram_Webhook_Automation_Webhook.json → "Telegram Webhook Automation"
0250_HTTP_Discord_Import_Scheduled.json → "HTTP Discord Import Scheduled"  
0966_OpenAI_Data_Processing_Manual.json → "OpenAI Data Processing Manual"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Technical Format&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Smart Capitalization Rules&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP&lt;/strong&gt; → HTTP (not Http)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt; → API (not Api)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;webhook&lt;/strong&gt; → Webhook&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;automation&lt;/strong&gt; → Automation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;scheduled&lt;/strong&gt; → Scheduled&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 API Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Endpoints&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /&lt;/code&gt; - Main workflow browser interface&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/stats&lt;/code&gt; - Database statistics and metrics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows&lt;/code&gt; - Search with filters and pagination&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}&lt;/code&gt; - Detailed workflow information&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/download&lt;/code&gt; - Download workflow JSON&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/{filename}/diagram&lt;/code&gt; - Generate Mermaid diagram&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Search&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/workflows/category/{category}&lt;/code&gt; - Search by service category&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/categories&lt;/code&gt; - List all available categories&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GET /api/integrations&lt;/code&gt; - Get integration statistics&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;POST /api/reindex&lt;/code&gt; - Trigger background reindexing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Response Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;// GET /api/stats
{
  "total": 2053,
  "active": 215,
  "inactive": 1838,
  "triggers": {
    "Complex": 831,
    "Webhook": 519,
    "Manual": 477,
    "Scheduled": 226
  },
  "total_nodes": 29445,
  "unique_integrations": 365
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🤝 Contributing&lt;/h2&gt; 
&lt;h3&gt;Adding New Workflows&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Export workflow&lt;/strong&gt; as JSON from n8n&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Name descriptively&lt;/strong&gt; following the established pattern&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add to workflows/&lt;/strong&gt; directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Remove sensitive data&lt;/strong&gt; (credentials, personal URLs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Run reindexing&lt;/strong&gt; to update the database&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Quality Standards&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ Workflow must be functional and tested&lt;/li&gt; 
 &lt;li&gt;✅ Remove all credentials and sensitive data&lt;/li&gt; 
 &lt;li&gt;✅ Follow naming convention for consistency&lt;/li&gt; 
 &lt;li&gt;✅ Verify compatibility with recent n8n versions&lt;/li&gt; 
 &lt;li&gt;✅ Include meaningful description or comments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⚠️ Important Notes&lt;/h2&gt; 
&lt;h3&gt;Security &amp;amp; Privacy&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Review before use&lt;/strong&gt; - All workflows shared as-is for educational purposes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Update credentials&lt;/strong&gt; - Replace API keys, tokens, and webhooks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test safely&lt;/strong&gt; - Verify in development environment first&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check permissions&lt;/strong&gt; - Ensure proper access rights for integrations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Compatibility&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;n8n Version&lt;/strong&gt; - Compatible with n8n 1.0+ (most workflows)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Nodes&lt;/strong&gt; - Some workflows may require additional node installations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API Changes&lt;/strong&gt; - External services may have updated their APIs since creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt; - Verify required integrations before importing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📚 Resources &amp;amp; References&lt;/h2&gt; 
&lt;h3&gt;Workflow Sources&lt;/h3&gt; 
&lt;p&gt;This comprehensive collection includes workflows from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Official n8n.io&lt;/strong&gt; - Documentation and community examples&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub repositories&lt;/strong&gt; - Open source community contributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Blog posts &amp;amp; tutorials&lt;/strong&gt; - Real-world automation patterns&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User submissions&lt;/strong&gt; - Tested and verified workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise use cases&lt;/strong&gt; - Business process automations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Learn More&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/"&gt;n8n Documentation&lt;/a&gt; - Official documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://community.n8n.io/"&gt;n8n Community&lt;/a&gt; - Community forum and support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://n8n.io/workflows/"&gt;Workflow Templates&lt;/a&gt; - Official template library&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.n8n.io/integrations/"&gt;Integration Docs&lt;/a&gt; - Service-specific guides&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🏆 Project Achievements&lt;/h2&gt; 
&lt;h3&gt;Repository Transformation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2,053 workflows&lt;/strong&gt; professionally organized and named&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;365 unique integrations&lt;/strong&gt; automatically detected and categorized&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% meaningful names&lt;/strong&gt; (improved from basic filename patterns)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero data loss&lt;/strong&gt; during intelligent renaming process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced search&lt;/strong&gt; with 12 service categories&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance Revolution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Sub-100ms search&lt;/strong&gt; with SQLite FTS5 full-text indexing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant filtering&lt;/strong&gt; across 29,445 workflow nodes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mobile-optimized&lt;/strong&gt; responsive design for all devices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time statistics&lt;/strong&gt; with live database queries&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Professional interface&lt;/strong&gt; with modern UX principles&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Reliability&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robust error handling&lt;/strong&gt; with graceful degradation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Change detection&lt;/strong&gt; for efficient database updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background processing&lt;/strong&gt; for non-blocking operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive logging&lt;/strong&gt; for debugging and monitoring&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Production-ready&lt;/strong&gt; with proper middleware and security&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;This repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🎯 Perfect for&lt;/strong&gt;: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Zie619/n8n-workflows/main/README_ZH.md"&gt;中文&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CorentinJ/Real-Time-Voice-Cloning</title>
      <link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link>
      <description>&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; 
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href="https://matheo.uliege.be/handle/2268.2/6801"&gt;master's thesis&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA"&gt;&lt;img src="https://i.imgur.com/8lFUlgz.png" alt="Toolbox demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Papers implemented&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;Designation&lt;/th&gt; 
   &lt;th&gt;Title&lt;/th&gt; 
   &lt;th&gt;Implementation source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf"&gt;1802.08435&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; 
   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1703.10135.pdf"&gt;1703.10135&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; 
   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf"&gt;1710.10467&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GE2E (encoder)&lt;/td&gt; 
   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Heads up&lt;/h2&gt; 
&lt;p&gt;Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out &lt;a href="https://paperswithcode.com/task/speech-synthesis/"&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;Chatterbox&lt;/a&gt; for a similar project up to date with the 2025 SOTA in voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;h3&gt;1. Install Requirements&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.&lt;/li&gt; 
 &lt;li&gt;Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using &lt;code&gt;venv&lt;/code&gt;, but this is optional.&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://ffmpeg.org/download.html#get-packages"&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files.&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt;. Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.&lt;/li&gt; 
 &lt;li&gt;Install the remaining requirements with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. (Optional) Download Pretrained Models&lt;/h3&gt; 
&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;3. (Optional) Test Configuration&lt;/h3&gt; 
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt; 
&lt;h3&gt;4. (Optional) Download Datasets&lt;/h3&gt; 
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="https://www.openslr.org/resources/12/train-clean-100.tar.gz"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt; 
&lt;h3&gt;5. Launch the Toolbox&lt;/h3&gt; 
&lt;p&gt;You can then try the toolbox:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br /&gt; or&lt;br /&gt; &lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zama-ai/fhevm</title>
      <link>https://github.com/zama-ai/fhevm</link>
      <description>&lt;p&gt;FHEVM, a full-stack framework for integrating Fully Homomorphic Encryption (FHE) with blockchain applications&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="docs/.gitbook/assets/fhevm-header-dark.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="docs/.gitbook/assets/fhevm-header-light.png" /&gt; 
  &lt;img width="500" alt="fhevm" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/fhevm-whitepaper.pdf"&gt; 📃 Read white paper&lt;/a&gt; |&lt;a href="https://docs.zama.ai/protocol"&gt; 📒 Documentation&lt;/a&gt; | &lt;a href="https://zama.ai/community"&gt; 💛 Community support&lt;/a&gt; | &lt;a href="https://github.com/zama-ai/awesome-zama"&gt; 📚 FHE resources by Zama&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/zama-ai/fhevm/releases"&gt; &lt;img src="https://img.shields.io/github/v/release/zama-ai/fhevm?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zama-ai/fhevm/raw/main/LICENSE"&gt; 
  &lt;!-- markdown-link-check-disable-next-line --&gt; &lt;img src="https://img.shields.io/badge/License-BSD--3--Clause--Clear-%23ffb243?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zama-ai/bounty-program"&gt; 
  &lt;!-- markdown-link-check-disable-next-line --&gt; &lt;img src="https://img.shields.io/badge/Contribute-Zama%20Bounty%20Program-%23ffd208?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://slsa.dev"&gt;&lt;img alt="SLSA 3" src="https://slsa.dev/images/gh-badge-level3.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;h3&gt;What is FHEVM?&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;FHEVM&lt;/strong&gt; is the core framework of the &lt;em&gt;Zama Confidential Blockchain Protocol&lt;/em&gt;. It enables confidential smart contracts on EVM-compatible blockchains by leveraging Fully Homomorphic Encryption (FHE), allowing encrypted data to be processed directly onchain.&lt;/p&gt; 
&lt;p&gt;FHEVM ensures both confidentiality and composability, with the following guarantees:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end encryption of transactions and state:&lt;/strong&gt; Data included in transactions is encrypted and never visible to anyone.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Composability and data availability on-chain:&lt;/strong&gt; States are updated while remaining encrypted at all times.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No impact on existing dApps and state:&lt;/strong&gt; Encrypted state co-exists alongside public one, and doesn't impact existing dApps. &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Table of contents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#about"&gt;About&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#what-is-fhevm"&gt;What is FHEVM?&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#project-structure"&gt;Project structure&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#main-features"&gt;Main features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#use-cases"&gt;Use cases&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#resources"&gt;Resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#working-with-fhevm"&gt;Working with FHEVM&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#citations"&gt;Citations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#support"&gt;Support&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Project structure&lt;/h3&gt; 
&lt;p&gt;The directories of this repository are organized in the following way:&lt;/p&gt; 
&lt;h6&gt;FHEVM Contracts&lt;/h6&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;gateway-contracts/&lt;/code&gt;&lt;/strong&gt;: Smart contracts managing the gateway between on-chain and off-chain components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;host-contracts/&lt;/code&gt;&lt;/strong&gt;: Smart Contracts deployed on the host chain for orchestrating FHE workflows.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h6&gt;FHEVM Compute Engines&lt;/h6&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;coprocessor/&lt;/code&gt;&lt;/strong&gt;: Rust-based coprocessor implementation for FHE operations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;kms-connector/&lt;/code&gt;&lt;/strong&gt;: Interface for integrating with Key Management Services (KMS) to handle encryption keys securely.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h6&gt;FHEVM Utilities&lt;/h6&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;charts/&lt;/code&gt;&lt;/strong&gt;: Helm charts and deployment configurations for the stack.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;golden-container-images/&lt;/code&gt;&lt;/strong&gt;: Docker golden images for Node.js and Rust environments used as base images by the stack.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;test-suite/&lt;/code&gt;&lt;/strong&gt;: Integration with docker-compose and tests covering end-to-end FHEVM stack behavior.&lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Main features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Privacy by design:&lt;/strong&gt; Building decentralized apps with full privacy and confidentiality on Ethereum, leveraging FHE.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Solidity integration:&lt;/strong&gt; Write FHEVM contracts like any standard Solidity contract using Solidity. Compatible with existing toolchains — such as Hardhat and Foundry (&lt;em&gt;coming soon&lt;/em&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Programmable privacy:&lt;/strong&gt; Define exactly what data is encrypted and write the access control logic directly in your smart contracts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High precision encrypted integers :&lt;/strong&gt; Up to 256 bits of precision for integers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full range of operators:&lt;/strong&gt; All typical operators are available: &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;==&lt;/code&gt;, ternary-if, boolean operations…. Consecutive FHE operations are not limited.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security:&lt;/strong&gt; The underlying FHE crypto-scheme of FHEVM is quantum-resistant. Decryption is managed via a key management system (KMS) using multi-party computation (MPC), ensuring security even if some parties are compromised or misbehaving.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Symbolic execution of FHE computations:&lt;/strong&gt; All FHE operations are executed symbolically on the host chain, significantly reducing execution time. The actual computations on encrypted data are offloaded asynchronously to our coprocessor, allowing for faster, efficient, and scalable processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Learn more about FHEVM features in the &lt;a href="https://docs.zama.ai/protocol"&gt;documentation&lt;/a&gt; and in our &lt;a href="https://github.com/zama-ai/fhevm/raw/main/fhevm-whitepaper.pdf"&gt;whitepaper&lt;/a&gt;.&lt;/em&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;Use cases&lt;/h3&gt; 
&lt;p&gt;FHEVM is built for developers to write confidential smart contracts without the need to learn cryptography. Leveraging FHEVM, you can unlock a myriad of new use cases such as DeFi, gaming, and more. For instance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Confidential transfers&lt;/strong&gt;: Keep balances and amounts private, without using mixers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tokenization&lt;/strong&gt;: Swap tokens and RWAs on-chain without others seeing the amounts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Blind auctions&lt;/strong&gt;: Bid on items without revealing the amount or the winner.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-chain games&lt;/strong&gt;: Keep moves, selections, cards, or items hidden until ready to reveal.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Confidential voting&lt;/strong&gt;: Prevents bribery and blackmailing by keeping votes private.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Encrypted DIDs&lt;/strong&gt;: Store identities on-chain and generate attestations without ZK.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Learn more use cases in the &lt;a href="https://docs.zama.ai/protocol/examples"&gt;list of examples&lt;/a&gt;.&lt;/em&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.zama.ai/protocol"&gt;Documentation&lt;/a&gt; — Official documentation of FHEVM.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/fhevm-whitepaper.pdf"&gt;Whitepaper&lt;/a&gt; — Technical overview of FHEVM's cryptographic design.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.zama.ai/protocol/examples"&gt;Examples&lt;/a&gt; — Examples of building confidential smart contracts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zama-ai/awesome-zama?tab=readme-ov-file#fhevm"&gt;Awesome Zama – FHEVM&lt;/a&gt; — Curated articles, talks, and ecosystem projects.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt; &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#about"&gt; ↑ Back to top &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Working with FHEVM&lt;/h2&gt; 
&lt;h3&gt;Citations&lt;/h3&gt; 
&lt;p&gt;To cite FHEVM or the whitepaper in academic papers, please use the following entries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;@Misc{FHEVM,
title={{FHEVM: A full-stack framework for integrating Fully Homomorphic Encryption (FHE) with blockchain applications},
author={Zama},
year={2025},
note={\url{https://github.com/zama-ai/fhevm}},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;There are two ways to contribute to FHEVM:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zama-ai/fhevm/issues/new/choose"&gt;Open issues&lt;/a&gt; to report bugs and typos, or to suggest new ideas&lt;/li&gt; 
 &lt;li&gt;Request to become an official contributor by emailing &lt;a href="mailto:hello@zama.ai"&gt;hello@zama.ai&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Becoming an approved contributor involves signing our Contributor License Agreement (CLA). Only approved contributors can send pull requests, so please make sure to get in touch before you do! &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;This software is distributed under the &lt;strong&gt;BSD-3-Clause-Clear&lt;/strong&gt; license. Read &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/LICENSE"&gt;this&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;FAQ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Is Zama’s technology free to use?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Zama’s libraries are free to use under the BSD 3-Clause Clear license only for development, research, prototyping, and experimentation purposes. However, for any commercial use of Zama's open source code, companies must purchase Zama’s commercial patent license.&lt;/p&gt; 
 &lt;p&gt;Everything we do is open source, and we are very transparent on what it means for our users, you can read more about how we monetize our open source products at Zama in &lt;a href="https://www.zama.ai/post/open-source"&gt;this blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;What do I need to do if I want to use Zama’s technology for commercial purposes?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;To commercially use Zama’s technology you need to be granted Zama’s patent license. Please contact us at &lt;a href="mailto:hello@zama.ai"&gt;hello@zama.ai&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Do you file IP on your technology?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Yes, all Zama’s technologies are patented.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Can you customize a solution for my specific use case?&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We are open to collaborating and advancing the FHE space with our partners. If you have specific needs, please email us at &lt;a href="mailto:hello@zama.ai"&gt;hello@zama.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;a target="_blank" href="https://community.zama.ai"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="docs/.gitbook/assets/support-banner-dark.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="docs/.gitbook/assets/support-banner-light.png" /&gt; 
  &lt;img alt="Support" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;🌟 If you find this project helpful or interesting, please consider giving it a star on GitHub! Your support helps to grow the community and motivates further development.&lt;/p&gt; 
&lt;p align="right"&gt; &lt;a href="https://raw.githubusercontent.com/zama-ai/fhevm/main/#about"&gt; ↑ Back to top &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ml-explore/mlx-lm</title>
      <link>https://github.com/ml-explore/mlx-lm</link>
      <description>&lt;p&gt;Run LLMs with MLX&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;MLX LM&lt;/h2&gt; 
&lt;p&gt;MLX LM is a Python package for generating text and fine-tuning large language models on Apple silicon with MLX.&lt;/p&gt; 
&lt;p&gt;Some key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integration with the Hugging Face Hub to easily use thousands of LLMs with a single command.&lt;/li&gt; 
 &lt;li&gt;Support for quantizing and uploading models to the Hugging Face Hub.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/LORA.md"&gt;Low-rank and full model fine-tuning&lt;/a&gt; with support for quantized models.&lt;/li&gt; 
 &lt;li&gt;Distributed inference and fine-tuning with &lt;code&gt;mx.distributed&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The easiest way to get started is to install the &lt;code&gt;mlx-lm&lt;/code&gt; package:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;To generate text with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate --prompt "How tall is Mt Everest?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To chat with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.chat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will give you a chat REPL that you can use to interact with the LLM. The chat context is preserved during the lifetime of the REPL.&lt;/p&gt; 
&lt;p&gt;Commands in &lt;code&gt;mlx-lm&lt;/code&gt; typically take command line options which let you specify the model, sampling parameters, and more. Use &lt;code&gt;-h&lt;/code&gt; to see a list of available options for a command, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default model for generation and chat is &lt;code&gt;mlx-community/Llama-3.2-3B-Instruct-4bit&lt;/code&gt;. You can specify any MLX-compatible model with the &lt;code&gt;--model&lt;/code&gt; flag. Thousands are available in the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Community&lt;/a&gt; Hugging Face organization.&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;You can use &lt;code&gt;mlx-lm&lt;/code&gt; as a module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

text = generate(model, tokenizer, prompt=prompt, verbose=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(generate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/generate_response.py"&gt;generation example&lt;/a&gt; to see how to use the API in more detail. Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/batch_generate_response.py"&gt;batch generation example&lt;/a&gt; to see how to efficiently generate continuations for a batch of prompts.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;mlx-lm&lt;/code&gt; package also comes with functionality to quantize and optionally upload models to the Hugging Face Hub.&lt;/p&gt; 
&lt;p&gt;You can convert models using the Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import convert

repo = "mistralai/Mistral-7B-Instruct-v0.3"
upload_repo = "mlx-community/My-Mistral-7B-Instruct-v0.3-4bit"

convert(repo, quantize=True, upload_repo=upload_repo)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will generate a 4-bit quantized Mistral 7B and upload it to the repo &lt;code&gt;mlx-community/My-Mistral-7B-Instruct-v0.3-4bit&lt;/code&gt;. It will also save the converted model in the path &lt;code&gt;mlx_model&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(convert)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Streaming&lt;/h4&gt; 
&lt;p&gt;For streaming generation, use the &lt;code&gt;stream_generate&lt;/code&gt; function. This yields a generation response object.&lt;/p&gt; 
&lt;p&gt;For example,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, stream_generate

repo = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
model, tokenizer = load(repo)

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

for response in stream_generate(model, tokenizer, prompt, max_tokens=512):
    print(response.text, end="", flush=True)
print()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Sampling&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;generate&lt;/code&gt; and &lt;code&gt;stream_generate&lt;/code&gt; functions accept &lt;code&gt;sampler&lt;/code&gt; and &lt;code&gt;logits_processors&lt;/code&gt; keyword arguments. A sampler is any callable which accepts a possibly batched logits array and returns an array of sampled tokens. The &lt;code&gt;logits_processors&lt;/code&gt; must be a list of callables which take the token history and current logits as input and return the processed logits. The logits processors are applied in order.&lt;/p&gt; 
&lt;p&gt;Some standard sampling functions and logits processors are provided in &lt;code&gt;mlx_lm.sample_utils&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;p&gt;You can also use &lt;code&gt;mlx-lm&lt;/code&gt; from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt "hello"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will download a Mistral 7B model from the Hugging Face Hub and generate text using the given prompt.&lt;/p&gt; 
&lt;p&gt;For a full list of options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To quantize a model from the command line run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can upload new models to Hugging Face by specifying &lt;code&gt;--upload-repo&lt;/code&gt; to &lt;code&gt;convert&lt;/code&gt;. For example, to upload a quantized Mistral-7B model to the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Hugging Face community&lt;/a&gt; you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert \
    --hf-path mistralai/Mistral-7B-Instruct-v0.3 \
    -q \
    --upload-repo mlx-community/my-4bit-mistral
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models can also be converted and quantized directly in the &lt;a href="https://huggingface.co/spaces/mlx-community/mlx-my-repo"&gt;mlx-my-repo&lt;/a&gt; Hugging Face Space.&lt;/p&gt; 
&lt;h3&gt;Long Prompts and Generations&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; has some tools to scale efficiently to long prompts and generations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A rotating fixed-size key-value cache.&lt;/li&gt; 
 &lt;li&gt;Prompt caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use the rotating key-value cache pass the argument &lt;code&gt;--max-kv-size n&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; can be any integer. Smaller values like &lt;code&gt;512&lt;/code&gt; will use very little RAM but result in worse quality. Larger values like &lt;code&gt;4096&lt;/code&gt; or higher will use more RAM but have better quality.&lt;/p&gt; 
&lt;p&gt;Caching prompts can substantially speedup reusing the same long context with different queries. To cache a prompt use &lt;code&gt;mlx_lm.cache_prompt&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat prompt.txt | mlx_lm.cache_prompt \
  --model mistralai/Mistral-7B-Instruct-v0.3 \
  --prompt - \
  --prompt-cache-file mistral_prompt.safetensors
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then use the cached prompt with &lt;code&gt;mlx_lm.generate&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate \
    --prompt-cache-file mistral_prompt.safetensors \
    --prompt "\nSummarize the above text."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The cached prompt is treated as a prefix to the supplied prompt. Also notice when using a cached prompt, the model to use is read from the cache and need not be supplied explicitly.&lt;/p&gt; 
&lt;p&gt;Prompt caching can also be used in the Python API in order to avoid recomputing the prompt. This is useful in multi-turn dialogues or across requests that use the same context. See the &lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/examples/chat.py"&gt;example&lt;/a&gt; for more usage details.&lt;/p&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; supports thousands of Hugging Face format LLMs. If the model you want to run is not supported, file an &lt;a href="https://github.com/ml-explore/mlx-lm/issues/new"&gt;issue&lt;/a&gt; or better yet, submit a pull request.&lt;/p&gt; 
&lt;p&gt;Here are a few examples of Hugging Face models that work with this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-7B-v0.1"&gt;mistralai/Mistral-7B-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-2-7b-hf"&gt;meta-llama/Llama-2-7b-hf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct"&gt;deepseek-ai/deepseek-coder-6.7b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/01-ai/Yi-6B-Chat"&gt;01-ai/Yi-6B-Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/phi-2"&gt;microsoft/phi-2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"&gt;mistralai/Mixtral-8x7B-Instruct-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-7B"&gt;Qwen/Qwen-7B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b"&gt;pfnet/plamo-13b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b-instruct"&gt;pfnet/plamo-13b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b"&gt;stabilityai/stablelm-2-zephyr-1_6b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/internlm/internlm2-7b"&gt;internlm/internlm2-7b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/tiiuae/falcon-mamba-7b-instruct"&gt;tiiuae/falcon-mamba-7b-instruct&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mistral&amp;amp;sort=trending"&gt;Mistral&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=llama&amp;amp;sort=trending"&gt;Llama&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=phi&amp;amp;sort=trending"&gt;Phi-2&lt;/a&gt;, and &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mixtral&amp;amp;sort=trending"&gt;Mixtral&lt;/a&gt; style models should work out of the box.&lt;/p&gt; 
&lt;p&gt;For some models (such as &lt;code&gt;Qwen&lt;/code&gt; and &lt;code&gt;plamo&lt;/code&gt;) the tokenizer requires you to enable the &lt;code&gt;trust_remote_code&lt;/code&gt; option. You can do this by passing &lt;code&gt;--trust-remote-code&lt;/code&gt; in the command line. If you don't specify the flag explicitly, you will be prompted to trust remote code in the terminal when running the model.&lt;/p&gt; 
&lt;p&gt;For &lt;code&gt;Qwen&lt;/code&gt; models you must also specify the &lt;code&gt;eos_token&lt;/code&gt;. You can do this by passing &lt;code&gt;--eos-token "&amp;lt;|endoftext|&amp;gt;"&lt;/code&gt; in the command line.&lt;/p&gt; 
&lt;p&gt;These options can also be set in the Python API. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model, tokenizer = load(
    "qwen/Qwen-7B",
    tokenizer_config={"eos_token": "&amp;lt;|endoftext|&amp;gt;", "trust_remote_code": True},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Large Models&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This requires macOS 15.0 or higher to work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Models which are large relative to the total RAM available on the machine can be slow. &lt;code&gt;mlx-lm&lt;/code&gt; will attempt to make them faster by wiring the memory occupied by the model and cache. This requires macOS 15 or higher to work.&lt;/p&gt; 
&lt;p&gt;If you see the following warning message:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[WARNING] Generating with a model that requires ...&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;then the model will likely be slow on the given machine. If the model fits in RAM then it can often be sped up by increasing the system wired memory limit. To increase the limit, set the following &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo sysctl iogpu.wired_limit_mb=N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The value &lt;code&gt;N&lt;/code&gt; should be larger than the size of the model in megabytes but smaller than the memory size of the machine.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>