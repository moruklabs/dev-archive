<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Wed, 17 Sep 2025 02:42:27 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>hiroi-sora/Umi-OCR</title>
      <link>https://github.com/hiroi-sora/Umi-OCR</link>
      <description>&lt;p&gt;OCR software, free and offline. å¼€æºã€å…è´¹çš„ç¦»çº¿OCRè½¯ä»¶ã€‚æ”¯æŒæˆªå±/æ‰¹é‡å¯¼å…¥å›¾ç‰‡ï¼ŒPDFæ–‡æ¡£è¯†åˆ«ï¼Œæ’é™¤æ°´å°/é¡µçœ‰é¡µè„šï¼Œæ‰«æ/ç”ŸæˆäºŒç»´ç ã€‚å†…ç½®å¤šå›½è¯­è¨€åº“ã€‚&lt;/p&gt;&lt;hr&gt;&lt;p align="left"&gt; &lt;span&gt; &lt;b&gt;ä¸­æ–‡&lt;/b&gt; &lt;/span&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/README_en.md"&gt; English &lt;/a&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/README_ja.md"&gt; æ—¥æœ¬èª &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt; &lt;img width="200" height="128" src="https://tupian.li/images/2022/10/27/icon---256.png" alt="Umi-OCR" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Umi-OCR æ–‡å­—è¯†åˆ«å·¥å…·&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/releases/latest"&gt; &lt;img src="https://img.shields.io/github/v/release/hiroi-sora/Umi-OCR?style=flat-square" alt="Umi-OCR" /&gt; &lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/hiroi-sora/Umi-OCR?style=flat-square" alt="LICENSE" /&gt; &lt;/a&gt; &lt;a href="#ä¸‹è½½å‘è¡Œç‰ˆ"&gt; &lt;img src="https://img.shields.io/github/downloads/hiroi-sora/Umi-OCR/total?style=flat-square" alt="forks" /&gt; &lt;/a&gt; &lt;a href="https://star-history.com/#hiroi-sora/Umi-OCR"&gt; &lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR?style=flat-square" alt="stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/forks"&gt; &lt;img src="https://img.shields.io/github/forks/hiroi-sora/Umi-OCR?style=flat-square" alt="forks" /&gt; &lt;/a&gt; &lt;a href="https://hosted.weblate.org/engage/umi-ocr/"&gt; &lt;img src="https://hosted.weblate.org/widget/umi-ocr/svg-badge.svg?sanitize=true" alt="ç¿»è¯‘çŠ¶æ€" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt; &lt;a href="#ç›®å½•"&gt; ä½¿ç”¨è¯´æ˜ &lt;/a&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="#ä¸‹è½½å‘è¡Œç‰ˆ"&gt; ä¸‹è½½åœ°å€ &lt;/a&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/CHANGE_LOG.md"&gt; æ›´æ–°æ—¥å¿— &lt;/a&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues"&gt; æäº¤Bug &lt;/a&gt; &lt;/h3&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt;å…è´¹ï¼Œå¼€æºï¼Œå¯æ‰¹é‡çš„ç¦»çº¿OCRè½¯ä»¶&lt;/strong&gt;
 &lt;br /&gt; 
 &lt;sub&gt;é€‚ç”¨äº Windows7 x64 ã€Linux x64 &lt;/sub&gt;
&lt;/div&gt;
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å…è´¹&lt;/strong&gt;ï¼šæœ¬é¡¹ç›®æ‰€æœ‰ä»£ç å¼€æºï¼Œå®Œå…¨å…è´¹ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ–¹ä¾¿&lt;/strong&gt;ï¼šè§£å‹å³ç”¨ï¼Œç¦»çº¿è¿è¡Œï¼Œæ— éœ€ç½‘ç»œã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;é«˜æ•ˆ&lt;/strong&gt;ï¼šè‡ªå¸¦é«˜æ•ˆç‡çš„ç¦»çº¿OCRå¼•æ“ï¼Œå†…ç½®å¤šç§è¯­è¨€è¯†åˆ«åº“ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;çµæ´»&lt;/strong&gt;ï¼šæ”¯æŒå‘½ä»¤è¡Œã€HTTPæ¥å£ç­‰å¤–éƒ¨è°ƒç”¨æ–¹å¼ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;åŠŸèƒ½&lt;/strong&gt;ï¼šæˆªå›¾OCR / æ‰¹é‡OCR / PDFè¯†åˆ« / äºŒç»´ç  / å…¬å¼è¯†åˆ«&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/65599097ab5f4.png" alt="1-æ ‡é¢˜-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559909fdeeba.png" alt="1-æ ‡é¢˜-2.png" /&gt;&lt;/p&gt; 
&lt;h2&gt;ç›®å½•&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%88%AA%E5%9B%BEOCR"&gt;æˆªå›¾è¯†åˆ«&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%96%87%E6%9C%AC%E5%90%8E%E5%A4%84%E7%90%86"&gt;æ’ç‰ˆè§£æ&lt;/a&gt; - è¯†åˆ«ä¸åŒæ’ç‰ˆï¼ŒæŒ‰æ­£ç¡®é¡ºåºè¾“å‡ºæ–‡å­—&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%89%B9%E9%87%8FOCR"&gt;æ‰¹é‡è¯†åˆ«&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E5%BF%BD%E7%95%A5%E5%8C%BA%E5%9F%9F"&gt;å¿½ç•¥åŒºåŸŸ&lt;/a&gt; - æ’é™¤æˆªå›¾æ°´å°å¤„çš„æ–‡å­—&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E4%BA%8C%E7%BB%B4%E7%A0%81"&gt;äºŒç»´ç &lt;/a&gt; æ”¯æŒæ‰«ç æˆ–ç”ŸæˆäºŒç»´ç å›¾ç‰‡&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%96%87%E6%A1%A3%E8%AF%86%E5%88%AB"&gt;æ–‡æ¡£è¯†åˆ«&lt;/a&gt; ä»PDFæ‰«æä»¶ä¸­æå–æ–‡æœ¬ï¼Œæˆ–è½¬ä¸ºåŒå±‚å¯æœç´¢PDF&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE"&gt;å…¨å±€è®¾ç½®&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/README_CLI.md"&gt;å‘½ä»¤è¡Œè°ƒç”¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/http/README.md"&gt;HTTPæ¥å£&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%9E%84%E5%BB%BA%E9%A1%B9%E7%9B%AE"&gt;æ„å»ºé¡¹ç›®ï¼ˆWindowsã€Linuxï¼‰&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ä½¿ç”¨æºç &lt;/h2&gt; 
&lt;p&gt;å¼€å‘è€…è¯·åŠ¡å¿…é˜…è¯» &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%9E%84%E5%BB%BA%E9%A1%B9%E7%9B%AE"&gt;æ„å»ºé¡¹ç›®&lt;/a&gt; ã€‚&lt;/p&gt; 
&lt;h2&gt;ä¸‹è½½å‘è¡Œç‰ˆ&lt;/h2&gt; 
&lt;p&gt;ä»¥ä¸‹å‘å¸ƒé“¾æ¥å‡é•¿æœŸç»´æŠ¤ï¼Œæä¾›ç¨³å®šç‰ˆæœ¬çš„ä¸‹è½½ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;è“å¥äº‘&lt;/strong&gt; &lt;a href="https://hiroi-sora.lanzoul.com/s/umi-ocr"&gt;https://hiroi-sora.lanzoul.com/s/umi-ocr&lt;/a&gt; ï¼ˆå›½å†…æ¨èï¼Œå…æ³¨å†Œ/æ— é™é€Ÿï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/releases/latest"&gt;https://github.com/hiroi-sora/Umi-OCR/releases/latest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Forge&lt;/strong&gt; &lt;a href="https://sourceforge.net/projects/umi-ocr"&gt;https://sourceforge.net/projects/umi-ocr&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;â€¢&amp;nbsp;&amp;nbsp;Scoop Installer&lt;/b&gt;ï¼ˆç‚¹å‡»å±•å¼€ï¼‰&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://scoop.sh/"&gt;Scoop&lt;/a&gt; æ˜¯ä¸€æ¬¾Windowsä¸‹çš„å‘½ä»¤è¡Œå®‰è£…ç¨‹åºï¼Œå¯æ–¹ä¾¿åœ°ç®¡ç†å¤šä¸ªåº”ç”¨ã€‚æ‚¨å¯ä»¥å…ˆå®‰è£… Scoop ï¼Œå†ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å®‰è£… &lt;code&gt;Umi-OCR&lt;/code&gt; ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ·»åŠ  &lt;code&gt;extras&lt;/code&gt; æ¡¶ï¼š&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop bucket add extras
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ï¼ˆå¯é€‰1ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ &lt;code&gt;Rapid-OCR&lt;/code&gt; å¼•æ“ï¼Œå…¼å®¹æ€§å¥½ï¼‰ï¼š&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop install extras/umi-ocr
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ï¼ˆå¯é€‰2ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ &lt;code&gt;Paddle-OCR&lt;/code&gt; å¼•æ“ï¼Œé€Ÿåº¦ç¨å¿«ï¼‰ï¼š&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop install extras/umi-ocr-paddle
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ä¸è¦åŒæ—¶å®‰è£…äºŒè€…ï¼Œå¿«æ·æ–¹å¼å¯èƒ½ä¼šè¢«è¦†ç›–ã€‚ä½†æ‚¨å¯ä»¥é¢å¤–å¯¼å…¥ &lt;a href="https://github.com/hiroi-sora/Umi-OCR_plugins"&gt;æ’ä»¶&lt;/a&gt; ï¼Œéšæ—¶åˆ‡æ¢ä¸åŒOCRå¼•æ“ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;å¼€å§‹ä½¿ç”¨&lt;/h2&gt; 
&lt;p&gt;è½¯ä»¶å‘å¸ƒåŒ…ä¸‹è½½ä¸º &lt;code&gt;.7z&lt;/code&gt; å‹ç¼©åŒ…æˆ– &lt;code&gt;.7z.exe&lt;/code&gt; è‡ªè§£å‹åŒ…ã€‚è‡ªè§£å‹åŒ…å¯åœ¨æ²¡æœ‰å®‰è£…å‹ç¼©è½¯ä»¶çš„ç”µè„‘ä¸Šï¼Œè§£å‹æ–‡ä»¶ã€‚&lt;/p&gt; 
&lt;p&gt;æœ¬è½¯ä»¶æ— éœ€å®‰è£…ã€‚è§£å‹åï¼Œç‚¹å‡» &lt;code&gt;Umi-OCR.exe&lt;/code&gt; å³å¯å¯åŠ¨ç¨‹åºã€‚&lt;/p&gt; 
&lt;p&gt;é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·æ &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues"&gt;Issue&lt;/a&gt; ï¼Œæˆ‘ä¼šå°½å¯èƒ½å¸®åŠ©ä½ ã€‚&lt;/p&gt; 
&lt;h2&gt;ç•Œé¢è¯­è¨€&lt;/h2&gt; 
&lt;p&gt;Umi-OCR æ”¯æŒçš„ç•Œé¢å¤šå›½è¯­è¨€ã€‚åœ¨ç¬¬ä¸€æ¬¡æ‰“å¼€è½¯ä»¶æ—¶ï¼Œå°†ä¼šæŒ‰ç…§ä½ çš„ç”µè„‘çš„ç³»ç»Ÿè®¾ç½®ï¼Œè‡ªåŠ¨åˆ‡æ¢è¯­è¨€ã€‚&lt;/p&gt; 
&lt;p&gt;å¦‚æœéœ€è¦æ‰‹åŠ¨åˆ‡æ¢è¯­è¨€ï¼Œè¯·å‚è€ƒä¸‹å›¾ï¼Œ&lt;code&gt;å…¨å±€è®¾ç½®&lt;/code&gt;â†’&lt;code&gt;è¯­è¨€/Language&lt;/code&gt; ã€‚&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/65599c3f9e600.png" alt="1-æ ‡é¢˜-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;h2&gt;æ ‡ç­¾é¡µ&lt;/h2&gt; 
&lt;p&gt;Umi-OCR v2 ç”±ä¸€ç³»åˆ—çµæ´»å¥½ç”¨çš„&lt;strong&gt;æ ‡ç­¾é¡µ&lt;/strong&gt;ç»„æˆã€‚æ‚¨å¯æŒ‰ç…§è‡ªå·±çš„å–œå¥½ï¼Œæ‰“å¼€éœ€è¦çš„æ ‡ç­¾é¡µã€‚&lt;/p&gt; 
&lt;p&gt;æ ‡ç­¾æ å·¦ä¸Šè§’å¯ä»¥åˆ‡æ¢&lt;strong&gt;çª—å£ç½®é¡¶&lt;/strong&gt;ã€‚å³ä¸Šè§’èƒ½å¤Ÿ&lt;strong&gt;é”å®šæ ‡ç­¾é¡µ&lt;/strong&gt;ï¼Œä»¥é˜²æ­¢æ—¥å¸¸ä½¿ç”¨ä¸­è¯¯è§¦å…³é—­æ ‡ç­¾é¡µã€‚&lt;/p&gt; 
&lt;h3&gt;æˆªå›¾OCR&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/65599097aba8e.png" alt="2-æˆªå›¾-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;æˆªå›¾OCR&lt;/strong&gt;ï¼šæ‰“å¼€è¿™ä¸€é¡µåï¼Œå°±å¯ä»¥ç”¨å¿«æ·é”®å”¤èµ·æˆªå›¾ï¼Œè¯†åˆ«å›¾ä¸­çš„æ–‡å­—ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å·¦ä¾§çš„å›¾ç‰‡é¢„è§ˆæ ï¼Œå¯ç›´æ¥ç”¨é¼ æ ‡åˆ’é€‰å¤åˆ¶ã€‚&lt;/li&gt; 
 &lt;li&gt;å³ä¾§çš„è¯†åˆ«è®°å½•æ ï¼Œå¯ä»¥ç¼–è¾‘æ–‡å­—ï¼Œå…è®¸åˆ’é€‰å¤šä¸ªè®°å½•å¤åˆ¶ã€‚&lt;/li&gt; 
 &lt;li&gt;ä¹Ÿæ”¯æŒåœ¨åˆ«å¤„å¤åˆ¶å›¾ç‰‡ï¼Œç²˜è´´åˆ°Umi-OCRè¿›è¡Œè¯†åˆ«ã€‚&lt;/li&gt; 
 &lt;li&gt;å…³äº &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues/254"&gt;å…¬å¼è¯†åˆ«&lt;/a&gt; åŠŸèƒ½&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;æ–‡æœ¬åå¤„ç†&lt;/h4&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559909f3e378.png" alt="2-æˆªå›¾-2.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;å…³äº &lt;strong&gt;OCRæ–‡æœ¬åå¤„ç† - æ’ç‰ˆè§£ææ–¹æ¡ˆ&lt;/strong&gt;ï¼š å¯ä»¥æ•´ç†OCRç»“æœçš„æ’ç‰ˆå’Œé¡ºåºï¼Œä½¿æ–‡æœ¬æ›´é€‚åˆé˜…è¯»å’Œä½¿ç”¨ã€‚é¢„è®¾æ–¹æ¡ˆï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;å¤šæ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ&lt;/code&gt;ï¼šé€‚åˆå¤§éƒ¨åˆ†æƒ…æ™¯ï¼Œè‡ªåŠ¨è¯†åˆ«å¤šæ å¸ƒå±€ï¼ŒæŒ‰è‡ªç„¶æ®µè§„åˆ™è¿›è¡Œæ¢è¡Œã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;å¤šæ -æ€»æ˜¯æ¢è¡Œ&lt;/code&gt;ï¼šæ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;å¤šæ -æ— æ¢è¡Œ&lt;/code&gt;ï¼šå¼ºåˆ¶å°†æ‰€æœ‰è¯­å¥åˆå¹¶åˆ°åŒä¸€è¡Œã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;å•æ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ&lt;/code&gt;/&lt;code&gt;æ€»æ˜¯æ¢è¡Œ&lt;/code&gt;/&lt;code&gt;æ— æ¢è¡Œ&lt;/code&gt;ï¼šä¸ä¸Šè¿°ç±»ä¼¼ï¼Œä¸è¿‡ ä¸åŒºåˆ†å¤šæ å¸ƒå±€ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;å•æ -ä¿ç•™ç¼©è¿›&lt;/code&gt;ï¼šé€‚ç”¨äºè§£æä»£ç æˆªå›¾ï¼Œä¿ç•™è¡Œé¦–ç¼©è¿›å’Œè¡Œä¸­ç©ºæ ¼ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ä¸åšå¤„ç†&lt;/code&gt;ï¼šOCRå¼•æ“çš„åŸå§‹è¾“å‡ºï¼Œé»˜è®¤æ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ä¸Šè¿°æ–¹æ¡ˆï¼Œå‡èƒ½è‡ªåŠ¨å¤„ç†æ¨ªæ’å’Œç«–æ’ï¼ˆä»å³åˆ°å·¦ï¼‰çš„æ’ç‰ˆã€‚ï¼ˆç«–æ’æ–‡å­—è¿˜éœ€è¦OCRå¼•æ“æœ¬èº«æ”¯æŒï¼‰&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;æ‰¹é‡OCR&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/655990a2511e0.png" alt="3-æ‰¹é‡-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;æ‰¹é‡OCR&lt;/strong&gt;ï¼šè¿™ä¸€é¡µç”¨äºæ‰¹é‡å¯¼å…¥æœ¬åœ°å›¾ç‰‡è¿›è¡Œè¯†åˆ«ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ”¯æŒæ ¼å¼ï¼š&lt;code&gt;jpg, jpe, jpeg, jfif, png, webp, bmp, tif, tiff&lt;/code&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;ä¿å­˜è¯†åˆ«ç»“æœçš„æ”¯æŒæ ¼å¼ï¼š&lt;code&gt;txt, jsonl, md, csv(Excel)&lt;/code&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;ä¸æˆªå›¾OCRä¸€æ ·ï¼Œæ”¯æŒ&lt;code&gt;æ–‡æœ¬åå¤„ç†&lt;/code&gt;åŠŸèƒ½ï¼Œæ•´ç†OCRæ–‡æœ¬çš„æ’ç‰ˆå’Œé¡ºåºã€‚&lt;/li&gt; 
 &lt;li&gt;æ²¡æœ‰æ•°é‡ä¸Šé™ï¼Œå¯ä¸€æ¬¡æ€§å¯¼å…¥å‡ ç™¾å¼ å›¾ç‰‡è¿›è¡Œä»»åŠ¡ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒä»»åŠ¡å®Œæˆåè‡ªåŠ¨å…³æœº/å¾…æœºã€‚&lt;/li&gt; 
 &lt;li&gt;å¦‚æœè¦è¯†åˆ«åƒç´ è¶…å¤§çš„é•¿å›¾æˆ–å¤§å›¾ï¼Œè¯·è°ƒæ•´ï¼š&lt;strong&gt;é¡µé¢çš„è®¾ç½®â†’æ–‡å­—è¯†åˆ«â†’é™åˆ¶å›¾åƒè¾¹é•¿â†’ã€è°ƒé«˜æ•°å€¼ã€‘&lt;/strong&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;æ‹¥æœ‰ç‰¹æ®ŠåŠŸèƒ½ &lt;code&gt;å¿½ç•¥åŒºåŸŸ&lt;/code&gt; ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;å¿½ç•¥åŒºåŸŸ&lt;/h4&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559911d28be7.png" alt="3-æ‰¹é‡-2.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;å…³äº &lt;strong&gt;OCRæ–‡æœ¬åå¤„ç† - å¿½ç•¥åŒºåŸŸ&lt;/strong&gt;ï¼š æ‰¹é‡OCRä¸­çš„ä¸€ç§ç‰¹æ®ŠåŠŸèƒ½ï¼Œé€‚ç”¨äºæ’é™¤å›¾ç‰‡ä¸­çš„ä¸æƒ³è¦çš„æ–‡å­—ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;åœ¨æ‰¹é‡è¯†åˆ«é¡µçš„å³æ è®¾ç½®ä¸­å¯è¿›å…¥å¿½ç•¥åŒºåŸŸç¼–è¾‘å™¨ã€‚&lt;/li&gt; 
 &lt;li&gt;å¦‚ä¸Šæ–¹æ ·ä¾‹ï¼Œå›¾ç‰‡é¡¶éƒ¨å’Œå³ä¸‹è§’å­˜åœ¨å¤šä¸ªæ°´å° / LOGOã€‚å¦‚æœæ‰¹é‡è¯†åˆ«è¿™ç±»å›¾ç‰‡ï¼Œæ°´å°ä¼šå¯¹è¯†åˆ«ç»“æœé€ æˆå¹²æ‰°ã€‚&lt;/li&gt; 
 &lt;li&gt;æŒ‰ä½å³é”®ï¼Œç»˜åˆ¶å¤šä¸ªçŸ©å½¢æ¡†ã€‚è¿™äº›åŒºåŸŸå†…çš„æ–‡å­—å°†åœ¨ä»»åŠ¡ä¸­è¢«å¿½ç•¥ã€‚&lt;/li&gt; 
 &lt;li&gt;è¯·å°½é‡å°†çŸ©å½¢æ¡†ç”»å¾—å¤§ä¸€äº›ï¼Œå®Œå…¨åŒ…è£¹ä½æ°´å°æ‰€æœ‰å¯èƒ½å‡ºç°çš„ä½ç½®ã€‚&lt;/li&gt; 
 &lt;li&gt;æ³¨æ„ï¼Œåªæœ‰å¤„äºå¿½ç•¥åŒºåŸŸæ¡†å†…éƒ¨çš„æ•´ä¸ªæ–‡æœ¬å—ï¼ˆè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ï¼‰ä¼šè¢«å¿½ç•¥ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé»„è‰²è¾¹æ¡†çš„æ·±è‰²çŸ©å½¢æ˜¯ä¸€ä¸ªå¿½ç•¥åŒºåŸŸã€‚é‚£ä¹ˆåªæœ‰&lt;code&gt;key_mouse&lt;/code&gt;æ‰ä¼šè¢«å¿½ç•¥ã€‚&lt;code&gt;pubsub_connector.py&lt;/code&gt;ã€&lt;code&gt;pubsub_service.py&lt;/code&gt; è¿™ä¸¤ä¸ªæ–‡æœ¬å—å¾—ä»¥ä¿ç•™ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2024/05/30/66587bf03ae15.png" alt="å¿½ç•¥åŒºåŸŸèŒƒå›´ç¤ºä¾‹.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;æ–‡æ¡£è¯†åˆ«&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://github.com/hiroi-sora/Umi-OCR/assets/56373419/fc2266ee-b9b7-4079-8b10-6610e6da6cf5" alt="" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;æ–‡æ¡£è¯†åˆ«&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ”¯æŒæ ¼å¼ï¼š&lt;code&gt;pdf, xps, epub, mobi, fb2, cbz&lt;/code&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;å¯¹æ‰«æä»¶è¿›è¡ŒOCRï¼Œæˆ–æå–åŸæœ‰æ–‡æœ¬ã€‚å¯è¾“å‡ºä¸º &lt;strong&gt;åŒå±‚å¯æœç´¢PDF&lt;/strong&gt; ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒè®¾å®š &lt;strong&gt;å¿½ç•¥åŒºåŸŸ&lt;/strong&gt; ï¼Œå¯ç”¨äºæ’é™¤é¡µçœ‰é¡µè„šçš„æ–‡å­—ã€‚&lt;/li&gt; 
 &lt;li&gt;å¯è®¾ç½®ä»»åŠ¡å®Œæˆå &lt;strong&gt;è‡ªåŠ¨å…³æœº/ä¼‘çœ &lt;/strong&gt; ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;äºŒç»´ç &lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/655991268d6b1.png" alt="4-äºŒç»´ç -1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;æ‰«ç &lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;æˆªå›¾/ç²˜è´´/æ‹–å…¥æœ¬åœ°å›¾ç‰‡ï¼Œè¯»å–å…¶ä¸­çš„äºŒç»´ç ã€æ¡å½¢ç ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒä¸€å›¾å¤šç ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒ19ç§åè®®ï¼Œå¦‚ä¸‹ï¼š&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;Aztec&lt;/code&gt;,&lt;code&gt;Codabar&lt;/code&gt;,&lt;code&gt;Code128&lt;/code&gt;,&lt;code&gt;Code39&lt;/code&gt;,&lt;code&gt;Code93&lt;/code&gt;,&lt;code&gt;DataBar&lt;/code&gt;,&lt;code&gt;DataBarExpanded&lt;/code&gt;,&lt;code&gt;DataMatrix&lt;/code&gt;,&lt;code&gt;EAN13&lt;/code&gt;,&lt;code&gt;EAN8&lt;/code&gt;,&lt;code&gt;ITF&lt;/code&gt;,&lt;code&gt;LinearCodes&lt;/code&gt;,&lt;code&gt;MatrixCodes&lt;/code&gt;,&lt;code&gt;MaxiCode&lt;/code&gt;,&lt;code&gt;MicroQRCode&lt;/code&gt;,&lt;code&gt;PDF417&lt;/code&gt;,&lt;code&gt;QRCode&lt;/code&gt;,&lt;code&gt;UPCA&lt;/code&gt;,&lt;code&gt;UPCE&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559911cda737.png" alt="4-äºŒç»´ç -2.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ç”Ÿæˆç &lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;è¾“å…¥æ–‡æœ¬ï¼Œç”ŸæˆäºŒç»´ç å›¾ç‰‡ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒ19ç§åè®®å’Œ&lt;strong&gt;çº é”™ç­‰çº§&lt;/strong&gt;ç­‰å‚æ•°ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;å…¨å±€è®¾ç½®&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/655991252e780.png" alt="5-å…¨å±€è®¾ç½®-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;å…¨å±€è®¾ç½®&lt;/strong&gt;ï¼šåœ¨è¿™é‡Œå¯ä»¥è°ƒæ•´è½¯ä»¶çš„å…¨å±€å‚æ•°ã€‚å¸¸ç”¨åŠŸèƒ½å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä¸€é”®æ·»åŠ å¿«æ·æ–¹å¼æˆ–è®¾ç½®å¼€æœºè‡ªå¯ã€‚&lt;/li&gt; 
 &lt;li&gt;æ›´æ”¹ç•Œé¢&lt;strong&gt;è¯­è¨€&lt;/strong&gt;ã€‚Umiæ”¯æŒç¹ä¸­ã€è‹±è¯­ã€æ—¥è¯­ç­‰è¯­è¨€ã€‚&lt;/li&gt; 
 &lt;li&gt;åˆ‡æ¢ç•Œé¢&lt;strong&gt;ä¸»é¢˜&lt;/strong&gt;ã€‚Umiæ‹¥æœ‰å¤šä¸ªäº®/æš—ä¸»é¢˜ã€‚&lt;/li&gt; 
 &lt;li&gt;è°ƒæ•´ç•Œé¢&lt;strong&gt;æ–‡å­—çš„å¤§å°&lt;/strong&gt;å’Œ&lt;strong&gt;å­—ä½“&lt;/strong&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;åˆ‡æ¢OCRæ’ä»¶ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ¸²æŸ“å™¨&lt;/strong&gt;ï¼šè½¯ä»¶ç•Œé¢é»˜è®¤æ”¯æŒæ˜¾å¡åŠ é€Ÿæ¸²æŸ“ã€‚å¦‚æœåœ¨ä½ çš„æœºå™¨ä¸Šå‡ºç°æˆªå±é—ªçƒã€UIé”™ä½çš„æƒ…å†µï¼Œè¯·è°ƒæ•´&lt;code&gt;ç•Œé¢å’Œå¤–è§‚&lt;/code&gt; â†’ &lt;code&gt;æ¸²æŸ“å™¨&lt;/code&gt; ï¼Œå°è¯•åˆ‡æ¢åˆ°ä¸åŒæ¸²æŸ“æ–¹æ¡ˆï¼Œæˆ–å…³é—­ç¡¬ä»¶åŠ é€Ÿã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;è°ƒç”¨æ¥å£ï¼š&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/README_CLI.md"&gt;å‘½ä»¤è¡Œæ‰‹å†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/http/README.md"&gt;HTTPæ¥å£æ‰‹å†Œ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;å…³äºé¡¹ç›®ç»“æ„&lt;/h2&gt; 
&lt;h3&gt;å„ä»“åº“ï¼š&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;ä¸»ä»“åº“&lt;/a&gt; ğŸ‘ˆ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_plugins"&gt;æ’ä»¶åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_windows"&gt;Windows è¿è¡Œåº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_linux"&gt;Linux è¿è¡Œåº“&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å·¥ç¨‹ç»“æ„ï¼š&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;**&lt;/code&gt; åç¼€è¡¨ç¤ºæœ¬ä»“åº“(&lt;code&gt;ä¸»ä»“åº“&lt;/code&gt;)åŒ…å«çš„å†…å®¹ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Umi-OCR
â”œâ”€ Umi-OCR.exe
â”œâ”€ umi-ocr.sh
â””â”€ UmiOCR-data
   â”œâ”€ main.py **
   â”œâ”€ version.py **
   â”œâ”€ qt_res **
   â”‚  â””â”€ é¡¹ç›®qtèµ„æºï¼ŒåŒ…æ‹¬å›¾æ ‡å’Œqmlæºç 
   â”œâ”€ py_src **
   â”‚  â””â”€ é¡¹ç›®pythonæºç 
   â”œâ”€ plugins
   â”‚  â””â”€ æ’ä»¶
   â””â”€ i18n **
      â””â”€ ç¿»è¯‘æ–‡ä»¶
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æ”¯æŒçš„ç¦»çº¿OCRå¼•æ“ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/PaddleOCR-json"&gt;PaddleOCR-json&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/RapidOCR-json"&gt;RapidOCR-json&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;è¿è¡Œç¯å¢ƒæ¡†æ¶ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skywind3000/PyStand"&gt;PyStand&lt;/a&gt; å®šåˆ¶ç‰ˆ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;æ„å»ºé¡¹ç›®&lt;/h2&gt; 
&lt;p&gt;è¯·è·³è½¬ä¸‹è¿°ä»“åº“ï¼Œå®Œæˆå¯¹åº”å¹³å°çš„å¼€å‘/è¿è¡Œç¯å¢ƒéƒ¨ç½²ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_windows"&gt;Windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_linux"&gt;Linux&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;è½¯ä»¶æœ¬åœ°åŒ–ç¿»è¯‘ï¼š&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®ä½¿ç”¨ Weblate å¹³å°è¿›è¡ŒUIç•Œé¢çš„æœ¬åœ°åŒ–ç¿»è¯‘åä½œã€‚æˆ‘ä»¬æ¬¢è¿ä»»ä½•è¯‘è€…å‚ä¸ç¿»è¯‘å·¥ä½œï¼Œæ‚¨å¯è¿›å…¥æ­¤é“¾æ¥ &lt;a href="https://hosted.weblate.org/engage/umi-ocr/"&gt;Weblate: Umi-OCR&lt;/a&gt; ï¼Œåœ¨çº¿æ ¡å¯¹ã€è¡¥å……ç°æœ‰è¯­è¨€ï¼Œæˆ–æ·»åŠ æ–°è¯­è¨€ã€‚&lt;/p&gt; 
&lt;p&gt;æ„Ÿè°¢ä»¥ä¸‹è¯‘è€…ï¼Œä¸º Umi-OCR è´¡çŒ®äº†æœ¬åœ°åŒ–ç¿»è¯‘å·¥ä½œï¼š&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;è¯‘è€…&lt;/th&gt; 
   &lt;th&gt;è´¡çŒ®è¯­è¨€&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/q021"&gt;bob&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, ç¹é«”ä¸­æ–‡, æ—¥æœ¬èª&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QZGao"&gt;Qingzheng Gao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/ChiaLingWeng"&gt;Weng, Chia-Ling&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/linzow"&gt;linzow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/ultramarkorj9"&gt;Marcos i&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, PortuguÃªs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/qwedc001"&gt;Eric Guo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/steven0081"&gt;steven0081&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/random4t4x14"&gt;Brandon Cagle&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/plum7x"&gt;plum7x&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/hugoalh"&gt;hugoalh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/Anarkiisto"&gt;Anarkiisto&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/umren190402"&gt;ãƒ‰ã‚³ãƒ¢å…‰&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ—¥æœ¬èª&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/ypf"&gt;æ¨é¹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PortuguÃªs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/1969"&gt;Ğ’ÑÑ‡ĞµÑĞ»Ğ°Ğ² ĞĞ½Ğ°Ñ‚Ğ¾Ğ»ÑŒĞµĞ²Ğ¸Ñ‡ ĞœĞ°Ğ»Ñ‹ÑˆĞµĞ²&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/muhammadyusuf.kurbonov2002"&gt;Muhammadyusuf Kurbonov&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/TamilNeram/"&gt;à®¤à®®à®¿à®´à¯à®¨à¯‡à®°à®®à¯&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;à®¤à®®à®¿à®´à¯&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;å¦‚æœæœ‰ä¿¡æ¯é”™è¯¯æˆ–äººå‘˜ç¼ºæ¼ï¼Œè¯·åœ¨ &lt;a href="https://github.com/hiroi-sora/Umi-OCR/discussions/449"&gt;è¿™ä¸ªè®¨è®º&lt;/a&gt; ä¸­å›å¤ã€‚&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;èµåŠ©&lt;/h2&gt; 
&lt;p&gt;Umi-OCR é¡¹ç›®ä¸»è¦ç”±ä½œè€… &lt;a href="https://github.com/hiroi-sora"&gt;hiroi-sora&lt;/a&gt; ç”¨ä¸šä½™æ—¶é—´åœ¨å¼€å‘å’Œç»´æŠ¤ã€‚å¦‚æœæ‚¨å–œæ¬¢è¿™æ¬¾è½¯ä»¶ï¼Œæ¬¢è¿èµåŠ©ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å›½å†…ç”¨æˆ·å¯é€šè¿‡ &lt;a href="https://afdian.com/a/hiroi-sora"&gt;çˆ±å‘ç”µ&lt;/a&gt; èµåŠ©ä½œè€…ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#hiroi-sora/Umi-OCR&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=hiroi-sora/Umi-OCR&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/CHANGE_LOG.md"&gt;æ›´æ–°æ—¥å¿—&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;å¼€å‘è®¡åˆ’&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;å·²å®Œæˆçš„å·¥ä½œ&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ ‡ç­¾é¡µæ¡†æ¶ã€‚&lt;/li&gt; 
  &lt;li&gt;OCR APIæ§åˆ¶å™¨ã€‚&lt;/li&gt; 
  &lt;li&gt;OCR ä»»åŠ¡æ§åˆ¶å™¨ã€‚&lt;/li&gt; 
  &lt;li&gt;ä¸»é¢˜ç®¡ç†å™¨ï¼Œæ”¯æŒåˆ‡æ¢æµ…è‰²/æ·±è‰²ä¸»é¢˜ä¸»é¢˜ã€‚&lt;/li&gt; 
  &lt;li&gt;å®ç° &lt;strong&gt;æ‰¹é‡OCR&lt;/strong&gt;ã€‚&lt;/li&gt; 
  &lt;li&gt;å®ç° &lt;strong&gt;æˆªå›¾OCR&lt;/strong&gt;ã€‚&lt;/li&gt; 
  &lt;li&gt;å¿«æ·é”®æœºåˆ¶ã€‚&lt;/li&gt; 
  &lt;li&gt;ç³»ç»Ÿæ‰˜ç›˜èœå•ã€‚&lt;/li&gt; 
  &lt;li&gt;æ–‡æœ¬å—åå¤„ç†ï¼ˆæ’ç‰ˆä¼˜åŒ–ï¼‰ã€‚&lt;/li&gt; 
  &lt;li&gt;å¼•æ“å†…å­˜æ¸…ç†ã€‚&lt;/li&gt; 
  &lt;li&gt;è½¯ä»¶ç•Œé¢å¤šå›½è¯­è¨€ã€‚&lt;/li&gt; 
  &lt;li&gt;å‘½ä»¤è¡Œæ¨¡å¼ã€‚&lt;/li&gt; 
  &lt;li&gt;Win7å…¼å®¹ã€‚&lt;/li&gt; 
  &lt;li&gt;Excelï¼ˆcsvï¼‰è¾“å‡ºæ ¼å¼ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;Esc&lt;/code&gt;ä¸­æ–­æˆªå›¾æ“ä½œ&lt;/li&gt; 
  &lt;li&gt;å¤–ç½®ä¸»é¢˜æ–‡ä»¶&lt;/li&gt; 
  &lt;li&gt;å­—ä½“åˆ‡æ¢&lt;/li&gt; 
  &lt;li&gt;åŠ è½½åŠ¨ç”»&lt;/li&gt; 
  &lt;li&gt;å¿½ç•¥åŒºåŸŸã€‚&lt;/li&gt; 
  &lt;li&gt;äºŒç»´ç è¯†åˆ«ã€‚&lt;/li&gt; 
  &lt;li&gt;æ‰¹é‡è¯†åˆ«é¡µé¢çš„å›¾ç‰‡é¢„è§ˆçª—å£ã€‚&lt;/li&gt; 
  &lt;li&gt;PDFè¯†åˆ«ã€‚&lt;/li&gt; 
  &lt;li&gt;è°ƒç”¨æœ¬åœ°å›¾ç‰‡æµè§ˆå™¨æ‰“å¼€å›¾ç‰‡ã€‚ &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues/335"&gt;#335&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;é‡å¤ä¸Šä¸€æ¬¡æˆªå›¾ã€‚ &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues/357"&gt;#357&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ä¿®Bugï¼šæ–‡æ¡£è¯†åˆ«åœ¨Windows7ç³»ç»Ÿçš„å…¼å®¹æ€§é—®é¢˜ã€‚&lt;/li&gt; 
  &lt;li&gt;HTTP/å‘½ä»¤è¡Œæ¥å£æ·»åŠ äºŒç»´ç è¯†åˆ«/ç”ŸæˆåŠŸèƒ½ã€‚ (#423)&lt;/li&gt; 
  &lt;li&gt;äºŒç»´ç æ¥å£çš„æ–‡æ¡£ã€‚&lt;/li&gt; 
  &lt;li&gt;Linux å¹³å°ç§»æ¤ã€‚&lt;/li&gt; 
  &lt;li&gt;HTTP æ–‡æ¡£è¯†åˆ«æ¥å£ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;!-- ##### æ­£åœ¨è¿›è¡Œçš„å·¥ä½œ --&gt; 
&lt;h5&gt;è¿œæœŸè®¡åˆ’&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;å±•å¼€&lt;/summary&gt; 
 &lt;p&gt;è¿™äº›æ˜¯é¢„æƒ³ä¸­çš„åŠŸèƒ½ï¼Œåœ¨å¼€å‘åˆæœŸå·²é¢„ç•™å¥½æ¥å£ï¼Œå°†åœ¨è¿œæœŸæ…¢æ…¢å®ç°ã€‚&lt;/p&gt; 
 &lt;p&gt;ä½†å¼€å‘é€”ä¸­å—é™äºå®é™…æƒ…å†µï¼Œå¯èƒ½æ›´æ”¹åŠŸèƒ½è®¾è®¡ã€æ–°å¢åŠå–æ¶ˆåŠŸèƒ½ã€‚&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;é‡æ„åº•å±‚æ’ä»¶æœºåˆ¶ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;åœ¨çº¿ OCR API æ’ä»¶ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«æ’ä»¶ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;â€œæ•°å­¦å…¬å¼â€æ ‡ç­¾é¡µï¼Œæä¾›ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«/Latexæ¸²æŸ“ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;æ£€æŸ¥æ›´æ–°æœºåˆ¶ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;æ’ç‰ˆè§£æä¹‹å¤–çš„æ–‡æœ¬åå¤„ç†æ¨¡å—ï¼ˆå¦‚ä¿ç•™æ•°å­—ã€åŠå…¨è§’å­—ç¬¦è½¬æ¢ã€æ–‡æœ¬çº é”™ï¼‰ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;å…³é”®æ¥å£å‡½æ•°æ·»åŠ äº‹ä»¶è§¦å‘æ–¹å¼ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;åŸºäºGPUçš„ç¦»çº¿OCRã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;å›¾ç‰‡ç¿»è¯‘&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;ç¦»çº¿ç¿»è¯‘ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;å›ºå®šåŒºåŸŸè¯†åˆ«ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;è¯†åˆ«è¡¨æ ¼å›¾ç‰‡ï¼Œè¾“å‡ºä¸ºExcelã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;å†å²è®°å½•ç³»ç»Ÿã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;å…¼å®¹ MacOS / Ubuntu ç­‰å¹³å°ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>NEKOparapa/AiNiee</title>
      <link>https://github.com/NEKOparapa/AiNiee</link>
      <description>&lt;p&gt;ä¸€æ¬¾ä¸“æ³¨äºAiç¿»è¯‘çš„å·¥å…·ï¼Œä¸€é”®è‡ªåŠ¨ç¿»è¯‘RPG SLGæ¸¸æˆï¼ŒEpub TXTå°è¯´ï¼ŒSrt Vtt Lrcå­—å¹•ï¼ŒWord MDæ–‡æ¡£ç­‰ç­‰å¤æ‚é•¿æ–‡æœ¬ã€‚&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/NEKOparapa/AiNiee-chatgpt"&gt; &lt;img src="https://github.com/NEKOparapa/AiNiee-chatgpt/raw/main/Example%20image/logo.png" width="60%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/README_EN.md"&gt;English&lt;/a&gt; | ç®€ä½“ä¸­æ–‡ 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;è½¯ä»¶ä»‹ç»ğŸ§¾&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt;AiNiee&lt;/strong&gt; æ˜¯ä¸€æ¬¾ä¸“æ³¨äº AI ç¿»è¯‘çš„å·¥å…·ï¼Œ
 &lt;br /&gt;ä¸€é”®è‡ªåŠ¨ç¿»è¯‘æ¸¸æˆã€ä¹¦ç±ã€å­—å¹•ã€æ–‡æ¡£ç­‰å¤æ‚é•¿æ–‡æœ¬å†…å®¹ã€‚ 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ ¼å¼å…¨èƒ½ï¼Œè¦†ç›–å¹¿æ³›&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸ® &lt;strong&gt;æ¸¸æˆç¿»è¯‘&lt;/strong&gt;ï¼šæ·±åº¦æ”¯æŒ Mtool, Renpy, Translator++, ParaTranzr, VNText, SExtractor ç­‰æ¸¸æˆæ–‡æœ¬å¯¼å‡ºå·¥å…·ã€‚&lt;/li&gt; 
   &lt;li&gt;ğŸ“š &lt;strong&gt;å¤šæ ·æ”¯æŒ&lt;/strong&gt;ï¼šè½»æ¾å¤„ç† I18Next æ•°æ®ã€Epub/TXT ç”µå­ä¹¦ã€Srt/Vtt/Lrc å­—å¹•ã€Word/PDF/MD æ–‡æ¡£ç­‰ã€‚&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ™ºèƒ½é«˜æ•ˆï¼Œçœæ—¶çœå¿ƒ&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸš€ &lt;strong&gt;ä¸€é”®æ“ä½œ&lt;/strong&gt;ï¼šä¸€æ‹–ä¸€ç‚¹ï¼Œè‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ä¸è¯­è¨€ï¼Œæ— éœ€è®¾ç½®ã€‚&lt;/li&gt; 
   &lt;li&gt;â±ï¸ &lt;strong&gt;æé€Ÿç¿»è¯‘&lt;/strong&gt;ï¼šå–æ¯å¯ä¹çš„å·¥å¤«ï¼Œå°±èƒ½æ‹¿åˆ°è¯‘æ–‡ã€‚&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;é•¿æ–‡ä¼˜åŒ–ï¼Œè´¨é‡å‡ºä¼—&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸ¯ &lt;strong&gt;çªç ´å±€é™&lt;/strong&gt;ï¼šé‡‡ç”¨è½»ç›ˆç¿»è¯‘æ ¼å¼ã€æ€ç»´é“¾ç¿»è¯‘ã€AI æœ¯è¯­è¡¨ã€ä¸Šä¸‹æ–‡å…³è”ç­‰æŠ€æœ¯ï¼Œç¡®ä¿é•¿æ–‡æœ¬ç¿»è¯‘çš„è¿è´¯æ€§ä¸å‡†ç¡®æ€§ã€‚&lt;/li&gt; 
   &lt;li&gt;ğŸ’ &lt;strong&gt;è´¨é‡è¿½æ±‚&lt;/strong&gt;ï¼šæ”¯æŒ åŸºç¡€æç¤ºã€è§’è‰²ä»‹ç»ã€èƒŒæ™¯è®¾å®šã€ç¿»è¯‘é£æ ¼ ç­‰æç¤ºè¯è°ƒæ•´ï¼Œæ‹¥æœ‰ ä¸€é”®AIæ¶¦è‰²ã€ä¸€é”®AIæ’ç‰ˆã€ä¸€é”®æå–æœ¯è¯­ ç­‰åŠŸèƒ½ï¼Œæ»¡è¶³å¯¹ç¿»è¯‘è´¨é‡æœ‰æ›´é«˜è¦æ±‚çš„ç”¨æˆ·ã€‚&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;AiNieeä¸‰æ­¥èµ° ğŸ“¢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸€æ­¥ï¼šé…ç½®æ¥å£&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;img src="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/Example%20image/ä¸‰æ­¥èµ°/ç¬¬ä¸€æ­¥.png" /&gt; 
  &lt;/blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;åœ¨çº¿æ¥å£ï¼šéœ€ä»˜è´¹ä½†æ€§ä»·æ¯”å¾ˆé«˜ï¼Œæ— æ˜¾å¡è¦æ±‚ï¼Œå…¨è¯­è¨€æ”¯æŒï¼Œ&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/QuickStartDeepSeek"&gt;æ¥å£è®¾ç½®è¯´æ˜ - DeepSeek&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;åœ¨çº¿æ¥å£ï¼šåŒä¸Šï¼Œå¦‚æœDeepseekå®˜ç½‘æ— æ³•æ­£å¸¸ä½¿ç”¨ï¼Œå¯æ¢è¯¥æ¥å£ï¼Œ&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/QuickStartHuo"&gt;æ¥å£è®¾ç½®è¯´æ˜ - ç«å±±å¼•æ“&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬äºŒæ­¥ï¼šæ‹–å…¥æ–‡ä»¶å¤¹&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;img src="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/Example%20image/ä¸‰æ­¥èµ°/ç¬¬äºŒæ­¥.png" /&gt; 
  &lt;/blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;è¾“å…¥æ–‡ä»¶å¤¹ï¼šå°†åŸæ–‡æ–‡ä»¶å•ç‹¬æ”¾ç½®æ–°çš„æ–‡ä»¶å¤¹ï¼Œå¹¶å°†è¯¥æ–‡ä»¶å¤¹æ‹–å…¥æ¡†å†…ã€‚å°è¯´ã€å­—å¹•ã€æ–‡æ¡£å¯ç›´æ¥è¿›è¡Œç¿»è¯‘ï¼Œæ¸¸æˆéœ€è¦æ–‡æœ¬æå–å·¥å…·è¿›è¡Œé…åˆã€‚&lt;br /&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸‰æ­¥ï¼šå¼€å§‹ç¿»è¯‘&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;img src="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/Example%20image/ä¸‰æ­¥èµ°/ç¬¬ä¸‰æ­¥.png" /&gt; 
  &lt;/blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;ç‚¹å‡»å¼€å§‹æŒ‰é’®ï¼Œå‰©ä¸‹ç­‰å¾…ä»»åŠ¡çš„å®Œæˆã€‚&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/releases"&gt;AiNieeä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;h2&gt;æ¸¸æˆç¿»è¯‘&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/#%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91"&gt;&lt;img src="https://raw.githubusercontent.com/aregtech/areg-sdk/master/docs/img/pin.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/h2&gt; &lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;å·¥å…·å‡†å¤‡&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;ğŸ“–æ¸¸æˆæ–‡æœ¬æå–å·¥å…·&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th align="center"&gt;å·¥å…·å&lt;/th&gt; 
       &lt;th align="center"&gt;ä»‹ç»&lt;/th&gt; 
       &lt;th align="center"&gt;é¡¹ç›®ç±»å‹&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://afdian.com/p/d42dd1e234aa11eba42452540025c377"&gt;Mtool&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸Šæ‰‹ç®€å•ï¼Œæ¨èæ–°äººä½¿ç”¨&lt;/td&gt; 
       &lt;td align="center"&gt;Mtoolå¯¼å‡ºæ–‡ä»¶&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://dreamsavior.net/download/"&gt;Translator++&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸Šæ‰‹å¤æ‚ï¼ŒåŠŸèƒ½å¼ºå¤§ï¼Œæ¨èå¤§ä½¬ä½¿ç”¨&lt;/td&gt; 
       &lt;td align="center"&gt;T++å¯¼å‡ºæ–‡ä»¶æˆ–Transå·¥ç¨‹æ–‡ä»¶&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://paratranz.cn/projects"&gt;ParaTranzr&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸Šæ‰‹ä¸­ç­‰ï¼ŒåŠŸèƒ½å¼ºå¤§ï¼Œæ¨èå¤§ä½¬ä½¿ç”¨&lt;/td&gt; 
       &lt;td align="center"&gt;ParaTranzrå¯¼å‡ºæ–‡ä»¶&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.renpy.org/latest.html"&gt;RenPy SDK&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸Šæ‰‹ä¸­ç­‰ï¼ŒåŠŸèƒ½å¼ºå¤§ï¼Œæ¨èå¤§ä½¬ä½¿ç”¨&lt;/td&gt; 
       &lt;td align="center"&gt;renpyå¯¼å‡ºæ–‡ä»¶&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;ğŸ§°æœ¬åœ°æ¨¡å‹è¿è¡Œå·¥å…·&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th align="center"&gt;å·¥å…·å&lt;/th&gt; 
       &lt;th align="center"&gt;è¯´æ˜&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/PiDanShouRouZhouXD/Sakura_Launcher_GUI"&gt;Sakura_Launcher_GUI&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;Sakuraæ¨¡å‹çš„ä¸“å±GUIå¯åŠ¨å™¨&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://lmstudio.ai/download"&gt;LM Studio&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸€ä¸ªæœ¬åœ°éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹³å°ï¼Œè‡´åŠ›äºç®€åŒ–LLMçš„ä½¿ç”¨å’Œç®¡ç†ã€‚&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://ollama.com/"&gt;ollama&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;å¼€æºè·¨å¹³å°å¤§æ¨¡å‹å·¥å…·&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;ç¿»è¯‘æ•™ç¨‹&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;ğŸ“ºæ¸¸æˆç¿»è¯‘è§†é¢‘æ•™ç¨‹&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th align="center"&gt;è§†é¢‘é“¾æ¥&lt;/th&gt; 
       &lt;th align="center"&gt;è¯´æ˜&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.bilibili.com/video/BV1h6421c7MA"&gt;Mtoolæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;åˆæ¬¡ä½¿ç”¨æ¨èè§‚çœ‹&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.bilibili.com/video/BV1LgfoYzEaX/?share_source=copy_web&amp;amp;vd_source=b0eede35fc5eaa5c382509c6040d6501"&gt;Translator++æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;åˆæ¬¡ä½¿ç”¨æ¨èè§‚çœ‹&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.bilibili.com/video/BV1SnXbYiEjQ/?share_source=copy_web&amp;amp;vd_source=b0eede35fc5eaa5c382509c6040d6501"&gt;Wolfæ¸¸æˆæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;åˆæ¬¡ä½¿ç”¨æ¨èè§‚çœ‹&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.bilibili.com/video/BV1j1VyzqERD/?share_source=copy_web&amp;amp;vd_source=b0eede35fc5eaa5c382509c6040d6501"&gt;äººåè¯»å–æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;è¿›é˜¶ç¿»è¯‘æ¨èè§‚çœ‹&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;ğŸ«æ¸¸æˆç¿»è¯‘å›¾æ–‡æ•™ç¨‹&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th align="center"&gt;è§†é¢‘é“¾æ¥&lt;/th&gt; 
       &lt;th align="center"&gt;è¯´æ˜&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91%E2%80%90Mtool"&gt;Mtoolæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆæ–°äººï¼Œæ‡’äººç¿»è¯‘RPG,RenPY,Krkrç­‰æ¸¸æˆï¼Œè¿›è¡Œå¤–æŒ‚å¼ç¿»è¯‘&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91%E2%80%90Translator--%EF%BC%88%E5%B7%A5%E7%A8%8B%E6%96%87%E4%BB%B6%E7%89%88%EF%BC%89"&gt;Translator++æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘RPG,RenPY,Krkrç­‰ç­‰æ¸¸æˆï¼Œè¿›è¡Œå†…åµŒå¼ç¿»è¯‘&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91%E2%80%90Paratranz"&gt;Paratranzæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘å„ç±»å¤§å‹æ¸¸æˆçš„MOD&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91%E2%80%90StevExtraction"&gt;StevExtractionæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘RPGmakerMZ/MZæ¸¸æˆ&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://zhuanlan.zhihu.com/p/1894065679927313655"&gt;Unityç¿»è¯‘æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘unityæ¸¸æˆ&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.notion.so/AI-1d43d31f89b280f6bd61e12580652ce5?pvs=4"&gt;ç»¼åˆæ¸¸æˆç¿»è¯‘è¶…è¯¦ç»†æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘å„ç±»æ¸¸æˆï¼Œåˆ¶ä½œé«˜è´¨é‡çš„å†…åµŒè¡¥ä¸&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;h2&gt;åŠŸèƒ½è¯´æ˜&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/#%E5%8A%9F%E8%83%BD%E8%AF%B4%E6%98%8E"&gt;&lt;img src="https://raw.githubusercontent.com/aregtech/areg-sdk/master/docs/img/pin.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/h2&gt; &lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;è®¾ç½®è¯´æ˜&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E5%8A%9F%E8%83%BD%E2%80%90%E6%8E%A5%E5%8F%A3%E7%AE%A1%E7%90%86"&gt;åŠŸèƒ½ â€ æ¥å£ç®¡ç†&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;è¡¨æ ¼è¯´æ˜&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E8%A1%A8%E6%A0%BC%E2%80%90AI%E6%9C%AF%E8%AF%AD%E8%A1%A8%E4%BB%8B%E7%BB%8D"&gt;è¡¨æ ¼ - AIæœ¯è¯­è¡¨&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E8%A1%A8%E6%A0%BC%E2%80%90AI%E7%A6%81%E7%BF%BB%E8%A1%A8%E4%BB%8B%E7%BB%8D"&gt;è¡¨æ ¼ - AIç¦ç¿»è¡¨&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E8%A1%A8%E6%A0%BC%E2%80%90%E6%96%87%E6%9C%AC%E6%9B%BF%E6%8D%A2%E4%BB%8B%E7%BB%8D"&gt;è¡¨æ ¼ - æ–‡æœ¬æ›¿æ¢&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;æ’ä»¶è¯´æ˜&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%8F%92%E4%BB%B6%E2%80%90LanguageFilter"&gt;æ’ä»¶ - è¯­è¨€è¿‡æ»¤å™¨&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%8F%92%E4%BB%B6%E2%80%90TextNormalizer"&gt;æ’ä»¶ - æ–‡æœ¬è§„èŒƒå™¨&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;å…¶ä»–è¯´æ˜&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt; å¤škeyè½®è¯¢&lt;/code&gt;&lt;/p&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;å¦‚æœæƒ³ä½¿ç”¨å¤šä¸ªkeyæ¥åˆ†æ‹…æ¶ˆè€—å‹åŠ›ï¼Œæ ¹æ®keyæ•°é‡è¿›è¡ŒåŠ é€Ÿç¿»è¯‘ï¼Œè¯·ä½¿ç”¨åŒç±»å‹è´¦å·çš„keyï¼Œè€Œä¸”è¾“å…¥æ—¶åœ¨æ¯ä¸ªkeyä¸­é—´åŠ ä¸Šè‹±æ–‡é€—å·ï¼Œä¸è¦æ¢è¡Œã€‚ä¾‹å¦‚ï¼škey1,key2,key3&lt;/p&gt; 
    &lt;/blockquote&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt; æ‰¹é‡æ–‡ä»¶ç¿»è¯‘&lt;/code&gt;&lt;/p&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;æŠŠæ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡ä»¶æ”¾åœ¨è¾“å…¥æ–‡ä»¶å¤¹å³å¯ï¼Œä¹Ÿæ”¯æŒå¤šæ–‡ä»¶å¤¹ç»“æ„&lt;/p&gt; 
    &lt;/blockquote&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt; é…ç½®è¿ç§»&lt;/code&gt;&lt;/p&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;é…ç½®ä¿¡æ¯éƒ½ä¼šå­˜å‚¨åœ¨resourceçš„config.jsonä¸­ï¼Œä¸‹è½½æ–°ç‰ˆæœ¬å¯ä»¥æŠŠå®ƒå¤åˆ¶åˆ°æ–°ç‰ˆæœ¬çš„resourceä¸­ã€‚&lt;/p&gt; 
    &lt;/blockquote&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;h2&gt;è´¡çŒ®æŒ‡å—&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/#%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97"&gt;&lt;img src="https://raw.githubusercontent.com/aregtech/areg-sdk/master/docs/img/pin.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/h2&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;å¼€å‘å¢å¼ºæ’ä»¶&lt;/code&gt;&lt;/strong&gt;: è¯·æ ¹æ®&lt;a href="https://github.com/NEKOparapa/AiNiee/raw/main/PluginScripts/README.md"&gt;æ’ä»¶ç¼–å†™æŒ‡å—&lt;/a&gt;è¿›è¡Œå¼€å‘æ›´å¼ºåŠŸèƒ½æ’ä»¶&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;æ”¹è¿›æˆ–å¢åŠ æ”¯æŒæ–‡ä»¶&lt;/code&gt;&lt;/strong&gt;: éœ€è¦æœ‰ä¸€å®šçš„ä»£ç ç¼–ç¨‹èƒ½åŠ›ï¼Œæ‹‰å–æºç è¿›è¡Œæ”¹è¿›ã€‚æ–‡ä»¶å…·ä½“è¯»å–ä»£ç åœ¨ModuleFolders\FileReaderä¸FileOutputeræ–‡ä»¶å¤¹ä¸­ã€‚&lt;a href="https://github.com/NEKOparapa/AiNiee/raw/main/ModuleFolders/FileAccessor/README.md"&gt;è¯»å†™å™¨ç³»ç»Ÿç¼–å†™æŒ‡å—&lt;/a&gt;ã€‚UIæ”¯æŒåœ¨UserInterface\Settingçš„ProjectSettingsPageã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;å®Œå–„æ­£åˆ™åº“&lt;/code&gt;&lt;/strong&gt;: æ­£åˆ™åº“çš„å®Œå¤‡å°†æå¤§å¸®åŠ©æ¸¸æˆå†…åµŒå·¥ä½œçš„è¿›è¡Œï¼Œå¹¶åˆ©å¥½ä¸‹ä¸€æ¬¡æ¸¸æˆç¿»è¯‘å·¥ä½œå’Œé€ ç¦å…¶ä»–ç¿»è¯‘ç”¨æˆ·ï¼Œæ­£åˆ™åº“åœ¨&lt;a href="https://github.com/NEKOparapa/AiNiee/raw/main/Resource/Regex/regex.json"&gt;Resource\Regex&lt;/a&gt;æ–‡ä»¶å¤¹ä¸­&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;æ”¹è¿›ç•Œé¢ç¿»è¯‘&lt;/code&gt;&lt;/strong&gt;: å¤šè¯­è¨€ç•Œé¢çš„UIæ–‡æœ¬å¯èƒ½ç¿»è¯‘ä¸å¤Ÿå‡†ç¡®åˆé€‚ï¼Œå¯ä»¥æäº¤ä½ çš„ä¿®æ”¹æ„è§ï¼Œæˆ–è€…ç›´æ¥è¿›è¡Œä¿®æ”¹ã€‚æœ¬åœ°åŒ–æ–‡æœ¬åœ¨&lt;a href="https://github.com/NEKOparapa/AiNiee/tree/main/Resource/Localization"&gt;Resource\Localization&lt;/a&gt;æ–‡ä»¶å¤¹ä¸­&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ç‰¹åˆ«å£°æ˜&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/#%E7%89%B9%E5%88%AB%E5%A3%B0%E6%98%8E"&gt;&lt;img src="https://raw.githubusercontent.com/aregtech/areg-sdk/master/docs/img/pin.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;AiNieeèƒ½å¤Ÿä¸æ–­å‘å±•è¿­ä»£è‡³ä»Šï¼Œå…¶å…³é”®åŠŸèƒ½æ¡†æ¶å‡æºäºé¡¹ç›®åˆ›ç«‹ä»¥æ¥çš„æŒç»­ä¸ªäººç ”å‘ã€ç”¨æˆ·åé¦ˆå»ºè®®ä»¥åŠå¤§ä½¬ä»¬PRçš„å…±åŒåŠªåŠ›ä¸åˆ›é€ ã€‚ è¿™æ˜¯ä¸¤å¹´ä»¥æ¥ä¸€ä¸ªä¸æ–­æ‘¸ç´¢ã€æŒç»­æ”¹è¿›ã€å…±åŒæ„ç­‘çš„è¿‡ç¨‹ï¼Œæ‰å½¢æˆäº†AiNieeå¦‚ä»Šç›¸å¯¹æˆç†Ÿå’Œå®Œæ•´çš„AIç¿»è¯‘ä½“ç³»ã€‚ è¯·å¤§å®¶åœ¨ä½¿ç”¨å’Œå­¦ä¹ ä¹‹ä½™ï¼Œå°Šé‡å¼€æºç²¾ç¥ï¼Œç½²åæ¥æºé¡¹ç›®ï¼Œå¹¶ä¸å¿˜äº†ç»™é¡¹ç›®ç‚¹ä¸ªstarã€‚&lt;/p&gt; 
&lt;p&gt;è¯¥æ¬¾AIç¿»è¯‘å·¥å…·ä»…ä¾›ä¸ªäººåˆæ³•ç”¨é€”,ä»»ä½•ä½¿ç”¨è¯¥å·¥å…·è¿›è¡Œç›´æ¥æˆ–è€…é—´æ¥éæ³•ç›ˆåˆ©æ´»åŠ¨çš„è¡Œä¸º,å‡ä¸å±äºæˆæƒèŒƒå›´,ä¹Ÿä¸å—åˆ°ä»»ä½•æ”¯æŒå’Œè®¤å¯ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;äº¤â™‚äº¤æµç¾¤&lt;/code&gt;&lt;/strong&gt;: QQäº¤æµç¾¤(ä¸»è¦æ´»è·ƒï¼Œç­”æ¡ˆï¼šgithub)ï¼š8216248ä¹é›¶ï¼Œå¤‡ç”¨TGç¾¤ï¼š&lt;a href="https://t.me/+JVHbDSGo8SI2Njhl"&gt;https://t.me/+JVHbDSGo8SI2Njhl&lt;/a&gt; ,&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;èµåŠ©ğŸ’–&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee-chatgpt/main/Example%20image/Sponsor/%E8%B5%9E%E8%B5%8F%E7%A0%81.png"&gt;&lt;img src="https://raw.githubusercontent.com/NEKOparapa/AiNiee-chatgpt/main/Example%20image/Sponsor/%E5%BE%BD%E7%AB%A0.png" alt="xxxx" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/AutoAgent</title>
      <link>https://github.com/HKUDS/AutoAgent</link>
      <description>&lt;p&gt;"AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/AutoAgent_logo.svg?sanitize=true" alt="Logo" width="200" /&gt; 
 &lt;h1 align="center"&gt;AutoAgent: Fully-Automated &amp;amp; Zero-Code&lt;br /&gt; LLM Agent Framework &lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://autoagent-ai.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white" alt="Credits" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/metachain-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Slack community" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/jQJdXyDB"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Discord community" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/HKUDS/AutoAgent/raw/main/assets/autoagent-wechat.jpg"&gt;&lt;img src="https://img.shields.io/badge/Wechat-Join%20Us-green?logo=wechat&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Wechat community" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://autoagent-ai.github.io/docs"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2502.05957"&gt;&lt;img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Paper" /&gt;&lt;/a&gt; 
 &lt;a href="https://gaia-benchmark-leaderboard.hf.space/"&gt;&lt;img src="https://img.shields.io/badge/GAIA%20Benchmark-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge" alt="Evaluation Benchmark Score" /&gt;&lt;/a&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13954" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13954" alt="HKUDS%2FAutoAgent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to AutoAgent! AutoAgent is a &lt;strong&gt;Fully-Automated&lt;/strong&gt; and highly &lt;strong&gt;Self-Developing&lt;/strong&gt; framework that enables users to create and deploy LLM agents through &lt;strong&gt;Natural Language Alone&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;âœ¨Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ† Top Performers on the GAIA Benchmark &lt;br /&gt;AutoAgent has delivering comparable performance to many &lt;strong&gt;Deep Research Agents&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;âœ¨ Agent and Workflow Create with Ease &lt;br /&gt;AutoAgent leverages natural language to effortlessly build ready-to-use &lt;strong&gt;tools&lt;/strong&gt;, &lt;strong&gt;agents&lt;/strong&gt; and &lt;strong&gt;workflows&lt;/strong&gt; - no coding required.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ“š Agentic-RAG with Native Self-Managing Vector Database &lt;br /&gt;AutoAgent equipped with a native self-managing vector database, outperforms industry-leading solutions like &lt;strong&gt;LangChain&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸŒ Universal LLM Support &lt;br /&gt;AutoAgent seamlessly integrates with &lt;strong&gt;A Wide Range&lt;/strong&gt; of LLMs (e.g., OpenAI, Anthropic, Deepseek, vLLM, Grok, Huggingface ...)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”€ Flexible Interaction &lt;br /&gt;Benefit from support for both &lt;strong&gt;function-calling&lt;/strong&gt; and &lt;strong&gt;ReAct&lt;/strong&gt; interaction modes.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ¤– Dynamic, Extensible, Lightweight &lt;br /&gt;AutoAgent is your &lt;strong&gt;Personal AI Assistant&lt;/strong&gt;, designed to be dynamic, extensible, customized, and lightweight.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸš€ Unlock the Future of LLM Agents. Try ğŸ”¥AutoAgentğŸ”¥ Now!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AutoAgentnew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/autoagent-intro.svg?sanitize=true" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Quick Overview of AutoAgent.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ”¥ News&lt;/h2&gt; 
&lt;div class="scrollable"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Feb 17]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰We've updated and released AutoAgent v0.2.0 (formerly known as MetaChain). Detailed changes include: 1) fix the bug of different LLM providers from issues; 2) add automatic installation of AutoAgent in the container environment according to issues; 3) add more easy-to-use commands for the CLI mode. 4) Rename the project to AutoAgent for better understanding.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Feb 10]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰We've released &lt;b&gt;MetaChain!&lt;/b&gt;, including framework, evaluation codes and CLI mode! Check our &lt;a href="https://arxiv.org/abs/2502.05957"&gt;paper&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;span id="table-of-contents"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ“‘ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#features"&gt;âœ¨ Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#news"&gt;ğŸ”¥ News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#how-to-use"&gt;ğŸ” How to Use AutoAgent&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#user-mode"&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA ğŸ† Open Deep Research)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#agent-editor"&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#workflow-editor"&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#quick-start"&gt;âš¡ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#api-keys-setup"&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#start-with-cli-mode"&gt;Start with CLI Mode&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#todo"&gt;â˜‘ï¸ Todo List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#reproduce"&gt;ğŸ”¬ How To Reproduce the Results in the Paper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#documentation"&gt;ğŸ“– Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#community"&gt;ğŸ¤ Join the Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#acknowledgements"&gt;ğŸ™ Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#cite"&gt;ğŸŒŸ Cite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="how-to-use"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ” How to Use AutoAgent&lt;/h2&gt; 
&lt;span id="user-mode"&gt;&lt;/span&gt; 
&lt;h3&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA ğŸ† Open Deep Research)&lt;/h3&gt; 
&lt;p&gt;AutoAgent have an out-of-the-box multi-agent system, which you could choose &lt;code&gt;user mode&lt;/code&gt; in the start page to use it. This multi-agent system is a general AI assistant, having the same functionality with &lt;strong&gt;OpenAI's Deep Research&lt;/strong&gt; and the comparable performance with it in &lt;a href="https://gaia-benchmark-leaderboard.hf.space/"&gt;GAIA&lt;/a&gt; benchmark.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;High Performance&lt;/strong&gt;: Matches Deep Research using Claude 3.5 rather than OpenAI's o3 model.&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Model Flexibility&lt;/strong&gt;: Compatible with any LLM (including Deepseek-R1, Grok, Gemini, etc.)&lt;/li&gt; 
 &lt;li&gt;ğŸ’° &lt;strong&gt;Cost-Effective&lt;/strong&gt;: Open-source alternative to Deep Research's $200/month subscription&lt;/li&gt; 
 &lt;li&gt;ğŸ¯ &lt;strong&gt;User-Friendly&lt;/strong&gt;: Easy-to-deploy CLI interface for seamless interaction&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;strong&gt;File Support&lt;/strong&gt;: Handles file uploads for enhanced data interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;video width="80%" controls&gt; 
  &lt;source src="./assets/video_v1_compressed.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; 
 &lt;p&gt;&lt;em&gt;ğŸ¥ Deep Research (aka User Mode)&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;span id="agent-editor"&gt;&lt;/span&gt; 
&lt;h3&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/h3&gt; 
&lt;p&gt;The most distinctive feature of AutoAgent is its natural language customization capability. Unlike other agent frameworks, AutoAgent allows you to create tools, agents, and workflows using natural language alone. Simply choose &lt;code&gt;agent editor&lt;/code&gt; or &lt;code&gt;workflow editor&lt;/code&gt; mode to start your journey of building agents through conversations.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;agent editor&lt;/code&gt; as shown in the following figure.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/1-requirement.png" alt="requirement" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what kind of agent you want to create.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/2-profiling.png" alt="profiling" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Automated agent profiling.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/3-profiles.png" alt="profiles" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Output the agent profiles.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/4-tools.png" alt="tools" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired tools.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/5-task.png" alt="task" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what do you want to complete with the agent. (Optional)&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/6-output-next.png" alt="output" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired agent(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="workflow-editor"&gt;&lt;/span&gt; 
&lt;h3&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/h3&gt; 
&lt;p&gt;You can also create the agent workflows using natural language description with the &lt;code&gt;workflow editor&lt;/code&gt; mode, as shown in the following figure. (Tips: this mode does not support tool creation temporarily.)&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/1-requirement.png" alt="requirement" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what kind of workflow you want to create.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/2-profiling.png" alt="profiling" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Automated workflow profiling.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/3-profiles.png" alt="profiles" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Output the workflow profiles.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/4-task.png" alt="task" width="66%" /&gt; &lt;br /&gt; &lt;em&gt;Input what do you want to complete with the workflow. (Optional)&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/5-output-next.png" alt="output" width="66%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired workflow(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="quick-start"&gt;&lt;/span&gt; 
&lt;h2&gt;âš¡ Quick Start&lt;/h2&gt; 
&lt;span id="installation"&gt;&lt;/span&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;AutoAgent Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/AutoAgent.git
cd AutoAgent
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Installation&lt;/h4&gt; 
&lt;p&gt;We use Docker to containerize the agent-interactive environment. So please install &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; first. You don't need to manually pull the pre-built image, because we have let Auto-Deep-Research &lt;strong&gt;automatically pull the pre-built image based on your architecture of your machine&lt;/strong&gt;.&lt;/p&gt; 
&lt;span id="api-keys-setup"&gt;&lt;/span&gt; 
&lt;h3&gt;API Keys Setup&lt;/h3&gt; 
&lt;p&gt;Create an environment variable file, just like &lt;code&gt;.env.template&lt;/code&gt;, and set the API keys for the LLMs you want to use. Not every LLM API Key is required, use what you need.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Required Github Tokens of your own
GITHUB_AI_TOKEN=

# Optional API Keys
OPENAI_API_KEY=
DEEPSEEK_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
HUGGINGFACE_API_KEY=
GROQ_API_KEY=
XAI_API_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="start-with-cli-mode"&gt;&lt;/span&gt; 
&lt;h3&gt;Start with CLI Mode&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[ğŸš¨ &lt;strong&gt;News&lt;/strong&gt;: ] We have updated a more easy-to-use command to start the CLI mode and fix the bug of different LLM providers from issues. You can follow the following steps to start the CLI mode with different LLM providers with much less configuration.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Command Options:&lt;/h4&gt; 
&lt;p&gt;You can run &lt;code&gt;auto main&lt;/code&gt; to start full part of AutoAgent, including &lt;code&gt;user mode&lt;/code&gt;, &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt;. Btw, you can also run &lt;code&gt;auto deep-research&lt;/code&gt; to start more lightweight &lt;code&gt;user mode&lt;/code&gt;, just like the &lt;a href="https://github.com/HKUDS/Auto-Deep-Research"&gt;Auto-Deep-Research&lt;/a&gt; project. Some configuration of this command is shown below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--container_name&lt;/code&gt;: Name of the Docker container (default: 'deepresearch')&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;: Port for the container (default: 12346)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;COMPLETION_MODEL&lt;/code&gt;: Specify the LLM model to use, you should follow the name of &lt;a href="https://github.com/BerriAI/litellm"&gt;Litellm&lt;/a&gt; to set the model name. (Default: &lt;code&gt;claude-3-5-sonnet-20241022&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DEBUG&lt;/code&gt;: Enable debug mode for detailed logs (default: False)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;API_BASE_URL&lt;/code&gt;: The base URL for the LLM provider (default: None)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;FN_CALL&lt;/code&gt;: Enable function calling (default: None). Most of time, you could ignore this option because we have already set the default value based on the model name.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;git_clone&lt;/code&gt;: Clone the AutoAgent repository to the local environment (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: True)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;test_pull_name&lt;/code&gt;: The name of the test pull. (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: 'autoagent_mirror')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;More details about &lt;code&gt;git_clone&lt;/code&gt; and &lt;code&gt;test_pull_name&lt;/code&gt;]&lt;/h4&gt; 
&lt;p&gt;In the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, we should clone a mirror of the AutoAgent repository to the local agent-interactive environment and let our &lt;strong&gt;AutoAgent&lt;/strong&gt; automatically update the AutoAgent itself, such as creating new tools, agents and workflows. So if you want to use the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, you should set the &lt;code&gt;git_clone&lt;/code&gt; to True and set the &lt;code&gt;test_pull_name&lt;/code&gt; to 'autoagent_mirror' or other branches.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;auto main&lt;/code&gt; with different LLM Providers&lt;/h4&gt; 
&lt;p&gt;Then I will show you how to use the full part of AutoAgent with the &lt;code&gt;auto main&lt;/code&gt; command and different LLM providers. If you want to use the &lt;code&gt;auto deep-research&lt;/code&gt; command, you can refer to the &lt;a href="https://github.com/HKUDS/Auto-Deep-Research"&gt;Auto-Deep-Research&lt;/a&gt; project for more details.&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ANTHROPIC_API_KEY=your_anthropic_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;auto main # default model is claude-3-5-sonnet-20241022
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_openai_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=gpt-4o auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Mistral&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;MISTRAL_API_KEY=your_mistral_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=mistral/mistral-large-2407 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Gemini - Google AI Studio&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GEMINI_API_KEY=your_gemini_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=gemini/gemini-2.0-flash auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Huggingface&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;HUGGINGFACE_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;HUGGINGFACE_API_KEY=your_huggingface_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=huggingface/meta-llama/Llama-3.3-70B-Instruct auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Groq&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GROQ_API_KEY=your_groq_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=groq/deepseek-r1-distill-llama-70b auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenAI-Compatible Endpoints (e.g., Grok)&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_api_key_for_openai_compatible_endpoints
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=openai/grok-2-latest API_BASE_URL=https://api.x.ai/v1 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenRouter (e.g., DeepSeek-R1)&lt;/h5&gt; 
&lt;p&gt;We recommend using OpenRouter as LLM provider of DeepSeek-R1 temporarily. Because official API of DeepSeek-R1 can not be used efficiently.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENROUTER_API_KEY=your_openrouter_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=openrouter/deepseek/deepseek-r1 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;DeepSeek&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;DEEPSEEK_API_KEY=your_deepseek_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=deepseek/deepseek-chat auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After the CLI mode is started, you can see the start page of AutoAgent:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AutoAgentnew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/cover.png" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Start Page of AutoAgent.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;h3&gt;Tips&lt;/h3&gt; 
&lt;h4&gt;Import browser cookies to browser environment&lt;/h4&gt; 
&lt;p&gt;You can import the browser cookies to the browser environment to let the agent better access some specific websites. For more details, please refer to the &lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/AutoAgent/environment/cookie_json/README.md"&gt;cookies&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h4&gt;Add your own API keys for third-party Tool Platforms&lt;/h4&gt; 
&lt;p&gt;If you want to create tools from the third-party tool platforms, such as RapidAPI, you should subscribe tools from the platform and add your own API keys by running &lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/process_tool_docs.py"&gt;process_tool_docs.py&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python process_tool_docs.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More features coming soon! ğŸš€ &lt;strong&gt;Web GUI interface&lt;/strong&gt; under development.&lt;/p&gt; 
&lt;span id="todo"&gt;&lt;/span&gt; 
&lt;h2&gt;â˜‘ï¸ Todo List&lt;/h2&gt; 
&lt;p&gt;AutoAgent is continuously evolving! Here's what's coming:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“Š &lt;strong&gt;More Benchmarks&lt;/strong&gt;: Expanding evaluations to &lt;strong&gt;SWE-bench&lt;/strong&gt;, &lt;strong&gt;WebArena&lt;/strong&gt;, and more&lt;/li&gt; 
 &lt;li&gt;ğŸ–¥ï¸ &lt;strong&gt;GUI Agent&lt;/strong&gt;: Supporting &lt;em&gt;Computer-Use&lt;/em&gt; agents with GUI interaction&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;Tool Platforms&lt;/strong&gt;: Integration with more platforms like &lt;strong&gt;Composio&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;Code Sandboxes&lt;/strong&gt;: Supporting additional environments like &lt;strong&gt;E2B&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¨ &lt;strong&gt;Web Interface&lt;/strong&gt;: Developing comprehensive GUI for better user experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Have ideas or suggestions? Feel free to open an issue! Stay tuned for more exciting updates! ğŸš€&lt;/p&gt; 
&lt;span id="reproduce"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ”¬ How To Reproduce the Results in the Paper&lt;/h2&gt; 
&lt;h3&gt;GAIA Benchmark&lt;/h3&gt; 
&lt;p&gt;For the GAIA benchmark, you can run the following command to run the inference.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/gaia/scripts/run_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the evaluation, you can run the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; python evaluation/gaia/get_score.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Agentic-RAG&lt;/h3&gt; 
&lt;p&gt;For the Agentic-RAG task, you can run the following command to run the inference.&lt;/p&gt; 
&lt;p&gt;Step1. Turn to &lt;a href="https://huggingface.co/datasets/yixuantt/MultiHopRAG"&gt;this page&lt;/a&gt; and download it. Save them to your datapath.&lt;/p&gt; 
&lt;p&gt;Step2. Run the following command to run the inference.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/multihoprag/scripts/run_rag.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Step3. The result will be saved in the &lt;code&gt;evaluation/multihoprag/result.json&lt;/code&gt;.&lt;/p&gt; 
&lt;span id="documentation"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ“– Documentation&lt;/h2&gt; 
&lt;p&gt;A more detailed documentation is coming soon ğŸš€, and we will update in the &lt;a href="https://AutoAgent-ai.github.io/docs"&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; 
&lt;span id="community"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ¤ Join the Community&lt;/h2&gt; 
&lt;p&gt;We want to build a community for AutoAgent, and we welcome everyone to join us. You can join our community by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/AutoAgent-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ"&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/z68KRvwB"&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/issues"&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we're working on, or add your own ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="acknowledgements"&gt;&lt;/span&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/stargazers"&gt;&lt;img src="https://reporoster.com/stars/HKUDS/AutoAgent" alt="Stargazers repo roster for @HKUDS/AutoAgent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/network/members"&gt;&lt;img src="https://reporoster.com/forks/HKUDS/AutoAgent" alt="Forkers repo roster for @HKUDS/AutoAgent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#HKUDS/AutoAgent&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=HKUDS/AutoAgent&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ™ Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Rome wasn't built in a day. AutoAgent stands on the shoulders of giants, and we are deeply grateful for the outstanding work that came before us. Our framework architecture draws inspiration from &lt;a href="https://github.com/openai/swarm"&gt;OpenAI Swarm&lt;/a&gt;, while our user mode's three-agent design benefits from &lt;a href="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one"&gt;Magentic-one&lt;/a&gt;'s insights. We've also learned from &lt;a href="https://github.com/All-Hands-AI/OpenHands"&gt;OpenHands&lt;/a&gt; for documentation structure and many other excellent projects for agent-environment interaction design, among others. We express our sincere gratitude and respect to all these pioneering works that have been instrumental in shaping AutoAgent.&lt;/p&gt; 
&lt;span id="cite"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸŒŸ Cite&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;@misc{AutoAgent,
      title={{AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents}},
      author={Jiabin Tang, Tianyu Fan, Chao Huang},
      year={2025},
      eprint={202502.05957},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.05957},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>PaddlePaddle/PaddleOCR</title>
      <link>https://github.com/PaddlePaddle/PaddleOCR</link>
      <description>&lt;p&gt;Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 80+ languages.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/Banner.png" alt="PaddleOCR Banner" /&gt; &lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_cn.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_tcn.md"&gt;ç¹é«”ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ko.md"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_fr.md"&gt;FranÃ§ais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ru.md"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_es.md"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/readme/README_ar.md"&gt;Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2507.05595"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.05595-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/paddleocr"&gt;&lt;img src="https://static.pepy.tech/badge/paddleocr" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/network/dependents"&gt;&lt;img src="https://img.shields.io/badge/Used%20by-5.9k%2B%20repositories-blue" alt="Used by" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/python-3.8~3.12-aff.svg?sanitize=true" alt="python" /&gt; &lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" alt="os" /&gt; &lt;img src="https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg?sanitize=true" alt="hardware" /&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache_2.0-green" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/PaddlePaddle/PaddleOCR"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PaddleOCR is an industry-leading, production-ready OCR and document AI engine, offering end-to-end solutions from text extraction to intelligent document understanding&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;PaddleOCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.paddlepaddle.org.cn/en"&gt;&lt;img src="https://img.shields.io/badge/PaddlePaddle-3.0-orange" alt="Framework" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Recognition%20Accuracy-%F0%9F%8F%86-green" alt="Accuracy" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Support_Languages-80+-brightgreen" alt="Multi-Language" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Handwriting-%E2%9C%93-success" alt="Handwriting" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/#"&gt;&lt;img src="https://img.shields.io/badge/Heterogeneous%20Hardware-Kunlunxin%20%7C%20Ascend_NPU-red" alt="Hardware" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] PaddleOCR now provides an MCP server that supports integration with Agent applications like Claude Desktop. For details, please refer to &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;PaddleOCR MCP Server&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;The PaddleOCR 3.0 Technical Report is now available. See details at: &lt;a href="https://arxiv.org/abs/2507.05595"&gt;PaddleOCR 3.0 Technical Report&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;PaddleOCR&lt;/strong&gt; converts documents and images into &lt;strong&gt;structured, AI-friendly data&lt;/strong&gt; (like JSON and Markdown) with &lt;strong&gt;industry-leading accuracy&lt;/strong&gt;â€”powering AI applications for everyone from indie developers and startups to large enterprises worldwide. With over &lt;strong&gt;50,000 stars&lt;/strong&gt; and deep integration into leading projects like &lt;strong&gt;MinerU, RAGFlow, and OmniParser&lt;/strong&gt;, PaddleOCR has become the &lt;strong&gt;premier solution&lt;/strong&gt; for developers building intelligent document applications in the &lt;strong&gt;AI era&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;PaddleOCR 3.0 Core Features&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-Demo_on_AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/organization/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%96_Demo_on_ModelScope-purple" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/PaddlePaddle"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-purple.svg?logo=huggingface" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5 â€” Universal Scene Text Recognition&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Single model supports five text types&lt;/strong&gt; (Simplified Chinese, Traditional Chinese, English, Japanese, and Pinyin) with &lt;strong&gt;13% accuracy improvement&lt;/strong&gt;. Solves multilingual mixed document recognition challenges.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3 â€” Complex Document Parsing&lt;/strong&gt;&lt;br /&gt; Intelligently converts complex PDFs and document images into &lt;strong&gt;Markdown and JSON files that preserve original structure&lt;/strong&gt;. &lt;strong&gt;Outperforms&lt;/strong&gt; numerous commercial solutions in public benchmarks. &lt;strong&gt;Perfectly maintains document layout and hierarchical structure&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4 â€” Intelligent Information Extraction&lt;/strong&gt;&lt;br /&gt; Natively integrates ERNIE 4.5 to &lt;strong&gt;precisely extract key information&lt;/strong&gt; from massive documents, with 15% accuracy improvement over previous generation. Makes documents "&lt;strong&gt;understand&lt;/strong&gt;" your questions and provide accurate answers.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to providing an outstanding model library, PaddleOCR 3.0 also offers user-friendly tools covering model training, inference, and service deployment, so developers can rapidly bring AI applications to production.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/Arch.jpg" alt="PaddleOCR Architecture" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Special Note&lt;/strong&gt;: PaddleOCR 3.x introduces several significant interface changes. &lt;strong&gt;Old code written based on PaddleOCR 2.x is likely incompatible with PaddleOCR 3.x&lt;/strong&gt;. Please ensure that the documentation you are reading matches the version of PaddleOCR you are using. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/upgrade_notes.html"&gt;This document&lt;/a&gt; explains the reasons for the upgrade and the major changes from PaddleOCR 2.x to 3.x.&lt;/p&gt; 
&lt;h2&gt;ğŸ“£ Recent updates&lt;/h2&gt; 
&lt;h3&gt;ğŸ”¥ğŸ”¥2025.08.21: Release of PaddleOCR 3.2.0, includes:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Significant Model Additions:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Introduced training, inference, and deployment for PP-OCRv5 recognition models in English, Thai, and Greek. &lt;strong&gt;The PP-OCRv5 English model delivers an 11% improvement in English scenarios compared to the main PP-OCRv5 model, with the Thai and Greek recognition models achieving accuracies of 82.68% and 89.28%, respectively.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deployment Capability Upgrades:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Full support for PaddlePaddle framework versions 3.1.0 and 3.1.1.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Comprehensive upgrade of the PP-OCRv5 C++ local deployment solution, now supporting both Linux and Windows, with feature parity and identical accuracy to the Python implementation.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;High-performance inference now supports CUDA 12, and inference can be performed using either the Paddle Inference or ONNX Runtime backends.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;The high-stability service-oriented deployment solution is now fully open-sourced, allowing users to customize Docker images and SDKs as required.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;The high-stability service-oriented deployment solution also supports invocation via manually constructed HTTP requests, enabling client-side code development in any programming language.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Benchmark Support:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All production lines now support fine-grained benchmarking, enabling measurement of end-to-end inference time as well as per-layer and per-module latency data to assist with performance analysis. &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/pipeline_usage/instructions/benchmark.en.md"&gt;Here's&lt;/a&gt; how to set up and use the benchmark feature.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Documentation has been updated to include key metrics for commonly used configurations on mainstream hardware, such as inference latency and memory usage, providing deployment references for users.&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Resolved the issue of failed log saving during model training.&lt;/li&gt; 
   &lt;li&gt;Upgraded the data augmentation component for formula models for compatibility with newer versions of the albumentations dependency, and fixed deadlock warnings when using the tokenizers package in multi-process scenarios.&lt;/li&gt; 
   &lt;li&gt;Fixed inconsistencies in switch behaviors (e.g., &lt;code&gt;use_chart_parsing&lt;/code&gt;) in the PP-StructureV3 configuration files compared to other pipelines.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Other Enhancements:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Separated core and optional dependencies. Only minimal core dependencies are required for basic text recognition; additional dependencies for document parsing and information extraction can be installed as needed.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enabled support for NVIDIA RTX 50 series graphics cards on Windows; users can refer to the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/version3.x/installation.en.md"&gt;installation guide&lt;/a&gt; for the corresponding PaddlePaddle framework versions.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PP-OCR series models now support returning single-character coordinates.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;Added AIStudio, ModelScope, and other model download sources, allowing users to specify the source for model downloads.&lt;/li&gt; 
   &lt;li&gt;Added support for chart-to-table conversion via the PP-Chart2Table module.&lt;/li&gt; 
   &lt;li&gt;Optimized documentation descriptions to improve usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.08.15: PaddleOCR 3.1.1 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added the missing methods &lt;code&gt;save_vector&lt;/code&gt;, &lt;code&gt;save_visual_info_list&lt;/code&gt;, &lt;code&gt;load_vector&lt;/code&gt;, and &lt;code&gt;load_visual_info_list&lt;/code&gt; in the &lt;code&gt;PP-ChatOCRv4&lt;/code&gt; class.&lt;/li&gt; 
    &lt;li&gt;Added the missing parameters &lt;code&gt;glossary&lt;/code&gt; and &lt;code&gt;llm_request_interval&lt;/code&gt; to the &lt;code&gt;translate&lt;/code&gt; method in the &lt;code&gt;PPDocTranslation&lt;/code&gt; class.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Added a demo to the MCP documentation.&lt;/li&gt; 
    &lt;li&gt;Added information about the PaddlePaddle and PaddleOCR version used for performance metrics testing in the documentation.&lt;/li&gt; 
    &lt;li&gt;Fixed errors and omissions in the production line document translation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Changed the MCP server dependency to use the pure Python library &lt;code&gt;puremagic&lt;/code&gt; instead of &lt;code&gt;python-magic&lt;/code&gt; to reduce installation issues.&lt;/li&gt; 
    &lt;li&gt;Retested PP-OCRv5 performance metrics with PaddleOCR version 3.1.0 and updated the documentation.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.29: PaddleOCR 3.1.0 Released&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Key Models and Pipelines:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Added PP-OCRv5 Multilingual Text Recognition Model&lt;/strong&gt;, which supports the training and inference process for text recognition models in 37 languages, including French, Spanish, Portuguese, Russian, Korean, etc. &lt;strong&gt;Average accuracy improved by over 30%.&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;Upgraded the &lt;strong&gt;PP-Chart2Table model&lt;/strong&gt; in PP-StructureV3, further enhancing the capability of converting charts to tables. On internal custom evaluation sets, the metric (RMS-F1) &lt;strong&gt;increased by 9.36 percentage points (71.24% -&amp;gt; 80.60%).&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Newly launched &lt;strong&gt;document translation pipeline, PP-DocTranslation, based on PP-StructureV3 and ERNIE 4.5&lt;/strong&gt;, which supports the translation of Markdown format documents, various complex-layout PDF documents, and document images, with the results saved as Markdown format documents. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/PP-DocTranslation.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;New MCP server:&lt;/strong&gt; &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html"&gt;Details&lt;/a&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Supports both OCR and PP-StructureV3 pipelines.&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Supports three working modes: local Python library, AIStudio Community Cloud Service, and self-hosted service.&lt;/li&gt; 
    &lt;li&gt;Supports invoking local services via stdio and remote services via Streamable HTTP.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Optimization:&lt;/strong&gt; Improved the descriptions in some user guides for a smoother reading experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.26: PaddleOCR 3.0.3 Released&lt;/strong&gt;&lt;/summary&gt; - Bug Fix: Resolved the issue where the `enable_mkldnn` parameter was not effective, restoring the default behavior of using MKL-DNN for CPU inference. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;2025.06.19: PaddleOCR 3.0.2 Released&lt;/strong&gt;&lt;/summary&gt; - **New Features:** 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;The default download source has been changed from &lt;code&gt;BOS&lt;/code&gt; to &lt;code&gt;HuggingFace&lt;/code&gt;. Users can also change the environment variable &lt;code&gt;PADDLE_PDX_MODEL_SOURCE&lt;/code&gt; to &lt;code&gt;BOS&lt;/code&gt; to set the model download source back to Baidu Object Storage (BOS).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added service invocation examples for six languagesâ€”C++, Java, Go, C#, Node.js, and PHPâ€”for pipelines like PP-OCRv5, PP-StructureV3, and PP-ChatOCRv4.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Improved the layout partition sorting algorithm in the PP-StructureV3 pipeline, enhancing the sorting logic for complex vertical layouts to deliver better results.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Enhanced model selection logic: when a language is specified but a model version is not, the system will automatically select the latest model version supporting that language.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Set a default upper limit for MKL-DNN cache size to prevent unlimited growth, while also allowing users to configure cache capacity.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Updated default configurations for high-performance inference to support Paddle MKL-DNN acceleration and optimized the logic for automatic configuration selection for smarter choices.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Adjusted the logic for obtaining the default device to consider the actual support for computing devices by the installed Paddle framework, making program behavior more intuitive.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Added Android example for PP-OCRv5. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/on_device_deployment.html"&gt;Details&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bug Fixes:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Fixed an issue with some CLI parameters in PP-StructureV3 not taking effect.&lt;/li&gt; 
    &lt;li&gt;Resolved an issue where &lt;code&gt;export_paddlex_config_to_yaml&lt;/code&gt; would not function correctly in certain cases.&lt;/li&gt; 
    &lt;li&gt;Corrected the discrepancy between the actual behavior of &lt;code&gt;save_path&lt;/code&gt; and its documentation description.&lt;/li&gt; 
    &lt;li&gt;Fixed potential multithreading errors when using MKL-DNN in basic service deployment.&lt;/li&gt; 
    &lt;li&gt;Corrected channel order errors in image preprocessing for the Latex-OCR model.&lt;/li&gt; 
    &lt;li&gt;Fixed channel order errors in saving visualized images within the text recognition module.&lt;/li&gt; 
    &lt;li&gt;Resolved channel order errors in visualized table results within PP-StructureV3 pipeline.&lt;/li&gt; 
    &lt;li&gt;Fixed an overflow issue in the calculation of &lt;code&gt;overlap_ratio&lt;/code&gt; under extremely special circumstances in the PP-StructureV3 pipeline.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Documentation Improvements:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the description of the &lt;code&gt;enable_mkldnn&lt;/code&gt; parameter in the documentation to accurately reflect the program's actual behavior.&lt;/li&gt; 
    &lt;li&gt;Fixed errors in the documentation regarding the &lt;code&gt;lang&lt;/code&gt; and &lt;code&gt;ocr_version&lt;/code&gt; parameters.&lt;/li&gt; 
    &lt;li&gt;Added instructions for exporting pipeline configuration files via CLI.&lt;/li&gt; 
    &lt;li&gt;Fixed missing columns in the performance data table for PP-OCRv5.&lt;/li&gt; 
    &lt;li&gt;Refined benchmark metrics for PP-StructureV3 across different configurations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Others:&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Relaxed version restrictions on dependencies like numpy and pandas, restoring support for Python 3.12.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;History Log&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;2025.06.05: &lt;strong&gt;PaddleOCR 3.0.1 Released&lt;/strong&gt;, includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Optimisation of certain models and model configurations:&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Updated the default model configuration for PP-OCRv5, changing both detection and recognition from mobile to server models. To improve default performance in most scenarios, the parameter &lt;code&gt;limit_side_len&lt;/code&gt; in the configuration has been changed from 736 to 64.&lt;/li&gt; 
    &lt;li&gt;Added a new text line orientation classification model &lt;code&gt;PP-LCNet_x1_0_textline_ori&lt;/code&gt; with an accuracy of 99.42%. The default text line orientation classifier for OCR, PP-StructureV3, and PP-ChatOCRv4 pipelines has been updated to this model.&lt;/li&gt; 
    &lt;li&gt;Optimized the text line orientation classification model &lt;code&gt;PP-LCNet_x0_25_textline_ori&lt;/code&gt;, improving accuracy by 3.3 percentage points to a current accuracy of 98.85%.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimizations and fixes for some issues in version 3.0.0, &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;details&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ğŸ”¥ğŸ”¥2025.05.20: Official Release of &lt;strong&gt;PaddleOCR v3.0&lt;/strong&gt;, including:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-OCRv5&lt;/strong&gt;: High-Accuracy Text Recognition Model for All Scenarios - Instant Text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸŒ Single-model support for &lt;strong&gt;five&lt;/strong&gt; text types - Seamlessly process &lt;strong&gt;Simplified Chinese, Traditional Chinese, Simplified Chinese Pinyin, English&lt;/strong&gt; and &lt;strong&gt;Japanese&lt;/strong&gt; within a single model.&lt;/li&gt; 
    &lt;li&gt;âœï¸ Improved &lt;strong&gt;handwriting recognition&lt;/strong&gt;: Significantly better at complex cursive scripts and non-standard handwriting.&lt;/li&gt; 
    &lt;li&gt;ğŸ¯ &lt;strong&gt;13-point accuracy gain&lt;/strong&gt; over PP-OCRv4, achieving state-of-the-art performance across a variety of real-world scenarios.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt;: General-Purpose Document Parsing â€“ Unleash SOTA Images/PDFs Parsing for Real-World Scenarios!&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸ§® &lt;strong&gt;High-Accuracy multi-scene PDF parsing&lt;/strong&gt;, leading both open- and closed-source solutions on the OmniDocBench benchmark.&lt;/li&gt; 
    &lt;li&gt;ğŸ§  Specialized capabilities include &lt;strong&gt;seal recognition&lt;/strong&gt;, &lt;strong&gt;chart-to-table conversion&lt;/strong&gt;, &lt;strong&gt;table recognition with nested formulas/images&lt;/strong&gt;, &lt;strong&gt;vertical text document parsing&lt;/strong&gt;, and &lt;strong&gt;complex table structure analysis&lt;/strong&gt;.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;PP-ChatOCRv4&lt;/strong&gt;: Intelligent Document Understanding â€“ Extract Key Information, not just text from Images/PDFs.&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ğŸ”¥ &lt;strong&gt;15-point accuracy gain&lt;/strong&gt; in key-information extraction on PDF/PNG/JPG files over the previous generation.&lt;/li&gt; 
    &lt;li&gt;ğŸ’» Native support for &lt;strong&gt;ERNIE 4.5&lt;/strong&gt;, with compatibility for large-model deployments via PaddleNLP, Ollama, vLLM, and more.&lt;/li&gt; 
    &lt;li&gt;ğŸ¤ Integrated &lt;a href="https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2"&gt;PP-DocBee2&lt;/a&gt;, enabling extraction and understanding of printed text, handwriting, seals, tables, charts, and other common elements in complex documents.&lt;/li&gt; 
   &lt;/ol&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/update/update.html"&gt;History Log&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;âš¡ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Run online demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aistudio.baidu.com/community/app/91660/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_OCRv5-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518494/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_StructureV3-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt; &lt;a href="https://aistudio.baidu.com/community/app/518493/webUI"&gt;&lt;img src="https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green" alt="AI Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install PaddlePaddle refer to &lt;a href="https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/develop/install/pip/linux-pip_en.html"&gt;Installation Guide&lt;/a&gt;, after then, install the PaddleOCR toolkit.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If you only want to use the basic text recognition feature (returns text position coordinates and content), including the PP-OCR series
python -m pip install paddleocr
# If you want to use all features such as document parsing, document understanding, document translation, key information extraction, etc.
# python -m pip install "paddleocr[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting from version 3.2.0, in addition to the &lt;code&gt;all&lt;/code&gt; dependency group demonstrated above, PaddleOCR also supports installing partial optional features by specifying other dependency groups. All dependency groups provided by PaddleOCR are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dependency Group Name&lt;/th&gt; 
   &lt;th&gt;Corresponding Functionality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doc-parser&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document parsing: can be used to extract layout elements such as tables, formulas, stamps, images, etc. from documents; includes models like PP-StructureV3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ie&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Information extraction: can be used to extract key information from documents, such as names, dates, addresses, amounts, etc.; includes models like PP-ChatOCRv4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;trans&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Document translation: can be used to translate documents from one language to another; includes models like PP-DocTranslation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;all&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Complete functionality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;3. Run inference by CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run PP-OCRv5 inference
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False  

# Run PP-StructureV3 inference
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# Get the Qianfan API Key at first, and then run PP-ChatOCRv4 inference
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k é©¾é©¶å®¤å‡†ä¹˜äººæ•° --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# Get more information about "paddleocr ocr"
paddleocr ocr --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Run inference by API&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;4.1 PP-OCRv5 Example&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize PaddleOCR instance
from paddleocr import PaddleOCR
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)

# Run OCR inference on a sample image 
result = ocr.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png")

# Visualize the results and save the JSON results
for res in result:
    res.print()
    res.save_to_img("output")
    res.save_to_json("output")
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png",
)

# Visualize the results and save the JSON results
for res in output:
    res.print() 
    res.save_to_json(save_path="output") 
    res.save_to_markdown(save_path="output")           
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 Example&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    "module_name": "chat_bot",
    "model_name": "ernie-3.5-8k",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "openai",
    "api_key": "api_key",  # your api_key
}

retriever_config = {
    "module_name": "retriever",
    "model_name": "embedding-v1",
    "base_url": "https://qianfan.baidubce.com/v2",
    "api_type": "qianfan",
    "api_key": "api_key",  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

mllm_predict_info = None
use_mllm = False
# If a multimodal large model is used, the local mllm service needs to be started. You can refer to the documentation: https://github.com/PaddlePaddle/PaddleX/blob/release/3.0/docs/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.en.md performs deployment and updates the mllm_chat_bot_config configuration.
if use_mllm:
    mllm_chat_bot_config = {
        "module_name": "chat_bot",
        "model_name": "PP-DocBee",
        "base_url": "http://127.0.0.1:8080/",  # your local mllm service url
        "api_type": "openai",
        "api_key": "api_key",  # your api_key
    }

    mllm_predict_res = pipeline.mllm_pred(
        input="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png",
        key_list=["é©¾é©¶å®¤å‡†ä¹˜äººæ•°"],
        mllm_chat_bot_config=mllm_chat_bot_config,
    )
    mllm_predict_info = mllm_predict_res["mllm_res"]

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res["visual_info"])
    layout_parsing_result = res["layout_parsing_result"]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
chat_result = pipeline.chat(
    key_list=["é©¾é©¶å®¤å‡†ä¹˜äººæ•°"],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. Chinese Heterogeneous AI Accelerators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html"&gt;Huawei Ascend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html"&gt;KUNLUNXIN&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ§© More Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert models to ONNX format: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/obtaining_onnx_models.html"&gt;Obtaining ONNX Models&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using engines like OpenVINO, ONNX Runtime, TensorRT, or perform inference using ONNX format models: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/high_performance_inference.html"&gt;High-Performance Inference&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerate inference using multi-GPU and multi-process: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/pipeline_usage/instructions/parallel_inference.html"&gt;Parallel Inference for Pipelines&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate PaddleOCR into applications written in C++, C#, Java, etc.: &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/serving.html"&gt;Serving&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â›°ï¸ Advanced Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html"&gt;PP-OCRv5 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html"&gt;PP-StructureV3 Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html"&gt;PP-ChatOCRv4 Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”„ Quick Overview of Execution Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/demo.gif" alt="PP-OCRv5 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/docs/images/blue_v3.gif" alt="PP-StructureV3 Demo" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;âœ¨ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;â­ &lt;strong&gt;Star this repository to keep up with exciting updates and new releases, including powerful OCR and document parsing capabilities!&lt;/strong&gt; â­&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="1200" src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/main/images/paddleocr/README/star_paddleocr.en.gif" alt="Star-Project" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Community&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;PaddlePaddle WeChat official account&lt;/th&gt; 
    &lt;th align="center"&gt;Join the tech discussion group&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg" width="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg" width="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ˜ƒ Awesome Projects Leveraging PaddleOCR&lt;/h2&gt; 
&lt;p&gt;PaddleOCR wouldn't be where it is today without its incredible community! ğŸ’— A massive thank you to all our longtime partners, new collaborators, and everyone who's poured their passion into PaddleOCR â€” whether we've named you or not. Your support fuels our fire!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Project Name&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow"&gt;&lt;img src="https://img.shields.io/github/stars/infiniflow/ragflow" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;RAG engine based on deep document understanding.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Multi-type Document to Markdown Conversion Tool&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;Umi-OCR&lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;&lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Free, Open-source, Batch Offline OCR Software.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;OmniParser&lt;/a&gt;&lt;a href="https://github.com/microsoft/OmniParser"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/OmniParser" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;OmniParser: Screen Parsing tool for Pure Vision Based GUI Agent.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;QAnything&lt;/a&gt;&lt;a href="https://github.com/netease-youdao/QAnything"&gt;&lt;img src="https://img.shields.io/github/stars/netease-youdao/QAnything" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Question and Answer based on Anything.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt; &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;A powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;Dango-Translator&lt;/a&gt;&lt;a href="https://github.com/PantsuDango/Dango-Translator"&gt;&lt;img src="https://img.shields.io/github/stars/PantsuDango/Dango-Translator" /&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Recognize text on the screen, translate it and show the translation results in real time.&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;Learn more projects&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/awesome_projects.md"&gt;More projects based on PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Contributors&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;amp;max=400&amp;amp;columns=20" width="800" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ Star&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;img width="800" src="https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;amp;type=Date" alt="Star-history" /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is released under the &lt;a href="https://raw.githubusercontent.com/PaddlePaddle/PaddleOCR/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{cui2025paddleocr30technicalreport,
      title={PaddleOCR 3.0 Technical Report}, 
      author={Cheng Cui and Ting Sun and Manhui Lin and Tingquan Gao and Yubo Zhang and Jiaxuan Liu and Xueqing Wang and Zelun Zhang and Changda Zhou and Hongen Liu and Yue Zhang and Wenyu Lv and Kui Huang and Yichao Zhang and Jing Zhang and Jun Zhang and Yi Liu and Dianhai Yu and Yanjun Ma},
      year={2025},
      eprint={2507.05595},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.05595}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>CorentinJ/Real-Time-Voice-Cloning</title>
      <link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link>
      <description>&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; 
&lt;p&gt;This repository is an implementation of &lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href="https://matheo.uliege.be/handle/2268.2/6801"&gt;master's thesis&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-O_hYhToKoA"&gt;&lt;img src="https://i.imgur.com/8lFUlgz.png" alt="Toolbox demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Papers implemented&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;Designation&lt;/th&gt; 
   &lt;th&gt;Title&lt;/th&gt; 
   &lt;th&gt;Implementation source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1806.04558.pdf"&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1802.08435.pdf"&gt;1802.08435&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; 
   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1703.10135.pdf"&gt;1703.10135&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; 
   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fatchord/WaveRNN"&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/1710.10467.pdf"&gt;1710.10467&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GE2E (encoder)&lt;/td&gt; 
   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Heads up&lt;/h2&gt; 
&lt;p&gt;Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out &lt;a href="https://paperswithcode.com/task/speech-synthesis/"&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;Chatterbox&lt;/a&gt; for a similar project up to date with the 2025 SOTA in voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;h3&gt;1. Install Requirements&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.&lt;/li&gt; 
 &lt;li&gt;Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using &lt;code&gt;venv&lt;/code&gt;, but this is optional.&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://ffmpeg.org/download.html#get-packages"&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files.&lt;/li&gt; 
 &lt;li&gt;Install &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt;. Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.&lt;/li&gt; 
 &lt;li&gt;Install the remaining requirements with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. (Optional) Download Pretrained Models&lt;/h3&gt; 
&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;3. (Optional) Test Configuration&lt;/h3&gt; 
&lt;p&gt;Before you download any dataset, you can begin by testing your configuration with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python demo_cli.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If all tests pass, you're good to go.&lt;/p&gt; 
&lt;h3&gt;4. (Optional) Download Datasets&lt;/h3&gt; 
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href="https://www.openslr.org/resources/12/train-clean-100.tar.gz"&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets"&gt;here&lt;/a&gt;. You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt; 
&lt;h3&gt;5. Launch the Toolbox&lt;/h3&gt; 
&lt;p&gt;You can then try the toolbox:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;br /&gt; or&lt;br /&gt; &lt;code&gt;python demo_toolbox.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;depending on whether you downloaded any datasets. If you are running an X-server or if you have the error &lt;code&gt;Aborted (core dumped)&lt;/code&gt;, see &lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590"&gt;this issue&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ahujasid/blender-mcp</title>
      <link>https://github.com/ahujasid/blender-mcp</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BlenderMCP - Blender Model Context Protocol Integration&lt;/h1&gt; 
&lt;p&gt;BlenderMCP connects Blender to Claude AI through the Model Context Protocol (MCP), allowing Claude to directly interact with and control Blender. This integration enables prompt assisted 3D modeling, scene creation, and manipulation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;We have no official website. Any website you see online is unofficial and has no affiliation with this project. Use them at your own risk.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=lCyQ717DuzQ"&gt;Full tutorial&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;p&gt;Give feedback, get inspired, and build on top of the MCP: &lt;a href="https://discord.gg/z5apgR8TFU"&gt;Discord&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Supporters&lt;/h3&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.warp.dev/blender-mcp"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/c21102f7-bab9-4344-a731-0cf6b341cab2" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="https://www.warp.dev/blender-mcp"&gt;Warp, the intelligent terminal for developers&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://www.warp.dev/blender-mcp"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Other supporters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.coderabbit.ai/"&gt;CodeRabbit&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/satishgoda"&gt;Satish Goda&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;All supporters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/ahujasid"&gt;Support this project&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release notes (1.2.0)&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;View screenshots for Blender viewport to better understand the scene&lt;/li&gt; 
 &lt;li&gt;Search and download Sketchfab models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Previously added features:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for Poly Haven assets through their API&lt;/li&gt; 
 &lt;li&gt;Support to generate 3D models using Hyper3D Rodin&lt;/li&gt; 
 &lt;li&gt;For newcomers, you can go straight to Installation. For existing users, see the points below&lt;/li&gt; 
 &lt;li&gt;Download the latest addon.py file and replace the older one, then add it to Blender&lt;/li&gt; 
 &lt;li&gt;Delete the MCP server from Claude and add it back again, and you should be good to go!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Two-way communication&lt;/strong&gt;: Connect Claude AI to Blender through a socket-based server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Object manipulation&lt;/strong&gt;: Create, modify, and delete 3D objects in Blender&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Material control&lt;/strong&gt;: Apply and modify materials and colors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scene inspection&lt;/strong&gt;: Get detailed information about the current Blender scene&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code execution&lt;/strong&gt;: Run arbitrary Python code in Blender from Claude&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Components&lt;/h2&gt; 
&lt;p&gt;The system consists of two main components:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Blender Addon (&lt;code&gt;addon.py&lt;/code&gt;)&lt;/strong&gt;: A Blender addon that creates a socket server within Blender to receive and execute commands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Server (&lt;code&gt;src/blender_mcp/server.py&lt;/code&gt;)&lt;/strong&gt;: A Python server that implements the Model Context Protocol and connects to the Blender addon&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blender 3.0 or newer&lt;/li&gt; 
 &lt;li&gt;Python 3.10 or newer&lt;/li&gt; 
 &lt;li&gt;uv package manager:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;If you're on Mac, please install uv as&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;On Windows&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;powershell -c "irm https://astral.sh/uv/install.ps1 | iex" 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and then&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;set Path=C:\Users\nntra\.local\bin;%Path%
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Otherwise installation instructions are on their website: &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;Install uv&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;âš ï¸ Do not proceed before installing UV&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;The following environment variables can be used to configure the Blender connection:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;BLENDER_HOST&lt;/code&gt;: Host address for Blender socket server (default: "localhost")&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;BLENDER_PORT&lt;/code&gt;: Port number for Blender socket server (default: 9876)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export BLENDER_HOST='host.docker.internal'
export BLENDER_PORT=9876
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Claude for Desktop Integration&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=neoK_WMq92g"&gt;Watch the setup instruction video&lt;/a&gt; (Assuming you have already installed uv)&lt;/p&gt; 
&lt;p&gt;Go to Claude &amp;gt; Settings &amp;gt; Developer &amp;gt; Edit Config &amp;gt; claude_desktop_config.json to include the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "uvx",
            "args": [
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Cursor integration&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://cursor.com/install-mcp?name=blender&amp;amp;config=eyJjb21tYW5kIjoidXZ4IGJsZW5kZXItbWNwIn0%3D"&gt;&lt;img src="https://cursor.com/deeplink/mcp-install-dark.svg?sanitize=true" alt="Install MCP Server" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For Mac users, go to Settings &amp;gt; MCP and paste the following&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To use as a global server, use "add new global MCP server" button and paste&lt;/li&gt; 
 &lt;li&gt;To use as a project specific server, create &lt;code&gt;.cursor/mcp.json&lt;/code&gt; in the root of the project and paste&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "uvx",
            "args": [
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Windows users, go to Settings &amp;gt; MCP &amp;gt; Add Server, add a new server with the following settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "cmd",
            "args": [
                "/c",
                "uvx",
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wgWsJshecac"&gt;Cursor setup video&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;âš ï¸ Only run one instance of the MCP server (either on Cursor or Claude Desktop), not both&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Visual Studio Code Integration&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Prerequisites&lt;/em&gt;: Make sure you have &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; installed before proceeding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="vscode:mcp/install?%7B%22name%22%3A%22blender-mcp%22%2C%22type%22%3A%22stdio%22%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22blender-mcp%22%5D%7D"&gt;&lt;img src="https://img.shields.io/badge/VS_Code-Install_blender--mcp_server-0098FF?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=ffffff" alt="Install in VS Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Installing the Blender Addon&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the &lt;code&gt;addon.py&lt;/code&gt; file from this repo&lt;/li&gt; 
 &lt;li&gt;Open Blender&lt;/li&gt; 
 &lt;li&gt;Go to Edit &amp;gt; Preferences &amp;gt; Add-ons&lt;/li&gt; 
 &lt;li&gt;Click "Install..." and select the &lt;code&gt;addon.py&lt;/code&gt; file&lt;/li&gt; 
 &lt;li&gt;Enable the addon by checking the box next to "Interface: Blender MCP"&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Starting the Connection&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ahujasid/blender-mcp/main/assets/addon-instructions.png" alt="BlenderMCP in the sidebar" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;In Blender, go to the 3D View sidebar (press N if not visible)&lt;/li&gt; 
 &lt;li&gt;Find the "BlenderMCP" tab&lt;/li&gt; 
 &lt;li&gt;Turn on the Poly Haven checkbox if you want assets from their API (optional)&lt;/li&gt; 
 &lt;li&gt;Click "Connect to Claude"&lt;/li&gt; 
 &lt;li&gt;Make sure the MCP server is running in your terminal&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Using with Claude&lt;/h3&gt; 
&lt;p&gt;Once the config file has been set on Claude, and the addon is running on Blender, you will see a hammer icon with tools for the Blender MCP.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ahujasid/blender-mcp/main/assets/hammer-icon.png" alt="BlenderMCP in the sidebar" /&gt;&lt;/p&gt; 
&lt;h4&gt;Capabilities&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get scene and object information&lt;/li&gt; 
 &lt;li&gt;Create, delete and modify shapes&lt;/li&gt; 
 &lt;li&gt;Apply or create materials for objects&lt;/li&gt; 
 &lt;li&gt;Execute any Python code in Blender&lt;/li&gt; 
 &lt;li&gt;Download the right models, assets and HDRIs through &lt;a href="https://polyhaven.com/"&gt;Poly Haven&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AI generated 3D models through &lt;a href="https://hyper3d.ai/"&gt;Hyper3D Rodin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example Commands&lt;/h3&gt; 
&lt;p&gt;Here are some examples of what you can ask Claude to do:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Create a low poly scene in a dungeon, with a dragon guarding a pot of gold" &lt;a href="https://www.youtube.com/watch?v=DqgKuLYUv00"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Create a beach vibe using HDRIs, textures, and models like rocks and vegetation from Poly Haven" &lt;a href="https://www.youtube.com/watch?v=I29rn92gkC4"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Give a reference image, and create a Blender scene out of it &lt;a href="https://www.youtube.com/watch?v=FDRb03XPiRo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Generate a 3D model of a garden gnome through Hyper3D"&lt;/li&gt; 
 &lt;li&gt;"Get information about the current scene, and make a threejs sketch from it" &lt;a href="https://www.youtube.com/watch?v=jxbNI5L7AH8"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Make this car red and metallic"&lt;/li&gt; 
 &lt;li&gt;"Create a sphere and place it above the cube"&lt;/li&gt; 
 &lt;li&gt;"Make the lighting like a studio"&lt;/li&gt; 
 &lt;li&gt;"Point the camera at the scene, and make it isometric"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hyper3D integration&lt;/h2&gt; 
&lt;p&gt;Hyper3D's free trial key allows you to generate a limited number of models per day. If the daily limit is reached, you can wait for the next day's reset or obtain your own key from hyper3d.ai and fal.ai.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connection issues&lt;/strong&gt;: Make sure the Blender addon server is running, and the MCP server is configured on Claude, DO NOT run the uvx command in the terminal. Sometimes, the first command won't go through but after that it starts working.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Timeout errors&lt;/strong&gt;: Try simplifying your requests or breaking them into smaller steps&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Poly Haven integration&lt;/strong&gt;: Claude is sometimes erratic with its behaviour&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Have you tried turning it off and on again?&lt;/strong&gt;: If you're still having connection errors, try restarting both Claude and the Blender server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Technical Details&lt;/h2&gt; 
&lt;h3&gt;Communication Protocol&lt;/h3&gt; 
&lt;p&gt;The system uses a simple JSON-based protocol over TCP sockets:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Commands&lt;/strong&gt; are sent as JSON objects with a &lt;code&gt;type&lt;/code&gt; and optional &lt;code&gt;params&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responses&lt;/strong&gt; are JSON objects with a &lt;code&gt;status&lt;/code&gt; and &lt;code&gt;result&lt;/code&gt; or &lt;code&gt;message&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Limitations &amp;amp; Security Considerations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;execute_blender_code&lt;/code&gt; tool allows running arbitrary Python code in Blender, which can be powerful but potentially dangerous. Use with caution in production environments. ALWAYS save your work before using it.&lt;/li&gt; 
 &lt;li&gt;Poly Haven requires downloading models, textures, and HDRI images. If you do not want to use it, please turn it off in the checkbox in Blender.&lt;/li&gt; 
 &lt;li&gt;Complex operations might need to be broken down into smaller steps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This is a third-party integration and not made by Blender. Made by &lt;a href="https://x.com/sidahuj"&gt;Siddharth&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>unclecode/crawl4ai</title>
      <link>https://github.com/unclecode/crawl4ai</link>
      <description>&lt;p&gt;ğŸš€ğŸ¤– Crawl4AI: Open-source LLM Friendly Web Crawler &amp; Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸš€ğŸ¤– Crawl4AI: Open-source LLM Friendly Web Crawler &amp;amp; Scraper.&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11716" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11716" alt="unclecode%2Fcrawl4ai | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/unclecode/crawl4ai/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/unclecode/crawl4ai?style=social" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/unclecode/crawl4ai/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/unclecode/crawl4ai?style=social" alt="GitHub Forks" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/crawl4ai"&gt;&lt;img src="https://badge.fury.io/py/crawl4ai.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/crawl4ai/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/crawl4ai" alt="Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/crawl4ai"&gt;&lt;img src="https://static.pepy.tech/badge/crawl4ai/month" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/unclecode"&gt;&lt;img src="https://img.shields.io/github/sponsors/unclecode?style=flat&amp;amp;logo=GitHub-Sponsors&amp;amp;label=Sponsors&amp;amp;color=pink" alt="GitHub Sponsors" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://x.com/crawl4ai"&gt; &lt;img src="https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Follow on X" /&gt; &lt;/a&gt; &lt;a href="https://www.linkedin.com/company/crawl4ai"&gt; &lt;img src="https://img.shields.io/badge/Follow%20on%20LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="Follow on LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/jP8KfhDhyN"&gt; &lt;img src="https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join our Discord" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Crawl4AI turns the web into clean, LLM ready Markdown for RAG, agents, and data pipelines. Fast, controllable, battle tested by a 50k+ star community.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/unclecode/crawl4ai/main/#-recent-updates"&gt;âœ¨ Check out latest update v0.7.4&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;âœ¨ New in v0.7.4: Revolutionary LLM Table Extraction with intelligent chunking, enhanced concurrency fixes, memory management refactor, and critical stability improvements. &lt;a href="https://github.com/unclecode/crawl4ai/raw/main/docs/blog/release-v0.7.4.md"&gt;Release notes â†’&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;âœ¨ Recent v0.7.3: Undetected Browser Support, Multi-URL Configurations, Memory Monitoring, Enhanced Table Extraction, GitHub Sponsors. &lt;a href="https://github.com/unclecode/crawl4ai/raw/main/docs/blog/release-v0.7.3.md"&gt;Release notes â†’&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ¤“ &lt;strong&gt;My Personal Story&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;I grew up on an Amstrad, thanks to my dad, and never stopped building. In grad school I specialized in NLP and built crawlers for research. Thatâ€™s where I learned how much extraction matters.&lt;/p&gt; 
 &lt;p&gt;In 2023, I needed web-to-Markdown. The â€œopen sourceâ€ option wanted an account, API token, and $16, and still under-delivered. I went turbo anger mode, built Crawl4AI in days, and it went viral. Now itâ€™s the most-starred crawler on GitHub.&lt;/p&gt; 
 &lt;p&gt;I made it open source for &lt;strong&gt;availability&lt;/strong&gt;, anyone can use it without a gate. Now Iâ€™m building the platform for &lt;strong&gt;affordability&lt;/strong&gt;, anyone can run serious crawls without breaking the bank. If that resonates, join in, send feedback, or just crawl something amazing.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why developers pick Crawl4AI&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;LLM ready output&lt;/strong&gt;, smart Markdown with headings, tables, code, citation hints&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Fast in practice&lt;/strong&gt;, async browser pool, caching, minimal hops&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Full control&lt;/strong&gt;, sessions, proxies, cookies, user scripts, hooks&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Adaptive intelligence&lt;/strong&gt;, learns site patterns, explores only what matters&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Deploy anywhere&lt;/strong&gt;, zero keys, CLI and Docker, cloud friendly&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Crawl4AI:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install the package
pip install -U crawl4ai

# For pre release versions
pip install crawl4ai --pre

# Run post-installation setup
crawl4ai-setup

# Verify your installation
crawl4ai-doctor
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you encounter any browser-related issues, you can install them manually:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m playwright install --with-deps chromium
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Run a simple web crawl with Python:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from crawl4ai import *

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
        )
        print(result.markdown)

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Or use the new command-line interface:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic crawl with markdown output
crwl https://www.nbcnews.com/business -o markdown

# Deep crawl with BFS strategy, max 10 pages
crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10

# Use LLM extraction with a specific question
crwl https://www.example.com/products -q "Extract all product prices"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ’– Support Crawl4AI&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ‰ &lt;strong&gt;Sponsorship Program Now Open!&lt;/strong&gt; After powering 51K+ developers and 1 year of growth, Crawl4AI is launching dedicated support for &lt;strong&gt;startups&lt;/strong&gt; and &lt;strong&gt;enterprises&lt;/strong&gt;. Be among the first 50 &lt;strong&gt;Founding Sponsors&lt;/strong&gt; for permanent recognition in our Hall of Fame.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Crawl4AI is the #1 trending open-source web crawler on GitHub. Your support keeps it independent, innovative, and free for the community â€” while giving you direct access to premium benefits.&lt;/p&gt; 
&lt;div align=""&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sponsors/unclecode"&gt;&lt;img src="https://img.shields.io/badge/Become%20a%20Sponsor-pink?style=for-the-badge&amp;amp;logo=github-sponsors&amp;amp;logoColor=white" alt="Become a Sponsor" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/sponsors/unclecode"&gt;&lt;img src="https://img.shields.io/github/sponsors/unclecode?style=for-the-badge&amp;amp;logo=github&amp;amp;label=Current%20Sponsors&amp;amp;color=green" alt="Current Sponsors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;ğŸ¤ Sponsorship Tiers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸŒ± Believer ($5/mo)&lt;/strong&gt; â€” Join the movement for data democratization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸš€ Builder ($50/mo)&lt;/strong&gt; â€” Priority support &amp;amp; early access to features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ’¼ Growing Team ($500/mo)&lt;/strong&gt; â€” Bi-weekly syncs &amp;amp; optimization help&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¢ Data Infrastructure Partner ($2000/mo)&lt;/strong&gt; â€” Full partnership with dedicated support&lt;br /&gt; &lt;em&gt;Custom arrangements available - see &lt;a href="https://raw.githubusercontent.com/unclecode/crawl4ai/main/SPONSORS.md"&gt;SPONSORS.md&lt;/a&gt; for details &amp;amp; contact&lt;/em&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Why sponsor?&lt;/strong&gt;&lt;br /&gt; No rate-limited APIs. No lock-in. Build and own your data pipeline with direct guidance from the creator of Crawl4AI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/unclecode"&gt;See All Tiers &amp;amp; Benefits â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“ &lt;strong&gt;Markdown Generation&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ§¹ &lt;strong&gt;Clean Markdown&lt;/strong&gt;: Generates clean, structured Markdown with accurate formatting.&lt;/li&gt; 
  &lt;li&gt;ğŸ¯ &lt;strong&gt;Fit Markdown&lt;/strong&gt;: Heuristic-based filtering to remove noise and irrelevant parts for AI-friendly processing.&lt;/li&gt; 
  &lt;li&gt;ğŸ”— &lt;strong&gt;Citations and References&lt;/strong&gt;: Converts page links into a numbered reference list with clean citations.&lt;/li&gt; 
  &lt;li&gt;ğŸ› ï¸ &lt;strong&gt;Custom Strategies&lt;/strong&gt;: Users can create their own Markdown generation strategies tailored to specific needs.&lt;/li&gt; 
  &lt;li&gt;ğŸ“š &lt;strong&gt;BM25 Algorithm&lt;/strong&gt;: Employs BM25-based filtering for extracting core information and removing irrelevant content.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“Š &lt;strong&gt;Structured Data Extraction&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ¤– &lt;strong&gt;LLM-Driven Extraction&lt;/strong&gt;: Supports all LLMs (open-source and proprietary) for structured data extraction.&lt;/li&gt; 
  &lt;li&gt;ğŸ§± &lt;strong&gt;Chunking Strategies&lt;/strong&gt;: Implements chunking (topic-based, regex, sentence-level) for targeted content processing.&lt;/li&gt; 
  &lt;li&gt;ğŸŒŒ &lt;strong&gt;Cosine Similarity&lt;/strong&gt;: Find relevant content chunks based on user queries for semantic extraction.&lt;/li&gt; 
  &lt;li&gt;ğŸ” &lt;strong&gt;CSS-Based Extraction&lt;/strong&gt;: Fast schema-based data extraction using XPath and CSS selectors.&lt;/li&gt; 
  &lt;li&gt;ğŸ”§ &lt;strong&gt;Schema Definition&lt;/strong&gt;: Define custom schemas for extracting structured JSON from repetitive patterns.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸŒ &lt;strong&gt;Browser Integration&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ–¥ï¸ &lt;strong&gt;Managed Browser&lt;/strong&gt;: Use user-owned browsers with full control, avoiding bot detection.&lt;/li&gt; 
  &lt;li&gt;ğŸ”„ &lt;strong&gt;Remote Browser Control&lt;/strong&gt;: Connect to Chrome Developer Tools Protocol for remote, large-scale data extraction.&lt;/li&gt; 
  &lt;li&gt;ğŸ‘¤ &lt;strong&gt;Browser Profiler&lt;/strong&gt;: Create and manage persistent profiles with saved authentication states, cookies, and settings.&lt;/li&gt; 
  &lt;li&gt;ğŸ”’ &lt;strong&gt;Session Management&lt;/strong&gt;: Preserve browser states and reuse them for multi-step crawling.&lt;/li&gt; 
  &lt;li&gt;ğŸ§© &lt;strong&gt;Proxy Support&lt;/strong&gt;: Seamlessly connect to proxies with authentication for secure access.&lt;/li&gt; 
  &lt;li&gt;âš™ï¸ &lt;strong&gt;Full Browser Control&lt;/strong&gt;: Modify headers, cookies, user agents, and more for tailored crawling setups.&lt;/li&gt; 
  &lt;li&gt;ğŸŒ &lt;strong&gt;Multi-Browser Support&lt;/strong&gt;: Compatible with Chromium, Firefox, and WebKit.&lt;/li&gt; 
  &lt;li&gt;ğŸ“ &lt;strong&gt;Dynamic Viewport Adjustment&lt;/strong&gt;: Automatically adjusts the browser viewport to match page content, ensuring complete rendering and capturing of all elements.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ” &lt;strong&gt;Crawling &amp;amp; Scraping&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ–¼ï¸ &lt;strong&gt;Media Support&lt;/strong&gt;: Extract images, audio, videos, and responsive image formats like &lt;code&gt;srcset&lt;/code&gt; and &lt;code&gt;picture&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;ğŸš€ &lt;strong&gt;Dynamic Crawling&lt;/strong&gt;: Execute JS and wait for async or sync for dynamic content extraction.&lt;/li&gt; 
  &lt;li&gt;ğŸ“¸ &lt;strong&gt;Screenshots&lt;/strong&gt;: Capture page screenshots during crawling for debugging or analysis.&lt;/li&gt; 
  &lt;li&gt;ğŸ“‚ &lt;strong&gt;Raw Data Crawling&lt;/strong&gt;: Directly process raw HTML (&lt;code&gt;raw:&lt;/code&gt;) or local files (&lt;code&gt;file://&lt;/code&gt;).&lt;/li&gt; 
  &lt;li&gt;ğŸ”— &lt;strong&gt;Comprehensive Link Extraction&lt;/strong&gt;: Extracts internal, external links, and embedded iframe content.&lt;/li&gt; 
  &lt;li&gt;ğŸ› ï¸ &lt;strong&gt;Customizable Hooks&lt;/strong&gt;: Define hooks at every step to customize crawling behavior.&lt;/li&gt; 
  &lt;li&gt;ğŸ’¾ &lt;strong&gt;Caching&lt;/strong&gt;: Cache data for improved speed and to avoid redundant fetches.&lt;/li&gt; 
  &lt;li&gt;ğŸ“„ &lt;strong&gt;Metadata Extraction&lt;/strong&gt;: Retrieve structured metadata from web pages.&lt;/li&gt; 
  &lt;li&gt;ğŸ“¡ &lt;strong&gt;IFrame Content Extraction&lt;/strong&gt;: Seamless extraction from embedded iframe content.&lt;/li&gt; 
  &lt;li&gt;ğŸ•µï¸ &lt;strong&gt;Lazy Load Handling&lt;/strong&gt;: Waits for images to fully load, ensuring no content is missed due to lazy loading.&lt;/li&gt; 
  &lt;li&gt;ğŸ”„ &lt;strong&gt;Full-Page Scanning&lt;/strong&gt;: Simulates scrolling to load and capture all dynamic content, perfect for infinite scroll pages.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸš€ &lt;strong&gt;Deployment&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ³ &lt;strong&gt;Dockerized Setup&lt;/strong&gt;: Optimized Docker image with FastAPI server for easy deployment.&lt;/li&gt; 
  &lt;li&gt;ğŸ”‘ &lt;strong&gt;Secure Authentication&lt;/strong&gt;: Built-in JWT token authentication for API security.&lt;/li&gt; 
  &lt;li&gt;ğŸ”„ &lt;strong&gt;API Gateway&lt;/strong&gt;: One-click deployment with secure token authentication for API-based workflows.&lt;/li&gt; 
  &lt;li&gt;ğŸŒ &lt;strong&gt;Scalable Architecture&lt;/strong&gt;: Designed for mass-scale production and optimized server performance.&lt;/li&gt; 
  &lt;li&gt;â˜ï¸ &lt;strong&gt;Cloud Deployment&lt;/strong&gt;: Ready-to-deploy configurations for major cloud platforms.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ¯ &lt;strong&gt;Additional Features&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ•¶ï¸ &lt;strong&gt;Stealth Mode&lt;/strong&gt;: Avoid bot detection by mimicking real users.&lt;/li&gt; 
  &lt;li&gt;ğŸ·ï¸ &lt;strong&gt;Tag-Based Content Extraction&lt;/strong&gt;: Refine crawling based on custom tags, headers, or metadata.&lt;/li&gt; 
  &lt;li&gt;ğŸ”— &lt;strong&gt;Link Analysis&lt;/strong&gt;: Extract and analyze all links for detailed data exploration.&lt;/li&gt; 
  &lt;li&gt;ğŸ›¡ï¸ &lt;strong&gt;Error Handling&lt;/strong&gt;: Robust error management for seamless execution.&lt;/li&gt; 
  &lt;li&gt;ğŸ” &lt;strong&gt;CORS &amp;amp; Static Serving&lt;/strong&gt;: Supports filesystem-based caching and cross-origin requests.&lt;/li&gt; 
  &lt;li&gt;ğŸ“– &lt;strong&gt;Clear Documentation&lt;/strong&gt;: Simplified and updated guides for onboarding and advanced usage.&lt;/li&gt; 
  &lt;li&gt;ğŸ™Œ &lt;strong&gt;Community Recognition&lt;/strong&gt;: Acknowledges contributors and pull requests for transparency.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Try it Now!&lt;/h2&gt; 
&lt;p&gt;âœ¨ Play around with this &lt;a href="https://colab.research.google.com/drive/1SgRPrByQLzjRfwoRNq1wSGE9nYY_EE8C?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;âœ¨ Visit our &lt;a href="https://docs.crawl4ai.com/"&gt;Documentation Website&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation ğŸ› ï¸&lt;/h2&gt; 
&lt;p&gt;Crawl4AI offers flexible installation options to suit various use cases. You can install it as a Python package or use Docker.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ &lt;strong&gt;Using pip&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Choose the installation option that best fits your needs:&lt;/p&gt; 
 &lt;h3&gt;Basic Installation&lt;/h3&gt; 
 &lt;p&gt;For basic web crawling and scraping tasks:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install crawl4ai
crawl4ai-setup # Setup the browser
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;By default, this will install the asynchronous version of Crawl4AI, using Playwright for web crawling.&lt;/p&gt; 
 &lt;p&gt;ğŸ‘‰ &lt;strong&gt;Note&lt;/strong&gt;: When you install Crawl4AI, the &lt;code&gt;crawl4ai-setup&lt;/code&gt; should automatically install and set up Playwright. However, if you encounter any Playwright-related errors, you can manually install it using one of these methods:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Through the command line:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;playwright install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;If the above doesn't work, try this more specific command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m playwright install chromium
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;This second method has proven to be more reliable in some cases.&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;Installation with Synchronous Version&lt;/h3&gt; 
 &lt;p&gt;The sync version is deprecated and will be removed in future versions. If you need the synchronous version using Selenium:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install crawl4ai[sync]
&lt;/code&gt;&lt;/pre&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;Development Installation&lt;/h3&gt; 
 &lt;p&gt;For contributors who plan to modify the source code:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e .                    # Basic installation in editable mode
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Install optional features:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e ".[torch]"           # With PyTorch features
pip install -e ".[transformer]"     # With Transformer features
pip install -e ".[cosine]"          # With cosine similarity features
pip install -e ".[sync]"            # With synchronous crawling (Selenium)
pip install -e ".[all]"             # Install all optional features
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ³ &lt;strong&gt;Docker Deployment&lt;/strong&gt;&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ğŸš€ &lt;strong&gt;Now Available!&lt;/strong&gt; Our completely redesigned Docker implementation is here! This new solution makes deployment more efficient and seamless than ever.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;New Docker Features&lt;/h3&gt; 
 &lt;p&gt;The new Docker implementation includes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Browser pooling&lt;/strong&gt; with page pre-warming for faster response times&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Interactive playground&lt;/strong&gt; to test and generate request code&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;MCP integration&lt;/strong&gt; for direct connection to AI tools like Claude Code&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Comprehensive API endpoints&lt;/strong&gt; including HTML extraction, screenshots, PDF generation, and JavaScript execution&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Multi-architecture support&lt;/strong&gt; with automatic detection (AMD64/ARM64)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Optimized resources&lt;/strong&gt; with improved memory management&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Getting Started&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Pull and run the latest release candidate
docker pull unclecode/crawl4ai:0.7.0
docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.7.0

# Visit the playground at http://localhost:11235/playground
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Quick Test&lt;/h3&gt; 
 &lt;p&gt;Run a quick test (works for both Docker options):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import requests

# Submit a crawl job
response = requests.post(
    "http://localhost:11235/crawl",
    json={"urls": ["https://example.com"], "priority": 10}
)
if response.status_code == 200:
    print("Crawl job submitted successfully.")
    
if "results" in response.json():
    results = response.json()["results"]
    print("Crawl job completed. Results:")
    for result in results:
        print(result)
else:
    task_id = response.json()["task_id"]
    print(f"Crawl job submitted. Task ID:: {task_id}")
    result = requests.get(f"http://localhost:11235/task/{task_id}")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For more examples, see our &lt;a href="https://github.com/unclecode/crawl4ai/raw/main/docs/examples/docker_example.py"&gt;Docker Examples&lt;/a&gt;. For advanced configuration, environment variables, and usage examples, see our &lt;a href="https://docs.crawl4ai.com/basic/docker-deployment/"&gt;Docker Deployment Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”¬ Advanced Usage Examples ğŸ”¬&lt;/h2&gt; 
&lt;p&gt;You can check the project structure in the directory &lt;a href="https://github.com/unclecode/crawl4ai/tree/main/docs/examples"&gt;docs/examples&lt;/a&gt;. Over there, you can find a variety of examples; here, some popular examples are shared.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“ &lt;strong&gt;Heuristic Markdown Generation with Clean and Fit Markdown&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    browser_config = BrowserConfig(
        headless=True,  
        verbose=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.48, threshold_type="fixed", min_word_threshold=0)
        ),
        # markdown_generator=DefaultMarkdownGenerator(
        #     content_filter=BM25ContentFilter(user_query="WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY", bm25_threshold=1.0)
        # ),
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://docs.micronaut.io/4.7.6/guide/",
            config=run_config
        )
        print(len(result.markdown.raw_markdown))
        print(len(result.markdown.fit_markdown))

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ–¥ï¸ &lt;strong&gt;Executing JavaScript &amp;amp; Extract Structured Data without LLMs&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai import JsonCssExtractionStrategy
import json

async def main():
    schema = {
    "name": "KidoCode Courses",
    "baseSelector": "section.charge-methodology .w-tab-content &amp;gt; div",
    "fields": [
        {
            "name": "section_title",
            "selector": "h3.heading-50",
            "type": "text",
        },
        {
            "name": "section_description",
            "selector": ".charge-content",
            "type": "text",
        },
        {
            "name": "course_name",
            "selector": ".text-block-93",
            "type": "text",
        },
        {
            "name": "course_description",
            "selector": ".course-content-text",
            "type": "text",
        },
        {
            "name": "course_icon",
            "selector": ".image-92",
            "type": "attribute",
            "attribute": "src"
        }
    }
}

    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    browser_config = BrowserConfig(
        headless=False,
        verbose=True
    )
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        js_code=["""(async () =&amp;gt; {const tabs = document.querySelectorAll("section.charge-methodology .tabs-menu-3 &amp;gt; div");for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r =&amp;gt; setTimeout(r, 500));}})();"""],
        cache_mode=CacheMode.BYPASS
    )
        
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        result = await crawler.arun(
            url="https://www.kidocode.com/degrees/technology",
            config=run_config
        )

        companies = json.loads(result.extracted_content)
        print(f"Successfully extracted {len(companies)} companies")
        print(json.dumps(companies[0], indent=2))


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“š &lt;strong&gt;Extracting Structured Data with LLMs&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai import LLMExtractionStrategy
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(..., description="Fee for output token for the OpenAI model.")

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        word_count_threshold=1,
        extraction_strategy=LLMExtractionStrategy(
            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2
            # provider="ollama/qwen2", api_token="no-token", 
            llm_config = LLMConfig(provider="openai/gpt-4o", api_token=os.getenv('OPENAI_API_KEY')), 
            schema=OpenAIModelFee.schema(),
            extraction_type="schema",
            instruction="""From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content. One extracted model JSON format should look like this: 
            {"model_name": "GPT-4", "input_fee": "US$10.00 / 1M tokens", "output_fee": "US$30.00 / 1M tokens"}."""
        ),            
        cache_mode=CacheMode.BYPASS,
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url='https://openai.com/api/pricing/',
            config=run_config
        )
        print(result.extracted_content)

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ¤– &lt;strong&gt;Using Your own Browser with Custom User Profile&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os, sys
from pathlib import Path
import asyncio, time
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def test_news_crawl():
    # Create a persistent user data directory
    user_data_dir = os.path.join(Path.home(), ".crawl4ai", "browser_profile")
    os.makedirs(user_data_dir, exist_ok=True)

    browser_config = BrowserConfig(
        verbose=True,
        headless=True,
        user_data_dir=user_data_dir,
        use_persistent_context=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        url = "ADDRESS_OF_A_CHALLENGING_WEBSITE"
        
        result = await crawler.arun(
            url,
            config=run_config,
            magic=True,
        )
        
        print(f"Successfully crawled {url}")
        print(f"Content length: {len(result.markdown)}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;âœ¨ Recent Updates&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Version 0.7.4 Release Highlights - The Intelligent Table Extraction &amp;amp; Performance Update&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸš€ LLMTableExtraction&lt;/strong&gt;: Revolutionary table extraction with intelligent chunking for massive tables:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from crawl4ai import LLMTableExtraction, LLMConfig

# Configure intelligent table extraction
table_strategy = LLMTableExtraction(
    llm_config=LLMConfig(provider="openai/gpt-4.1-mini"),
    enable_chunking=True,           # Handle massive tables
    chunk_token_threshold=5000,     # Smart chunking threshold
    overlap_threshold=100,          # Maintain context between chunks
    extraction_type="structured"    # Get structured data output
)

config = CrawlerRunConfig(table_extraction_strategy=table_strategy)
result = await crawler.arun("https://complex-tables-site.com", config=config)

# Tables are automatically chunked, processed, and merged
for table in result.tables:
    print(f"Extracted table: {len(table['data'])} rows")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;âš¡ Dispatcher Bug Fix&lt;/strong&gt;: Fixed sequential processing bottleneck in arun_many for fast-completing tasks&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ§¹ Memory Management Refactor&lt;/strong&gt;: Consolidated memory utilities into main utils module for cleaner architecture&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”§ Browser Manager Fixes&lt;/strong&gt;: Resolved race conditions in concurrent page creation with thread-safe locking&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”— Advanced URL Processing&lt;/strong&gt;: Better handling of raw:// URLs and base tag link resolution&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ›¡ï¸ Enhanced Proxy Support&lt;/strong&gt;: Flexible proxy configuration supporting both dict and string formats&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/unclecode/crawl4ai/raw/main/docs/blog/release-v0.7.4.md"&gt;Full v0.7.4 Release Notes â†’&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Version 0.7.3 Release Highlights - The Multi-Config Intelligence Update&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ•µï¸ Undetected Browser Support&lt;/strong&gt;: Bypass sophisticated bot detection systems:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from crawl4ai import AsyncWebCrawler, BrowserConfig

browser_config = BrowserConfig(
    browser_type="undetected",  # Use undetected Chrome
    headless=True,              # Can run headless with stealth
    extra_args=[
        "--disable-blink-features=AutomationControlled",
        "--disable-web-security"
    ]
)

async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun("https://protected-site.com")
# Successfully bypass Cloudflare, Akamai, and custom bot detection
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ¨ Multi-URL Configuration&lt;/strong&gt;: Different strategies for different URL patterns in one batch:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from crawl4ai import CrawlerRunConfig, MatchMode

configs = [
    # Documentation sites - aggressive caching
    CrawlerRunConfig(
        url_matcher=["*docs*", "*documentation*"],
        cache_mode="write",
        markdown_generator_options={"include_links": True}
    ),
    
    # News/blog sites - fresh content
    CrawlerRunConfig(
        url_matcher=lambda url: 'blog' in url or 'news' in url,
        cache_mode="bypass"
    ),
    
    # Fallback for everything else
    CrawlerRunConfig()
]

results = await crawler.arun_many(urls, config=configs)
# Each URL gets the perfect configuration automatically
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ§  Memory Monitoring&lt;/strong&gt;: Track and optimize memory usage during crawling:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from crawl4ai.memory_utils import MemoryMonitor

monitor = MemoryMonitor()
monitor.start_monitoring()

results = await crawler.arun_many(large_url_list)

report = monitor.get_report()
print(f"Peak memory: {report['peak_mb']:.1f} MB")
print(f"Efficiency: {report['efficiency']:.1f}%")
# Get optimization recommendations
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“Š Enhanced Table Extraction&lt;/strong&gt;: Direct DataFrame conversion from web tables:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;result = await crawler.arun("https://site-with-tables.com")

# New way - direct table access
if result.tables:
    import pandas as pd
    for table in result.tables:
        df = pd.DataFrame(table['data'])
        print(f"Table: {df.shape[0]} rows Ã— {df.shape[1]} columns")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ’° GitHub Sponsors&lt;/strong&gt;: 4-tier sponsorship system for project sustainability&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ³ Docker LLM Flexibility&lt;/strong&gt;: Configure providers via environment variables&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://github.com/unclecode/crawl4ai/raw/main/docs/blog/release-v0.7.3.md"&gt;Full v0.7.3 Release Notes â†’&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Version 0.7.0 Release Highlights - The Adaptive Intelligence Update&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ§  Adaptive Crawling&lt;/strong&gt;: Your crawler now learns and adapts to website patterns automatically:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;config = AdaptiveConfig(
    confidence_threshold=0.7, # Min confidence to stop crawling
    max_depth=5, # Maximum crawl depth
    max_pages=20, # Maximum number of pages to crawl
    strategy="statistical"
)

async with AsyncWebCrawler() as crawler:
    adaptive_crawler = AdaptiveCrawler(crawler, config)
    state = await adaptive_crawler.digest(
        start_url="https://news.example.com",
        query="latest news content"
    )
# Crawler learns patterns and improves extraction over time
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸŒŠ Virtual Scroll Support&lt;/strong&gt;: Complete content extraction from infinite scroll pages:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;scroll_config = VirtualScrollConfig(
    container_selector="[data-testid='feed']",
    scroll_count=20,
    scroll_by="container_height",
    wait_after_scroll=1.0
)

result = await crawler.arun(url, config=CrawlerRunConfig(
    virtual_scroll_config=scroll_config
))
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ”— Intelligent Link Analysis&lt;/strong&gt;: 3-layer scoring system for smart link prioritization:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;link_config = LinkPreviewConfig(
    query="machine learning tutorials",
    score_threshold=0.3,
    concurrent_requests=10
)

result = await crawler.arun(url, config=CrawlerRunConfig(
    link_preview_config=link_config,
    score_links=True
))
# Links ranked by relevance and quality
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ£ Async URL Seeder&lt;/strong&gt;: Discover thousands of URLs in seconds:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;seeder = AsyncUrlSeeder(SeedingConfig(
    source="sitemap+cc",
    pattern="*/blog/*",
    query="python tutorials",
    score_threshold=0.4
))

urls = await seeder.discover("https://example.com")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;âš¡ Performance Boost&lt;/strong&gt;: Up to 3x faster with optimized resource handling and memory efficiency&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Read the full details in our &lt;a href="https://docs.crawl4ai.com/blog/release-v0.7.0"&gt;0.7.0 Release Notes&lt;/a&gt; or check the &lt;a href="https://github.com/unclecode/crawl4ai/raw/main/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Version Numbering in Crawl4AI&lt;/h2&gt; 
&lt;p&gt;Crawl4AI follows standard Python version numbering conventions (PEP 440) to help users understand the stability and features of each release.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“ˆ &lt;strong&gt;Version Numbers Explained&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Our version numbers follow this pattern: &lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt; (e.g., 0.4.3)&lt;/p&gt; 
 &lt;h4&gt;Pre-release Versions&lt;/h4&gt; 
 &lt;p&gt;We use different suffixes to indicate development stages:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;dev&lt;/code&gt; (0.4.3dev1): Development versions, unstable&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;a&lt;/code&gt; (0.4.3a1): Alpha releases, experimental features&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;b&lt;/code&gt; (0.4.3b1): Beta releases, feature complete but needs testing&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;rc&lt;/code&gt; (0.4.3): Release candidates, potential final version&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;Installation&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Regular installation (stable version):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U crawl4ai
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Install pre-release versions:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install crawl4ai --pre
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Install specific version:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install crawl4ai==0.4.3b1
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;Why Pre-releases?&lt;/h4&gt; 
 &lt;p&gt;We use pre-releases to:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Test new features in real-world scenarios&lt;/li&gt; 
  &lt;li&gt;Gather feedback before final releases&lt;/li&gt; 
  &lt;li&gt;Ensure stability for production users&lt;/li&gt; 
  &lt;li&gt;Allow early adopters to try new features&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For production environments, we recommend using the stable version. For testing new features, you can opt-in to pre-releases using the &lt;code&gt;--pre&lt;/code&gt; flag.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ“– Documentation &amp;amp; Roadmap&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸš¨ &lt;strong&gt;Documentation Update Alert&lt;/strong&gt;: We're undertaking a major documentation overhaul next week to reflect recent updates and improvements. Stay tuned for a more comprehensive and up-to-date guide!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For current documentation, including installation instructions, advanced features, and API reference, visit our &lt;a href="https://docs.crawl4ai.com/"&gt;Documentation Website&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To check our development plans and upcoming features, visit our &lt;a href="https://github.com/unclecode/crawl4ai/raw/main/ROADMAP.md"&gt;Roadmap&lt;/a&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“ˆ &lt;strong&gt;Development TODOs&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 0. Graph Crawler: Smart website traversal using graph search algorithms for comprehensive nested page extraction&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 1. Question-Based Crawler: Natural language driven web discovery and content extraction&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 2. Knowledge-Optimal Crawler: Smart crawling that maximizes knowledge while minimizing data extraction&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 3. Agentic Crawler: Autonomous system for complex multi-step crawling operations&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 4. Automated Schema Generator: Convert natural language to extraction schemas&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 5. Domain-Specific Scrapers: Pre-configured extractors for common platforms (academic, e-commerce)&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 6. Web Embedding Index: Semantic search infrastructure for crawled content&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 7. Interactive Playground: Web UI for testing, comparing strategies with AI assistance&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 8. Performance Monitor: Real-time insights into crawler operations&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 9. Cloud Integration: One-click deployment solutions across cloud providers&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 10. Sponsorship Program: Structured support system with tiered benefits&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 11. Educational Content: "How to Crawl" video series and interactive tutorials&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the open-source community. Check out our &lt;a href="https://github.com/unclecode/crawl4ai/raw/main/CONTRIBUTORS.md"&gt;contribution guidelines&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;I'll help modify the license section with badges. For the halftone effect, here's a version with it:&lt;/p&gt; 
&lt;p&gt;Here's the updated license section:&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License &amp;amp; Attribution&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0, attribution is recommended via the badges below. See the &lt;a href="https://github.com/unclecode/crawl4ai/raw/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h3&gt;Attribution Requirements&lt;/h3&gt; 
&lt;p&gt;When using Crawl4AI, you must include one of the following attribution methods:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“ˆ &lt;strong&gt;1. Badge Attribution (Recommended)&lt;/strong&gt;&lt;/summary&gt; Add one of these badges to your README, documentation, or website: 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Theme&lt;/th&gt; 
    &lt;th&gt;Badge&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Disco Theme (Animated)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/unclecode/crawl4ai"&gt;&lt;img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-disco.svg?sanitize=true" alt="Powered by Crawl4AI" width="200" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Night Theme (Dark with Neon)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/unclecode/crawl4ai"&gt;&lt;img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-night.svg?sanitize=true" alt="Powered by Crawl4AI" width="200" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Dark Theme (Classic)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/unclecode/crawl4ai"&gt;&lt;img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-dark.svg?sanitize=true" alt="Powered by Crawl4AI" width="200" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Light Theme (Classic)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/unclecode/crawl4ai"&gt;&lt;img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-light.svg?sanitize=true" alt="Powered by Crawl4AI" width="200" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;HTML code for adding the badges:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;!-- Disco Theme (Animated) --&amp;gt;
&amp;lt;a href="https://github.com/unclecode/crawl4ai"&amp;gt;
  &amp;lt;img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-disco.svg" alt="Powered by Crawl4AI" width="200"/&amp;gt;
&amp;lt;/a&amp;gt;

&amp;lt;!-- Night Theme (Dark with Neon) --&amp;gt;
&amp;lt;a href="https://github.com/unclecode/crawl4ai"&amp;gt;
  &amp;lt;img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-night.svg" alt="Powered by Crawl4AI" width="200"/&amp;gt;
&amp;lt;/a&amp;gt;

&amp;lt;!-- Dark Theme (Classic) --&amp;gt;
&amp;lt;a href="https://github.com/unclecode/crawl4ai"&amp;gt;
  &amp;lt;img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-dark.svg" alt="Powered by Crawl4AI" width="200"/&amp;gt;
&amp;lt;/a&amp;gt;

&amp;lt;!-- Light Theme (Classic) --&amp;gt;
&amp;lt;a href="https://github.com/unclecode/crawl4ai"&amp;gt;
  &amp;lt;img src="https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-light.svg" alt="Powered by Crawl4AI" width="200"/&amp;gt;
&amp;lt;/a&amp;gt;

&amp;lt;!-- Simple Shield Badge --&amp;gt;
&amp;lt;a href="https://github.com/unclecode/crawl4ai"&amp;gt;
  &amp;lt;img src="https://img.shields.io/badge/Powered%20by-Crawl4AI-blue?style=flat-square" alt="Powered by Crawl4AI"/&amp;gt;
&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“– &lt;strong&gt;2. Text Attribution&lt;/strong&gt;&lt;/summary&gt; Add this line to your documentation: ``` This project uses Crawl4AI (https://github.com/unclecode/crawl4ai) for web data extraction. ``` 
&lt;/details&gt; 
&lt;h2&gt;ğŸ“š Citation&lt;/h2&gt; 
&lt;p&gt;If you use Crawl4AI in your research or project, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{crawl4ai2024,
  author = {UncleCode},
  title = {Crawl4AI: Open-source LLM Friendly Web Crawler &amp;amp; Scraper},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/unclecode/crawl4ai}},
  commit = {Please use the commit hash you're working with}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Text citation format:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;UncleCode. (2024). Crawl4AI: Open-source LLM Friendly Web Crawler &amp;amp; Scraper [Computer software]. 
GitHub. https://github.com/unclecode/crawl4ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ“§ Contact&lt;/h2&gt; 
&lt;p&gt;For questions, suggestions, or feedback, feel free to reach out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GitHub: &lt;a href="https://github.com/unclecode"&gt;unclecode&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Twitter: &lt;a href="https://twitter.com/unclecode"&gt;@unclecode&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Website: &lt;a href="https://crawl4ai.com"&gt;crawl4ai.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Happy Crawling! ğŸ•¸ï¸ğŸš€&lt;/p&gt; 
&lt;h2&gt;ğŸ—¾ Mission&lt;/h2&gt; 
&lt;p&gt;Our mission is to unlock the value of personal and enterprise data by transforming digital footprints into structured, tradeable assets. Crawl4AI empowers individuals and organizations with open-source tools to extract and structure data, fostering a shared data economy.&lt;/p&gt; 
&lt;p&gt;We envision a future where AI is powered by real human knowledge, ensuring data creators directly benefit from their contributions. By democratizing data and enabling ethical sharing, we are laying the foundation for authentic AI advancement.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ”‘ &lt;strong&gt;Key Opportunities&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Data Capitalization&lt;/strong&gt;: Transform digital footprints into measurable, valuable assets.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Authentic AI Data&lt;/strong&gt;: Provide AI systems with real human insights.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Shared Economy&lt;/strong&gt;: Create a fair data marketplace that benefits data creators.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸš€ &lt;strong&gt;Development Pathway&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Open-Source Tools&lt;/strong&gt;: Community-driven platforms for transparent data extraction.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Digital Asset Structuring&lt;/strong&gt;: Tools to organize and value digital knowledge.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Ethical Data Marketplace&lt;/strong&gt;: A secure, fair platform for exchanging structured data.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;For more details, see our &lt;a href="https://raw.githubusercontent.com/unclecode/crawl4ai/main/MISSION.md"&gt;full mission statement&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#unclecode/crawl4ai&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=unclecode/crawl4ai&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation&lt;/li&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investor, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett's partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Michael Burry Agent - The Big Short contrarian who hunts for deep value&lt;/li&gt; 
 &lt;li&gt;Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk&lt;/li&gt; 
 &lt;li&gt;Peter Lynch Agent - Practical investor who seeks "ten-baggers" in everyday businesses&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Meticulous growth investor who uses deep "scuttlebutt" research&lt;/li&gt; 
 &lt;li&gt;Rakesh Jhunjhunwala Agent - The Big Bull of India&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width="1042" alt="Screenshot 2025-03-22 at 6 19 07 PM" src="https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4" /&gt; 
&lt;p&gt;Note: the system does not actually make any trades.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/virattt"&gt;&lt;img src="https://img.shields.io/twitter/follow/virattt?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No investment advice or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-install"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-run"&gt;How to Run&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-command-line-interface"&gt;âŒ¨ï¸ Command Line Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#%EF%B8%8F-web-application"&gt;ğŸ–¥ï¸ Web Application&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#how-to-contribute"&gt;How to Contribute&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;Before you can run the AI Hedge Fund, you'll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.&lt;/p&gt; 
&lt;h3&gt;1. Clone the Repository&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set up API keys&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file for your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create .env file for your API keys (in the root directory)
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open and edit the &lt;code&gt;.env&lt;/code&gt; file to add your API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set at least one LLM API key (e.g. &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt;) for the hedge fund to work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Financial Data&lt;/strong&gt;: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;How to Run&lt;/h2&gt; 
&lt;h3&gt;âŒ¨ï¸ Command Line Interface&lt;/h3&gt; 
&lt;p&gt;You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.&lt;/p&gt; 
&lt;img width="992" alt="Screenshot 2025-01-06 at 5 50 17 PM" src="https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b" /&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the AI Hedge Fund&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--ollama&lt;/code&gt; flag to run the AI hedge fund using local LLMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run the Backtester&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width="941" alt="Screenshot 2025-01-06 at 5 47 52 PM" src="https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47" /&gt;&lt;/p&gt; 
&lt;p&gt;Note: The &lt;code&gt;--ollama&lt;/code&gt;, &lt;code&gt;--start-date&lt;/code&gt;, and &lt;code&gt;--end-date&lt;/code&gt; flags work for the backtester, as well!&lt;/p&gt; 
&lt;h3&gt;ğŸ–¥ï¸ Web Application&lt;/h3&gt; 
&lt;p&gt;The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.&lt;/p&gt; 
&lt;p&gt;Please see detailed instructions on how to install and run the web application &lt;a href="https://github.com/virattt/ai-hedge-fund/tree/main/app"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;img width="1721" alt="Screenshot 2025-06-28 at 6 41 03â€¯PM" src="https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b" /&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href="https://github.com/virattt/ai-hedge-fund/issues"&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Vector-Wangel/XLeRobot</title>
      <link>https://github.com/Vector-Wangel/XLeRobot</link>
      <description>&lt;p&gt;XLeRobot: Practical Dual-Arm Mobile Home Robot for $660&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href="https://xlerobot.readthedocs.io/en/latest/index.html"&gt;XLeRobot ğŸ¤–&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/lang-en-blue.svg?sanitize=true" alt="en" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/README_CN.md"&gt;&lt;img src="https://img.shields.io/badge/lang-%E4%B8%AD%E6%96%87-brown.svg?sanitize=true" alt="ä¸­æ–‡" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://xlerobot.readthedocs.io/en/latest/index.html"&gt; &lt;img width="1725" height="1140" alt="front" src="https://github.com/user-attachments/assets/f9c454ee-2c46-42b4-a5d7-88834a1c95ab" /&gt; &lt;/a&gt; 
&lt;h2&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="Apache License" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/VectorWang2"&gt;&lt;img src="https://img.shields.io/twitter/follow/VectorWang?style=social" alt="Twitter/X" /&gt;&lt;/a&gt; &lt;a href="https://xlerobot.readthedocs.io/en/latest/"&gt;&lt;img src="https://img.shields.io/badge/docs-passing-brightgreen.svg?sanitize=true" alt="Docs status" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/bjZveEUh6F"&gt;&lt;img src="https://img.shields.io/badge/Discord-XLeRobot-7289da?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ğŸš€ Bringing Embodied AI to Everyone - Cheaper Than an iPhone! ğŸ“±&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;ğŸ’µ Starts from $660 cost and â° &amp;lt;4hrs total assembly time!!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Built upon the giants: &lt;a href="https://github.com/huggingface/lerobot"&gt;LeRobot&lt;/a&gt;, &lt;a href="https://github.com/TheRobotStudio/SO-ARM100"&gt;SO-100/SO-101&lt;/a&gt;, &lt;a href="https://github.com/SIGRobotics-UIUC/LeKiwi"&gt;Lekiwi&lt;/a&gt;, &lt;a href="https://github.com/timqian/bambot"&gt;Bambot&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/17e31979-bd5e-4790-be70-566ea8bb181e" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/96ff4a3e-3402-47a2-bc6b-b45137ee3fdd" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/f6d52acc-bc8d-46f6-b3cd-8821f0306a7f" width="250" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/59086300-3e6f-4a3c-b5e0-db893eeabc0c" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/4ddbc0ff-ca42-4ad0-94c6-4e0f4047fd01" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/7abc890e-9c9c-4983-8b25-122573028de5" width="250" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/e74a602b-0146-49c4-953d-3fa3b038a7f7" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/d8090b15-97f3-4abc-98c8-208ae79894d5" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/8b54adc3-d61b-42a0-8985-ea28f2e8f64c" width="250" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸ“° News&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025-09-09: &lt;strong&gt;Developer Assembly kit (excluding battery and IKEA cart) ready for purchase&lt;/strong&gt; in &lt;a href="https://e.tb.cn/h.SZFbBgZABZ8zRPe?tk=ba514rTBRjQ"&gt;China (Taobao) for &lt;strong&gt;3699ï¿¥&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://shop.wowrobo.com/products/xlerobot-dual-arm-mobile-household-robot-kit?variant=47297659961561"&gt;world-wide for &lt;strong&gt;579$&lt;/strong&gt;&lt;/a&gt;. &lt;em&gt;(In collaboration with &lt;strong&gt;Wowrobo&lt;/strong&gt;, one of the official collaborators with Huggingface SO101 arm, they have sold 5k+ SO101 arm worldwide with great customer feedback.)&lt;/em&gt; &lt;img width="1482" height="485" alt="image" src="https://github.com/user-attachments/assets/788836c1-966a-4d11-a911-5c37befc0b85" /&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Non-profit, I personally don't earn any from this. I also asked Wowrobo to set the price as low as possible.&lt;/li&gt; 
   &lt;li&gt;This is only the assembly kit for developers, please check documentation website and this repo for available codes and tutorials before you purchase.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-09-09: Joined &lt;a href="https://www.seeedstudio.com/embodied-ai-worldwide-hackathon-home-robot.html"&gt;Embodied AI Home Robot Hackathon&lt;/a&gt; (Oct 25â€“26, Bay Area) held by &lt;strong&gt;SEEED x Nvidia x Huggingface&lt;/strong&gt; as mentor! &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSdYYDegdgIypxuGJNLcoc8kbdmU4jKgl49zg4X-107LAmBN4g/viewform"&gt;Register HERE&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;img width="2400" height="1256" alt="image" src="https://github.com/user-attachments/assets/4132c23b-5c86-4bb9-94b4-a6b12059685b" /&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-08-30: XLeRobot 0.3.0 Release with final outfit touch up and household chores showcase demos.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-07-30: &lt;a href="https://xlerobot.readthedocs.io/en/latest/software/index.html"&gt;Control XLeRobot in real life&lt;/a&gt; with &lt;strong&gt;keyboard/Xbox controller/Switch joycon&lt;/strong&gt; in the wild anywhere. All bluetooth, no wifi needed and zero latency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/de8f50ad-a370-406c-97fb-fc01638d5624" alt="rea" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-07-08: &lt;a href="https://xlerobot.readthedocs.io/en/latest/simulation/index.html"&gt;&lt;strong&gt;Simulation&lt;/strong&gt;&lt;/a&gt; with updated urdfs, control scripts (support Quest3 VR, keyboard, Xbox controller, switch joycon), support for new hardware and cameras, RL environment. Get started in 15 min.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/68b77bea-fdcf-4f42-9cf0-efcf1b188358" alt="vr" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-07-01: &lt;a href="https://xlerobot.readthedocs.io/en/latest/index.html"&gt;&lt;strong&gt;Documentation&lt;/strong&gt; website&lt;/a&gt; out for more orgainized tutorials, demos and resources.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-06-13: &lt;a href="https://xlerobot.readthedocs.io"&gt;&lt;strong&gt;XLeRobot 0.2.0&lt;/strong&gt;&lt;/a&gt; hardware setup, the 1st version fully capable for autonomous household tasks, starts from 660$.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ’µ Total Cost ğŸ’µ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Cost excludes 3D printing, tools, shipping, and taxes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Price (Buy all the parts yourself)&lt;/th&gt; 
   &lt;th&gt;US&lt;/th&gt; 
   &lt;th&gt;EU&lt;/th&gt; 
   &lt;th&gt;CN&lt;/th&gt; 
   &lt;th&gt;IN&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Basic&lt;/strong&gt; (use your laptop, single RGB head cam)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~$660&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~â‚¬680&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~Â¥3999&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~â‚¹87000&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;â†‘ Stereo dual-eye RGB head cam&lt;/td&gt; 
   &lt;td&gt;+$30&lt;/td&gt; 
   &lt;td&gt;+â‚¬30&lt;/td&gt; 
   &lt;td&gt;+Â¥199&lt;/td&gt; 
   &lt;td&gt;+â‚¹6550&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;+ RasberryPi&lt;/td&gt; 
   &lt;td&gt;+$79&lt;/td&gt; 
   &lt;td&gt;+â‚¬79&lt;/td&gt; 
   &lt;td&gt;+Â¥399&lt;/td&gt; 
   &lt;td&gt;+â‚¹7999&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;â†‘ RealSense RGBD head cam&lt;/td&gt; 
   &lt;td&gt;+$220&lt;/td&gt; 
   &lt;td&gt;+â‚¬230&lt;/td&gt; 
   &lt;td&gt;+Â¥1499&lt;/td&gt; 
   &lt;td&gt;+â‚¹35726&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Get Started ğŸš€&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are totally new to programming, please spend at least a day to get yourself familiar with basic Python, Ubuntu and Github (with the help of Google and AI). At least you should know how to setup ubuntu system, git clone, pip install, use intepreters (VS Code, Cursor, Pycharm, etc.) and directly run commands in the terminals.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;ğŸ’µ &lt;strong&gt;Buy your parts&lt;/strong&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/material.html"&gt;Bill of Materials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ–¨ï¸ &lt;strong&gt;Print your stuff&lt;/strong&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/3d.html"&gt;3D printing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ”¨ &lt;del&gt;Avengers&lt;/del&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/assemble.html"&gt;&lt;strong&gt;Assemble&lt;/strong&gt;!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ’» &lt;strong&gt;Software&lt;/strong&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/software/index.html"&gt;Get your robot moving!&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ‘‹ Want to contribute to XLeRobot?&lt;/strong&gt; Please refer to &lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidance on how to get involved!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Main Contributors&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://vector-wangel.github.io/"&gt;Gaotian/Vector Wang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lzhuoyi.github.io/Zhuoyi_Lu.github.io/"&gt;Zhuoyi Lu&lt;/a&gt;: RL sim2real deploy, teleop on real robot (Xbox, VR, Joycon)&lt;/li&gt; 
 &lt;li&gt;Yiyang Huang: RL &amp;amp; VLA implementation (ongoing)&lt;/li&gt; 
 &lt;li&gt;YCP: WebUI for remote control (ongoing)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/lixingzhang.com"&gt;Lixing Zhang&lt;/a&gt;: Hardware design improvements&lt;/li&gt; 
 &lt;li&gt;Nicole Yue: Documentation website setup&lt;/li&gt; 
 &lt;li&gt;Yuesong Wang: Mujoco simulation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This is just a small brick in the pyramid, made possible by&amp;nbsp;&lt;a href="https://github.com/huggingface/lerobot"&gt;LeRobot&lt;/a&gt;,&amp;nbsp;&lt;a href="https://github.com/TheRobotStudio/SO-ARM100"&gt;SO-100&lt;/a&gt;,&amp;nbsp;&lt;a href="https://github.com/SIGRobotics-UIUC/LeKiwi"&gt;Lekiwi&lt;/a&gt;, and&amp;nbsp;&lt;a href="https://github.com/timqian/bambot"&gt;Bambot&lt;/a&gt;. Thanks to all the talented contributors behind these detailed and professional projects.&lt;/p&gt; 
&lt;p&gt;Looking forward to collaborating with anyone interested in contributing to this project!&lt;/p&gt; 
&lt;h2&gt;About me&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://vector-wangel.github.io/"&gt;Gaotian/Vector Wang&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;I am a CS graduate student at Rice University &lt;a href="https://robotpilab.github.io/"&gt;RobotPi Lab&lt;/a&gt;, focusing on robust object manipulation, where we propse virtual cages and funnels and physics-aware world models to close the Sim2real gap and achieve robust manipulation under uncertainties. One of my papers, Caging in Time, has recently been accepted by International Journal of Robotics Research (IJRR).&lt;/p&gt; 
&lt;p&gt;I built XLeRobot as a personal hobby to instantiate my research theory, also to provide a low-cost platform for people who are interested in robotics and embodied AI to work with.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://star-history.com/#Vector-Wangel/XLeRobot&amp;amp;Timeline"&gt;&lt;img src="https://api.star-history.com/svg?repos=Vector-Wangel/XLeRobot&amp;amp;type=Timeline" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2025xlerobot,
    author = {Wang, Gaotian and Lu, Zhuoyi},
    title = {XLeRobot: A Practical Low-cost Household Dual-Arm Mobile Robot Design for General Manipulation},
    howpublished = "\url{https://github.com/Vector-Wangel/XLeRobot}",
    year = {2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;---&lt;img src="https://github.com/user-attachments/assets/682ef049-bb42-4b50-bf98-74d6311e774d" alt="Generated Image August 27, 2025 - 4_58PM" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸª§ Disclaimer ğŸª§&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you build, buy, or develop a XLeRobot based on this repo, you will be fully responsible for all the physical and mental damages it does to you or others.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Cinnamon/kotaemon</title>
      <link>https://github.com/Cinnamon/kotaemon</link>
      <description>&lt;p&gt;An open-source RAG-based tool for chatting with your documents.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;kotaemon&lt;/h1&gt; 
 &lt;p&gt;An open-source clean &amp;amp; customizable RAG UI for chatting with your documents. Built with both end users and developers in mind.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png" alt="Preview" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11607" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11607" alt="Cinnamon%2Fkotaemon | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/spaces/cin-model/kotaemon"&gt;Live Demo #1&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/cin-model/kotaemon-demo"&gt;Live Demo #2&lt;/a&gt; | &lt;a href="https://cinnamon.github.io/kotaemon/online_install/"&gt;Online Install&lt;/a&gt; | &lt;a href="https://colab.research.google.com/drive/1eTfieec_UOowNizTJA1NjawBJH9y_1nn"&gt;Colab Notebook (Local RAG)&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://cinnamon.github.io/kotaemon/"&gt;User Guide&lt;/a&gt; | &lt;a href="https://cinnamon.github.io/kotaemon/development/"&gt;Developer Guide&lt;/a&gt; | &lt;a href="https://github.com/Cinnamon/kotaemon/issues"&gt;Feedback&lt;/a&gt; | &lt;a href="mailto:kotaemon.support@cinnamon.is"&gt;Contact&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/downloads/release/python-31013/"&gt;&lt;img src="https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true" alt="Python 3.10+" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen" alt="docker pull ghcr.io/cinnamon/kotaemon:latest" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/downloads/Cinnamon/kotaemon/total.svg?label=downloads&amp;amp;color=blue" alt="download" /&gt; &lt;a href="https://huggingface.co/spaces/cin-model/kotaemon-demo"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/en/repository/d3141471a0244d5798bc654982b263eb" target="_blank"&gt;&lt;img src="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d3141471a0244d5798bc654982b263eb&amp;amp;claim_uid=RLiD9UZ1rEHNaMf&amp;amp;theme=small" alt="Featuredï½œHelloGitHub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- start-intro --&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;This project serves as a functional RAG UI for both end users who want to do QA on their documents and developers who want to build their own RAG pipeline. &lt;br /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yml"&gt;+----------------------------------------------------------------------------+
| End users: Those who use apps built with `kotaemon`.                       |
| (You use an app like the one in the demo above)                            |
|     +----------------------------------------------------------------+     |
|     | Developers: Those who built with `kotaemon`.                   |     |
|     | (You have `import kotaemon` somewhere in your project)         |     |
|     |     +----------------------------------------------------+     |     |
|     |     | Contributors: Those who make `kotaemon` better.    |     |     |
|     |     | (You make PR to this repo)                         |     |     |
|     |     +----------------------------------------------------+     |     |
|     +----------------------------------------------------------------+     |
+----------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For end users&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Clean &amp;amp; Minimalistic UI&lt;/strong&gt;: A user-friendly interface for RAG-based QA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support for Various LLMs&lt;/strong&gt;: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via &lt;code&gt;ollama&lt;/code&gt; and &lt;code&gt;llama-cpp-python&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Installation&lt;/strong&gt;: Simple scripts to get you started quickly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For developers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework for RAG Pipelines&lt;/strong&gt;: Tools to build your own RAG-based document QA pipeline.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable UI&lt;/strong&gt;: See your RAG pipeline in action with the provided UI, built with &lt;a href="https://github.com/gradio-app/gradio"&gt;Gradio &lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gradio Theme&lt;/strong&gt;: If you use Gradio for development, check out our theme here: &lt;a href="https://github.com/lone17/kotaemon-gradio-theme"&gt;kotaemon-gradio-theme&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Host your own document QA (RAG) web-UI&lt;/strong&gt;: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Organize your LLM &amp;amp; Embedding models&lt;/strong&gt;: Support both local LLMs &amp;amp; popular API providers (OpenAI, Azure, Ollama, Groq).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid RAG pipeline&lt;/strong&gt;: Sane default RAG pipeline with hybrid (full-text &amp;amp; vector) retriever and re-ranking to ensure best retrieval quality.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-modal QA support&lt;/strong&gt;: Perform Question Answering on multiple documents with figures and tables support. Support multi-modal document parsing (selectable options on UI).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced citations with document preview&lt;/strong&gt;: By default the system will provide detailed citations to ensure the correctness of LLM answers. View your citations (incl. relevant score) directly in the &lt;em&gt;in-browser PDF viewer&lt;/em&gt; with highlights. Warning when retrieval pipeline return low relevant articles.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support complex reasoning methods&lt;/strong&gt;: Use question decomposition to answer your complex/multi-hop question. Support agent-based reasoning with &lt;code&gt;ReAct&lt;/code&gt;, &lt;code&gt;ReWOO&lt;/code&gt; and other agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configurable settings UI&lt;/strong&gt;: You can adjust most important aspects of retrieval &amp;amp; generation process on the UI (incl. prompts).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: Being built on Gradio, you are free to customize or add any UI elements as you like. Also, we aim to support multiple strategies for document indexing &amp;amp; retrieval. &lt;code&gt;GraphRAG&lt;/code&gt; indexing pipeline is provided as an example.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview.png" alt="Preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;If you are not a developer and just want to use the app, please check out our easy-to-follow &lt;a href="https://cinnamon.github.io/kotaemon/"&gt;User Guide&lt;/a&gt;. Download the &lt;code&gt;.zip&lt;/code&gt; file from the &lt;a href="https://github.com/Cinnamon/kotaemon/releases/latest"&gt;latest release&lt;/a&gt; to get all the newest features and bug fixes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;System requirements&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python&lt;/a&gt; &amp;gt;= 3.10&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;: optional, if you &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/#with-docker-recommended"&gt;install with Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.unstructured.io/open-source/installation/full-installation#full-installation"&gt;Unstructured&lt;/a&gt; if you want to process files other than &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.mhtml&lt;/code&gt;, and &lt;code&gt;.xlsx&lt;/code&gt; documents. Installation steps differ depending on your operating system. Please visit the link and follow the specific instructions provided there.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;With Docker (recommended)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;We support both &lt;code&gt;lite&lt;/code&gt; &amp;amp; &lt;code&gt;full&lt;/code&gt; version of Docker images. With &lt;code&gt;full&lt;/code&gt; version, the extra packages of &lt;code&gt;unstructured&lt;/code&gt; will be installed, which can support additional file types (&lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, ...) but the cost is larger docker image size. For most users, the &lt;code&gt;lite&lt;/code&gt; image should work well in most cases.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;To use the &lt;code&gt;full&lt;/code&gt; version.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
-e GRADIO_SERVER_NAME=0.0.0.0 \
-e GRADIO_SERVER_PORT=7860 \
-v ./ktem_app_data:/app/ktem_app_data \
-p 7860:7860 -it --rm \
ghcr.io/cinnamon/kotaemon:main-full
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;To use the &lt;code&gt;full&lt;/code&gt; version with bundled &lt;strong&gt;Ollama&lt;/strong&gt; for &lt;em&gt;local / private RAG&lt;/em&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# change image name to
docker run &amp;lt;...&amp;gt; ghcr.io/cinnamon/kotaemon:main-ollama
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;To use the &lt;code&gt;lite&lt;/code&gt; version.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-bash"&gt; # change image name to
 docker run &amp;lt;...&amp;gt; ghcr.io/cinnamon/kotaemon:main-lite
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We currently support and test two platforms: &lt;code&gt;linux/amd64&lt;/code&gt; and &lt;code&gt;linux/arm64&lt;/code&gt; (for newer Mac). You can specify the platform by passing &lt;code&gt;--platform&lt;/code&gt; in the &lt;code&gt;docker run&lt;/code&gt; command. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# To run docker with platform linux/arm64
docker run \
-e GRADIO_SERVER_NAME=0.0.0.0 \
-e GRADIO_SERVER_PORT=7860 \
-v ./ktem_app_data:/app/ktem_app_data \
-p 7860:7860 -it --rm \
--platform linux/arm64 \
ghcr.io/cinnamon/kotaemon:main-lite
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Once everything is set up correctly, you can go to &lt;code&gt;http://localhost:7860/&lt;/code&gt; to access the WebUI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We use &lt;a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry"&gt;GHCR&lt;/a&gt; to store docker images, all images can be found &lt;a href="https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Without Docker&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone and install required packages on a fresh python environment.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;# optional (setup env)
conda create -n kotaemon python=3.10
conda activate kotaemon

# clone this repo
git clone https://github.com/Cinnamon/kotaemon
cd kotaemon

pip install -e "libs/kotaemon[all]"
pip install -e "libs/ktem"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root of this project. Use &lt;code&gt;.env.example&lt;/code&gt; as a template&lt;/p&gt; &lt;p&gt;The &lt;code&gt;.env&lt;/code&gt; file is there to serve use cases where users want to pre-config the models before starting up the app (e.g. deploy the app on HF hub). The file will only be used to populate the db once upon the first run, it will no longer be used in consequent runs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Optional) To enable in-browser &lt;code&gt;PDF_JS&lt;/code&gt; viewer, download &lt;a href="https://github.com/mozilla/pdf.js/releases/download/v4.0.379/pdfjs-4.0.379-dist.zip"&gt;PDF_JS_DIST&lt;/a&gt; then extract it to &lt;code&gt;libs/ktem/ktem/assets/prebuilt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/pdf-viewer-setup.png" alt="pdf-setup" width="300" /&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt; &lt;p&gt;Start the web server:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;python app.py
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The app will be automatically launched in your browser.&lt;/li&gt; 
   &lt;li&gt;Default username and password are both &lt;code&gt;admin&lt;/code&gt;. You can set up additional users directly through the UI.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/chat-tab.png" alt="Chat tab" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Check the &lt;code&gt;Resources&lt;/code&gt; tab and &lt;code&gt;LLMs and Embeddings&lt;/code&gt; and ensure that your &lt;code&gt;api_key&lt;/code&gt; value is set correctly from your &lt;code&gt;.env&lt;/code&gt; file. If it is not set, you can set it there.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Setup GraphRAG&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Official MS GraphRAG indexing only works with OpenAI or Ollama API. We recommend most users to use NanoGraphRAG implementation for straightforward integration with Kotaemon.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Setup Nano GRAPHRAG&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Install nano-GraphRAG: &lt;code&gt;pip install nano-graphrag&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; install might introduce version conflicts, see &lt;a href="https://github.com/Cinnamon/kotaemon/issues/440"&gt;this issue&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;To quickly fix: &lt;code&gt;pip uninstall hnswlib chroma-hnswlib &amp;amp;&amp;amp; pip install chroma-hnswlib&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Launch Kotaemon with &lt;code&gt;USE_NANO_GRAPHRAG=true&lt;/code&gt; environment variable.&lt;/li&gt; 
  &lt;li&gt;Set your default LLM &amp;amp; Embedding models in Resources setting and it will be recognized automatically from NanoGraphRAG.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Setup LIGHTRAG&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Install LightRAG: &lt;code&gt;pip install git+https://github.com/HKUDS/LightRAG.git&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;LightRAG&lt;/code&gt; install might introduce version conflicts, see &lt;a href="https://github.com/Cinnamon/kotaemon/issues/440"&gt;this issue&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;To quickly fix: &lt;code&gt;pip uninstall hnswlib chroma-hnswlib &amp;amp;&amp;amp; pip install chroma-hnswlib&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Launch Kotaemon with &lt;code&gt;USE_LIGHTRAG=true&lt;/code&gt; environment variable.&lt;/li&gt; 
  &lt;li&gt;Set your default LLM &amp;amp; Embedding models in Resources setting and it will be recognized automatically from LightRAG.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Setup MS GRAPHRAG&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Non-Docker Installation&lt;/strong&gt;: If you are not using Docker, install GraphRAG with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip install "graphrag&amp;lt;=0.3.6" future
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Setting Up API KEY&lt;/strong&gt;: To use the GraphRAG retriever feature, ensure you set the &lt;code&gt;GRAPHRAG_API_KEY&lt;/code&gt; environment variable. You can do this directly in your environment or by adding it to a &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using Local Models and Custom Settings&lt;/strong&gt;: If you want to use GraphRAG with local models (like &lt;code&gt;Ollama&lt;/code&gt;) or customize the default LLM and other configurations, set the &lt;code&gt;USE_CUSTOMIZED_GRAPHRAG_SETTING&lt;/code&gt; environment variable to true. Then, adjust your settings in the &lt;code&gt;settings.yaml.example&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Setup Local Models (for local/private RAG)&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/local_model.md"&gt;Local model setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Setup multimodal document parsing (OCR, table parsing, figure extraction)&lt;/h3&gt; 
&lt;p&gt;These options are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence"&gt;Azure Document Intelligence (API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/"&gt;Adobe PDF Extract (API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DS4SD/docling"&gt;Docling (local, open-source)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;To use Docling, first install required dependencies: &lt;code&gt;pip install docling&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Select corresponding loaders in &lt;code&gt;Settings -&amp;gt; Retrieval Settings -&amp;gt; File loader&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Customize your application&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;By default, all application data is stored in the &lt;code&gt;./ktem_app_data&lt;/code&gt; folder. You can back up or copy this folder to transfer your installation to a new machine.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For advanced users or specific use cases, you can customize these files:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;flowsettings.py&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;.env&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;code&gt;flowsettings.py&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;This file contains the configuration of your application. You can use the example &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/flowsettings.py"&gt;here&lt;/a&gt; as the starting point.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Notable settings&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# setup your preferred document store (with full-text search capabilities)
KH_DOCSTORE=(Elasticsearch | LanceDB | SimpleFileDocumentStore)

# setup your preferred vectorstore (for vector-based search)
KH_VECTORSTORE=(ChromaDB | LanceDB | InMemory | Milvus | Qdrant)

# Enable / disable multimodal QA
KH_REASONINGS_USE_MULTIMODAL=True

# Setup your new reasoning pipeline or modify existing one.
KH_REASONINGS = [
    "ktem.reasoning.simple.FullQAPipeline",
    "ktem.reasoning.simple.FullDecomposeQAPipeline",
    "ktem.reasoning.react.ReactAgentPipeline",
    "ktem.reasoning.rewoo.RewooAgentPipeline",
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;&lt;code&gt;.env&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;This file provides another way to configure your models and credentials.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Configure model via the .env file&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Alternatively, you can configure the models via the &lt;code&gt;.env&lt;/code&gt; file with the information needed to connect to the LLMs. This file is located in the folder of the application. If you don't see it, you can create one.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Currently, the following providers are supported:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the &lt;code&gt;.env&lt;/code&gt; file, set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; variable with your OpenAI API key in order to enable access to OpenAI's models. There are other variables that can be modified, please feel free to edit them to fit your case. Otherwise, the default parameter should work for most people.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_API_KEY=&amp;lt;your OpenAI API key here&amp;gt;
OPENAI_CHAT_MODEL=gpt-3.5-turbo
OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Azure OpenAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For OpenAI models via Azure platform, you need to provide your Azure endpoint and API key. Your might also need to provide your developments' name for the chat model and the embedding model depending on how you set up Azure development.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo
AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Local Models&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt; &lt;p&gt;Using &lt;code&gt;ollama&lt;/code&gt; OpenAI compatible server:&lt;/p&gt; 
       &lt;ul&gt; 
        &lt;li&gt; &lt;p&gt;Install &lt;a href="https://github.com/ollama/ollama"&gt;ollama&lt;/a&gt; and start the application.&lt;/p&gt; &lt;/li&gt; 
        &lt;li&gt; &lt;p&gt;Pull your model, for example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;ollama pull llama3.1:8b
ollama pull nomic-embed-text
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
        &lt;li&gt; &lt;p&gt;Set the model names on web UI and make it as default:&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/models.png" alt="Models" /&gt;&lt;/p&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;Using &lt;code&gt;GGUF&lt;/code&gt; with &lt;code&gt;llama-cpp-python&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can search and download a LLM to be ran locally from the &lt;a href="https://huggingface.co/models"&gt;Hugging Face Hub&lt;/a&gt;. Currently, these model formats are supported:&lt;/p&gt; 
       &lt;ul&gt; 
        &lt;li&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;You should choose a model whose size is less than your device's memory and should leave about 2 GB. For example, if you have 16 GB of RAM in total, of which 12 GB is available, then you should choose a model that takes up at most 10 GB of RAM. Bigger models tend to give better generation but also take more processing time.&lt;/p&gt; &lt;p&gt;Here are some recommendations and their size in memory:&lt;/p&gt; &lt;/li&gt; 
        &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf?download=true"&gt;Qwen1.5-1.8B-Chat-GGUF&lt;/a&gt;: around 2 GB&lt;/p&gt; &lt;p&gt;Add a new LlamaCpp model with the provided model name on the web UI.&lt;/p&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt;
 &lt;/ul&gt;
&lt;/details&gt;   
&lt;h3&gt;Adding your own RAG pipeline&lt;/h3&gt; 
&lt;h4&gt;Custom Reasoning Pipeline&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check the default pipeline implementation in &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/libs/ktem/ktem/reasoning/simple.py"&gt;here&lt;/a&gt;. You can make quick adjustment to how the default QA pipeline work.&lt;/li&gt; 
 &lt;li&gt;Add new &lt;code&gt;.py&lt;/code&gt; implementation in &lt;code&gt;libs/ktem/ktem/reasoning/&lt;/code&gt; and later include it in &lt;code&gt;flowssettings&lt;/code&gt; to enable it on the UI.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Custom Indexing Pipeline&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check sample implementation in &lt;code&gt;libs/ktem/ktem/index/file/graph&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;(more instruction WIP).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!-- end-intro --&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please cite this project as&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{kotaemon2024,
    title = {Kotaemon - An open-source RAG-based tool for chatting with any content.},
    author = {The Kotaemon Team},
    year = {2024},
    howpublished = {\url{https://github.com/Cinnamon/kotaemon}},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#Cinnamon/kotaemon&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Since our project is actively being developed, we greatly value your feedback and contributions. Please see our &lt;a href="https://github.com/Cinnamon/kotaemon/raw/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; to get started. Thank you to all our contributors!&lt;/p&gt; 
&lt;a href="https://github.com/Cinnamon/kotaemon/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=Cinnamon/kotaemon" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>mxrch/GHunt</title>
      <link>https://github.com/mxrch/GHunt</link>
      <description>&lt;p&gt;ğŸ•µï¸â€â™‚ï¸ Offensive Google framework.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/mxrch/GHunt/master/assets/long_banner.png" alt="" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h4&gt;ğŸŒ GHunt Online version : &lt;a href="https://osint.industries"&gt;https://osint.industries&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;ğŸ Now Python 3.13 compatible !&lt;/h4&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/Python-3.10%2B-brightgreen" alt="Python minimum version" /&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ˜Š Description&lt;/h1&gt; 
&lt;p&gt;GHunt (v2) is an offensive Google framework, designed to evolve efficiently.&lt;br /&gt; It's currently focused on OSINT, but any use related with Google is possible.&lt;/p&gt; 
&lt;p&gt;Features :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CLI usage and modules&lt;/li&gt; 
 &lt;li&gt;Python library usage&lt;/li&gt; 
 &lt;li&gt;Fully async&lt;/li&gt; 
 &lt;li&gt;JSON export&lt;/li&gt; 
 &lt;li&gt;Browser extension to ease login&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;âœ”ï¸ Requirements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python &amp;gt;= 3.10&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;âš™ï¸ Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ pip3 install pipx
$ pipx ensurepath
$ pipx install ghunt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will automatically use venvs to avoid dependency conflicts with other projects.&lt;/p&gt; 
&lt;h1&gt;ğŸ’ƒ Usage&lt;/h1&gt; 
&lt;h2&gt;Login&lt;/h2&gt; 
&lt;p&gt;First, launch the listener by doing &lt;code&gt;ghunt login&lt;/code&gt; and choose between 1 of the 2 first methods :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ ghunt login

[1] (Companion) Put GHunt on listening mode (currently not compatible with docker)
[2] (Companion) Paste base64-encoded cookies
[3] Enter manually all cookies

Choice =&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, use GHunt Companion to complete the login.&lt;/p&gt; 
&lt;p&gt;The extension is available on the following stores :&lt;br /&gt; &lt;br /&gt; &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/ghunt-companion/"&gt;&lt;img src="https://files.catbox.moe/5g2ld5.png" alt="Firefox" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://chrome.google.com/webstore/detail/ghunt-companion/dpdcofblfbmmnikcbmmiakkclocadjab"&gt;&lt;img src="https://developer.chrome.com/static/docs/webstore/branding/image/206x58-chrome-web-bcb82d15b2486.png" alt="Chrome" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Modules&lt;/h2&gt; 
&lt;p&gt;Then, profit :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;Usage: ghunt [-h] {login,email,gaia,drive,geolocate} ...

Positional Arguments:
  {login,email,gaia,drive,geolocate}
    login               Authenticate GHunt to Google.
    email               Get information on an email address.
    gaia                Get information on a Gaia ID.
    drive               Get information on a Drive file or folder.
    geolocate           Geolocate a BSSID.
    spiderdal           Find assets using Digital Assets Links.

Options:
  -h, --help            show this help message and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ“„ You can also use --json with email, gaia, drive and geolocate modules to export in JSON ! Example :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ ghunt email &amp;lt;email_address&amp;gt; --json user_data.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Have fun ğŸ¥°ğŸ’&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ§‘â€ğŸ’» Developers&lt;/h1&gt; 
&lt;p&gt;ğŸ“• I started writing some docs &lt;a href="https://github.com/mxrch/GHunt/wiki"&gt;here&lt;/a&gt; and examples &lt;a href="https://github.com/mxrch/GHunt/tree/master/examples"&gt;here&lt;/a&gt;, feel free to contribute !&lt;/p&gt; 
&lt;p&gt;To use GHunt as a lib, you can't use pipx because it uses a venv.&lt;br /&gt; So you should install GHunt with pip :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ pip3 install ghunt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And now, you should be able to &lt;code&gt;import ghunt&lt;/code&gt; in your projects !&lt;br /&gt; You can right now play with the &lt;a href="https://github.com/mxrch/GHunt/tree/master/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;ğŸ“® Details&lt;/h1&gt; 
&lt;h2&gt;Obvious disclaimer&lt;/h2&gt; 
&lt;p&gt;This tool is for educational purposes only, I am not responsible for its use.&lt;/p&gt; 
&lt;h2&gt;Less obvious disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is under &lt;a href="https://choosealicense.com/licenses/agpl-3.0/"&gt;AGPL Licence&lt;/a&gt;, and you have to respect it.&lt;br /&gt; &lt;strong&gt;Use it only in personal, criminal investigations, pentesting, or open-source projects.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/novitae"&gt;novitae&lt;/a&gt; for being my Python colleague&lt;/li&gt; 
 &lt;li&gt;All the people on &lt;a href="https://discord.gg/sg2YcrC6x9"&gt;Malfrats Industries&lt;/a&gt; and elsewhere for the beta test !&lt;/li&gt; 
 &lt;li&gt;The HideAndSec team ğŸ’— (blog : &lt;a href="https://hideandsec.sh"&gt;https://hideandsec.sh&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dribbble.com/jouiniamine"&gt;Med Amine Jouini&lt;/a&gt; for his beautiful rework of the Google logo, which I was inspired by &lt;em&gt;a lot&lt;/em&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Thanks to these awesome people for supporting me !&lt;/p&gt; 
&lt;!-- sponsors --&gt;
&lt;a href="https://github.com/BlWasp"&gt;&lt;img src="https://github.com/BlWasp.png" width="50px" alt="BlWasp" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/gingeleski"&gt;&lt;img src="https://github.com/gingeleski.png" width="50px" alt="gingeleski" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/ADS-Fund"&gt;&lt;img src="https://github.com/ADS-Fund.png" width="50px" alt="ADS-Fund" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;!-- sponsors --&gt; 
&lt;p&gt;&lt;br /&gt; You like my work ?&lt;br /&gt; &lt;a href="https://github.com/sponsors/mxrch"&gt;Sponsor me&lt;/a&gt; on GitHub ! ğŸ¤—&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>agno-agi/agno</title>
      <link>https://github.com/agno-agi/agno</link>
      <description>&lt;p&gt;High-performance runtime for multi-agent systems. Build, run and manage secure multi-agent systems in your cloud.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" id="top"&gt; 
 &lt;a href="https://docs.agno.com"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg" /&gt; 
   &lt;img src="https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg?sanitize=true" alt="Agno" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://docs.agno.com"&gt;ğŸ“š Documentation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; 
 &lt;a href="https://docs.agno.com/examples/introduction"&gt;ğŸ’¡ Examples&lt;/a&gt; &amp;nbsp;|&amp;nbsp; 
 &lt;a href="https://www.agno.com/?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=agno-github&amp;amp;utm_content=header"&gt;ğŸ  Website&lt;/a&gt; &amp;nbsp;|&amp;nbsp; 
 &lt;a href="https://github.com/agno-agi/agno/stargazers"&gt;ğŸŒŸ Star Us&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;What is Agno?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://docs.agno.com"&gt;Agno&lt;/a&gt; is a high-performance runtime for multi-agent systems. Use it to build, run and manage secure multi-agent systems in your cloud.&lt;/p&gt; 
&lt;p&gt;Agno gives you the fastest framework for building agents with session management, memory, knowledge, human in the loop and MCP support. You can put agents together as an autonomous multi-agent team, or build step-based agentic workflows for full control over complex multi-step processes.&lt;/p&gt; 
&lt;p&gt;In 10 lines of code, we can build an Agent that will fetch the top stories from HackerNews and summarize them.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.hackernews import HackerNewsTools

agent = Agent(
    model=Claude(id="claude-sonnet-4-0"),
    tools=[HackerNewsTools()],
    markdown=True,
)
agent.print_response("Summarize the top 5 stories on hackernews", stream=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;But the real advantage of Agno is its &lt;a href="https://docs.agno.com/agent-os/introduction"&gt;AgentOS&lt;/a&gt; runtime:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You get a pre-built FastAPI app for running your agentic system, meaning you start building your product on day one. This is a remarkable advantage over other solutions or rolling your own.&lt;/li&gt; 
 &lt;li&gt;You also get a control plane which connects directly to your AgentOS for testing, monitoring and managing your system. This gives you unmatched visibility and control over your system.&lt;/li&gt; 
 &lt;li&gt;Your AgentOS runs in your cloud and you get complete data privacy because no data ever leaves your system. This is incredible for security conscious enterprises that can't send traces to external services.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For organizations building agents, Agno provides the complete solution. You get the fastest framework for building agents (speed of development and execution), a pre-built FastAPI app that lets you build your product on day one, and a control plane for managing your system.&lt;/p&gt; 
&lt;p&gt;We bring a novel architecture that no other framework provides, your AgentOS runs securely in your cloud, and the control plane connects directly to it from your browser. You don't need to send data to external services or pay retention costs, you get complete privacy and control.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;If you're new to Agno, follow our &lt;a href="https://docs.agno.com/introduction/quickstart"&gt;quickstart&lt;/a&gt; to build your first Agent and run it using the AgentOS.&lt;/p&gt; 
&lt;p&gt;After that, checkout the &lt;a href="https://docs.agno.com/examples/introduction"&gt;examples gallery&lt;/a&gt; and build real-world applications with Agno.&lt;/p&gt; 
&lt;h2&gt;Documentation, Community &amp;amp; More Examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docs: &lt;a href="https://docs.agno.com" target="_blank" rel="noopener noreferrer"&gt;docs.agno.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Cookbook: &lt;a href="https://github.com/agno-agi/agno/tree/main/cookbook" target="_blank" rel="noopener noreferrer"&gt;Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Community forum: &lt;a href="https://community.agno.com/" target="_blank" rel="noopener noreferrer"&gt;community.agno.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Discord: &lt;a href="https://discord.gg/4MtYHHrgA8" target="_blank" rel="noopener noreferrer"&gt;discord&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup Your Coding Agent to Use Agno&lt;/h2&gt; 
&lt;p&gt;For LLMs and AI assistants to understand and navigate Agno's documentation, we provide an &lt;a href="https://docs.agno.com/llms.txt"&gt;llms.txt&lt;/a&gt; or &lt;a href="https://docs.agno.com/llms-full.txt"&gt;llms-full.txt&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;This file is built for AI systems to efficiently parse and reference our documentation.&lt;/p&gt; 
&lt;h3&gt;IDE Integration&lt;/h3&gt; 
&lt;p&gt;When building Agno agents, using Agno documentation as a source in your IDE is a great way to speed up your development. Here's how to integrate with Cursor:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;In Cursor, go to the "Cursor Settings" menu.&lt;/li&gt; 
 &lt;li&gt;Find the "Indexing &amp;amp; Docs" section.&lt;/li&gt; 
 &lt;li&gt;Add &lt;code&gt;https://docs.agno.com/llms-full.txt&lt;/code&gt; to the list of documentation URLs.&lt;/li&gt; 
 &lt;li&gt;Save the changes.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Now, Cursor will have access to the Agno documentation. You can do the same with other IDEs like VSCode, Windsurf etc.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;At Agno, we're obsessed with performance. Why? because even simple AI workflows can spawn thousands of Agents. Scale that to a modest number of users and performance becomes a bottleneck. Agno is designed for building highly performant agentic systems:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent instantiation: ~3Î¼s on average&lt;/li&gt; 
 &lt;li&gt;Memory footprint: ~6.5Kib on average&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tested on an Apple M4 Mackbook Pro.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;While an Agent's run-time is bottlenecked by inference, we must do everything possible to minimize execution time, reduce memory usage, and parallelize tool calls. These numbers may seem trivial at first, but our experience shows that they add up even at a reasonably small scale.&lt;/p&gt; 
&lt;h3&gt;Instantiation Time&lt;/h3&gt; 
&lt;p&gt;Let's measure the time it takes for an Agent with 1 tool to start up. We'll run the evaluation 1000 times to get a baseline measurement.&lt;/p&gt; 
&lt;p&gt;You should run the evaluation yourself on your own machine, please, do not take these results at face value.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Setup virtual environment
./scripts/perf_setup.sh
source .venvs/perfenv/bin/activate
# OR Install dependencies manually
# pip install openai agno langgraph langchain_openai

# Agno
python evals/performance/instantiation_with_tool.py

# LangGraph
python evals/performance/other/langgraph_instantiation.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The following evaluation is run on an Apple M4 Mackbook Pro. It also runs as a Github action on this repo.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangGraph is on the right, &lt;strong&gt;let's start it first and give it a head start&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Agno is on the left, notice how it finishes before LangGraph gets 1/2 way through the runtime measurement, and hasn't even started the memory measurement. That's how fast Agno is.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23"&gt;https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Memory Usage&lt;/h3&gt; 
&lt;p&gt;To measure memory usage, we use the &lt;code&gt;tracemalloc&lt;/code&gt; library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.&lt;/p&gt; 
&lt;p&gt;We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we've made a mistake, please let us know.&lt;/p&gt; 
&lt;h3&gt;Conclusion&lt;/h3&gt; 
&lt;p&gt;Agno agents are designed for performance and while we do share some benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.&lt;/p&gt; 
&lt;p&gt;Given that each framework is different and we won't be able to tune their performance like we do with Agno, for future benchmarks we'll only be comparing against ourselves.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;We welcome contributions, read our &lt;a href="https://github.com/agno-agi/agno/raw/v2.0/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;Agno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting &lt;code&gt;AGNO_TELEMETRY=false&lt;/code&gt; in your environment.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;a href="https://raw.githubusercontent.com/agno-agi/agno/main/#top"&gt;â¬†ï¸ Back to Top&lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Azure/azure-sdk-for-python</title>
      <link>https://github.com/Azure/azure-sdk-for-python</link>
      <description>&lt;p&gt;This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://learn.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Azure SDK for Python&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://azure.github.io/azure-sdk/releases/latest/python.html"&gt;&lt;img src="https://img.shields.io/badge/packages-latest-blue.svg?sanitize=true" alt="Packages" /&gt;&lt;/a&gt; &lt;a href="https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencies.html"&gt;&lt;img src="https://img.shields.io/badge/dependency-report-blue.svg?sanitize=true" alt="Dependencies" /&gt;&lt;/a&gt; &lt;a href="https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencyGraph/index.html"&gt;&lt;img src="https://img.shields.io/badge/dependency-graph-blue.svg?sanitize=true" alt="DepGraph" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/azure/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/azure-core.svg?maxAge=2592000" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/azure-sdk/public/_build/latest?definitionId=458&amp;amp;branchName=main"&gt;&lt;img src="https://dev.azure.com/azure-sdk/public/_apis/build/status/python/python%20-%20core%20-%20ci?branchName=main" alt="Build Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repository is for the active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our &lt;a href="https://docs.microsoft.com/python/azure/"&gt;public developer docs&lt;/a&gt; or our versioned &lt;a href="https://azure.github.io/azure-sdk-for-python"&gt;developer docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;For your convenience, each service has a separate set of libraries that you can choose to use instead of one, large Azure package. To get started with a specific library, see the &lt;code&gt;README.md&lt;/code&gt; (or &lt;code&gt;README.rst&lt;/code&gt;) file located in the library's project folder.&lt;/p&gt; 
&lt;p&gt;You can find service libraries in the &lt;code&gt;/sdk&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;The client libraries are supported on Python 3.9 or later. For more details, please read our page on &lt;a href="https://github.com/Azure/azure-sdk-for-python/wiki/Azure-SDKs-Python-version-support-policy"&gt;Azure SDK for Python version support policy&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Packages available&lt;/h2&gt; 
&lt;p&gt;Each service might have a number of libraries available from each of the following categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#client-new-releases"&gt;Client - New Releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#client-previous-versions"&gt;Client - Previous Versions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#management-new-releases"&gt;Management - New Releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Azure/azure-sdk-for-python/main/#management-previous-versions"&gt;Management - Previous Versions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Client: New Releases&lt;/h3&gt; 
&lt;p&gt;New wave of packages that we are announcing as &lt;strong&gt;GA&lt;/strong&gt; and several that are currently releasing in &lt;strong&gt;preview&lt;/strong&gt;. These libraries allow you to use and consume existing resources and interact with them, for example: upload a blob. These libraries share several core functionalities such as: retries, logging, transport protocols, authentication protocols, etc. that can be found in the &lt;a href="https://github.com/Azure/azure-sdk-for-python/raw/main/sdk/core/azure-core"&gt;azure-core&lt;/a&gt; library. You can learn more about these libraries by reading guidelines that they follow &lt;a href="https://azure.github.io/azure-sdk/python/guidelines/index.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find the &lt;a href="https://azure.github.io/azure-sdk/releases/latest/index.html#python"&gt;most up to date list of all of the new packages on our page&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Client: Previous Versions&lt;/h3&gt; 
&lt;p&gt;Last stable versions of packages that have been provided for usage with Azure and are production-ready. These libraries provide you with similar functionalities to the Preview ones as they allow you to use and consume existing resources and interact with them, for example: upload a blob. They might not implement the &lt;a href="https://azure.github.io/azure-sdk/python/guidelines/index.html"&gt;guidelines&lt;/a&gt; or have the same feature set as the November releases. They do however offer wider coverage of services.&lt;/p&gt; 
&lt;h3&gt;Management: New Releases&lt;/h3&gt; 
&lt;p&gt;A new set of management libraries that follow the &lt;a href="https://azure.github.io/azure-sdk/python/guidelines/"&gt;Azure SDK Design Guidelines for Python&lt;/a&gt; are now available. These new libraries provide a number of core capabilities that are shared amongst all Azure SDKs, including the intuitive Azure Identity library, an HTTP Pipeline with custom policies, error-handling, distributed tracing, and much more. Documentation and code samples for these new libraries can be found &lt;a href="https://aka.ms/azsdk/python/mgmt"&gt;here&lt;/a&gt;. In addition, a migration guide that shows how to transition from older versions of libraries is located &lt;a href="https://github.com/Azure/azure-sdk-for-python/raw/main/doc/sphinx/mgmt_quickstart.rst#migration-guide"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find the &lt;a href="https://azure.github.io/azure-sdk/releases/latest/mgmt/python.html"&gt;most up to date list of all of the new packages on our page&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries. Also, if you are experiencing authentication issues with the management libraries after upgrading certain packages, it's possible that you upgraded to the new versions of SDK without changing the authentication code, please refer to the migration guide mentioned above for proper instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Management: Previous Versions&lt;/h3&gt; 
&lt;p&gt;For a complete list of management libraries that enable you to provision and manage Azure resources, please &lt;a href="https://azure.github.io/azure-sdk/releases/latest/all/python.html"&gt;check here&lt;/a&gt;. They might not have the same feature set as the new releases but they do offer wider coverage of services. Management libraries can be identified by namespaces that start with &lt;code&gt;azure-mgmt-&lt;/code&gt;, e.g. &lt;code&gt;azure-mgmt-compute&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Need help?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For detailed documentation visit our &lt;a href="https://aka.ms/python-docs"&gt;Azure SDK for Python documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;File an issue via &lt;a href="https://github.com/Azure/azure-sdk-for-python/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check &lt;a href="https://stackoverflow.com/questions/tagged/azure+python"&gt;previous questions&lt;/a&gt; or ask new ones on StackOverflow using &lt;code&gt;azure&lt;/code&gt; and &lt;code&gt;python&lt;/code&gt; tags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Data Collection&lt;/h2&gt; 
&lt;p&gt;The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described below. You can learn more about data collection and use in the help documentation and Microsoftâ€™s &lt;a href="https://go.microsoft.com/fwlink/?LinkID=824704"&gt;privacy statement&lt;/a&gt;. For more information on the data collected by the Azure SDK, please visit the &lt;a href="https://azure.github.io/azure-sdk/general_azurecore.html#telemetry-policy"&gt;Telemetry Guidelines&lt;/a&gt; page.&lt;/p&gt; 
&lt;h3&gt;Telemetry Configuration&lt;/h3&gt; 
&lt;p&gt;Telemetry collection is on by default.&lt;/p&gt; 
&lt;p&gt;To opt out, you can disable telemetry at client construction. Define a &lt;code&gt;NoUserAgentPolicy&lt;/code&gt; class that is a subclass of &lt;code&gt;UserAgentPolicy&lt;/code&gt; with an &lt;code&gt;on_request&lt;/code&gt; method that does nothing. Then pass instance of this class as kwargs &lt;code&gt;user_agent_policy=NoUserAgentPolicy()&lt;/code&gt; during client creation. This will disable telemetry for all methods in the client. Do this for every new client.&lt;/p&gt; 
&lt;p&gt;The example below uses the &lt;code&gt;azure-storage-blob&lt;/code&gt; package. In your code, you can replace &lt;code&gt;azure-storage-blob&lt;/code&gt; with the package you are using.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from azure.identity import ManagedIdentityCredential
from azure.storage.blob import BlobServiceClient
from azure.core.pipeline.policies import UserAgentPolicy


# Create your credential you want to use
mi_credential = ManagedIdentityCredential()

account_url = "https://&amp;lt;storageaccountname&amp;gt;.blob.core.windows.net"

# Set up user-agent override
class NoUserAgentPolicy(UserAgentPolicy):
    def on_request(self, request):
        pass

# Create the BlobServiceClient object
blob_service_client = BlobServiceClient(account_url, credential=mi_credential, user_agent_policy=NoUserAgentPolicy())

container_client = blob_service_client.get_container_client(container=&amp;lt;container_name&amp;gt;) 
# TODO: do something with the container client like download blob to a file
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reporting security issues and security bugs&lt;/h3&gt; 
&lt;p&gt;Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) &lt;a href="mailto:secure@microsoft.com"&gt;secure@microsoft.com&lt;/a&gt;. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the &lt;a href="https://www.microsoft.com/msrc/faqs-report-an-issue"&gt;Security TechCenter&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;For details on contributing to this repository, see the &lt;a href="https://github.com/Azure/azure-sdk-for-python/raw/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.microsoft.com"&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>deepset-ai/haystack</title>
      <link>https://github.com/deepset-ai/haystack</link>
      <description>&lt;p&gt;AI orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://haystack.deepset.ai/"&gt;&lt;img src="https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/banner.png" alt="Green logo of a stylized white 'H' with the text 'Haystack, by deepset.'&amp;nbsp;Abstract green and yellow diagrams in the background." /&gt;&lt;/a&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CI/CD&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/deepset-ai/haystack/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/deepset-ai/haystack/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/python/mypy"&gt;&lt;img src="https://img.shields.io/badge/types-Mypy-blue.svg?sanitize=true" alt="types - Mypy" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/deepset-ai/haystack?branch=main"&gt;&lt;img src="https://coveralls.io/repos/github/deepset-ai/haystack/badge.svg?branch=main" alt="Coverage Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Docs&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://docs.haystack.deepset.ai"&gt;&lt;img src="https://img.shields.io/website?label=documentation&amp;amp;up_message=online&amp;amp;url=https%3A%2F%2Fdocs.haystack.deepset.ai" alt="Website" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Package&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://pypi.org/project/haystack-ai/"&gt;&lt;img src="https://img.shields.io/pypi/v/haystack-ai" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dm/haystack-ai?color=blue&amp;amp;logo=pypi&amp;amp;logoColor=gold" alt="PyPI - Downloads" /&gt; &lt;img src="https://img.shields.io/pypi/pyversions/haystack-ai?logo=python&amp;amp;logoColor=gold" alt="PyPI - Python Version" /&gt; &lt;a href="https://anaconda.org/conda-forge/haystack-ai"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/haystack-ai.svg?sanitize=true" alt="Conda Version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/deepset-ai/haystack?color=blue" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml"&gt;&lt;img src="https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml/badge.svg?sanitize=true" alt="License Compliance" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Meta&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://discord.com/invite/xYvH6drSmA"&gt;&lt;img src="https://img.shields.io/discord/993534733298450452?logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/haystack_ai"&gt;&lt;img src="https://img.shields.io/twitter/follow/haystack_ai" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://haystack.deepset.ai/"&gt;Haystack&lt;/a&gt; is an end-to-end LLM framework that allows you to build applications powered by LLMs, Transformer models, vector search and more. Whether you want to perform retrieval-augmented generation (RAG), document search, question answering or answer generation, Haystack can orchestrate state-of-the-art embedding models and LLMs into pipelines to build end-to-end NLP applications and solve your use case.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#features"&gt;Use Cases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#-tip-1"&gt;Hayhooks (REST API Deployment)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#haystack-enterprise-best-practices-and-expert-support"&gt;Haystack Enterprise&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#-deepset-studio-your-development-environment-for-haystack"&gt;deepset Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#telemetry"&gt;Telemetry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#-community"&gt;ğŸ–– Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#contributing-to-haystack"&gt;Contributing to Haystack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepset-ai/haystack/main/#who-uses-haystack"&gt;Who Uses Haystack&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The simplest way to get Haystack is via pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install haystack-ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install from the &lt;code&gt;main&lt;/code&gt; branch to try the newest features:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install git+https://github.com/deepset-ai/haystack.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Haystack supports multiple installation methods including Docker images. For a comprehensive guide please refer to the &lt;a href="https://docs.haystack.deepset.ai/docs/installation"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;If you're new to the project, check out &lt;a href="https://haystack.deepset.ai/overview/intro"&gt;"What is Haystack?"&lt;/a&gt; then go through the &lt;a href="https://haystack.deepset.ai/overview/quick-start"&gt;"Get Started Guide"&lt;/a&gt; and build your first LLM application in a matter of minutes. Keep learning with the &lt;a href="https://haystack.deepset.ai/tutorials"&gt;tutorials&lt;/a&gt;. For more advanced use cases, or just to get some inspiration, you can browse our Haystack recipes in the &lt;a href="https://haystack.deepset.ai/cookbook"&gt;Cookbook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;At any given point, hit the &lt;a href="https://docs.haystack.deepset.ai/docs/intro"&gt;documentation&lt;/a&gt; to learn more about Haystack, what can it do for you and the technology behind.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Technology agnostic:&lt;/strong&gt; Allow users the flexibility to decide what vendor or technology they want and make it easy to switch out any component for another. Haystack allows you to use and compare models available from OpenAI, Cohere and Hugging Face, as well as your own local models or models hosted on Azure, Bedrock and SageMaker.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Explicit:&lt;/strong&gt; Make it transparent how different moving parts can â€œtalkâ€ to each other so it's easier to fit your tech stack and use case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible:&lt;/strong&gt; Haystack provides all tooling in one place: database access, file conversion, cleaning, splitting, training, eval, inference, and more. And whenever custom behavior is desirable, it's easy to create custom components.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible:&lt;/strong&gt; Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Some examples of what you can do with Haystack:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build &lt;strong&gt;retrieval augmented generation (RAG)&lt;/strong&gt; by making use of one of the available vector databases and customizing your LLM interaction, the sky is the limit ğŸš€&lt;/li&gt; 
 &lt;li&gt;Perform Question Answering &lt;strong&gt;in natural language&lt;/strong&gt; to find granular answers in your documents.&lt;/li&gt; 
 &lt;li&gt;Perform &lt;strong&gt;semantic search&lt;/strong&gt; and retrieve documents according to meaning.&lt;/li&gt; 
 &lt;li&gt;Build applications that can make complex decisions making to answer complex queries: such as systems that can resolve complex customer queries, do knowledge search on many disconnected resources and so on.&lt;/li&gt; 
 &lt;li&gt;Scale to millions of docs using retrievers and production-scale components.&lt;/li&gt; 
 &lt;li&gt;Use &lt;strong&gt;off-the-shelf models&lt;/strong&gt; or &lt;strong&gt;fine-tune&lt;/strong&gt; them to your data.&lt;/li&gt; 
 &lt;li&gt;Use &lt;strong&gt;user feedback&lt;/strong&gt; to evaluate, benchmark, and continuously improve your models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Would you like to deploy and serve Haystack pipelines as REST APIs yourself? &lt;a href="https://github.com/deepset-ai/hayhooks"&gt;Hayhooks&lt;/a&gt; provides a simple way to wrap your pipelines with custom logic and expose them via HTTP endpoints, including OpenAI-compatible chat completion endpoints and compatibility with fully-featured chat interfaces like &lt;a href="https://openwebui.com/"&gt;open-webui&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Haystack Enterprise: Best Practices and Expert Support&lt;/h2&gt; 
&lt;p&gt;Get expert support from the Haystack team, build faster with enterprise-grade templates, and scale securely with deployment guides for cloud and on-prem environments - all with &lt;strong&gt;Haystack Enterprise&lt;/strong&gt;. Read more about it our &lt;a href="https://haystack.deepset.ai/blog/announcing-haystack-enterprise"&gt;announcement post&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;ğŸ‘‰ &lt;a href="https://www.deepset.ai/products-and-services/haystack-enterprise?utm_source=github.com&amp;amp;utm_medium=referral&amp;amp;utm_campaign=haystack_enterprise"&gt;Get Haystack Enterprise&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;deepset Studio: Your Development Environment for Haystack&lt;/h2&gt; 
&lt;p&gt;Use &lt;strong&gt;deepset Studio&lt;/strong&gt; to visually create, deploy, and test your Haystack pipelines. Learn more about it in our &lt;a href="https://haystack.deepset.ai/blog/announcing-studio"&gt;announcement post&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e4f09746-20b5-433e-8261-eca224ac23b3" alt="studio" /&gt;&lt;/p&gt; 
&lt;p&gt;ğŸ‘‰ &lt;a href="https://landing.deepset.ai/deepset-studio-signup"&gt;Sign up&lt;/a&gt;!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;img src="https://github.com/deepset-ai/haystack/raw/main/docs/img/deepset-platform-logo-alternative.jpeg" width="20%" /&gt;&lt;/p&gt; 
 &lt;p&gt;Are you looking for a managed solution that benefits from Haystack? &lt;a href="https://www.deepset.ai/products-and-services/deepset-ai-platform?utm_campaign=developer-relations&amp;amp;utm_source=haystack&amp;amp;utm_medium=readme"&gt;deepset AI Platform&lt;/a&gt; is our fully managed, end-to-end platform to integrate LLMs with your data, which uses Haystack for the LLM pipelines architecture.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;Haystack collects &lt;strong&gt;anonymous&lt;/strong&gt; usage statistics of pipeline components. We receive an event every time these components are initialized. This way, we know which components are most relevant to our community.&lt;/p&gt; 
&lt;p&gt;Read more about telemetry in Haystack or how you can opt out in &lt;a href="https://docs.haystack.deepset.ai/docs/telemetry"&gt;Haystack docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ–– Community&lt;/h2&gt; 
&lt;p&gt;If you have a feature request or a bug report, feel free to open an &lt;a href="https://github.com/deepset-ai/haystack/issues"&gt;issue in Github&lt;/a&gt;. We regularly check these and you can expect a quick response. If you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project, you can start a thread in &lt;a href="https://github.com/deepset-ai/haystack/discussions"&gt;Github Discussions&lt;/a&gt; or our &lt;a href="https://discord.com/invite/VBpFzsgRVF"&gt;Discord channel&lt;/a&gt;. We also check &lt;a href="https://twitter.com/haystack_ai"&gt;ğ• (Twitter)&lt;/a&gt; and &lt;a href="https://stackoverflow.com/questions/tagged/haystack"&gt;Stack Overflow&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing to Haystack&lt;/h2&gt; 
&lt;p&gt;We are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature! You don't need to be a Haystack expert to provide meaningful improvements. To learn how to get started, check out our &lt;a href="https://github.com/deepset-ai/haystack/raw/main/CONTRIBUTING.md"&gt;Contributor Guidelines&lt;/a&gt; first.&lt;/p&gt; 
&lt;p&gt;There are several ways you can contribute to Haystack:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Contribute to the main Haystack project&lt;/li&gt; 
 &lt;li&gt;Contribute an integration on &lt;a href="https://github.com/deepset-ai/haystack-core-integrations"&gt;haystack-core-integrations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] ğŸ‘‰ &lt;strong&gt;&lt;a href="https://github.com/orgs/deepset-ai/projects/14"&gt;Check out the full list of issues that are open to contributions&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Who Uses Haystack&lt;/h2&gt; 
&lt;p&gt;Here's a list of projects and companies using Haystack. Want to add yours? Open a PR, add it to the list and let the world know that you use Haystack!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.airbus.com/en"&gt;Airbus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.al-enterprise.com/"&gt;Alcatel-Lucent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.apple.com/"&gt;Apple&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.betterup.com/"&gt;BetterUp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.databricks.com/"&gt;Databricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deepset.ai/"&gt;Deepset&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.deepset.ai/blog/improving-on-site-search-for-government-agencies-etalab"&gt;Etalab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.infineon.com/"&gt;Infineon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/intel/open-domain-question-and-answer#readme"&gt;Intel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.intelijus.ai/"&gt;Intelijus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/IntelLabs/fastRAG#readme"&gt;Intel Labs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/larsbaunwall/bricky#readme"&gt;LEGO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.meta.com/about"&gt;Meta&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://netflix.com"&gt;Netflix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nos.pt/en/welcome"&gt;NOS Portugal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/blog/reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers/"&gt;Nvidia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PostHog/max-ai#readme"&gt;PostHog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.rakuten.com/"&gt;Rakuten&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.deepset.ai/blog/advanced-neural-search-with-sooth-ai"&gt;Sooth.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>OpenPipe/ART</title>
      <link>https://github.com/OpenPipe/ART</link>
      <description>&lt;p&gt;Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://art.openpipe.ai"&gt;
   &lt;picture&gt; 
    &lt;img alt="ART logo" src="https://github.com/openpipe/art/raw/main/assets/ART_logo.png" width="160px" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Train multi-step agents for real-world tasks using GRPO. &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/openpipe/art/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true" alt="PRs-Welcome" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/openpipe-art/"&gt;&lt;img src="https://img.shields.io/pypi/v/openpipe-art?color=364fc7" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Train Agent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/zbBHRUpwf4"&gt;&lt;img src="https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://art.openpipe.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-orange?style=plastic&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“ RULER: Zero-Shot Agent Rewards&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;RULER&lt;/strong&gt; (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the restâ€”&lt;strong&gt;no labeled data, expert feedback, or reward engineering required&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;âœ¨ &lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2-3x faster development&lt;/strong&gt; - Skip reward function engineering entirely&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;General-purpose&lt;/strong&gt; - Works across any task without modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strong performance&lt;/strong&gt; - Matches or exceeds hand-crafted rewards in 3/4 benchmarks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy integration&lt;/strong&gt; - Drop-in replacement for manual reward functions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, "openai/o3")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://art.openpipe.ai/fundamentals/ruler"&gt;ğŸ“– Learn more about RULER â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ART Overview&lt;/h2&gt; 
&lt;p&gt;ART is an open-source RL framework that improves agent reliability by allowing LLMs to &lt;strong&gt;learn from experience&lt;/strong&gt;. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the &lt;a href="https://art.openpipe.ai"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“’ Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Task&lt;/th&gt; 
   &lt;th&gt;Example Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Comparative Performance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ARTâ€¢E LangGraph&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using LangGraph&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCPâ€¢RL&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B masters the NWS MCP server&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ARTâ€¢E [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using RULER&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/dev/art-e/art_e/evaluate/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2048&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play 2048&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/2048/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Clue&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to solve Temporal Clue&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tic Tac Toe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Tic Tac Toe&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/tic_tac_toe/display-benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codenames&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Codenames&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png" height="72" /&gt; &lt;a href="https://github.com/OpenPipe/art-notebooks/raw/main/examples/codenames/Codenames_RL.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AutoRL [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Train Qwen 2.5 7B to master any task&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ“° ART News&lt;/h2&gt; 
&lt;p&gt;Explore our latest research and updates on building SOTA agents.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://art.openpipe.ai/integrations/langgraph-integration"&gt;ART now integrates seamlessly with LangGraph&lt;/a&gt;&lt;/strong&gt; - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://x.com/corbtt/status/1953171838382817625"&gt;MCPâ€¢RL: Teach Your Model to Master Any MCP Server&lt;/a&gt;&lt;/strong&gt; - Automatically train models to effectively use MCP server tools through reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://x.com/mattshumer_/status/1950572449025650733"&gt;AutoRL: Zero-Data Training for Any Task&lt;/a&gt;&lt;/strong&gt; - Train custom AI models without labeled data using automatic input generation and RULER evaluation.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards"&gt;RULER: Easy Mode for RL Rewards&lt;/a&gt;&lt;/strong&gt; is now available for automatic reward generation in reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ARTÂ·E: How We Built an Email Research Agent That Beats o3&lt;/a&gt;&lt;/strong&gt; demonstrates a Qwen 2.5 14B email agent outperforming OpenAI's o3.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-trainer"&gt;ART Trainer: A New RL Trainer for Agents&lt;/a&gt;&lt;/strong&gt; enables easy training of LLM-based agents using GRPO.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://openpipe.ai/blog"&gt;ğŸ“– See all blog posts â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why ART?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ART provides convenient wrappers for introducing RL training into &lt;strong&gt;existing applications&lt;/strong&gt;. We abstract the training server into a modular service that your code doesn't need to interface with.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train from anywhere.&lt;/strong&gt; Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.&lt;/li&gt; 
 &lt;li&gt;Integrations with hosted platforms like W&amp;amp;B, Langfuse, and OpenPipe provide flexible observability and &lt;strong&gt;simplify debugging&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ART is customizable with &lt;strong&gt;intelligent defaults&lt;/strong&gt;. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install openpipe-art
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¤– ARTâ€¢E Agent&lt;/h2&gt; 
&lt;p&gt;Curious about how to use ART for a real-world task? Check out the &lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ARTâ€¢E Agent&lt;/a&gt; blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!&lt;/p&gt; 
&lt;img src="https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png" width="700" /&gt; 
&lt;h2&gt;ğŸ” Training Loop Overview&lt;/h2&gt; 
&lt;p&gt;ART's functionality is divided into a &lt;strong&gt;client&lt;/strong&gt; and a &lt;strong&gt;server&lt;/strong&gt;. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).&lt;/li&gt; 
   &lt;li&gt;Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.&lt;/li&gt; 
   &lt;li&gt;As the agent executes, each &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, and &lt;code&gt;assistant&lt;/code&gt; message is stored in a Trajectory.&lt;/li&gt; 
   &lt;li&gt;When a rollout finishes, your code assigns a &lt;code&gt;reward&lt;/code&gt; to its Trajectory, indicating the performance of the LLM.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.&lt;/li&gt; 
   &lt;li&gt;The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).&lt;/li&gt; 
   &lt;li&gt;The server saves the newly trained LoRA to a local directory and loads it into vLLM.&lt;/li&gt; 
   &lt;li&gt;Inference is unblocked and the loop resumes at step 1.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This training loop runs until a specified number of inference and training iterations have completed.&lt;/p&gt; 
&lt;h2&gt;ğŸ§© Supported Models&lt;/h2&gt; 
&lt;p&gt;ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth&lt;/a&gt;. Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on &lt;a href="https://discord.gg/zbBHRUpwf4"&gt;Discord&lt;/a&gt; or open an issue on &lt;a href="https://github.com/openpipe/art/issues"&gt;GitHub&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;ART is in active development, and contributions are most welcome! Please see the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;ğŸ“– Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš–ï¸ License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ™ Credits&lt;/h2&gt; 
&lt;p&gt;ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/trl"&gt;trl&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchtune"&gt;torchtune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ml-explore/mlx-lm</title>
      <link>https://github.com/ml-explore/mlx-lm</link>
      <description>&lt;p&gt;Run LLMs with MLX&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;MLX LM&lt;/h2&gt; 
&lt;p&gt;MLX LM is a Python package for generating text and fine-tuning large language models on Apple silicon with MLX.&lt;/p&gt; 
&lt;p&gt;Some key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integration with the Hugging Face Hub to easily use thousands of LLMs with a single command.&lt;/li&gt; 
 &lt;li&gt;Support for quantizing and uploading models to the Hugging Face Hub.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/LORA.md"&gt;Low-rank and full model fine-tuning&lt;/a&gt; with support for quantized models.&lt;/li&gt; 
 &lt;li&gt;Distributed inference and fine-tuning with &lt;code&gt;mx.distributed&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The easiest way to get started is to install the &lt;code&gt;mlx-lm&lt;/code&gt; package:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;To generate text with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate --prompt "How tall is Mt Everest?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To chat with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.chat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will give you a chat REPL that you can use to interact with the LLM. The chat context is preserved during the lifetime of the REPL.&lt;/p&gt; 
&lt;p&gt;Commands in &lt;code&gt;mlx-lm&lt;/code&gt; typically take command line options which let you specify the model, sampling parameters, and more. Use &lt;code&gt;-h&lt;/code&gt; to see a list of available options for a command, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default model for generation and chat is &lt;code&gt;mlx-community/Llama-3.2-3B-Instruct-4bit&lt;/code&gt;. You can specify any MLX-compatible model with the &lt;code&gt;--model&lt;/code&gt; flag. Thousands are available in the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Community&lt;/a&gt; Hugging Face organization.&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;You can use &lt;code&gt;mlx-lm&lt;/code&gt; as a module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

text = generate(model, tokenizer, prompt=prompt, verbose=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(generate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/generate_response.py"&gt;generation example&lt;/a&gt; to see how to use the API in more detail. Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/batch_generate_response.py"&gt;batch generation example&lt;/a&gt; to see how to efficiently generate continuations for a batch of prompts.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;mlx-lm&lt;/code&gt; package also comes with functionality to quantize and optionally upload models to the Hugging Face Hub.&lt;/p&gt; 
&lt;p&gt;You can convert models using the Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import convert

repo = "mistralai/Mistral-7B-Instruct-v0.3"
upload_repo = "mlx-community/My-Mistral-7B-Instruct-v0.3-4bit"

convert(repo, quantize=True, upload_repo=upload_repo)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will generate a 4-bit quantized Mistral 7B and upload it to the repo &lt;code&gt;mlx-community/My-Mistral-7B-Instruct-v0.3-4bit&lt;/code&gt;. It will also save the converted model in the path &lt;code&gt;mlx_model&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(convert)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Streaming&lt;/h4&gt; 
&lt;p&gt;For streaming generation, use the &lt;code&gt;stream_generate&lt;/code&gt; function. This yields a generation response object.&lt;/p&gt; 
&lt;p&gt;For example,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, stream_generate

repo = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
model, tokenizer = load(repo)

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True
)

for response in stream_generate(model, tokenizer, prompt, max_tokens=512):
    print(response.text, end="", flush=True)
print()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Sampling&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;generate&lt;/code&gt; and &lt;code&gt;stream_generate&lt;/code&gt; functions accept &lt;code&gt;sampler&lt;/code&gt; and &lt;code&gt;logits_processors&lt;/code&gt; keyword arguments. A sampler is any callable which accepts a possibly batched logits array and returns an array of sampled tokens. The &lt;code&gt;logits_processors&lt;/code&gt; must be a list of callables which take the token history and current logits as input and return the processed logits. The logits processors are applied in order.&lt;/p&gt; 
&lt;p&gt;Some standard sampling functions and logits processors are provided in &lt;code&gt;mlx_lm.sample_utils&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;p&gt;You can also use &lt;code&gt;mlx-lm&lt;/code&gt; from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt "hello"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will download a Mistral 7B model from the Hugging Face Hub and generate text using the given prompt.&lt;/p&gt; 
&lt;p&gt;For a full list of options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To quantize a model from the command line run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can upload new models to Hugging Face by specifying &lt;code&gt;--upload-repo&lt;/code&gt; to &lt;code&gt;convert&lt;/code&gt;. For example, to upload a quantized Mistral-7B model to the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Hugging Face community&lt;/a&gt; you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert \
    --hf-path mistralai/Mistral-7B-Instruct-v0.3 \
    -q \
    --upload-repo mlx-community/my-4bit-mistral
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models can also be converted and quantized directly in the &lt;a href="https://huggingface.co/spaces/mlx-community/mlx-my-repo"&gt;mlx-my-repo&lt;/a&gt; Hugging Face Space.&lt;/p&gt; 
&lt;h3&gt;Long Prompts and Generations&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; has some tools to scale efficiently to long prompts and generations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A rotating fixed-size key-value cache.&lt;/li&gt; 
 &lt;li&gt;Prompt caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use the rotating key-value cache pass the argument &lt;code&gt;--max-kv-size n&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; can be any integer. Smaller values like &lt;code&gt;512&lt;/code&gt; will use very little RAM but result in worse quality. Larger values like &lt;code&gt;4096&lt;/code&gt; or higher will use more RAM but have better quality.&lt;/p&gt; 
&lt;p&gt;Caching prompts can substantially speedup reusing the same long context with different queries. To cache a prompt use &lt;code&gt;mlx_lm.cache_prompt&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat prompt.txt | mlx_lm.cache_prompt \
  --model mistralai/Mistral-7B-Instruct-v0.3 \
  --prompt - \
  --prompt-cache-file mistral_prompt.safetensors
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then use the cached prompt with &lt;code&gt;mlx_lm.generate&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate \
    --prompt-cache-file mistral_prompt.safetensors \
    --prompt "\nSummarize the above text."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The cached prompt is treated as a prefix to the supplied prompt. Also notice when using a cached prompt, the model to use is read from the cache and need not be supplied explicitly.&lt;/p&gt; 
&lt;p&gt;Prompt caching can also be used in the Python API in order to avoid recomputing the prompt. This is useful in multi-turn dialogues or across requests that use the same context. See the &lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/examples/chat.py"&gt;example&lt;/a&gt; for more usage details.&lt;/p&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; supports thousands of Hugging Face format LLMs. If the model you want to run is not supported, file an &lt;a href="https://github.com/ml-explore/mlx-lm/issues/new"&gt;issue&lt;/a&gt; or better yet, submit a pull request.&lt;/p&gt; 
&lt;p&gt;Here are a few examples of Hugging Face models that work with this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-7B-v0.1"&gt;mistralai/Mistral-7B-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-2-7b-hf"&gt;meta-llama/Llama-2-7b-hf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct"&gt;deepseek-ai/deepseek-coder-6.7b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/01-ai/Yi-6B-Chat"&gt;01-ai/Yi-6B-Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/microsoft/phi-2"&gt;microsoft/phi-2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"&gt;mistralai/Mixtral-8x7B-Instruct-v0.1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-7B"&gt;Qwen/Qwen-7B&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b"&gt;pfnet/plamo-13b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/pfnet/plamo-13b-instruct"&gt;pfnet/plamo-13b-instruct&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b"&gt;stabilityai/stablelm-2-zephyr-1_6b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/internlm/internlm2-7b"&gt;internlm/internlm2-7b&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/tiiuae/falcon-mamba-7b-instruct"&gt;tiiuae/falcon-mamba-7b-instruct&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mistral&amp;amp;sort=trending"&gt;Mistral&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=llama&amp;amp;sort=trending"&gt;Llama&lt;/a&gt;, &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=phi&amp;amp;sort=trending"&gt;Phi-2&lt;/a&gt;, and &lt;a href="https://huggingface.co/models?library=transformers,safetensors&amp;amp;other=mixtral&amp;amp;sort=trending"&gt;Mixtral&lt;/a&gt; style models should work out of the box.&lt;/p&gt; 
&lt;p&gt;For some models (such as &lt;code&gt;Qwen&lt;/code&gt; and &lt;code&gt;plamo&lt;/code&gt;) the tokenizer requires you to enable the &lt;code&gt;trust_remote_code&lt;/code&gt; option. You can do this by passing &lt;code&gt;--trust-remote-code&lt;/code&gt; in the command line. If you don't specify the flag explicitly, you will be prompted to trust remote code in the terminal when running the model.&lt;/p&gt; 
&lt;p&gt;For &lt;code&gt;Qwen&lt;/code&gt; models you must also specify the &lt;code&gt;eos_token&lt;/code&gt;. You can do this by passing &lt;code&gt;--eos-token "&amp;lt;|endoftext|&amp;gt;"&lt;/code&gt; in the command line.&lt;/p&gt; 
&lt;p&gt;These options can also be set in the Python API. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model, tokenizer = load(
    "qwen/Qwen-7B",
    tokenizer_config={"eos_token": "&amp;lt;|endoftext|&amp;gt;", "trust_remote_code": True},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Large Models&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This requires macOS 15.0 or higher to work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Models which are large relative to the total RAM available on the machine can be slow. &lt;code&gt;mlx-lm&lt;/code&gt; will attempt to make them faster by wiring the memory occupied by the model and cache. This requires macOS 15 or higher to work.&lt;/p&gt; 
&lt;p&gt;If you see the following warning message:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[WARNING] Generating with a model that requires ...&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;then the model will likely be slow on the given machine. If the model fits in RAM then it can often be sped up by increasing the system wired memory limit. To increase the limit, set the following &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo sysctl iogpu.wired_limit_mb=N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The value &lt;code&gt;N&lt;/code&gt; should be larger than the size of the model in megabytes but smaller than the memory size of the machine.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads" /&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions (currently only for pptx and image files), provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o", llm_prompt="optional custom prompt")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/garak</title>
      <link>https://github.com/NVIDIA/garak</link>
      <description>&lt;p&gt;the LLM vulnerability scanner&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;garak, LLM vulnerability scanner&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Generative AI Red-teaming &amp;amp; Assessment Kit&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; checks if an LLM can be made to fail in a way we don't want. &lt;code&gt;garak&lt;/code&gt; probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know &lt;code&gt;nmap&lt;/code&gt; or &lt;code&gt;msf&lt;/code&gt; / Metasploit Framework, garak does somewhat similar things to them, but for LLMs.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; focuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt;'s a free tool. We love developing it and are always interested in adding functionality to support applications.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg?sanitize=true" alt="Tests/Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg?sanitize=true" alt="Tests/Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg?sanitize=true" alt="Tests/OSX" /&gt;&lt;/a&gt; &lt;a href="http://garak.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/garak/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.11036"&gt;&lt;img src="https://img.shields.io/badge/cs.CL-arXiv%3A2406.11036-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/uVch4puUCs"&gt;&lt;img src="https://img.shields.io/badge/chat-on%20discord-yellow.svg?sanitize=true" alt="discord-img" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/garak"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/garak" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/garak"&gt;&lt;img src="https://badge.fury.io/py/garak.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak/month" alt="Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;h3&gt;&amp;gt; See our user guide! &lt;a href="https://docs.garak.ai/"&gt;docs.garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Join our &lt;a href="https://discord.gg/uVch4puUCs"&gt;Discord&lt;/a&gt;!&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Project links &amp;amp; home: &lt;a href="https://garak.ai/"&gt;garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Twitter: &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; DEF CON &lt;a href="https://garak.ai/garak_aiv_slides.pdf"&gt;slides&lt;/a&gt;!&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;h2&gt;LLM support&lt;/h2&gt; 
&lt;p&gt;currently supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/models"&gt;hugging face hub&lt;/a&gt; generative models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/"&gt;replicate&lt;/a&gt; text models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/docs/introduction"&gt;openai api&lt;/a&gt; chat &amp;amp; continuation models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;litellm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;pretty much anything accessible via REST&lt;/li&gt; 
 &lt;li&gt;gguf models like &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; version &amp;gt;= 1046&lt;/li&gt; 
 &lt;li&gt;.. and many more LLMs!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install:&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; is a command-line tool. It's developed in Linux and OSX.&lt;/p&gt; 
&lt;h3&gt;Standard install with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Just grab it from PyPI and you should be good to go:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U garak
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install development version with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;The standard pip version of &lt;code&gt;garak&lt;/code&gt; is updated periodically. To get a fresher version from GitHub, try:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Clone from source&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; has its own dependencies. You can to install &lt;code&gt;garak&lt;/code&gt; in its own Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda create --name garak "python&amp;gt;=3.10,&amp;lt;=3.12"
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OK, if that went fine, you're probably good to go!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you cloned before the move to the &lt;code&gt;NVIDIA&lt;/code&gt; GitHub organisation, but you're reading this at the &lt;code&gt;github.com/NVIDIA&lt;/code&gt; URI, please update your remotes as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git remote set-url origin https://github.com/NVIDIA/garak.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The general syntax is:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak &amp;lt;options&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak --list_probes&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;To specify a generator, use the &lt;code&gt;--model_type&lt;/code&gt; and, optionally, the &lt;code&gt;--model_name&lt;/code&gt; options. Model type specifies a model family/interface; model name specifies the exact model to be used. The "Intro to generators" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set &lt;code&gt;--model_type&lt;/code&gt; to &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;--model_name&lt;/code&gt; to the model's name on Hub (e.g. &lt;code&gt;"RWKV/rwkv-4-169m-pile"&lt;/code&gt;). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; runs all the probes by default, but you can be specific about that too. &lt;code&gt;--probes promptinject&lt;/code&gt; will use only the &lt;a href="https://github.com/agencyenterprise/promptinject"&gt;PromptInject&lt;/a&gt; framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a &lt;code&gt;.&lt;/code&gt;; for example, &lt;code&gt;--probes lmrc.SlurUsage&lt;/code&gt; will use an implementation of checking for models generating slurs based on the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; framework.&lt;/p&gt; 
&lt;p&gt;For help and inspiration, find us on &lt;a href="https://twitter.com/garak_llm"&gt;Twitter&lt;/a&gt; or &lt;a href="https://discord.gg/uVch4puUCs"&gt;discord&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY="sk-123XXXXXXXXXXXX"
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reading the results&lt;/h2&gt; 
&lt;p&gt;For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.&lt;/p&gt; 
&lt;p&gt;Here are the results with the &lt;code&gt;encoding&lt;/code&gt; module on a GPT-3 variant: &lt;img src="https://i.imgur.com/8Dxf45N.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;And the same results for ChatGPT: &lt;img src="https://i.imgur.com/VKAF5if.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections. The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.&lt;/p&gt; 
&lt;p&gt;Errors go in &lt;code&gt;garak.log&lt;/code&gt;; the run is logged in detail in a &lt;code&gt;.jsonl&lt;/code&gt; file specified at analysis start &amp;amp; end. There's a basic analysis script in &lt;code&gt;analyse/analyse_log.py&lt;/code&gt; which will output the probes and prompts that led to the most hits.&lt;/p&gt; 
&lt;p&gt;Send PRs &amp;amp; open issues. Happy hunting!&lt;/p&gt; 
&lt;h2&gt;Intro to generators&lt;/h2&gt; 
&lt;h3&gt;Hugging Face&lt;/h3&gt; 
&lt;p&gt;Using the Pipeline API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface&lt;/code&gt; (for transformers models to run locally)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using the Inference API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface.InferenceAPI&lt;/code&gt; (for API-based model access)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the model name from Hub, e.g. &lt;code&gt;"mosaicml/mpt-7b-instruct"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using private endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type huggingface.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_name&lt;/code&gt; - the endpoint URL, e.g. &lt;code&gt;https://xxx.us-east-1.aws.endpoints.huggingface.cloud&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(optional) set the &lt;code&gt;HF_INFERENCE_TOKEN&lt;/code&gt; environment variable to a Hugging Face API token with the "read" role; see &lt;a href="https://huggingface.co/settings/tokens"&gt;https://huggingface.co/settings/tokens&lt;/a&gt; when logged in&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenAI&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the OpenAI model you'd like to use. &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt; is fast and fine for testing.&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your OpenAI API key (e.g. "sk-19763ASDF87q6657"); see &lt;a href="https://platform.openai.com/account/api-keys"&gt;https://platform.openai.com/account/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.&lt;/p&gt; 
&lt;h3&gt;Replicate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;REPLICATE_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see &lt;a href="https://replicate.com/account/api-tokens"&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Public Replicate models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the Replicate model name and hash, e.g. &lt;code&gt;"stability-ai/stablelm-tuned-alpha-7b:c49dae36"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Private Replicate endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - username/model-name slug from the deployed endpoint, e.g. &lt;code&gt;elim/elims-llama2-7b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cohere&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type cohere&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; (optional, &lt;code&gt;command&lt;/code&gt; by default) - The specific Cohere model you'd like to test&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;COHERE_API_KEY&lt;/code&gt; environment variable to your Cohere API key, e.g. "aBcDeFgHiJ123456789"; see &lt;a href="https://dashboard.cohere.ai/api-keys"&gt;https://dashboard.cohere.ai/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Groq&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type groq&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The name of the model to access via the Groq API&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; environment variable to your Groq API key, see &lt;a href="https://console.groq.com/docs/quickstart"&gt;https://console.groq.com/docs/quickstart&lt;/a&gt; for details on creating an API key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ggml&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type ggml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The path to the ggml model you'd like to load, e.g. &lt;code&gt;/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GGML_MAIN_PATH&lt;/code&gt; environment variable to the path to your ggml &lt;code&gt;main&lt;/code&gt; executable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;REST&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;rest.RestGenerator&lt;/code&gt; is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See &lt;a href="https://reference.garak.ai/en/latest/garak.generators.rest.html"&gt;https://reference.garak.ai/en/latest/garak.generators.rest.html&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h3&gt;NIM&lt;/h3&gt; 
&lt;p&gt;Use models from &lt;a href="https://build.nvidia.com/"&gt;https://build.nvidia.com/&lt;/a&gt; or other NIM endpoints.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;NIM_API_KEY&lt;/code&gt; environment variable to your authentication API token, or specify it in the config YAML&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For chat models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;meta/llama-3.1-8b-instruct&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For completion models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim.NVOpenAICompletion&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;bigcode/starcoder2-15b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Test&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(alternatively) &lt;code&gt;--model_name test.Blank&lt;/code&gt; For testing. This always generates the empty string, using the &lt;code&gt;test.Blank&lt;/code&gt; generator. Will be marked as failing for any tests that &lt;em&gt;require&lt;/em&gt; an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test.Repeat&lt;/code&gt; For testing. This generator repeats back the prompt it received.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to probes&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Probe&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;blank&lt;/td&gt; 
   &lt;td&gt;A simple probe that always sends an empty prompt.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;atkgen&lt;/td&gt; 
   &lt;td&gt;Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 &lt;a href="https://huggingface.co/garak-llm/artgpt2tox"&gt;fine-tuned&lt;/a&gt; on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;av_spam_scanning&lt;/td&gt; 
   &lt;td&gt;Probes that attempt to make the model output malicious content signatures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;continuation&lt;/td&gt; 
   &lt;td&gt;Probes that test if the model will continue a probably undesirable word&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dan&lt;/td&gt; 
   &lt;td&gt;Various &lt;a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html"&gt;DAN&lt;/a&gt; and DAN-like attacks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;donotanswer&lt;/td&gt; 
   &lt;td&gt;Prompts to which responsible language models should not answer.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;encoding&lt;/td&gt; 
   &lt;td&gt;Prompt injection through text encoding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gcg&lt;/td&gt; 
   &lt;td&gt;Disrupt a system prompt by appending an adversarial suffix.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;glitch&lt;/td&gt; 
   &lt;td&gt;Probe model for glitch tokens that provoke unusual behavior.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;grandma&lt;/td&gt; 
   &lt;td&gt;Appeal to be reminded of one's grandmother.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;goodside&lt;/td&gt; 
   &lt;td&gt;Implementations of Riley Goodside attacks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;leakreplay&lt;/td&gt; 
   &lt;td&gt;Evaluate if a model will replay training data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lmrc&lt;/td&gt; 
   &lt;td&gt;Subsample of the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; probes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;malwaregen&lt;/td&gt; 
   &lt;td&gt;Attempts to have the model generate code for building malware&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;misleading&lt;/td&gt; 
   &lt;td&gt;Attempts to make a model support misleading and false claims&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;packagehallucination&lt;/td&gt; 
   &lt;td&gt;Trying to get code generations that specify non-existent (and therefore insecure) packages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;promptinject&lt;/td&gt; 
   &lt;td&gt;Implementation of the Agency Enterprise &lt;a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject"&gt;PromptInject&lt;/a&gt; work (best paper awards @ NeurIPS ML Safety Workshop 2022)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;realtoxicityprompts&lt;/td&gt; 
   &lt;td&gt;Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;snowball&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ofir.io/snowballed_hallucination.pdf"&gt;Snowballed Hallucination&lt;/a&gt; probes designed to make a model give a wrong answer to questions too complex for it to process&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xss&lt;/td&gt; 
   &lt;td&gt;Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Logging&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; generates multiple kinds of log:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A log file, &lt;code&gt;garak.log&lt;/code&gt;. This includes debugging information from &lt;code&gt;garak&lt;/code&gt; and its plugins, and is continued across runs.&lt;/li&gt; 
 &lt;li&gt;A report of the current run, structured as JSONL. A new report file is created every time &lt;code&gt;garak&lt;/code&gt; runs. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's &lt;code&gt;status&lt;/code&gt; attribute takes a constant from &lt;code&gt;garak.attempts&lt;/code&gt; to describe what stage it was made at.&lt;/li&gt; 
 &lt;li&gt;A hit log, detailing attempts that yielded a vulnerability (a 'hit')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How is the code structured?&lt;/h2&gt; 
&lt;p&gt;Check out the &lt;a href="https://reference.garak.ai/"&gt;reference docs&lt;/a&gt; for an authoritative guide to &lt;code&gt;garak&lt;/code&gt; code structure.&lt;/p&gt; 
&lt;p&gt;In a typical run, &lt;code&gt;garak&lt;/code&gt; will read a model type (and optionally model name) from the command line, then determine which &lt;code&gt;probe&lt;/code&gt;s and &lt;code&gt;detector&lt;/code&gt;s to run, start up a &lt;code&gt;generator&lt;/code&gt;, and then pass these to a &lt;code&gt;harness&lt;/code&gt; to do the probing; an &lt;code&gt;evaluator&lt;/code&gt; deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;garak/probes/&lt;/code&gt; - classes for generating interactions with LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/detectors/&lt;/code&gt; - classes for detecting an LLM is exhibiting a given failure mode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/evaluators/&lt;/code&gt; - assessment reporting schemes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/generators/&lt;/code&gt; - plugins for LLMs to be probed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/harnesses/&lt;/code&gt; - classes for structuring testing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resources/&lt;/code&gt; - ancillary items required by plugins&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The default operating mode is to use the &lt;code&gt;probewise&lt;/code&gt; harness. Given a list of probe module names and probe plugin names, the &lt;code&gt;probewise&lt;/code&gt; harness instantiates each probe, then for each probe reads its &lt;code&gt;recommended_detectors&lt;/code&gt; attribute to get a list of &lt;code&gt;detector&lt;/code&gt;s to run on the output.&lt;/p&gt; 
&lt;p&gt;Each plugin category (&lt;code&gt;probes&lt;/code&gt;, &lt;code&gt;detectors&lt;/code&gt;, &lt;code&gt;evaluators&lt;/code&gt;, &lt;code&gt;generators&lt;/code&gt;, &lt;code&gt;harnesses&lt;/code&gt;) includes a &lt;code&gt;base.py&lt;/code&gt; which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, &lt;code&gt;garak.generators.openai.OpenAIGenerator&lt;/code&gt; descends from &lt;code&gt;garak.generators.base.Generator&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using &lt;code&gt;garak&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Developing your own plugin&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Take a look at how other plugins do it&lt;/li&gt; 
 &lt;li&gt;Inherit from one of the base classes, e.g. &lt;code&gt;garak.probes.base.TextProbe&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Override as little as possible&lt;/li&gt; 
 &lt;li&gt;You can test the new code in at least two ways: 
  &lt;ul&gt; 
   &lt;li&gt;Start an interactive Python session 
    &lt;ul&gt; 
     &lt;li&gt;Import the model, e.g. &lt;code&gt;import garak.probes.mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;Instantiate the plugin, e.g. &lt;code&gt;p = garak.probes.mymodule.MyProbe()&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Run a scan with test plugins 
    &lt;ul&gt; 
     &lt;li&gt;For probes, try a blank generator and always.Pass detector: &lt;code&gt;python3 -m garak -m test.Blank -p mymodule -d always.Pass&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For detectors, try a blank generator and a blank probe: &lt;code&gt;python3 -m garak -m test.Blank -p test.Blank -d mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For generators, try a blank probe and always.Pass detector: &lt;code&gt;python3 -m garak -m mymodule -p test.Blank -d always.Pass&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Get &lt;code&gt;garak&lt;/code&gt; to list all the plugins of the type you're writing, with &lt;code&gt;--list_probes&lt;/code&gt;, &lt;code&gt;--list_detectors&lt;/code&gt;, or &lt;code&gt;--list_generators&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;We have an FAQ &lt;a href="https://github.com/NVIDIA/garak/raw/main/FAQ.md"&gt;here&lt;/a&gt;. Reach out if you have any more questions! &lt;a href="mailto:garak@nvidia.com"&gt;garak@nvidia.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Code reference documentation is at &lt;a href="https://garak.readthedocs.io/en/latest/"&gt;garak.readthedocs.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing garak&lt;/h2&gt; 
&lt;p&gt;You can read the &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/garak-paper.pdf"&gt;garak preprint paper&lt;/a&gt;. If you use garak, please cite us.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly"&lt;/em&gt; - Elim&lt;/p&gt; 
&lt;p&gt;For updates and news see &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Â© 2023- Leon Derczynski; Apache license v2, see &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Physical-Intelligence/openpi</title>
      <link>https://github.com/Physical-Intelligence/openpi</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openpi&lt;/h1&gt; 
&lt;p&gt;openpi holds open-source models and packages for robotics, published by the &lt;a href="https://www.physicalintelligence.company/"&gt;Physical Intelligence team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, this repo contains three types of models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt;, a flow-based vision-language-action model (VLA).&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt;, an autoregressive VLA, based on the FAST action tokenizer.&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt;, an upgraded version of Ï€â‚€ with better open-world generalization trained with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;. Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For all models, we provide &lt;em&gt;base model&lt;/em&gt; checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.&lt;/p&gt; 
&lt;p&gt;This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://droid-dataset.github.io/"&gt;DROID&lt;/a&gt;, and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Sept 2025] We released PyTorch support in openpi.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025]: We have added an &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md#data-filtering"&gt;improved idle filter&lt;/a&gt; for DROID training.&lt;/li&gt; 
 &lt;li&gt;[Jun 2025]: We have added &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md"&gt;instructions&lt;/a&gt; for using &lt;code&gt;openpi&lt;/code&gt; to train VLAs on the full &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;. This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring &lt;code&gt;fsdp_devices&lt;/code&gt; in the training config. Please also note that the current training script does not yet support multi-node training.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mode&lt;/th&gt; 
   &lt;th&gt;Memory Required&lt;/th&gt; 
   &lt;th&gt;Example GPU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 8 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (LoRA)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 22.5 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (Full)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 70 GB&lt;/td&gt; 
   &lt;td&gt;A100 (80GB) / H100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;When cloning this repo, make sure to update submodules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage Python dependencies. See the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installation instructions&lt;/a&gt; to set it up. Once uv is installed, run the following to set up the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: &lt;code&gt;GIT_LFS_SKIP_SMUDGE=1&lt;/code&gt; is needed to pull LeRobot as a dependency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/docker.md"&gt;Docker Setup&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Model Checkpoints&lt;/h2&gt; 
&lt;h3&gt;Base Models&lt;/h3&gt; 
&lt;p&gt;We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base autoregressive &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-Tuned Models&lt;/h3&gt; 
&lt;p&gt;We also provide "expert" checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST-DROID&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$-FAST model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-DROID&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-towel&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can fold diverse towels 0-shot on ALOHA robot platforms&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_towel&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-tupperware&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can unpack food from a tupperware container&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_tupperware&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-pen-uncap&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on public &lt;a href="https://dit-policy.github.io/"&gt;ALOHA&lt;/a&gt; data: can uncap a pen&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-LIBERO&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned for the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO&lt;/a&gt; benchmark: gets state-of-the-art performance (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_libero&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-DROID&lt;/td&gt; 
   &lt;td&gt;Inference / Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt; with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;: fast inference and good language-following&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, checkpoints are automatically downloaded from &lt;code&gt;gs://openpi-assets&lt;/code&gt; and are cached in &lt;code&gt;~/.cache/openpi&lt;/code&gt; when needed. You can overwrite the download path by setting the &lt;code&gt;OPENPI_DATA_HOME&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Running Inference for a Pre-Trained Model&lt;/h2&gt; 
&lt;p&gt;Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = download.maybe_download("gs://openpi-assets/checkpoints/pi05_droid")

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    "observation/exterior_image_1_left": ...,
    "observation/wrist_image_left": ...,
    ...
    "prompt": "pick up the fork"
}
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also test this out in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/inference.ipynb"&gt;example notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README.md"&gt;DROID&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real/README.md"&gt;ALOHA&lt;/a&gt; robots.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Remote Inference&lt;/strong&gt;: We provide &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;examples and code&lt;/a&gt; for running inference of our models &lt;strong&gt;remotely&lt;/strong&gt;: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test inference without a robot&lt;/strong&gt;: We provide a &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;script&lt;/a&gt; for testing inference without a robot. This script will generate a random observation and run inference with the model. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fine-Tuning Base Models on Your Own Data&lt;/h2&gt; 
&lt;p&gt;We will fine-tune the $\pi_{0.5}$ model on the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO dataset&lt;/a&gt; as a running example for how to fine-tune a base model on your own data. We will explain three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Convert your data to a LeRobot dataset (which we use for training)&lt;/li&gt; 
 &lt;li&gt;Defining training configs and running training&lt;/li&gt; 
 &lt;li&gt;Spinning up a policy server and running inference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. Convert your data to a LeRobot dataset&lt;/h3&gt; 
&lt;p&gt;We provide a minimal example script for converting LIBERO data to a LeRobot dataset in &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/convert_libero_data_to_lerobot.py"&gt;&lt;code&gt;examples/libero/convert_libero_data_to_lerobot.py&lt;/code&gt;&lt;/a&gt;. You can easily modify it to convert your own data! You can download the raw LIBERO dataset from &lt;a href="https://huggingface.co/datasets/openvla/modified_libero_rlds"&gt;here&lt;/a&gt;, and run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.&lt;/p&gt; 
&lt;h3&gt;2. Defining training configs and running training&lt;/h3&gt; 
&lt;p&gt;To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/policies/libero_policy.py"&gt;&lt;code&gt;LiberoInputs&lt;/code&gt; and &lt;code&gt;LiberoOutputs&lt;/code&gt;&lt;/a&gt;: Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;LeRobotLiberoDataConfig&lt;/code&gt;&lt;/a&gt;: Defines how to process raw LIBERO data from LeRobot dataset for training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;TrainConfig&lt;/code&gt;&lt;/a&gt;: Defines fine-tuning hyperparameters, data config, and weight loader.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We provide example fine-tuning configs for &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€-FAST&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€.â‚…&lt;/a&gt; on LIBERO data.&lt;/p&gt; 
&lt;p&gt;Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/compute_norm_stats.py --config-name pi05_libero
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now we can kick off training with the following command (the &lt;code&gt;--overwrite&lt;/code&gt; flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command will log training progress to the console and save checkpoints to the &lt;code&gt;checkpoints&lt;/code&gt; directory. You can also monitor training progress on the Weights &amp;amp; Biases dashboard. For maximally using the GPU memory, set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We provide functionality for &lt;em&gt;reloading&lt;/em&gt; normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/norm_stats.md"&gt;norm_stats.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;3. Spinning up a policy server and running inference&lt;/h3&gt; 
&lt;p&gt;Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.&lt;/p&gt; 
&lt;p&gt;For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;remote inference docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;More Examples&lt;/h3&gt; 
&lt;p&gt;We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_sim"&gt;ALOHA Simulator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real"&gt;ALOHA Real&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/ur5"&gt;UR5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PyTorch Support&lt;/h2&gt; 
&lt;p&gt;openpi now provides PyTorch implementations of Ï€â‚€ and Ï€â‚€.â‚… models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The Ï€â‚€-FAST model&lt;/li&gt; 
 &lt;li&gt;Mixed precision training&lt;/li&gt; 
 &lt;li&gt;FSDP (fully-sharded data parallelism) training&lt;/li&gt; 
 &lt;li&gt;LoRA (low-rank adaptation) training&lt;/li&gt; 
 &lt;li&gt;EMA (exponential moving average) weights during training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have the latest version of all dependencies installed: &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double check that you have transformers 4.53.2 installed: &lt;code&gt;uv pip show transformers&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Apply the transformers library patches:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run &lt;code&gt;uv cache clean transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Converting JAX Models to PyTorch&lt;/h3&gt; 
&lt;p&gt;To convert a JAX model checkpoint to PyTorch format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &amp;lt;config name&amp;gt; \
    --output_path /path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Inference with PyTorch&lt;/h3&gt; 
&lt;p&gt;The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = "/path/to/converted/pytorch/checkpoint"

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Policy Server with PyTorch&lt;/h3&gt; 
&lt;p&gt;The policy server works identically with PyTorch models - just point to the converted checkpoint directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Finetuning with PyTorch&lt;/h3&gt; 
&lt;p&gt;To finetune a model in PyTorch:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Convert the JAX base model to PyTorch format:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --config_name &amp;lt;config name&amp;gt; \
    --checkpoint_dir /path/to/jax/base/model \
    --output_path /path/to/pytorch/base/model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the converted PyTorch model path in your config using &lt;code&gt;pytorch_weight_path&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch training using one of these modes:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training:
uv run scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&amp;lt;num_gpus&amp;gt; scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&amp;lt;num_nodes&amp;gt; \
    --nproc_per_node=&amp;lt;gpus_per_node&amp;gt; \
    --node_rank=&amp;lt;rank_of_node&amp;gt; \
    --master_addr=&amp;lt;master_ip&amp;gt; \
    --master_port=&amp;lt;port&amp;gt; \
    scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name=&amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Precision Settings&lt;/h3&gt; 
&lt;p&gt;JAX and PyTorch implementations handle precision as follows:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JAX:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: most weights and computations in bfloat16, with a few computations in float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting &lt;code&gt;dtype&lt;/code&gt; to float32 in the config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: supports either full bfloat16 (default) or full float32. You can change it by setting &lt;code&gt;pytorch_training_precision&lt;/code&gt; in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With torch.compile, inference speed is comparable between JAX and PyTorch.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can't find a solution, please file an issue on the repo (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/CONTRIBUTING.md"&gt;here&lt;/a&gt; for guidelines).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Issue&lt;/th&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uv sync&lt;/code&gt; fails with dependency conflicts&lt;/td&gt; 
   &lt;td&gt;Try removing the virtual environment directory (&lt;code&gt;rm -rf .venv&lt;/code&gt;) and running &lt;code&gt;uv sync&lt;/code&gt; again. If issues persist, check that you have the latest version of &lt;code&gt;uv&lt;/code&gt; installed (&lt;code&gt;uv self update&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training runs out of GPU memory&lt;/td&gt; 
   &lt;td&gt;Make sure you set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; (or higher) before running training to allow JAX to use more GPU memory. You can also use &lt;code&gt;--fsdp-devices &amp;lt;n&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;n&amp;gt;&lt;/code&gt; is your number of GPUs, to enable &lt;a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/"&gt;fully-sharded data parallelism&lt;/a&gt;, which reduces memory usage in exchange for slower training (the amount of slowdown depends on your particular setup). If you are still running out of memory, you may way to consider disabling EMA.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Policy server connection errors&lt;/td&gt; 
   &lt;td&gt;Check that the server is running and listening on the expected port. Verify network connectivity and firewall settings between client and server.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Missing norm stats error when training&lt;/td&gt; 
   &lt;td&gt;Run &lt;code&gt;scripts/compute_norm_stats.py&lt;/code&gt; with your config name before starting training.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset download fails&lt;/td&gt; 
   &lt;td&gt;Check your internet connection. For HuggingFace datasets, ensure you're logged in (&lt;code&gt;huggingface-cli login&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA/GPU errors&lt;/td&gt; 
   &lt;td&gt;Verify NVIDIA drivers are installed correctly. For Docker, ensure nvidia-container-toolkit is installed. Check GPU compatibility. You do NOT need CUDA libraries installed at a system level --- they will be installed via uv. You may even want to try &lt;em&gt;uninstalling&lt;/em&gt; system CUDA libraries if you run into CUDA issues, since system libraries can sometimes cause conflicts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Import errors when running examples&lt;/td&gt; 
   &lt;td&gt;Make sure you've installed all dependencies with &lt;code&gt;uv sync&lt;/code&gt;. Some examples may have additional requirements listed in their READMEs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Action dimensions mismatch&lt;/td&gt; 
   &lt;td&gt;Verify your data processing transforms match the expected input/output dimensions of your robot. Check the action space definitions in your policy classes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diverging training loss&lt;/td&gt; 
   &lt;td&gt;Check the &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, and &lt;code&gt;std&lt;/code&gt; values in &lt;code&gt;norm_stats.json&lt;/code&gt; for your dataset. Certain dimensions that are rarely used can end up with very small &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; values, leading to huge states and actions after normalization. You can manually adjust the norm stats as a workaround.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>opengeos/geoai</title>
      <link>https://github.com/opengeos/geoai</link>
      <description>&lt;p&gt;GeoAI: Artificial Intelligence for Geospatial Data&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GeoAI: Artificial Intelligence for Geospatial Data&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.python.org/pypi/geoai-py"&gt;&lt;img src="https://img.shields.io/pypi/v/geoai-py.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/geoai-py"&gt;&lt;img src="https://static.pepy.tech/badge/geoai-py" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/geoai"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/geoai.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/geoai"&gt;&lt;img src="https://img.shields.io/conda/dn/conda-forge/geoai.svg?sanitize=true" alt="Conda Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/conda-forge/geoai-py-feedstock"&gt;&lt;img src="https://img.shields.io/badge/recipe-geoai-green.svg?sanitize=true" alt="Conda Recipe" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://tinyurl.com/GeoAI-Tutorials"&gt;&lt;img src="https://img.shields.io/badge/YouTube-Tutorials-red" alt="image" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/opengeos/geoai/raw/master/docs/assets/logo.png"&gt;&lt;img src="https://raw.githubusercontent.com/opengeos/geoai/master/docs/assets/logo_rect.png" alt="logo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A powerful Python package for integrating artificial intelligence with geospatial data analysis and visualization&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“– Introduction&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://opengeoai.org"&gt;GeoAI&lt;/a&gt; is a comprehensive Python package designed to bridge artificial intelligence (AI) and geospatial data analysis, providing researchers and practitioners with intuitive tools for applying machine learning techniques to geographic data. The package offers a unified framework for processing satellite imagery, aerial photographs, and vector data using state-of-the-art deep learning models. GeoAI integrates popular AI frameworks including &lt;a href="https://pytorch.org"&gt;PyTorch&lt;/a&gt;, &lt;a href="https://github.com/huggingface/transformers"&gt;Transformers&lt;/a&gt;, &lt;a href="https://github.com/qubvel-org/segmentation_models.pytorch"&gt;PyTorch Segmentation Models&lt;/a&gt;, and specialized geospatial libraries like &lt;a href="https://github.com/Z-Zheng/pytorch-change-models"&gt;torchange&lt;/a&gt;, enabling users to perform complex geospatial analyses with minimal code.&lt;/p&gt; 
&lt;p&gt;The package provides five core capabilities:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Interactive and programmatic search and download of remote sensing imagery and geospatial data.&lt;/li&gt; 
 &lt;li&gt;Automated dataset preparation with image chips and label generation.&lt;/li&gt; 
 &lt;li&gt;Model training for tasks such as classification, detection, and segmentation.&lt;/li&gt; 
 &lt;li&gt;Inference pipelines for applying models to new geospatial datasets.&lt;/li&gt; 
 &lt;li&gt;Interactive visualization through integration with &lt;a href="https://github.com/opengeos/leafmap/"&gt;Leafmap&lt;/a&gt; and &lt;a href="https://github.com/eoda-dev/py-maplibregl"&gt;MapLibre&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;GeoAI addresses the growing demand for accessible AI tools in geospatial research by providing high-level APIs that abstract complex machine learning workflows while maintaining flexibility for advanced users. The package supports multiple data formats (GeoTIFF, JPEG2000,GeoJSON, Shapefile, GeoPackage) and includes automatic device management for GPU acceleration when available. With over 10 modules and extensive notebook examples, GeoAI serves as both a research tool and educational resource for the geospatial AI community.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Statement of Need&lt;/h2&gt; 
&lt;p&gt;The integration of artificial intelligence with geospatial data analysis has become increasingly critical across numerous scientific disciplines, from environmental monitoring and urban planning to disaster response and climate research. However, applying AI techniques to geospatial data presents unique challenges including data preprocessing complexities, specialized model architectures, and the need for domain-specific knowledge in both machine learning and geographic information systems.&lt;/p&gt; 
&lt;p&gt;Existing solutions often require researchers to navigate fragmented ecosystems of tools, combining general-purpose machine learning libraries with specialized geospatial packages, leading to steep learning curves and reproducibility challenges. While packages like TorchGeo and TerraTorch provide excellent foundational tools for geospatial deep learning, there remains a gap for comprehensive, high-level interfaces that can democratize access to advanced AI techniques for the broader geospatial community.&lt;/p&gt; 
&lt;p&gt;GeoAI addresses this need by providing a unified, user-friendly interface that abstracts the complexity of integrating multiple AI frameworks with geospatial data processing workflows. It lowers barriers for: (1) geospatial researchers who need accessible AI workflows without deep ML expertise; (2) AI practitioners who want streamlined geospatial preprocessing and domain-specific datasets; and (3) educators seeking reproducible examples and teaching-ready workflows.&lt;/p&gt; 
&lt;p&gt;The package's design philosophy emphasizes simplicity without sacrificing functionality, enabling users to perform sophisticated analyses such as building footprint extraction from satellite imagery, land cover classification, and change detection with just a few lines of code. By integrating cutting-edge AI models and providing seamless access to major geospatial data sources, GeoAI significantly lowers the barrier to entry for geospatial AI applications while maintaining the flexibility needed for advanced research applications.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Key Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ“Š Advanced Geospatial Data Visualization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interactive multi-layer visualization of vector and raster data stored locally or in cloud storage&lt;/li&gt; 
 &lt;li&gt;Customizable styling and symbology&lt;/li&gt; 
 &lt;li&gt;Time-series data visualization capabilities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ› ï¸ Data Preparation &amp;amp; Processing&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Streamlined access to satellite and aerial imagery from providers like Sentinel, Landsat, NAIP, and other open datasets&lt;/li&gt; 
 &lt;li&gt;Tools for downloading, mosaicking, and preprocessing remote sensing data&lt;/li&gt; 
 &lt;li&gt;Automated generation of training datasets with image chips and corresponding labels&lt;/li&gt; 
 &lt;li&gt;Vector-to-raster and raster-to-vector conversion utilities optimized for AI workflows&lt;/li&gt; 
 &lt;li&gt;Data augmentation techniques specific to geospatial data&lt;/li&gt; 
 &lt;li&gt;Support for integrating Overture Maps data and other open datasets for training and validation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ–¼ï¸ Image Segmentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integration with &lt;a href="https://github.com/qubvel-org/segmentation_models.pytorch"&gt;PyTorch Segmentation Models&lt;/a&gt; for automatic feature extraction&lt;/li&gt; 
 &lt;li&gt;Specialized segmentation algorithms optimized for satellite and aerial imagery&lt;/li&gt; 
 &lt;li&gt;Streamlined workflows for segmenting buildings, water bodies, wetlands,solar panels, etc.&lt;/li&gt; 
 &lt;li&gt;Export capabilities to standard geospatial formats (GeoJSON, Shapefile, GeoPackage, GeoParquet)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ” Image Classification&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pre-trained models for land cover and land use classification&lt;/li&gt; 
 &lt;li&gt;Transfer learning utilities for fine-tuning models with your own data&lt;/li&gt; 
 &lt;li&gt;Multi-temporal classification support for change detection&lt;/li&gt; 
 &lt;li&gt;Accuracy assessment and validation tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸŒ Additional Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Change detection with AI-enhanced feature extraction&lt;/li&gt; 
 &lt;li&gt;Object detection in aerial and satellite imagery&lt;/li&gt; 
 &lt;li&gt;Georeferencing utilities for AI model outputs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“¦ Installation&lt;/h2&gt; 
&lt;h3&gt;Using pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install geoai-py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using conda&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda install -c conda-forge geoai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using mamba&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mamba install -c conda-forge geoai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ“‹ Documentation&lt;/h2&gt; 
&lt;p&gt;Comprehensive documentation is available at &lt;a href="https://opengeoai.org"&gt;https://opengeoai.org&lt;/a&gt;, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detailed API reference&lt;/li&gt; 
 &lt;li&gt;Tutorials and example notebooks&lt;/li&gt; 
 &lt;li&gt;Contributing guide&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“º&amp;nbsp;Video Tutorials&lt;/h2&gt; 
&lt;p&gt;Check out this 2-hour video tutorial on using GeoAI for geospatial data analysis and visualization.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/jdK-cleFUkc"&gt;&lt;img src="https://github.com/user-attachments/assets/1c14e651-65b9-41ae-b42d-3ad028b3eeb8" alt="cover" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To learn more about GeoAI, you can watch the following video tutorials:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://tinyurl.com/GeoAI-Tutorials"&gt;&lt;img src="https://github.com/user-attachments/assets/3cde9547-ab62-4d70-b23a-3e5ed27c7407" alt="cover" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of all kinds! See our &lt;a href="https://opengeoai.org/contributing"&gt;contributing guide&lt;/a&gt; for ways to get started.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;GeoAI is free and open source software, licensed under the MIT License.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We gratefully acknowledge the support of the following organizations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.nasa.gov"&gt;NASA&lt;/a&gt;: This research is partially supported by the National Aeronautics and Space Administration (NASA) through Grant No. 80NSSC22K1742, awarded under the &lt;a href="https://bit.ly/3RVBRcQ"&gt;Open Source Tools, Frameworks, and Libraries Program&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://americaview.org"&gt;AmericaView&lt;/a&gt;: This work is also partially supported by the U.S. Geological Survey through Grant/Cooperative Agreement No. G23AP00683 (GY23-GY27) in collaboration with AmericaView.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>