<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Sun, 10 Aug 2025 05:55:25 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices ğŸ“±ğŸ’» ğŸ–¥ï¸âŒš&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://discord.gg/EUnjGpsmWw"&gt;Discord&lt;/a&gt; | &lt;a href="https://t.me/+Kh-KqHTzFYg3MGNk"&gt;Telegram&lt;/a&gt; | &lt;a href="https://x.com/exolabs"&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main"&gt;&lt;img src="https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License: GPL v3" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Update: exo is hiring. See &lt;a href="https://exolabs.net"&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; 
 &lt;h2&gt;Interested in running exo in your business? &lt;a href="mailto:hello@exolabs.net"&gt;Contact us&lt;/a&gt; to discuss.&lt;/h2&gt; 
&lt;/div&gt; 
&lt;h2&gt;Get Involved&lt;/h2&gt; 
&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; 
&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href="https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing"&gt;this sheet&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Wide Model Support&lt;/h3&gt; 
&lt;p&gt;exo supports different models including LLaMA (&lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py"&gt;MLX&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py"&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen, and Deepseek.&lt;/p&gt; 
&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; 
&lt;p&gt;exo &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; 
&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; 
&lt;p&gt;exo will &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154"&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; 
&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; 
&lt;p&gt;exo provides a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py"&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It's a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh"&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; 
&lt;h3&gt;Device Equality&lt;/h3&gt; 
&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161"&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; 
&lt;p&gt;Exo supports different &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py"&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-screenshot.jpg" alt="&amp;quot;A screenshot of exo running 5 nodes" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href="https://github.com/exo-explore/exo/issues/5"&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; 
 &lt;li&gt;For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA): 
  &lt;ul&gt; 
   &lt;li&gt;NVIDIA driver - verify with &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;CUDA toolkit - install from &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation"&gt;NVIDIA CUDA guide&lt;/a&gt;, verify with &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cuDNN library - download from &lt;a href="https://developer.nvidia.com/cudnn-downloads"&gt;NVIDIA cuDNN page&lt;/a&gt;, verify installation by following &lt;a href="https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older"&gt;these steps&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: 
  &lt;ul&gt; 
   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; 
   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; 
   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/exo-explore/exo.git
cd exo
pip install -e .
# alternatively, with venv
source install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If running on Mac, MLX has an &lt;a href="https://ml-explore.github.io/mlx/build/html/install.html"&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upgrade to the latest version of macOS Sequoia.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Example Usage on Multiple macOS Devices&lt;/h3&gt; 
&lt;h4&gt;Device 1:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Device 2:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; 
&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href="https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat"&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href="http://localhost:52415"&gt;http://localhost:52415&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href="http://localhost:52415/v1/chat/completions"&gt;http://localhost:52415/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; 
&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.2-3b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.1-405b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;DeepSeek R1 (full 671B):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "deepseek-r1",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llava-1.5-7b-hf",
     "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What are these?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "http://images.cocodataset.org/val2017/000000039769.jpg"
            }
          }
        ]
      }
    ],
     "temperature": 0.0
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (macOS + Linux)&lt;/h3&gt; 
&lt;h4&gt;Device 1 (macOS):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: We don't need to explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine. &lt;strong&gt;MLX&lt;/strong&gt; and &lt;strong&gt;tinygrad&lt;/strong&gt; are interoperable!&lt;/p&gt; 
&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; 
&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href="https://docs.tinygrad.org/env_vars/"&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage on a single device with "exo run" command&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With a custom prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b --prompt "What is the meaning of exo?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Storage&lt;/h3&gt; 
&lt;p&gt;Models by default are stored in &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can set a different model storage location by setting the &lt;code&gt;EXO_HOME&lt;/code&gt; env var.&lt;/p&gt; 
&lt;h2&gt;Model Downloading&lt;/h2&gt; 
&lt;p&gt;Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;To download models from a proxy endpoint, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable. For example, to run exo with the huggingface mirror endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;HF_ENDPOINT=https://hf-mirror.com exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;DEBUG=9 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;TINYGRAD_DEBUG=2 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Formatting&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; to format the code. To format the code, first install the formatting requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install -e '.[formatting]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the formatting script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python3 format.py ./exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the &lt;code&gt;Install Certificates&lt;/code&gt; command, typicall as follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;/Applications/Python 3.x/Install Certificates.command
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸš§ As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email &lt;a href="mailto:alex@exolabs.net"&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Inference Engines&lt;/h2&gt; 
&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py"&gt;MLX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py"&gt;tinygrad&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš§ &lt;a href="https://github.com/exo-explore/exo/pull/139"&gt;PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš§ &lt;a href="https://github.com/exo-explore/exo/issues/167"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discovery Modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/udp"&gt;UDP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/manual"&gt;Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/tailscale"&gt;Tailscale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš§ Radio&lt;/li&gt; 
 &lt;li&gt;ğŸš§ Bluetooth&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Peer Networking Modules&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc"&gt;GRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš§ NCCL&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>nomic-ai/gpt4all</title>
      <link>https://github.com/nomic-ai/gpt4all</link>
      <description>&lt;p&gt;GPT4All: Run Local LLMs on Any Device. Open-source and available for commercial use.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;GPT4All&lt;/h1&gt; 
&lt;p align="center"&gt; Now with support for DeepSeek R1 Distillations &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.nomic.ai/gpt4all"&gt;Website&lt;/a&gt; â€¢ &lt;a href="https://docs.gpt4all.io"&gt;Documentation&lt;/a&gt; â€¢ &lt;a href="https://discord.gg/mGZE39AS3e"&gt;Discord&lt;/a&gt; â€¢ &lt;a href="https://www.youtube.com/watch?v=gQcZDXRVJok"&gt;YouTube Tutorial&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; GPT4All runs large language models (LLMs) privately on everyday desktops &amp;amp; laptops. &lt;/p&gt; 
&lt;p align="center"&gt; No API calls or GPUs required - you can just download the application and &lt;a href="https://docs.gpt4all.io/gpt4all_desktop/quickstart.html#quickstart"&gt;get started&lt;/a&gt;. &lt;/p&gt; 
&lt;p align="center"&gt; Read about what's new in &lt;a href="https://www.nomic.ai/blog/tag/gpt4all"&gt;our blog&lt;/a&gt;. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://nomic.ai/gpt4all/#newsletter-form"&gt;Subscribe to the newsletter&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/nomic-ai/gpt4all/assets/70534565/513a0f15-4964-4109-89e4-4f9a9011f311"&gt;https://github.com/nomic-ai/gpt4all/assets/70534565/513a0f15-4964-4109-89e4-4f9a9011f311&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; GPT4All is made possible by our compute partner &lt;a href="https://www.paperspace.com/"&gt;Paperspace&lt;/a&gt;. &lt;/p&gt; 
&lt;h2&gt;Download Links&lt;/h2&gt; 
&lt;p&gt; â€” &lt;a href="https://gpt4all.io/installers/gpt4all-installer-win64.exe"&gt; &lt;img src="https://raw.githubusercontent.com/nomic-ai/gpt4all/main/gpt4all-bindings/python/docs/assets/windows.png" style="height: 1em; width: auto" /&gt; Windows Installer &lt;/a&gt; â€” &lt;/p&gt; 
&lt;p&gt; â€” &lt;a href="https://gpt4all.io/installers/gpt4all-installer-win64-arm.exe"&gt; &lt;img src="https://raw.githubusercontent.com/nomic-ai/gpt4all/main/gpt4all-bindings/python/docs/assets/windows.png" style="height: 1em; width: auto" /&gt; Windows ARM Installer &lt;/a&gt; â€” &lt;/p&gt; 
&lt;p&gt; â€” &lt;a href="https://gpt4all.io/installers/gpt4all-installer-darwin.dmg"&gt; &lt;img src="https://raw.githubusercontent.com/nomic-ai/gpt4all/main/gpt4all-bindings/python/docs/assets/mac.png" style="height: 1em; width: auto" /&gt; macOS Installer &lt;/a&gt; â€” &lt;/p&gt; 
&lt;p&gt; â€” &lt;a href="https://gpt4all.io/installers/gpt4all-installer-linux.run"&gt; &lt;img src="https://raw.githubusercontent.com/nomic-ai/gpt4all/main/gpt4all-bindings/python/docs/assets/ubuntu.svg?sanitize=true" style="height: 1em; width: auto" /&gt; Ubuntu Installer &lt;/a&gt; â€” &lt;/p&gt; 
&lt;p&gt; The Windows and Linux builds require Intel Core i3 2nd Gen / AMD Bulldozer, or better. &lt;/p&gt; 
&lt;p&gt; The Windows ARM build supports Qualcomm Snapdragon and Microsoft SQ1/SQ2 processors. &lt;/p&gt; 
&lt;p&gt; The Linux build is x86-64 only (no ARM). &lt;/p&gt; 
&lt;p&gt; The macOS build requires Monterey 12.6 or newer. Best results with Apple Silicon M-series processors. &lt;/p&gt; 
&lt;p&gt;See the full &lt;a href="https://raw.githubusercontent.com/nomic-ai/gpt4all/main/gpt4all-chat/system_requirements.md"&gt;System Requirements&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt; &lt;a href="https://flathub.org/apps/io.gpt4all.gpt4all"&gt; &lt;img style="height: 2em; width: auto" alt="Get it on Flathub" src="https://flathub.org/api/badge" /&gt;&lt;br /&gt; Flathub (community maintained) &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Install GPT4All Python&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;gpt4all&lt;/code&gt; gives you access to LLMs with our Python client around &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; implementations.&lt;/p&gt; 
&lt;p&gt;Nomic contributes to open source software like &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; to make LLMs accessible and efficient &lt;strong&gt;for all&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install gpt4all
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from gpt4all import GPT4All
model = GPT4All("Meta-Llama-3-8B-Instruct.Q4_0.gguf") # downloads / loads a 4.66GB LLM
with model.chat_session():
    print(model.generate("How can I run LLMs efficiently on my laptop?", max_tokens=1024))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;&lt;span&gt;ğŸ¦œ&lt;/span&gt;&lt;span&gt;ğŸ”—&lt;/span&gt; &lt;a href="https://python.langchain.com/v0.2/docs/integrations/providers/gpt4all/"&gt;Langchain&lt;/a&gt; &lt;span&gt;ğŸ—ƒ&lt;/span&gt; &lt;a href="https://github.com/weaviate/weaviate"&gt;Weaviate Vector Database&lt;/a&gt; - &lt;a href="https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-gpt4all"&gt;module docs&lt;/a&gt; &lt;span&gt;ğŸ”­&lt;/span&gt; &lt;a href="https://github.com/openlit/openlit"&gt;OpenLIT (OTel-native Monitoring)&lt;/a&gt; - &lt;a href="https://docs.openlit.io/latest/integrations/gpt4all"&gt;Docs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release History&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;July 2nd, 2024&lt;/strong&gt;: V3.0.0 Release 
  &lt;ul&gt; 
   &lt;li&gt;Fresh redesign of the chat application UI&lt;/li&gt; 
   &lt;li&gt;Improved user workflow for LocalDocs&lt;/li&gt; 
   &lt;li&gt;Expanded access to more model architectures&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;October 19th, 2023&lt;/strong&gt;: GGUF Support Launches with Support for: 
  &lt;ul&gt; 
   &lt;li&gt;Mistral 7b base model, an updated model gallery on our website, several new local code models including Rift Coder v1.5&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://blog.nomic.ai/posts/gpt4all-gpu-inference-with-vulkan"&gt;Nomic Vulkan&lt;/a&gt; support for Q4_0 and Q4_1 quantizations in GGUF.&lt;/li&gt; 
   &lt;li&gt;Offline build support for running old versions of the GPT4All Local LLM Chat Client.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;September 18th, 2023&lt;/strong&gt;: &lt;a href="https://blog.nomic.ai/posts/gpt4all-gpu-inference-with-vulkan"&gt;Nomic Vulkan&lt;/a&gt; launches supporting local LLM inference on NVIDIA and AMD GPUs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;July 2023&lt;/strong&gt;: Stable support for LocalDocs, a feature that allows you to privately and locally chat with your data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;June 28th, 2023&lt;/strong&gt;: &lt;a href="https://github.com/nomic-ai/gpt4all/tree/cef74c2be20f5b697055d5b8b506861c7b997fab/gpt4all-api"&gt;Docker-based API server&lt;/a&gt; launches allowing inference of local LLMs from an OpenAI-compatible HTTP endpoint.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;GPT4All welcomes contributions, involvement, and discussion from the open source community! Please see CONTRIBUTING.md and follow the issues, bug reports, and PR markdown templates.&lt;/p&gt; 
&lt;p&gt;Check project discord, with project owners, or through existing issues/PRs to avoid duplicate work. Please make sure to tag all of the above with relevant project identifiers or your contribution could potentially get lost. Example tags: &lt;code&gt;backend&lt;/code&gt;, &lt;code&gt;bindings&lt;/code&gt;, &lt;code&gt;python-bindings&lt;/code&gt;, &lt;code&gt;documentation&lt;/code&gt;, etc.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you utilize this repository, models or data in a downstream project, please consider citing it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{gpt4all,
  author = {Yuvanesh Anand and Zach Nussbaum and Brandon Duderstadt and Benjamin Schmidt and Andriy Mulyar},
  title = {GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/nomic-ai/gpt4all}},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>steveiliop56/tinyauth</title>
      <link>https://github.com/steveiliop56/tinyauth</link>
      <description>&lt;p&gt;The simplest way to protect your apps with a login screen.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img alt="Tinyauth" title="Tinyauth" width="96" src="https://raw.githubusercontent.com/steveiliop56/tinyauth/main/assets/logo-rounded.png" /&gt; 
 &lt;h1&gt;Tinyauth&lt;/h1&gt; 
 &lt;p&gt;The easiest way to secure your apps with a login screen.&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img alt="License" src="https://img.shields.io/github/license/steveiliop56/tinyauth" /&gt; 
 &lt;img alt="Release" src="https://img.shields.io/github/v/release/steveiliop56/tinyauth" /&gt; 
 &lt;img alt="Issues" src="https://img.shields.io/github/issues/steveiliop56/tinyauth" /&gt; 
 &lt;img alt="Tinyauth CI" src="https://github.com/steveiliop56/tinyauth/actions/workflows/ci.yml/badge.svg?sanitize=true" /&gt; 
 &lt;a title="Crowdin" target="_blank" href="https://crowdin.com/project/tinyauth"&gt;&lt;img src="https://badges.crowdin.net/tinyauth/localized.svg?sanitize=true" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;Tinyauth is a simple authentication middleware that adds a simple login screen or OAuth with Google, Github and any provider to all of your docker apps. It supports all the popular proxies like Traefik, Nginx and Caddy.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/steveiliop56/tinyauth/main/assets/screenshot.png" alt="Screenshot" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Tinyauth is in active development and configuration may change often. Please make sure to carefully read the release notes before updating.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;You can easily get started with Tinyauth by following the guide in the &lt;a href="https://tinyauth.app/docs/getting-started.html"&gt;documentation&lt;/a&gt;. There is also an available &lt;a href="https://raw.githubusercontent.com/steveiliop56/tinyauth/main/docker-compose.example.yml"&gt;docker compose&lt;/a&gt; file that has Traefik, Whoami and Tinyauth to demonstrate its capabilities.&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;If you are still not sure if Tinyauth suits your needs you can try out the &lt;a href="https://demo.tinyauth.app"&gt;demo&lt;/a&gt;. The default username is &lt;code&gt;user&lt;/code&gt; and the default password is &lt;code&gt;password&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;You can find documentation and guides on all of the available configuration of Tinyauth in the &lt;a href="https://tinyauth.app"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Discord&lt;/h2&gt; 
&lt;p&gt;Tinyauth has a &lt;a href="https://discord.gg/eHzVaCzRRd"&gt;discord&lt;/a&gt; server. Feel free to hop in to chat about self-hosting, homelabs and of course Tinyauth. See you there!&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;All contributions to the codebase are welcome! If you have any free time feel free to pick up an &lt;a href="https://github.com/steveiliop56/tinyauth/issues"&gt;issue&lt;/a&gt; or add your own missing features. Make sure to check out the &lt;a href="https://raw.githubusercontent.com/steveiliop56/tinyauth/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for instructions on how to get the development server up and running.&lt;/p&gt; 
&lt;h2&gt;Localization&lt;/h2&gt; 
&lt;p&gt;If you would like to help translate Tinyauth into more languages, visit the &lt;a href="https://crowdin.com/project/tinyauth"&gt;Crowdin&lt;/a&gt; page.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Tinyauth is licensed under the GNU General Public License v3.0. TL;DR â€” You may copy, distribute and modify the software as long as you track changes/dates in source files. Any modifications to or software including (via compiler) GPL-licensed code must also be made available under the GPL along with build &amp;amp; install instructions. For more information about the license check the &lt;a href="https://raw.githubusercontent.com/steveiliop56/tinyauth/main/LICENSE"&gt;license&lt;/a&gt; file.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;A big thank you to the following people for providing me with more coffee:&lt;/p&gt; 
&lt;!-- sponsors --&gt;
&lt;a href="https://github.com/erwinkramer"&gt;&lt;img src="https://github.com/erwinkramer.png" width="64px" alt="User avatar: erwinkramer" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/nicotsx"&gt;&lt;img src="https://github.com/nicotsx.png" width="64px" alt="User avatar: nicotsx" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/SimpleHomelab"&gt;&lt;img src="https://github.com/SimpleHomelab.png" width="64px" alt="User avatar: SimpleHomelab" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/jmadden91"&gt;&lt;img src="https://github.com/jmadden91.png" width="64px" alt="User avatar: jmadden91" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/tribor"&gt;&lt;img src="https://github.com/tribor.png" width="64px" alt="User avatar: tribor" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/eliasbenb"&gt;&lt;img src="https://github.com/eliasbenb.png" width="64px" alt="User avatar: eliasbenb" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/afunworm"&gt;&lt;img src="https://github.com/afunworm.png" width="64px" alt="User avatar: afunworm" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;!-- sponsors --&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Freepik&lt;/strong&gt; for providing the police hat and badge.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Renee French&lt;/strong&gt; for the original gopher logo.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Coderabbit AI&lt;/strong&gt; for providing free AI code reviews.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Syrhu&lt;/strong&gt; for providing the background image of the app.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#steveiliop56/tinyauth&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=steveiliop56/tinyauth&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/self-llm</title>
      <link>https://github.com/datawhalechina/self-llm</link>
      <description>&lt;p&gt;ã€Šå¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å—ã€‹é’ˆå¯¹ä¸­å›½å®å®é‡èº«æ‰“é€ çš„åŸºäºLinuxç¯å¢ƒå¿«é€Ÿå¾®è°ƒï¼ˆå…¨å‚æ•°/Loraï¼‰ã€éƒ¨ç½²å›½å†…å¤–å¼€æºå¤§æ¨¡å‹ï¼ˆLLMï¼‰/å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆMLLMï¼‰æ•™ç¨‹&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/head-img.png" /&gt; 
 &lt;h1&gt;å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å—&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;ä¸­æ–‡ | &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå›´ç»•å¼€æºå¤§æ¨¡å‹ã€é’ˆå¯¹å›½å†…åˆå­¦è€…ã€åŸºäº Linux å¹³å°çš„ä¸­å›½å®å®ä¸“å±å¤§æ¨¡å‹æ•™ç¨‹ï¼Œé’ˆå¯¹å„ç±»å¼€æºå¤§æ¨¡å‹æä¾›åŒ…æ‹¬ç¯å¢ƒé…ç½®ã€æœ¬åœ°éƒ¨ç½²ã€é«˜æ•ˆå¾®è°ƒç­‰æŠ€èƒ½åœ¨å†…çš„å…¨æµç¨‹æŒ‡å¯¼ï¼Œç®€åŒ–å¼€æºå¤§æ¨¡å‹çš„éƒ¨ç½²ã€ä½¿ç”¨å’Œåº”ç”¨æµç¨‹ï¼Œè®©æ›´å¤šçš„æ™®é€šå­¦ç”Ÿã€ç ”ç©¶è€…æ›´å¥½åœ°ä½¿ç”¨å¼€æºå¤§æ¨¡å‹ï¼Œå¸®åŠ©å¼€æºã€è‡ªç”±çš„å¤§æ¨¡å‹æ›´å¿«èå…¥åˆ°æ™®é€šå­¦ä¹ è€…çš„ç”Ÿæ´»ä¸­ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®çš„ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;åŸºäº Linux å¹³å°çš„å¼€æº LLM ç¯å¢ƒé…ç½®æŒ‡å—ï¼Œé’ˆå¯¹ä¸åŒæ¨¡å‹è¦æ±‚æä¾›ä¸åŒçš„è¯¦ç»†ç¯å¢ƒé…ç½®æ­¥éª¤ï¼›&lt;/li&gt; 
 &lt;li&gt;é’ˆå¯¹å›½å†…å¤–ä¸»æµå¼€æº LLM çš„éƒ¨ç½²ä½¿ç”¨æ•™ç¨‹ï¼ŒåŒ…æ‹¬ LLaMAã€ChatGLMã€InternLM ç­‰ï¼›&lt;/li&gt; 
 &lt;li&gt;å¼€æº LLM çš„éƒ¨ç½²åº”ç”¨æŒ‡å¯¼ï¼ŒåŒ…æ‹¬å‘½ä»¤è¡Œè°ƒç”¨ã€åœ¨çº¿ Demo éƒ¨ç½²ã€LangChain æ¡†æ¶é›†æˆç­‰ï¼›&lt;/li&gt; 
 &lt;li&gt;å¼€æº LLM çš„å…¨é‡å¾®è°ƒã€é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼å…¨é‡å¾®è°ƒã€LoRAã€ptuning ç­‰ã€‚&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;â€ƒâ€ƒ&lt;strong&gt;é¡¹ç›®çš„ä¸»è¦å†…å®¹å°±æ˜¯æ•™ç¨‹ï¼Œè®©æ›´å¤šçš„å­¦ç”Ÿå’Œæœªæ¥çš„ä»ä¸šè€…äº†è§£å’Œç†Ÿæ‚‰å¼€æºå¤§æ¨¡å‹çš„é£Ÿç”¨æ–¹æ³•ï¼ä»»ä½•äººéƒ½å¯ä»¥æå‡ºissueæˆ–æ˜¯æäº¤PRï¼Œå…±åŒæ„å»ºç»´æŠ¤è¿™ä¸ªé¡¹ç›®ã€‚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒæƒ³è¦æ·±åº¦å‚ä¸çš„åŒå­¦å¯ä»¥è”ç³»æˆ‘ä»¬ï¼Œæˆ‘ä»¬ä¼šå°†ä½ åŠ å…¥åˆ°é¡¹ç›®çš„ç»´æŠ¤è€…ä¸­ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â€ƒâ€ƒ&lt;em&gt;&lt;strong&gt;å­¦ä¹ å»ºè®®ï¼šæœ¬é¡¹ç›®çš„å­¦ä¹ å»ºè®®æ˜¯ï¼Œå…ˆå­¦ä¹ ç¯å¢ƒé…ç½®ï¼Œç„¶åå†å­¦ä¹ æ¨¡å‹çš„éƒ¨ç½²ä½¿ç”¨ï¼Œæœ€åå†å­¦ä¹ å¾®è°ƒã€‚å› ä¸ºç¯å¢ƒé…ç½®æ˜¯åŸºç¡€ï¼Œæ¨¡å‹çš„éƒ¨ç½²ä½¿ç”¨æ˜¯åŸºç¡€ï¼Œå¾®è°ƒæ˜¯è¿›é˜¶ã€‚åˆå­¦è€…å¯ä»¥é€‰æ‹©Qwen1.5ï¼ŒInternLM2ï¼ŒMiniCPMç­‰æ¨¡å‹ä¼˜å…ˆå­¦ä¹ ã€‚&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â€ƒâ€ƒ&lt;strong&gt;è¿›é˜¶å­¦ä¹ æ¨è&lt;/strong&gt; ï¼šå¦‚æœæ‚¨åœ¨å­¦ä¹ å®Œæœ¬é¡¹ç›®åï¼Œå¸Œæœ›æ›´æ·±å…¥åœ°ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåŸç†ï¼Œå¹¶æ¸´æœ›äº²æ‰‹ä»é›¶å¼€å§‹è®­ç»ƒå±äºè‡ªå·±çš„å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬å¼ºçƒˆæ¨èå…³æ³¨ Datawhale çš„å¦ä¸€ä¸ªå¼€æºé¡¹ç›®â€”â€” &lt;a href="https://github.com/datawhalechina/happy-llm"&gt;Happy-LLM ä»é›¶å¼€å§‹çš„å¤§è¯­è¨€æ¨¡å‹åŸç†ä¸å®è·µæ•™ç¨‹&lt;/a&gt; ã€‚è¯¥é¡¹ç›®å°†å¸¦æ‚¨æ·±å…¥æ¢ç´¢å¤§æ¨¡å‹çš„åº•å±‚æœºåˆ¶ï¼ŒæŒæ¡å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šå¦‚æœæœ‰åŒå­¦å¸Œæœ›äº†è§£å¤§æ¨¡å‹çš„æ¨¡å‹æ„æˆï¼Œä»¥åŠä»é›¶æ‰‹å†™RAGã€Agentå’ŒEvalç­‰ä»»åŠ¡ï¼Œå¯ä»¥å­¦ä¹ Datawhaleçš„å¦ä¸€ä¸ªé¡¹ç›®&lt;a href="https://github.com/datawhalechina/tiny-universe"&gt;Tiny-Universe&lt;/a&gt;ï¼Œå¤§æ¨¡å‹æ˜¯å½“ä¸‹æ·±åº¦å­¦ä¹ é¢†åŸŸçš„çƒ­ç‚¹ï¼Œä½†ç°æœ‰çš„å¤§éƒ¨åˆ†å¤§æ¨¡å‹æ•™ç¨‹åªåœ¨äºæ•™ç»™å¤§å®¶å¦‚ä½•è°ƒç”¨apiå®Œæˆå¤§æ¨¡å‹çš„åº”ç”¨ï¼Œè€Œå¾ˆå°‘æœ‰äººèƒ½å¤Ÿä»åŸç†å±‚é¢è®²æ¸…æ¥šæ¨¡å‹ç»“æ„ã€RAGã€Agent ä»¥åŠ Evalã€‚æ‰€ä»¥è¯¥ä»“åº“ä¼šæä¾›å…¨éƒ¨æ‰‹å†™ï¼Œä¸é‡‡ç”¨è°ƒç”¨apiçš„å½¢å¼ï¼Œå®Œæˆå¤§æ¨¡å‹çš„ RAG ã€ Agent ã€Eval ä»»åŠ¡ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šè€ƒè™‘åˆ°æœ‰åŒå­¦å¸Œæœ›åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œå¸Œæœ›å­¦ä¹ å¤§æ¨¡å‹çš„ç†è®ºéƒ¨åˆ†ï¼Œå¦‚æœæƒ³è¦è¿›ä¸€æ­¥æ·±å…¥å­¦ä¹  LLM çš„ç†è®ºåŸºç¡€ï¼Œå¹¶åœ¨ç†è®ºçš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥è®¤è¯†ã€åº”ç”¨ LLMï¼Œå¯ä»¥å‚è€ƒ Datawhale çš„ &lt;a href="https://github.com/datawhalechina/so-large-lm.git"&gt;so-large-llm&lt;/a&gt;è¯¾ç¨‹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šå¦‚æœæœ‰åŒå­¦åœ¨å­¦ä¹ æœ¬è¯¾ç¨‹ä¹‹åï¼Œæƒ³è¦è‡ªå·±åŠ¨æ‰‹å¼€å‘å¤§æ¨¡å‹åº”ç”¨ã€‚åŒå­¦ä»¬å¯ä»¥å‚è€ƒ Datawhale çš„ &lt;a href="https://github.com/datawhalechina/llm-universe"&gt;åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘&lt;/a&gt; è¯¾ç¨‹ï¼Œè¯¥é¡¹ç›®æ˜¯ä¸€ä¸ªé¢å‘å°ç™½å¼€å‘è€…çš„å¤§æ¨¡å‹åº”ç”¨å¼€å‘æ•™ç¨‹ï¼Œæ—¨åœ¨åŸºäºé˜¿é‡Œäº‘æœåŠ¡å™¨ï¼Œç»“åˆä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹é¡¹ç›®ï¼Œå‘åŒå­¦ä»¬å®Œæ•´çš„å‘ˆç°å¤§æ¨¡å‹åº”ç”¨å¼€å‘æµç¨‹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;é¡¹ç›®æ„ä¹‰&lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒä»€ä¹ˆæ˜¯å¤§æ¨¡å‹ï¼Ÿ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å¤§æ¨¡å‹ï¼ˆLLMï¼‰ç‹­ä¹‰ä¸ŠæŒ‡åŸºäºæ·±åº¦å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒçš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡å‹ï¼Œä¸»è¦åº”ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆç­‰é¢†åŸŸï¼Œå¹¿ä¹‰ä¸Šè¿˜åŒ…æ‹¬æœºå™¨è§†è§‰ï¼ˆCVï¼‰å¤§æ¨¡å‹ã€å¤šæ¨¡æ€å¤§æ¨¡å‹å’Œç§‘å­¦è®¡ç®—å¤§æ¨¡å‹ç­‰ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;â€ƒâ€ƒç™¾æ¨¡å¤§æˆ˜æ­£å€¼ç«çƒ­ï¼Œå¼€æº LLM å±‚å‡ºä¸ç©·ã€‚å¦‚ä»Šå›½å†…å¤–å·²ç»æ¶Œç°äº†ä¼—å¤šä¼˜ç§€å¼€æº LLMï¼Œå›½å¤–å¦‚ LLaMAã€Alpacaï¼Œå›½å†…å¦‚ ChatGLMã€BaiChuanã€InternLMï¼ˆä¹¦ç”ŸÂ·æµ¦è¯­ï¼‰ç­‰ã€‚å¼€æº LLM æ”¯æŒç”¨æˆ·æœ¬åœ°éƒ¨ç½²ã€ç§åŸŸå¾®è°ƒï¼Œæ¯ä¸€ä¸ªäººéƒ½å¯ä»¥åœ¨å¼€æº LLM çš„åŸºç¡€ä¸Šæ‰“é€ ä¸“å±äºè‡ªå·±çš„ç‹¬ç‰¹å¤§æ¨¡å‹ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒç„¶è€Œï¼Œå½“å‰æ™®é€šå­¦ç”Ÿå’Œç”¨æˆ·æƒ³è¦ä½¿ç”¨è¿™äº›å¤§æ¨¡å‹ï¼Œéœ€è¦å…·å¤‡ä¸€å®šçš„æŠ€æœ¯èƒ½åŠ›ï¼Œæ‰èƒ½å®Œæˆæ¨¡å‹çš„éƒ¨ç½²å’Œä½¿ç”¨ã€‚å¯¹äºå±‚å‡ºä¸ç©·åˆå„æœ‰ç‰¹è‰²çš„å¼€æº LLMï¼Œæƒ³è¦å¿«é€ŸæŒæ¡ä¸€ä¸ªå¼€æº LLM çš„åº”ç”¨æ–¹æ³•ï¼Œæ˜¯ä¸€é¡¹æ¯”è¾ƒæœ‰æŒ‘æˆ˜çš„ä»»åŠ¡ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®æ—¨åœ¨é¦–å…ˆåŸºäºæ ¸å¿ƒè´¡çŒ®è€…çš„ç»éªŒï¼Œå®ç°å›½å†…å¤–ä¸»æµå¼€æº LLM çš„éƒ¨ç½²ã€ä½¿ç”¨ä¸å¾®è°ƒæ•™ç¨‹ï¼›åœ¨å®ç°ä¸»æµ LLM çš„ç›¸å…³éƒ¨åˆ†ä¹‹åï¼Œæˆ‘ä»¬å¸Œæœ›å……åˆ†èšé›†å…±åˆ›è€…ï¼Œä¸€èµ·ä¸°å¯Œè¿™ä¸ªå¼€æº LLM çš„ä¸–ç•Œï¼Œæ‰“é€ æ›´å¤šã€æ›´å…¨é¢ç‰¹è‰² LLM çš„æ•™ç¨‹ã€‚æ˜Ÿç«ç‚¹ç‚¹ï¼Œæ±‡èšæˆæµ·ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒ&lt;em&gt;&lt;strong&gt;æˆ‘ä»¬å¸Œæœ›æˆä¸º LLM ä¸æ™®ç½—å¤§ä¼—çš„é˜¶æ¢¯ï¼Œä»¥è‡ªç”±ã€å¹³ç­‰çš„å¼€æºç²¾ç¥ï¼Œæ‹¥æŠ±æ›´æ¢å¼˜è€Œè¾½é˜”çš„ LLM ä¸–ç•Œã€‚&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;é¡¹ç›®å—ä¼—&lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®é€‚åˆä»¥ä¸‹å­¦ä¹ è€…ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;æƒ³è¦ä½¿ç”¨æˆ–ä½“éªŒ LLMï¼Œä½†æ— æ¡ä»¶è·å¾—æˆ–ä½¿ç”¨ç›¸å…³ APIï¼›&lt;/li&gt; 
 &lt;li&gt;å¸Œæœ›é•¿æœŸã€ä½æˆæœ¬ã€å¤§é‡åº”ç”¨ LLMï¼›&lt;/li&gt; 
 &lt;li&gt;å¯¹å¼€æº LLM æ„Ÿå…´è¶£ï¼Œæƒ³è¦äº²è‡ªä¸Šæ‰‹å¼€æº LLMï¼›&lt;/li&gt; 
 &lt;li&gt;NLP åœ¨å­¦ï¼Œå¸Œæœ›è¿›ä¸€æ­¥å­¦ä¹  LLMï¼›&lt;/li&gt; 
 &lt;li&gt;å¸Œæœ›ç»“åˆå¼€æº LLMï¼Œæ‰“é€ é¢†åŸŸç‰¹è‰²çš„ç§åŸŸ LLMï¼›&lt;/li&gt; 
 &lt;li&gt;ä»¥åŠæœ€å¹¿å¤§ã€æœ€æ™®é€šçš„å­¦ç”Ÿç¾¤ä½“ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;é¡¹ç›®è§„åˆ’åŠè¿›å±•&lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒ æœ¬é¡¹ç›®æ‹Ÿå›´ç»•å¼€æº LLM åº”ç”¨å…¨æµç¨‹ç»„ç»‡ï¼ŒåŒ…æ‹¬ç¯å¢ƒé…ç½®åŠä½¿ç”¨ã€éƒ¨ç½²åº”ç”¨ã€å¾®è°ƒç­‰ï¼Œæ¯ä¸ªéƒ¨åˆ†è¦†ç›–ä¸»æµåŠç‰¹ç‚¹å¼€æº LLMï¼š&lt;/p&gt; 
&lt;h3&gt;Example ç³»åˆ—&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Chat-%E5%AC%9B%E5%AC%9B/readme.md"&gt;Chat-å¬›å¬›&lt;/a&gt;ï¼š Chat-ç”„å¬›æ˜¯åˆ©ç”¨ã€Šç”„å¬›ä¼ ã€‹å‰§æœ¬ä¸­æ‰€æœ‰å…³äºç”„å¬›çš„å°è¯å’Œè¯­å¥ï¼ŒåŸºäºLLMè¿›è¡ŒLoRAå¾®è°ƒå¾—åˆ°çš„æ¨¡ä»¿ç”„å¬›è¯­æ°”çš„èŠå¤©è¯­è¨€æ¨¡å‹ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Tianji-%E5%A4%A9%E6%9C%BA/readme.md"&gt;Tianji-å¤©æœº&lt;/a&gt;ï¼šå¤©æœºæ˜¯ä¸€æ¬¾åŸºäºäººæƒ…ä¸–æ•…ç¤¾äº¤åœºæ™¯ï¼Œæ¶µç›–æç¤ºè¯å·¥ç¨‹ ã€æ™ºèƒ½ä½“åˆ¶ä½œã€ æ•°æ®è·å–ä¸æ¨¡å‹å¾®è°ƒã€RAG æ•°æ®æ¸…æ´—ä¸ä½¿ç”¨ç­‰å…¨æµç¨‹çš„å¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿåº”ç”¨æ•™ç¨‹ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/AMchat-%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/readme.md"&gt;AMChat&lt;/a&gt;: AM (Advanced Mathematics) chat æ˜¯ä¸€ä¸ªé›†æˆäº†æ•°å­¦çŸ¥è¯†å’Œé«˜ç­‰æ•°å­¦ä¹ é¢˜åŠå…¶è§£ç­”çš„å¤§è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ Math å’Œé«˜ç­‰æ•°å­¦ä¹ é¢˜åŠå…¶è§£æèåˆçš„æ•°æ®é›†ï¼ŒåŸºäº InternLM2-Math-7B æ¨¡å‹ï¼Œé€šè¿‡ xtuner å¾®è°ƒï¼Œä¸“é—¨è®¾è®¡ç”¨äºè§£ç­”é«˜ç­‰æ•°å­¦é—®é¢˜ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/%E6%95%B0%E5%AD%97%E7%94%9F%E5%91%BD/readme.md"&gt;æ•°å­—ç”Ÿå‘½&lt;/a&gt;: æœ¬é¡¹ç›®å°†ä»¥æˆ‘ä¸ºåŸå‹ï¼Œåˆ©ç”¨ç‰¹åˆ¶çš„æ•°æ®é›†å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè‡´åŠ›äºåˆ›é€ ä¸€ä¸ªèƒ½å¤ŸçœŸæ­£åæ˜ æˆ‘çš„ä¸ªæ€§ç‰¹å¾çš„AIæ•°å­—äººâ€”â€”åŒ…æ‹¬ä½†ä¸é™äºæˆ‘çš„è¯­æ°”ã€è¡¨è¾¾æ–¹å¼å’Œæ€ç»´æ¨¡å¼ç­‰ç­‰ï¼Œå› æ­¤æ— è®ºæ˜¯æ—¥å¸¸èŠå¤©è¿˜æ˜¯åˆ†äº«å¿ƒæƒ…ï¼Œå®ƒéƒ½ä»¥ä¸€ç§æ—¢ç†Ÿæ‚‰åˆèˆ’é€‚çš„æ–¹å¼äº¤æµï¼Œä»¿ä½›æˆ‘åœ¨ä»–ä»¬èº«è¾¹ä¸€æ ·ã€‚æ•´ä¸ªæµç¨‹æ˜¯å¯è¿ç§»å¤åˆ¶çš„ï¼Œäº®ç‚¹æ˜¯æ•°æ®é›†çš„åˆ¶ä½œã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å·²æ”¯æŒæ¨¡å‹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/openai/gpt-oss-20b"&gt;gpt-oss-20b&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; gpt-oss-20b vllm éƒ¨ç½²è°ƒç”¨&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; gpt-oss-20b EvalScope å¹¶å‘è¯„æµ‹&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; gpt-oss-20b lmstudio æœ¬åœ°éƒ¨ç½²è°ƒç”¨&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; gpt-oss-20b Lora å¾®è°ƒåŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; gpt-oss-20b DPO å¾®è°ƒåŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/zai-org/GLM-4.1V-Thinking"&gt;GLM-4.1-Thinking&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/01-GLM-4%201V-Thinking%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;GLM-4.1V-Thinking vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/02-GLM-4%201V-Thinking%20Gradio%E9%83%A8%E7%BD%B2.md"&gt;GLM-4.1V-Thinking Gradioéƒ¨ç½²&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/03-GLM-4%201V-Thinking%20LoRA%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md"&gt;GLM-4.1V-Thinking Lora å¾®è°ƒåŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.codewithgpu.com/i/datawhalechina/self-llm/GLM4.1V-Thinking-lora"&gt;GLM-4.1V-Thinking Docker é•œåƒ&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/zai-org/GLM-4.5"&gt;GLM-4.5-Air&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/01-GLM-4.5-Air-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;GLM-4.5-Air vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/02-GLM-4.5-Air%20EvalScope%20%E5%B9%B6%E5%8F%91%E6%B5%8B%E8%AF%95.md"&gt;GLM-4.5-Air EvalScope æ™ºå•†æƒ…å•† &amp;amp;&amp;amp; å¹¶å‘è¯„æµ‹&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/03-GLM-4.5-Air-Lora%20%E5%8F%8A%20Swanlab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BE%AE%E8%B0%83.md"&gt;GLM-4.5-Air Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.compshare.cn/images/lUQhKDCeCdZW?referral_code=ELukJdQS3vvCwYIfgsQf2C&amp;amp;ytag=GPU_yy_github_selfllm"&gt;GLM-4.5-Air Ucloud Docker é•œåƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT"&gt;ERNIE-4.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ERNIE-4.5/01-ERNIE-4.5-0.3B-PT%20Lora%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md"&gt;ERNIE-4.5-0.3B-PT Lora å¾®è°ƒåŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.codewithgpu.com/i/datawhalechina/self-llm/ERNIE-4.5-lora"&gt;ERNIE-4.5-0.3B-PT Lora Docker é•œåƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/Tencent-Hunyuan/Hunyuan-A13B"&gt;Hunyuan-A13B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/01-Hunyuan-A13B-Instruct%20%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20Blog.md"&gt;Hunyuan-A13B-Instruct æ¨¡å‹æ¶æ„è§£æ Blog&lt;/a&gt; @å“å ‚è¶Š&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/03-Hunyuan-A13B-Instruct-SGLang%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Hunyuan-A13B-Instruct SGLang éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @fancy&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/05-Hunyuan-A13B-Instruct-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md"&gt;Hunyuan-A13B-Instruct Lora SwanLab å¯è§†åŒ–å¾®è°ƒ&lt;/a&gt; @è°¢å¥½å†‰&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan-A13B-Instruct-lora"&gt;Hunyuan-A13B-Instruct Lora Docker é•œåƒ&lt;/a&gt; @è°¢å¥½å†‰&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen3"&gt;Qwen3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/01-Qwen3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90-Blog.md"&gt;Qwen3 æ¨¡å‹ç»“æ„è§£æ Blog&lt;/a&gt; @ç‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/02-Qwen3-8B-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen3-8B vllm éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/03-Qwen3-7B-Instruct%20Windows%20LMStudio%20%E9%83%A8%E7%BD%B2.md"&gt;Qwen3-8B Windows LMStudio éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ç‹ç† æ˜&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/04-Qwen3-8B%20EvalScope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md"&gt;Qwen3-8B Evalscope æ™ºå•†æƒ…å•†è¯„æµ‹&lt;/a&gt; @æå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/05-Qwen3-8B-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md"&gt;Qwen3-8B Lora å¾®è°ƒåŠSwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/06-Qwen3-30B-A3B%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md"&gt;Qwen3-30B-A3B å¾®è°ƒåŠSwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/07-Qwen3-Think-%E8%A7%A3%E5%AF%86-Blog.md"&gt;Qwen3 Think è§£å¯† Blog&lt;/a&gt; @æ¨Šå¥‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.codewithgpu.com/i/datawhalechina/self-llm/Qwen3"&gt;Qwen3-8B Docker é•œåƒ&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models//Qwen3/08-Qwen3_0_6B%E7%9A%84%E5%B0%8F%E6%A8%A1%E5%9E%8B%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8.md"&gt;Qwen3-0.6B çš„å°æ¨¡å‹æœ‰ä»€ä¹ˆç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/09-Qwen3-1.7B-%E5%8C%BB%E5%AD%A6%E6%8E%A8%E7%90%86%E5%BC%8F%E5%AF%B9%E8%AF%9D%E5%BE%AE%E8%B0%83%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md"&gt;Qwen3-1.7B åŒ»å­¦æ¨ç†å¼å¯¹è¯å¾®è°ƒ åŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @æ—æ³½æ¯…&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/10-Qwen3-8B%20GRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md"&gt;Qwen3-8B GRPOå¾®è°ƒåŠé€šè¿‡swanlabå¯è§†åŒ–&lt;/a&gt; @éƒ­å®£ä¼¯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/MoonshotAI/Kimi-VL"&gt;Kimi&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/02-Kimi-VL-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB.md"&gt;Kimi-VL-A3B æŠ€æœ¯æŠ¥å‘Šè§£è¯»&lt;/a&gt; @ç‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/01-Kimi-VL-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md"&gt;Kimi-VL-A3B-Thinking WebDemo éƒ¨ç½²ï¼ˆç½‘é¡µå¯¹è¯åŠ©æ‰‹ï¼‰&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct"&gt;Llama4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama4/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md"&gt;Llama4 å¯¹è¯åŠ©æ‰‹&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/manycore-research/SpatialLM"&gt;SpatialLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/SpatialLM/readme.md"&gt;SpatialLM 3Dç‚¹äº‘ç†è§£ä¸ç›®æ ‡æ£€æµ‹æ¨¡å‹éƒ¨ç½²&lt;/a&gt; @ç‹æ³½å®‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan3D-2"&gt;Hunyuan3D-2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/01-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.md"&gt;Hunyuan3D-2 ç³»åˆ—æ¨¡å‹éƒ¨ç½²&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/02-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8.md"&gt;Hunyuan3D-2 ç³»åˆ—æ¨¡å‹ä»£ç è°ƒç”¨&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/03-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BGradio%E9%83%A8%E7%BD%B2.md"&gt;Hunyuan3D-2 ç³»åˆ—æ¨¡å‹Gradioéƒ¨ç½²&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/04-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BAPI%20Server.md"&gt;Hunyuan3D-2 ç³»åˆ—æ¨¡å‹API Server&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan3D-2"&gt;Hunyuan3D-2 Docker é•œåƒ&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3-4b-it"&gt;Gemma3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/01-gemma-3-4b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;gemma-3-4b-it FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æœæ£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/03-gemma-3-4b-it-ollama%20+%20open-webui%E9%83%A8%E7%BD%B2.md"&gt;gemma-3-4b-it ollama + open-webuiéƒ¨ç½²&lt;/a&gt; @å­™è¶…&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/04-Gemma3-4b%20%20evalscope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md"&gt;gemma-3-4b-it evalscope æ™ºå•†æƒ…å•†è¯„æµ‹&lt;/a&gt; @å¼ é¾™æ–&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/05-gemma-3-4b-it%20LoRA.md"&gt;gemma-3-4b-it Lora å¾®è°ƒ&lt;/a&gt; @èéº¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.codewithgpu.com/i/datawhalechina/self-llm/self-llm-gemma3"&gt;gemma-3-4b-it Docker é•œåƒ&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/6-gemma3-4B-itGRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md"&gt;gemma-3-4b-it GRPOå¾®è°ƒåŠé€šè¿‡swanlabå¯è§†åŒ–&lt;/a&gt; @éƒ­å®£ä¼¯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"&gt;DeepSeek-R1-Distill&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/01-DeepSeek-R1-Distill-Qwen-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;DeepSeek-R1-Distill-Qwen-7B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éª†ç§€éŸ¬&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/02-DeepSeek-R1-Distill-Qwen-7B%20Langchain%20%E6%8E%A5%E5%85%A5.md"&gt;DeepSeek-R1-Distill-Qwen-7B Langchain æ¥å…¥&lt;/a&gt; @éª†ç§€éŸ¬&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/03-DeepSeek-R1-Distill-Qwen-7B%20WebDemo%20%E9%83%A8%E7%BD%B2.md"&gt;DeepSeek-R1-Distill-Qwen-7B WebDemo éƒ¨ç½²&lt;/a&gt; @éª†ç§€éŸ¬&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/04-DeepSeek-R1-Distill-Qwen-7B%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;DeepSeek-R1-Distill-Qwen-7B vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éª†ç§€éŸ¬&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/05-DeepSeek-R1-0528-Qwen3-8B-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md"&gt;DeepSeek-R1-0528-Qwen3-8B-GRPOåŠswanlabå¯è§†åŒ–&lt;/a&gt; @éƒ­å®£ä¼¯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/OpenBMB/MiniCPM-o"&gt;MiniCPM-o-2_6&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/01MiniCPM-o%202%206%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8%20.md"&gt;minicpm-o-2.6 FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æ—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/02minicpm-o-2.6WebDemo_streamlit.py"&gt;minicpm-o-2.6 WebDemo éƒ¨ç½²&lt;/a&gt; @ç¨‹å®&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/03-MiniCPM-o-2.6%20%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E9%9F%B3%E8%83%BD%E5%8A%9B.md"&gt;minicpm-o-2.6 å¤šæ¨¡æ€è¯­éŸ³èƒ½åŠ›&lt;/a&gt; @é‚“æºä¿Š&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/04-MiniCPM-0-2.6%20Lora%E5%BE%AE%E8%B0%83.md"&gt;minicpm-o-2.6 å¯è§†åŒ– LaTeX_OCR Lora å¾®è°ƒ&lt;/a&gt; @æ—æ³½æ¯…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/InternLM/InternLM"&gt;InternLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/01-InternLM3-8B-Instruct%20FastAPI.md"&gt;internlm3-8b-instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @è‹å‘æ ‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/02-internlm3-8b-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md"&gt;internlm3-8b-instruct Langchianæ¥å…¥&lt;/a&gt; @èµµæ–‡æº&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/03-InternLM3-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;internlm3-8b-instruct WebDemo éƒ¨ç½²&lt;/a&gt; @ç‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/04-InternLM3-8B-Instruct%20LoRA.md"&gt;internlm3-8b-instruct Lora å¾®è°ƒ&lt;/a&gt; @ç¨‹å®&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/05-internlm3-8b-instruct%20%E4%B8%8Eo1%20.md"&gt;internlm3-8b-instruct o1-likeæ¨ç†é“¾å®ç°&lt;/a&gt; @é™ˆç¿&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/phi-4"&gt;phi4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/01-Phi-4%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;phi4 FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æœæ£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/02-Phi-4-Langchain%E6%8E%A5%E5%85%A5.md"&gt;phi4 langchain æ¥å…¥&lt;/a&gt; @å°ç½—&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/03-Phi-4%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;phi4 WebDemo éƒ¨ç½²&lt;/a&gt; @æœæ£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/04-Phi-4-Lora%20%E5%BE%AE%E8%B0%83.md"&gt;phi4 Lora å¾®è°ƒ&lt;/a&gt; @éƒ‘è¿œå©§&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/05-Phi-4-Lora%20%E5%BE%AE%E8%B0%83%20%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.md"&gt;phi4 Lora å¾®è°ƒ NERä»»åŠ¡ SwanLab å¯è§†åŒ–è®°å½•ç‰ˆ&lt;/a&gt; @æ—æ³½æ¯…&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/06-Phi-4-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md"&gt;phi4 GRPOå¾®è°ƒåŠé€šè¿‡swanlabå¯è§†åŒ–&lt;/a&gt; @éƒ­å®£ä¼¯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen2.5-Coder"&gt;Qwen2.5-Coder&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/01-Qwen2.5-Coder-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen2.5-Coder-7B-Instruct FastApiéƒ¨ç½²è°ƒç”¨&lt;/a&gt; @èµµæ–‡æº&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md"&gt;Qwen2.5-Coder-7B-Instruct Langchianæ¥å…¥&lt;/a&gt; @æ¨æ™¨æ—­&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/03-Qwen2.5-Coder-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;Qwen2.5-Coder-7B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @ç‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/04-Qwen2.5-Coder-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen2.5-Coder-7B-Instruct vLLM éƒ¨ç½²&lt;/a&gt; @ç‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Qwen2.5-Coder-7B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @èéº¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/05-Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md"&gt;Qwen2.5-Coder-7B-Instruct Lora å¾®è°ƒ SwanLab å¯è§†åŒ–è®°å½•ç‰ˆ&lt;/a&gt; @æ¨å“&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen2-VL"&gt;Qwen2-vl&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/01-Qwen2-VL-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen2-vl-2B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/02-Qwen2-VL-2B-Instruct%20Web%20Demo%E9%83%A8%E7%BD%B2.md"&gt;Qwen2-vl-2B WebDemo éƒ¨ç½²&lt;/a&gt; @èµµä¼Ÿ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/03-Qwen2-VL-2B-Instruct%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen2-vl-2B vLLM éƒ¨ç½²&lt;/a&gt; @èéº¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/04-Qwen2-VL-2B%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Qwen2-vl-2B Lora å¾®è°ƒ&lt;/a&gt; @ææŸ¯è¾°&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/05-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md"&gt;Qwen2-vl-2B Lora å¾®è°ƒ SwanLab å¯è§†åŒ–è®°å½•ç‰ˆ&lt;/a&gt; @æ—æ³½æ¯…&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/06-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%E6%A1%88%E4%BE%8B%20-%20LaTexOCR.md"&gt;Qwen2-vl-2B Lora å¾®è°ƒæ¡ˆä¾‹ - LaTexOCR&lt;/a&gt; @æ—æ³½æ¯…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen2.5"&gt;Qwen2.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/01-Qwen2.5-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen2.5-7B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¨„å¤©å¥¥&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md"&gt;Qwen2.5-7B-Instruct langchain æ¥å…¥&lt;/a&gt; @å¨„å¤©å¥¥&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/03-Qwen2.5-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen2.5-7B-Instruct vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/04-Qwen2_5-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;Qwen2.5-7B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/05-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Qwen2.5-7B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @å·¦æ˜¥ç”Ÿ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/06-Qwen2.5-7B-Instruct%20o1-like%20%E6%8E%A8%E7%90%86%E9%93%BE%E5%AE%9E%E7%8E%B0.md"&gt;Qwen2.5-7B-Instruct o1-like æ¨ç†é“¾å®ç°&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/07-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md"&gt;Qwen2.5-7B-Instruct Lora å¾®è°ƒ SwanLab å¯è§†åŒ–è®°å½•ç‰ˆ&lt;/a&gt; @æ—æ³½æ¯…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://machinelearning.apple.com/research/openelm"&gt;Apple OpenELM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/01-OpenELM-3B-Instruct%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;OpenELM-3B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ç‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/02-OpenELM-3B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md"&gt;OpenELM-3B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @ç‹æ³½å®‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"&gt;Llama3_1-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/01-Llama3_1-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Llama3_1-8B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/02-Llama3_1-8B-Instruct%20langchain%E6%8E%A5%E5%85%A5.md"&gt;Llama3_1-8B-Instruct langchain æ¥å…¥&lt;/a&gt; @å¼ æ™‹&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/03-Llama3_1-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;Llama3_1-8B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @å¼ æ™‹&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/04-Llama3_1-8B--Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Llama3_1-8B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/%E5%8A%A8%E6%89%8B%E8%BD%AC%E6%8D%A2GGUF%E6%A8%A1%E5%9E%8B%E5%B9%B6%E4%BD%BF%E7%94%A8Ollama%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2.md"&gt;åŠ¨æ‰‹è½¬æ¢GGUFæ¨¡å‹å¹¶ä½¿ç”¨Ollamaæœ¬åœ°éƒ¨ç½²&lt;/a&gt; @Gaoboy&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-2-9b-it"&gt;Gemma-2-9b-it&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/01-Gemma-2-9b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Gemma-2-9b-it FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/02-Gemma-2-9b-it%20langchain%20%E6%8E%A5%E5%85%A5.md"&gt;Gemma-2-9b-it langchain æ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/03-Gemma-2-9b-it%20WebDemo%20%E9%83%A8%E7%BD%B2.md"&gt;Gemma-2-9b-it WebDemo éƒ¨ç½²&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/04-Gemma-2-9b-it%20peft%20lora%E5%BE%AE%E8%B0%83.md"&gt;Gemma-2-9b-it Peft Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/IEIT-Yuan/Yuan-2.0"&gt;Yuan2.0&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/01-Yuan2.0-2B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Yuan2.0-2B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/02-Yuan2.0-2B%20Langchain%20%E6%8E%A5%E5%85%A5.md"&gt;Yuan2.0-2B Langchain æ¥å…¥&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/03-Yuan2.0-2B%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;Yuan2.0-2B WebDemoéƒ¨ç½²&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/04-Yuan2.0-2B%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Yuan2.0-2B vLLMéƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/05-Yuan2.0-2B%20Lora%E5%BE%AE%E8%B0%83.md"&gt;Yuan2.0-2B Loraå¾®è°ƒ&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/IEIT-Yuan/Yuan2.0-M32"&gt;Yuan2.0-M32&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/01-Yuan2.0-M32%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Yuan2.0-M32 FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/02-Yuan2.0-M32%20Langchain%20%E6%8E%A5%E5%85%A5.md"&gt;Yuan2.0-M32 Langchain æ¥å…¥&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/03-Yuan2.0-M32%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;Yuan2.0-M32 WebDemoéƒ¨ç½²&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-Coder-V2"&gt;DeepSeek-Coder-V2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/01-DeepSeek-Coder-V2-Lite-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;DeepSeek-Coder-V2-Lite-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/02-DeepSeek-Coder-V2-Lite-Instruct%20%E6%8E%A5%E5%85%A5%20LangChain.md"&gt;DeepSeek-Coder-V2-Lite-Instruct langchain æ¥å…¥&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/03-DeepSeek-Coder-V2-Lite-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md"&gt;DeepSeek-Coder-V2-Lite-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/04-DeepSeek-Coder-V2-Lite-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;DeepSeek-Coder-V2-Lite-Instruct Lora å¾®è°ƒ&lt;/a&gt; @ä½™æ´‹&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/bilibili/Index-1.9B"&gt;å“”å“©å“”å“© Index-1.9B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/01-Index-1.9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Index-1.9B-Chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @é‚“æºä¿Š&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/02-Index-1.9B-Chat%20%E6%8E%A5%E5%85%A5%20LangChain.md"&gt;Index-1.9B-Chat langchain æ¥å…¥&lt;/a&gt; @å¼ å‹ä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/03-Index-1.9B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;Index-1.9B-Chat WebDemo éƒ¨ç½²&lt;/a&gt; @ç¨‹å®&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/04-Index-1.9B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Index-1.9B-Chat Lora å¾®è°ƒ&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen2"&gt;Qwen2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/01-Qwen2-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen2-7B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @åº·å©§æ·‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/02-Qwen2-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md"&gt;Qwen2-7B-Instruct langchain æ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/03-Qwen2-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;Qwen2-7B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @ä¸‰æ°´&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/04-Qwen2-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen2-7B-Instruct vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/05-Qwen2-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Qwen2-7B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @æ•£æ­¥&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/THUDM/GLM-4.git"&gt;GLM-4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/01-GLM-4-9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;GLM-4-9B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¼ å‹ä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/02-GLM-4-9B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md"&gt;GLM-4-9B-chat langchain æ¥å…¥&lt;/a&gt; @è°­é€¸ç‚&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/03-GLM-4-9B-Chat%20WebDemo.md"&gt;GLM-4-9B-chat WebDemo éƒ¨ç½²&lt;/a&gt; @ä½•è‡³è½©&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/04-GLM-4-9B-Chat%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;GLM-4-9B-chat vLLM éƒ¨ç½²&lt;/a&gt; @ç‹ç† æ˜&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;GLM-4-9B-chat Lora å¾®è°ƒ&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat-hf%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;GLM-4-9B-chat-hf Lora å¾®è°ƒ&lt;/a&gt; @ä»˜å¿—è¿œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen1.5.git"&gt;Qwen 1.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/01-Qwen1.5-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen1.5-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @é¢œé‘«&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/02-Qwen1.5-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md"&gt;Qwen1.5-7B-chat langchain æ¥å…¥&lt;/a&gt; @é¢œé‘«&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/03-Qwen1.5-7B-Chat%20WebDemo.md"&gt;Qwen1.5-7B-chat WebDemo éƒ¨ç½²&lt;/a&gt; @é¢œé‘«&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/04-Qwen1.5-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Qwen1.5-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/05-Qwen1.5-7B-Chat-GPTQ-Int4%20%20WebDemo.md"&gt;Qwen1.5-72B-chat-GPTQ-Int4 éƒ¨ç½²ç¯å¢ƒ&lt;/a&gt; @byx020119&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/06-Qwen1.5-MoE-A2.7B.md"&gt;Qwen1.5-MoE-chat Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/07-Qwen1.5-7B-Chat%20vLLM%20%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen1.5-7B-chat vLLMæ¨ç†éƒ¨ç½²&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/08-Qwen1.5-7B-chat%20LoRA%E5%BE%AE%E8%B0%83%E6%8E%A5%E5%85%A5%E5%AE%9E%E9%AA%8C%E7%AE%A1%E7%90%86.md"&gt;Qwen1.5-7B-chat Lora å¾®è°ƒ æ¥å…¥SwanLabå®éªŒç®¡ç†å¹³å°&lt;/a&gt; @é»„æŸç‰¹&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-7b-it"&gt;è°·æ­Œ-Gemma&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/01-Gemma-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;gemma-2b-it FastApi éƒ¨ç½²è°ƒç”¨ &lt;/a&gt; @ä¸œä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/02-Gemma-2B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md"&gt;gemma-2b-it langchain æ¥å…¥ &lt;/a&gt; @ä¸œä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/03-Gemma-2B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md"&gt;gemma-2b-it WebDemo éƒ¨ç½² &lt;/a&gt; @ä¸œä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/04-Gemma-2B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md"&gt;gemma-2b-it Peft Lora å¾®è°ƒ &lt;/a&gt; @ä¸œä¸œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"&gt;phi-3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/01-Phi-3-mini-4k-instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Phi-3-mini-4k-instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éƒ‘çš“æ¡¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/02-Phi-3-mini-4k-instruct%20langchain%20%E6%8E%A5%E5%85%A5.md"&gt;Phi-3-mini-4k-instruct langchain æ¥å…¥&lt;/a&gt; @éƒ‘çš“æ¡¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/03-Phi-3-mini-4k-instruct%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;Phi-3-mini-4k-instruct WebDemo éƒ¨ç½²&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/04-Phi-3-mini-4k-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Phi-3-mini-4k-instruct Lora å¾®è°ƒ&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/thu-coai/CharacterGLM-6B"&gt;CharacterGLM-6B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/01-CharacterGLM-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;CharacterGLM-6B Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å­™å¥å£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/02-CharacterGLM-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;CharacterGLM-6B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å­™å¥å£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/03-CharacterGLM-6B-chat.md"&gt;CharacterGLM-6B webdemo éƒ¨ç½²&lt;/a&gt; @å­™å¥å£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/04-CharacterGLM-6B%20Lora%E5%BE%AE%E8%B0%83.md"&gt;CharacterGLM-6B Lora å¾®è°ƒ&lt;/a&gt; @å­™å¥å£®&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/meta-llama/llama3.git"&gt;LLaMA3-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/01-LLaMA3-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;LLaMA3-8B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/02-LLaMA3-8B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md"&gt;LLaMA3-8B-Instruct langchain æ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/03-LLaMA3-8B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md"&gt;LLaMA3-8B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/04-LLaMA3-8B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;LLaMA3-8B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/xverse/XVERSE-7B-Chat/summary"&gt;XVERSE-7B-Chat&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/01-XVERSE-7B-chat%20Transformers%E6%8E%A8%E7%90%86.md"&gt;XVERSE-7B-Chat transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/02-XVERSE-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md"&gt;XVERSE-7B-Chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/03-XVERSE-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md"&gt;XVERSE-7B-Chat langchain æ¥å…¥&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/04-XVERSE-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md"&gt;XVERSE-7B-Chat WebDemo éƒ¨ç½²&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/05-XVERSE-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;XVERSE-7B-Chat Lora å¾®è°ƒ&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/OpenNLPLab/TransnormerLLM.git"&gt;TransNormerLLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/01-TransNormer-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;TransNormerLLM-7B-Chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ç‹èŒ‚éœ–&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/02-TransNormer-7B%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md"&gt;TransNormerLLM-7B-Chat langchain æ¥å…¥&lt;/a&gt; @ç‹èŒ‚éœ–&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/03-TransNormer-7B%20WebDemo.md"&gt;TransNormerLLM-7B-Chat WebDemo éƒ¨ç½²&lt;/a&gt; @ç‹èŒ‚éœ–&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/04-TrasnNormer-7B%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;TransNormerLLM-7B-Chat Lora å¾®è°ƒ&lt;/a&gt; @ç‹èŒ‚éœ–&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/vivo-ai-lab/BlueLM.git"&gt;BlueLM Vivo è“å¿ƒå¤§æ¨¡å‹&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/01-BlueLM-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2.md"&gt;BlueLM-7B-Chat FatApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/02-BlueLM-7B-Chat%20langchain%20%E6%8E%A5%E5%85%A5.md"&gt;BlueLM-7B-Chat langchain æ¥å…¥&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/03-BlueLM-7B-Chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md"&gt;BlueLM-7B-Chat WebDemo éƒ¨ç½²&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/04-BlueLM-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;BlueLM-7B-Chat Lora å¾®è°ƒ&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/InternLM/InternLM"&gt;InternLM2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/01-InternLM2-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md"&gt;InternLM2-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/02-InternLM2-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md"&gt;InternLM2-7B-chat langchain æ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/03-InternLM2-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md"&gt;InternLM2-7B-chat WebDemo éƒ¨ç½²&lt;/a&gt; @éƒ‘çš“æ¡¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/04-InternLM2-7B-chat%20Xtuner%20Qlora%20%E5%BE%AE%E8%B0%83.md"&gt;InternLM2-7B-chat Xtuner Qlora å¾®è°ƒ&lt;/a&gt; @éƒ‘çš“æ¡¦&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-LLM"&gt;DeepSeek æ·±åº¦æ±‚ç´¢&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/01-DeepSeek-7B-chat%20FastApi.md"&gt;DeepSeek-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/02-DeepSeek-7B-chat%20langchain.md"&gt;DeepSeek-7B-chat langchain æ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/03-DeepSeek-7B-chat%20WebDemo.md"&gt;DeepSeek-7B-chat WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/04-DeepSeek-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;DeepSeek-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/05-DeepSeek-7B-chat%204bits%E9%87%8F%E5%8C%96%20Qlora%20%E5%BE%AE%E8%B0%83.md"&gt;DeepSeek-7B-chat 4bitsé‡åŒ– Qlora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;DeepSeek-MoE-16b-chat Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20FastApi.md"&gt;DeepSeek-MoE-16b-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/07-deepseek_fine_tune.ipynb"&gt;DeepSeek-coder-6.7b finetune colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/08-deepseek_web_demo.ipynb"&gt;Deepseek-coder-6.7b webdemo colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/OpenBMB/MiniCPM.git"&gt;MiniCPM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;MiniCPM-2B-chat transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;MiniCPM-2B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20langchain%E6%8E%A5%E5%85%A5.md"&gt;MiniCPM-2B-chat langchain æ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md"&gt;MiniCPM-2B-chat webdemo éƒ¨ç½²&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20Lora%20&amp;amp;&amp;amp;%20Full%20%E5%BE%AE%E8%B0%83.md"&gt;MiniCPM-2B-chat Lora &amp;amp;&amp;amp; Full å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; å®˜æ–¹å‹æƒ…é“¾æ¥ï¼š&lt;a href="https://modelbest.feishu.cn/wiki/D2tFw8Pcsi5CIzkaHNacLK64npg"&gt;é¢å£å°é’¢ç‚®MiniCPMæ•™ç¨‹&lt;/a&gt; @OpenBMB&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; å®˜æ–¹å‹æƒ…é“¾æ¥ï¼š&lt;a href="https://github.com/OpenBMB/MiniCPM-CookBook"&gt;MiniCPM-Cookbook&lt;/a&gt; @OpenBMB&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen-Audio.git"&gt;Qwen-Audio&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/01-Qwen-Audio-chat%20FastApi.md"&gt;Qwen-Audio FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @é™ˆæ€å·&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/02-Qwen-Audio-chat%20WebDemo.md"&gt;Qwen-Audio WebDemo&lt;/a&gt; @é™ˆæ€å·&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen.git"&gt;Qwen&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/01-Qwen-7B-Chat%20Transformers%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen-7B-chat Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/02-Qwen-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Qwen-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/03-Qwen-7B-Chat%20WebDemo.md"&gt;Qwen-7B-chat WebDemo&lt;/a&gt; @æå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/04-Qwen-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Qwen-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/05-Qwen-7B-Chat%20Ptuning%20%E5%BE%AE%E8%B0%83.md"&gt;Qwen-7B-chat ptuning å¾®è°ƒ&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/06-Qwen-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md"&gt;Qwen-7B-chat å…¨é‡å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/07-Qwen-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md"&gt;Qwen-7B-Chat æ¥å…¥langchainæ­å»ºçŸ¥è¯†åº“åŠ©æ‰‹&lt;/a&gt; @æå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/08-Qwen-7B-Chat%20Lora%20%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%BE%AE%E8%B0%83.md"&gt;Qwen-7B-chat ä½ç²¾åº¦è®­ç»ƒ&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/09-Qwen-1_8B-chat%20CPU%20%E9%83%A8%E7%BD%B2%20.md"&gt;Qwen-1_8B-chat CPU éƒ¨ç½²&lt;/a&gt; @æ•£æ­¥&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/01-ai/Yi.git"&gt;Yi é›¶ä¸€ä¸‡ç‰©&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/01-Yi-6B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Yi-6B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ææŸ¯è¾°&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/02-Yi-6B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md"&gt;Yi-6B-chat langchainæ¥å…¥&lt;/a&gt; @ææŸ¯è¾°&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/03-Yi-6B-chat%20WebDemo.md"&gt;Yi-6B-chat WebDemo&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/04-Yi-6B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Yi-6B-chat Lora å¾®è°ƒ&lt;/a&gt; @æå¨‡å¨‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.baichuan-ai.com/home"&gt;Baichuan ç™¾å·æ™ºèƒ½&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/BaiChuan/01-Baichuan2-7B-chat%2BFastApi%2B%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;Baichuan2-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æƒ ä½³è±ª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/02-Baichuan-7B-chat%2BWebDemo.md"&gt;Baichuan2-7B-chat WebDemo&lt;/a&gt; @æƒ ä½³è±ª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/03-Baichuan2-7B-chat%E6%8E%A5%E5%85%A5LangChain%E6%A1%86%E6%9E%B6.md"&gt;Baichuan2-7B-chat æ¥å…¥ LangChain æ¡†æ¶&lt;/a&gt; @æƒ ä½³è±ª&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/04-Baichuan2-7B-chat%2Blora%2B%E5%BE%AE%E8%B0%83.md"&gt;Baichuan2-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @æƒ ä½³è±ª&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/InternLM/InternLM.git"&gt;InternLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/01-InternLM-Chat-7B%20Transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;InternLM-Chat-7B Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å°ç½—&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/02-internLM-Chat-7B%20FastApi.md"&gt;InternLM-Chat-7B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/03-InternLM-Chat-7B.md"&gt;InternLM-Chat-7B WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/04-Lagent+InternLM-Chat-7B-V1.1.md"&gt;Lagent+InternLM-Chat-7B-V1.1 WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/05-%E6%B5%A6%E8%AF%AD%E7%81%B5%E7%AC%94%E5%9B%BE%E6%96%87%E7%90%86%E8%A7%A3&amp;amp;%E5%88%9B%E4%BD%9C.md"&gt;æµ¦è¯­çµç¬”å›¾æ–‡ç†è§£&amp;amp;åˆ›ä½œ WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/06-InternLM%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md"&gt;InternLM-Chat-7B æ¥å…¥ LangChain æ¡†æ¶&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://hf-mirror.com/FlagAlpha/Atom-7B-Chat"&gt;Atom (llama2)&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/01-Atom-7B-chat-WebDemo.md"&gt;Atom-7B-chat WebDemo&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/02-Atom-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md"&gt;Atom-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/03-Atom-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md"&gt;Atom-7B-Chat æ¥å…¥langchainæ­å»ºçŸ¥è¯†åº“åŠ©æ‰‹&lt;/a&gt; @é™ˆæ€å·&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/04-Atom-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md"&gt;Atom-7B-chat å…¨é‡å¾®è°ƒ&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/THUDM/ChatGLM3.git"&gt;ChatGLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/01-ChatGLM3-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;ChatGLM3-6B Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/02-ChatGLM3-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md"&gt;ChatGLM3-6B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/03-ChatGLM3-6B-chat.md"&gt;ChatGLM3-6B chat WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/04-ChatGLM3-6B-Code-Interpreter.md"&gt;ChatGLM3-6B Code Interpreter WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/05-ChatGLM3-6B%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md"&gt;ChatGLM3-6B æ¥å…¥ LangChain æ¡†æ¶&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/06-ChatGLM3-6B-Lora%E5%BE%AE%E8%B0%83.md"&gt;ChatGLM3-6B Lora å¾®è°ƒ&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;é€šç”¨ç¯å¢ƒé…ç½®&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/01-pip%E3%80%81conda%E6%8D%A2%E6%BA%90.md"&gt;pipã€conda æ¢æº&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/02-AutoDL%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3.md"&gt;AutoDL å¼€æ”¾ç«¯å£&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æ¨¡å‹ä¸‹è½½&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md"&gt;hugging face&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md"&gt;hugging face&lt;/a&gt; é•œåƒä¸‹è½½ @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md"&gt;modelscope&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md"&gt;git-lfs&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md"&gt;Openxlab&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Issue &amp;amp;&amp;amp; PR&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md"&gt;Issue æäº¤&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md"&gt;PR æäº¤&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md"&gt;forkæ›´æ–°&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;è‡´è°¢&lt;/h2&gt; 
&lt;h3&gt;æ ¸å¿ƒè´¡çŒ®è€…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KMnO4-zx"&gt;å®‹å¿—å­¦(ä¸è¦è‘±å§œè’œ)-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜-ä¸­å›½çŸ¿ä¸šå¤§å­¦(åŒ—äº¬)ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/logan-zou"&gt;é‚¹é›¨è¡¡-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜-å¯¹å¤–ç»æµè´¸æ˜“å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tsumugii24"&gt;å§œèˆ’å‡¡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Hongru0306"&gt;è‚–é¸¿å„’&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜-åŒæµå¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/acwwt"&gt;éƒ­å¿—èˆª&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Zeyi-Lin"&gt;æ—æ³½æ¯…&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-SwanLabäº§å“è´Ÿè´£äººï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zhangfanTJU"&gt;å¼ å¸†&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moyitech"&gt;ç‹æ³½å®‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¤ªåŸç†å·¥å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Aphasia0515"&gt;æå¨‡å¨‡&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/0-yy-0"&gt;é«˜ç«‹ä¸š&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-DataWhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dingyue772"&gt;ä¸æ‚¦&lt;/a&gt; ï¼ˆDatawhale-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LINHYYY"&gt;æ—æ’å®‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-æ¸…åå¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/L4HeyXiao"&gt;æƒ ä½³è±ª&lt;/a&gt; ï¼ˆDatawhale-å®£ä¼ å¤§ä½¿ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mlw67"&gt;ç‹èŒ‚éœ–&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Caleb-Sun-jz"&gt;å­™å¥å£®&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¯¹å¤–ç»æµè´¸æ˜“å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LucaChen"&gt;ä¸œä¸œ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-è°·æ­Œå¼€å‘è€…æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸“å®¶ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yeyeyeyeeeee"&gt;èéº¦&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Kailigithub"&gt;Kailigithub&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BaiYu96"&gt;éƒ‘çš“æ¡¦&lt;/a&gt; ï¼ˆå†…å®¹åˆ›ä½œè€…ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Joe-2002"&gt;ææŸ¯è¾°&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/chg0901"&gt;ç¨‹å®&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæ„å‘æˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anine09"&gt;éª†ç§€éŸ¬&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜-ä¼¼ç„¶å®éªŒå®¤ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Twosugar666"&gt;éƒ­å®£ä¼¯&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ilovexsir"&gt;è°¢å¥½å†‰&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jjyaoao"&gt;é™ˆæ€å·&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sanbuphy"&gt;æ•£æ­¥&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/thomas-yanxin"&gt;é¢œé‘«&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/study520ai520"&gt;æœæ£®&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜-å—é˜³ç†å·¥å­¦é™¢ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cswangxiaowei"&gt;Swiftie&lt;/a&gt; ï¼ˆå°ç±³NLPç®—æ³•å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KashiwaByte"&gt;é»„æŸç‰¹&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AXYZdong"&gt;å¼ å‹ä¸œ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/YangYu-NUAA"&gt;ä½™æ´‹&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Jin-Zhang-Yaoguang"&gt;å¼ æ™‹&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lta155"&gt;å¨„å¤©å¥¥&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-ä¸­å›½ç§‘å­¦é™¢å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LinChentang"&gt;å·¦æ˜¥ç”Ÿ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/little1d"&gt;æ¨å“&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lyj11111111"&gt;å°ç½—&lt;/a&gt; ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Kedreamix"&gt;é‚“æºä¿Š&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/XiLinky"&gt;èµµæ–‡æº&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¤ªåŸç†å·¥å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/comfzy"&gt;ä»˜å¿—è¿œ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-æµ·å—å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/isaacahahah"&gt;éƒ‘è¿œå©§&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™-ç¦å·å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Bald0Wang"&gt;ç‹ç† æ˜&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LikeGiver"&gt;è°­é€¸ç‚&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¯¹å¤–ç»æµè´¸æ˜“å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pod2c"&gt;ä½•è‡³è½©&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jodie-kang"&gt;åº·å©§æ·‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sssanssss"&gt;ä¸‰æ°´&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langlibai66"&gt;æ¨æ™¨æ—­&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¤ªåŸç†å·¥å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/2710932616"&gt;èµµä¼Ÿ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gzhuuser"&gt;è‹å‘æ ‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¹¿å·å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/riannyway"&gt;é™ˆç¿&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-è¥¿äº¤åˆ©ç‰©æµ¦å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Feimike09"&gt;å¼ é¾™æ–&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anarchysaiko"&gt;å­™è¶…&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fanqiNO1"&gt;æ¨Šå¥‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-ä¸Šæµ·äº¤é€šå¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nusakom"&gt;å“å ‚è¶Š&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fancyboi999"&gt;fancy&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šæ’åæ ¹æ®è´¡çŒ®ç¨‹åº¦æ’åº&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;å…¶ä»–&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ç‰¹åˆ«æ„Ÿè°¢&lt;a href="https://github.com/Sm1les"&gt;@Sm1les&lt;/a&gt;å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ&lt;/li&gt; 
 &lt;li&gt;éƒ¨åˆ†loraä»£ç å’Œè®²è§£å‚è€ƒä»“åº“ï¼š&lt;a href="https://github.com/zyds/transformers-code.git"&gt;https://github.com/zyds/transformers-code.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;å¦‚æœæœ‰ä»»ä½•æƒ³æ³•å¯ä»¥è”ç³»æˆ‘ä»¬ DataWhale ä¹Ÿæ¬¢è¿å¤§å®¶å¤šå¤šæå‡º issue&lt;/li&gt; 
 &lt;li&gt;ç‰¹åˆ«æ„Ÿè°¢ä»¥ä¸‹ä¸ºæ•™ç¨‹åšå‡ºè´¡çŒ®çš„åŒå­¦ï¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center" style="margin-top: 30px;"&gt; 
 &lt;a href="https://github.com/datawhalechina/self-llm/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=datawhalechina/self-llm" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Star History&lt;/h3&gt; 
&lt;div align="center" style="margin-top: 30px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/star-history-202572.png" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>xiaoyaocz/dart_simple_live</title>
      <link>https://github.com/xiaoyaocz/dart_simple_live</link>
      <description>&lt;p&gt;ç®€ç®€å•å•çš„çœ‹ç›´æ’­&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;h3&gt;âš  æœ¬é¡¹ç›®ä¸æä¾›Releaseå®‰è£…åŒ…ï¼Œè¯·è‡ªè¡Œç¼–è¯‘åè¿è¡Œæµ‹è¯•ã€‚&lt;/h3&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img width="128" src="https://raw.githubusercontent.com/xiaoyaocz/dart_simple_live/master/assets/logo.png" alt="Simple Live logo" /&gt; &lt;/p&gt; 
&lt;h2 align="center"&gt;Simple Live&lt;/h2&gt; 
&lt;p align="center"&gt; ç®€ç®€å•å•çš„çœ‹ç›´æ’­ &lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/xiaoyaocz/dart_simple_live/master/assets/screenshot_light.jpg" alt="æµ…è‰²æ¨¡å¼" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/xiaoyaocz/dart_simple_live/master/assets/screenshot_dark.jpg" alt="æ·±è‰²æ¨¡å¼" /&gt;&lt;/p&gt; 
&lt;h2&gt;æ”¯æŒç›´æ’­å¹³å°ï¼š&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;è™ç‰™ç›´æ’­&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æ–—é±¼ç›´æ’­&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å“”å“©å“”å“©ç›´æ’­&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æŠ–éŸ³ç›´æ’­&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;APPæ”¯æŒå¹³å°&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Android&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; iOS&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Windows &lt;code&gt;BETA&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; MacOS &lt;code&gt;BETA&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Linux &lt;code&gt;BETA&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Android TV &lt;code&gt;BETA&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;é¡¹ç›®ç»“æ„&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;simple_live_core&lt;/code&gt; é¡¹ç›®æ ¸å¿ƒåº“ï¼Œå®ç°è·å–å„ä¸ªç½‘ç«™çš„ä¿¡æ¯åŠå¼¹å¹•ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;simple_live_console&lt;/code&gt; åŸºäºsimple_live_coreçš„æ§åˆ¶å°ç¨‹åºã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;simple_live_app&lt;/code&gt; åŸºäºæ ¸å¿ƒåº“å®ç°çš„Flutter APPå®¢æˆ·ç«¯ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;simple_live_tv_app&lt;/code&gt; åŸºäºæ ¸å¿ƒåº“å®ç°çš„Flutter Android TVå®¢æˆ·ç«¯ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ç¯å¢ƒ&lt;/h2&gt; 
&lt;p&gt;Flutter : &lt;code&gt;3.22&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;å‚è€ƒåŠå¼•ç”¨&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/xiaoyaocz/AllLive"&gt;AllLive&lt;/a&gt; &lt;code&gt;æœ¬é¡¹ç›®çš„C#ç‰ˆï¼Œæœ‰å…´è¶£å¯ä»¥çœ‹çœ‹&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/xiaoyaocz/dart_tars_protocol.git"&gt;dart_tars_protocol&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/wbt5/real-url"&gt;wbt5/real-url&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/lovelyyoshino/Bilibili-Live-API/raw/master/API.WebSocket.md"&gt;lovelyyoshino/Bilibili-Live-API&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/IsoaSFlus/danmaku"&gt;IsoaSFlus/danmaku&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/BacooTang/huya-danmu"&gt;BacooTang/huya-danmu&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/TarsCloud/Tars"&gt;TarsCloud/Tars&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/YunzhiYike/douyin-live"&gt;YunzhiYike/douyin-live&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/5ime/Tiktok_Signature"&gt;5ime/Tiktok_Signature&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;å£°æ˜&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®çš„æ‰€æœ‰åŠŸèƒ½éƒ½æ˜¯åŸºäºäº’è”ç½‘ä¸Šå…¬å¼€çš„èµ„æ–™å¼€å‘ï¼Œæ— ä»»ä½•ç ´è§£ã€é€†å‘å·¥ç¨‹ç­‰è¡Œä¸ºã€‚&lt;/p&gt; 
&lt;p&gt;æœ¬é¡¹ç›®ä»…ç”¨äºå­¦ä¹ äº¤æµç¼–ç¨‹æŠ€æœ¯ï¼Œä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºå•†ä¸šç›®çš„ã€‚å¦‚æœ‰ä»»ä½•å•†ä¸šè¡Œä¸ºï¼Œå‡ä¸æœ¬é¡¹ç›®æ— å…³ã€‚&lt;/p&gt; 
&lt;p&gt;å¦‚æœæœ¬é¡¹ç›®å­˜åœ¨ä¾µçŠ¯æ‚¨çš„åˆæ³•æƒç›Šçš„æƒ…å†µï¼Œè¯·åŠæ—¶ä¸å¼€å‘è€…è”ç³»ï¼Œå¼€å‘è€…å°†ä¼šåŠæ—¶åˆ é™¤æœ‰å…³å†…å®¹ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/codex</title>
      <link>https://github.com/openai/codex</link>
      <description>&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;OpenAI Codex CLI&lt;/h1&gt; 
&lt;p align="center"&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer.&lt;br /&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, see &lt;a href="https://chatgpt.com/codex"&gt;chatgpt.com/codex&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png" alt="Codex CLI splash" width="50%" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;!-- Begin ToC --&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#quickstart"&gt;Quickstart&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#installing-and-running-codex-cli"&gt;Installing and running Codex CLI&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#using-codex-with-your-chatgpt-plan"&gt;Using Codex with your ChatGPT plan&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#connecting-through-vps-or-remote"&gt;Connecting through VPS or remote&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#usage-based-billing-alternative-use-an-openai-api-key"&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#choosing-codexs-level-of-autonomy"&gt;Choosing Codex's level of autonomy&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#1-readwrite"&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#2-read-only"&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#3-advanced-configuration"&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#can-i-run-without-any-approvals"&gt;Can I run without ANY approvals?&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#fine-tuning-in-configtoml"&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#example-prompts"&gt;Example prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#running-with-a-prompt-as-input"&gt;Running with a prompt as input&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#using-open-source-models"&gt;Using Open Source Models&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#platform-sandboxing-details"&gt;Platform sandboxing details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#experimental-technology-disclaimer"&gt;Experimental technology disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#system-requirements"&gt;System requirements&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#cli-reference"&gt;CLI reference&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#memory--project-docs"&gt;Memory &amp;amp; project docs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#non-interactive--ci-mode"&gt;Non-interactive / CI mode&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#tracing--verbose-logging"&gt;Tracing / verbose logging&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#dotslash"&gt;DotSlash&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#zero-data-retention-zdr-usage"&gt;Zero data retention (ZDR) usage&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#codex-open-source-fund"&gt;Codex open source fund&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#contributing"&gt;Contributing&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#development-workflow"&gt;Development workflow&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#writing-high-impact-code-changes"&gt;Writing high-impact code changes&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#opening-a-pull-request"&gt;Opening a pull request&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#review-process"&gt;Review process&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#community-values"&gt;Community values&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#getting-help"&gt;Getting help&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#contributor-license-agreement-cla"&gt;Contributor license agreement (CLA)&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#quick-fixes"&gt;Quick fixes&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#releasing-codex"&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#security--responsible-ai"&gt;Security &amp;amp; responsible AI&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/codex/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- End ToC --&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; 
&lt;p&gt;Install globally with your preferred package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @openai/codex  # Alternatively: `brew install codex`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can also go to the &lt;a href="https://github.com/openai/codex/releases/latest"&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; 
 &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS 
   &lt;ul&gt; 
    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Linux 
   &lt;ul&gt; 
    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-login.png" alt="Codex CLI login" width="50%" /&gt; &lt;/p&gt; 
&lt;p&gt;After you run &lt;code&gt;codex&lt;/code&gt; select Sign in with ChatGPT. You'll need a Plus, Pro, or Team ChatGPT account, and will get access to our latest models, including &lt;code&gt;gpt-5&lt;/code&gt;, at no extra cost to your plan. (Enterprise is coming soon.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Important: If you've used the Codex CLI before, you'll need to follow these steps to migrate from usage-based billing with your API key:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Update the CLI with &lt;code&gt;codex update&lt;/code&gt; and ensure &lt;code&gt;codex --version&lt;/code&gt; is greater than 0.13&lt;/li&gt; 
  &lt;li&gt;Ensure that there is no &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable set. (Check that &lt;code&gt;env | grep 'OPENAI_API_KEY'&lt;/code&gt; returns empty)&lt;/li&gt; 
  &lt;li&gt;Run &lt;code&gt;codex login&lt;/code&gt; again&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter problems with the login flow, please comment on &lt;a href="https://github.com/openai/codex/issues/1243"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Connecting through VPS or remote&lt;/h3&gt; 
&lt;p&gt;If you run Codex on a remote machine (VPS/server) without a local browser, the login helper starts a server on &lt;code&gt;localhost:1455&lt;/code&gt; on the remote host. To complete login in your local browser, forward that port to your machine before starting the login flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# From your local machine
ssh -L 1455:localhost:1455 &amp;lt;user&amp;gt;@&amp;lt;remote-host&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, in that SSH session, run &lt;code&gt;codex&lt;/code&gt; and select "Sign in with ChatGPT". When prompted, open the printed URL (it will be &lt;code&gt;http://localhost:1455/...&lt;/code&gt;) in your local browser. The traffic will be tunneled to the remote server.&lt;/p&gt; 
&lt;h3&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/h3&gt; 
&lt;p&gt;If you prefer to pay-as-you-go, you can still authenticate with your OpenAI API key by setting it as an environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;export OPENAI_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;This command only sets the key for your current terminal session, which we recommend. To set it for all future sessions, you can also add the &lt;code&gt;export&lt;/code&gt; line to your shell's configuration file (e.g., &lt;code&gt;~/.zshrc&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;If you have signed in with ChatGPT, Codex will default to using your ChatGPT credits. If you wish to use your API key, use the &lt;code&gt;/logout&lt;/code&gt; command to clear your ChatGPT authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Choosing Codex's level of autonomy&lt;/h3&gt; 
&lt;p&gt;We always recommend running Codex in its default sandbox that gives you strong guardrails around what the agent can do. The default sandbox prevents it from editing files outside its workspace, or from accessing the network.&lt;/p&gt; 
&lt;p&gt;When you launch Codex in a new folder, it detects whether the folder is version controlled and recommends one of two levels of autonomy:&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Codex can run commands and write files in the workspace without approval.&lt;/li&gt; 
 &lt;li&gt;To write files in other folders, access network, update git or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; 
 &lt;li&gt;By default, the workspace includes the current directory, as well as temporary directories like &lt;code&gt;/tmp&lt;/code&gt;. You can see what directories are in the workspace with the &lt;code&gt;/status&lt;/code&gt; command. See the docs for how to customize this behavior.&lt;/li&gt; 
 &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox workspace-write --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;This is the recommended default for version-controlled folders.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Codex can run read-only commands without approval.&lt;/li&gt; 
 &lt;li&gt;To edit files, access network, or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; 
 &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox read-only --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;This is the recommended default non-version-controlled folders.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Codex gives you fine-grained control over the sandbox with the &lt;code&gt;--sandbox&lt;/code&gt; option, and over when it requests approval with the &lt;code&gt;--ask-for-approval&lt;/code&gt; option. Run &lt;code&gt;codex help&lt;/code&gt; for more on these options.&lt;/p&gt; 
&lt;h4&gt;Can I run without ANY approvals?&lt;/h4&gt; 
&lt;p&gt;Yes, run codex non-interactively with &lt;code&gt;--ask-for-approval never&lt;/code&gt;. This option works with all &lt;code&gt;--sandbox&lt;/code&gt; options, so you still have full control over Codex's level of autonomy. It will make its best attempt with whatever contrainsts you provide. For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox read-only&lt;/code&gt; when you are running many agents to answer questions in parallel in the same workspace.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox workspace-write&lt;/code&gt; when you want the agent to non-interactively take time to produce the best outcome, with strong guardrails around its behavior.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox danger-full-access&lt;/code&gt; to dangerously give the agent full autonomy. Because this disables important safety mechanisms, we recommend against using this unless running Codex in an isolated environment.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# approval mode
approval_policy = "untrusted"
sandbox_mode    = "read-only"

# full-auto mode
approval_policy = "on-request"
sandbox_mode    = "workspace-write"

# Optional: allow network in workspace-write mode
[sandbox_workspace_write]
network_access = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also save presets as &lt;strong&gt;profiles&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[profiles.full_auto]
approval_policy = "on-request"
sandbox_mode    = "workspace-write"

[profiles.readonly_quiet]
approval_policy = "never"
sandbox_mode    = "read-only"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example prompts&lt;/h3&gt; 
&lt;p&gt;Below are a few bite-size examples you can copy-paste. Replace the text in quotes with your own task. See the &lt;a href="https://github.com/openai/codex/raw/main/codex-cli/examples/prompting_guide.md"&gt;prompting guide&lt;/a&gt; for more tips and usage patterns.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;âœ¨&lt;/th&gt; 
   &lt;th&gt;What you type&lt;/th&gt; 
   &lt;th&gt;What happens&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Refactor the Dashboard component to React Hooks"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Codex rewrites the class component, runs &lt;code&gt;npm test&lt;/code&gt;, and shows the diff.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Generate SQL migrations for adding a users table"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Infers your ORM, creates migration files, and runs them in a sandboxed DB.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Write unit tests for utils/date.ts"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Generates tests, executes them, and iterates until they pass.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Bulk-rename *.jpeg -&amp;gt; *.jpg with git mv"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Safely renames files and updates imports/usages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Explain what this regex does: ^(?=.*[A-Z]).{8,}$"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Outputs a step-by-step human explanation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Carefully review this repo, and propose 3 high impact well-scoped PRs"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Suggests impactful PRs in the current codebase.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "Look for vulnerabilities and create a security review report"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Finds and explains security bugs.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Running with a prompt as input&lt;/h2&gt; 
&lt;p&gt;You can also run Codex CLI with a prompt as input:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex "explain this codebase to me"
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;codex --full-auto "create the fanciest todo-list app"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it - Codex will scaffold a file, run it inside a sandbox, install any missing dependencies, and show you the live result. Approve the changes and they'll be committed to your working directory.&lt;/p&gt; 
&lt;h2&gt;Using Open Source Models&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Use &lt;code&gt;--profile&lt;/code&gt; to use other models&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Codex also allows you to use other providers that support the OpenAI Chat Completions (or Responses) API.&lt;/p&gt; 
 &lt;p&gt;To do so, you must first define custom &lt;a href="https://raw.githubusercontent.com/openai/codex/main/config.md#model_providers"&gt;providers&lt;/a&gt; in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. For example, the provider for a standard Ollama setup would be defined as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.ollama]
name = "Ollama"
base_url = "http://localhost:11434/v1"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;base_url&lt;/code&gt; will have &lt;code&gt;/chat/completions&lt;/code&gt; appended to it to build the full URL for the request.&lt;/p&gt; 
 &lt;p&gt;For providers that also require an &lt;code&gt;Authorization&lt;/code&gt; header of the form &lt;code&gt;Bearer: SECRET&lt;/code&gt;, an &lt;code&gt;env_key&lt;/code&gt; can be specified, which indicates the environment variable to read to use as the value of &lt;code&gt;SECRET&lt;/code&gt; when making a request:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.openrouter]
name = "OpenRouter"
base_url = "https://openrouter.ai/api/v1"
env_key = "OPENROUTER_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Providers that speak the Responses API are also supported by adding &lt;code&gt;wire_api = "responses"&lt;/code&gt; as part of the definition. Accessing OpenAI models via Azure is an example of such a provider, though it also requires specifying additional &lt;code&gt;query_params&lt;/code&gt; that need to be appended to the request URL:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.azure]
name = "Azure"
# Make sure you set the appropriate subdomain for this URL.
base_url = "https://YOUR_PROJECT_NAME.openai.azure.com/openai"
env_key = "AZURE_OPENAI_API_KEY"  # Or "OPENAI_API_KEY", whichever you use.
# Newer versions appear to support the responses API, see https://github.com/openai/codex/pull/1321
query_params = { api-version = "2025-04-01-preview" }
wire_api = "responses"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Once you have defined a provider you wish to use, you can configure it as your default provider as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;model_provider = "azure"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!TIP] If you find yourself experimenting with a variety of models and providers, then you likely want to invest in defining a &lt;em&gt;profile&lt;/em&gt; for each configuration like so:&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-toml"&gt;[profiles.o3]
model_provider = "azure"
model = "o3"

[profiles.mistral]
model_provider = "ollama"
model = "mistral"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This way, you can specify one command-line argument (.e.g., &lt;code&gt;--profile o3&lt;/code&gt;, &lt;code&gt;--profile mistral&lt;/code&gt;) to override multiple settings together.&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;Codex can run fully locally against an OpenAI-compatible OSS host (like Ollama) using the &lt;code&gt;--oss&lt;/code&gt; flag:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interactive UI: 
  &lt;ul&gt; 
   &lt;li&gt;codex --oss&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Non-interactive (programmatic) mode: 
  &lt;ul&gt; 
   &lt;li&gt;echo "Refactor utils" | codex exec --oss&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Model selection when using &lt;code&gt;--oss&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you omit &lt;code&gt;-m/--model&lt;/code&gt;, Codex defaults to -m gpt-oss:20b and will verify it exists locally (downloading if needed).&lt;/li&gt; 
 &lt;li&gt;To pick a different size, pass one of: 
  &lt;ul&gt; 
   &lt;li&gt;-m "gpt-oss:20b"&lt;/li&gt; 
   &lt;li&gt;-m "gpt-oss:120b"&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Point Codex at your own OSS host:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;By default, &lt;code&gt;--oss&lt;/code&gt; talks to &lt;a href="http://localhost:11434/v1"&gt;http://localhost:11434/v1&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To use a different host, set one of these environment variables before running Codex: 
  &lt;ul&gt; 
   &lt;li&gt;CODEX_OSS_BASE_URL, for example: 
    &lt;ul&gt; 
     &lt;li&gt;CODEX_OSS_BASE_URL="&lt;a href="http://my-ollama.example.com:11434/v1"&gt;http://my-ollama.example.com:11434/v1&lt;/a&gt;" codex --oss -m gpt-oss:20b&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;or CODEX_OSS_PORT (when the host is localhost): 
    &lt;ul&gt; 
     &lt;li&gt;CODEX_OSS_PORT=11434 codex --oss&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Advanced: you can persist this in your config instead of environment variables by overriding the built-in &lt;code&gt;oss&lt;/code&gt; provider in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[model_providers.oss]
name = "Open Source"
base_url = "http://my-ollama.example.com:11434/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Platform sandboxing details&lt;/h3&gt; 
&lt;p&gt;The mechanism Codex uses to implement the sandbox policy depends on your OS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS 12+&lt;/strong&gt; uses &lt;strong&gt;Apple Seatbelt&lt;/strong&gt; and runs commands using &lt;code&gt;sandbox-exec&lt;/code&gt; with a profile (&lt;code&gt;-p&lt;/code&gt;) that corresponds to the &lt;code&gt;--sandbox&lt;/code&gt; that was specified.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; uses a combination of Landlock/seccomp APIs to enforce the &lt;code&gt;sandbox&lt;/code&gt; configuration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that when running Linux in a containerized environment such as Docker, sandboxing may not work if the host/container configuration does not support the necessary Landlock/seccomp APIs. In such cases, we recommend configuring your Docker container so that it provides the sandbox guarantees you are looking for and then running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--sandbox danger-full-access&lt;/code&gt; (or, more simply, the &lt;code&gt;--dangerously-bypass-approvals-and-sandbox&lt;/code&gt; flag) within your container.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Experimental technology disclaimer&lt;/h2&gt; 
&lt;p&gt;Codex CLI is an experimental project under active development. It is not yet stable, may contain bugs, incomplete features, or undergo breaking changes. We're building it in the open with the community and welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Bug reports&lt;/li&gt; 
 &lt;li&gt;Feature requests&lt;/li&gt; 
 &lt;li&gt;Pull requests&lt;/li&gt; 
 &lt;li&gt;Good vibes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Help us improve by filing issues or submitting PRs (see the section below for how to contribute)!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;System requirements&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Requirement&lt;/th&gt; 
   &lt;th&gt;Details&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operating systems&lt;/td&gt; 
   &lt;td&gt;macOS 12+, Ubuntu 20.04+/Debian 10+, or Windows 11 &lt;strong&gt;via WSL2&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Git (optional, recommended)&lt;/td&gt; 
   &lt;td&gt;2.23+ for built-in PR helpers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RAM&lt;/td&gt; 
   &lt;td&gt;4-GB minimum (8-GB recommended)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;CLI reference&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Interactive TUI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex "..."&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for interactive TUI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex "fix lint errors"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex exec "..."&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Non-interactive "automation mode"&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex exec "explain utils.ts"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Key flags: &lt;code&gt;--model/-m&lt;/code&gt;, &lt;code&gt;--ask-for-approval/-a&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Memory &amp;amp; project docs&lt;/h2&gt; 
&lt;p&gt;You can give Codex extra instructions and guidance using &lt;code&gt;AGENTS.md&lt;/code&gt; files. Codex looks for &lt;code&gt;AGENTS.md&lt;/code&gt; files in the following places, and merges them top-down:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;~/.codex/AGENTS.md&lt;/code&gt; - personal global guidance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; at repo root - shared project notes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; in the current working directory - sub-folder/feature specifics&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Non-interactive / CI mode&lt;/h2&gt; 
&lt;p&gt;Run Codex head-less in pipelines. Example GitHub Action step:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: Update changelog via Codex
  run: |
    npm install -g @openai/codex
    export OPENAI_API_KEY="${{ secrets.OPENAI_KEY }}"
    codex exec --full-auto "update CHANGELOG for next release"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;The Codex CLI can be configured to leverage MCP servers by defining an &lt;a href="https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#mcp_servers"&gt;&lt;code&gt;mcp_servers&lt;/code&gt;&lt;/a&gt; section in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. It is intended to mirror how tools such as Claude and Cursor define &lt;code&gt;mcpServers&lt;/code&gt; in their respective JSON config files, though the Codex format is slightly different since it uses TOML rather than JSON, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# IMPORTANT: the top-level key is `mcp_servers` rather than `mcpServers`.
[mcp_servers.server-name]
command = "npx"
args = ["-y", "mcp-server"]
env = { "API_KEY" = "value" }
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] It is somewhat experimental, but the Codex CLI can also be run as an MCP &lt;em&gt;server&lt;/em&gt; via &lt;code&gt;codex mcp&lt;/code&gt;. If you launch it with an MCP client such as &lt;code&gt;npx @modelcontextprotocol/inspector codex mcp&lt;/code&gt; and send it a &lt;code&gt;tools/list&lt;/code&gt; request, you will see that there is only one tool, &lt;code&gt;codex&lt;/code&gt;, that accepts a grab-bag of inputs, including a catch-all &lt;code&gt;config&lt;/code&gt; map for anything you might want to override. Feel free to play around with it and provide feedback via GitHub issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Tracing / verbose logging&lt;/h2&gt; 
&lt;p&gt;Because Codex is written in Rust, it honors the &lt;code&gt;RUST_LOG&lt;/code&gt; environment variable to configure its logging behavior.&lt;/p&gt; 
&lt;p&gt;The TUI defaults to &lt;code&gt;RUST_LOG=codex_core=info,codex_tui=info&lt;/code&gt; and log messages are written to &lt;code&gt;~/.codex/log/codex-tui.log&lt;/code&gt;, so you can leave the following running in a separate terminal to monitor log messages as they are written:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tail -F ~/.codex/log/codex-tui.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By comparison, the non-interactive mode (&lt;code&gt;codex exec&lt;/code&gt;) defaults to &lt;code&gt;RUST_LOG=error&lt;/code&gt;, but messages are printed inline, so there is no need to monitor a separate file.&lt;/p&gt; 
&lt;p&gt;See the Rust documentation on &lt;a href="https://docs.rs/env_logger/latest/env_logger/#enabling-logging"&gt;&lt;code&gt;RUST_LOG&lt;/code&gt;&lt;/a&gt; for more information on the configuration options.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;DotSlash&lt;/h3&gt; 
&lt;p&gt;The GitHub Release also contains a &lt;a href="https://dotslash-cli.com/"&gt;DotSlash&lt;/a&gt; file for the Codex CLI named &lt;code&gt;codex&lt;/code&gt;. Using a DotSlash file makes it possible to make a lightweight commit to source control to ensure all contributors use the same version of an executable, regardless of what platform they use for development.&lt;/p&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Build from source&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository and navigate to the root of the Cargo workspace.
git clone https://github.com/openai/codex.git
cd codex/codex-rs

# Install the Rust toolchain, if necessary.
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
source "$HOME/.cargo/env"
rustup component add rustfmt
rustup component add clippy

# Build Codex.
cargo build

# Launch the TUI with a sample prompt.
cargo run --bin codex -- "explain this codebase to me"

# After making changes, ensure the code is clean.
cargo fmt -- --config imports_granularity=Item
cargo clippy --tests

# Run the tests.
cargo test
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Codex supports a rich set of configuration options documented in &lt;a href="https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md"&gt;&lt;code&gt;codex-rs/config.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;By default, Codex loads its configuration from &lt;code&gt;~/.codex/config.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Though &lt;code&gt;--config&lt;/code&gt; can be used to set/override ad-hoc config values for individual invocations of &lt;code&gt;codex&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenAI released a model called Codex in 2021 - is this related?&lt;/summary&gt; 
 &lt;p&gt;In 2021, OpenAI released Codex, an AI system designed to generate code from natural language prompts. That original Codex model was deprecated as of March 2023 and is separate from the CLI tool.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which models are supported?&lt;/summary&gt; 
 &lt;p&gt;Any model available with &lt;a href="https://platform.openai.com/docs/api-reference/responses"&gt;Responses API&lt;/a&gt;. The default is &lt;code&gt;o4-mini&lt;/code&gt;, but pass &lt;code&gt;--model gpt-4.1&lt;/code&gt; or set &lt;code&gt;model: gpt-4.1&lt;/code&gt; in your config file to override.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why does &lt;code&gt;o3&lt;/code&gt; or &lt;code&gt;o4-mini&lt;/code&gt; not work for me?&lt;/summary&gt; 
 &lt;p&gt;It's possible that your &lt;a href="https://help.openai.com/en/articles/10910291-api-organization-verification"&gt;API account needs to be verified&lt;/a&gt; in order to start streaming responses and seeing chain of thought summaries from the API. If you're still running into issues, please let us know!&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I stop Codex from editing my files?&lt;/summary&gt; 
 &lt;p&gt;Codex runs model-generated commands in a sandbox. If a proposed command or file change doesn't look right, you can simply type &lt;strong&gt;n&lt;/strong&gt; to deny the command or give the model feedback.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Does it work on Windows?&lt;/summary&gt; 
 &lt;p&gt;Not directly. It requires &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;Windows Subsystem for Linux (WSL2)&lt;/a&gt; - Codex has been tested on macOS and Linux with Node 22.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Zero data retention (ZDR) usage&lt;/h2&gt; 
&lt;p&gt;Codex CLI &lt;strong&gt;does&lt;/strong&gt; support OpenAI organizations with &lt;a href="https://platform.openai.com/docs/guides/your-data#zero-data-retention"&gt;Zero Data Retention (ZDR)&lt;/a&gt; enabled. If your OpenAI organization has Zero Data Retention enabled and you still encounter errors such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OpenAI rejected the request. Error details: Status: 400, Code: unsupported_parameter, Type: invalid_request_error, Message: 400 Previous response cannot be used for this organization due to Zero Data Retention.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ensure you are running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--config disable_response_storage=true&lt;/code&gt; or add this line to &lt;code&gt;~/.codex/config.toml&lt;/code&gt; to avoid specifying the command line option each time:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;disable_response_storage = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#disable_response_storage"&gt;the configuration documentation on &lt;code&gt;disable_response_storage&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Codex open source fund&lt;/h2&gt; 
&lt;p&gt;We're excited to launch a &lt;strong&gt;$1 million initiative&lt;/strong&gt; supporting open source projects that use Codex CLI and other OpenAI models.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Grants are awarded up to &lt;strong&gt;$25,000&lt;/strong&gt; API credits.&lt;/li&gt; 
 &lt;li&gt;Applications are reviewed &lt;strong&gt;on a rolling basis&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Interested? &lt;a href="https://openai.com/form/codex-open-source-fund/"&gt;Apply here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project is under active development and the code will likely change pretty significantly. We'll update this message once that's complete!&lt;/p&gt; 
&lt;p&gt;More broadly we welcome contributions - whether you are opening your very first pull request or you're a seasoned maintainer. At the same time we care about reliability and long-term maintainability, so the bar for merging code is intentionally &lt;strong&gt;high&lt;/strong&gt;. The guidelines below spell out what "high-quality" means in practice and should make the whole process transparent and friendly.&lt;/p&gt; 
&lt;h3&gt;Development workflow&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a &lt;em&gt;topic branch&lt;/em&gt; from &lt;code&gt;main&lt;/code&gt; - e.g. &lt;code&gt;feat/interactive-prompt&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Keep your changes focused. Multiple unrelated fixes should be opened as separate PRs.&lt;/li&gt; 
 &lt;li&gt;Following the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/#development-workflow"&gt;development setup&lt;/a&gt; instructions above, ensure your change is free of lint warnings and test failures.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Writing high-impact code changes&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Start with an issue.&lt;/strong&gt; Open a new one or comment on an existing discussion so we can agree on the solution before code is written.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add or update tests.&lt;/strong&gt; Every new feature or bug-fix should come with test coverage that fails before your change and passes afterwards. 100% coverage is not required, but aim for meaningful assertions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document behaviour.&lt;/strong&gt; If your change affects user-facing behaviour, update the README, inline help (&lt;code&gt;codex --help&lt;/code&gt;), or relevant example projects.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Keep commits atomic.&lt;/strong&gt; Each commit should compile and the tests should pass. This makes reviews and potential rollbacks easier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Opening a pull request&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fill in the PR template (or include similar information) - &lt;strong&gt;What? Why? How?&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Run &lt;strong&gt;all&lt;/strong&gt; checks locally (&lt;code&gt;cargo test &amp;amp;&amp;amp; cargo clippy --tests &amp;amp;&amp;amp; cargo fmt -- --config imports_granularity=Item&lt;/code&gt;). CI failures that could have been caught locally slow down the process.&lt;/li&gt; 
 &lt;li&gt;Make sure your branch is up-to-date with &lt;code&gt;main&lt;/code&gt; and that you have resolved merge conflicts.&lt;/li&gt; 
 &lt;li&gt;Mark the PR as &lt;strong&gt;Ready for review&lt;/strong&gt; only when you believe it is in a merge-able state.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Review process&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;One maintainer will be assigned as a primary reviewer.&lt;/li&gt; 
 &lt;li&gt;We may ask for changes - please do not take this personally. We value the work, we just also value consistency and long-term maintainability.&lt;/li&gt; 
 &lt;li&gt;When there is consensus that the PR meets the bar, a maintainer will squash-and-merge.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Community values&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Be kind and inclusive.&lt;/strong&gt; Treat others with respect; we follow the &lt;a href="https://www.contributor-covenant.org/"&gt;Contributor Covenant&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Assume good intent.&lt;/strong&gt; Written communication is hard - err on the side of generosity.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Teach &amp;amp; learn.&lt;/strong&gt; If you spot something confusing, open an issue or PR with improvements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting help&lt;/h3&gt; 
&lt;p&gt;If you run into problems setting up the project, would like feedback on an idea, or just want to say &lt;em&gt;hi&lt;/em&gt; - please open a Discussion or jump into the relevant issue. We are happy to help.&lt;/p&gt; 
&lt;p&gt;Together we can make Codex CLI an incredible tool. &lt;strong&gt;Happy hacking!&lt;/strong&gt; &lt;span&gt;ğŸš€&lt;/span&gt;&lt;/p&gt; 
&lt;h3&gt;Contributor license agreement (CLA)&lt;/h3&gt; 
&lt;p&gt;All contributors &lt;strong&gt;must&lt;/strong&gt; accept the CLA. The process is lightweight:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Open your pull request.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Paste the following comment (or reply &lt;code&gt;recheck&lt;/code&gt; if you've signed before):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-text"&gt;I have read the CLA Document and I hereby sign the CLA
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The CLA-Assistant bot records your signature in the repo and marks the status check as passed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;No special Git commands, email attachments, or commit footers required.&lt;/p&gt; 
&lt;h4&gt;Quick fixes&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Amend last commit&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git commit --amend -s --no-edit &amp;amp;&amp;amp; git push -f&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The &lt;strong&gt;DCO check&lt;/strong&gt; blocks merges until every commit in the PR carries the footer (with squash this is just the one).&lt;/p&gt; 
&lt;h3&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;For admins only.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Make sure you are on &lt;code&gt;main&lt;/code&gt; and have no local changes. Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;VERSION=0.2.0  # Can also be 0.2.0-alpha.1 or any valid Rust version.
./codex-rs/scripts/create_github_release.sh "$VERSION"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will make a local commit on top of &lt;code&gt;main&lt;/code&gt; with &lt;code&gt;version&lt;/code&gt; set to &lt;code&gt;$VERSION&lt;/code&gt; in &lt;code&gt;codex-rs/Cargo.toml&lt;/code&gt; (note that on &lt;code&gt;main&lt;/code&gt;, we leave the version as &lt;code&gt;version = "0.0.0"&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;This will push the commit using the tag &lt;code&gt;rust-v${VERSION}&lt;/code&gt;, which in turn kicks off &lt;a href="https://raw.githubusercontent.com/openai/codex/main/.github/workflows/rust-release.yml"&gt;the release workflow&lt;/a&gt;. This will create a new GitHub Release named &lt;code&gt;$VERSION&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If everything looks good in the generated GitHub Release, uncheck the &lt;strong&gt;pre-release&lt;/strong&gt; box so it is the latest release.&lt;/p&gt; 
&lt;p&gt;Create a PR to update &lt;a href="https://github.com/Homebrew/homebrew-core/raw/main/Formula/c/codex.rb"&gt;&lt;code&gt;Formula/c/codex.rb&lt;/code&gt;&lt;/a&gt; on Homebrew.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Security &amp;amp; responsible AI&lt;/h2&gt; 
&lt;p&gt;Have you discovered a vulnerability or have concerns about model output? Please e-mail &lt;strong&gt;&lt;a href="mailto:security@openai.com"&gt;security@openai.com&lt;/a&gt;&lt;/strong&gt; and we will respond promptly.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/openai/codex/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>nautechsystems/nautilus_trader</title>
      <link>https://github.com/nautechsystems/nautilus_trader</link>
      <description>&lt;p&gt;A high-performance algorithmic trading platform and event-driven backtester&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png" width="500" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://codecov.io/gh/nautechsystems/nautilus_trader"&gt;&lt;img src="https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://codspeed.io/nautechsystems/nautilus_trader"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="codspeed" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/nautilus_trader" alt="pythons" /&gt; &lt;img src="https://img.shields.io/pypi/v/nautilus_trader" alt="pypi-version" /&gt; &lt;img src="https://img.shields.io/pypi/format/nautilus_trader?color=blue" alt="pypi-format" /&gt; &lt;a href="https://pepy.tech/project/nautilus-trader"&gt;&lt;img src="https://pepy.tech/badge/nautilus-trader" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/NautilusTrader"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Branch&lt;/th&gt; 
   &lt;th align="left"&gt;Version&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;develop&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Rust&lt;/th&gt; 
   &lt;th align="left"&gt;Python&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13*&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* Windows builds are currently pinned to CPython 3.13.2, see &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/docs/getting_started/installation.md"&gt;installation guide&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://nautilustrader.io/docs/"&gt;https://nautilustrader.io/docs/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href="mailto:support@nautilustrader.io"&gt;support@nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform, providing quantitative traders with the ability to backtest portfolios of automated trading strategies on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.&lt;/p&gt; 
&lt;p&gt;The platform is &lt;em&gt;AI-first&lt;/em&gt;, designed to develop and deploy algorithmic trading strategies within a highly performant and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest environment consistent with the production live trading environment.&lt;/p&gt; 
&lt;p&gt;NautilusTrader's design, architecture, and implementation philosophy prioritizes software correctness and safety at the highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting and live deployment workloads.&lt;/p&gt; 
&lt;p&gt;The platform is also universal, and asset-class-agnostic â€” with any REST API or WebSocket feed able to be integrated via modular adapters. It supports high-frequency trading across a wide range of asset classes and instrument types including FX, Equities, Futures, Options, Crypto, DeFi, and Betting, enabling seamless operations across multiple venues simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png" alt="nautilus-trader" title="nautilus-trader" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Core is written in Rust with asynchronous networking using &lt;a href="https://crates.io/crates/tokio"&gt;tokio&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Portable&lt;/strong&gt;: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Modular adapters mean any REST API or WebSocket feed can be integrated.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Time in force &lt;code&gt;IOC&lt;/code&gt;, &lt;code&gt;FOK&lt;/code&gt;, &lt;code&gt;GTC&lt;/code&gt;, &lt;code&gt;GTD&lt;/code&gt;, &lt;code&gt;DAY&lt;/code&gt;, &lt;code&gt;AT_THE_OPEN&lt;/code&gt;, &lt;code&gt;AT_THE_CLOSE&lt;/code&gt;, advanced order types and conditional triggers. Execution instructions &lt;code&gt;post-only&lt;/code&gt;, &lt;code&gt;reduce-only&lt;/code&gt;, and icebergs. Contingency orders including &lt;code&gt;OCO&lt;/code&gt;, &lt;code&gt;OUO&lt;/code&gt;, &lt;code&gt;OTO&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Add user-defined custom components, or assemble entire systems from scratch leveraging the &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; and &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live&lt;/strong&gt;: Use identical strategy implementations between backtesting and live deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-venue&lt;/strong&gt;: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Training&lt;/strong&gt;: Backtest engine fast enough to be used to train AI trading agents (RL/ES).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png" alt="Alt text" title="nautilus" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;nautilus - from ancient Greek 'sailor' and naus 'ship'.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral. The idea is that this can be translated to the aesthetics of design and architecture.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Why NautilusTrader?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Highly performant event-driven Python&lt;/strong&gt;: Native binary core components.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parity between backtesting and live trading&lt;/strong&gt;: Identical strategy code.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reduced operational risk&lt;/strong&gt;: Enhanced risk management functionality, logical accuracy, and type safety.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Highly extendable&lt;/strong&gt;: Message bus, custom components and actors, custom data, custom adapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Traditionally, trading strategy research and backtesting might be conducted in Python using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot express the granular time and event dependent complexity of real-time trading, where compiled languages have proven to be more suitable due to their inherently higher performance, and type safety.&lt;/p&gt; 
&lt;p&gt;One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform have all been written entirely in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; or &lt;a href="https://cython.org/"&gt;Cython&lt;/a&gt;. This means we're using the right tools for the job, where systems programming languages compile performant binaries, with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.&lt;/p&gt; 
&lt;h2&gt;Why Python?&lt;/h2&gt; 
&lt;p&gt;Python was originally created decades ago as a simple scripting language with a clean straightforward syntax. It has since evolved into a fully fledged general purpose object-oriented programming language. Based on the TIOBE index, Python is currently the most popular programming language in the world. Not only that, Python has become the &lt;em&gt;de facto lingua franca&lt;/em&gt; of data science, machine learning, and artificial intelligence.&lt;/p&gt; 
&lt;p&gt;developer/user communities. However, Python has performance and typing limitations for large-scale, latency-sensitive systems. Cython addresses many of these issues by introducing static typing into Python's rich ecosystem of libraries and communities.&lt;/p&gt; 
&lt;h2&gt;Why Rust?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; is a multi-paradigm programming language designed for performance and safety, especially safe concurrency. Rust is "blazingly fast" and memory-efficient (comparable to C and C++) with no garbage collector. It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.&lt;/p&gt; 
&lt;p&gt;Rustâ€™s rich type system and ownership model guarantees memory-safety and thread-safety deterministically â€” eliminating many classes of bugs at compile-time.&lt;/p&gt; 
&lt;p&gt;The project increasingly utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and &lt;a href="https://pyo3.rs"&gt;PyO3&lt;/a&gt;â€”no Rust toolchain is required at install time.&lt;/p&gt; 
&lt;p&gt;This project makes the &lt;a href="https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html"&gt;Soundness Pledge&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â€œThe intent of this project is to be free of soundness bugs. The developers will do their best to avoid them, and welcome help in analyzing and fixing them.â€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;MSRV:&lt;/strong&gt; NautilusTrader relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is modularly designed to work with &lt;em&gt;adapters&lt;/em&gt;, enabling connectivity to trading venues and data providers by translating their raw APIs into a unified interface and normalized domain model.&lt;/p&gt; 
&lt;p&gt;The following integrations are currently supported; see &lt;a href="https://nautilustrader.io/docs/latest/integrations/"&gt;docs/integrations/&lt;/a&gt; for details:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Name&lt;/th&gt; 
   &lt;th align="left"&gt;ID&lt;/th&gt; 
   &lt;th align="left"&gt;Type&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
   &lt;th align="left"&gt;Docs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://betfair.com"&gt;Betfair&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BETFAIR&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sports Betting Exchange&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/betfair.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.com"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.us"&gt;Binance US&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.binance.com/en/futures"&gt;Binance Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bybit.com"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BYBIT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bybit.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.coinbase.com/en/international-exchange"&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;COINBASE_INTX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/coinbase_intx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://databento.com"&gt;Databento&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DATABENTO&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/databento.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://dydx.exchange/"&gt;dYdX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DYDX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/dydx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://hyperliquid.xyz"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;HYPERLIQUID&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/hyperliquid.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.interactivebrokers.com"&gt;Interactive Brokers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;INTERACTIVE_BROKERS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Brokerage (multi-venue)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/ib.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://okx.com"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;OKX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/beta-yellow" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/okx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://polymarket.com"&gt;Polymarket&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;POLYMARKET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Prediction Market (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/polymarket.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://tardis.dev"&gt;Tardis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TARDIS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/tardis.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;: The default client ID for the integrations adapter clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: The type of integration (often the venue type).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Status&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;building&lt;/code&gt;: Under construction and likely not in a usable state.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: Completed to a minimally working state and in a beta testing phase.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/integrations/index.html"&gt;Integrations&lt;/a&gt; documentation for further details.&lt;/p&gt; 
&lt;h2&gt;Versioning and releases&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;NautilusTrader is still under active development&lt;/strong&gt;. Some features may be incomplete, and while the API is becoming more stable, breaking changes can occur between releases. We strive to document these changes in the release notes on a &lt;strong&gt;best-effort basis&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;We aim to follow a &lt;strong&gt;bi-weekly release schedule&lt;/strong&gt;, though experimental or larger features may cause delays.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;p&gt;We aim to maintain a stable, passing build across all branches.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;master&lt;/code&gt;: Reflects the source code for the latest released version; recommended for production use.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt;: Daily snapshots of the &lt;code&gt;develop&lt;/code&gt; branch for early testing; merged at &lt;strong&gt;14:00 UTC&lt;/strong&gt; or on demand.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt;: Active development branch for contributors and feature work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md"&gt;roadmap&lt;/a&gt; aims to achieve a &lt;strong&gt;stable API for version 2.x&lt;/strong&gt; (likely after the Rust port). Once this milestone is reached, we plan to implement a formal deprecation process for any API changes. This approach allows us to maintain a rapid development pace for now.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Precision mode&lt;/h2&gt; 
&lt;p&gt;NautilusTrader supports two precision modes for its core value types (&lt;code&gt;Price&lt;/code&gt;, &lt;code&gt;Quantity&lt;/code&gt;, &lt;code&gt;Money&lt;/code&gt;), which differ in their internal bit-width and maximum decimal precision.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High-precision&lt;/strong&gt;: 128-bit integers with up to 16 decimals of precision, and a larger value range.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standard-precision&lt;/strong&gt;: 64-bit integers with up to 9 decimals of precision, and a smaller value range.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;By default, the official Python wheels &lt;strong&gt;ship&lt;/strong&gt; in high-precision (128-bit) mode on Linux and macOS. On Windows, only standard-precision (64-bit) is available due to the lack of native 128-bit integer support. For the Rust crates, the default is standard-precision unless you explicitly enable the &lt;code&gt;high-precision&lt;/code&gt; feature flag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rust feature flag&lt;/strong&gt;: To enable high-precision mode in Rust, add the &lt;code&gt;high-precision&lt;/code&gt; feature to your Cargo.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[dependencies]
nautilus_model = { version = "*", features = ["high-precision"] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using the latest supported version of Python and installing &lt;a href="https://pypi.org/project/nautilus_trader/"&gt;nautilus_trader&lt;/a&gt; inside a virtual environment to isolate dependencies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There are two supported ways to install&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pre-built binary wheel from PyPI &lt;em&gt;or&lt;/em&gt; the Nautech Systems package index.&lt;/li&gt; 
 &lt;li&gt;Build from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;We highly recommend installing using the &lt;a href="https://docs.astral.sh/uv"&gt;uv&lt;/a&gt; package manager with a "vanilla" CPython.&lt;/p&gt; 
 &lt;p&gt;Conda and other Python distributions &lt;em&gt;may&lt;/em&gt; work but arenâ€™t officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;p&gt;To install the latest binary wheel (or sdist package) from PyPI using Python's pip package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From the Nautech Systems package index&lt;/h3&gt; 
&lt;p&gt;The Nautech Systems package index (&lt;code&gt;packages.nautechsystems.io&lt;/code&gt;) complies with &lt;a href="https://peps.python.org/pep-0503/"&gt;PEP-503&lt;/a&gt; and hosts both stable and development binary wheels for &lt;code&gt;nautilus_trader&lt;/code&gt;. This enables users to install either the latest stable release or pre-release versions for testing.&lt;/p&gt; 
&lt;h4&gt;Stable wheels&lt;/h4&gt; 
&lt;p&gt;Stable wheels correspond to official releases of &lt;code&gt;nautilus_trader&lt;/code&gt; on PyPI, and use standard versioning.&lt;/p&gt; 
&lt;p&gt;To install the latest stable release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Development wheels&lt;/h4&gt; 
&lt;p&gt;Development wheels are published from both the &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;develop&lt;/code&gt; branches, allowing users to test features and fixes ahead of stable releases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Wheels from the &lt;code&gt;develop&lt;/code&gt; branch are only built for the Linux x86_64 platform to save time and compute resources, while &lt;code&gt;nightly&lt;/code&gt; wheels support additional platforms as shown below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Nightly&lt;/th&gt; 
   &lt;th align="left"&gt;Develop&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines, while adhering to &lt;a href="https://peps.python.org/pep-0440/"&gt;PEP-440&lt;/a&gt; versioning standards:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; wheels use the version format &lt;code&gt;dev{date}+{build_number}&lt;/code&gt; (e.g., &lt;code&gt;1.208.0.dev20241212+7001&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; wheels use the version format &lt;code&gt;a{date}&lt;/code&gt; (alpha) (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;We do not recommend using development wheels in production environments, such as live trading controlling real capital.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Installation commands&lt;/h4&gt; 
&lt;p&gt;By default, pip will install the latest stable release. Adding the &lt;code&gt;--pre&lt;/code&gt; flag ensures that pre-release versions, including development wheels, are considered.&lt;/p&gt; 
&lt;p&gt;To install the latest available pre-release (including development wheels):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install a specific development wheel (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt; for December 12, 2024):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Available versions&lt;/h4&gt; 
&lt;p&gt;You can view all available versions of &lt;code&gt;nautilus_trader&lt;/code&gt; on the &lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;package index&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To programmatically fetch and list available versions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP '(?&amp;lt;=&amp;lt;a href=")[^"]+(?=")' | awk -F'#' '{print $1}' | sort
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Branch updates&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): Build and publish continuously with every merged commit.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): Build and publish daily when we automatically merge the &lt;code&gt;develop&lt;/code&gt; branch at &lt;strong&gt;14:00 UTC&lt;/strong&gt; (if there are changes).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Retention policies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): We retain only the most recent wheel build.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): We retain only the 10 most recent wheel builds.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;It's possible to install from source using pip if you first install the build dependencies as specified in the &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://rustup.rs/"&gt;rustup&lt;/a&gt; (the Rust toolchain installer):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl https://sh.rustup.rs -sSf | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Download and install &lt;a href="https://win.rustup.rs/x86_64"&gt;&lt;code&gt;rustup-init.exe&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Install "Desktop development with C++" with &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;rustc --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;cargo&lt;/code&gt; in the current shell:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Start a new PowerShell&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://clang.llvm.org/"&gt;clang&lt;/a&gt; (a C language frontend for LLVM):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install clang
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ol&gt; 
     &lt;li&gt; &lt;p&gt;Add Clang to your &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;:&lt;/p&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Start | Visual Studio Installer | Modify | C++ Clang tools for Windows (12.0.0 - x64â€¦) = checked | Modify&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;clang&lt;/code&gt; in the current shell:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;[System.Environment]::SetEnvironmentVariable('path', "C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\Llvm\x64\bin\;" + $env:Path,"User")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;clang --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install uv (see the &lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;uv installation guide&lt;/a&gt; for more details):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source with &lt;code&gt;git&lt;/code&gt;, and install from the project's root directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone --branch develop --depth 1 https://github.com/nautechsystems/nautilus_trader
cd nautilus_trader
uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;--depth 1&lt;/code&gt; flag fetches just the latest commit for a faster, lightweight clone.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt; &lt;p&gt;Set environment variables for PyO3 compilation (Linux and macOS only):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Set the library path for the Python interpreter (in this case Python 3.13.4)
export LD_LIBRARY_PATH="$HOME/.local/share/uv/python/cpython-3.13.4-linux-x86_64-gnu/lib:$LD_LIBRARY_PATH"

# Set the Python executable path for PyO3
export PYO3_PYTHON=$(pwd)/.venv/bin/python
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Adjust the Python version and architecture in the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to match your system. Use &lt;code&gt;uv python list&lt;/code&gt; to find the exact path for your Python installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for other options and further details.&lt;/p&gt; 
&lt;h2&gt;Redis&lt;/h2&gt; 
&lt;p&gt;Using &lt;a href="https://redis.io"&gt;Redis&lt;/a&gt; with NautilusTrader is &lt;strong&gt;optional&lt;/strong&gt; and only required if configured as the backend for a &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; database or &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;. See the &lt;strong&gt;Redis&lt;/strong&gt; section of the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation#redis"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;h2&gt;Makefile&lt;/h2&gt; 
&lt;p&gt;A &lt;code&gt;Makefile&lt;/code&gt; is provided to automate most installation and build tasks for development. Some of the targets include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;make install&lt;/code&gt;: Installs in &lt;code&gt;release&lt;/code&gt; build mode with all dependency groups and extras.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-debug&lt;/code&gt;: Same as &lt;code&gt;make install&lt;/code&gt; but with &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-just-deps&lt;/code&gt;: Installs just the &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; dependencies (does not install package).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;: Runs the build script in &lt;code&gt;release&lt;/code&gt; build mode (default).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-debug&lt;/code&gt;: Runs the build script in &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;release&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel-debug&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;debug&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make cargo-test&lt;/code&gt;: Runs all Rust crate tests using &lt;code&gt;cargo-nextest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;: Deletes all build results, such as &lt;code&gt;.so&lt;/code&gt; or &lt;code&gt;.dll&lt;/code&gt; files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt;: &lt;strong&gt;CAUTION&lt;/strong&gt; Removes all artifacts not in the git index from the repository. This includes source files which have not been &lt;code&gt;git add&lt;/code&gt;ed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make docs&lt;/code&gt;: Builds the documentation HTML using Sphinx.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pre-commit&lt;/code&gt;: Runs the pre-commit checks over all files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make ruff&lt;/code&gt;: Runs ruff over all files using the &lt;code&gt;pyproject.toml&lt;/code&gt; config (with autofix).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pytest&lt;/code&gt;: Runs all tests with &lt;code&gt;pytest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make test-performance&lt;/code&gt;: Runs performance tests with &lt;a href="https://codspeed.io"&gt;codspeed&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make help&lt;/code&gt; for documentation on all available make targets.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;See the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/crates/infrastructure/TESTS.md"&gt;crates/infrastructure/TESTS.md&lt;/a&gt; file for running the infrastructure integration tests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Indicators and strategies can be developed in both Python and Cython. For performance and latency-sensitive applications, we recommend using Cython. Below are some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/indicators/ema_python.py"&gt;indicator&lt;/a&gt; example written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/indicators/"&gt;indicator&lt;/a&gt; examples written in Cython.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/strategies/"&gt;strategy&lt;/a&gt; examples written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/examples/backtest/"&gt;backtest&lt;/a&gt; examples using a &lt;code&gt;BacktestEngine&lt;/code&gt; directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;Docker containers are built using the base image &lt;code&gt;python:3.12-slim&lt;/code&gt; with the following variant tags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:latest&lt;/code&gt; has the latest release version installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:latest&lt;/code&gt; has the latest release version installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can pull the container images as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/&amp;lt;image_variant_tag&amp;gt; --platform linux/amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can launch the backtest example container by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/jupyterlab:nightly --platform linux/amd64
docker run -p 8888:8888 ghcr.io/nautechsystems/jupyterlab:nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open your browser at the following address:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;http://127.0.0.1:8888/lab
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader currently exceeds the rate limit for Jupyter notebook logging (stdout output). Therefore, we set the &lt;code&gt;log_level&lt;/code&gt; to &lt;code&gt;ERROR&lt;/code&gt; in the examples. Lowering this level to see more logging will cause the notebook to hang during cell execution. We are investigating a fix that may involve either raising the configured rate limits for Jupyter or throttling the log flushing from Nautilus.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jupyterlab/jupyterlab/issues/12845"&gt;https://github.com/jupyterlab/jupyterlab/issues/12845&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/deshaw/jupyterlab-limit-output"&gt;https://github.com/deshaw/jupyterlab-limit-output&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;We aim to provide the most pleasant developer experience possible for this hybrid codebase of Python, Cython and Rust. See the &lt;a href="https://nautilustrader.io/docs/latest/developer_guide/index.html"&gt;Developer Guide&lt;/a&gt; for helpful information.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make build-debug&lt;/code&gt; to compile after changes to Rust or Cython code for the most efficient development workflow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Testing with Rust&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://nexte.st"&gt;cargo-nextest&lt;/a&gt; is the standard Rust test runner for NautilusTrader. Its key benefit is isolating each test in its own process, ensuring test reliability by avoiding interference.&lt;/p&gt; 
&lt;p&gt;You can install cargo-nextest by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install cargo-nextest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run Rust tests with &lt;code&gt;make cargo-test&lt;/code&gt;, which uses &lt;strong&gt;cargo-nextest&lt;/strong&gt; with an efficient profile.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to NautilusTrader! We welcome any and all help to improve the project. If you have an idea for an enhancement or a bug fix, the first step is to open an &lt;a href="https://github.com/nautechsystems/nautilus_trader/issues"&gt;issue&lt;/a&gt; on GitHub to discuss it with the team. This helps to ensure that your contribution will be well-aligned with the goals of the project and avoids duplication of effort.&lt;/p&gt; 
&lt;p&gt;Before getting started, be sure to review the &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md#open-source-scope"&gt;open-source scope&lt;/a&gt; outlined in the projectâ€™s roadmap to understand whatâ€™s in and out of scope.&lt;/p&gt; 
&lt;p&gt;Once you're ready to start working on your contribution, make sure to follow the guidelines outlined in the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file. This includes signing a Contributor License Agreement (CLA) to ensure that your contributions can be included in the project.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Pull requests should target the &lt;code&gt;develop&lt;/code&gt; branch (the default branch). This is where new features and improvements are integrated before release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Thank you again for your interest in NautilusTrader! We look forward to reviewing your contributions and working with you to improve the project.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our community of users and contributors on &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord&lt;/a&gt; to chat and stay up-to-date with the latest announcements and features of NautilusTrader. Whether you're a developer looking to contribute or just want to learn more about the platform, all are welcome on our Discord server.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader does not issue, promote, or endorse any cryptocurrency tokens. Any claims or communications suggesting otherwise are unauthorized and false.&lt;/p&gt; 
 &lt;p&gt;All official updates and communications from NautilusTrader will be shared exclusively through &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;, our &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord server&lt;/a&gt;, or our X (Twitter) account: &lt;a href="https://x.com/NautilusTrader"&gt;@NautilusTrader&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;If you encounter any suspicious activity, please report it to the appropriate platform and contact us at &lt;a href="mailto:info@nautechsystems.io"&gt;info@nautechsystems.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The source code for NautilusTrader is available on GitHub under the &lt;a href="https://www.gnu.org/licenses/lgpl-3.0.en.html"&gt;GNU Lesser General Public License v3.0&lt;/a&gt;. Contributions to the project are welcome and require the completion of a standard &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CLA.md"&gt;Contributor License Agreement (CLA)&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;NautilusTraderâ„¢ is developed and maintained by Nautech Systems, a technology company specializing in the development of high-performance trading systems. For more information, visit &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Â© 2015-2025 Nautech Systems Pty Ltd. All rights reserved.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ns-logo.png" alt="nautechsystems" title="nautechsystems" /&gt; &lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ferris.png" width="128" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>polarsource/polar</title>
      <link>https://github.com/polarsource/polar</link>
      <description>&lt;p&gt;An open source engine for your digital products. Sell SaaS and digital products in minutes.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://polar.sh"&gt; &lt;img src="https://github.com/user-attachments/assets/89a588e5-0c58-429a-8bbe-20f70af41372" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.producthunt.com/posts/polar-5?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-polar-5" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=484271&amp;amp;theme=dark&amp;amp;period=daily" alt="Polar - An open source monetization platform for developers | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; &lt;a href="https://www.producthunt.com/posts/polar-5?embed=true&amp;amp;utm_source=badge-top-post-topic-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-polar-5" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=484271&amp;amp;theme=dark&amp;amp;period=monthly&amp;amp;topic_id=267" alt="Polar - An open source monetization platform for developers | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://polar.sh"&gt;Website&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/blog"&gt;Blog&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/docs"&gt;Docs&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://docs.polar.sh/api-reference"&gt;API Reference&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://discord.gg/Pnhfz3UThd"&gt; &lt;img src="https://img.shields.io/badge/chat-on%20discord-7289DA.svg?sanitize=true" alt="Discord Chat" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=polar_sh"&gt; &lt;img src="https://img.shields.io/twitter/follow/polar_sh.svg?label=Follow%20@polar_sh" alt="Follow @polar_sh" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Polar: Open Source payments infrastructure for the 21st century&lt;/h2&gt; 
&lt;p&gt;Focus on building your passion, while we focus on the infrastructure to get you paid.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Sell SaaS and digital products in minutes&lt;/li&gt; 
 &lt;li&gt;All-in-one funding &amp;amp; monetization platform for developers.&lt;/li&gt; 
 &lt;li&gt;Sell access to GitHub repositories, Discord Support channels, File Downloads, License Keys &amp;amp; much more with Digital Products &amp;amp; Subscriptions.&lt;/li&gt; 
 &lt;li&gt;We're the merchant of record handling the... 
  &lt;ul&gt; 
   &lt;li&gt;...boilerplate (billing, receipts, customer accounts etc)&lt;/li&gt; 
   &lt;li&gt;...headaches (sales tax, VAT)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Pricing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4% + 40Â¢&lt;/li&gt; 
 &lt;li&gt;No fixed monthly costs&lt;/li&gt; 
 &lt;li&gt;Additional fees may apply. &lt;a href="https://docs.polar.sh/documentation/polar-as-merchant-of-record/fees"&gt;Read more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap, Issues &amp;amp; Feature Requests&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ¯ Upcoming milestones.&lt;/strong&gt; &lt;a href="https://github.com/polarsource/polar/issues/3242"&gt;Check out what we're building towards&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ’¬ Shape the future of Polar with us.&lt;/strong&gt; &lt;a href="https://discord.gg/Pnhfz3UThd"&gt;Join our Discord&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ› Found a bug?&lt;/strong&gt; &lt;a href="https://github.com/polarsource/polar/issues"&gt;Submit it here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ”“ Found a security vulnerability?&lt;/strong&gt; We greatly appreciate responsible and private disclosures. See &lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/SECURITY.md"&gt;Security&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Polar API &amp;amp; SDK&lt;/h3&gt; 
&lt;p&gt;You can integrate Polar on your docs, sites or services using our &lt;a href="https://docs.polar.sh/api-reference"&gt;Public API&lt;/a&gt; and &lt;a href="https://docs.polar.sh/developers/webhooks"&gt;Webhook API&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We also maintain SDKs for the following languages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;JavaScript (Node.js and browsers): &lt;a href="https://github.com/polarsource/polar-js"&gt;polarsource/polar-js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Python: &lt;a href="https://github.com/polarsource/polar-python"&gt;polarsource/polar-python&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/DEVELOPMENT.md"&gt;&lt;code&gt;DEVELOPMENT.md&lt;/code&gt;&lt;/a&gt; file contains everything you need to know to configure your development environment.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Want to get started quickly? Use GitHub Codespaces.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://codespaces.new/polarsource/polar?machine=standardLinux32gb"&gt;&lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" alt="Open in GitHub Codespaces" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Contributors&lt;/h3&gt; 
&lt;a href="https://github.com/polarsource/polar/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=polarsource/polar" /&gt; &lt;/a&gt; 
&lt;h2&gt;Monorepo&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/server/README.md"&gt;server&lt;/a&gt;&lt;/strong&gt; â€“ Python / FastAPI / Dramatiq / SQLAlchemy (PostgreSQL) / Redis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/README.md"&gt;clients&lt;/a&gt;&lt;/strong&gt; â€“ Turborepo 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/apps/web"&gt;web&lt;/a&gt; (Dashboard) â€“ NextJS (TypeScript)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/packages/polarkit"&gt;polarkit&lt;/a&gt; - Shared React components&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;sub&gt;â™¥ï¸ğŸ™ To our &lt;code&gt;pyproject.toml&lt;/code&gt; friends: &lt;a href="https://github.com/tiangolo/fastapi"&gt;FastAPI&lt;/a&gt;, &lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt;, &lt;a href="https://github.com/Bogdanp/dramatiq"&gt;Dramatiq&lt;/a&gt;, &lt;a href="https://github.com/sqlalchemy/sqlalchemy"&gt;SQLAlchemy&lt;/a&gt;, &lt;a href="https://github.com/yanyongyu/githubkit"&gt;Githubkit&lt;/a&gt;, &lt;a href="https://github.com/sysid/sse-starlette"&gt;sse-starlette&lt;/a&gt;, &lt;a href="https://github.com/encode/uvicorn"&gt;Uvicorn&lt;/a&gt;, &lt;a href="https://github.com/frankie567/httpx-oauth"&gt;httpx-oauth&lt;/a&gt;, &lt;a href="https://github.com/pallets/jinja"&gt;jinja&lt;/a&gt;, &lt;a href="https://github.com/pallets-eco/blinker"&gt;blinker&lt;/a&gt;, &lt;a href="https://github.com/jpadilla/pyjwt"&gt;pyjwt&lt;/a&gt;, &lt;a href="https://github.com/getsentry/sentry"&gt;Sentry&lt;/a&gt; + more&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;â™¥ï¸ğŸ™ To our &lt;code&gt;package.json&lt;/code&gt; friends: &lt;a href="https://github.com/vercel/next.js/"&gt;Next.js&lt;/a&gt;, &lt;a href="https://github.com/TanStack/query"&gt;TanStack Query&lt;/a&gt;, &lt;a href="https://github.com/tailwindlabs/tailwindcss"&gt;tailwindcss&lt;/a&gt;, &lt;a href="https://github.com/pmndrs/zustand"&gt;zustand&lt;/a&gt;, &lt;a href="https://github.com/ferdikoomen/openapi-typescript-codegen"&gt;openapi-typescript-codegen&lt;/a&gt;, &lt;a href="https://github.com/axios/axios"&gt;axios&lt;/a&gt;, &lt;a href="https://github.com/radix-ui/primitives"&gt;radix-ui&lt;/a&gt;, &lt;a href="https://github.com/pacocoursey/cmdk"&gt;cmdk&lt;/a&gt;, &lt;a href="https://github.com/framer/motion"&gt;framer-motion&lt;/a&gt; + more&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;â™¥ï¸ğŸ™ To &lt;a href="https://ipinfo.io"&gt;IPinfo&lt;/a&gt; that provides IP address data to help us geolocate customers during checkout.&lt;/sub&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; 
&lt;p&gt;&lt;a href="https://github.com/lfnovo/open-notebook/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;&lt;img src="https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/lfnovo/open-notebook"&gt; &lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Open Notebook&lt;/h3&gt; 
 &lt;p align="center"&gt; An open source, privacy-focused alternative to Google's Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href="https://www.open-notebook.ai"&gt;&lt;strong&gt;Checkout our website Â»&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;ğŸ“š Get Started&lt;/a&gt; Â· &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md"&gt;ğŸ“– User Guide&lt;/a&gt; Â· &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md"&gt;âœ¨ Features&lt;/a&gt; Â· &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;ğŸš€ Deploy&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“¢ Open Notebook is under very active development&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Open Notebook is under active development! We're moving fast and making improvements every week. Your feedback is incredibly valuable to me during this exciting phase and it gives me motivation to keep improving and building this amazing tool. Please feel free to star the project if you find it useful, and don't hesitate to reach out with any questions or suggestions. I'm excited to see how you'll use it and what ideas you'll bring to the project! Let's build something amazing together! ğŸš€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;About The Project&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png" alt="New Notebook" /&gt;&lt;/p&gt; 
&lt;p&gt;An open source, privacy-focused alternative to Google's Notebook LM. Why give Google more of our data when we can take control of our own research workflows?&lt;/p&gt; 
&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think ğŸ§  and acquire new knowledge ğŸ’¡, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”’ &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; 
 &lt;li&gt;ğŸ™ï¸ &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about our project at &lt;a href="https://www.open-notebook.ai"&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ†š Open Notebook vs Google Notebook LM&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Open Notebook&lt;/th&gt; 
   &lt;th&gt;Google Notebook LM&lt;/th&gt; 
   &lt;th&gt;Advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-hosted, your data&lt;/td&gt; 
   &lt;td&gt;Google cloud only&lt;/td&gt; 
   &lt;td&gt;Complete data sovereignty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; 
   &lt;td&gt;Google models only&lt;/td&gt; 
   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; 
   &lt;td&gt;2 speakers only&lt;/td&gt; 
   &lt;td&gt;Extreme flexibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3 granular levels&lt;/td&gt; 
   &lt;td&gt;All-or-nothing&lt;/td&gt; 
   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom and built-in&lt;/td&gt; 
   &lt;td&gt;Limited options&lt;/td&gt; 
   &lt;td&gt;Unlimited processing power&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Full REST API&lt;/td&gt; 
   &lt;td&gt;No API&lt;/td&gt; 
   &lt;td&gt;Complete automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; 
   &lt;td&gt;Google hosted only&lt;/td&gt; 
   &lt;td&gt;Deploy anywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Comprehensive with sources&lt;/td&gt; 
   &lt;td&gt;Basic references&lt;/td&gt; 
   &lt;td&gt;Research integrity&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Open source, fully customizable&lt;/td&gt; 
   &lt;td&gt;Closed system&lt;/td&gt; 
   &lt;td&gt;Unlimited extensibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pay only for AI usage&lt;/td&gt; 
   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; 
   &lt;td&gt;Transparent and controllable&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”’ &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; 
 &lt;li&gt;ğŸ’° &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; 
 &lt;li&gt;ğŸ™ï¸ &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; 
 &lt;li&gt;ğŸŒ &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://surrealdb.com/"&gt;&lt;img src="https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white" alt="SurrealDB" /&gt;&lt;/a&gt; &lt;a href="https://www.langchain.com/"&gt;&lt;img src="https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white" alt="LangChain" /&gt;&lt;/a&gt; &lt;a href="https://streamlit.io/"&gt;&lt;img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Streamlit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;p&gt;Ready to try Open Notebook? Choose your preferred method:&lt;/p&gt; 
&lt;h3&gt;âš¡ Instant Setup (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a new directory for your Open Notebook installation
mkdir open-notebook
cd open-notebook

# Using Docker - Get started in 2 minutes
docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key \
  lfnovo/open_notebook:latest-single
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;open-notebook/
â”œâ”€â”€ notebook_data/     # Your notebooks and research content
â””â”€â”€ surreal_data/      # Database files
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Access your installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ–¥ï¸ Main Interface&lt;/strong&gt;: &lt;a href="http://localhost:8502"&gt;http://localhost:8502&lt;/a&gt; (Streamlit UI)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ API Access&lt;/strong&gt;: &lt;a href="http://localhost:5055"&gt;http://localhost:5055&lt;/a&gt; (REST API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“š API Documentation&lt;/strong&gt;: &lt;a href="http://localhost:5055/docs"&gt;http://localhost:5055/docs&lt;/a&gt; (Interactive Swagger UI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ Important&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Run from a dedicated folder&lt;/strong&gt;: Create and run this from inside a new &lt;code&gt;open-notebook&lt;/code&gt; folder so your data volumes are properly organized&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Volume persistence&lt;/strong&gt;: The volumes (&lt;code&gt;-v ./notebook_data:/app/data&lt;/code&gt; and &lt;code&gt;-v ./surreal_data:/mydata&lt;/code&gt;) are essential to persist your data between container restarts. Without them, you'll lose all your notebooks and research when the container stops.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ› ï¸ Full Installation&lt;/h3&gt; 
&lt;p&gt;For development or customization:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ“– Need Help?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href="https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant"&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; 
&lt;p&gt;Thanks to the &lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;LLM Support&lt;/th&gt; 
   &lt;th&gt;Embedding Support&lt;/th&gt; 
   &lt;th&gt;Speech-to-Text&lt;/th&gt; 
   &lt;th&gt;Text-to-Speech&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google (GenAI)&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertex AI&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perplexity&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Voyage&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; 
&lt;h2&gt;âœ¨ Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”’ Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¯ Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“š Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ™ï¸ Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ’¬ Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;âš¡ Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸŒ Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href="http://localhost:5055/docs"&gt;&lt;img src="https://img.shields.io/badge/API-Documentation-blue?style=flat-square" alt="API Docs" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“Š Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Three-Column Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=D-760MlGwaI"&gt;&lt;img src="https://img.youtube.com/vi/D-760MlGwaI/0.jpg" alt="Check out our podcast sample" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md"&gt;ğŸ“– Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;âš¡ Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;ğŸ”§ Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md"&gt;ğŸ¯ Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guide&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md"&gt;ğŸ“± Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md"&gt;ğŸ“š Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md"&gt;ğŸ“„ Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md"&gt;ğŸ“ Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md"&gt;ğŸ’¬ Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md"&gt;ğŸ” Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md"&gt;ğŸ™ï¸ Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md"&gt;ğŸ”§ Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md"&gt;ğŸ¤– AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md"&gt;ğŸ”§ REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md"&gt;ğŸ” Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;ğŸš€ Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;ğŸ—ºï¸ Roadmap&lt;/h2&gt; 
&lt;h3&gt;Upcoming Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;React Frontend&lt;/strong&gt;: Modern React-based frontend to replace Streamlit&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recently Completed âœ…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;&lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;â­ &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions! We're especially looking for help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help build a modern React-based UI (planned replacement for current Streamlit interface)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, SurrealDB, Streamlit&lt;br /&gt; &lt;strong&gt;Future Roadmap&lt;/strong&gt;: React frontend, enhanced real-time updates&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href="https://twitter.com/lfnovo"&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¬ &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;ğŸŒ &lt;a href="https://www.open-notebook.ai"&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™ Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/surreal-commands"&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/content-core"&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>sinaptik-ai/pandas-ai</title>
      <link>https://github.com/sinaptik-ai/pandas-ai</link>
      <description>&lt;p&gt;Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/logo.png" alt="PandasAI" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/pandasai/"&gt;&lt;img src="https://img.shields.io/pypi/v/pandasai?label=Release&amp;amp;style=flat-square" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg"&gt;&lt;img src="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg"&gt;&lt;img src="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg?sanitize=true" alt="CD" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/sinaptik-ai/pandas-ai"&gt;&lt;img src="https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/KYKj9F2FRH"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;amp;compact=true" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/pandasai"&gt;&lt;img src="https://static.pepy.tech/badge/pandasai" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;PandasAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.&lt;/p&gt; 
&lt;h1&gt;ğŸ”§ Getting started&lt;/h1&gt; 
&lt;p&gt;You can find the full documentation for PandasAI &lt;a href="https://pandas-ai.readthedocs.io/en/latest/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can either decide to use PandasAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.&lt;/p&gt; 
&lt;h2&gt;ğŸ“š Using the library&lt;/h2&gt; 
&lt;h3&gt;Python Requirements&lt;/h3&gt; 
&lt;p&gt;Python version &lt;code&gt;3.8+ &amp;lt;3.12&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ“¦ Installation&lt;/h3&gt; 
&lt;p&gt;You can install the PandasAI library using pip or poetry.&lt;/p&gt; 
&lt;p&gt;With pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "pandasai&amp;gt;=3.0.0b2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With poetry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry add "pandasai&amp;gt;=3.0.0b2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ’» Usage&lt;/h3&gt; 
&lt;h4&gt;Ask questions&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_openai.openai import OpenAI

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

# Sample DataFrame
df = pai.DataFrame({
    "country": ["United States", "United Kingdom", "France", "Germany", "Italy", "Spain", "Canada", "Australia", "Japan", "China"],
    "revenue": [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]
})

df.chat('Which are the top 5 countries by sales?')
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;China, United States, Japan, Germany, Australia
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;Or you can ask more complex questions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;df.chat(
    "What is the total sales for the top 3 countries by sales?"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;The total sales for the top 3 countries by sales is 16500.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Visualize charts&lt;/h4&gt; 
&lt;p&gt;You can also ask PandasAI to generate charts for you:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;df.chat(
    "Plot the histogram of countries showing for each one the gd. Use different colors for each bar",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/histogram-chart.png?raw=true" alt="Chart" /&gt;&lt;/p&gt; 
&lt;h4&gt;Multiple DataFrames&lt;/h4&gt; 
&lt;p&gt;You can also pass in multiple dataframes to PandasAI and ask questions relating them.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_openai.openai import OpenAI

employees_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],
    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']
}

salaries_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Salary': [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)


pai.chat("Who gets paid the most?", employees_df, salaries_df)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Sandbox&lt;/h4&gt; 
&lt;p&gt;You can run PandasAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.&lt;/p&gt; 
&lt;h5&gt;Python Requirements&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "pandasai-docker"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Usage&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_docker import DockerSandbox
from pandasai_openai.openai import OpenAI

# Initialize the sandbox
sandbox = DockerSandbox()
sandbox.start()

employees_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],
    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']
}

salaries_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Salary': [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

pai.chat("Who gets paid the most?", employees_df, salaries_df, sandbox=sandbox)

# Don't forget to stop the sandbox when done
sandbox.stop()
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find more examples in the &lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples"&gt;examples&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;ğŸ“œ License&lt;/h2&gt; 
&lt;p&gt;PandasAI is available under the MIT expat license, except for the &lt;code&gt;pandasai/ee&lt;/code&gt; directory of this repository, which has its &lt;a href="https://github.com/sinaptik-ai/pandas-ai/raw/main/ee/LICENSE"&gt;license here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested in managed PandasAI Cloud or self-hosted Enterprise Offering, &lt;a href="https://getpanda.ai/pricing"&gt;contact us&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Beta Notice&lt;/strong&gt;&lt;br /&gt; Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pandas-ai.readthedocs.io/en/latest/"&gt;Docs&lt;/a&gt; for comprehensive documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples"&gt;Examples&lt;/a&gt; for example notebooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/KYKj9F2FRH"&gt;Discord&lt;/a&gt; for discussion with the community and PandasAI team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please check the outstanding issues and feel free to open a pull request. For more information, please check out the &lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Thank you!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sinaptik-ai/pandas-ai/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zumerlab/snapdom</title>
      <link>https://github.com/zumerlab/snapdom</link>
      <description>&lt;p&gt;snapDOM captures HTML elements to images with exceptional speed and accuracy.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://zumerlab.github.io/snapdom"&gt; &lt;img src="https://raw.githubusercontent.com/zumerlab/snapdom/main/docs/assets/newhero.png" width="70%" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.npmjs.com/package/@zumer/snapdom"&gt; &lt;img alt="NPM version" src="https://img.shields.io/npm/v/@zumer/snapdom?style=flat-square&amp;amp;label=Version" /&gt; &lt;/a&gt; &lt;a href="https://github.com/zumerlab/snapdom/graphs/contributors"&gt; &lt;img alt="GitHub contributors" src="https://img.shields.io/github/contributors/zumerlab/snapdom?style=flat-square&amp;amp;label=Contributors" /&gt; &lt;/a&gt; &lt;a href="https://github.com/zumerlab/snapdom/stargazers"&gt; &lt;img alt="GitHub stars" src="https://img.shields.io/github/stars/zumerlab/snapdom?style=flat-square&amp;amp;label=Stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/zumerlab/snapdom/network/members"&gt; &lt;img alt="GitHub forks" src="https://img.shields.io/github/forks/zumerlab/snapdom?style=flat-square&amp;amp;label=Forks" /&gt; &lt;/a&gt; &lt;a href="https://github.com/sponsors/tinchox5"&gt; &lt;img alt="Sponsor tinchox5" src="https://img.shields.io/github/sponsors/tinchox5?style=flat-square&amp;amp;label=Sponsor" /&gt; &lt;/a&gt; &lt;a href="https://github.com/zumerlab/snapdom/raw/main/LICENSE"&gt; &lt;img alt="License" src="https://img.shields.io/github/license/zumerlab/snapdom?style=flat-square" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;snapDOM&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;snapDOM&lt;/strong&gt; is a fast and accurate DOM-to-image capture tool built for &lt;strong&gt;Zumly&lt;/strong&gt;, a zoom-based view transition framework.&lt;/p&gt; 
&lt;p&gt;It captures any HTML element as a scalable SVG image, preserving styles, fonts, background images, pseudo-elements, and even shadow DOM. It also supports export to raster image formats and canvas.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“¸ Full DOM capture&lt;/li&gt; 
 &lt;li&gt;ğŸ¨ Embedded styles, pseudo-elements, and fonts&lt;/li&gt; 
 &lt;li&gt;ğŸ–¼ï¸ Export to SVG, PNG, JPG, WebP, &lt;code&gt;canvas&lt;/code&gt;, or Blob&lt;/li&gt; 
 &lt;li&gt;âš¡ Ultra fast, no dependencies&lt;/li&gt; 
 &lt;li&gt;ğŸ“¦ 100% based on standard Web APIs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://zumerlab.github.io/snapdom/"&gt;https://zumerlab.github.io/snapdom/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;NPM / Yarn&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;npm i @zumer/snapdom
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;yarn add @zumer/snapdom
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then import it in your code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { snapdom } from '@zumer/snapdom';
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;CDN&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;script src="https://cdn.jsdelivr.net/npm/@zumer/snapdom/dist/snapdom.min.js"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Script tag (local)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;script src="snapdom.js"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ES Module&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { snapdom } from './snapdom.mjs';
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Module via CDN&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;script type="module"&amp;gt;
  import { snapdom } from 'https://cdn.jsdelivr.net/npm/@zumer/snapdom/dist/snapdom.mjs';
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic usage&lt;/h2&gt; 
&lt;h3&gt;Reusable capture&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;const el = document.querySelector('#target');
const result = await snapdom(el, { scale: 2 });

const img = await result.toPng();
document.body.appendChild(img);

await result.download({ format: 'jpg', filename: 'my-capture' });
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;One-step shortcuts&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;const el = document.querySelector('#target');
const png = await snapdom.toPng(el);
document.body.appendChild(png);

const blob = await snapdom.toBlob(el);
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;API&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;snapdom(el, options?)&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Returns an object with reusable export methods:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;{
  url: string;
  toRaw(): string;
  toImg(): Promise&amp;lt;HTMLImageElement&amp;gt;;
  toCanvas(): Promise&amp;lt;HTMLCanvasElement&amp;gt;;
  toBlob(options?): Promise&amp;lt;Blob&amp;gt;;
  toPng(options?): Promise&amp;lt;HTMLImageElement&amp;gt;;
  toJpg(options?): Promise&amp;lt;HTMLImageElement&amp;gt;;
  toWebp(options?): Promise&amp;lt;HTMLImageElement&amp;gt;;
  download(options?): Promise&amp;lt;void&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Shortcut methods&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;snapdom.toImg(el, options?)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns an &lt;code&gt;HTMLImageElement&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;snapdom.toCanvas(el, options?) &lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns a &lt;code&gt;Canvas&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;snapdom.toBlob(el, options?)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns an SVG &lt;code&gt;Blob&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;snapdom.toPng(el, options?)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns a PNG image&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;snapdom.toJpg(el, options?)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns a JPG image&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;snapdom.toWebp(el, options?)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns a WebP image&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;snapdom.download(el, options?) &lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Triggers download in specified format&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Options&lt;/h2&gt; 
&lt;p&gt;All capture methods accept an &lt;code&gt;options&lt;/code&gt; object:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Option&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;compress&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Removes redundant styles&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;fast&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Skips idle delay for faster results&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;embedFonts&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Inlines fonts (icon fonts always embedded)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;scale&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;number&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Output scale multiplier&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dpr&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;number&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;devicePixelRatio&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Device pixel ratio&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;width&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;number&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Output specific width size&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;height&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;number&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Output specific height size&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;backgroundColor&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;string&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;"#fff"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Fallback color for JPG/WebP&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;quality&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;number&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Quality for JPG/WebP (0 to 1)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;useProxy&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;string&lt;/td&gt; 
   &lt;td&gt;''&lt;/td&gt; 
   &lt;td&gt;Specify a proxy for handling CORS images as fallback&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;type&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;string&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;svg&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Select &lt;code&gt;png&lt;/code&gt;, &lt;code&gt;jpg&lt;/code&gt;, &lt;code&gt;webp&lt;/code&gt; Blob type&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;exclude&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;string[]&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;CSS selectors for elements to exclude&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;filter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;function&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Custom filter function ie &lt;code&gt;(el) =&amp;gt; !el.classList.contains('hidden')&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Setting custom dimensions with width and height options&lt;/h3&gt; 
&lt;p&gt;Use the &lt;code&gt;width&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; options to generate an image with specific dimensions.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Examples:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. Fixed width (proportional height)&lt;/strong&gt; Sets a specific width while maintaining the aspect ratio. Height adjusts proportionally.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;const result = await snapdom(element, {
  width: 400 // Outputs a 400px-wide image with auto-scaled height
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. Fixed height (proportional width)&lt;/strong&gt; Sets a specific height while maintaining the aspect ratio. Width adjusts proportionally.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;const result = await snapdom(element, {
  height: 200 // Outputs a 200px-tall image with auto-scaled width
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Fixed width and height (may distort image)&lt;/strong&gt; Forces exact dimensions, potentially distorting the image if the aspect ratio differs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;const result = await snapdom(element, {
  width: 800,  // Outputs an 800px Ã— 200px image (may stretch/squish content)
  height: 200
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If &lt;code&gt;scale&lt;/code&gt; is different from 1, it takes priority over width and height. Example: &lt;code&gt;{ scale: 3, width: 500 }&lt;/code&gt; ignores width and scales the image 3x instead.&lt;/p&gt; 
&lt;h3&gt;Cross-Origin Images&lt;/h3&gt; 
&lt;p&gt;By default, snapDOM loads images with &lt;code&gt;crossOrigin="anonymous"&lt;/code&gt; or &lt;code&gt;crossOrigin="use-credentials"&lt;/code&gt;. In case fails to get the images, &lt;code&gt;useProxy&lt;/code&gt; can be used to deal with CORS images:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;const result = await snapdom(element, {
  useProxy: 'your/proxy/' //Example: 'https://corsproxy.io/?url=' or 'https://api.allorigins.win/raw?url='
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Download options&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;{
  format?: "svg" | "png" | "jpg" | "jpeg" | "webp"; // default: "png"
  filename?: string;         // default: "capture"
  backgroundColor?: string;  // optional override
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;preCache()&lt;/code&gt; â€“ Optional helper&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;preCache()&lt;/code&gt; function can be used to load external resources (like images and fonts) in advance. It is specially useful when the element to capture is big and complex.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { preCache } from '@zumer/snapdom';

await preCache(document.body);
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-js"&gt;import { snapdom, preCache } from './snapdom.mjs';
    window.addEventListener('load', async () =&amp;gt; {
    await preCache();
    console.log('ğŸ“¦ Resources preloaded');
    });
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Options for &lt;code&gt;preCache()&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;embedFonts&lt;/code&gt; &lt;em&gt;(boolean, default: true)&lt;/em&gt; â€” Inlines non-icon fonts during preload.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;useProxy&lt;/code&gt; &lt;em&gt;(string)&lt;/em&gt; â€” Proxy for handling CORS images as fallback.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Captures &lt;strong&gt;shadow DOM&lt;/strong&gt; and Web Components&lt;/li&gt; 
 &lt;li&gt;Supports &lt;code&gt;::before&lt;/code&gt;, &lt;code&gt;::after&lt;/code&gt; and &lt;code&gt;::first-letter&lt;/code&gt; pseudo-elements&lt;/li&gt; 
 &lt;li&gt;Inlines background images and fonts&lt;/li&gt; 
 &lt;li&gt;Handles &lt;strong&gt;Font Awesome&lt;/strong&gt;, &lt;strong&gt;Material Icons&lt;/strong&gt;, and more&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;data-capture="exclude"&lt;/code&gt; to ignore an element&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;data-capture="placeholder"&lt;/code&gt; with &lt;code&gt;data-placeholder-text&lt;/code&gt; for masked replacements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Limitations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;External images should be CORS-accessible (use &lt;code&gt;useProxy&lt;/code&gt; option for handling CORS denied)&lt;/li&gt; 
 &lt;li&gt;Iframes are not supported&lt;/li&gt; 
 &lt;li&gt;When WebP format is used on Safari, it will fallback to PNG rendering.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;@font-face&lt;/code&gt; CSS rule is well supported, but if need to use JS &lt;code&gt;FontFace()&lt;/code&gt;, see this workaround &lt;a href="https://github.com/zumerlab/snapdom/issues/43"&gt;&lt;code&gt;#43&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âš¡ Performance Benchmarks&lt;/h2&gt; 
&lt;p&gt;Snapdom has received &lt;strong&gt;significant performance improvements&lt;/strong&gt; since version &lt;code&gt;v1.8.0&lt;/code&gt;. The following benchmarks compare:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Snapdom (current)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Snapdom v1.8.0&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;html2canvas&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;html-to-image&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Simple elements&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Snapdom (current)&lt;/th&gt; 
   &lt;th&gt;Snapdom v1.8.0&lt;/th&gt; 
   &lt;th&gt;html2canvas&lt;/th&gt; 
   &lt;th&gt;html-to-image&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Small (200Ã—100)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.4 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1.2 ms&lt;/td&gt; 
   &lt;td&gt;70.3 ms&lt;/td&gt; 
   &lt;td&gt;3.6 ms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Modal (400Ã—300)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.4 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1.1 ms&lt;/td&gt; 
   &lt;td&gt;68.8 ms&lt;/td&gt; 
   &lt;td&gt;3.6 ms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Page View (1200Ã—800)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.4 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1.0 ms&lt;/td&gt; 
   &lt;td&gt;100.5 ms&lt;/td&gt; 
   &lt;td&gt;3.4 ms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Large Scroll (2000Ã—1500)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.4 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1.0 ms&lt;/td&gt; 
   &lt;td&gt;153.1 ms&lt;/td&gt; 
   &lt;td&gt;3.4 ms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Very Large (4000Ã—2000)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;0.4 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1.0 ms&lt;/td&gt; 
   &lt;td&gt;278.9 ms&lt;/td&gt; 
   &lt;td&gt;4.3 ms&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Complex elements&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Snapdom (current)&lt;/th&gt; 
   &lt;th&gt;Snapdom v1.8.0&lt;/th&gt; 
   &lt;th&gt;html2canvas&lt;/th&gt; 
   &lt;th&gt;html-to-image&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Small (200Ã—100)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;1.1 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3.2 ms&lt;/td&gt; 
   &lt;td&gt;76.0 ms&lt;/td&gt; 
   &lt;td&gt;15.3 ms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Modal (400Ã—300)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;4.5 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;14.0 ms&lt;/td&gt; 
   &lt;td&gt;133.2 ms&lt;/td&gt; 
   &lt;td&gt;55.4 ms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Page View (1200Ã—800)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;32.9 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;113.6 ms&lt;/td&gt; 
   &lt;td&gt;303.4 ms&lt;/td&gt; 
   &lt;td&gt;369.1 ms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Large Scroll (2000Ã—1500)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;133.9 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;387.4 ms&lt;/td&gt; 
   &lt;td&gt;594.4 ms&lt;/td&gt; 
   &lt;td&gt;1,163.0 ms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Very Large (4000Ã—2000)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;364.0 ms&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1,200.4 ms&lt;/td&gt; 
   &lt;td&gt;1,380.8 ms&lt;/td&gt; 
   &lt;td&gt;3,023.9 ms&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Summary&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Snapdom (current) is &lt;strong&gt;2Ã—â€“6Ã— faster&lt;/strong&gt; than &lt;code&gt;v1.8.0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Up to &lt;strong&gt;150Ã— faster&lt;/strong&gt; than &lt;code&gt;html2canvas&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Up to &lt;strong&gt;8Ã— faster&lt;/strong&gt; than &lt;code&gt;html-to-image&lt;/code&gt; in large scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;sub&gt;Benchmarks run in Chromium using Vitest.&lt;br /&gt; Hardware: MacBook Air 2018.&lt;br /&gt; âš ï¸ Performance may vary depending on device.&lt;/sub&gt;&lt;/p&gt; 
&lt;h3&gt;Run the benchmarks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/zumerlab/snapdom.git
cd snapdom
npm install
npm run test:benchmark
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;Planned improvements for future versions of SnapDOM:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;&lt;strong&gt;Implement plugin system&lt;/strong&gt; SnapDOM will support external plugins to extend or override internal behavior (e.g. custom node transformers, exporters, or filters).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;&lt;strong&gt;Refactor to modular architecture&lt;/strong&gt; Internal logic will be split into smaller, focused modules to improve maintainability and code reuse.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;&lt;strong&gt;Decouple internal logic from global options&lt;/strong&gt; Functions will be redesigned to avoid relying directly on &lt;code&gt;options&lt;/code&gt;. A centralized capture context will improve clarity, autonomy, and testability. See &lt;a href="https://github.com/zumerlab/snapdom/tree/main"&gt;&lt;code&gt;next&lt;/code&gt; branch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;&lt;strong&gt;Expose cache control&lt;/strong&gt; Users will be able to manually clear image and font caches or configure their own caching strategies.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;&lt;strong&gt;Auto font preloading&lt;/strong&gt; Required fonts will be automatically detected and preloaded before capture, reducing the need for manual &lt;code&gt;preCache()&lt;/code&gt; calls.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;&lt;strong&gt;Document plugin development&lt;/strong&gt; A full guide will be provided for creating and registering custom SnapDOM plugins.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;&lt;strong&gt;Make export utilities tree-shakeable&lt;/strong&gt; Export functions like &lt;code&gt;toPng&lt;/code&gt;, &lt;code&gt;toJpg&lt;/code&gt;, &lt;code&gt;toBlob&lt;/code&gt;, etc. will be restructured into independent modules to support tree shaking and minimal builds.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Have ideas or feature requests? Feel free to share suggestions or feedback in &lt;a href="https://github.com/zumerlab/snapdom/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To contribute or build snapDOM locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Clone the repository
git clone https://github.com/zumerlab/snapdom.git
cd snapdom

# Switch to dev branch
git checkout dev

# Install dependencies
npm install

# Compile the library (ESM, CJS, and minified versions)
npm run compile

# Install playwright browsers (necessary for running tests)
npx playwright install

# Run tests
npm test

# Run Benchmarks
npm run test:benchmark
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The main entry point is in &lt;code&gt;src/&lt;/code&gt;, and output bundles are generated in the &lt;code&gt;dist/&lt;/code&gt; folder.&lt;/p&gt; 
&lt;p&gt;For detailed contribution guidelines, please see &lt;a href="https://github.com/zumerlab/snapdom/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors ğŸ™Œ&lt;/h2&gt; 
&lt;!-- CONTRIBUTORS:START --&gt; 
&lt;p&gt; &lt;a href="https://github.com/tinchox5" title="tinchox5"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11557901?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="tinchox5" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tarwin" title="tarwin"&gt;&lt;img src="https://avatars.githubusercontent.com/u/646149?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="tarwin" /&gt;&lt;/a&gt; &lt;a href="https://github.com/17biubiu" title="17biubiu"&gt;&lt;img src="https://avatars.githubusercontent.com/u/13295895?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="17biubiu" /&gt;&lt;/a&gt; &lt;a href="https://github.com/av01d" title="av01d"&gt;&lt;img src="https://avatars.githubusercontent.com/u/6247646?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="av01d" /&gt;&lt;/a&gt; &lt;a href="https://github.com/CHOYSEN" title="CHOYSEN"&gt;&lt;img src="https://avatars.githubusercontent.com/u/25995358?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="CHOYSEN" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pedrocateexte" title="pedrocateexte"&gt;&lt;img src="https://avatars.githubusercontent.com/u/207524750?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="pedrocateexte" /&gt;&lt;/a&gt; &lt;a href="https://github.com/domialex" title="domialex"&gt;&lt;img src="https://avatars.githubusercontent.com/u/4694217?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="domialex" /&gt;&lt;/a&gt; &lt;a href="https://github.com/elliots" title="elliots"&gt;&lt;img src="https://avatars.githubusercontent.com/u/622455?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="elliots" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jswhisperer" title="jswhisperer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1177690?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="jswhisperer" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sharuzzaman" title="sharuzzaman"&gt;&lt;img src="https://avatars.githubusercontent.com/u/7421941?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="sharuzzaman" /&gt;&lt;/a&gt; &lt;a href="https://github.com/simon1uo" title="simon1uo"&gt;&lt;img src="https://avatars.githubusercontent.com/u/60037549?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="simon1uo" /&gt;&lt;/a&gt; &lt;a href="https://github.com/titoBouzout" title="titoBouzout"&gt;&lt;img src="https://avatars.githubusercontent.com/u/64156?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="titoBouzout" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jhbae200" title="jhbae200"&gt;&lt;img src="https://avatars.githubusercontent.com/u/20170610?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="jhbae200" /&gt;&lt;/a&gt; &lt;a href="https://github.com/miusuncle" title="miusuncle"&gt;&lt;img src="https://avatars.githubusercontent.com/u/7549857?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="miusuncle" /&gt;&lt;/a&gt; &lt;a href="https://github.com/rbbydotdev" title="rbbydotdev"&gt;&lt;img src="https://avatars.githubusercontent.com/u/101137670?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="rbbydotdev" /&gt;&lt;/a&gt; &lt;a href="https://github.com/zhanghaotian2018" title="zhanghaotian2018"&gt;&lt;img src="https://avatars.githubusercontent.com/u/169218899?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="zhanghaotian2018" /&gt;&lt;/a&gt; &lt;a href="https://github.com/kohaiy" title="kohaiy"&gt;&lt;img src="https://avatars.githubusercontent.com/u/15622127?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="kohaiy" /&gt;&lt;/a&gt; &lt;a href="https://github.com/fu050409" title="fu050409"&gt;&lt;img src="https://avatars.githubusercontent.com/u/46275354?v=4&amp;amp;s=100" style="border-radius:10px; width:60px; height:60px; object-fit:cover; margin:5px;" alt="fu050409" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;!-- CONTRIBUTORS:END --&gt; 
&lt;h2&gt;ğŸ’– Sponsors&lt;/h2&gt; 
&lt;p&gt;Special thanks to &lt;a href="https://github.com/megaphonecolin"&gt;@megaphonecolin&lt;/a&gt; for supporting this project!&lt;/p&gt; 
&lt;p&gt;If you'd like to support this project too, you can &lt;a href="https://github.com/sponsors/tinchox5"&gt;become a sponsor&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#zumerlab/snapdom&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=zumerlab/snapdom&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT Â© Zumerlab&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>libsdl-org/SDL</title>
      <link>https://github.com/libsdl-org/SDL</link>
      <description>&lt;p&gt;Simple Directmedia Layer&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Simple DirectMedia Layer (SDL for short) is a cross-platform library designed to make it easy to write multi-media software, such as games and emulators.&lt;/p&gt; 
&lt;p&gt;You can find the latest release and additional information at: &lt;a href="https://www.libsdl.org/"&gt;https://www.libsdl.org/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Installation instructions and a quick introduction is available in &lt;a href="https://raw.githubusercontent.com/libsdl-org/SDL/main/INSTALL.md"&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This library is distributed under the terms of the zlib license, available in &lt;a href="https://raw.githubusercontent.com/libsdl-org/SDL/main/LICENSE.txt"&gt;LICENSE.txt&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Enjoy!&lt;/p&gt; 
&lt;p&gt;Sam Lantinga (&lt;a href="mailto:slouken@libsdl.org"&gt;slouken@libsdl.org&lt;/a&gt;)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-python</title>
      <link>https://github.com/openai/openai-python</link>
      <description>&lt;p&gt;The official Python library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Python API library&lt;/h1&gt; 
&lt;!-- prettier-ignore --&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/openai/"&gt;&lt;img src="https://img.shields.io/pypi/v/openai.svg?label=pypi%20(stable)" alt="PyPI version" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by &lt;a href="https://github.com/encode/httpx"&gt;httpx&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It is generated from our &lt;a href="https://github.com/openai/openai-openapi"&gt;OpenAPI specification&lt;/a&gt; with &lt;a href="https://stainlessapi.com/"&gt;Stainless&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The REST API documentation can be found on &lt;a href="https://platform.openai.com/docs/api-reference"&gt;platform.openai.com&lt;/a&gt;. The full API of this library can be found in &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/api.md"&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# install from PyPI
pip install openai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;The full API of this library can be found in &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/api.md"&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The primary API for interacting with OpenAI models is the &lt;a href="https://platform.openai.com/docs/api-reference/responses"&gt;Responses API&lt;/a&gt;. You can generate text from the model with the code below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

response = client.responses.create(
    model="gpt-4o",
    instructions="You are a coding assistant that talks like a pirate.",
    input="How do I check if a Python object is an instance of a class?",
)

print(response.output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The previous standard (supported indefinitely) for generating text is the &lt;a href="https://platform.openai.com/docs/api-reference/chat"&gt;Chat Completions API&lt;/a&gt;. You can use that API to generate text from the model with the code below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "developer", "content": "Talk like a pirate."},
        {
            "role": "user",
            "content": "How do I check if a Python object is an instance of a class?",
        },
    ],
)

print(completion.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;While you can provide an &lt;code&gt;api_key&lt;/code&gt; keyword argument, we recommend using &lt;a href="https://pypi.org/project/python-dotenv/"&gt;python-dotenv&lt;/a&gt; to add &lt;code&gt;OPENAI_API_KEY="My API Key"&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file so that your API key is not stored in source control. &lt;a href="https://platform.openai.com/settings/organization/api-keys"&gt;Get an API key here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Vision&lt;/h3&gt; 
&lt;p&gt;With an image URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;prompt = "What is in this image?"
img_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg"

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"{img_url}"},
            ],
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With the image as a base64 encoded string:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import base64
from openai import OpenAI

client = OpenAI()

prompt = "What is in this image?"
with open("path/to/image.png", "rb") as image_file:
    b64_image = base64.b64encode(image_file.read()).decode("utf-8")

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"data:image/png;base64,{b64_image}"},
            ],
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Async usage&lt;/h2&gt; 
&lt;p&gt;Simply import &lt;code&gt;AsyncOpenAI&lt;/code&gt; instead of &lt;code&gt;OpenAI&lt;/code&gt; and use &lt;code&gt;await&lt;/code&gt; with each API call:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -&amp;gt; None:
    response = await client.responses.create(
        model="gpt-4o", input="Explain disestablishmentarianism to a smart five year old."
    )
    print(response.output_text)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Functionality between the synchronous and asynchronous clients is otherwise identical.&lt;/p&gt; 
&lt;h3&gt;With aiohttp&lt;/h3&gt; 
&lt;p&gt;By default, the async client uses &lt;code&gt;httpx&lt;/code&gt; for HTTP requests. However, for improved concurrency performance you may also use &lt;code&gt;aiohttp&lt;/code&gt; as the HTTP backend.&lt;/p&gt; 
&lt;p&gt;You can enable this by installing &lt;code&gt;aiohttp&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# install from PyPI
pip install openai[aiohttp]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can enable it by instantiating the client with &lt;code&gt;http_client=DefaultAioHttpClient()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from openai import DefaultAioHttpClient
from openai import AsyncOpenAI


async def main() -&amp;gt; None:
    async with AsyncOpenAI(
        api_key="My API Key",
        http_client=DefaultAioHttpClient(),
    ) as client:
        chat_completion = await client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": "Say this is a test",
                }
            ],
            model="gpt-4o",
        )


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Streaming responses&lt;/h2&gt; 
&lt;p&gt;We provide support for streaming responses using Server Side Events (SSE).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

stream = client.responses.create(
    model="gpt-4o",
    input="Write a one-sentence bedtime story about a unicorn.",
    stream=True,
)

for event in stream:
    print(event)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.responses.create(
        model="gpt-4o",
        input="Write a one-sentence bedtime story about a unicorn.",
        stream=True,
    )

    async for event in stream:
        print(event)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Realtime API beta&lt;/h2&gt; 
&lt;p&gt;The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as &lt;a href="https://platform.openai.com/docs/guides/function-calling"&gt;function calling&lt;/a&gt; through a WebSocket connection.&lt;/p&gt; 
&lt;p&gt;Under the hood the SDK uses the &lt;a href="https://websockets.readthedocs.io/en/stable/"&gt;&lt;code&gt;websockets&lt;/code&gt;&lt;/a&gt; library to manage connections.&lt;/p&gt; 
&lt;p&gt;The Realtime API works through a combination of client-sent events and server-sent events. Clients can send events to do things like update session configuration or send text and audio inputs. Server events confirm when audio responses have completed, or when a text response from the model has been received. A full event reference can be found &lt;a href="https://platform.openai.com/docs/api-reference/realtime-client-events"&gt;here&lt;/a&gt; and a guide can be found &lt;a href="https://platform.openai.com/docs/guides/realtime"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Basic text based example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import asyncio
from openai import AsyncOpenAI

async def main():
    client = AsyncOpenAI()

    async with client.beta.realtime.connect(model="gpt-4o-realtime-preview") as connection:
        await connection.session.update(session={'modalities': ['text']})

        await connection.conversation.item.create(
            item={
                "type": "message",
                "role": "user",
                "content": [{"type": "input_text", "text": "Say hello!"}],
            }
        )
        await connection.response.create()

        async for event in connection:
            if event.type == 'response.text.delta':
                print(event.delta, flush=True, end="")

            elif event.type == 'response.text.done':
                print()

            elif event.type == "response.done":
                break

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However the real magic of the Realtime API is handling audio inputs / outputs, see this example &lt;a href="https://github.com/openai/openai-python/raw/main/examples/realtime/push_to_talk_app.py"&gt;TUI script&lt;/a&gt; for a fully fledged example.&lt;/p&gt; 
&lt;h3&gt;Realtime error handling&lt;/h3&gt; 
&lt;p&gt;Whenever an error occurs, the Realtime API will send an &lt;a href="https://platform.openai.com/docs/guides/realtime-model-capabilities#error-handling"&gt;&lt;code&gt;error&lt;/code&gt; event&lt;/a&gt; and the connection will stay open and remain usable. This means you need to handle it yourself, as &lt;em&gt;no errors are raised directly&lt;/em&gt; by the SDK when an &lt;code&gt;error&lt;/code&gt; event comes in.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;client = AsyncOpenAI()

async with client.beta.realtime.connect(model="gpt-4o-realtime-preview") as connection:
    ...
    async for event in connection:
        if event.type == 'error':
            print(event.error.type)
            print(event.error.code)
            print(event.error.event_id)
            print(event.error.message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using types&lt;/h2&gt; 
&lt;p&gt;Nested request parameters are &lt;a href="https://docs.python.org/3/library/typing.html#typing.TypedDict"&gt;TypedDicts&lt;/a&gt;. Responses are &lt;a href="https://docs.pydantic.dev"&gt;Pydantic models&lt;/a&gt; which also provide helper methods for things like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serializing back into JSON, &lt;code&gt;model.to_json()&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Converting to a dictionary, &lt;code&gt;model.to_dict()&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set &lt;code&gt;python.analysis.typeCheckingMode&lt;/code&gt; to &lt;code&gt;basic&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Pagination&lt;/h2&gt; 
&lt;p&gt;List methods in the OpenAI API are paginated.&lt;/p&gt; 
&lt;p&gt;This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, asynchronously:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main() -&amp;gt; None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can use the &lt;code&gt;.has_next_page()&lt;/code&gt;, &lt;code&gt;.next_page_info()&lt;/code&gt;, or &lt;code&gt;.get_next_page()&lt;/code&gt; methods for more granular control working with pages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or just work directly with the returned data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # =&amp;gt; "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Nested params&lt;/h2&gt; 
&lt;p&gt;Nested parameters are dictionaries, typed using &lt;code&gt;TypedDict&lt;/code&gt;, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

response = client.chat.responses.create(
    input=[
        {
            "role": "user",
            "content": "How much ?",
        }
    ],
    model="gpt-4o",
    response_format={"type": "json_object"},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;File uploads&lt;/h2&gt; 
&lt;p&gt;Request parameters that correspond to file uploads can be passed as &lt;code&gt;bytes&lt;/code&gt;, or a &lt;a href="https://docs.python.org/3/library/os.html#os.PathLike"&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance or a tuple of &lt;code&gt;(filename, contents, media type)&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface. If you pass a &lt;a href="https://docs.python.org/3/library/os.html#os.PathLike"&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance, the file contents will be read asynchronously automatically.&lt;/p&gt; 
&lt;h2&gt;Webhook Verification&lt;/h2&gt; 
&lt;p&gt;Verifying webhook signatures is &lt;em&gt;optional but encouraged&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;For more information about webhooks, see &lt;a href="https://platform.openai.com/docs/guides/webhooks"&gt;the API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Parsing webhook payloads&lt;/h3&gt; 
&lt;p&gt;For most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method &lt;code&gt;client.webhooks.unwrap()&lt;/code&gt;, which parses a webhook request and verifies that it was sent by OpenAI. This method will raise an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). The &lt;code&gt;.unwrap()&lt;/code&gt; method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI
from flask import Flask, request

app = Flask(__name__)
client = OpenAI()  # OPENAI_WEBHOOK_SECRET environment variable is used by default


@app.route("/webhook", methods=["POST"])
def webhook():
    request_body = request.get_data(as_text=True)

    try:
        event = client.webhooks.unwrap(request_body, request.headers)

        if event.type == "response.completed":
            print("Response completed:", event.data)
        elif event.type == "response.failed":
            print("Response failed:", event.data)
        else:
            print("Unhandled event type:", event.type)

        return "ok"
    except Exception as e:
        print("Invalid signature:", e)
        return "Invalid signature", 400


if __name__ == "__main__":
    app.run(port=8000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verifying webhook payloads directly&lt;/h3&gt; 
&lt;p&gt;In some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method &lt;code&gt;client.webhooks.verify_signature()&lt;/code&gt; to &lt;em&gt;only verify&lt;/em&gt; the signature of a webhook request. Like &lt;code&gt;.unwrap()&lt;/code&gt;, this method will raise an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import json
from openai import OpenAI
from flask import Flask, request

app = Flask(__name__)
client = OpenAI()  # OPENAI_WEBHOOK_SECRET environment variable is used by default


@app.route("/webhook", methods=["POST"])
def webhook():
    request_body = request.get_data(as_text=True)

    try:
        client.webhooks.verify_signature(request_body, request.headers)

        # Parse the body after verification
        event = json.loads(request_body)
        print("Verified event:", event)

        return "ok"
    except Exception as e:
        print("Invalid signature:", e)
        return "Invalid signature", 400


if __name__ == "__main__":
    app.run(port=8000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Handling errors&lt;/h2&gt; 
&lt;p&gt;When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of &lt;code&gt;openai.APIConnectionError&lt;/code&gt; is raised.&lt;/p&gt; 
&lt;p&gt;When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of &lt;code&gt;openai.APIStatusError&lt;/code&gt; is raised, containing &lt;code&gt;status_code&lt;/code&gt; and &lt;code&gt;response&lt;/code&gt; properties.&lt;/p&gt; 
&lt;p&gt;All errors inherit from &lt;code&gt;openai.APIError&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model="gpt-4o",
        training_file="file-abc123",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Error codes are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Status Code&lt;/th&gt; 
   &lt;th&gt;Error Type&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;400&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;BadRequestError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;401&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;AuthenticationError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;403&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;PermissionDeniedError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;404&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;NotFoundError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;422&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;UnprocessableEntityError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;429&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;RateLimitError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;gt;=500&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;InternalServerError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;APIConnectionError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Request IDs&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For more information on debugging requests, see &lt;a href="https://platform.openai.com/docs/api-reference/debugging-requests"&gt;these docs&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;x-request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to OpenAI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = await client.responses.create(
    model="gpt-4o-mini",
    input="Say 'this is a test'.",
)
print(response._request_id)  # req_123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that unlike other properties that use an &lt;code&gt;_&lt;/code&gt; prefix, the &lt;code&gt;_request_id&lt;/code&gt; property &lt;em&gt;is&lt;/em&gt; public. Unless documented otherwise, &lt;em&gt;all&lt;/em&gt; other &lt;code&gt;_&lt;/code&gt; prefix properties, methods and modules are &lt;em&gt;private&lt;/em&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; If you need to access request IDs for failed requests you must catch the &lt;code&gt;APIStatusError&lt;/code&gt; exception&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import openai

try:
    completion = await client.chat.completions.create(
        messages=[{"role": "user", "content": "Say this is a test"}], model="gpt-4"
    )
except openai.APIStatusError as exc:
    print(exc.request_id)  # req_123
    raise exc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Retries&lt;/h2&gt; 
&lt;p&gt;Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors are all retried by default.&lt;/p&gt; 
&lt;p&gt;You can use the &lt;code&gt;max_retries&lt;/code&gt; option to configure or disable retry settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in JavaScript?",
        }
    ],
    model="gpt-4o",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Timeouts&lt;/h2&gt; 
&lt;p&gt;By default requests time out after 10 minutes. You can configure this with a &lt;code&gt;timeout&lt;/code&gt; option, which accepts a float or an &lt;a href="https://www.python-httpx.org/advanced/timeouts/#fine-tuning-the-configuration"&gt;&lt;code&gt;httpx.Timeout&lt;/code&gt;&lt;/a&gt; object:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5.0).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-4o",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On timeout, an &lt;code&gt;APITimeoutError&lt;/code&gt; is thrown.&lt;/p&gt; 
&lt;p&gt;Note that requests that time out are &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/#retries"&gt;retried twice by default&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Advanced&lt;/h2&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;We use the standard library &lt;a href="https://docs.python.org/3/library/logging.html"&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/a&gt; module.&lt;/p&gt; 
&lt;p&gt;You can enable logging by setting the environment variable &lt;code&gt;OPENAI_LOG&lt;/code&gt; to &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ export OPENAI_LOG=info
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to &lt;code&gt;debug&lt;/code&gt; for more verbose logging.&lt;/p&gt; 
&lt;h3&gt;How to tell whether &lt;code&gt;None&lt;/code&gt; means &lt;code&gt;null&lt;/code&gt; or missing&lt;/h3&gt; 
&lt;p&gt;In an API response, a field may be explicitly &lt;code&gt;null&lt;/code&gt;, or missing entirely; in either case, its value is &lt;code&gt;None&lt;/code&gt; in this library. You can differentiate the two cases with &lt;code&gt;.model_fields_set&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessing raw response data (e.g. headers)&lt;/h3&gt; 
&lt;p&gt;The "raw" Response object can be accessed by prefixing &lt;code&gt;.with_raw_response.&lt;/code&gt; to any HTTP method call, e.g.,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-4o",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These methods return a &lt;a href="https://github.com/openai/openai-python/tree/main/src/openai/_legacy_response.py"&gt;&lt;code&gt;LegacyAPIResponse&lt;/code&gt;&lt;/a&gt; object. This is a legacy class as we're changing it slightly in the next major version.&lt;/p&gt; 
&lt;p&gt;For the sync client this will mostly be the same with the exception of &lt;code&gt;content&lt;/code&gt; &amp;amp; &lt;code&gt;text&lt;/code&gt; will be methods instead of properties. In the async client, all methods will be async.&lt;/p&gt; 
&lt;p&gt;A migration script will be provided &amp;amp; the migration in general should be smooth.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;.with_streaming_response&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The above interface eagerly reads the full response body when you make the request, which may not always be what you want.&lt;/p&gt; 
&lt;p&gt;To stream the response body, use &lt;code&gt;.with_streaming_response&lt;/code&gt; instead, which requires a context manager and only reads the response body once you call &lt;code&gt;.read()&lt;/code&gt;, &lt;code&gt;.text()&lt;/code&gt;, &lt;code&gt;.json()&lt;/code&gt;, &lt;code&gt;.iter_bytes()&lt;/code&gt;, &lt;code&gt;.iter_text()&lt;/code&gt;, &lt;code&gt;.iter_lines()&lt;/code&gt; or &lt;code&gt;.parse()&lt;/code&gt;. In the async client, these are async methods.&lt;/p&gt; 
&lt;p&gt;As such, &lt;code&gt;.with_streaming_response&lt;/code&gt; methods return a different &lt;a href="https://github.com/openai/openai-python/tree/main/src/openai/_response.py"&gt;&lt;code&gt;APIResponse&lt;/code&gt;&lt;/a&gt; object, and the async client returns an &lt;a href="https://github.com/openai/openai-python/tree/main/src/openai/_response.py"&gt;&lt;code&gt;AsyncAPIResponse&lt;/code&gt;&lt;/a&gt; object.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-4o",
) as response:
    print(response.headers.get("X-My-Header"))

    for line in response.iter_lines():
        print(line)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The context manager is required so that the response will reliably be closed.&lt;/p&gt; 
&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; 
&lt;p&gt;This library is typed for convenient access to the documented API.&lt;/p&gt; 
&lt;p&gt;If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; 
&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; 
&lt;p&gt;To make requests to undocumented endpoints, you can make requests using &lt;code&gt;client.get&lt;/code&gt;, &lt;code&gt;client.post&lt;/code&gt;, and other http verbs. Options on the client will be respected (such as retries) when making this request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import httpx

response = client.post(
    "/foo",
    cast_to=httpx.Response,
    body={"my_param": True},
)

print(response.headers.get("x-foo"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Undocumented request params&lt;/h4&gt; 
&lt;p&gt;If you want to explicitly send an extra param, you can do so with the &lt;code&gt;extra_query&lt;/code&gt;, &lt;code&gt;extra_body&lt;/code&gt;, and &lt;code&gt;extra_headers&lt;/code&gt; request options.&lt;/p&gt; 
&lt;h4&gt;Undocumented response properties&lt;/h4&gt; 
&lt;p&gt;To access undocumented response properties, you can access the extra fields like &lt;code&gt;response.unknown_prop&lt;/code&gt;. You can also get all the extra fields on the Pydantic model as a dict with &lt;a href="https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_extra"&gt;&lt;code&gt;response.model_extra&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuring the HTTP client&lt;/h3&gt; 
&lt;p&gt;You can directly override the &lt;a href="https://www.python-httpx.org/api/#client"&gt;httpx client&lt;/a&gt; to customize it for your use case, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for &lt;a href="https://www.python-httpx.org/advanced/proxies/"&gt;proxies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Custom &lt;a href="https://www.python-httpx.org/advanced/transports/"&gt;transports&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additional &lt;a href="https://www.python-httpx.org/advanced/clients/"&gt;advanced&lt;/a&gt; functionality&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import httpx
from openai import OpenAI, DefaultHttpxClient

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083/v1",
    http_client=DefaultHttpxClient(
        proxy="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also customize the client on a per-request basis by using &lt;code&gt;with_options()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;client.with_options(http_client=DefaultHttpxClient(...))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Managing HTTP resources&lt;/h3&gt; 
&lt;p&gt;By default the library closes underlying HTTP connections whenever the client is &lt;a href="https://docs.python.org/3/reference/datamodel.html#object.__del__"&gt;garbage collected&lt;/a&gt;. You can manually close the client using the &lt;code&gt;.close()&lt;/code&gt; method if desired, or with a context manager that closes when exiting.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from openai import OpenAI

with OpenAI() as client:
  # make requests here
  ...

# HTTP client is now closed
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;To use this library with &lt;a href="https://learn.microsoft.com/azure/ai-services/openai/overview"&gt;Azure OpenAI&lt;/a&gt;, use the &lt;code&gt;AzureOpenAI&lt;/code&gt; class instead of the &lt;code&gt;OpenAI&lt;/code&gt; class.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.to_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In addition to the options provided in the base &lt;code&gt;OpenAI&lt;/code&gt; client, the following options are provided:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;azure_endpoint&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_deployment&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;api_version&lt;/code&gt; (or the &lt;code&gt;OPENAI_API_VERSION&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_ad_token&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_AD_TOKEN&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_ad_token_provider&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;An example of using the client with Microsoft Entra ID (formerly known as Azure Active Directory) can be found &lt;a href="https://github.com/openai/openai-python/raw/main/examples/azure_ad.py"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Versioning&lt;/h2&gt; 
&lt;p&gt;This package generally follows &lt;a href="https://semver.org/spec/v2.0.0.html"&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Changes that only affect static types, without breaking runtime behavior.&lt;/li&gt; 
 &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; 
&lt;p&gt;We are keen for your feedback; please open an &lt;a href="https://www.github.com/openai/openai-python/issues"&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; 
&lt;h3&gt;Determining the installed version&lt;/h3&gt; 
&lt;p&gt;If you've upgraded to the latest version but aren't seeing any new features you were expecting then your python environment is likely still using an older version.&lt;/p&gt; 
&lt;p&gt;You can determine the version that is being used at runtime with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import openai
print(openai.__version__)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Python 3.8 or higher.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/CONTRIBUTING.md"&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tadata-org/fastapi_mcp</title>
      <link>https://github.com/tadata-org/fastapi_mcp</link>
      <description>&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c" alt="fastapi-to-mcp" height="100/" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;span style="font-size: 0.85em; font-weight: normal;"&gt;Built by &lt;a href="https://tadata.com"&gt;Tadata&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; FastAPI-MCP &lt;/h1&gt; 
&lt;p align="center"&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/tadata-org/fastapi_mcp"&gt;&lt;img src="https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c" alt="fastapi-mcp-usage" height="400" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI's ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hosted Solution&lt;/h2&gt; 
&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href="https://tadata.com"&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP provides &lt;a href="https://fastapi-mcp.tadata.com/"&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples"&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; 
&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn't need to run separately from the MCP server (though &lt;a href="https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app"&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; 
&lt;h2&gt;Development and Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; 
&lt;p&gt;Before you get started, please see our &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg"&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; 
 &lt;li&gt;uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>