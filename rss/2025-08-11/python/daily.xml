<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 10 Aug 2025 06:00:47 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>polarsource/polar</title>
      <link>https://github.com/polarsource/polar</link>
      <description>&lt;p&gt;An open source engine for your digital products. Sell SaaS and digital products in minutes.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://polar.sh"&gt; &lt;img src="https://github.com/user-attachments/assets/89a588e5-0c58-429a-8bbe-20f70af41372" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.producthunt.com/posts/polar-5?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-polar-5" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=484271&amp;amp;theme=dark&amp;amp;period=daily" alt="Polar - An open source monetization platform for developers | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; &lt;a href="https://www.producthunt.com/posts/polar-5?embed=true&amp;amp;utm_source=badge-top-post-topic-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-polar-5" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=484271&amp;amp;theme=dark&amp;amp;period=monthly&amp;amp;topic_id=267" alt="Polar - An open source monetization platform for developers | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://polar.sh"&gt;Website&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/blog"&gt;Blog&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://polar.sh/docs"&gt;Docs&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href="https://docs.polar.sh/api-reference"&gt;API Reference&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://discord.gg/Pnhfz3UThd"&gt; &lt;img src="https://img.shields.io/badge/chat-on%20discord-7289DA.svg?sanitize=true" alt="Discord Chat" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=polar_sh"&gt; &lt;img src="https://img.shields.io/twitter/follow/polar_sh.svg?label=Follow%20@polar_sh" alt="Follow @polar_sh" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Polar: Open Source payments infrastructure for the 21st century&lt;/h2&gt; 
&lt;p&gt;Focus on building your passion, while we focus on the infrastructure to get you paid.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Sell SaaS and digital products in minutes&lt;/li&gt; 
 &lt;li&gt;All-in-one funding &amp;amp; monetization platform for developers.&lt;/li&gt; 
 &lt;li&gt;Sell access to GitHub repositories, Discord Support channels, File Downloads, License Keys &amp;amp; much more with Digital Products &amp;amp; Subscriptions.&lt;/li&gt; 
 &lt;li&gt;We're the merchant of record handling the... 
  &lt;ul&gt; 
   &lt;li&gt;...boilerplate (billing, receipts, customer accounts etc)&lt;/li&gt; 
   &lt;li&gt;...headaches (sales tax, VAT)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Pricing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4% + 40¬¢&lt;/li&gt; 
 &lt;li&gt;No fixed monthly costs&lt;/li&gt; 
 &lt;li&gt;Additional fees may apply. &lt;a href="https://docs.polar.sh/documentation/polar-as-merchant-of-record/fees"&gt;Read more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap, Issues &amp;amp; Feature Requests&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;üéØ Upcoming milestones.&lt;/strong&gt; &lt;a href="https://github.com/polarsource/polar/issues/3242"&gt;Check out what we're building towards&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üí¨ Shape the future of Polar with us.&lt;/strong&gt; &lt;a href="https://discord.gg/Pnhfz3UThd"&gt;Join our Discord&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üêõ Found a bug?&lt;/strong&gt; &lt;a href="https://github.com/polarsource/polar/issues"&gt;Submit it here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üîì Found a security vulnerability?&lt;/strong&gt; We greatly appreciate responsible and private disclosures. See &lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/SECURITY.md"&gt;Security&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Polar API &amp;amp; SDK&lt;/h3&gt; 
&lt;p&gt;You can integrate Polar on your docs, sites or services using our &lt;a href="https://docs.polar.sh/api-reference"&gt;Public API&lt;/a&gt; and &lt;a href="https://docs.polar.sh/developers/webhooks"&gt;Webhook API&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We also maintain SDKs for the following languages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;JavaScript (Node.js and browsers): &lt;a href="https://github.com/polarsource/polar-js"&gt;polarsource/polar-js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Python: &lt;a href="https://github.com/polarsource/polar-python"&gt;polarsource/polar-python&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/DEVELOPMENT.md"&gt;&lt;code&gt;DEVELOPMENT.md&lt;/code&gt;&lt;/a&gt; file contains everything you need to know to configure your development environment.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Want to get started quickly? Use GitHub Codespaces.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://codespaces.new/polarsource/polar?machine=standardLinux32gb"&gt;&lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" alt="Open in GitHub Codespaces" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Contributors&lt;/h3&gt; 
&lt;a href="https://github.com/polarsource/polar/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=polarsource/polar" /&gt; &lt;/a&gt; 
&lt;h2&gt;Monorepo&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/server/README.md"&gt;server&lt;/a&gt;&lt;/strong&gt; ‚Äì Python / FastAPI / Dramatiq / SQLAlchemy (PostgreSQL) / Redis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/README.md"&gt;clients&lt;/a&gt;&lt;/strong&gt; ‚Äì Turborepo 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/apps/web"&gt;web&lt;/a&gt; (Dashboard) ‚Äì NextJS (TypeScript)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/polarsource/polar/main/clients/packages/polarkit"&gt;polarkit&lt;/a&gt; - Shared React components&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;sub&gt;‚ô•Ô∏èüôè To our &lt;code&gt;pyproject.toml&lt;/code&gt; friends: &lt;a href="https://github.com/tiangolo/fastapi"&gt;FastAPI&lt;/a&gt;, &lt;a href="https://github.com/pydantic/pydantic"&gt;Pydantic&lt;/a&gt;, &lt;a href="https://github.com/Bogdanp/dramatiq"&gt;Dramatiq&lt;/a&gt;, &lt;a href="https://github.com/sqlalchemy/sqlalchemy"&gt;SQLAlchemy&lt;/a&gt;, &lt;a href="https://github.com/yanyongyu/githubkit"&gt;Githubkit&lt;/a&gt;, &lt;a href="https://github.com/sysid/sse-starlette"&gt;sse-starlette&lt;/a&gt;, &lt;a href="https://github.com/encode/uvicorn"&gt;Uvicorn&lt;/a&gt;, &lt;a href="https://github.com/frankie567/httpx-oauth"&gt;httpx-oauth&lt;/a&gt;, &lt;a href="https://github.com/pallets/jinja"&gt;jinja&lt;/a&gt;, &lt;a href="https://github.com/pallets-eco/blinker"&gt;blinker&lt;/a&gt;, &lt;a href="https://github.com/jpadilla/pyjwt"&gt;pyjwt&lt;/a&gt;, &lt;a href="https://github.com/getsentry/sentry"&gt;Sentry&lt;/a&gt; + more&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;‚ô•Ô∏èüôè To our &lt;code&gt;package.json&lt;/code&gt; friends: &lt;a href="https://github.com/vercel/next.js/"&gt;Next.js&lt;/a&gt;, &lt;a href="https://github.com/TanStack/query"&gt;TanStack Query&lt;/a&gt;, &lt;a href="https://github.com/tailwindlabs/tailwindcss"&gt;tailwindcss&lt;/a&gt;, &lt;a href="https://github.com/pmndrs/zustand"&gt;zustand&lt;/a&gt;, &lt;a href="https://github.com/ferdikoomen/openapi-typescript-codegen"&gt;openapi-typescript-codegen&lt;/a&gt;, &lt;a href="https://github.com/axios/axios"&gt;axios&lt;/a&gt;, &lt;a href="https://github.com/radix-ui/primitives"&gt;radix-ui&lt;/a&gt;, &lt;a href="https://github.com/pacocoursey/cmdk"&gt;cmdk&lt;/a&gt;, &lt;a href="https://github.com/framer/motion"&gt;framer-motion&lt;/a&gt; + more&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;‚ô•Ô∏èüôè To &lt;a href="https://ipinfo.io"&gt;IPinfo&lt;/a&gt; that provides IP address data to help us geolocate customers during checkout.&lt;/sub&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>xlang-ai/OSWorld</title>
      <link>https://github.com/xlang-ai/OSWorld</link>
      <description>&lt;p&gt;[NeurIPS 2024] OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://huggingface.co/datasets/xlangai/assets/resolve/main/github_banner_v2.png" alt="Banner" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://os-world.github.io/"&gt;Website&lt;/a&gt; ‚Ä¢ &lt;a href="https://arxiv.org/abs/2404.07972"&gt;Paper&lt;/a&gt; ‚Ä¢ &lt;a href="https://timothyxxx.github.io/OSWorld/"&gt;Doc&lt;/a&gt; ‚Ä¢ &lt;a href="https://github.com/xlang-ai/OSWorld/tree/main/evaluation_examples"&gt;Data&lt;/a&gt; ‚Ä¢ &lt;a href="https://os-world.github.io/explorer.html"&gt;Data Viewer&lt;/a&gt; ‚Ä¢ &lt;a href="https://discord.gg/4Gnw7eTEZR"&gt;Discord&lt;/a&gt; ‚Ä¢ &lt;a href="https://drive.google.com/file/d/1XlEy49otYDyBlA3O9NbR0BpPfr2TXgaD/view?usp=drive_link"&gt;Cache&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://img.shields.io/badge/PRs-Welcome-red"&gt; &lt;img src="https://img.shields.io/badge/PRs-Welcome-red" /&gt; &lt;/a&gt; &lt;a href="https://img.shields.io/github/last-commit/xlang-ai/OSWorld?color=green"&gt; &lt;img src="https://img.shields.io/github/last-commit/xlang-ai/OSWorld?color=green" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt; &lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://badge.fury.io/py/desktop-env"&gt; &lt;img src="https://badge.fury.io/py/desktop-env.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://pepy.tech/project/desktop-env"&gt; &lt;img src="https://static.pepy.tech/badge/desktop-env" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;h2&gt;üì¢ Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-07-28: Introducing &lt;strong&gt;OSWorld-Verified&lt;/strong&gt;! We have made major updates, fixed several issues reported by the community, with more support for AWS (can reduce evaluation time to within 1 hour through parallelization!), and making the benchmark signals more effective. Check out more in the &lt;a href="https://xlang.ai/blog/osworld-verified"&gt;report&lt;/a&gt;. We have run new model results in the latest version and updated them on the &lt;a href="https://os-world.github.io/"&gt;official website&lt;/a&gt;. Please compare your OSWorld results with the new benchmark results when running the latest version.&lt;/li&gt; 
 &lt;li&gt;2025-05-01: If you need pre-downloaded files for init state setup, we downloaded for you &lt;a href="https://drive.google.com/file/d/1XlEy49otYDyBlA3O9NbR0BpPfr2TXgaD/view?usp=drive_link"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2024-10-22: We supported Dockerüê≥ for hosting virtual machines on virtualized platforms. Check below for detailed instructions!&lt;/li&gt; 
 &lt;li&gt;2024-06-15: We refactor the code of environment part to decompose VMware Integration, and start to support other platforms such as VirtualBox, AWS, Azure, etc. Hold tight!&lt;/li&gt; 
 &lt;li&gt;2024-04-11: We released our &lt;a href="https://arxiv.org/abs/2404.07972"&gt;paper&lt;/a&gt;, &lt;a href="https://github.com/xlang-ai/OSWorld"&gt;environment and benchmark&lt;/a&gt;, and &lt;a href="https://os-world.github.io/"&gt;project page&lt;/a&gt;. Check it out!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üíæ Installation&lt;/h2&gt; 
&lt;h3&gt;VMware/VirtualBox (Desktop, Laptop, Bare Metal Machine)&lt;/h3&gt; 
&lt;p&gt;Suppose you are operating on a system that has not been virtualized (e.g. your desktop, laptop, bare metal machine), meaning you are not utilizing a virtualized environment like AWS, Azure, or k8s. If this is the case, proceed with the instructions below. However, if you are on a virtualized platform, please refer to the &lt;a href="https://github.com/xlang-ai/OSWorld?tab=readme-ov-file#docker-server-with-kvm-support-for-the-better"&gt;Docker&lt;/a&gt; section.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;First, clone this repository and &lt;code&gt;cd&lt;/code&gt; into it. Then, install the dependencies listed in &lt;code&gt;requirements.txt&lt;/code&gt;. It is recommended that you use the latest version of Conda to manage the environment, but you can also choose to manually install the dependencies. Please ensure that the version of Python is &amp;gt;= 3.10.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the OSWorld repository
git clone https://github.com/xlang-ai/OSWorld

# Change directory into the cloned repository
cd OSWorld

# Optional: Create a Conda environment for OSWorld
# conda create -n osworld python=3.10
# conda activate osworld

# Install required dependencies
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install the environment without any benchmark tasks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install desktop-env
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install &lt;a href="https://www.vmware.com/products/workstation-pro/workstation-pro-evaluation.html"&gt;VMware Workstation Pro&lt;/a&gt; (for systems with Apple Chips, you should install &lt;a href="https://support.broadcom.com/group/ecx/productdownloads?subfamily=VMware+Fusion"&gt;VMware Fusion&lt;/a&gt;) and configure the &lt;code&gt;vmrun&lt;/code&gt; command. The installation process can refer to &lt;a href="https://raw.githubusercontent.com/xlang-ai/OSWorld/main/desktop_env/providers/vmware/INSTALL_VMWARE.md"&gt;How to install VMware Workstation Pro&lt;/a&gt;. Verify the successful installation by running the following:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vmrun -T ws list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the installation along with the environment variable set is successful, you will see the message showing the current running virtual machines.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We also support using &lt;a href="https://www.virtualbox.org/"&gt;VirtualBox&lt;/a&gt; if you have issues with VMware Pro. However, features such as parallelism and macOS on Apple chips might not be well-supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;All set! Our setup script will automatically download the necessary virtual machines and configure the environment for you.&lt;/p&gt; 
&lt;h3&gt;Docker (Server with KVM Support for Better Performance)&lt;/h3&gt; 
&lt;p&gt;If you are running on a non-bare metal server, or prefer not to use VMware and VirtualBox platforms, we recommend using our Docker support.&lt;/p&gt; 
&lt;h4&gt;Prerequisite: Check if your machine supports KVM&lt;/h4&gt; 
&lt;p&gt;We recommend running the VM with KVM support. To check if your hosting platform supports KVM, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;egrep -c '(vmx|svm)' /proc/cpuinfo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;on Linux. If the return value is greater than zero, the processor should be able to support KVM.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: macOS hosts generally do not support KVM. You are advised to use VMware if you would like to run OSWorld on macOS.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Install Docker&lt;/h4&gt; 
&lt;p&gt;If your hosting platform supports a graphical user interface (GUI), you may refer to &lt;a href="https://docs.docker.com/desktop/install/linux/"&gt;Install Docker Desktop on Linux&lt;/a&gt; or &lt;a href="https://docs.docker.com/desktop/install/windows-install/"&gt;Install Docker Desktop on Windows&lt;/a&gt; based on your OS. Otherwise, you may &lt;a href="https://docs.docker.com/engine/install/"&gt;Install Docker Engine&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Running Experiments&lt;/h4&gt; 
&lt;p&gt;Add the following arguments when initializing &lt;code&gt;DesktopEnv&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;provider_name&lt;/code&gt;: &lt;code&gt;docker&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;os_type&lt;/code&gt;: &lt;code&gt;Ubuntu&lt;/code&gt; or &lt;code&gt;Windows&lt;/code&gt;, depending on the OS of the VM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If the experiment is interrupted abnormally (e.g., by interrupting signals), there may be residual docker containers which could affect system performance over time. Please run &lt;code&gt;docker stop $(docker ps -q) &amp;amp;&amp;amp; docker rm $(docker ps -a -q)&lt;/code&gt; to clean up.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;AWS&lt;/h3&gt; 
&lt;p&gt;Using cloud services for parallel evaluation can significantly accelerate evaluation efficiency (can reduce evaluation time to within 1 hour through parallelization!) and can even be used as infrastructure for training. We provide comprehensive AWS support with a Host-Client architecture that enables large-scale parallel evaluation of OSWorld tasks. For detailed setup instructions, see &lt;a href="https://github.com/xlang-ai/OSWorld/raw/main/PUBLIC_EVALUATION_GUIDELINE.md"&gt;Public Evaluation Guideline&lt;/a&gt; and &lt;a href="https://github.com/xlang-ai/OSWorld/raw/main/desktop_env/providers/aws/AWS_GUIDELINE.md"&gt;AWS Configuration Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Others&lt;/h3&gt; 
&lt;p&gt;We are working on supporting more üë∑. Please hold tight!&lt;/p&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Run the following minimal example to interact with the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from desktop_env.desktop_env import DesktopEnv

example = {
    "id": "94d95f96-9699-4208-98ba-3c3119edf9c2",
    "instruction": "I want to install Spotify on my current system. Could you please help me?",
    "config": [
        {
            "type": "execute",
            "parameters": {
                "command": [
                    "python",
                    "-c",
                    "import pyautogui; import time; pyautogui.click(960, 540); time.sleep(0.5);"
                ]
            }
        }
    ],
    "evaluator": {
        "func": "check_include_exclude",
        "result": {
            "type": "vm_command_line",
            "command": "which spotify"
        },
        "expected": {
            "type": "rule",
            "rules": {
                "include": ["spotify"],
                "exclude": ["not found"]
            }
        }
    }
}

env = DesktopEnv(action_space="pyautogui")

obs = env.reset(task_config=example)
obs, reward, done, info = env.step("pyautogui.rightClick()")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will see all the logs of the system running normally, including the successful creation of the environment, completion of setup, and successful execution of actions. In the end, you will observe a successful right-click on the screen, which means you are ready to go.&lt;/p&gt; 
&lt;h2&gt;üß™ Experiments&lt;/h2&gt; 
&lt;h3&gt;Agent Baselines&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important Configuration Requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Google Account Tasks&lt;/strong&gt;: Some tasks require Google account access and OAuth2.0 configuration. Please refer to &lt;a href="https://raw.githubusercontent.com/xlang-ai/OSWorld/main/ACCOUNT_GUIDELINE.md"&gt;Google Account Guideline&lt;/a&gt; for detailed setup instructions.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Proxy Configuration&lt;/strong&gt;: Some tasks may require proxy settings to function properly (this depends on the strength of website defenses against your network location). Please refer to your system's proxy configuration documentation.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Impact of Missing Configuration&lt;/strong&gt;: If these configurations are not properly set up, the corresponding tasks will fail to execute correctly, leading to lower evaluation scores.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you wish to run the baseline agent used in our paper, you can execute the following command as an example under the GPT-4o pure-screenshot setting:&lt;/p&gt; 
&lt;p&gt;Set &lt;strong&gt;OPENAI_API_KEY&lt;/strong&gt; environment variable with your API key&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY='changeme'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, set &lt;strong&gt;OPENAI_BASE_URL&lt;/strong&gt; to use a custom OpenAI-compatible API endpoint&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_BASE_URL='http://your-custom-endpoint.com/v1'  # Optional: defaults to https://api.openai.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Single-threaded execution (deprecated, using &lt;code&gt;vmware&lt;/code&gt; provider as example)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python run.py \
    --provider_name vmware \
    --path_to_vm Ubuntu/Ubuntu.vmx \
    --headless \
    --observation_type screenshot \
    --model gpt-4o \
    --sleep_after_execution 3 \
    --max_steps 15 \
    --result_dir ./results \
    --client_password password
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Parallel execution (example showing switching provider to &lt;code&gt;docker&lt;/code&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python run_multienv.py \
    --provider_name docker \
    --headless \
    --observation_type screenshot \
    --model gpt-4o \
    --sleep_after_execution 3 \
    --max_steps 15 \
    --num_envs 10 \
    --client_password password
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The results, which include screenshots, actions, and video recordings of the agent's task completion, will be saved in the &lt;code&gt;./results&lt;/code&gt; (or other &lt;code&gt;result_dir&lt;/code&gt; you specified) directory in this case. You can then run the following command to obtain the result:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python show_result.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;h3&gt;Local Evaluation&lt;/h3&gt; 
&lt;p&gt;Please start by reading through the &lt;a href="https://github.com/xlang-ai/OSWorld/raw/main/mm_agents/README.md"&gt;agent interface&lt;/a&gt; and the &lt;a href="https://github.com/xlang-ai/OSWorld/raw/main/desktop_env/README.md"&gt;environment interface&lt;/a&gt;. Correctly implement the agent interface and import your customized version in the &lt;code&gt;run.py&lt;/code&gt; or &lt;code&gt;run_multienv.py&lt;/code&gt; file. Afterward, you can execute a command similar to the one in the previous section to run the benchmark on your agent.&lt;/p&gt; 
&lt;h3&gt;Public Evaluation&lt;/h3&gt; 
&lt;p&gt;If you want your results to be verified and displayed on the verified leaderboard, you need to schedule a meeting with us (current maintainer: &lt;a href="mailto:tianbaoxiexxx@gmail.com"&gt;tianbaoxiexxx@gmail.com&lt;/a&gt;, &lt;a href="mailto:yuanmengqi732@gmail.com"&gt;yuanmengqi732@gmail.com&lt;/a&gt;) to run your agent code on our side and have us report the results. You need to upload and allow us to disclose your agent implementation under the OSWorld framework (you may choose not to expose your model API to the public), along with a report that allows the public to understand what's happening behind the scenes. Alternatively, if you are from a trusted institution, you can share your monitoring data and trajectories with us. Please carefully follow the &lt;a href="https://github.com/xlang-ai/OSWorld/raw/main/PUBLIC_EVALUATION_GUIDELINE.md"&gt;Public Evaluation Guideline&lt;/a&gt; to get results.&lt;/p&gt; 
&lt;h2&gt;‚ùì FAQ&lt;/h2&gt; 
&lt;h3&gt;What is the username and password for the virtual machines?&lt;/h3&gt; 
&lt;p&gt;The username and password for the virtual machines are as follows (for provider &lt;code&gt;vmware&lt;/code&gt;, &lt;code&gt;virtualbox&lt;/code&gt; and &lt;code&gt;docker&lt;/code&gt;): we set the account credentials for Ubuntu as &lt;code&gt;user&lt;/code&gt; / &lt;code&gt;password&lt;/code&gt;. For cloud service providers like &lt;code&gt;aws&lt;/code&gt;, to prevent attacks due to weak passwords, we default to &lt;code&gt;osworld-public-evaluation&lt;/code&gt;. If you make further modifications, remember to set the client_password variable and pass it to DesktopEnv and Agent (if supported) when running experiments. Some features like setting up proxy require the environment to have the client VM password to obtain sudo privileges, and for some OSWorld tasks, the agent needs the password to obtain sudo privileges to complete them.&lt;/p&gt; 
&lt;h3&gt;How to setup the account and credentials for Google and Google Drive?&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/xlang-ai/OSWorld/main/ACCOUNT_GUIDELINE.md"&gt;Account Guideline&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;How can I configure a proxy for the VM (if I'm behind the GFW, or I don't want some of my tasks to be identified as bot and get lower scores)?&lt;/h3&gt; 
&lt;p&gt;If you want to set it up yourself, please refer to &lt;a href="https://raw.githubusercontent.com/xlang-ai/OSWorld/main/PROXY_GUIDELINE.md"&gt;Proxy Guideline&lt;/a&gt;. We also provide a pre-configured solution based on dataimpulse, please refer to &lt;a href="https://github.com/xlang-ai/OSWorld/raw/main/PUBLIC_EVALUATION_GUIDELINE.md#22-proxy-setup"&gt;proxy-setup section in PUBLIC_EVALUATION_GUIDELINE&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Open Source Contributors&lt;/h3&gt; 
&lt;p&gt;Thanks to all the contributors!&lt;/p&gt; 
&lt;a href="https://github.com/xlang-ai/OSWorld/graphs/contributors"&gt; &lt;img src="https://stg.contrib.rocks/image?repo=xlang-ai/OSWorld" /&gt; &lt;/a&gt; 
&lt;h2&gt;üìÑ Citation&lt;/h2&gt; 
&lt;p&gt;If you find this environment useful, please consider citing our work:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{OSWorld,
      title={OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments}, 
      author={Tianbao Xie and Danyang Zhang and Jixuan Chen and Xiaochuan Li and Siheng Zhao and Ruisheng Cao and Toh Jing Hua and Zhoujun Cheng and Dongchan Shin and Fangyu Lei and Yitao Liu and Yiheng Xu and Shuyan Zhou and Silvio Savarese and Caiming Xiong and Victor Zhong and Tao Yu},
      year={2024},
      eprint={2404.07972},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement for OSWorld-Verified&lt;/h2&gt; 
&lt;p&gt;Special thanks to the following institutions that provided feedback and participated in the fixes (as well as institutions that provided feedback during the process): &lt;a href="https://www.moonshot.ai/"&gt;MoonShot AI, a.k.a. Kimi&lt;/a&gt;Ôºå&lt;a href="https://www.hud.so/"&gt;Human Data&lt;/a&gt;, &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://seed-tars.com/"&gt;ByteDance Seed TARS&lt;/a&gt;, &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt;, &lt;a href="https://www.simular.ai/"&gt;Simular&lt;/a&gt;, &lt;a href="https://sites.google.com/view/chaoh"&gt;HKU Data Intelligence Lab&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Special thanks to the following students who participated in the specific fixes: &lt;a href="https://yuanmengqi.github.io/"&gt;Mengqi Yuan&lt;/a&gt;, &lt;a href="https://zdy023.github.io/"&gt;Danyang Zhang&lt;/a&gt;, &lt;a href="https://thisisxxz.com/"&gt;Xinzhuang Xiong&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=JPwg5MwAAAAJ&amp;amp;hl=en"&gt;Zhennan Shen&lt;/a&gt;, &lt;a href="https://github.com/adlsdztony"&gt;Zilong Zhou&lt;/a&gt;, Yanxu Chen, &lt;a href="https://www.linkedin.com/in/jiaqideng"&gt;JIaqi Deng&lt;/a&gt;, &lt;a href="https://tianbaoxie.com/"&gt;Tianbao Xie&lt;/a&gt;, Junda Chen, &lt;a href="https://chenjix.github.io/"&gt;Jixuan Chen&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/haoyuan-wu-240878291/"&gt;Haoyuan Wu&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Special thanks to the following students who participated in running the re-evaluation: &lt;a href="https://yuanmengqi.github.io/"&gt;Mengqi Yuan&lt;/a&gt;, &lt;a href="https://github.com/adlsdztony"&gt;Zilong Zhou&lt;/a&gt;, &lt;a href="https://xinyuanwangcs.github.io/"&gt;Xinyuan Wang&lt;/a&gt;, &lt;a href="https://bowenbryanwang.github.io/"&gt;Bowen Wang&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; 
&lt;p&gt;&lt;a href="https://github.com/lfnovo/open-notebook/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;&lt;img src="https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/lfnovo/open-notebook"&gt; &lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Open Notebook&lt;/h3&gt; 
 &lt;p align="center"&gt; An open source, privacy-focused alternative to Google's Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href="https://www.open-notebook.ai"&gt;&lt;strong&gt;Checkout our website ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;üìö Get Started&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md"&gt;üìñ User Guide&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md"&gt;‚ú® Features&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;üöÄ Deploy&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üì¢ Open Notebook is under very active development&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Open Notebook is under active development! We're moving fast and making improvements every week. Your feedback is incredibly valuable to me during this exciting phase and it gives me motivation to keep improving and building this amazing tool. Please feel free to star the project if you find it useful, and don't hesitate to reach out with any questions or suggestions. I'm excited to see how you'll use it and what ideas you'll bring to the project! Let's build something amazing together! üöÄ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;About The Project&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png" alt="New Notebook" /&gt;&lt;/p&gt; 
&lt;p&gt;An open source, privacy-focused alternative to Google's Notebook LM. Why give Google more of our data when we can take control of our own research workflows?&lt;/p&gt; 
&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think üß† and acquire new knowledge üí°, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about our project at &lt;a href="https://www.open-notebook.ai"&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üÜö Open Notebook vs Google Notebook LM&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Open Notebook&lt;/th&gt; 
   &lt;th&gt;Google Notebook LM&lt;/th&gt; 
   &lt;th&gt;Advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-hosted, your data&lt;/td&gt; 
   &lt;td&gt;Google cloud only&lt;/td&gt; 
   &lt;td&gt;Complete data sovereignty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; 
   &lt;td&gt;Google models only&lt;/td&gt; 
   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; 
   &lt;td&gt;2 speakers only&lt;/td&gt; 
   &lt;td&gt;Extreme flexibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3 granular levels&lt;/td&gt; 
   &lt;td&gt;All-or-nothing&lt;/td&gt; 
   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom and built-in&lt;/td&gt; 
   &lt;td&gt;Limited options&lt;/td&gt; 
   &lt;td&gt;Unlimited processing power&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Full REST API&lt;/td&gt; 
   &lt;td&gt;No API&lt;/td&gt; 
   &lt;td&gt;Complete automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; 
   &lt;td&gt;Google hosted only&lt;/td&gt; 
   &lt;td&gt;Deploy anywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Comprehensive with sources&lt;/td&gt; 
   &lt;td&gt;Basic references&lt;/td&gt; 
   &lt;td&gt;Research integrity&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Open source, fully customizable&lt;/td&gt; 
   &lt;td&gt;Closed system&lt;/td&gt; 
   &lt;td&gt;Unlimited extensibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pay only for AI usage&lt;/td&gt; 
   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; 
   &lt;td&gt;Transparent and controllable&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; 
 &lt;li&gt;üí∞ &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://surrealdb.com/"&gt;&lt;img src="https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white" alt="SurrealDB" /&gt;&lt;/a&gt; &lt;a href="https://www.langchain.com/"&gt;&lt;img src="https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white" alt="LangChain" /&gt;&lt;/a&gt; &lt;a href="https://streamlit.io/"&gt;&lt;img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Streamlit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Ready to try Open Notebook? Choose your preferred method:&lt;/p&gt; 
&lt;h3&gt;‚ö° Instant Setup (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a new directory for your Open Notebook installation
mkdir open-notebook
cd open-notebook

# Using Docker - Get started in 2 minutes
docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key \
  lfnovo/open_notebook:latest-single
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;open-notebook/
‚îú‚îÄ‚îÄ notebook_data/     # Your notebooks and research content
‚îî‚îÄ‚îÄ surreal_data/      # Database files
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Access your installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üñ•Ô∏è Main Interface&lt;/strong&gt;: &lt;a href="http://localhost:8502"&gt;http://localhost:8502&lt;/a&gt; (Streamlit UI)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß API Access&lt;/strong&gt;: &lt;a href="http://localhost:5055"&gt;http://localhost:5055&lt;/a&gt; (REST API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö API Documentation&lt;/strong&gt;: &lt;a href="http://localhost:5055/docs"&gt;http://localhost:5055/docs&lt;/a&gt; (Interactive Swagger UI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Run from a dedicated folder&lt;/strong&gt;: Create and run this from inside a new &lt;code&gt;open-notebook&lt;/code&gt; folder so your data volumes are properly organized&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Volume persistence&lt;/strong&gt;: The volumes (&lt;code&gt;-v ./notebook_data:/app/data&lt;/code&gt; and &lt;code&gt;-v ./surreal_data:/mydata&lt;/code&gt;) are essential to persist your data between container restarts. Without them, you'll lose all your notebooks and research when the container stops.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üõ†Ô∏è Full Installation&lt;/h3&gt; 
&lt;p&gt;For development or customization:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üìñ Need Help?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href="https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant"&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; 
&lt;p&gt;Thanks to the &lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;LLM Support&lt;/th&gt; 
   &lt;th&gt;Embedding Support&lt;/th&gt; 
   &lt;th&gt;Speech-to-Text&lt;/th&gt; 
   &lt;th&gt;Text-to-Speech&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google (GenAI)&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertex AI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perplexity&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Voyage&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîí Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéôÔ∏è Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí¨ Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìù AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href="http://localhost:5055/docs"&gt;&lt;img src="https://img.shields.io/badge/API-Documentation-blue?style=flat-square" alt="API Docs" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîê Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìé Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Three-Column Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=D-760MlGwaI"&gt;&lt;img src="https://img.youtube.com/vi/D-760MlGwaI/0.jpg" alt="Check out our podcast sample" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md"&gt;üìñ Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;‚ö° Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;üîß Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md"&gt;üéØ Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guide&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md"&gt;üì± Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md"&gt;üìö Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md"&gt;üìÑ Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md"&gt;üìù Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md"&gt;üí¨ Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md"&gt;üîç Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md"&gt;üéôÔ∏è Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md"&gt;üîß Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md"&gt;ü§ñ AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md"&gt;üîß REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md"&gt;üîê Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;üöÄ Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;üó∫Ô∏è Roadmap&lt;/h2&gt; 
&lt;h3&gt;Upcoming Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;React Frontend&lt;/strong&gt;: Modern React-based frontend to replace Streamlit&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recently Completed ‚úÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;ü§ù Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;&lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;&lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;‚≠ê &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions! We're especially looking for help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help build a modern React-based UI (planned replacement for current Streamlit interface)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, SurrealDB, Streamlit&lt;br /&gt; &lt;strong&gt;Future Roadmap&lt;/strong&gt;: React frontend, enhanced real-time updates&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üìû Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href="https://twitter.com/lfnovo"&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;üåê &lt;a href="https://www.open-notebook.ai"&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/surreal-commands"&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/content-core"&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>trycua/cua</title>
      <link>https://github.com/trycua/cua</link>
      <description>&lt;p&gt;c/ua is the Docker Container for Computer-Use AI Agents.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" alt="Cua logo" height="150" srcset="img/logo_white.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" alt="Cua logo" height="150" srcset="img/logo_black.png" /&gt; 
  &lt;img alt="Cua logo" height="150" src="https://raw.githubusercontent.com/trycua/cua/main/img/logo_black.png" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#"&gt;&lt;img src="https://img.shields.io/badge/Python-333333?logo=python&amp;amp;logoColor=white&amp;amp;labelColor=333333" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#"&gt;&lt;img src="https://img.shields.io/badge/Swift-F05138?logo=swift&amp;amp;logoColor=white" alt="Swift" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#"&gt;&lt;img src="https://img.shields.io/badge/macOS-000000?logo=apple&amp;amp;logoColor=F0F0F0" alt="macOS" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/mVnXXpdE85"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://trendshift.io/repositories/13685" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13685" alt="trycua%2Fcua | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;cua&lt;/strong&gt; ("koo-ah") is Docker for &lt;a href="https://www.oneusefulthing.org/p/when-you-give-a-claude-a-mouse"&gt;Computer-Use Agents&lt;/a&gt; - it enables AI agents to control full operating systems in virtual containers and deploy them locally or to the cloud.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;video src="https://github.com/user-attachments/assets/c619b4ea-bb8e-4382-860e-f3757e36af20" width="800" controls&gt;&lt;/video&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Check out more demos of the Computer-Use Agent in action &lt;/b&gt;&lt;/summary&gt; 
 &lt;details open&gt; 
  &lt;summary&gt;&lt;b&gt;MCP Server: Work with Claude Desktop and Tableau&lt;/b&gt;&lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;div align="center"&gt; 
   &lt;video src="https://github.com/user-attachments/assets/9f573547-5149-493e-9a72-396f3cff29df" width="800" controls&gt;&lt;/video&gt; 
  &lt;/div&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;b&gt;AI-Gradio: Multi-app workflow with browser, VS Code and terminal&lt;/b&gt;&lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;div align="center"&gt; 
   &lt;video src="https://github.com/user-attachments/assets/723a115d-1a07-4c8e-b517-88fbdf53ed0f" width="800" controls&gt;&lt;/video&gt; 
  &lt;/div&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;b&gt;Notebook: Fix GitHub issue in Cursor&lt;/b&gt;&lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;div align="center"&gt; 
   &lt;video src="https://github.com/user-attachments/assets/f67f0107-a1e1-46dc-aa9f-0146eb077077" width="800" controls&gt;&lt;/video&gt; 
  &lt;/div&gt; 
 &lt;/details&gt; 
&lt;/details&gt;
&lt;br /&gt; 
&lt;h1&gt;üöÄ Quick Start with a Computer-Use Agent UI&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Need to automate desktop tasks? Launch the Computer-Use Agent UI with a single command.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Option 1: Fully-managed install with Docker (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Docker-based guided install for quick use&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;macOS/Linux/Windows (via WSL):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Requires Docker
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/scripts/playground-docker.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This script will guide you through setup using Docker containers and launch the Computer-Use Agent UI.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option 2: &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/.devcontainer/README.md"&gt;Dev Container&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Best for contributors and development&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;This repository includes a &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/.devcontainer/README.md"&gt;Dev Container&lt;/a&gt; configuration that simplifies setup to a few steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Install the Dev Containers extension (&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;VS Code&lt;/a&gt; or &lt;a href="https://docs.windsurf.com/windsurf/advanced#dev-containers-beta"&gt;WindSurf&lt;/a&gt;)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open the repository in the Dev Container:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Press &lt;code&gt;Ctrl+Shift+P&lt;/code&gt; (or &lt;code&gt;‚åò+Shift+P&lt;/code&gt; on macOS)&lt;/li&gt; 
   &lt;li&gt;Select &lt;code&gt;Dev Containers: Clone Repository in Container Volume...&lt;/code&gt; and paste the repository URL: &lt;code&gt;https://github.com/trycua/cua.git&lt;/code&gt; (if not cloned) or &lt;code&gt;Dev Containers: Open Folder in Container...&lt;/code&gt; (if git cloned).&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: On WindSurf, the post install hook might not run automatically. If so, run &lt;code&gt;/bin/bash .devcontainer/post-install.sh&lt;/code&gt; manually.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open the VS Code workspace:&lt;/strong&gt; Once the post-install.sh is done running, open the &lt;code&gt;.vscode/py.code-workspace&lt;/code&gt; workspace and press &lt;img src="https://github.com/user-attachments/assets/923bdd43-8c8f-4060-8d78-75bfa302b48c" alt="Open Workspace" /&gt; .&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Run the Agent UI example:&lt;/strong&gt; Click &lt;img src="https://github.com/user-attachments/assets/7a61ef34-4b22-4dab-9864-f86bf83e290b" alt="Run Agent UI" /&gt; to start the Gradio UI. If prompted to install &lt;strong&gt;debugpy (Python Debugger)&lt;/strong&gt; to enable remote debugging, select 'Yes' to proceed.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Access the Gradio UI:&lt;/strong&gt; The Gradio UI will be available at &lt;code&gt;http://localhost:7860&lt;/code&gt; and will automatically forward to your host machine.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option 3: PyPI&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Direct Python package installation&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# conda create -yn cua python==3.12

pip install -U "cua-computer[all]" "cua-agent[all]"
python -m agent.ui # Start the agent UI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or check out the &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#-usage-guide"&gt;Usage Guide&lt;/a&gt; to learn how to use our Python SDK in your own code.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Supported &lt;a href="https://github.com/trycua/cua/raw/main/libs/python/agent/README.md#agent-loops"&gt;Agent Loops&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/trycua/cua/raw/main/libs/python/agent/README.md#agent-loops"&gt;UITARS-1.5&lt;/a&gt; - Run locally on Apple Silicon with MLX, or use cloud providers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/trycua/cua/raw/main/libs/python/agent/README.md#agent-loops"&gt;OpenAI CUA&lt;/a&gt; - Use OpenAI's Computer-Use Preview model&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/trycua/cua/raw/main/libs/python/agent/README.md#agent-loops"&gt;Anthropic CUA&lt;/a&gt; - Use Anthropic's Computer-Use capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/trycua/cua/raw/main/libs/python/agent/README.md#agent-loops"&gt;OmniParser-v2.0&lt;/a&gt; - Control UI with &lt;a href="https://som-gpt4v.github.io/"&gt;Set-of-Marks prompting&lt;/a&gt; using any vision model&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üñ•Ô∏è Compatibility&lt;/h2&gt; 
&lt;p&gt;For detailed compatibility information including host OS support, VM emulation capabilities, and model provider compatibility, see the &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/COMPATIBILITY.md"&gt;Compatibility Matrix&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h1&gt;üêç Usage Guide&lt;/h1&gt; 
&lt;p&gt;Follow these steps to use Cua in your own Python code. See &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/docs/Developer-Guide.md"&gt;Developer Guide&lt;/a&gt; for building from source.&lt;/p&gt; 
&lt;h3&gt;Step 1: Install Lume CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Lume CLI manages high-performance macOS/Linux VMs with near-native speed on Apple Silicon.&lt;/p&gt; 
&lt;h3&gt;Step 2: Pull the macOS CUA Image&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;lume pull macos-sequoia-cua:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The macOS CUA image contains the default Mac apps and the Computer Server for easy automation.&lt;/p&gt; 
&lt;h3&gt;Step 3: Install Python SDK&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "cua-computer[all]" "cua-agent[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 4: Use in Your Code&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from computer import Computer
from agent import ComputerAgent

async def main():
    # Start a local macOS VM
    computer = Computer(os_type="macos")
    await computer.run()

    # Or with Cua Cloud Container
    computer = Computer(
      os_type="linux",
      api_key="your_cua_api_key_here",
      name="your_container_name_here"
    )

    # Example: Direct control of a macOS VM with Computer
    computer.interface.delay = 0.1 # Wait 0.1 seconds between kb/m actions
    await computer.interface.left_click(100, 200)
    await computer.interface.type_text("Hello, world!")
    screenshot_bytes = await computer.interface.screenshot()
    
    # Example: Create and run an agent locally using mlx-community/UI-TARS-1.5-7B-6bit
    agent = ComputerAgent(
      model="mlx/mlx-community/UI-TARS-1.5-7B-6bit",
      tools=[computer],
    )
    async for result in agent.run("Find the trycua/cua repository on GitHub and follow the quick start guide"):
        print(result)

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For ready-to-use examples, check out our &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/notebooks/"&gt;Notebooks&lt;/a&gt; collection.&lt;/p&gt; 
&lt;h3&gt;Lume CLI Reference&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Lume CLI and background service
curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash

# List all VMs
lume ls

# Pull a VM image
lume pull macos-sequoia-cua:latest

# Create a new VM
lume create my-vm --os macos --cpu 4 --memory 8GB --disk-size 50GB

# Run a VM (creates and starts if it doesn't exist)
lume run macos-sequoia-cua:latest

# Stop a VM
lume stop macos-sequoia-cua_latest

# Delete a VM
lume delete macos-sequoia-cua_latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Lumier CLI Reference&lt;/h3&gt; 
&lt;p&gt;For advanced container-like virtualization, check out &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lumier/README.md"&gt;Lumier&lt;/a&gt; - a Docker interface for macOS and Linux VMs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Lume CLI and background service
curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash

# Run macOS in a Docker container
docker run -it --rm \
    --name lumier-vm \
    -p 8006:8006 \
    -v $(pwd)/storage:/storage \
    -v $(pwd)/shared:/shared \
    -e VM_NAME=lumier-vm \
    -e VERSION=ghcr.io/trycua/macos-sequoia-cua:latest \
    -e CPU_CORES=4 \
    -e RAM_SIZE=8192 \
    -e HOST_STORAGE_PATH=$(pwd)/storage \
    -e HOST_SHARED_PATH=$(pwd)/shared \
    trycua/lumier:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/mcp-server/README.md"&gt;How to use the MCP Server with Claude Desktop or other MCP clients&lt;/a&gt; - One of the easiest ways to get started with Cua&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/agent/README.md"&gt;How to use OpenAI Computer-Use, Anthropic, OmniParser, or UI-TARS for your Computer-Use Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lume/README.md"&gt;How to use Lume CLI for managing desktops&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.trycua.com/blog/training-computer-use-models-trajectories-1"&gt;Training Computer-Use Models: Collecting Human Trajectories with Cua (Part 1)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.trycua.com/blog/build-your-own-operator-on-macos-1"&gt;Build Your Own Operator on macOS (Part 1)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Modules&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Installation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lume/README.md"&gt;&lt;strong&gt;Lume&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;VM management for macOS/Linux using Apple's Virtualization.Framework&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/lumier/README.md"&gt;&lt;strong&gt;Lumier&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Docker interface for macOS and Linux VMs&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;docker pull trycua/lumier:latest&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/computer/README.md"&gt;&lt;strong&gt;Computer (Python)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python Interface for controlling virtual machines&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install "cua-computer[all]"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/typescript/computer/README.md"&gt;&lt;strong&gt;Computer (Typescript)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Typescript Interface for controlling virtual machines&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;npm install @trycua/computer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/agent/README.md"&gt;&lt;strong&gt;Agent&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AI agent framework for automating tasks&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install "cua-agent[all]"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/mcp-server/README.md"&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP server for using CUA with Claude Desktop&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-mcp-server&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/som/README.md"&gt;&lt;strong&gt;SOM&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Self-of-Mark library for Agent&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-som&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/computer-server/README.md"&gt;&lt;strong&gt;Computer Server&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Server component for Computer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-computer-server&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/python/core/README.md"&gt;&lt;strong&gt;Core (Python)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python Core utilities&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install cua-core&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/libs/typescript/core/README.md"&gt;&lt;strong&gt;Core (Typescript)&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Typescript Core utilities&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;npm install @trycua/core&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Computer Interface Reference&lt;/h2&gt; 
&lt;p&gt;For complete examples, see &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/examples/computer_examples.py"&gt;computer_examples.py&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/notebooks/computer_nb.ipynb"&gt;computer_nb.ipynb&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Shell Actions
result = await computer.interface.run_command(cmd)       # Run shell command
# result.stdout, result.stderr, result.returncode

# Mouse Actions
await computer.interface.left_click(x, y)       # Left click at coordinates
await computer.interface.right_click(x, y)      # Right click at coordinates
await computer.interface.double_click(x, y)     # Double click at coordinates
await computer.interface.move_cursor(x, y)      # Move cursor to coordinates
await computer.interface.drag_to(x, y, duration)  # Drag to coordinates
await computer.interface.get_cursor_position()  # Get current cursor position
await computer.interface.mouse_down(x, y, button="left")  # Press and hold a mouse button
await computer.interface.mouse_up(x, y, button="left")    # Release a mouse button

# Keyboard Actions
await computer.interface.type_text("Hello")     # Type text
await computer.interface.press_key("enter")     # Press a single key
await computer.interface.hotkey("command", "c") # Press key combination
await computer.interface.key_down("command")    # Press and hold a key
await computer.interface.key_up("command")      # Release a key

# Scrolling Actions
await computer.interface.scroll(x, y)           # Scroll the mouse wheel
await computer.interface.scroll_down(clicks)    # Scroll down
await computer.interface.scroll_up(clicks)      # Scroll up

# Screen Actions
await computer.interface.screenshot()           # Take a screenshot
await computer.interface.get_screen_size()      # Get screen dimensions

# Clipboard Actions
await computer.interface.set_clipboard(text)    # Set clipboard content
await computer.interface.copy_to_clipboard()    # Get clipboard content

# File System Operations
await computer.interface.file_exists(path)      # Check if file exists
await computer.interface.directory_exists(path) # Check if directory exists
await computer.interface.read_text(path, encoding="utf-8")        # Read file content
await computer.interface.write_text(path, content, encoding="utf-8") # Write file content
await computer.interface.read_bytes(path)       # Read file content as bytes
await computer.interface.write_bytes(path, content) # Write file content as bytes
await computer.interface.delete_file(path)      # Delete file
await computer.interface.create_dir(path)       # Create directory
await computer.interface.delete_dir(path)       # Delete directory
await computer.interface.list_dir(path)         # List directory contents

# Accessibility
await computer.interface.get_accessibility_tree() # Get accessibility tree

# Delay Configuration
# Set default delay between all actions (in seconds)
computer.interface.delay = 0.5  # 500ms delay between actions

# Or specify delay for individual actions
await computer.interface.left_click(x, y, delay=1.0)     # 1 second delay after click
await computer.interface.type_text("Hello", delay=0.2)   # 200ms delay after typing
await computer.interface.press_key("enter", delay=0.5)   # 500ms delay after key press

# Python Virtual Environment Operations
await computer.venv_install("demo_venv", ["requests", "macos-pyxa"]) # Install packages in a virtual environment
await computer.venv_cmd("demo_venv", "python -c 'import requests; print(requests.get(`https://httpbin.org/ip`).json())'") # Run a shell command in a virtual environment
await computer.venv_exec("demo_venv", python_function_or_code, *args, **kwargs) # Run a Python function in a virtual environment and return the result / raise an exception

# Example: Use sandboxed functions to execute code in a Cua Container
from computer.helpers import sandboxed

@sandboxed("demo_venv")
def greet_and_print(name):
    """Get the HTML of the current Safari tab"""
    import PyXA
    safari = PyXA.Application("Safari")
    html = safari.current_document.source()
    print(f"Hello from inside the container, {name}!")
    return {"greeted": name, "safari_html": html}

# When a @sandboxed function is called, it will execute in the container
result = await greet_and_print("Cua")
# Result: {"greeted": "Cua", "safari_html": "&amp;lt;html&amp;gt;...&amp;lt;/html&amp;gt;"}
# stdout and stderr are also captured and printed / raised
print("Result from sandboxed function:", result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ComputerAgent Reference&lt;/h2&gt; 
&lt;p&gt;For complete examples, see &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/examples/agent_examples.py"&gt;agent_examples.py&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/notebooks/agent_nb.ipynb"&gt;agent_nb.ipynb&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Import necessary components
from agent import ComputerAgent

# UI-TARS-1.5 agent for local execution with MLX
ComputerAgent(model="mlx/mlx-community/UI-TARS-1.5-7B-6bit")   
# OpenAI Computer-Use agent using OPENAI_API_KEY  
ComputerAgent(model="computer-use-preview")
# Anthropic Claude agent using ANTHROPIC_API_KEY
ComputerAgent(model="anthropic/claude-3-5-sonnet-20240620")

# OmniParser loop for UI control using Set-of-Marks (SOM) prompting and any vision LLM
ComputerAgent(model="omniparser+ollama_chat/gemma3:12b-it-q4_K_M")      
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.com/invite/mVnXXpdE85"&gt;Discord community&lt;/a&gt; to discuss ideas, get assistance, or share your demos!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Cua is open-sourced under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;Microsoft's OmniParser, which is used in this project, is licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0) - see the &lt;a href="https://github.com/microsoft/OmniParser/raw/master/LICENSE"&gt;OmniParser LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to CUA! Please refer to our &lt;a href="https://raw.githubusercontent.com/trycua/cua/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;Apple, macOS, and Apple Silicon are trademarks of Apple Inc. Ubuntu and Canonical are registered trademarks of Canonical Ltd. Microsoft is a registered trademark of Microsoft Corporation. This project is not affiliated with, endorsed by, or sponsored by Apple Inc., Canonical Ltd., or Microsoft Corporation.&lt;/p&gt; 
&lt;h2&gt;Stargazers&lt;/h2&gt; 
&lt;p&gt;Thank you to all our supporters!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/trycua/cua"&gt;&lt;img src="https://starchart.cc/trycua/cua.svg?variant=adaptive" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/f-trycua"&gt;&lt;img src="https://avatars.githubusercontent.com/u/195596869?v=4?s=100" width="100px;" alt="f-trycua" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;f-trycua&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-f-trycua" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://pepicrft.me"&gt;&lt;img src="https://avatars.githubusercontent.com/u/663605?v=4?s=100" width="100px;" alt="Pedro Pi√±era Buend√≠a" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Pedro Pi√±era Buend√≠a&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-pepicrft" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://iamit.in"&gt;&lt;img src="https://avatars.githubusercontent.com/u/5647941?v=4?s=100" width="100px;" alt="Amit Kumar" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Amit Kumar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-aktech" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://productsway.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/870029?v=4?s=100" width="100px;" alt="Dung Duc Huynh (Kaka)" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Dung Duc Huynh (Kaka)&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-jellydn" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="http://zaydkrunz.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/70227235?v=4?s=100" width="100px;" alt="Zayd Krunz" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zayd Krunz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-ShrootBuck" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/PrashantRaj18198"&gt;&lt;img src="https://avatars.githubusercontent.com/u/23168997?v=4?s=100" width="100px;" alt="Prashant Raj" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Prashant Raj&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-PrashantRaj18198" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.mobile.dev"&gt;&lt;img src="https://avatars.githubusercontent.com/u/847683?v=4?s=100" width="100px;" alt="Leland Takamine" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Leland Takamine&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-Leland-Takamine" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/ddupont808"&gt;&lt;img src="https://avatars.githubusercontent.com/u/3820588?v=4?s=100" width="100px;" alt="ddupont" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ddupont&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-ddupont808" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/Lizzard1123"&gt;&lt;img src="https://avatars.githubusercontent.com/u/46036335?v=4?s=100" width="100px;" alt="Ethan Gutierrez" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ethan Gutierrez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-Lizzard1123" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://ricterz.me"&gt;&lt;img src="https://avatars.githubusercontent.com/u/5282759?v=4?s=100" width="100px;" alt="Ricter Zheng" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ricter Zheng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-RicterZ" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://www.trytruffle.ai/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50844303?v=4?s=100" width="100px;" alt="Rahul Karajgikar" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rahul Karajgikar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-rahulkarajgikar" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/trospix"&gt;&lt;img src="https://avatars.githubusercontent.com/u/81363696?v=4?s=100" width="100px;" alt="trospix" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;trospix&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-trospix" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="14.28%"&gt;&lt;a href="https://github.com/evnsnclr"&gt;&lt;img src="https://avatars.githubusercontent.com/u/139897548?v=4?s=100" width="100px;" alt="Evan smith" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Evan smith&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href="https://raw.githubusercontent.com/trycua/cua/main/#code-evnsnclr" title="Code"&gt;üíª&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;h3&gt; &lt;p&gt;&lt;a href="https://discord.gg/EUnjGpsmWw"&gt;Discord&lt;/a&gt; | &lt;a href="https://t.me/+Kh-KqHTzFYg3MGNk"&gt;Telegram&lt;/a&gt; | &lt;a href="https://x.com/exolabs"&gt;X&lt;/a&gt;&lt;/p&gt; &lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main"&gt;&lt;img src="https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License: GPL v3" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;Update: exo is hiring. See &lt;a href="https://exolabs.net"&gt;here&lt;/a&gt; for more details.&lt;/h2&gt; 
 &lt;h2&gt;Interested in running exo in your business? &lt;a href="mailto:hello@exolabs.net"&gt;Contact us&lt;/a&gt; to discuss.&lt;/h2&gt; 
&lt;/div&gt; 
&lt;h2&gt;Get Involved&lt;/h2&gt; 
&lt;p&gt;exo is &lt;strong&gt;experimental&lt;/strong&gt; software. Expect bugs early on. Create issues so they can be fixed. The &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt; team will strive to resolve issues quickly.&lt;/p&gt; 
&lt;p&gt;We also welcome contributions from the community. We have a list of bounties in &lt;a href="https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing"&gt;this sheet&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Wide Model Support&lt;/h3&gt; 
&lt;p&gt;exo supports different models including LLaMA (&lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/models/llama.py"&gt;MLX&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/models/llama.py"&gt;tinygrad&lt;/a&gt;), Mistral, LlaVA, Qwen, and Deepseek.&lt;/p&gt; 
&lt;h3&gt;Dynamic Model Partitioning&lt;/h3&gt; 
&lt;p&gt;exo &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;optimally splits up models&lt;/a&gt; based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.&lt;/p&gt; 
&lt;h3&gt;Automatic Device Discovery&lt;/h3&gt; 
&lt;p&gt;exo will &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154"&gt;automatically discover&lt;/a&gt; other devices using the best method available. Zero manual configuration.&lt;/p&gt; 
&lt;h3&gt;ChatGPT-compatible API&lt;/h3&gt; 
&lt;p&gt;exo provides a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/api/chatgpt_api.py"&gt;ChatGPT-compatible API&lt;/a&gt; for running models. It's a &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/examples/chatgpt_api.sh"&gt;one-line change&lt;/a&gt; in your application to run models on your own hardware using exo.&lt;/p&gt; 
&lt;h3&gt;Device Equality&lt;/h3&gt; 
&lt;p&gt;Unlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices &lt;a href="https://github.com/exo-explore/exo/raw/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161"&gt;connect p2p&lt;/a&gt;. As long as a device is connected somewhere in the network, it can be used to run models.&lt;/p&gt; 
&lt;p&gt;Exo supports different &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/partitioning_strategy.py"&gt;partitioning strategies&lt;/a&gt; to split up a model across devices. The default partitioning strategy is &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/topology/ring_memory_weighted_partitioning_strategy.py"&gt;ring memory weighted partitioning&lt;/a&gt;. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-screenshot.jpg" alt="&amp;quot;A screenshot of exo running 5 nodes" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The current recommended way to install exo is from source.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python&amp;gt;=3.12.0 is required because of &lt;a href="https://github.com/exo-explore/exo/issues/5"&gt;issues with asyncio&lt;/a&gt; in previous versions.&lt;/li&gt; 
 &lt;li&gt;For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA): 
  &lt;ul&gt; 
   &lt;li&gt;NVIDIA driver - verify with &lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;CUDA toolkit - install from &lt;a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation"&gt;NVIDIA CUDA guide&lt;/a&gt;, verify with &lt;code&gt;nvcc --version&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;cuDNN library - download from &lt;a href="https://developer.nvidia.com/cudnn-downloads"&gt;NVIDIA cuDNN page&lt;/a&gt;, verify installation by following &lt;a href="https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older"&gt;these steps&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Hardware Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total: 
  &lt;ul&gt; 
   &lt;li&gt;2 x 8GB M3 MacBook Airs&lt;/li&gt; 
   &lt;li&gt;1 x 16GB NVIDIA RTX 4070 Ti Laptop&lt;/li&gt; 
   &lt;li&gt;2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/exo-explore/exo.git
cd exo
pip install -e .
# alternatively, with venv
source install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If running on Mac, MLX has an &lt;a href="https://ml-explore.github.io/mlx/build/html/install.html"&gt;install guide&lt;/a&gt; with troubleshooting steps.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;There are a number of things users have empirically found to improve performance on Apple Silicon Macs:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upgrade to the latest version of macOS Sequoia.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;./configure_mlx.sh&lt;/code&gt;. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Example Usage on Multiple macOS Devices&lt;/h3&gt; 
&lt;h4&gt;Device 1:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Device 2:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! No configuration required - exo will automatically discover the other device(s).&lt;/p&gt; 
&lt;p&gt;exo starts a ChatGPT-like WebUI (powered by &lt;a href="https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat"&gt;tinygrad tinychat&lt;/a&gt;) on &lt;a href="http://localhost:52415"&gt;http://localhost:52415&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For developers, exo also starts a ChatGPT-compatible API endpoint on &lt;a href="http://localhost:52415/v1/chat/completions"&gt;http://localhost:52415/v1/chat/completions&lt;/a&gt;. Examples with curl:&lt;/p&gt; 
&lt;h4&gt;Llama 3.2 3B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.2-3b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llama 3.1 405B:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llama-3.1-405b",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;DeepSeek R1 (full 671B):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "deepseek-r1",
     "messages": [{"role": "user", "content": "What is the meaning of exo?"}],
     "temperature": 0.7
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Llava 1.5 7B (Vision Language Model):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl http://localhost:52415/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "llava-1.5-7b-hf",
     "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What are these?"
          },
          {
            "type": "image_url",
            "image_url": {
              "url": "http://images.cocodataset.org/val2017/000000039769.jpg"
            }
          }
        ]
      }
    ],
     "temperature": 0.0
   }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Usage on Multiple Heterogenous Devices (macOS + Linux)&lt;/h3&gt; 
&lt;h4&gt;Device 1 (macOS):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: We don't need to explicitly tell exo to use the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine. &lt;strong&gt;MLX&lt;/strong&gt; and &lt;strong&gt;tinygrad&lt;/strong&gt; are interoperable!&lt;/p&gt; 
&lt;h4&gt;Device 2 (Linux):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Linux devices will automatically default to using the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine.&lt;/p&gt; 
&lt;p&gt;You can read about tinygrad-specific env vars &lt;a href="https://docs.tinygrad.org/env_vars/"&gt;here&lt;/a&gt;. For example, you can configure tinygrad to use the cpu by specifying &lt;code&gt;CLANG=1&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage on a single device with "exo run" command&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With a custom prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;exo run llama-3.2-3b --prompt "What is the meaning of exo?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Storage&lt;/h3&gt; 
&lt;p&gt;Models by default are stored in &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can set a different model storage location by setting the &lt;code&gt;EXO_HOME&lt;/code&gt; env var.&lt;/p&gt; 
&lt;h2&gt;Model Downloading&lt;/h2&gt; 
&lt;p&gt;Models are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the &lt;code&gt;~/.cache/exo/downloads&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;To download models from a proxy endpoint, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable. For example, to run exo with the huggingface mirror endpoint:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;HF_ENDPOINT=https://hf-mirror.com exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Debugging&lt;/h2&gt; 
&lt;p&gt;Enable debug logs with the DEBUG environment variable (0-9).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;DEBUG=9 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the &lt;strong&gt;tinygrad&lt;/strong&gt; inference engine specifically, there is a separate DEBUG flag &lt;code&gt;TINYGRAD_DEBUG&lt;/code&gt; that can be used to enable debug logs (1-6).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;TINYGRAD_DEBUG=2 exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Formatting&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/google/yapf"&gt;yapf&lt;/a&gt; to format the code. To format the code, first install the formatting requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install -e '.[formatting]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run the formatting script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python3 format.py ./exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the &lt;code&gt;Install Certificates&lt;/code&gt; command, typicall as follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;/Applications/Python 3.x/Install Certificates.command
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöß As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email &lt;a href="mailto:alex@exolabs.net"&gt;alex@exolabs.net&lt;/a&gt; with your GitHub username explaining your use-case and you will be granted access on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Inference Engines&lt;/h2&gt; 
&lt;p&gt;exo supports the following inference engines:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/mlx/sharded_inference_engine.py"&gt;MLX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/inference/tinygrad/inference.py"&gt;tinygrad&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß &lt;a href="https://github.com/exo-explore/exo/pull/139"&gt;PyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß &lt;a href="https://github.com/exo-explore/exo/issues/167"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Discovery Modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/udp"&gt;UDP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/manual"&gt;Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/tailscale"&gt;Tailscale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß Radio&lt;/li&gt; 
 &lt;li&gt;üöß Bluetooth&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Peer Networking Modules&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/exo/networking/grpc"&gt;GRPC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöß NCCL&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>modelscope/FunASR</title>
      <link>https://github.com/modelscope/FunASR</link>
      <description>&lt;p&gt;A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/README_zh.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;|English)&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Akshay090/svg-banners"&gt;&lt;img src="https://svg-banners.vercel.app/api?type=origin&amp;amp;text1=FunASR%F0%9F%A4%A0&amp;amp;text2=%F0%9F%92%96%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&amp;amp;width=800&amp;amp;height=210" alt="SVG Banners" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/funasr/"&gt;&lt;img src="https://img.shields.io/pypi/v/funasr" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/3839" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3839" alt="alibaba-damo-academy%2FFunASR | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;FunASR&lt;/strong&gt; hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training &amp;amp; finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#highlights"&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/alibaba-damo-academy/FunASR#whats-new"&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#installation"&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#quick-start"&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/alibaba-damo-academy/FunASR/raw/main/docs/tutorial/README.md"&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;&lt;strong&gt;Runtime&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#model-zoo"&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#contact"&gt;&lt;strong&gt;Contact&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a name="highlights"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.&lt;/li&gt; 
 &lt;li&gt;We have released a vast collection of academic and industrial pretrained models on the &lt;a href="https://www.modelscope.cn/models?page=1&amp;amp;tasks=auto-speech-recognition"&gt;ModelScope&lt;/a&gt; and &lt;a href="https://huggingface.co/FunASR"&gt;huggingface&lt;/a&gt;, which can be accessed through our &lt;a href="https://github.com/alibaba-damo-academy/FunASR/raw/main/docs/model_zoo/modelscope_models.md"&gt;Model Zoo&lt;/a&gt;. The representative &lt;a href="https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary"&gt;Paraformer-large&lt;/a&gt;, a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme_cn.md"&gt;service deployment document&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a name="whats-new"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's new:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal modelÔºõ(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;);&lt;/li&gt; 
 &lt;li&gt;2024/10/10ÔºöAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/whisper/demo.py"&gt;modelscope&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/whisper/demo_from_openai.py"&gt;openai&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak &amp;amp; Support the SensevoiceSmall onnx modelÔºõFile Transcription Service 2.0 GPU released, Fix GPU memory leak; (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;);&lt;/li&gt; 
 &lt;li&gt;2024/09/25Ôºökeyword spotting models are new supported. Supports fine-tuning and inference for four models: &lt;a href="https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online"&gt;fsmn_kws&lt;/a&gt;, &lt;a href="https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online"&gt;fsmn_kws_mt&lt;/a&gt;, &lt;a href="https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline"&gt;sanm_kws&lt;/a&gt;, &lt;a href="https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online"&gt;sanm_kws_streaming&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2024/07/04Ôºö&lt;a href="https://github.com/FunAudioLLM/SenseVoice"&gt;SenseVoice&lt;/a&gt; is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.&lt;/li&gt; 
 &lt;li&gt;2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads' speedup is 1200+ (compared to 330+ on CPU); ref to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;2024/05/15Ôºöemotion recognition models are new supported. &lt;a href="https://modelscope.cn/models/iic/emotion2vec_plus_large/summary"&gt;emotion2vec+large&lt;/a&gt;Ôºå&lt;a href="https://modelscope.cn/models/iic/emotion2vec_plus_base/summary"&gt;emotion2vec+base&lt;/a&gt;Ôºå&lt;a href="https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary"&gt;emotion2vec+seed&lt;/a&gt;. currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.&lt;/li&gt; 
 &lt;li&gt;2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structureÔºõ(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt;
 &lt;summary&gt;Full Changelog&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;2024/03/05ÔºöAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/qwen_audio"&gt;usage&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;2024/03/05ÔºöAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/whisper/demo.py"&gt;modelscope&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/whisper/demo_from_openai.py"&gt;openai&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5ÔºåReal-time Transcription Service 1.9 releasedÔºådocker image supports ARM64 platform, update modelscopeÔºõ(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/30Ôºöfunasr-1.0 has been released (&lt;a href="https://github.com/alibaba-damo-academy/FunASR/discussions/1319"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/30Ôºöemotion recognition models are new supported. &lt;a href="https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary"&gt;model link&lt;/a&gt;, modified from &lt;a href="https://github.com/ddlBoJack/emotion2vec"&gt;repo&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 releasedÔºåoptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 releasedÔºåoptimizatized the client-sideÔºõ(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes(&lt;a href="https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary"&gt;FunASR-Runtime-Windows&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md#file-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2024/01/03: Real-time Transcription Service 1.6 releasedÔºåThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md#the-real-time-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/03: Fixed known crash issues as well as memory leak problems, (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md#file-transcription-service-english-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes(&lt;a href="https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary"&gt;FunASR-Runtime-Windows&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime#file-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime#file-transcription-service-english-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/10/13: &lt;a href="https://slidespeech.github.io/"&gt;SlideSpeech&lt;/a&gt;: A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.&lt;/li&gt; 
  &lt;li&gt;2023/10/10: The ASR-SpeakersDiarization combined pipeline &lt;a href="https://github.com/alibaba-damo-academy/FunASR/raw/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py"&gt;Paraformer-VAD-SPK&lt;/a&gt; is now released. Experience the model to get recognition results with speaker information.&lt;/li&gt; 
  &lt;li&gt;2023/10/07: &lt;a href="https://github.com/alibaba-damo-academy/FunCodec"&gt;FunCodec&lt;/a&gt;: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.&lt;/li&gt; 
  &lt;li&gt;2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime#file-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime#the-real-time-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/egs/aishell/bat"&gt;BAT&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to (&lt;a href="https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html"&gt;M2MeT2.0&lt;/a&gt;).&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a name="Installation"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Requirements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;python&amp;gt;=3.8
torch&amp;gt;=1.13
torchaudio
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install for pypi&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install -U funasr
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Or install from source code&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/alibaba/FunASR.git &amp;amp;&amp;amp; cd FunASR
pip3 install -e ./
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install modelscope or huggingface_hub for the pretrained models (Optional)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install -U modelscope huggingface_hub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model Zoo&lt;/h2&gt; 
&lt;p&gt;FunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/MODEL_LICENSE"&gt;Model License Agreement&lt;/a&gt;. Below are some representative models, for more models please refer to the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/model_zoo"&gt;Model Zoo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;(Note: ‚≠ê represents the ModelScope model zoo, ü§ó represents the Huggingface model zoo, üçÄ represents the OpenAI model zoo)&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model Name&lt;/th&gt; 
   &lt;th align="center"&gt;Task Details&lt;/th&gt; 
   &lt;th align="center"&gt;Training Data&lt;/th&gt; 
   &lt;th align="center"&gt;Parameters&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;SenseVoiceSmall &lt;br /&gt; (&lt;a href="https://www.modelscope.cn/models/iic/SenseVoiceSmall"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/FunAudioLLM/SenseVoiceSmall"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko&lt;/td&gt; 
   &lt;td align="center"&gt;300000 hours&lt;/td&gt; 
   &lt;td align="center"&gt;234M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;paraformer-zh &lt;br /&gt; (&lt;a href="https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/paraformer-zh"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, with timestamps, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;60000 hours, Mandarin&lt;/td&gt; 
   &lt;td align="center"&gt;220M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;
    &lt;nobr&gt;
     paraformer-zh-streaming 
     &lt;br /&gt; ( 
     &lt;a href="https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary"&gt;‚≠ê&lt;/a&gt; 
     &lt;a href="https://huggingface.co/funasr/paraformer-zh-streaming"&gt;ü§ó&lt;/a&gt; )
    &lt;/nobr&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, streaming&lt;/td&gt; 
   &lt;td align="center"&gt;60000 hours, Mandarin&lt;/td&gt; 
   &lt;td align="center"&gt;220M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;paraformer-en &lt;br /&gt; ( &lt;a href="https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/paraformer-en"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, without timestamps, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;50000 hours, English&lt;/td&gt; 
   &lt;td align="center"&gt;220M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;conformer-en &lt;br /&gt; ( &lt;a href="https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/conformer-en"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;50000 hours, English&lt;/td&gt; 
   &lt;td align="center"&gt;220M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;ct-punc &lt;br /&gt; ( &lt;a href="https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/ct-punc"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;punctuation restoration&lt;/td&gt; 
   &lt;td align="center"&gt;100M, Mandarin and English&lt;/td&gt; 
   &lt;td align="center"&gt;290M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;fsmn-vad &lt;br /&gt; ( &lt;a href="https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/fsmn-vad"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;voice activity detection&lt;/td&gt; 
   &lt;td align="center"&gt;5000 hours, Mandarin and English&lt;/td&gt; 
   &lt;td align="center"&gt;0.4M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;fsmn-kws &lt;br /&gt; ( &lt;a href="https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary"&gt;‚≠ê&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;keyword spottingÔºåstreaming&lt;/td&gt; 
   &lt;td align="center"&gt;5000 hours, Mandarin&lt;/td&gt; 
   &lt;td align="center"&gt;0.7M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;fa-zh &lt;br /&gt; ( &lt;a href="https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/fa-zh"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;timestamp prediction&lt;/td&gt; 
   &lt;td align="center"&gt;5000 hours, Mandarin&lt;/td&gt; 
   &lt;td align="center"&gt;38M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;cam++ &lt;br /&gt; ( &lt;a href="https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/campplus"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speaker verification/diarization&lt;/td&gt; 
   &lt;td align="center"&gt;5000 hours&lt;/td&gt; 
   &lt;td align="center"&gt;7.2M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Whisper-large-v3 &lt;br /&gt; (&lt;a href="https://www.modelscope.cn/models/iic/Whisper-large-v3/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://github.com/openai/whisper"&gt;üçÄ&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, with timestamps, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;multilingual&lt;/td&gt; 
   &lt;td align="center"&gt;1550 M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Whisper-large-v3-turbo &lt;br /&gt; (&lt;a href="https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://github.com/openai/whisper"&gt;üçÄ&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, with timestamps, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;multilingual&lt;/td&gt; 
   &lt;td align="center"&gt;809 M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Qwen-Audio &lt;br /&gt; (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/qwen_audio/demo.py"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-Audio"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;audio-text multimodal models (pretraining)&lt;/td&gt; 
   &lt;td align="center"&gt;multilingual&lt;/td&gt; 
   &lt;td align="center"&gt;8B&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Qwen-Audio-Chat &lt;br /&gt; (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/qwen_audio/demo_chat.py"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-Audio-Chat"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;audio-text multimodal models (chat)&lt;/td&gt; 
   &lt;td align="center"&gt;multilingual&lt;/td&gt; 
   &lt;td align="center"&gt;8B&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;emotion2vec+large &lt;br /&gt; (&lt;a href="https://modelscope.cn/models/iic/emotion2vec_plus_large/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/emotion2vec/emotion2vec_plus_large"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech emotion recongintion&lt;/td&gt; 
   &lt;td align="center"&gt;40000 hours&lt;/td&gt; 
   &lt;td align="center"&gt;300M&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a name="quick-start"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Below is a quick start tutorial. Test audio files (&lt;a href="https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav"&gt;Mandarin&lt;/a&gt;, &lt;a href="https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav"&gt;English&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;Command-line usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;funasr ++model=paraformer-zh ++vad_model="fsmn-vad" ++punc_model="ct-punc" ++input=asr_example_zh.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: &lt;code&gt;wav_id wav_pat&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Speech Recognition (Non-streaming)&lt;/h3&gt; 
&lt;h4&gt;SenseVoice&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model_dir = "iic/SenseVoiceSmall"

model = AutoModel(
    model=model_dir,
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)

# en
res = model.generate(
    input=f"{model.model_path}/example/en.mp3",
    cache={},
    language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
    use_itn=True,
    batch_size_s=60,
    merge_vad=True,  #
    merge_length_s=15,
)
text = rich_transcription_postprocess(res[0]["text"])
print(text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Parameter Description:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;model_dir&lt;/code&gt;: The name of the model, or the path to the model on the local disk.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vad_model&lt;/code&gt;: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model's inference time separately, the VAD model can be disabled.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vad_kwargs&lt;/code&gt;: Specifies the configurations for the VAD model. &lt;code&gt;max_single_segment_time&lt;/code&gt;: denotes the maximum duration for audio segmentation by the &lt;code&gt;vad_model&lt;/code&gt;, with the unit being milliseconds (ms).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;use_itn&lt;/code&gt;: Whether the output result includes punctuation and inverse text normalization.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;batch_size_s&lt;/code&gt;: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;merge_vad&lt;/code&gt;: Whether to merge short audio fragments segmented by the VAD model, with the merged length being &lt;code&gt;merge_length_s&lt;/code&gt;, in seconds (s).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ban_emo_unk&lt;/code&gt;: Whether to ban the output of the &lt;code&gt;emo_unk&lt;/code&gt; token.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Paraformer&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel
# paraformer-zh is a multi-functional asr model
# use vad, punc, spk or not as you need
model = AutoModel(model="paraformer-zh",  vad_model="fsmn-vad",  punc_model="ct-punc", 
                  # spk_model="cam++", 
                  )
res = model.generate(input=f"{model.model_path}/example/asr_example.wav", 
                     batch_size_s=300, 
                     hotword='È≠îÊê≠')
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: &lt;code&gt;hub&lt;/code&gt;: represents the model repository, &lt;code&gt;ms&lt;/code&gt; stands for selecting ModelScope download, &lt;code&gt;hf&lt;/code&gt; stands for selecting Huggingface download.&lt;/p&gt; 
&lt;h3&gt;Speech Recognition (Streaming)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

chunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms
encoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention
decoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention

model = AutoModel(model="paraformer-zh-streaming")

import soundfile
import os

wav_file = os.path.join(model.model_path, "example/asr_example.wav")
speech, sample_rate = soundfile.read(wav_file)
chunk_stride = chunk_size[1] * 960 # 600ms

cache = {}
total_chunk_num = int(len((speech)-1)/chunk_stride+1)
for i in range(total_chunk_num):
    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]
    is_final = i == total_chunk_num - 1
    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)
    print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: &lt;code&gt;chunk_size&lt;/code&gt; is the configuration for streaming latency.&lt;code&gt; [0,10,5]&lt;/code&gt; indicates that the real-time display granularity is &lt;code&gt;10*60=600ms&lt;/code&gt;, and the lookahead information is &lt;code&gt;5*60=300ms&lt;/code&gt;. Each inference input is &lt;code&gt;600ms&lt;/code&gt; (sample points are &lt;code&gt;16000*0.6=960&lt;/code&gt;), and the output is the corresponding text. For the last speech segment input, &lt;code&gt;is_final=True&lt;/code&gt; needs to be set to force the output of the last word.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;More Examples&lt;/summary&gt; 
 &lt;h3&gt;Voice Activity Detection (Non-Streaming)&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="fsmn-vad")
wav_file = f"{model.model_path}/example/vad_example.wav"
res = model.generate(input=wav_file)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Note: The output format of the VAD model is: &lt;code&gt;[[beg1, end1], [beg2, end2], ..., [begN, endN]]&lt;/code&gt;, where &lt;code&gt;begN/endN&lt;/code&gt; indicates the starting/ending point of the &lt;code&gt;N-th&lt;/code&gt; valid audio segment, measured in milliseconds.&lt;/p&gt; 
 &lt;h3&gt;Voice Activity Detection (Streaming)&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

chunk_size = 200 # ms
model = AutoModel(model="fsmn-vad")

import soundfile

wav_file = f"{model.model_path}/example/vad_example.wav"
speech, sample_rate = soundfile.read(wav_file)
chunk_stride = int(chunk_size * sample_rate / 1000)

cache = {}
total_chunk_num = int(len((speech)-1)/chunk_stride+1)
for i in range(total_chunk_num):
    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]
    is_final = i == total_chunk_num - 1
    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)
    if len(res[0]["value"]):
        print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Note: The output format for the streaming VAD model can be one of four scenarios:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;[[beg1, end1], [beg2, end2], .., [begN, endN]]&lt;/code&gt;ÔºöThe same as the offline VAD output result mentioned above.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;[[beg, -1]]&lt;/code&gt;ÔºöIndicates that only a starting point has been detected.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;[[-1, end]]&lt;/code&gt;ÔºöIndicates that only an ending point has been detected.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;[]&lt;/code&gt;ÔºöIndicates that neither a starting point nor an ending point has been detected.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;The output is measured in milliseconds and represents the absolute time from the starting point.&lt;/p&gt; 
 &lt;h3&gt;Punctuation Restoration&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="ct-punc")
res = model.generate(input="ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ")
print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Timestamp Prediction&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="fa-zh")
wav_file = f"{model.model_path}/example/asr_example.wav"
text_file = f"{model.model_path}/example/text.txt"
res = model.generate(input=(wav_file, text_file), data_type=("sound", "text"))
print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Speech Emotion Recognition&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="emotion2vec_plus_large")

wav_file = f"{model.model_path}/example/test.wav"

res = model.generate(wav_file, output_dir="./outputs", granularity="utterance", extract_embedding=False)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;More usages ref to &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/tutorial/README_zh.md"&gt;docs&lt;/a&gt;, more examples ref to &lt;a href="https://github.com/alibaba-damo-academy/FunASR/tree/main/examples/industrial_data_pretraining"&gt;demo&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Export ONNX&lt;/h2&gt; 
&lt;h3&gt;Command-line usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;funasr-export ++model=paraformer ++quantize=false ++device=cpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="paraformer", device="cpu")

res = model.export(quantize=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Test ONNX&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# pip3 install -U funasr-onnx
from pathlib import Path
from runtime.python.onnxruntime.funasr_onnx.paraformer_bin import Paraformer


home_dir = Path.home()

model_dir = "damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
model = Paraformer(model_dir, batch_size=1, quantize=True)

wav_path = [f"{home_dir}/.cache/modelscope/hub/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav"]

result = model(wav_path)
print(result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More examples ref to &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/python/onnxruntime"&gt;demo&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Deployment Service&lt;/h2&gt; 
&lt;p&gt;FunASR supports deploying pre-trained or further fine-tuned models for service. Currently, it supports the following types of service deployment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;File transcription service, Mandarin, CPU version, done&lt;/li&gt; 
 &lt;li&gt;The real-time transcription service, Mandarin (CPU), done&lt;/li&gt; 
 &lt;li&gt;File transcription service, English, CPU version, done&lt;/li&gt; 
 &lt;li&gt;File transcription service, Mandarin, GPU version, in progress&lt;/li&gt; 
 &lt;li&gt;and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more detailed information, please refer to the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;service deployment documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a name="contact"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Community Communication&lt;/h2&gt; 
&lt;p&gt;If you encounter problems in use, you can directly raise Issues on the github page.&lt;/p&gt; 
&lt;p&gt;You can also scan the following DingTalk group to join the community group for communication and discussion.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;DingTalk group&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;
    &lt;div align="left"&gt;
     &lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/dingding.png" width="250" /&gt;
    &lt;/div&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;
    &lt;div align="left"&gt;
     &lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/alibaba.png" width="260" /&gt;
    &lt;/div&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;
    &lt;div align="left"&gt;
     &lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/nwpu.png" width="260" /&gt;
    &lt;/div&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/China_Telecom.png" width="200" /&gt; &lt;/th&gt; 
   &lt;th align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/RapidAI.png" width="200" /&gt; &lt;/th&gt; 
   &lt;th align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/aihealthx.png" width="200" /&gt; &lt;/th&gt; 
   &lt;th align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/XVERSE.png" width="250" /&gt; &lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
&lt;/table&gt; 
&lt;p&gt;The contributors can be found in &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/Acknowledge.md"&gt;contributors list&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under &lt;a href="https://opensource.org/licenses/MIT"&gt;The MIT License&lt;/a&gt;. FunASR also contains various third-party components and some code modified from other repos under other open source licenses. The use of pretraining model is subject to &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/MODEL_LICENSE"&gt;model license&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citations&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{gao2023funasr,
  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},
  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},
  year={2023},
  booktitle={INTERSPEECH},
}
@inproceedings{An2023bat,
  author={Keyu An and Xian Shi and Shiliang Zhang},
  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},
  year={2023},
  booktitle={INTERSPEECH},
}
@inproceedings{gao22b_interspeech,
  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},
  title={Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={2063--2067},
  doi={10.21437/Interspeech.2022-9996}
}
@inproceedings{shi2023seaco,
  author={Xian Shi and Yexin Yang and Zerui Li and Yanni Chen and Zhifu Gao and Shiliang Zhang},
  title={SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability},
  year={2023},
  booktitle={ICASSP2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Pointcept/Pointcept</title>
      <link>https://github.com/Pointcept/Pointcept</link>
      <description>&lt;p&gt;Pointcept: Perceive the world with sparse points, a codebase for point cloud perception research. Latest works: Sonata (CVPR'25 Highlight), PTv3 (CVPR'24 Oral)&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;!-- pypi-strip --&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/pointcept/assets/main/pointcept/logo_dark.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/pointcept/assets/main/pointcept/logo.png" /&gt; 
  &lt;!-- /pypi-strip --&gt; 
  &lt;img alt="pointcept" src="https://raw.githubusercontent.com/pointcept/assets/main/pointcept/logo.png" width="400" /&gt; 
  &lt;!-- pypi-strip --&gt; 
 &lt;/picture&gt;&lt;br /&gt; 
 &lt;!-- /pypi-strip --&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/pointcept/pointcept/actions/workflows/formatter.yml"&gt;&lt;img src="https://github.com/pointcept/pointcept/actions/workflows/formatter.yml/badge.svg?sanitize=true" alt="Formatter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Pointcept&lt;/strong&gt; is a powerful and flexible codebase for point cloud perception research. It is also an official implementation of the following paper:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Sonata: Self-Supervised Learning of Reliable Point Representations&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, Julian Straub&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2025 - Highlight&lt;br /&gt; [ Pretrain ] [Sonata] - [ &lt;a href="https://xywu.me/sonata/"&gt;Project&lt;/a&gt; ] [ &lt;a href="https://arxiv.org/abs/2503.16429"&gt;arXiv&lt;/a&gt; ] [ &lt;a href="https://xywu.me/research/sonata/bib.txt"&gt;Bib&lt;/a&gt; ] [ &lt;a href="https://github.com/facebookresearch/sonata"&gt;Demo&lt;/a&gt; ] [ &lt;a href="https://huggingface.co/facebook/sonata"&gt;Weight&lt;/a&gt; ] ‚Üí &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sonata"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Point Transformer V3: Simpler, Faster, Stronger&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, Hengshuang Zhao&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2024 - Oral&lt;br /&gt; [ Backbone ] [PTv3] - [ &lt;a href="https://arxiv.org/abs/2312.10035"&gt;arXiv&lt;/a&gt; ] [ &lt;a href="https://xywu.me/research/ptv3/bib.txt"&gt;Bib&lt;/a&gt; ] [ &lt;a href="https://github.com/Pointcept/PointTransformerV3"&gt;Project&lt;/a&gt; ] ‚Üí &lt;a href="https://github.com/Pointcept/PointTransformerV3"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, Jiaya Jia&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2024&lt;br /&gt; [ Backbone ] [ OA-CNNs ] - [ &lt;a href="https://arxiv.org/abs/2403.14418"&gt;arXiv&lt;/a&gt; ] [ &lt;a href="https://xywu.me/research/oacnns/bib.txt"&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#oa-cnns"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui Liu, Kaicheng Yu, Hengshuang Zhao&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2024&lt;br /&gt; [ Pretrain ] [PPT] - [ &lt;a href="https://arxiv.org/abs/2308.09718"&gt;arXiv&lt;/a&gt; ] [ &lt;a href="https://xywu.me/research/ppt/bib.txt"&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-prompt-training-ppt"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Xin Wen, Xihui Liu, Hengshuang Zhao&lt;/em&gt;&lt;br /&gt; IEEE Conference on Computer Vision and Pattern Recognition (&lt;strong&gt;CVPR&lt;/strong&gt;) 2023&lt;br /&gt; [ Pretrain ] [ MSC ] - [ &lt;a href="https://arxiv.org/abs/2303.14191"&gt;arXiv&lt;/a&gt; ] [ &lt;a href="https://xywu.me/research/msc/bib.txt"&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#masked-scene-contrast-msc"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning Context-aware Classifier for Semantic Segmentation&lt;/strong&gt; (3D Part)&lt;br /&gt; &lt;em&gt;Zhuotao Tian, Jiequan Cui, Li Jiang, Xiaojuan Qi, Xin Lai, Yixin Chen, Shu Liu, Jiaya Jia&lt;/em&gt;&lt;br /&gt; AAAI Conference on Artificial Intelligence (&lt;strong&gt;AAAI&lt;/strong&gt;) 2023 - Oral&lt;br /&gt; [ SemSeg ] [ CAC ] - [ &lt;a href="https://arxiv.org/abs/2303.11633"&gt;arXiv&lt;/a&gt; ] [ &lt;a href="https://xywu.me/research/cac/bib.txt"&gt;Bib&lt;/a&gt; ] [ &lt;a href="https://github.com/tianzhuotao/CAC"&gt;2D Part&lt;/a&gt; ] ‚Üí &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#context-aware-classifier"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Point Transformer V2: Grouped Vector Attention and Partition-based Pooling&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, Hengshuang Zhao&lt;/em&gt;&lt;br /&gt; Conference on Neural Information Processing Systems (&lt;strong&gt;NeurIPS&lt;/strong&gt;) 2022&lt;br /&gt; [ Backbone ] [ PTv2 ] - [ &lt;a href="https://arxiv.org/abs/2210.05666"&gt;arXiv&lt;/a&gt; ] [ &lt;a href="https://xywu.me/research/ptv2/bib.txt"&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Point Transformer&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun&lt;/em&gt;&lt;br /&gt; IEEE International Conference on Computer Vision (&lt;strong&gt;ICCV&lt;/strong&gt;) 2021 - Oral&lt;br /&gt; [ Backbone ] [ PTv1 ] - [ &lt;a href="https://arxiv.org/abs/2012.09164"&gt;arXiv&lt;/a&gt; ] [ &lt;a href="https://hszhao.github.io/papers/iccv21_pointtransformer_bib.txt"&gt;Bib&lt;/a&gt; ] ‚Üí &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, &lt;strong&gt;Pointcept&lt;/strong&gt; integrates the following excellent work (contain above):&lt;br /&gt; Backbone: &lt;a href="https://github.com/NVIDIA/MinkowskiEngine"&gt;MinkUNet&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sparseunet"&gt;here&lt;/a&gt;), &lt;a href="https://github.com/traveller59/spconv"&gt;SpUNet&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sparseunet"&gt;here&lt;/a&gt;), &lt;a href="https://github.com/mit-han-lab/spvnas"&gt;SPVCNN&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#spvcnn"&gt;here&lt;/a&gt;), &lt;a href="https://arxiv.org/abs/2403.14418"&gt;OACNNs&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#oa-cnns"&gt;here&lt;/a&gt;), &lt;a href="https://arxiv.org/abs/2012.09164"&gt;PTv1&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers"&gt;here&lt;/a&gt;), &lt;a href="https://arxiv.org/abs/2210.05666"&gt;PTv2&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers"&gt;here&lt;/a&gt;), &lt;a href="https://arxiv.org/abs/2312.10035"&gt;PTv3&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-transformers"&gt;here&lt;/a&gt;), &lt;a href="https://github.com/dvlab-research/Stratified-Transformer"&gt;StratifiedFormer&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#stratified-transformer"&gt;here&lt;/a&gt;), &lt;a href="https://github.com/octree-nn/octformer"&gt;OctFormer&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#octformer"&gt;here&lt;/a&gt;), &lt;a href="https://github.com/microsoft/Swin3D"&gt;Swin3D&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#swin3d"&gt;here&lt;/a&gt;);&lt;br /&gt; Semantic Segmentation: &lt;a href="https://github.com/kumuji/mix3d"&gt;Mix3d&lt;/a&gt; (&lt;a href="https://github.com/Pointcept/Pointcept/raw/main/configs/scannet/semseg-spunet-v1m1-0-base.py#L5"&gt;here&lt;/a&gt;), &lt;a href="https://arxiv.org/abs/2303.11633"&gt;CAC&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#context-aware-classifier"&gt;here&lt;/a&gt;);&lt;br /&gt; Instance Segmentation: &lt;a href="https://github.com/dvlab-research/PointGroup"&gt;PointGroup&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#pointgroup"&gt;here&lt;/a&gt;);&lt;br /&gt; Pre-training: &lt;a href="https://github.com/facebookresearch/PointContrast"&gt;PointContrast&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#pointcontrast"&gt;here&lt;/a&gt;), &lt;a href="https://github.com/facebookresearch/ContrastiveSceneContexts"&gt;Contrastive Scene Contexts&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#contrastive-scene-contexts"&gt;here&lt;/a&gt;), &lt;a href="https://arxiv.org/abs/2303.14191"&gt;Masked Scene Contrast&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#masked-scene-contrast-msc"&gt;here&lt;/a&gt;), &lt;a href="https://arxiv.org/abs/2308.09718"&gt;Point Prompt Training&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#point-prompt-training-ppt"&gt;here&lt;/a&gt;), &lt;a href=""&gt;Sonata&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sonata"&gt;here&lt;/a&gt;);&lt;br /&gt; Datasets: &lt;a href="http://www.scan-net.org/"&gt;ScanNet&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#scannet-v2"&gt;here&lt;/a&gt;), &lt;a href="http://www.scan-net.org/"&gt;ScanNet200&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#scannet-v2"&gt;here&lt;/a&gt;), &lt;a href="https://kaldir.vc.in.tum.de/scannetpp/"&gt;ScanNet++&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#scannet"&gt;here&lt;/a&gt;), &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLScDimvNMCGhy_rmBA2gHfDu3naktRm6A8BPwAWWDv-Uhm6Shw/viewform?c=0&amp;amp;w=1"&gt;S3DIS&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#s3dis"&gt;here&lt;/a&gt;), &lt;a href="https://github.com/apple/ARKitScenes"&gt;ArkitScene&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#arkitscenes"&gt;here&lt;/a&gt;), &lt;a href="https://github.com/facebookresearch/habitat-matterport3d-dataset/"&gt;HM3D&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#habitat---matterport-3d-hm3d"&gt;here&lt;/a&gt;), &lt;a href="https://niessner.github.io/Matterport/"&gt;Matterport3D&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#matterport3d"&gt;here&lt;/a&gt;), &lt;a href="https://structured3d-dataset.org/"&gt;Structured3D&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#structured3d"&gt;here&lt;/a&gt;), &lt;a href="http://www.semantic-kitti.org/"&gt;SemanticKITTI&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#semantickitti"&gt;here&lt;/a&gt;), &lt;a href="https://www.nuscenes.org/nuscenes"&gt;nuScenes&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#nuscenes"&gt;here&lt;/a&gt;), &lt;a href="https://modelnet.cs.princeton.edu/"&gt;ModelNet40&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#modelnet"&gt;here&lt;/a&gt;), &lt;a href="https://waymo.com/open/"&gt;Waymo&lt;/a&gt; (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#waymo"&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;Apr 2025&lt;/em&gt; üöÄ: We now support &lt;code&gt;wandb&lt;/code&gt;, check the &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#quick-start"&gt;Quick Start&lt;/a&gt; training section for more information. (Thanks @Streakfull for his contribution!)&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Mar 2025&lt;/em&gt; üöÄ: &lt;strong&gt;Sonata&lt;/strong&gt; is accepted by CVPR 2025 and selected as one of the &lt;strong&gt;Highlight&lt;/strong&gt; presentations (3.0% submissions)! We release the code with Pointcept v1.6.0. We release the pre-training &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#sonata"&gt;code&lt;/a&gt;&lt;/strong&gt; along with Pointcept v1.6.0 and provide an easy-to-use pre-trained model for inference, tuning, and visualization in our project &lt;strong&gt;&lt;a href="https://github.com/facebookresearch/sonata"&gt;repository&lt;/a&gt;&lt;/strong&gt; hosted by Meta.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;May 2024&lt;/em&gt;: In v1.5.2, we redesigned the default structure for each dataset for better performance. Please &lt;strong&gt;re-preprocess&lt;/strong&gt; datasets or &lt;strong&gt;download&lt;/strong&gt; our preprocessed datasets from &lt;strong&gt;&lt;a href="https://huggingface.co/Pointcept"&gt;here&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Apr 2024&lt;/em&gt;: &lt;strong&gt;PTv3&lt;/strong&gt; is selected as one of the 90 &lt;strong&gt;Oral&lt;/strong&gt; papers (3.3% accepted papers, 0.78% submissions) by CVPR'24!&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Mar 2024&lt;/em&gt;: We release code for &lt;strong&gt;OA-CNNs&lt;/strong&gt;, accepted by CVPR'24. Issue related to &lt;strong&gt;OA-CNNs&lt;/strong&gt; can @Pbihao.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Feb 2024&lt;/em&gt;: &lt;strong&gt;PTv3&lt;/strong&gt; and &lt;strong&gt;PPT&lt;/strong&gt; are accepted by CVPR'24, another &lt;strong&gt;two&lt;/strong&gt; papers by our Pointcept team have also been accepted by CVPR'24 üéâüéâüéâ. We will make them publicly available soon!&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Dec 2023&lt;/em&gt;: &lt;strong&gt;PTv3&lt;/strong&gt; is released on arXiv, and the code is available in Pointcept. PTv3 is an efficient backbone model that achieves SOTA performances across indoor and outdoor scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Aug 2023&lt;/em&gt;: &lt;strong&gt;PPT&lt;/strong&gt; is released on arXiv. PPT presents a multi-dataset pre-training framework that achieves SOTA performance in both &lt;strong&gt;indoor&lt;/strong&gt; and &lt;strong&gt;outdoor&lt;/strong&gt; scenarios. It is compatible with various existing pre-training frameworks and backbones. A &lt;strong&gt;pre-release&lt;/strong&gt; version of the code is accessible; for those interested, please feel free to contact me directly for access.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Mar 2023&lt;/em&gt;: We released our codebase, &lt;strong&gt;Pointcept&lt;/strong&gt;, a highly potent tool for point cloud representation learning and perception. We welcome new work to join the &lt;em&gt;Pointcept&lt;/em&gt; family and highly recommend reading &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#quick-start"&gt;Quick Start&lt;/a&gt; before starting your trail.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Feb 2023&lt;/em&gt;: &lt;strong&gt;MSC&lt;/strong&gt; and &lt;strong&gt;CeCo&lt;/strong&gt; accepted by CVPR 2023. &lt;em&gt;MSC&lt;/em&gt; is a highly efficient and effective pretraining framework that facilitates cross-dataset large-scale pretraining, while &lt;em&gt;CeCo&lt;/em&gt; is a segmentation method specifically designed for long-tail datasets. Both approaches are compatible with all existing backbone models in our codebase, and we will soon make the code available for public use.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Jan 2023&lt;/em&gt;: &lt;strong&gt;CAC&lt;/strong&gt;, oral work of AAAI 2023, has expanded its 3D result with the incorporation of Pointcept. This addition will allow CAC to serve as a pluggable segmentor within our codebase.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Sep 2022&lt;/em&gt;: &lt;strong&gt;PTv2&lt;/strong&gt; accepted by NeurIPS 2022. It is a continuation of the Point Transformer. The proposed GVA theory can apply to most existing attention mechanisms, while Grid Pooling is also a practical addition to existing pooling methods.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find &lt;em&gt;Pointcept&lt;/em&gt; useful to your research, please cite our work as encouragement. (‡©≠ÀäÍí≥‚ÄãÀã)‡©≠‚úß&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{pointcept2023,
    title={Pointcept: A Codebase for Point Cloud Perception Research},
    author={Pointcept Contributors},
    howpublished = {\url{https://github.com/Pointcept/Pointcept}},
    year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#model-zoo"&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ubuntu: 18.04 and above.&lt;/li&gt; 
 &lt;li&gt;CUDA: 11.3 and above.&lt;/li&gt; 
 &lt;li&gt;PyTorch: 1.10.0 and above.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Conda Environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Method 1&lt;/strong&gt;: Utilize conda &lt;code&gt;environment.yml&lt;/code&gt; to create a new environment with one line code:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate conda environment named as 'pointcept-torch2.5.0-cu12.4'
# cuda: 12.4, pytorch: 2.5.0

# run `unset CUDA_PATH` if you have installed cuda in your local environment
conda env create -f environment.yml --verbose
conda activate pointcept-torch2.5.0-cu12.4
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Method 2&lt;/strong&gt;: Use our pre-built Docker image and refer to the supported tags &lt;a href="https://hub.docker.com/repository/docker/pointcept/pointcept/general"&gt;here&lt;/a&gt;. Quickly verify the Docker image on your local machine with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run --gpus all -it --rm pointcept/pointcept:v1.6.0-pytorch2.5.0-cuda12.4-cudnn9-devel bash
git clone https://github.com/facebookresearch/sonata
cd sonata
export PYTHONPATH=./ &amp;amp;&amp;amp; python demo/0_pca.py
# Ignore the GUI error, we cannot expect a container to have its GUI, right?
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Method 3&lt;/strong&gt;: Manually create a conda environment:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n pointcept python=3.10 -y
conda activate pointcept

# (Optional) If no CUDA installed
conda install nvidia/label/cuda-12.4.1::cuda conda-forge::cudnn conda-forge::gcc=13.2 conda-forge::gxx=13.2 -y

conda install ninja -y
# Choose version you want here: https://pytorch.org/get-started/previous-versions/
conda install pytorch==2.5.0 torchvision==0.13.1 torchaudio==0.20.0 pytorch-cuda=12.4 -c pytorch -y
conda install h5py pyyaml -c anaconda -y
conda install sharedarray tensorboard tensorboardx wandb yapf addict einops scipy plyfile termcolor timm -c conda-forge -y
conda install pytorch-cluster pytorch-scatter pytorch-sparse -c pyg -y
pip install torch-geometric

# spconv (SparseUNet)
# refer https://github.com/traveller59/spconv
pip install spconv-cu124

# PPT (clip)
pip install ftfy regex tqdm
pip install git+https://github.com/openai/CLIP.git

# PTv1 &amp;amp; PTv2 or precise eval
cd libs/pointops
# usual
python setup.py install
# docker &amp;amp; multi GPU arch
TORCH_CUDA_ARCH_LIST="ARCH LIST" python  setup.py install
# e.g. 7.5: RTX 3000; 8.0: a100 More available in: https://developer.nvidia.com/cuda-gpus
TORCH_CUDA_ARCH_LIST="7.5 8.0" python  setup.py install
cd ../..

# Open3D (visualization, optional)
pip install open3d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Data Preparation&lt;/h2&gt; 
&lt;h3&gt;ScanNet v2&lt;/h3&gt; 
&lt;p&gt;The preprocessing supports semantic and instance segmentation for both &lt;code&gt;ScanNet20&lt;/code&gt;, &lt;code&gt;ScanNet200&lt;/code&gt;, and &lt;code&gt;ScanNet Data Efficient&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Download the &lt;a href="http://www.scan-net.org/"&gt;ScanNet&lt;/a&gt; v2 dataset.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run preprocessing code for raw ScanNet as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# RAW_SCANNET_DIR: the directory of downloaded ScanNet v2 raw dataset.
# PROCESSED_SCANNET_DIR: the directory of the processed ScanNet dataset (output dir).
python pointcept/datasets/preprocessing/scannet/preprocess_scannet.py --dataset_root ${RAW_SCANNET_DIR} --output_root ${PROCESSED_SCANNET_DIR}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Optional) Download ScanNet Data Efficient files:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# download-scannet.py is the official download script
# or follow instructions here: https://kaldir.vc.in.tum.de/scannet_benchmark/data_efficient/documentation#download
python download-scannet.py --data_efficient -o ${RAW_SCANNET_DIR}
# unzip downloads
cd ${RAW_SCANNET_DIR}/tasks
unzip limited-annotation-points.zip
unzip limited-reconstruction-scenes.zip
# copy files to processed dataset folder
mkdir ${PROCESSED_SCANNET_DIR}/tasks
cp -r ${RAW_SCANNET_DIR}/tasks/points ${PROCESSED_SCANNET_DIR}/tasks
cp -r ${RAW_SCANNET_DIR}/tasks/scenes ${PROCESSED_SCANNET_DIR}/tasks
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can be directly downloaded [&lt;a href="https://huggingface.co/datasets/Pointcept/scannet-compressed"&gt;here&lt;/a&gt;], please agree the official license before download it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Link processed dataset to codebase:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_SCANNET_DIR: the directory of the processed ScanNet dataset.
mkdir data
ln -s ${PROCESSED_SCANNET_DIR} ${CODEBASE_DIR}/data/scannet
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ScanNet++&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download the &lt;a href="https://kaldir.vc.in.tum.de/scannetpp/"&gt;ScanNet++&lt;/a&gt; dataset.&lt;/li&gt; 
 &lt;li&gt;Run preprocessing code for raw ScanNet++ as follows: &lt;pre&gt;&lt;code class="language-bash"&gt;# RAW_SCANNETPP_DIR: the directory of downloaded ScanNet++ raw dataset.
# PROCESSED_SCANNETPP_DIR: the directory of the processed ScanNet++ dataset (output dir).
# NUM_WORKERS: the number of workers for parallel preprocessing.
python pointcept/datasets/preprocessing/scannetpp/preprocess_scannetpp.py --dataset_root ${RAW_SCANNETPP_DIR} --output_root ${PROCESSED_SCANNETPP_DIR} --num_workers ${NUM_WORKERS}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Sampling and chunking large point cloud data in train/val split as follows (only used for training): &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_SCANNETPP_DIR: the directory of the processed ScanNet++ dataset (output dir).
# NUM_WORKERS: the number of workers for parallel preprocessing.
python pointcept/datasets/preprocessing/sampling_chunking_data.py --dataset_root ${PROCESSED_SCANNETPP_DIR} --grid_size 0.01 --chunk_range 6 6 --chunk_stride 3 3 --split train --num_workers ${NUM_WORKERS}
python pointcept/datasets/preprocessing/sampling_chunking_data.py --dataset_root ${PROCESSED_SCANNETPP_DIR} --grid_size 0.01 --chunk_range 6 6 --chunk_stride 3 3 --split val --num_workers ${NUM_WORKERS}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Link processed dataset to codebase: &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_SCANNETPP_DIR: the directory of the processed ScanNet dataset.
mkdir data
ln -s ${PROCESSED_SCANNETPP_DIR} ${CODEBASE_DIR}/data/scannetpp
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;S3DIS&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Download S3DIS data by filling this &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLScDimvNMCGhy_rmBA2gHfDu3naktRm6A8BPwAWWDv-Uhm6Shw/viewform?c=0&amp;amp;w=1"&gt;Google form&lt;/a&gt;. Download the &lt;code&gt;Stanford3dDataset_v1.2.zip&lt;/code&gt; file and unzip it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Fix error in &lt;code&gt;Area_5/office_19/Annotations/ceiling&lt;/code&gt; Line 323474 (103.0ÔøΩ0000 =&amp;gt; 103.000000).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Optional) Download Full 2D-3D S3DIS dataset (no XYZ) from &lt;a href="https://github.com/alexsax/2D-3D-Semantics"&gt;here&lt;/a&gt; for parsing normal.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run preprocessing code for S3DIS as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# S3DIS_DIR: the directory of downloaded Stanford3dDataset_v1.2 dataset.
# RAW_S3DIS_DIR: the directory of Stanford2d3dDataset_noXYZ dataset. (optional, for parsing normal)
# PROCESSED_S3DIS_DIR: the directory of processed S3DIS dataset (output dir).

# S3DIS without aligned angle
python pointcept/datasets/preprocessing/s3dis/preprocess_s3dis.py --dataset_root ${S3DIS_DIR} --output_root ${PROCESSED_S3DIS_DIR}
# S3DIS with aligned angle
python pointcept/datasets/preprocessing/s3dis/preprocess_s3dis.py --dataset_root ${S3DIS_DIR} --output_root ${PROCESSED_S3DIS_DIR} --align_angle
# S3DIS with normal vector (recommended, normal is helpful)
python pointcept/datasets/preprocessing/s3dis/preprocess_s3dis.py --dataset_root ${S3DIS_DIR} --output_root ${PROCESSED_S3DIS_DIR} --raw_root ${RAW_S3DIS_DIR} --parse_normal
python pointcept/datasets/preprocessing/s3dis/preprocess_s3dis.py --dataset_root ${S3DIS_DIR} --output_root ${PROCESSED_S3DIS_DIR} --raw_root ${RAW_S3DIS_DIR} --align_angle --parse_normal
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can also be downloaded [&lt;a href="https://huggingface.co/datasets/Pointcept/s3dis-compressed"&gt;here&lt;/a&gt;] (with normal vector and aligned angle), please agree with the official license before downloading it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_S3DIS_DIR: the directory of processed S3DIS dataset.
mkdir data
ln -s ${PROCESSED_S3DIS_DIR} ${CODEBASE_DIR}/data/s3dis
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ArkitScenes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Download ArkitScenes 3DOD split with the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# RAW_AS_DIR: the directory of downloaded Raw ArkitScenes dataset.
git clone https://github.com/apple/ARKitScenes.git
cd ARKitScenes
python download_data.py 3dod --download_dir $RAW_AS_DIR --video_id_csv threedod/3dod_train_val_splits.csv
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run preprocessing code for ArkitScenes as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# RAW_AS_DIR: the directory of downloaded ArkitScenes dataset.
# PROCESSED_AS_DIR: the directory of processed ArkitScenes dataset (output dir).
# NUM_WORKERS: Number for workers for preprocessing, default same as cpu count (might OOM).
cd $POINTCEPT_DIR
export PYTHONPATH=./
python pointcept/datasets/preprocessing/arkitscenes/preprocess_arkitscenes_mesh.py --dataset_root $RAW_AS_DIR --output_root $PROCESSED_AS_DIR --num_workers $NUM_WORKERS
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can also be downloaded [&lt;a href="https://huggingface.co/datasets/Pointcept/arkitscenes-compressed"&gt;here&lt;/a&gt;] please read and agree the official &lt;a href="https://github.com/apple/ARKitScenes?tab=License-1-ov-file#readme"&gt;license&lt;/a&gt; before download it. (Unzip with the following command:&lt;br /&gt; &lt;code&gt;find ./ -name '*.tar.gz' | xargs -n 1 -P 8 -I {} sh -c 'tar -xzvf {}'&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_AR_DIR: the directory of processed ArkitScenes dataset (output dir).
mkdir data
ln -s ${PROCESSED_AR_DIR} ${CODEBASE_DIR}/data/arkitscenes
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Habitat - Matterport 3D (HM3D)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Download HM3D &lt;code&gt;hm3d-train-glb-v0.2.tar&lt;/code&gt; and &lt;code&gt;hm3d-val-glb-v0.2.tar&lt;/code&gt; with instuction &lt;a href="https://github.com/facebookresearch/habitat-sim/raw/main/DATASETS.md#habitat-matterport-3d-research-dataset-hm3d"&gt;here&lt;/a&gt; and unzip them.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run preprocessing code for HM3D as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# RAW_HM_DIR: the directory of downloaded HM3D dataset.
# PROCESSED_HM_DIR: the directory of processed HM3D dataset (output dir).
# NUM_WORKERS: Number for workers for preprocessing, default same as cpu count (might OOM).
export PYTHONPATH=./
python pointcept/datasets/preprocessing/hm3d/preprocess_hm3d.py --dataset_root $RAW_HM_DIR --output_root $PROCESSED_HM_DIR --density 0.02 --num_workers $NUM_WORKERS
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can also be downloaded [&lt;a href="https://huggingface.co/datasets/Pointcept/hm3d-compressed"&gt;here&lt;/a&gt;] please read and agree the official &lt;a href="https://matterport.com/legal/matterport-end-user-license-agreement-academic-use-model-data"&gt;license&lt;/a&gt; before download it. (Unzip with the following command:&lt;br /&gt; &lt;code&gt;find ./ -name '*.tar.gz' | xargs -n 1 -P 4 -I {} sh -c 'tar -xzvf {}'&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_HM_DIR: the directory of processed HM3D dataset (output dir).
mkdir data
ln -s ${PROCESSED_HM_DIR} ${CODEBASE_DIR}/data/hm3d


&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Matterport3D&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow &lt;a href="https://niessner.github.io/Matterport/#download"&gt;this page&lt;/a&gt; to request access to the dataset.&lt;/li&gt; 
 &lt;li&gt;Download the "region_segmentation" type, which represents the division of a scene into individual rooms. &lt;pre&gt;&lt;code class="language-bash"&gt;# download-mp.py is the official download script
# MATTERPORT3D_DIR: the directory of downloaded Matterport3D dataset.
python download-mp.py -o {MATTERPORT3D_DIR} --type region_segmentations
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Unzip the region_segmentations data &lt;pre&gt;&lt;code class="language-bash"&gt;# MATTERPORT3D_DIR: the directory of downloaded Matterport3D dataset.
python pointcept/datasets/preprocessing/matterport3d/unzip_matterport3d_region_segmentation.py --dataset_root {MATTERPORT3D_DIR}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Run preprocessing code for Matterport3D as follows: &lt;pre&gt;&lt;code class="language-bash"&gt;# MATTERPORT3D_DIR: the directory of downloaded Matterport3D dataset.
# PROCESSED_MATTERPORT3D_DIR: the directory of processed Matterport3D dataset (output dir).
# NUM_WORKERS: the number of workers for this preprocessing.
python pointcept/datasets/preprocessing/matterport3d/preprocess_matterport3d_mesh.py --dataset_root ${MATTERPORT3D_DIR} --output_root ${PROCESSED_MATTERPORT3D_DIR} --num_workers ${NUM_WORKERS}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Link processed dataset to codebase. &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_MATTERPORT3D_DIR: the directory of processed Matterport3D dataset (output dir).
mkdir data
ln -s ${PROCESSED_MATTERPORT3D_DIR} ${CODEBASE_DIR}/data/matterport3d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Following the instruction of &lt;a href="https://github.com/ViLab-UCSD/OpenRooms"&gt;OpenRooms&lt;/a&gt;, we remapped Matterport3D's categories to ScanNet 20 semantic categories with the addition of a ceiling category.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;(Alternative) Our preprocess data can also be downloaded &lt;a href="https://huggingface.co/datasets/Pointcept/matterport3d-compressed"&gt;here&lt;/a&gt;, please agree the official license before download it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Structured3D&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download Structured3D panorama related and perspective (full) related zip files by filling this &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSc0qtvh4vHSoZaW6UvlXYy79MbcGdZfICjh4_t4bYofQIVIdw/viewform?pli=1"&gt;Google form&lt;/a&gt; (no need to unzip them).&lt;/li&gt; 
 &lt;li&gt;Organize all downloaded zip file in one folder (&lt;code&gt;${STRUCT3D_DIR}&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Run preprocessing code for Structured3D as follows: &lt;pre&gt;&lt;code class="language-bash"&gt;# STRUCT3D_DIR: the directory of downloaded Structured3D dataset.
# PROCESSED_STRUCT3D_DIR: the directory of processed Structured3D dataset (output dir).
# NUM_WORKERS: Number for workers for preprocessing, default same as cpu count (might OOM).
export PYTHONPATH=./
python pointcept/datasets/preprocessing/structured3d/preprocess_structured3d.py --dataset_root ${STRUCT3D_DIR} --output_root ${PROCESSED_STRUCT3D_DIR} --num_workers ${NUM_WORKERS} --grid_size 0.01 --fuse_prsp --fuse_pano
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Following the instruction of &lt;a href="https://arxiv.org/abs/2304.06906"&gt;Swin3D&lt;/a&gt;, we keep 25 categories with frequencies of more than 0.001, out of the original 40 categories.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess data can also be downloaded [&lt;a href="https://huggingface.co/datasets/Pointcept/structured3d-compressed"&gt;here&lt;/a&gt;] (with perspective views and panorama view, 471.7G after unzipping), please agree the official license before download it. (Unzip with the following command:&lt;br /&gt; &lt;code&gt;find ./ -name '*.tar.gz' | xargs -n 1 -P 15 -I {} sh -c 'tar -xzvf {}'&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_STRUCT3D_DIR: the directory of processed Structured3D dataset (output dir).
mkdir data
ln -s ${PROCESSED_STRUCT3D_DIR} ${CODEBASE_DIR}/data/structured3d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SemanticKITTI&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download &lt;a href="http://www.semantic-kitti.org/dataset.html#download"&gt;SemanticKITTI&lt;/a&gt; dataset.&lt;/li&gt; 
 &lt;li&gt;Link dataset to codebase. &lt;pre&gt;&lt;code class="language-bash"&gt;# SEMANTIC_KITTI_DIR: the directory of SemanticKITTI dataset.
# |- SEMANTIC_KITTI_DIR
#   |- dataset
#     |- sequences
#       |- 00
#       |- 01
#       |- ...

mkdir -p data
ln -s ${SEMANTIC_KITTI_DIR} ${CODEBASE_DIR}/data/semantic_kitti
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;nuScenes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Download the official &lt;a href="https://www.nuscenes.org/nuscenes#download"&gt;NuScene&lt;/a&gt; dataset (with Lidar Segmentation) and organize the downloaded files as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;NUSCENES_DIR
‚îÇ‚îÄ‚îÄ samples
‚îÇ‚îÄ‚îÄ sweeps
‚îÇ‚îÄ‚îÄ lidarseg
...
‚îÇ‚îÄ‚îÄ v1.0-trainval 
‚îÇ‚îÄ‚îÄ v1.0-test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run information preprocessing code (modified from OpenPCDet) for nuScenes as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# NUSCENES_DIR: the directory of downloaded nuScenes dataset.
# PROCESSED_NUSCENES_DIR: the directory of processed nuScenes dataset (output dir).
# MAX_SWEEPS: Max number of sweeps. Default: 10.
pip install nuscenes-devkit pyquaternion
python pointcept/datasets/preprocessing/nuscenes/preprocess_nuscenes_info.py --dataset_root ${NUSCENES_DIR} --output_root ${PROCESSED_NUSCENES_DIR} --max_sweeps ${MAX_SWEEPS} --with_camera
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Alternative) Our preprocess nuScenes information data can also be downloaded [&lt;a href="https://huggingface.co/datasets/Pointcept/nuscenes-compressed"&gt;here&lt;/a&gt;] (only processed information, still need to download raw dataset and link to the folder), please agree the official license before download it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Link raw dataset to processed NuScene dataset folder:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# NUSCENES_DIR: the directory of downloaded nuScenes dataset.
# PROCESSED_NUSCENES_DIR: the directory of processed nuScenes dataset (output dir).
ln -s ${NUSCENES_DIR} {PROCESSED_NUSCENES_DIR}/raw
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then the processed nuscenes folder is organized as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;nuscene
|‚îÄ‚îÄ raw
    ‚îÇ‚îÄ‚îÄ samples
    ‚îÇ‚îÄ‚îÄ sweeps
    ‚îÇ‚îÄ‚îÄ lidarseg
    ...
    ‚îÇ‚îÄ‚îÄ v1.0-trainval
    ‚îÇ‚îÄ‚îÄ v1.0-test
|‚îÄ‚îÄ info
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Link processed dataset to codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_NUSCENES_DIR: the directory of processed nuScenes dataset (output dir).
mkdir data
ln -s ${PROCESSED_NUSCENES_DIR} ${CODEBASE_DIR}/data/nuscenes
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Waymo&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Download the official &lt;a href="https://waymo.com/open/download/"&gt;Waymo&lt;/a&gt; dataset (v1.4.3) and organize the downloaded files as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;WAYMO_RAW_DIR
‚îÇ‚îÄ‚îÄ training
‚îÇ‚îÄ‚îÄ validation
‚îÇ‚îÄ‚îÄ testing
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the following dependence:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# If shows "No matching distribution found", download whl directly from Pypi and install the package.
conda create -n waymo python=3.10 -y
conda activate waymo
pip install waymo-open-dataset-tf-2-12-0
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the preprocessing code as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# WAYMO_DIR: the directory of the downloaded Waymo dataset.
# PROCESSED_WAYMO_DIR: the directory of the processed Waymo dataset (output dir).
# NUM_WORKERS: num workers for preprocessing
python pointcept/datasets/preprocessing/waymo/preprocess_waymo.py --dataset_root ${WAYMO_DIR} --output_root ${PROCESSED_WAYMO_DIR} --splits training validation --num_workers ${NUM_WORKERS}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Link processed dataset to the codebase.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# PROCESSED_WAYMO_DIR: the directory of the processed Waymo dataset (output dir).
mkdir data
ln -s ${PROCESSED_WAYMO_DIR} ${CODEBASE_DIR}/data/waymo
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ModelNet&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download &lt;a href="https://huggingface.co/datasets/Pointcept/modelnet40_normal_resampled-compressed"&gt;modelnet40_normal_resampled.zip&lt;/a&gt; and unzip&lt;/li&gt; 
 &lt;li&gt;Link dataset to the codebase. &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p data
ln -s ${MODELNET_DIR} ${CODEBASE_DIR}/data/modelnet40_normal_resampled
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Training&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Train from scratch.&lt;/strong&gt; The training processing is based on configs in &lt;code&gt;configs&lt;/code&gt; folder. The training script will generate an experiment folder in &lt;code&gt;exp&lt;/code&gt; folder and backup essential code in the experiment folder. Training config, log, tensorboard, and checkpoints will also be saved into the experiment folder during the training process.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}
# Script (Recommended)
sh scripts/train.sh -p ${INTERPRETER_PATH} -g ${NUM_GPU} -d ${DATASET_NAME} -c ${CONFIG_NAME} -n ${EXP_NAME}
# Direct
export PYTHONPATH=./
python tools/train.py --config-file ${CONFIG_PATH} --num-gpus ${NUM_GPU} --options save_path=${SAVE_PATH}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# By script (Recommended)
# -p is default set as python and can be ignored
sh scripts/train.sh -p python -d scannet -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base
# Direct
export PYTHONPATH=./
python tools/train.py --config-file configs/scannet/semseg-pt-v2m2-0-base.py --options save_path=exp/scannet/semseg-pt-v2m2-0-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Resume training from checkpoint.&lt;/strong&gt; If the training process is interrupted by accident, the following script can resume training from a given checkpoint.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}
# Script (Recommended)
# simply add "-r true"
sh scripts/train.sh -p ${INTERPRETER_PATH} -g ${NUM_GPU} -d ${DATASET_NAME} -c ${CONFIG_NAME} -n ${EXP_NAME} -r true
# Direct
export PYTHONPATH=./
python tools/train.py --config-file ${CONFIG_PATH} --num-gpus ${NUM_GPU} --options save_path=${SAVE_PATH} resume=True weight=${CHECKPOINT_PATH}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Weights and Biases.&lt;/strong&gt; Pointcept by default enables both &lt;code&gt;tensorboard&lt;/code&gt; and &lt;code&gt;wandb&lt;/code&gt;. There are some usage notes related to &lt;code&gt;wandb&lt;/code&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Disable by set &lt;code&gt;enable_wandb=False&lt;/code&gt;;&lt;/li&gt; 
 &lt;li&gt;Sync with &lt;code&gt;wandb&lt;/code&gt; remote server by &lt;code&gt;wandb login&lt;/code&gt; in the terminal or set &lt;code&gt;wandb_key=YOUR_WANDB_KEY&lt;/code&gt; in config.&lt;/li&gt; 
 &lt;li&gt;The project name is "Pointcept" by default, custom it to your research project name by setting &lt;code&gt;wandb_project=YOUR_PROJECT_NAME&lt;/code&gt; (e.g. Sonata-Dev, PointTransformerV3-Dev)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;During training, model evaluation is performed on point clouds after grid sampling (voxelization), providing an initial assessment of model performance. &lt;del&gt;However, to obtain precise evaluation results, testing is &lt;strong&gt;essential&lt;/strong&gt;&lt;/del&gt; &lt;em&gt;(now we automatically run the testing process after training with the &lt;code&gt;PreciseEvaluation&lt;/code&gt; hook)&lt;/em&gt;. The testing process involves subsampling a dense point cloud into a sequence of voxelized point clouds, ensuring comprehensive coverage of all points. These sub-results are then predicted and collected to form a complete prediction of the entire point cloud. This approach yields higher evaluation results compared to simply mapping/interpolating the prediction. In addition, our testing code supports TTA (test time augmentation) testing, which further enhances the stability of evaluation performance.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# By script (Based on experiment folder created by training script)
sh scripts/test.sh -p ${INTERPRETER_PATH} -g ${NUM_GPU} -d ${DATASET_NAME} -n ${EXP_NAME} -w ${CHECKPOINT_NAME}
# Direct
export PYTHONPATH=./
python tools/test.py --config-file ${CONFIG_PATH} --num-gpus ${NUM_GPU} --options save_path=${SAVE_PATH} weight=${CHECKPOINT_PATH}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# By script (Based on experiment folder created by training script)
# -p is default set as python and can be ignored
# -w is default set as model_best and can be ignored
sh scripts/test.sh -p python -d scannet -n semseg-pt-v2m2-0-base -w model_best
# Direct
export PYTHONPATH=./
python tools/test.py --config-file configs/scannet/semseg-pt-v2m2-0-base.py --options save_path=exp/scannet/semseg-pt-v2m2-0-base weight=exp/scannet/semseg-pt-v2m2-0-base/model/model_best.pth
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The TTA can be disabled by replace &lt;code&gt;data.test.test_cfg.aug_transform = [...]&lt;/code&gt; with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;data = dict(
    train = dict(...),
    val = dict(...),
    test = dict(
        ...,
        test_cfg = dict(
            ...,
            aug_transform = [
                [dict(type="RandomRotateTargetAngle", angle=[0], axis="z", center=[0, 0, 0], p=1)]
            ]
        )
    )
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Offset&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Offset&lt;/code&gt; is the separator of point clouds in batch data, and it is similar to the concept of &lt;code&gt;Batch&lt;/code&gt; in PyG. A visual illustration of batch and offset is as follows:&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- pypi-strip --&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/pointcept/assets/main/pointcept/offset_dark.png" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/pointcept/assets/main/pointcept/offset.png" /&gt; 
  &lt;!-- /pypi-strip --&gt; 
  &lt;img alt="pointcept" src="https://raw.githubusercontent.com/pointcept/assets/main/pointcept/offset.png" width="480" /&gt; 
  &lt;!-- pypi-strip --&gt; 
 &lt;/picture&gt;&lt;br /&gt; 
 &lt;!-- /pypi-strip --&gt; &lt;/p&gt; 
&lt;h2&gt;Model Zoo&lt;/h2&gt; 
&lt;h3&gt;1. Backbones and Semantic Segmentation&lt;/h3&gt; 
&lt;h4&gt;SparseUNet&lt;/h4&gt; 
&lt;p&gt;&lt;em&gt;Pointcept&lt;/em&gt; provides &lt;code&gt;SparseUNet&lt;/code&gt; implemented by &lt;code&gt;SpConv&lt;/code&gt; and &lt;code&gt;MinkowskiEngine&lt;/code&gt;. The SpConv version is recommended since SpConv is easy to install and faster than MinkowskiEngine. Meanwhile, SpConv is also widely applied in outdoor perception.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SpConv (recommend)&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The SpConv version &lt;code&gt;SparseUNet&lt;/code&gt; in the codebase was fully rewrite from &lt;code&gt;MinkowskiEngine&lt;/code&gt; version, example running script is as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet val
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base
# ScanNet200
sh scripts/train.sh -g 4 -d scannet200 -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base
# S3DIS
sh scripts/train.sh -g 4 -d s3dis -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base
# S3DIS (with normal)
sh scripts/train.sh -g 4 -d s3dis -c semseg-spunet-v1m1-0-cn-base -n semseg-spunet-v1m1-0-cn-base
# SemanticKITTI
sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base
# nuScenes
sh scripts/train.sh -g 4 -d nuscenes -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base
# ModelNet40
sh scripts/train.sh -g 2 -d modelnet40 -c cls-spunet-v1m1-0-base -n cls-spunet-v1m1-0-base

# ScanNet Data Efficient
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-la20 -n semseg-spunet-v1m1-2-efficient-la20
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-la50 -n semseg-spunet-v1m1-2-efficient-la50
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-la100 -n semseg-spunet-v1m1-2-efficient-la100
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-la200 -n semseg-spunet-v1m1-2-efficient-la200
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-lr1 -n semseg-spunet-v1m1-2-efficient-lr1
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-lr5 -n semseg-spunet-v1m1-2-efficient-lr5
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-lr10 -n semseg-spunet-v1m1-2-efficient-lr10
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-2-efficient-lr20 -n semseg-spunet-v1m1-2-efficient-lr20

# Profile model run time
sh scripts/train.sh -g 4 -d scannet -c semseg-spunet-v1m1-0-enable-profiler -n semseg-spunet-v1m1-0-enable-profiler
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MinkowskiEngine&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The MinkowskiEngine version &lt;code&gt;SparseUNet&lt;/code&gt; in the codebase was modified from the original MinkowskiEngine repo, and example running scripts are as follows:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install MinkowskiEngine, refer &lt;a href="https://github.com/NVIDIA/MinkowskiEngine"&gt;https://github.com/NVIDIA/MinkowskiEngine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Training with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Uncomment "# from .sparse_unet import *" in "pointcept/models/__init__.py"
# Uncomment "# from .mink_unet import *" in "pointcept/models/sparse_unet/__init__.py"
# ScanNet
sh scripts/train.sh -g 4 -d scannet -c semseg-minkunet34c-0-base -n semseg-minkunet34c-0-base
# ScanNet200
sh scripts/train.sh -g 4 -d scannet200 -c semseg-minkunet34c-0-base -n semseg-minkunet34c-0-base
# S3DIS
sh scripts/train.sh -g 4 -d s3dis -c semseg-minkunet34c-0-base -n semseg-minkunet34c-0-base
# SemanticKITTI
sh scripts/train.sh -g 2 -d semantic_kitti -c semseg-minkunet34c-0-base -n semseg-minkunet34c-0-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;OA-CNNs&lt;/h4&gt; 
&lt;p&gt;Introducing Omni-Adaptive 3D CNNs (&lt;strong&gt;OA-CNNs&lt;/strong&gt;), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse CNNs at minimal computational cost. Without any self-attention modules, &lt;strong&gt;OA-CNNs&lt;/strong&gt; favorably surpass point transformers in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Issue related to &lt;strong&gt;OA-CNNs&lt;/strong&gt; can @Pbihao.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet
sh scripts/train.sh -g 4 -d scannet -c semseg-oacnns-v1m1-0-base -n semseg-oacnns-v1m1-0-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Point Transformers&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PTv3&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2312.10035"&gt;PTv3&lt;/a&gt; is an efficient backbone model that achieves SOTA performances across indoor and outdoor scenarios. The full PTv3 relies on FlashAttention, while FlashAttention relies on CUDA 11.6 and above, make sure your local Pointcept environment satisfies the requirements.&lt;/p&gt; 
&lt;p&gt;If you can not upgrade your local environment to satisfy the requirements (CUDA &amp;gt;= 11.6), then you can disable FlashAttention by setting the model parameter &lt;code&gt;enable_flash&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; and reducing the &lt;code&gt;enc_patch_size&lt;/code&gt; and &lt;code&gt;dec_patch_size&lt;/code&gt; to a level (e.g. 128).&lt;/p&gt; 
&lt;p&gt;FlashAttention force disables RPE and forces the accuracy reduced to fp16. If you require these features, please disable &lt;code&gt;enable_flash&lt;/code&gt; and adjust &lt;code&gt;enable_rpe&lt;/code&gt;, &lt;code&gt;upcast_attention&lt;/code&gt; and&lt;code&gt;upcast_softmax&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Detailed instructions and experiment records (containing weights) are available on the &lt;a href="https://github.com/Pointcept/PointTransformerV3"&gt;project repository&lt;/a&gt;. Example running scripts are as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Scratched ScanNet
sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base
# PPT joint training (ScanNet + Structured3D) and evaluate in ScanNet
sh scripts/train.sh -g 8 -d scannet -c semseg-pt-v3m1-1-ppt-extreme -n semseg-pt-v3m1-1-ppt-extreme

# Scratched ScanNet200
sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base
# Fine-tuning from  PPT joint training (ScanNet + Structured3D) with ScanNet200
# PTV3_PPT_WEIGHT_PATH: Path to model weight trained by PPT multi-dataset joint training
# e.g. exp/scannet/semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth
sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v3m1-1-ppt-ft -n semseg-pt-v3m1-1-ppt-ft -w ${PTV3_PPT_WEIGHT_PATH}

# Scratched ScanNet++
sh scripts/train.sh -g 4 -d scannetpp -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base
# Scratched ScanNet++ test
sh scripts/train.sh -g 4 -d scannetpp -c semseg-pt-v3m1-1-submit -n semseg-pt-v3m1-1-submit


# Scratched S3DIS
sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base
# an example for disbale flash_attention and enable rpe.
sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v3m1-1-rpe -n semseg-pt-v3m1-0-rpe
# PPT joint training (ScanNet + S3DIS + Structured3D) and evaluate in ScanNet
sh scripts/train.sh -g 8 -d s3dis -c semseg-pt-v3m1-1-ppt-extreme -n semseg-pt-v3m1-1-ppt-extreme
# S3DIS 6-fold cross validation
# 1. The default configs are evaluated on Area_5, modify the "data.train.split", "data.val.split", and "data.test.split" to make the config evaluated on Area_1 ~ Area_6 respectively.
# 2. Train and evaluate the model on each split of areas and gather result files located in "exp/s3dis/EXP_NAME/result/Area_x.pth" in one single folder, noted as RECORD_FOLDER.
# 3. Run the following script to get S3DIS 6-fold cross validation performance:
export PYTHONPATH=./
python tools/test_s3dis_6fold.py --record_root ${RECORD_FOLDER}

# Scratched nuScenes
sh scripts/train.sh -g 4 -d nuscenes -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base
# Scratched Waymo
sh scripts/train.sh -g 4 -d waymo -c semseg-pt-v3m1-0-base -n semseg-pt-v3m1-0-base

# More configs and exp records for PTv3 will be available soon.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Indoor semantic segmentation&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Benchmark&lt;/th&gt; 
   &lt;th align="center"&gt;Additional Data&lt;/th&gt; 
   &lt;th align="center"&gt;Num GPUs&lt;/th&gt; 
   &lt;th align="center"&gt;Val mIoU&lt;/th&gt; 
   &lt;th align="center"&gt;Config&lt;/th&gt; 
   &lt;th align="center"&gt;Tensorboard&lt;/th&gt; 
   &lt;th align="center"&gt;Exp Record&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;PTv3&lt;/td&gt; 
   &lt;td align="center"&gt;ScanNet&lt;/td&gt; 
   &lt;td align="center"&gt;‚úó&lt;/td&gt; 
   &lt;td align="center"&gt;4&lt;/td&gt; 
   &lt;td align="center"&gt;77.6%&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Pointcept/Pointcept/raw/main/configs/scannet/semseg-pt-v3m1-0-base.py"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tensorboard"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tree/main/scannet-semseg-pt-v3m1-0-base"&gt;link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;PTv3 + PPT&lt;/td&gt; 
   &lt;td align="center"&gt;ScanNet&lt;/td&gt; 
   &lt;td align="center"&gt;‚úì&lt;/td&gt; 
   &lt;td align="center"&gt;8&lt;/td&gt; 
   &lt;td align="center"&gt;78.5%&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Pointcept/Pointcept/raw/main/configs/scannet/semseg-pt-v3m1-1-ppt-extreme.py"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tensorboard"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tree/main/scannet-semseg-pt-v3m1-1-ppt-extreme"&gt;link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;PTv3&lt;/td&gt; 
   &lt;td align="center"&gt;ScanNet200&lt;/td&gt; 
   &lt;td align="center"&gt;‚úó&lt;/td&gt; 
   &lt;td align="center"&gt;4&lt;/td&gt; 
   &lt;td align="center"&gt;35.3%&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Pointcept/Pointcept/raw/main/configs/scannet200/semseg-pt-v3m1-0-base.py"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tensorboard"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tree/main/scannet200-semseg-pt-v3m1-0-base"&gt;link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;PTv3&lt;/td&gt; 
   &lt;td align="center"&gt;S3DIS (Area5)&lt;/td&gt; 
   &lt;td align="center"&gt;‚úó&lt;/td&gt; 
   &lt;td align="center"&gt;4&lt;/td&gt; 
   &lt;td align="center"&gt;73.6%&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Pointcept/Pointcept/raw/main/configs/s3dis/semseg-pt-v3m1-0-rpe.py"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tensorboard"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tree/main/s3dis-semseg-pt-v3m1-0-rpe"&gt;link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;PTv3 + PPT&lt;/td&gt; 
   &lt;td align="center"&gt;S3DIS (Area5)&lt;/td&gt; 
   &lt;td align="center"&gt;‚úì&lt;/td&gt; 
   &lt;td align="center"&gt;8&lt;/td&gt; 
   &lt;td align="center"&gt;75.4%&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/Pointcept/Pointcept/raw/main/configs/s3dis/semseg-pt-v3m1-1-ppt-extreme.py"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tensorboard"&gt;link&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/Pointcept/PointTransformerV3/tree/main/s3dis-semseg-pt-v3m1-1-ppt-extreme"&gt;link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;em&gt;&lt;strong&gt;*Released model weights are trained for v1.5.1, weights for v1.5.2 and later is still ongoing.&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PTv2 mode2&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The original PTv2 was trained on 4 * RTX a6000 (48G memory). Even enabling AMP, the memory cost of the original PTv2 is slightly larger than 24G. Considering GPUs with 24G memory are much more accessible, I tuned the PTv2 on the latest Pointcept and made it runnable on 4 * RTX 3090 machines.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;PTv2 Mode2&lt;/code&gt; enables AMP and disables &lt;em&gt;Position Encoding Multiplier&lt;/em&gt; &amp;amp; &lt;em&gt;Grouped Linear&lt;/em&gt;. During our further research, we found that precise coordinates are not necessary for point cloud understanding (Replacing precise coordinates with grid coordinates doesn't influence the performance. Also, SparseUNet is an example). As for Grouped Linear, my implementation of Grouped Linear seems to cost more memory than the Linear layer provided by PyTorch. Benefiting from the codebase and better parameter tuning, we also relieve the overfitting problem. The reproducing performance is even better than the results reported in our paper.&lt;/p&gt; 
&lt;p&gt;Example running scripts are as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ptv2m2: PTv2 mode2, disable PEM &amp;amp; Grouped Linear, GPU memory cost &amp;lt; 24G (recommend)
# ScanNet
sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base
sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v2m2-3-lovasz -n semseg-pt-v2m2-3-lovasz

# ScanNet test
sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v2m2-1-submit -n semseg-pt-v2m2-1-submit
# ScanNet200
sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base
# ScanNet++
sh scripts/train.sh -g 4 -d scannetpp -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base
# ScanNet++ test
sh scripts/train.sh -g 4 -d scannetpp -c semseg-pt-v2m2-1-submit -n semseg-pt-v2m2-1-submit
# S3DIS
sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base
# SemanticKITTI
sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base
# nuScenes
sh scripts/train.sh -g 4 -d nuscenes -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PTv2 mode1&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;PTv2 mode1&lt;/code&gt; is the original PTv2 we reported in our paper, example running scripts are as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ptv2m1: PTv2 mode1, Original PTv2, GPU memory cost &amp;gt; 24G
# ScanNet
sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v2m1-0-base -n semseg-pt-v2m1-0-base
# ScanNet200
sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v2m1-0-base -n semseg-pt-v2m1-0-base
# S3DIS
sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v2m1-0-base -n semseg-pt-v2m1-0-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PTv1&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The original PTv1 is also available in our Pointcept codebase. I haven't run PTv1 for a long time, but I have ensured that the example running script works well.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet
sh scripts/train.sh -g 4 -d scannet -c semseg-pt-v1-0-base -n semseg-pt-v1-0-base
# ScanNet200
sh scripts/train.sh -g 4 -d scannet200 -c semseg-pt-v1-0-base -n semseg-pt-v1-0-base
# S3DIS
sh scripts/train.sh -g 4 -d s3dis -c semseg-pt-v1-0-base -n semseg-pt-v1-0-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Stratified Transformer&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Additional requirements:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install torch-points3d
# Fix dependence, caused by installing torch-points3d 
pip uninstall SharedArray
pip install SharedArray==3.2.1

cd libs/pointops2
python setup.py install
cd ../..
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Uncomment &lt;code&gt;# from .stratified_transformer import *&lt;/code&gt; in &lt;code&gt;pointcept/models/__init__.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Refer &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/installation"&gt;Optional Installation&lt;/a&gt; to install dependence.&lt;/li&gt; 
 &lt;li&gt;Training with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# stv1m1: Stratified Transformer mode1, Modified from the original Stratified Transformer code.
# PTv2m2: Stratified Transformer mode2, My rewrite version (recommend).

# ScanNet
sh scripts/train.sh -g 4 -d scannet -c semseg-st-v1m2-0-refined -n semseg-st-v1m2-0-refined
sh scripts/train.sh -g 4 -d scannet -c semseg-st-v1m1-0-origin -n semseg-st-v1m1-0-origin
# ScanNet200
sh scripts/train.sh -g 4 -d scannet200 -c semseg-st-v1m2-0-refined -n semseg-st-v1m2-0-refined
# S3DIS
sh scripts/train.sh -g 4 -d s3dis -c semseg-st-v1m2-0-refined -n semseg-st-v1m2-0-refined
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;SPVCNN&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;SPVCNN&lt;/code&gt; is a baseline model of &lt;a href="https://github.com/mit-han-lab/spvnas"&gt;SPVNAS&lt;/a&gt;, it is also a practical baseline for outdoor datasets.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install torchsparse:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# refer https://github.com/mit-han-lab/torchsparse
# install method without sudo apt install
conda install google-sparsehash -c bioconda
export C_INCLUDE_PATH=${CONDA_PREFIX}/include:$C_INCLUDE_PATH
export CPLUS_INCLUDE_PATH=${CONDA_PREFIX}/include:CPLUS_INCLUDE_PATH
pip install --upgrade git+https://github.com/mit-han-lab/torchsparse.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Training with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# SemanticKITTI
sh scripts/train.sh -g 2 -d semantic_kitti -c semseg-spvcnn-v1m1-0-base -n semseg-spvcnn-v1m1-0-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;OctFormer&lt;/h4&gt; 
&lt;p&gt;OctFormer from &lt;em&gt;OctFormer: Octree-based Transformers for 3D Point Clouds&lt;/em&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Additional requirements:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd libs
git clone https://github.com/octree-nn/dwconv.git
pip install ./dwconv
pip install ocnn
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Uncomment &lt;code&gt;# from .octformer import *&lt;/code&gt; in &lt;code&gt;pointcept/models/__init__.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Training with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet
sh scripts/train.sh -g 4 -d scannet -c semseg-octformer-v1m1-0-base -n semseg-octformer-v1m1-0-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Swin3D&lt;/h4&gt; 
&lt;p&gt;Swin3D from &lt;em&gt;Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding&lt;/em&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Additional requirements:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MinkEngine v0.5.4, follow readme in https://github.com/NVIDIA/MinkowskiEngine;
# 2. Install Swin3D, mainly for cuda operation:
cd libs
git clone https://github.com/microsoft/Swin3D.git
cd Swin3D
pip install ./
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Uncomment &lt;code&gt;# from .swin3d import *&lt;/code&gt; in &lt;code&gt;pointcept/models/__init__.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Pre-Training with the following example scripts (Structured3D preprocessing refer &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#structured3d"&gt;here&lt;/a&gt;):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Structured3D + Swin-S
sh scripts/train.sh -g 4 -d structured3d -c semseg-swin3d-v1m1-0-small -n semseg-swin3d-v1m1-0-small
# Structured3D + Swin-L
sh scripts/train.sh -g 4 -d structured3d -c semseg-swin3d-v1m1-1-large -n semseg-swin3d-v1m1-1-large

# Addition
# Structured3D + SpUNet
sh scripts/train.sh -g 4 -d structured3d -c semseg-spunet-v1m1-0-base -n semseg-spunet-v1m1-0-base
# Structured3D + PTv2
sh scripts/train.sh -g 4 -d structured3d -c semseg-pt-v2m2-0-base -n semseg-pt-v2m2-0-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Fine-tuning with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet + Swin-S
sh scripts/train.sh -g 4 -d scannet -w exp/structured3d/semseg-swin3d-v1m1-1-large/model/model_last.pth -c semseg-swin3d-v1m1-0-small -n semseg-swin3d-v1m1-0-small
# ScanNet + Swin-L
sh scripts/train.sh -g 4 -d scannet -w exp/structured3d/semseg-swin3d-v1m1-1-large/model/model_last.pth -c semseg-swin3d-v1m1-1-large -n semseg-swin3d-v1m1-1-large

# S3DIS + Swin-S (here we provide config support S3DIS normal vector)
sh scripts/train.sh -g 4 -d s3dis -w exp/structured3d/semseg-swin3d-v1m1-1-large/model/model_last.pth -c semseg-swin3d-v1m1-0-small -n semseg-swin3d-v1m1-0-small
# S3DIS + Swin-L (here we provide config support S3DIS normal vector)
sh scripts/train.sh -g 4 -d s3dis -w exp/structured3d/semseg-swin3d-v1m1-1-large/model/model_last.pth -c semseg-swin3d-v1m1-1-large -n semseg-swin3d-v1m1-1-large
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Context-Aware Classifier&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;Context-Aware Classifier&lt;/code&gt; is a segmentor that can further boost the performance of each backbone, as a replacement for &lt;code&gt;Default Segmentor&lt;/code&gt;. Training with the following example scripts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet
sh scripts/train.sh -g 4 -d scannet -c semseg-cac-v1m1-0-spunet-base -n semseg-cac-v1m1-0-spunet-base
sh scripts/train.sh -g 4 -d scannet -c semseg-cac-v1m1-1-spunet-lovasz -n semseg-cac-v1m1-1-spunet-lovasz
sh scripts/train.sh -g 4 -d scannet -c semseg-cac-v1m1-2-ptv2-lovasz -n semseg-cac-v1m1-2-ptv2-lovasz

# ScanNet200
sh scripts/train.sh -g 4 -d scannet200 -c semseg-cac-v1m1-0-spunet-base -n semseg-cac-v1m1-0-spunet-base
sh scripts/train.sh -g 4 -d scannet200 -c semseg-cac-v1m1-1-spunet-lovasz -n semseg-cac-v1m1-1-spunet-lovasz
sh scripts/train.sh -g 4 -d scannet200 -c semseg-cac-v1m1-2-ptv2-lovasz -n semseg-cac-v1m1-2-ptv2-lovasz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Instance Segmentation&lt;/h3&gt; 
&lt;h4&gt;PointGroup&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/dvlab-research/PointGroup"&gt;PointGroup&lt;/a&gt; is a baseline framework for point cloud instance segmentation.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Additional requirements:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda install -c bioconda google-sparsehash 
cd libs/pointgroup_ops
python setup.py install --include_dirs=${CONDA_PREFIX}/include
cd ../..
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Uncomment &lt;code&gt;# from .point_group import *&lt;/code&gt; in &lt;code&gt;pointcept/models/__init__.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Training with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet
sh scripts/train.sh -g 4 -d scannet -c insseg-pointgroup-v1m1-0-spunet-base -n insseg-pointgroup-v1m1-0-spunet-base
# S3DIS
sh scripts/train.sh -g 4 -d scannet -c insseg-pointgroup-v1m1-0-spunet-base -n insseg-pointgroup-v1m1-0-spunet-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Pre-training&lt;/h3&gt; 
&lt;h4&gt;Sonata&lt;/h4&gt; 
&lt;p&gt;Follow the instruction &lt;a href="https://github.com/Pointcept/Pointcept/tree/main/pointcept/models/sonata"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Masked Scene Contrast (MSC)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pre-training with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet
sh scripts/train.sh -g 8 -d scannet -c pretrain-msc-v1m1-0-spunet-base -n pretrain-msc-v1m1-0-spunet-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Fine-tuning with the following example scripts:&lt;br /&gt; enable PointGroup (&lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#pointgroup"&gt;here&lt;/a&gt;) before fine-tuning on instance segmentation task.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet20 Semantic Segmentation
sh scripts/train.sh -g 8 -d scannet -w exp/scannet/pretrain-msc-v1m1-0-spunet-base/model/model_last.pth -c semseg-spunet-v1m1-4-ft -n semseg-msc-v1m1-0f-spunet-base
# ScanNet20 Instance Segmentation (enable PointGroup before running the script)
sh scripts/train.sh -g 4 -d scannet -w exp/scannet/pretrain-msc-v1m1-0-spunet-base/model/model_last.pth -c insseg-pointgroup-v1m1-0-spunet-base -n insseg-msc-v1m1-0f-pointgroup-spunet-base
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Example log and weight: [&lt;a href="https://connecthkuhk-my.sharepoint.com/:u:/g/personal/wuxy_connect_hku_hk/EYvNV4XUJ_5Mlk-g15RelN4BW_P8lVBfC_zhjC_BlBDARg?e=UoGFWH"&gt;Pretrain&lt;/a&gt;] [&lt;a href="https://connecthkuhk-my.sharepoint.com/:u:/g/personal/wuxy_connect_hku_hk/EQkDiv5xkOFKgCpGiGtAlLwBon7i8W6my3TIbGVxuiTttQ?e=tQFnbr"&gt;Semseg&lt;/a&gt;]&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Point Prompt Training (PPT)&lt;/h4&gt; 
&lt;p&gt;PPT presents a multi-dataset pre-training framework, and it is compatible with various existing pre-training frameworks and backbones.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;PPT supervised joint training with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet + Structured3d, validate on ScanNet (S3DIS might cause long data time, w/o S3DIS for a quick validation) &amp;gt;= 3090 * 8 
sh scripts/train.sh -g 8 -d scannet -c semseg-ppt-v1m1-0-sc-st-spunet -n semseg-ppt-v1m1-0-sc-st-spunet
sh scripts/train.sh -g 8 -d scannet -c semseg-ppt-v1m1-1-sc-st-spunet-submit -n semseg-ppt-v1m1-1-sc-st-spunet-submit
# ScanNet + S3DIS + Structured3d, validate on S3DIS (&amp;gt;= a100 * 8)
sh scripts/train.sh -g 8 -d s3dis -c semseg-ppt-v1m1-0-s3-sc-st-spunet -n semseg-ppt-v1m1-0-s3-sc-st-spunet
# SemanticKITTI + nuScenes + Waymo, validate on SemanticKITTI (bs12 &amp;gt;= 3090 * 4 &amp;gt;= 3090 * 8, v1m1-0 is still on tuning)
sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-ppt-v1m1-0-nu-sk-wa-spunet -n semseg-ppt-v1m1-0-nu-sk-wa-spunet
sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-ppt-v1m2-0-sk-nu-wa-spunet -n semseg-ppt-v1m2-0-sk-nu-wa-spunet
sh scripts/train.sh -g 4 -d semantic_kitti -c semseg-ppt-v1m2-1-sk-nu-wa-spunet-submit -n semseg-ppt-v1m2-1-sk-nu-wa-spunet-submit
# SemanticKITTI + nuScenes + Waymo, validate on nuScenes (bs12 &amp;gt;= 3090 * 4; bs24 &amp;gt;= 3090 * 8, v1m1-0 is still on tuning))
sh scripts/train.sh -g 4 -d nuscenes -c semseg-ppt-v1m1-0-nu-sk-wa-spunet -n semseg-ppt-v1m1-0-nu-sk-wa-spunet
sh scripts/train.sh -g 4 -d nuscenes -c semseg-ppt-v1m2-0-nu-sk-wa-spunet -n semseg-ppt-v1m2-0-nu-sk-wa-spunet
sh scripts/train.sh -g 4 -d nuscenes -c semseg-ppt-v1m2-1-nu-sk-wa-spunet-submit -n semseg-ppt-v1m2-1-nu-sk-wa-spunet-submit
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;PointContrast&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Preprocess and link ScanNet-Pair dataset (pair-wise matching with ScanNet raw RGB-D frame, ~1.5T):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# RAW_SCANNET_DIR: the directory of downloaded ScanNet v2 raw dataset.
# PROCESSED_SCANNET_PAIR_DIR: the directory of processed ScanNet pair dataset (output dir).
python pointcept/datasets/preprocessing/scannet/scannet_pair/preprocess.py --dataset_root ${RAW_SCANNET_DIR} --output_root ${PROCESSED_SCANNET_PAIR_DIR}
ln -s ${PROCESSED_SCANNET_PAIR_DIR} ${CODEBASE_DIR}/data/scannet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Pre-training with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet
sh scripts/train.sh -g 8 -d scannet -c pretrain-msc-v1m1-1-spunet-pointcontrast -n pretrain-msc-v1m1-1-spunet-pointcontrast
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Fine-tuning refer &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#masked-scene-contrast-msc"&gt;MSC&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Contrastive Scene Contexts&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Preprocess and link ScanNet-Pair dataset (refer &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#pointcontrast"&gt;PointContrast&lt;/a&gt;):&lt;/li&gt; 
 &lt;li&gt;Pre-training with the following example scripts:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ScanNet
sh scripts/train.sh -g 8 -d scannet -c pretrain-msc-v1m2-0-spunet-csc -n pretrain-msc-v1m2-0-spunet-csc
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Fine-tuning refer &lt;a href="https://raw.githubusercontent.com/Pointcept/Pointcept/main/#masked-scene-contrast-msc"&gt;MSC&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Pointcept&lt;/em&gt; is designed by &lt;a href="https://xywu.me/"&gt;Xiaoyang&lt;/a&gt;, named by &lt;a href="https://github.com/yxlao"&gt;Yixing&lt;/a&gt; and the logo is created by &lt;a href="https://julianjuaner.github.io/"&gt;Yuechen&lt;/a&gt;. It is derived from &lt;a href="https://hszhao.github.io/"&gt;Hengshuang&lt;/a&gt;'s &lt;a href="https://github.com/hszhao/semseg"&gt;Semseg&lt;/a&gt; and inspirited by several repos, e.g., &lt;a href="https://github.com/NVIDIA/MinkowskiEngine"&gt;MinkowskiEngine&lt;/a&gt;, &lt;a href="https://github.com/charlesq34/pointnet2"&gt;pointnet2&lt;/a&gt;, &lt;a href="https://github.com/open-mmlab/mmcv/tree/master/mmcv"&gt;mmcv&lt;/a&gt;, and &lt;a href="https://github.com/facebookresearch/detectron2"&gt;Detectron2&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÁÆ±Âç≥Áî®&lt;/strong&gt;: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êòì‰∫éÈõÜÊàê&lt;/strong&gt;: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ÂèØÊéß&lt;/strong&gt;: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;ÂÆâË£ÖÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨„ÄÇ&lt;br /&gt; Âú®ËøêË°å SQLBot ÂâçÔºåËØ∑Á°Æ‰øùÂ∑≤ÂÆâË£ÖÂ•Ω &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt; Âíå &lt;a href="https://docs.docker.com/compose/install/"&gt;Docker Compose&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂàõÂª∫ÁõÆÂΩï
mkdir -p /opt/sqlbot
cd /opt/sqlbot

# ‰∏ãËΩΩ docker-compose.yaml
curl -o docker-compose.yaml https://raw.githubusercontent.com/dataease/SQLBot/main/docker-compose.yaml

# ÂêØÂä®ÊúçÂä°
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‰Ω†‰πüÂèØ‰ª•ÈÄöËøá &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel Â∫îÁî®ÂïÜÂ∫ó&lt;/a&gt; Âø´ÈÄüÈÉ®ÁΩ≤ SQLBotÔºõ&lt;/p&gt; 
&lt;h3&gt;ËÆøÈóÆÊñπÂºè&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&amp;lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;Áî®Êà∑Âêç: admin&lt;/li&gt; 
 &lt;li&gt;ÂØÜÁ†Å: SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÅîÁ≥ªÊàë‰ª¨&lt;/h3&gt; 
&lt;p&gt;Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ&lt;/p&gt; 
&lt;img width="396" height="396" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI Â±ïÁ§∫&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ªìÂ∫ìÈÅµÂæ™ &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sinaptik-ai/pandas-ai</title>
      <link>https://github.com/sinaptik-ai/pandas-ai</link>
      <description>&lt;p&gt;Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/logo.png" alt="PandasAI" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/pandasai/"&gt;&lt;img src="https://img.shields.io/pypi/v/pandasai?label=Release&amp;amp;style=flat-square" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg"&gt;&lt;img src="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg"&gt;&lt;img src="https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg?sanitize=true" alt="CD" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/sinaptik-ai/pandas-ai"&gt;&lt;img src="https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/KYKj9F2FRH"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;amp;compact=true" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/pandasai"&gt;&lt;img src="https://static.pepy.tech/badge/pandasai" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;PandasAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.&lt;/p&gt; 
&lt;h1&gt;üîß Getting started&lt;/h1&gt; 
&lt;p&gt;You can find the full documentation for PandasAI &lt;a href="https://pandas-ai.readthedocs.io/en/latest/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can either decide to use PandasAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.&lt;/p&gt; 
&lt;h2&gt;üìö Using the library&lt;/h2&gt; 
&lt;h3&gt;Python Requirements&lt;/h3&gt; 
&lt;p&gt;Python version &lt;code&gt;3.8+ &amp;lt;3.12&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;üì¶ Installation&lt;/h3&gt; 
&lt;p&gt;You can install the PandasAI library using pip or poetry.&lt;/p&gt; 
&lt;p&gt;With pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "pandasai&amp;gt;=3.0.0b2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With poetry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;poetry add "pandasai&amp;gt;=3.0.0b2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üíª Usage&lt;/h3&gt; 
&lt;h4&gt;Ask questions&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_openai.openai import OpenAI

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

# Sample DataFrame
df = pai.DataFrame({
    "country": ["United States", "United Kingdom", "France", "Germany", "Italy", "Spain", "Canada", "Australia", "Japan", "China"],
    "revenue": [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]
})

df.chat('Which are the top 5 countries by sales?')
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;China, United States, Japan, Germany, Australia
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;Or you can ask more complex questions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;df.chat(
    "What is the total sales for the top 3 countries by sales?"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;The total sales for the top 3 countries by sales is 16500.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Visualize charts&lt;/h4&gt; 
&lt;p&gt;You can also ask PandasAI to generate charts for you:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;df.chat(
    "Plot the histogram of countries showing for each one the gd. Use different colors for each bar",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/assets/histogram-chart.png?raw=true" alt="Chart" /&gt;&lt;/p&gt; 
&lt;h4&gt;Multiple DataFrames&lt;/h4&gt; 
&lt;p&gt;You can also pass in multiple dataframes to PandasAI and ask questions relating them.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_openai.openai import OpenAI

employees_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],
    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']
}

salaries_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Salary': [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)


pai.chat("Who gets paid the most?", employees_df, salaries_df)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Sandbox&lt;/h4&gt; 
&lt;p&gt;You can run PandasAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.&lt;/p&gt; 
&lt;h5&gt;Python Requirements&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "pandasai-docker"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Usage&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandasai as pai
from pandasai_docker import DockerSandbox
from pandasai_openai.openai import OpenAI

# Initialize the sandbox
sandbox = DockerSandbox()
sandbox.start()

employees_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],
    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']
}

salaries_data = {
    'EmployeeID': [1, 2, 3, 4, 5],
    'Salary': [5000, 6000, 4500, 7000, 5500]
}

llm = OpenAI("OPEN_AI_API_KEY")

pai.config.set({
    "llm": llm
})

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

pai.chat("Who gets paid the most?", employees_df, salaries_df, sandbox=sandbox)

# Don't forget to stop the sandbox when done
sandbox.stop()
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;Olivia gets paid the most.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find more examples in the &lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples"&gt;examples&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;PandasAI is available under the MIT expat license, except for the &lt;code&gt;pandasai/ee&lt;/code&gt; directory of this repository, which has its &lt;a href="https://github.com/sinaptik-ai/pandas-ai/raw/main/ee/LICENSE"&gt;license here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested in managed PandasAI Cloud or self-hosted Enterprise Offering, &lt;a href="https://getpanda.ai/pricing"&gt;contact us&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Beta Notice&lt;/strong&gt;&lt;br /&gt; Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pandas-ai.readthedocs.io/en/latest/"&gt;Docs&lt;/a&gt; for comprehensive documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/examples"&gt;Examples&lt;/a&gt; for example notebooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/KYKj9F2FRH"&gt;Discord&lt;/a&gt; for discussion with the community and PandasAI team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please check the outstanding issues and feel free to open a pull request. For more information, please check out the &lt;a href="https://raw.githubusercontent.com/sinaptik-ai/pandas-ai/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Thank you!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sinaptik-ai/pandas-ai/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-python</title>
      <link>https://github.com/openai/openai-python</link>
      <description>&lt;p&gt;The official Python library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Python API library&lt;/h1&gt; 
&lt;!-- prettier-ignore --&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/openai/"&gt;&lt;img src="https://img.shields.io/pypi/v/openai.svg?label=pypi%20(stable)" alt="PyPI version" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by &lt;a href="https://github.com/encode/httpx"&gt;httpx&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It is generated from our &lt;a href="https://github.com/openai/openai-openapi"&gt;OpenAPI specification&lt;/a&gt; with &lt;a href="https://stainlessapi.com/"&gt;Stainless&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The REST API documentation can be found on &lt;a href="https://platform.openai.com/docs/api-reference"&gt;platform.openai.com&lt;/a&gt;. The full API of this library can be found in &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/api.md"&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# install from PyPI
pip install openai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;The full API of this library can be found in &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/api.md"&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The primary API for interacting with OpenAI models is the &lt;a href="https://platform.openai.com/docs/api-reference/responses"&gt;Responses API&lt;/a&gt;. You can generate text from the model with the code below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

response = client.responses.create(
    model="gpt-4o",
    instructions="You are a coding assistant that talks like a pirate.",
    input="How do I check if a Python object is an instance of a class?",
)

print(response.output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The previous standard (supported indefinitely) for generating text is the &lt;a href="https://platform.openai.com/docs/api-reference/chat"&gt;Chat Completions API&lt;/a&gt;. You can use that API to generate text from the model with the code below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "developer", "content": "Talk like a pirate."},
        {
            "role": "user",
            "content": "How do I check if a Python object is an instance of a class?",
        },
    ],
)

print(completion.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;While you can provide an &lt;code&gt;api_key&lt;/code&gt; keyword argument, we recommend using &lt;a href="https://pypi.org/project/python-dotenv/"&gt;python-dotenv&lt;/a&gt; to add &lt;code&gt;OPENAI_API_KEY="My API Key"&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file so that your API key is not stored in source control. &lt;a href="https://platform.openai.com/settings/organization/api-keys"&gt;Get an API key here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Vision&lt;/h3&gt; 
&lt;p&gt;With an image URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;prompt = "What is in this image?"
img_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg"

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"{img_url}"},
            ],
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With the image as a base64 encoded string:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import base64
from openai import OpenAI

client = OpenAI()

prompt = "What is in this image?"
with open("path/to/image.png", "rb") as image_file:
    b64_image = base64.b64encode(image_file.read()).decode("utf-8")

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"data:image/png;base64,{b64_image}"},
            ],
        }
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Async usage&lt;/h2&gt; 
&lt;p&gt;Simply import &lt;code&gt;AsyncOpenAI&lt;/code&gt; instead of &lt;code&gt;OpenAI&lt;/code&gt; and use &lt;code&gt;await&lt;/code&gt; with each API call:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -&amp;gt; None:
    response = await client.responses.create(
        model="gpt-4o", input="Explain disestablishmentarianism to a smart five year old."
    )
    print(response.output_text)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Functionality between the synchronous and asynchronous clients is otherwise identical.&lt;/p&gt; 
&lt;h3&gt;With aiohttp&lt;/h3&gt; 
&lt;p&gt;By default, the async client uses &lt;code&gt;httpx&lt;/code&gt; for HTTP requests. However, for improved concurrency performance you may also use &lt;code&gt;aiohttp&lt;/code&gt; as the HTTP backend.&lt;/p&gt; 
&lt;p&gt;You can enable this by installing &lt;code&gt;aiohttp&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# install from PyPI
pip install openai[aiohttp]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can enable it by instantiating the client with &lt;code&gt;http_client=DefaultAioHttpClient()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from openai import DefaultAioHttpClient
from openai import AsyncOpenAI


async def main() -&amp;gt; None:
    async with AsyncOpenAI(
        api_key="My API Key",
        http_client=DefaultAioHttpClient(),
    ) as client:
        chat_completion = await client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": "Say this is a test",
                }
            ],
            model="gpt-4o",
        )


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Streaming responses&lt;/h2&gt; 
&lt;p&gt;We provide support for streaming responses using Server Side Events (SSE).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

stream = client.responses.create(
    model="gpt-4o",
    input="Write a one-sentence bedtime story about a unicorn.",
    stream=True,
)

for event in stream:
    print(event)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main():
    stream = await client.responses.create(
        model="gpt-4o",
        input="Write a one-sentence bedtime story about a unicorn.",
        stream=True,
    )

    async for event in stream:
        print(event)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Realtime API beta&lt;/h2&gt; 
&lt;p&gt;The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as &lt;a href="https://platform.openai.com/docs/guides/function-calling"&gt;function calling&lt;/a&gt; through a WebSocket connection.&lt;/p&gt; 
&lt;p&gt;Under the hood the SDK uses the &lt;a href="https://websockets.readthedocs.io/en/stable/"&gt;&lt;code&gt;websockets&lt;/code&gt;&lt;/a&gt; library to manage connections.&lt;/p&gt; 
&lt;p&gt;The Realtime API works through a combination of client-sent events and server-sent events. Clients can send events to do things like update session configuration or send text and audio inputs. Server events confirm when audio responses have completed, or when a text response from the model has been received. A full event reference can be found &lt;a href="https://platform.openai.com/docs/api-reference/realtime-client-events"&gt;here&lt;/a&gt; and a guide can be found &lt;a href="https://platform.openai.com/docs/guides/realtime"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Basic text based example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import asyncio
from openai import AsyncOpenAI

async def main():
    client = AsyncOpenAI()

    async with client.beta.realtime.connect(model="gpt-4o-realtime-preview") as connection:
        await connection.session.update(session={'modalities': ['text']})

        await connection.conversation.item.create(
            item={
                "type": "message",
                "role": "user",
                "content": [{"type": "input_text", "text": "Say hello!"}],
            }
        )
        await connection.response.create()

        async for event in connection:
            if event.type == 'response.text.delta':
                print(event.delta, flush=True, end="")

            elif event.type == 'response.text.done':
                print()

            elif event.type == "response.done":
                break

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However the real magic of the Realtime API is handling audio inputs / outputs, see this example &lt;a href="https://github.com/openai/openai-python/raw/main/examples/realtime/push_to_talk_app.py"&gt;TUI script&lt;/a&gt; for a fully fledged example.&lt;/p&gt; 
&lt;h3&gt;Realtime error handling&lt;/h3&gt; 
&lt;p&gt;Whenever an error occurs, the Realtime API will send an &lt;a href="https://platform.openai.com/docs/guides/realtime-model-capabilities#error-handling"&gt;&lt;code&gt;error&lt;/code&gt; event&lt;/a&gt; and the connection will stay open and remain usable. This means you need to handle it yourself, as &lt;em&gt;no errors are raised directly&lt;/em&gt; by the SDK when an &lt;code&gt;error&lt;/code&gt; event comes in.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;client = AsyncOpenAI()

async with client.beta.realtime.connect(model="gpt-4o-realtime-preview") as connection:
    ...
    async for event in connection:
        if event.type == 'error':
            print(event.error.type)
            print(event.error.code)
            print(event.error.event_id)
            print(event.error.message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using types&lt;/h2&gt; 
&lt;p&gt;Nested request parameters are &lt;a href="https://docs.python.org/3/library/typing.html#typing.TypedDict"&gt;TypedDicts&lt;/a&gt;. Responses are &lt;a href="https://docs.pydantic.dev"&gt;Pydantic models&lt;/a&gt; which also provide helper methods for things like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serializing back into JSON, &lt;code&gt;model.to_json()&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Converting to a dictionary, &lt;code&gt;model.to_dict()&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Typed requests and responses provide autocomplete and documentation within your editor. If you would like to see type errors in VS Code to help catch bugs earlier, set &lt;code&gt;python.analysis.typeCheckingMode&lt;/code&gt; to &lt;code&gt;basic&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Pagination&lt;/h2&gt; 
&lt;p&gt;List methods in the OpenAI API are paginated.&lt;/p&gt; 
&lt;p&gt;This library provides auto-paginating iterators with each list response, so you do not have to request successive pages manually:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

all_jobs = []
# Automatically fetches more pages as needed.
for job in client.fine_tuning.jobs.list(
    limit=20,
):
    # Do something with job here
    all_jobs.append(job)
print(all_jobs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, asynchronously:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI()


async def main() -&amp;gt; None:
    all_jobs = []
    # Iterate through items across all pages, issuing requests as needed.
    async for job in client.fine_tuning.jobs.list(
        limit=20,
    ):
        all_jobs.append(job)
    print(all_jobs)


asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can use the &lt;code&gt;.has_next_page()&lt;/code&gt;, &lt;code&gt;.next_page_info()&lt;/code&gt;, or &lt;code&gt;.get_next_page()&lt;/code&gt; methods for more granular control working with pages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;first_page = await client.fine_tuning.jobs.list(
    limit=20,
)
if first_page.has_next_page():
    print(f"will fetch next page using these details: {first_page.next_page_info()}")
    next_page = await first_page.get_next_page()
    print(f"number of items we just fetched: {len(next_page.data)}")

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or just work directly with the returned data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;first_page = await client.fine_tuning.jobs.list(
    limit=20,
)

print(f"next page cursor: {first_page.after}")  # =&amp;gt; "next page cursor: ..."
for job in first_page.data:
    print(job.id)

# Remove `await` for non-async usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Nested params&lt;/h2&gt; 
&lt;p&gt;Nested parameters are dictionaries, typed using &lt;code&gt;TypedDict&lt;/code&gt;, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI()

response = client.chat.responses.create(
    input=[
        {
            "role": "user",
            "content": "How much ?",
        }
    ],
    model="gpt-4o",
    response_format={"type": "json_object"},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;File uploads&lt;/h2&gt; 
&lt;p&gt;Request parameters that correspond to file uploads can be passed as &lt;code&gt;bytes&lt;/code&gt;, or a &lt;a href="https://docs.python.org/3/library/os.html#os.PathLike"&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance or a tuple of &lt;code&gt;(filename, contents, media type)&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The async client uses the exact same interface. If you pass a &lt;a href="https://docs.python.org/3/library/os.html#os.PathLike"&gt;&lt;code&gt;PathLike&lt;/code&gt;&lt;/a&gt; instance, the file contents will be read asynchronously automatically.&lt;/p&gt; 
&lt;h2&gt;Webhook Verification&lt;/h2&gt; 
&lt;p&gt;Verifying webhook signatures is &lt;em&gt;optional but encouraged&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;For more information about webhooks, see &lt;a href="https://platform.openai.com/docs/guides/webhooks"&gt;the API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Parsing webhook payloads&lt;/h3&gt; 
&lt;p&gt;For most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method &lt;code&gt;client.webhooks.unwrap()&lt;/code&gt;, which parses a webhook request and verifies that it was sent by OpenAI. This method will raise an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). The &lt;code&gt;.unwrap()&lt;/code&gt; method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI
from flask import Flask, request

app = Flask(__name__)
client = OpenAI()  # OPENAI_WEBHOOK_SECRET environment variable is used by default


@app.route("/webhook", methods=["POST"])
def webhook():
    request_body = request.get_data(as_text=True)

    try:
        event = client.webhooks.unwrap(request_body, request.headers)

        if event.type == "response.completed":
            print("Response completed:", event.data)
        elif event.type == "response.failed":
            print("Response failed:", event.data)
        else:
            print("Unhandled event type:", event.type)

        return "ok"
    except Exception as e:
        print("Invalid signature:", e)
        return "Invalid signature", 400


if __name__ == "__main__":
    app.run(port=8000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verifying webhook payloads directly&lt;/h3&gt; 
&lt;p&gt;In some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method &lt;code&gt;client.webhooks.verify_signature()&lt;/code&gt; to &lt;em&gt;only verify&lt;/em&gt; the signature of a webhook request. Like &lt;code&gt;.unwrap()&lt;/code&gt;, this method will raise an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import json
from openai import OpenAI
from flask import Flask, request

app = Flask(__name__)
client = OpenAI()  # OPENAI_WEBHOOK_SECRET environment variable is used by default


@app.route("/webhook", methods=["POST"])
def webhook():
    request_body = request.get_data(as_text=True)

    try:
        client.webhooks.verify_signature(request_body, request.headers)

        # Parse the body after verification
        event = json.loads(request_body)
        print("Verified event:", event)

        return "ok"
    except Exception as e:
        print("Invalid signature:", e)
        return "Invalid signature", 400


if __name__ == "__main__":
    app.run(port=8000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Handling errors&lt;/h2&gt; 
&lt;p&gt;When the library is unable to connect to the API (for example, due to network connection problems or a timeout), a subclass of &lt;code&gt;openai.APIConnectionError&lt;/code&gt; is raised.&lt;/p&gt; 
&lt;p&gt;When the API returns a non-success status code (that is, 4xx or 5xx response), a subclass of &lt;code&gt;openai.APIStatusError&lt;/code&gt; is raised, containing &lt;code&gt;status_code&lt;/code&gt; and &lt;code&gt;response&lt;/code&gt; properties.&lt;/p&gt; 
&lt;p&gt;All errors inherit from &lt;code&gt;openai.APIError&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model="gpt-4o",
        training_file="file-abc123",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Error codes are as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Status Code&lt;/th&gt; 
   &lt;th&gt;Error Type&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;400&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;BadRequestError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;401&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;AuthenticationError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;403&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;PermissionDeniedError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;404&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;NotFoundError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;422&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;UnprocessableEntityError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;429&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;RateLimitError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;gt;=500&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;InternalServerError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;APIConnectionError&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Request IDs&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For more information on debugging requests, see &lt;a href="https://platform.openai.com/docs/api-reference/debugging-requests"&gt;these docs&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;x-request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to OpenAI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = await client.responses.create(
    model="gpt-4o-mini",
    input="Say 'this is a test'.",
)
print(response._request_id)  # req_123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that unlike other properties that use an &lt;code&gt;_&lt;/code&gt; prefix, the &lt;code&gt;_request_id&lt;/code&gt; property &lt;em&gt;is&lt;/em&gt; public. Unless documented otherwise, &lt;em&gt;all&lt;/em&gt; other &lt;code&gt;_&lt;/code&gt; prefix properties, methods and modules are &lt;em&gt;private&lt;/em&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; If you need to access request IDs for failed requests you must catch the &lt;code&gt;APIStatusError&lt;/code&gt; exception&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import openai

try:
    completion = await client.chat.completions.create(
        messages=[{"role": "user", "content": "Say this is a test"}], model="gpt-4"
    )
except openai.APIStatusError as exc:
    print(exc.request_id)  # req_123
    raise exc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Retries&lt;/h2&gt; 
&lt;p&gt;Certain errors are automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors are all retried by default.&lt;/p&gt; 
&lt;p&gt;You can use the &lt;code&gt;max_retries&lt;/code&gt; option to configure or disable retry settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in JavaScript?",
        }
    ],
    model="gpt-4o",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Timeouts&lt;/h2&gt; 
&lt;p&gt;By default requests time out after 10 minutes. You can configure this with a &lt;code&gt;timeout&lt;/code&gt; option, which accepts a float or an &lt;a href="https://www.python-httpx.org/advanced/timeouts/#fine-tuning-the-configuration"&gt;&lt;code&gt;httpx.Timeout&lt;/code&gt;&lt;/a&gt; object:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # 20 seconds (default is 10 minutes)
    timeout=20.0,
)

# More granular control:
client = OpenAI(
    timeout=httpx.Timeout(60.0, read=5.0, write=10.0, connect=2.0),
)

# Override per-request:
client.with_options(timeout=5.0).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I list all files in a directory using Python?",
        }
    ],
    model="gpt-4o",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;On timeout, an &lt;code&gt;APITimeoutError&lt;/code&gt; is thrown.&lt;/p&gt; 
&lt;p&gt;Note that requests that time out are &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/#retries"&gt;retried twice by default&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Advanced&lt;/h2&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;We use the standard library &lt;a href="https://docs.python.org/3/library/logging.html"&gt;&lt;code&gt;logging&lt;/code&gt;&lt;/a&gt; module.&lt;/p&gt; 
&lt;p&gt;You can enable logging by setting the environment variable &lt;code&gt;OPENAI_LOG&lt;/code&gt; to &lt;code&gt;info&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ export OPENAI_LOG=info
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to &lt;code&gt;debug&lt;/code&gt; for more verbose logging.&lt;/p&gt; 
&lt;h3&gt;How to tell whether &lt;code&gt;None&lt;/code&gt; means &lt;code&gt;null&lt;/code&gt; or missing&lt;/h3&gt; 
&lt;p&gt;In an API response, a field may be explicitly &lt;code&gt;null&lt;/code&gt;, or missing entirely; in either case, its value is &lt;code&gt;None&lt;/code&gt; in this library. You can differentiate the two cases with &lt;code&gt;.model_fields_set&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;if response.my_field is None:
  if 'my_field' not in response.model_fields_set:
    print('Got json like {}, without a "my_field" key present at all.')
  else:
    print('Got json like {"my_field": null}.')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessing raw response data (e.g. headers)&lt;/h3&gt; 
&lt;p&gt;The "raw" Response object can be accessed by prefixing &lt;code&gt;.with_raw_response.&lt;/code&gt; to any HTTP method call, e.g.,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from openai import OpenAI

client = OpenAI()
response = client.chat.completions.with_raw_response.create(
    messages=[{
        "role": "user",
        "content": "Say this is a test",
    }],
    model="gpt-4o",
)
print(response.headers.get('X-My-Header'))

completion = response.parse()  # get the object that `chat.completions.create()` would have returned
print(completion)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These methods return a &lt;a href="https://github.com/openai/openai-python/tree/main/src/openai/_legacy_response.py"&gt;&lt;code&gt;LegacyAPIResponse&lt;/code&gt;&lt;/a&gt; object. This is a legacy class as we're changing it slightly in the next major version.&lt;/p&gt; 
&lt;p&gt;For the sync client this will mostly be the same with the exception of &lt;code&gt;content&lt;/code&gt; &amp;amp; &lt;code&gt;text&lt;/code&gt; will be methods instead of properties. In the async client, all methods will be async.&lt;/p&gt; 
&lt;p&gt;A migration script will be provided &amp;amp; the migration in general should be smooth.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;.with_streaming_response&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The above interface eagerly reads the full response body when you make the request, which may not always be what you want.&lt;/p&gt; 
&lt;p&gt;To stream the response body, use &lt;code&gt;.with_streaming_response&lt;/code&gt; instead, which requires a context manager and only reads the response body once you call &lt;code&gt;.read()&lt;/code&gt;, &lt;code&gt;.text()&lt;/code&gt;, &lt;code&gt;.json()&lt;/code&gt;, &lt;code&gt;.iter_bytes()&lt;/code&gt;, &lt;code&gt;.iter_text()&lt;/code&gt;, &lt;code&gt;.iter_lines()&lt;/code&gt; or &lt;code&gt;.parse()&lt;/code&gt;. In the async client, these are async methods.&lt;/p&gt; 
&lt;p&gt;As such, &lt;code&gt;.with_streaming_response&lt;/code&gt; methods return a different &lt;a href="https://github.com/openai/openai-python/tree/main/src/openai/_response.py"&gt;&lt;code&gt;APIResponse&lt;/code&gt;&lt;/a&gt; object, and the async client returns an &lt;a href="https://github.com/openai/openai-python/tree/main/src/openai/_response.py"&gt;&lt;code&gt;AsyncAPIResponse&lt;/code&gt;&lt;/a&gt; object.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-4o",
) as response:
    print(response.headers.get("X-My-Header"))

    for line in response.iter_lines():
        print(line)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The context manager is required so that the response will reliably be closed.&lt;/p&gt; 
&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; 
&lt;p&gt;This library is typed for convenient access to the documented API.&lt;/p&gt; 
&lt;p&gt;If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; 
&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; 
&lt;p&gt;To make requests to undocumented endpoints, you can make requests using &lt;code&gt;client.get&lt;/code&gt;, &lt;code&gt;client.post&lt;/code&gt;, and other http verbs. Options on the client will be respected (such as retries) when making this request.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import httpx

response = client.post(
    "/foo",
    cast_to=httpx.Response,
    body={"my_param": True},
)

print(response.headers.get("x-foo"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Undocumented request params&lt;/h4&gt; 
&lt;p&gt;If you want to explicitly send an extra param, you can do so with the &lt;code&gt;extra_query&lt;/code&gt;, &lt;code&gt;extra_body&lt;/code&gt;, and &lt;code&gt;extra_headers&lt;/code&gt; request options.&lt;/p&gt; 
&lt;h4&gt;Undocumented response properties&lt;/h4&gt; 
&lt;p&gt;To access undocumented response properties, you can access the extra fields like &lt;code&gt;response.unknown_prop&lt;/code&gt;. You can also get all the extra fields on the Pydantic model as a dict with &lt;a href="https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_extra"&gt;&lt;code&gt;response.model_extra&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuring the HTTP client&lt;/h3&gt; 
&lt;p&gt;You can directly override the &lt;a href="https://www.python-httpx.org/api/#client"&gt;httpx client&lt;/a&gt; to customize it for your use case, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for &lt;a href="https://www.python-httpx.org/advanced/proxies/"&gt;proxies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Custom &lt;a href="https://www.python-httpx.org/advanced/transports/"&gt;transports&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additional &lt;a href="https://www.python-httpx.org/advanced/clients/"&gt;advanced&lt;/a&gt; functionality&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import httpx
from openai import OpenAI, DefaultHttpxClient

client = OpenAI(
    # Or use the `OPENAI_BASE_URL` env var
    base_url="http://my.test.server.example.com:8083/v1",
    http_client=DefaultHttpxClient(
        proxy="http://my.test.proxy.example.com",
        transport=httpx.HTTPTransport(local_address="0.0.0.0"),
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also customize the client on a per-request basis by using &lt;code&gt;with_options()&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;client.with_options(http_client=DefaultHttpxClient(...))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Managing HTTP resources&lt;/h3&gt; 
&lt;p&gt;By default the library closes underlying HTTP connections whenever the client is &lt;a href="https://docs.python.org/3/reference/datamodel.html#object.__del__"&gt;garbage collected&lt;/a&gt;. You can manually close the client using the &lt;code&gt;.close()&lt;/code&gt; method if desired, or with a context manager that closes when exiting.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from openai import OpenAI

with OpenAI() as client:
  # make requests here
  ...

# HTTP client is now closed
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;To use this library with &lt;a href="https://learn.microsoft.com/azure/ai-services/openai/overview"&gt;Azure OpenAI&lt;/a&gt;, use the &lt;code&gt;AzureOpenAI&lt;/code&gt; class instead of the &lt;code&gt;OpenAI&lt;/code&gt; class.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The Azure API shape differs from the core API shape which means that the static types for responses / params won't always be correct.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
client = AzureOpenAI(
    # https://learn.microsoft.com/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)

completion = client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
print(completion.to_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In addition to the options provided in the base &lt;code&gt;OpenAI&lt;/code&gt; client, the following options are provided:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;azure_endpoint&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_deployment&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;api_version&lt;/code&gt; (or the &lt;code&gt;OPENAI_API_VERSION&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_ad_token&lt;/code&gt; (or the &lt;code&gt;AZURE_OPENAI_AD_TOKEN&lt;/code&gt; environment variable)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;azure_ad_token_provider&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;An example of using the client with Microsoft Entra ID (formerly known as Azure Active Directory) can be found &lt;a href="https://github.com/openai/openai-python/raw/main/examples/azure_ad.py"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Versioning&lt;/h2&gt; 
&lt;p&gt;This package generally follows &lt;a href="https://semver.org/spec/v2.0.0.html"&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Changes that only affect static types, without breaking runtime behavior.&lt;/li&gt; 
 &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; 
&lt;p&gt;We are keen for your feedback; please open an &lt;a href="https://www.github.com/openai/openai-python/issues"&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; 
&lt;h3&gt;Determining the installed version&lt;/h3&gt; 
&lt;p&gt;If you've upgraded to the latest version but aren't seeing any new features you were expecting then your python environment is likely still using an older version.&lt;/p&gt; 
&lt;p&gt;You can determine the version that is being used at runtime with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import openai
print(openai.__version__)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Python 3.8 or higher.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/openai/openai-python/main/CONTRIBUTING.md"&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mindverse/Second-Me</title>
      <link>https://github.com/mindverse/Second-Me</link>
      <description>&lt;p&gt;Train your AI self, amplify you, bridge the world&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/mindverse/Second-Me/raw/master/images/cover.png" alt="Second Me" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.secondme.io/"&gt;&lt;img src="https://img.shields.io/badge/Second_Me-Homepage-blue?style=flat-square&amp;amp;logo=homebridge" alt="Homepage" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.18312"&gt;&lt;img src="https://img.shields.io/badge/AI--native_Memory-arXiv-orange?style=flat-square&amp;amp;logo=academia" alt="AI-native Memory" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2503.08102"&gt;&lt;img src="https://img.shields.io/badge/AI--native_Memory_2.0-arXiv-red?style=flat-square&amp;amp;logo=arxiv" alt="AI-native Memory 2.0" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/GpWHQNUwrg"&gt;&lt;img src="https://img.shields.io/badge/Chat-Discord-5865F2?style=flat-square&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/SecondMe_AI1"&gt;&lt;img src="https://img.shields.io/badge/Follow-@SecondMe_AI-1DA1F2?style=flat-square&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/SecondMeAI/"&gt;&lt;img src="https://img.shields.io/badge/Join-Reddit-FF4500?style=flat-square&amp;amp;logo=reddit&amp;amp;logoColor=white" alt="Reddit" /&gt;&lt;/a&gt; &lt;a href="https://secondme.gitbook.io/secondme/faq"&gt;&lt;img src="https://img.shields.io/badge/FAQ-GitBook-blue?style=flat-square" alt="View FAQ" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Our Vision&lt;/h2&gt; 
&lt;p&gt;Companies like OpenAI built "Super AI" that threatens human independence. We crave individuality: AI that amplifies, not erases, &lt;strong&gt;YOU&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;We‚Äôre challenging that with "&lt;strong&gt;Second Me&lt;/strong&gt;": an open-source prototype where you craft your own &lt;strong&gt;AI self&lt;/strong&gt;‚Äîa new AI species that preserves you, delivers your context, and defends your interests.&lt;/p&gt; 
&lt;p&gt;It‚Äôs &lt;strong&gt;locally trained and hosted&lt;/strong&gt;‚Äîyour data, your control‚Äîyet &lt;strong&gt;globally connected&lt;/strong&gt;, scaling your intelligence across an AI network. Beyond that, it‚Äôs your AI identity interface‚Äîa bold standard linking your AI to the world, sparks collaboration among AI selves, and builds tomorrow‚Äôs truly native AI apps.&lt;/p&gt; 
&lt;p&gt;Tech enthusiasts, AI pros, domain experts, Join us! Second Me is your launchpad to extend your mind into the digital horizon.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;Train Your AI Self&lt;/strong&gt; with AI-Native Memory (&lt;a href="https://arxiv.org/abs/2503.08102"&gt;Paper&lt;/a&gt;)&lt;/h3&gt; 
&lt;p&gt;Start training your Second Me today with your own memories! Using Hierarchical Memory Modeling (HMM) and the Me-Alignment Algorithm, your AI self captures your identity, understands your context, and reflects you authentically.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/a84c6135-26dc-4413-82aa-f4a373c0ff89" width="94%" /&gt; &lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Scale Your Intelligence&lt;/strong&gt; on the Second Me Network&lt;/h3&gt; 
&lt;p&gt;Launch your AI self from your laptop onto our decentralized network‚Äîanyone or any app can connect with your permission, sharing your context as your digital identity.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/9a74a3f4-d8fd-41c1-8f24-534ed94c842a" width="94%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Build Tomorrow‚Äôs Apps with Second Me&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Roleplay&lt;/strong&gt;: Your AI self switches personas to represent you in different scenarios.&lt;br /&gt; &lt;strong&gt;AI Space&lt;/strong&gt;: Collaborate with other Second Mes to spark ideas or solve problems.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/bc6125c1-c84f-4ecc-b620-8932cc408094" width="94%" /&gt; &lt;/p&gt; 
&lt;h3&gt;100% &lt;strong&gt;Privacy and Control&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Unlike traditional centralized AI systems, Second Me ensures that your information and intelligence remain local and completely private.&lt;/p&gt; 
&lt;h2&gt;Getting started &amp;amp; staying tuned with us&lt;/h2&gt; 
&lt;p&gt;Star and join us, and you will receive all release notifications from GitHub without any delay!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/5c14d956-f931-4c25-b0b3-3c2c96cd7581" width="94%" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;üìä Model Size vs. Memory (Reference Guide)&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Note: "B" in the table represents "billion parameters model". Data shown are examples only; actual supported model sizes may vary depending on system optimization, deployment environment, and other hardware/software conditions.&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Memory (GB)&lt;/th&gt; 
   &lt;th&gt;Docker Deployment (Windows/Linux)&lt;/th&gt; 
   &lt;th&gt;Docker Deployment (Mac)&lt;/th&gt; 
   &lt;th&gt;Integrated Setup (Windows/Linux)&lt;/th&gt; 
   &lt;th&gt;Integrated Setup (Mac)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;~0.8B (example)&lt;/td&gt; 
   &lt;td&gt;~0.4B (example)&lt;/td&gt; 
   &lt;td&gt;~1.0B (example)&lt;/td&gt; 
   &lt;td&gt;~0.6B (example)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;1.5B (example)&lt;/td&gt; 
   &lt;td&gt;0.5B (example)&lt;/td&gt; 
   &lt;td&gt;~2.0B (example)&lt;/td&gt; 
   &lt;td&gt;~0.8B (example)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;~2.8B (example)&lt;/td&gt; 
   &lt;td&gt;~1.2B (example)&lt;/td&gt; 
   &lt;td&gt;~3.5B (example)&lt;/td&gt; 
   &lt;td&gt;~1.5B (example)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Models below 0.5B may not provide satisfactory performance for complex tasks. And we're continuously improving cross-platform support - please &lt;a href="https://github.com/mindverse/Second-Me/issues/new"&gt;submit an issue&lt;/a&gt; for feedback or compatibility problems on different operating systems.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;MLX Acceleration&lt;/strong&gt;: Mac M-series users can use &lt;a href="https://github.com/mindverse/Second-Me/tree/master/lpm_kernel/L2/mlx_training"&gt;MLX&lt;/a&gt; to run larger models (CLI-only).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚ö° Get your Second Me running in just 3 steps:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Clone the repository
git clone https://github.com/mindverse/Second-Me.git
cd Second-Me
# 2. Start Docker containers
make docker-up
# 3. Access the web interface
# Open your browser and visit: http://localhost:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üëâ For detailed instructions ‚Äî including integrated (non-Docker) setup, model selection, memory requirements, and platform-specific tips, check the full &lt;a href="https://secondme.gitbook.io/secondme/guides/deployment"&gt;Deployment Guide on GitBook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;‚ùì Got questions about setup, models, or any troubleshooting? &lt;a href="https://secondme.gitbook.io/secondme/faq"&gt;Check our FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tutorial and Use Cases&lt;/h2&gt; 
&lt;p&gt;üõ†Ô∏è Feel free to follow &lt;a href="https://secondme.gitbook.io/secondme/getting-started"&gt;User tutorial&lt;/a&gt; to build your Second Me.&lt;/p&gt; 
&lt;p&gt;üí° Check out the links below to see how Second Me can be used in real-life scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.secondme.io/example/ama"&gt;Felix AMA (Roleplay app)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.secondme.io/example/brainstorming"&gt;Brainstorming a 15-Day European City Itinerary (Network app)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.secondme.io/example/Icebreaker"&gt;Icebreaking as a Speed Dating Match (Network app)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's Next: May 2025&lt;/h2&gt; 
&lt;p&gt;Second Me continues to evolve as the open-source identity infrastructure for AI. Here's what's on deck for May:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üóÇÔ∏è &lt;strong&gt;Version Control&lt;/strong&gt;: Smarter versioning of memory and identity states&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;Continuous Training Pipelines&lt;/strong&gt;: Keep your AI self evolving over time, with ongoing updates based on new memory inputs.&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Performance &amp;amp; Stability Improvements&lt;/strong&gt;: Enhancements across inference ability, model alignment, and base model upgrades&lt;/li&gt; 
 &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;Cloud Solutions&lt;/strong&gt;: Explore cloud-based solutions for both model training (fine-tuning) and model deployment, to reduce the hardware burden on users' local machines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We‚Äôd love for you to help shape what‚Äôs coming next ‚Äî whether it‚Äôs fixing bugs, building new features, or improving docs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìò Check out our &lt;a href="https://raw.githubusercontent.com/mindverse/Second-Me/master/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; to get started&lt;/li&gt; 
 &lt;li&gt;üíª Submit ideas, issues, or PRs on &lt;a href="https://github.com/mindverse/Second-Me"&gt;GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ Join the conversation and stay updated in our &lt;a href="https://discord.gg/GpWHQNUwrg"&gt;Discord&lt;/a&gt; ‚Äî it‚Äôs where the community lives&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;We would like to express our gratitude to all the individuals who have contributed to Second Me! If you're interested in contributing to the future of intelligence uploading, whether through code, documentation, or ideas, please feel free to submit a pull request to our repository: &lt;a href="https://github.com/Mindverse/Second-Me"&gt;Second-Me&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/mindverse/Second-Me/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=mindverse/Second-Me" /&gt; &lt;/a&gt; 
&lt;p&gt;Made with &lt;a href="https://contrib.rocks"&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This work leverages the power of the open-source community.&lt;/p&gt; 
&lt;p&gt;For data synthesis, we utilized &lt;a href="https://github.com/microsoft/graphrag"&gt;GraphRAG&lt;/a&gt; from Microsoft.&lt;/p&gt; 
&lt;p&gt;For model deployment, we utilized &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt;, which provides efficient inference capabilities.&lt;/p&gt; 
&lt;p&gt;Our base models primarily come from the &lt;a href="https://huggingface.co/Qwen"&gt;Qwen2.5&lt;/a&gt; series.&lt;/p&gt; 
&lt;p&gt;We also want to extend our sincere gratitude to all users who have experienced Second Me. We recognize that there is significant room for optimization throughout the entire pipeline, and we are fully committed to iterative improvements to ensure everyone can enjoy the best possible experience locally.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Second Me is open source software licensed under the Apache License 2.0. See the &lt;a href="https://raw.githubusercontent.com/mindverse/Second-Me/master/LICENSE"&gt;LICENSE&lt;/a&gt; file for more details.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#mindverse/Second-Me&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mindverse/Second-Me&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mindverse/Second-Me&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mindverse/Second-Me&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>tadata-org/fastapi_mcp</title>
      <link>https://github.com/tadata-org/fastapi_mcp</link>
      <description>&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c" alt="fastapi-to-mcp" height="100/" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;span style="font-size: 0.85em; font-weight: normal;"&gt;Built by &lt;a href="https://tadata.com"&gt;Tadata&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; FastAPI-MCP &lt;/h1&gt; 
&lt;p align="center"&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/tadata-org/fastapi_mcp"&gt;&lt;img src="https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c" alt="fastapi-mcp-usage" height="400" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI's ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hosted Solution&lt;/h2&gt; 
&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href="https://tadata.com"&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP provides &lt;a href="https://fastapi-mcp.tadata.com/"&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples"&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; 
&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn't need to run separately from the MCP server (though &lt;a href="https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app"&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; 
&lt;h2&gt;Development and Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; 
&lt;p&gt;Before you get started, please see our &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg"&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; 
 &lt;li&gt;uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>assafelovic/gpt-researcher</title>
      <link>https://github.com/assafelovic/gpt-researcher</link>
      <description>&lt;p&gt;LLM based autonomous agent that conducts deep local and web research on any topic and generates a long report with citations.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" id="top"&gt; 
 &lt;img src="https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3" alt="Logo" width="80" /&gt; 
 &lt;h4&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;a href="https://gptr.dev"&gt;&lt;img src="https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&amp;amp;logo=world&amp;amp;logoColor=white&amp;amp;color=0891b2" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://docs.gptr.dev"&gt;&lt;img src="https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/QgZXvJAccX"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&amp;amp;theme=clean-inverted&amp;amp;?compact=true" alt="Discord Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/gpt-researcher"&gt;&lt;img src="https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&amp;amp;logoColor=white&amp;amp;style=flat" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&amp;amp;logo=github" alt="GitHub Release" /&gt; &lt;a href="https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?message=Open%20in%20Colab&amp;amp;logo=googlecolab&amp;amp;labelColor=grey&amp;amp;color=yellow&amp;amp;label=%20&amp;amp;style=flat&amp;amp;logoSize=40" alt="Open In Colab" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/gptresearcher/gpt-researcher"&gt;&lt;img src="https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&amp;amp;style=flat&amp;amp;logo=docker&amp;amp;logoColor=white&amp;amp;color=1D63ED" alt="Docker Image Version" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/assaf_elovic"&gt;&lt;img src="https://img.shields.io/twitter/follow/assaf_elovic?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-zh_CN.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-ja_JP.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/README-ko_KR.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;üîé GPT Researcher&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;GPT Researcher is an open deep research agent designed for both web and local research on any given task.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The agent produces detailed, factual, and unbiased research reports with citations. GPT Researcher provides a full suite of customization options to create tailor made and domain specific research agents. Inspired by the recent &lt;a href="https://arxiv.org/abs/2305.04091"&gt;Plan-and-Solve&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2005.11401"&gt;RAG&lt;/a&gt; papers, GPT Researcher addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Our mission is to empower individuals and organizations with accurate, unbiased, and factual information through AI.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Why GPT Researcher?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Objective conclusions for manual research can take weeks, requiring vast resources and time.&lt;/li&gt; 
 &lt;li&gt;LLMs trained on outdated information can hallucinate, becoming irrelevant for current research tasks.&lt;/li&gt; 
 &lt;li&gt;Current LLMs have token limitations, insufficient for generating long research reports.&lt;/li&gt; 
 &lt;li&gt;Limited web sources in existing services lead to misinformation and shallow results.&lt;/li&gt; 
 &lt;li&gt;Selective web sources can introduce bias into research tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/8fcaaa4c-31e5-4814-89b4-94f1433d139d"&gt;https://github.com/user-attachments/assets/8fcaaa4c-31e5-4814-89b4-94f1433d139d&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;The core idea is to utilize 'planner' and 'execution' agents. The planner generates research questions, while the execution agents gather relevant information. The publisher then aggregates all findings into a comprehensive report.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img align="center" height="600" src="https://github.com/assafelovic/gpt-researcher/assets/13554167/4ac896fd-63ab-4b77-9688-ff62aafcc527" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a task-specific agent based on a research query.&lt;/li&gt; 
 &lt;li&gt;Generate questions that collectively form an objective opinion on the task.&lt;/li&gt; 
 &lt;li&gt;Use a crawler agent for gathering information for each question.&lt;/li&gt; 
 &lt;li&gt;Summarize and source-track each resource.&lt;/li&gt; 
 &lt;li&gt;Filter and aggregate summaries into a final research report.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.gptr.dev/blog/building-gpt-researcher"&gt;How it Works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea"&gt;How to Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8"&gt;Live Demo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù Generate detailed research reports using web and local documents.&lt;/li&gt; 
 &lt;li&gt;üñºÔ∏è Smart image scraping and filtering for reports.&lt;/li&gt; 
 &lt;li&gt;üìú Generate detailed reports exceeding 2,000 words.&lt;/li&gt; 
 &lt;li&gt;üåê Aggregate over 20 sources for objective conclusions.&lt;/li&gt; 
 &lt;li&gt;üñ•Ô∏è Frontend available in lightweight (HTML/CSS/JS) and production-ready (NextJS + Tailwind) versions.&lt;/li&gt; 
 &lt;li&gt;üîç JavaScript-enabled web scraping.&lt;/li&gt; 
 &lt;li&gt;üìÇ Maintains memory and context throughout research.&lt;/li&gt; 
 &lt;li&gt;üìÑ Export reports to PDF, Word, and other formats.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://docs.gptr.dev/docs/gpt-researcher/getting-started"&gt;Documentation&lt;/a&gt; for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Installation and setup guides&lt;/li&gt; 
 &lt;li&gt;Configuration and customization options&lt;/li&gt; 
 &lt;li&gt;How-To examples&lt;/li&gt; 
 &lt;li&gt;Full API references&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚öôÔ∏è Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install Python 3.11 or later. &lt;a href="https://www.tutorialsteacher.com/python/install-python"&gt;Guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the project and navigate to the directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/assafelovic/gpt-researcher.git
cd gpt-researcher
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Set up API keys by exporting them or storing them in a &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY={Your OpenAI API Key here}
export TAVILY_API_KEY={Your Tavily API Key here}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install dependencies and start the server:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
python -m uvicorn main:app --reload
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Visit &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; to start.&lt;/p&gt; 
&lt;p&gt;For other setups (e.g., Poetry or virtual environments), check the &lt;a href="https://docs.gptr.dev/docs/gpt-researcher/getting-started"&gt;Getting Started page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Run as PIP package&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install gpt-researcher

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Usage:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
from gpt_researcher import GPTResearcher

query = "why is Nvidia stock going up?"
researcher = GPTResearcher(query=query)
# Conduct research on the given query
research_result = await researcher.conduct_research()
# Write the report
report = await researcher.write_report()
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;For more examples and configurations, please refer to the &lt;a href="https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package"&gt;PIP documentation&lt;/a&gt; page.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;üîß MCP Client&lt;/h3&gt; 
&lt;p&gt;GPT Researcher supports MCP integration to connect with specialized data sources like GitHub repositories, databases, and custom APIs. This enables research from data sources alongside web search.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export RETRIEVER=tavily,mcp  # Enable hybrid web + MCP research
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from gpt_researcher import GPTResearcher
import asyncio
import os

async def mcp_research_example():
    # Enable MCP with web search
    os.environ["RETRIEVER"] = "tavily,mcp"
    
    researcher = GPTResearcher(
        query="What are the top open source web research agents?",
        mcp_configs=[
            {
                "name": "github",
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-github"],
                "env": {"GITHUB_TOKEN": os.getenv("GITHUB_TOKEN")}
            }
        ]
    )
    
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()
    return report
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For comprehensive MCP documentation and advanced examples, visit the &lt;a href="https://docs.gptr.dev/docs/gpt-researcher/retrievers/mcp-configs"&gt;MCP Integration Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;‚ú® Deep Research&lt;/h2&gt; 
&lt;p&gt;GPT Researcher now includes Deep Research - an advanced recursive research workflow that explores topics with agentic depth and breadth. This feature employs a tree-like exploration pattern, diving deeper into subtopics while maintaining a comprehensive view of the research subject.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üå≥ Tree-like exploration with configurable depth and breadth&lt;/li&gt; 
 &lt;li&gt;‚ö°Ô∏è Concurrent processing for faster results&lt;/li&gt; 
 &lt;li&gt;ü§ù Smart context management across research branches&lt;/li&gt; 
 &lt;li&gt;‚è±Ô∏è Takes ~5 minutes per deep research&lt;/li&gt; 
 &lt;li&gt;üí∞ Costs ~$0.4 per research (using &lt;code&gt;o3-mini&lt;/code&gt; on "high" reasoning effort)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.gptr.dev/docs/gpt-researcher/gptr/deep_research"&gt;Learn more about Deep Research&lt;/a&gt; in our documentation.&lt;/p&gt; 
&lt;h2&gt;Run with Docker&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - &lt;a href="https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker"&gt;Install Docker&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Clone the '.env.example' file, add your API Keys to the cloned file and save the file as '.env'&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Within the docker-compose file comment out services that you don't want to run with Docker.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up --build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If that doesn't work, try running it without the dash:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up --build
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - By default, if you haven't uncommented anything in your docker-compose file, this flow will start 2 processes:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;the Python server running on localhost:8000&lt;br /&gt;&lt;/li&gt; 
 &lt;li&gt;the React app running on localhost:3000&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Visit localhost:3000 on any browser and enjoy researching!&lt;/p&gt; 
&lt;h2&gt;üìÑ Research on Local Documents&lt;/h2&gt; 
&lt;p&gt;You can instruct the GPT Researcher to run research tasks based on your local documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.&lt;/p&gt; 
&lt;p&gt;Step 1: Add the env variable &lt;code&gt;DOC_PATH&lt;/code&gt; pointing to the folder where your documents are located.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export DOC_PATH="./my-docs"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Step 2:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you're running the frontend app on localhost:8000, simply select "My Documents" from the "Report Source" Dropdown Options.&lt;/li&gt; 
 &lt;li&gt;If you're running GPT Researcher with the &lt;a href="https://docs.tavily.com/guides/gpt-researcher/gpt-researcher#pip-package"&gt;PIP package&lt;/a&gt;, pass the &lt;code&gt;report_source&lt;/code&gt; argument as "local" when you instantiate the &lt;code&gt;GPTResearcher&lt;/code&gt; class &lt;a href="https://docs.gptr.dev/docs/gpt-researcher/context/tailored-research"&gt;code sample here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ñ MCP Server&lt;/h2&gt; 
&lt;p&gt;We've moved our MCP server to a dedicated repository: &lt;a href="https://github.com/assafelovic/gptr-mcp"&gt;gptr-mcp&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The GPT Researcher MCP Server enables AI applications like Claude to conduct deep research. While LLM apps can access web search tools with MCP, GPT Researcher MCP delivers deeper, more reliable research results.&lt;/p&gt; 
&lt;p&gt;Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Deep research capabilities for AI assistants&lt;/li&gt; 
 &lt;li&gt;Higher quality information with optimized context usage&lt;/li&gt; 
 &lt;li&gt;Comprehensive results with better reasoning for LLMs&lt;/li&gt; 
 &lt;li&gt;Claude Desktop integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For detailed installation and usage instructions, please visit the &lt;a href="https://github.com/assafelovic/gptr-mcp"&gt;official repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üë™ Multi-Agent Assistant&lt;/h2&gt; 
&lt;p&gt;As AI evolves from prompt engineering and RAG to multi-agent systems, we're excited to introduce our new multi-agent assistant built with &lt;a href="https://python.langchain.com/v0.1/docs/langgraph/"&gt;LangGraph&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;By using LangGraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. Inspired by the recent &lt;a href="https://arxiv.org/abs/2402.14207"&gt;STORM&lt;/a&gt; paper, this project showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication.&lt;/p&gt; 
&lt;p&gt;An average run generates a 5-6 page research report in multiple formats such as PDF, Docx and Markdown.&lt;/p&gt; 
&lt;p&gt;Check it out &lt;a href="https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents"&gt;here&lt;/a&gt; or head over to our &lt;a href="https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph"&gt;documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;üñ•Ô∏è Frontend Applications&lt;/h2&gt; 
&lt;p&gt;GPT-Researcher now features an enhanced frontend to improve the user experience and streamline the research process. The frontend offers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An intuitive interface for inputting research queries&lt;/li&gt; 
 &lt;li&gt;Real-time progress tracking of research tasks&lt;/li&gt; 
 &lt;li&gt;Interactive display of research findings&lt;/li&gt; 
 &lt;li&gt;Customizable settings for tailored research experiences&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Two deployment options are available:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A lightweight static frontend served by FastAPI&lt;/li&gt; 
 &lt;li&gt;A feature-rich NextJS application for advanced functionality&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For detailed setup instructions and more information about the frontend features, please visit our &lt;a href="https://docs.gptr.dev/docs/gpt-researcher/frontend/introduction"&gt;documentation page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üöÄ Contributing&lt;/h2&gt; 
&lt;p&gt;We highly welcome contributions! Please check out &lt;a href="https://github.com/assafelovic/gpt-researcher/raw/master/CONTRIBUTING.md"&gt;contributing&lt;/a&gt; if you're interested.&lt;/p&gt; 
&lt;p&gt;Please check out our &lt;a href="https://trello.com/b/3O7KBePw/gpt-researcher-roadmap"&gt;roadmap&lt;/a&gt; page and reach out to us via our &lt;a href="https://discord.gg/QgZXvJAccX"&gt;Discord community&lt;/a&gt; if you're interested in joining our mission. &lt;a href="https://github.com/assafelovic/gpt-researcher/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=assafelovic/gpt-researcher" /&gt; &lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚úâÔ∏è Support / Contact us&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/spBgZmm3Xe"&gt;Community Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Author Email: &lt;a href="mailto:assaf.elovic@gmail.com"&gt;assaf.elovic@gmail.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üõ° Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project, GPT Researcher, is an experimental application and is provided "as-is" without any warranty, express or implied. We are sharing codes for academic purposes under the Apache 2 license. Nothing herein is academic advice, and NOT a recommendation to use in academic or research papers.&lt;/p&gt; 
&lt;p&gt;Our view on unbiased research claims:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The main goal of GPT Researcher is to reduce incorrect and biased facts. How? We assume that the more sites we scrape the less chances of incorrect data. By scraping multiple sites per research, and choosing the most frequent information, the chances that they are all wrong is extremely low.&lt;/li&gt; 
 &lt;li&gt;We do not aim to eliminate biases; we aim to reduce it as much as possible. &lt;strong&gt;We are here as a community to figure out the most effective human/llm interactions.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;In research, people also tend towards biases as most have already opinions on the topics they research about. This tool scrapes many opinions and will evenly explain diverse views that a biased person would never have read.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://star-history.com/#assafelovic/gpt-researcher"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&amp;amp;type=Date" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="right"&gt; &lt;a href="https://raw.githubusercontent.com/assafelovic/gpt-researcher/master/#top"&gt;‚¨ÜÔ∏è Back to Top&lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/adk-python</title>
      <link>https://github.com/google/adk-python</link>
      <description>&lt;p&gt;An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Development Kit (ADK)&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-python/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml"&gt;&lt;img src="https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml/badge.svg?sanitize=true" alt="Python Unit Tests" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/agentdevelopmentkit/"&gt;&lt;img src="https://img.shields.io/badge/Reddit-r%2Fagentdevelopmentkit-FF4500?style=flat&amp;amp;logo=reddit&amp;amp;logoColor=white" alt="r/agentdevelopmentkit" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/google/adk-python"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt;  
&lt;h2 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png" width="256" /&gt; &lt;/h2&gt; 
&lt;h3 align="center"&gt; An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control. &lt;/h3&gt; 
&lt;h3 align="center"&gt; Important Links: &lt;a href="https://google.github.io/adk-docs/"&gt;Docs&lt;/a&gt;, &lt;a href="https://github.com/google/adk-samples"&gt;Samples&lt;/a&gt;, &lt;a href="https://github.com/google/adk-java"&gt;Java ADK&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/google/adk-web"&gt;ADK Web&lt;/a&gt;. &lt;/h3&gt;  
&lt;p&gt;Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents. While optimized for Gemini and the Google ecosystem, ADK is model-agnostic, deployment-agnostic, and is built for compatibility with other frameworks. ADK was designed to make agent development feel more like software development, to make it easier for developers to create, deploy, and orchestrate agentic architectures that range from simple tasks to complex workflows.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rich Tool Ecosystem&lt;/strong&gt;: Utilize pre-built tools, custom functions, OpenAPI specs, or integrate existing tools to give agents diverse capabilities, all for tight integration with the Google ecosystem.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code-First Development&lt;/strong&gt;: Define agent logic, tools, and orchestration directly in Python for ultimate flexibility, testability, and versioning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Multi-Agent Systems&lt;/strong&gt;: Design scalable applications by composing multiple specialized agents into flexible hierarchies.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deploy Anywhere&lt;/strong&gt;: Easily containerize and deploy agents on Cloud Run or scale seamlessly with Vertex AI Agent Engine.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ñ Agent2Agent (A2A) Protocol and ADK Integration&lt;/h2&gt; 
&lt;p&gt;For remote agent-to-agent communication, ADK integrates with the &lt;a href="https://github.com/google-a2a/A2A/"&gt;A2A protocol&lt;/a&gt;. See this &lt;a href="https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents"&gt;example&lt;/a&gt; for how they can work together.&lt;/p&gt; 
&lt;h2&gt;üöÄ Installation&lt;/h2&gt; 
&lt;h3&gt;Stable Release (Recommended)&lt;/h3&gt; 
&lt;p&gt;You can install the latest stable version of ADK using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install google-adk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The release cadence is weekly.&lt;/p&gt; 
&lt;p&gt;This version is recommended for most users as it represents the most recent official release.&lt;/p&gt; 
&lt;h3&gt;Development Version&lt;/h3&gt; 
&lt;p&gt;Bug fixes and new features are merged into the main branch on GitHub first. If you need access to changes that haven't been included in an official PyPI release yet, you can install directly from the main branch:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/google/adk-python.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: The development version is built directly from the latest code commits. While it includes the newest fixes and features, it may also contain experimental changes or bugs not present in the stable release. Use it primarily for testing upcoming changes or accessing critical fixes before they are officially released.&lt;/p&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;Explore the full documentation for detailed guides on building, evaluating, and deploying agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://google.github.io/adk-docs"&gt;Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üèÅ Feature Highlight&lt;/h2&gt; 
&lt;h3&gt;Define a single agent:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.adk.agents import Agent
from google.adk.tools import google_search

root_agent = Agent(
    name="search_assistant",
    model="gemini-2.0-flash", # Or your preferred Gemini model
    instruction="You are a helpful assistant. Answer user questions using Google Search when needed.",
    description="An assistant that can search the web.",
    tools=[google_search]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Define a multi-agent system:&lt;/h3&gt; 
&lt;p&gt;Define a multi-agent system with coordinator agent, greeter agent, and task execution agent. Then ADK engine and the model will guide the agents works together to accomplish the task.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.adk.agents import LlmAgent, BaseAgent

# Define individual agents
greeter = LlmAgent(name="greeter", model="gemini-2.0-flash", ...)
task_executor = LlmAgent(name="task_executor", model="gemini-2.0-flash", ...)

# Create parent agent and assign children via sub_agents
coordinator = LlmAgent(
    name="Coordinator",
    model="gemini-2.0-flash",
    description="I coordinate greetings and tasks.",
    sub_agents=[ # Assign sub_agents here
        greeter,
        task_executor
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development UI&lt;/h3&gt; 
&lt;p&gt;A built-in development UI to help you test, evaluate, debug, and showcase your agent(s).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png" /&gt; 
&lt;h3&gt;Evaluate Agents&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;adk eval \
    samples_for_testing/hello_world \
    samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://google.github.io/adk-docs/contributing-guide/"&gt;General contribution guideline and flow&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Then if you want to contribute code, please read &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/CONTRIBUTING.md"&gt;Code Contributing Guidelines&lt;/a&gt; to get started.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Vibe Coding&lt;/h2&gt; 
&lt;p&gt;If you are to develop agent via vibe coding the &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/llms.txt"&gt;llms.txt&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/llms-full.txt"&gt;llms-full.txt&lt;/a&gt; can be used as context to LLM. While the former one is a summarized one and the later one has the full information in case your LLM has big enough context window.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Happy Agent Building!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hesreallyhim/awesome-claude-code</title>
      <link>https://github.com/hesreallyhim/awesome-claude-code</link>
      <description>&lt;p&gt;A curated list of awesome commands, files, and workflows for Claude Code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;/h1&gt; 
&lt;!-- [![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re) --&gt; 
&lt;pre style="display: inline-block; text-align: left;"&gt;
 ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚îê    ‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚îê   ‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ    ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ ‚ñà‚îê ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚ñà‚ñà‚ñà‚ñà‚îå‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ‚îî‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îò
‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚îî‚ñà‚ñà‚ñà‚îå‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚îÇ ‚îî‚îÄ‚îò ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚îî‚îÄ‚îò  ‚îî‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îò     ‚îî‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚îê      ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚îê   ‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îê‚ñà‚ñà‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê      ‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îå‚îÄ‚îÄ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îò      ‚ñà‚ñà‚îÇ     ‚ñà‚ñà‚îÇ   ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚ñà‚ñà‚îå‚îÄ‚îÄ‚îò
‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚ñà‚ñà‚îÇ  ‚ñà‚ñà‚îÇ‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê    ‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê‚îî‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îå‚îò‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚îê
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îò  ‚îî‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/pre&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;p&gt;&lt;a href="https://awesome.re"&gt;&lt;img src="https://awesome.re/badge-flat2.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;Awesome Claude Code&lt;/a&gt; ü§ù &lt;a href="https://github.com/hesreallyhim/awesome-claude-code-agents"&gt;Awesome Claude Code Agents&lt;/a&gt;&lt;/h1&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;p&gt;This is a curated list of slash-commands, &lt;code&gt;CLAUDE.md&lt;/code&gt; files, CLI tools, and other resources and guides for enhancing your &lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;Claude Code&lt;/a&gt; workflow, productivity, and vibes.&lt;/p&gt; 
&lt;!--lint enable double-link--&gt; 
&lt;p&gt;Claude Code is a cutting-edge CLI-based coding assistant and agent that you can access in your terminal or IDE. It is a rapidly evolving tool that offers a number of powerful capabilities, and allows for a lot of configuration, in a lot of different ways. Users are actively working out best practices and workflows. It is the hope that this repo will help the community share knowledge and understand how to get the most out of Claude Code.&lt;/p&gt; 
&lt;h3&gt;Announcements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-07-30 - Quick Update: Still trying to iron out the submission flow (sorry for anyone that received duplicate "Congratulations!" issues). If you end up fighting any of the programmatic submission tools, just submit something that has all the necessary data, and I'll take it from there once approved. Other notes: (i) I think it would be really cool/fun to set up a "Claude Code Leaderboard", so feel free to weigh in on the &lt;a href="https://github.com/hesreallyhim/awesome-claude-code/discussions/81"&gt;Discussion&lt;/a&gt;; (ii) I'm still trying to figure out what to do about &lt;strong&gt;SUB AGENTS&lt;/strong&gt;, and I've reached out to some of the other folks who have started similar repo's; (iii) Added a small section that will be updated with new submissions as they roll in.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;New Additions&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dazuiba/CCNotify"&gt;&lt;code&gt;CC Notify&lt;/code&gt;&lt;/a&gt; by &lt;a href="https://github.com/dazuiba"&gt;dazuiba&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Piebald-AI/tweakcc"&gt;&lt;code&gt;tweakcc&lt;/code&gt;&lt;/a&gt; by &lt;a href="https://github.com/Piebald-AI"&gt;Piebald-AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GowayLee/cchooks"&gt;&lt;code&gt;cchooks&lt;/code&gt;&lt;/a&gt; by &lt;a href="https://github.com/GowayLee"&gt;GowayLee&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;p&gt;‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#workflows--knowledge-guides-"&gt;Workflows &amp;amp; Knowledge Guides&lt;/a&gt;&lt;br /&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#tooling-"&gt;Tooling&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ide-integrations"&gt;IDE Integrations&lt;/a&gt;&lt;br /&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#hooks-"&gt;Hooks&lt;/a&gt;&lt;br /&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#slash-commands-"&gt;Slash-Commands&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#version-control--git"&gt;Version Control &amp;amp; Git&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#code-analysis--testing"&gt;Code Analysis &amp;amp; Testing&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#context-loading--priming"&gt;Context Loading &amp;amp; Priming&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#documentation--changelogs"&gt;Documentation &amp;amp; Changelogs&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ci--deployment"&gt;CI / Deployment&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project--task-management"&gt;Project &amp;amp; Task Management&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#miscellaneous"&gt;Miscellaneous&lt;/a&gt;&lt;br /&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#claudemd-files-"&gt;CLAUDE.md Files&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#language-specific"&gt;Language-Specific&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#domain-specific"&gt;Domain-Specific&lt;/a&gt;&lt;br /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;‚ñ´&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project-scaffolding--mcp"&gt;Project Scaffolding &amp;amp; MCP&lt;/a&gt;&lt;br /&gt; ‚ñ™&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#official-documentation-"&gt;Official Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Workflows &amp;amp; Knowledge Guides üß†&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A &lt;strong&gt;workflow&lt;/strong&gt; is a tightly coupled set of Claude Code-native resources that facilitate specific projects&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/cloudartisan/cloudartisan.github.io/tree/main/.claude/commands"&gt;&lt;code&gt;Blogging Platform Instructions&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/cloudartisan"&gt;cloudartisan&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;CC-BY-SA-4.0&lt;br /&gt; Provides a well-structured set of commands for publishing and maintaining a blogging platform, including commands for creating posts, managing categories, and handling media files.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://claudelog.com"&gt;&lt;code&gt;ClaudeLog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://www.reddit.com/user/inventor_black/"&gt;InventorBlack&lt;/a&gt;&lt;br /&gt; A comprehensive knowledge base with detailed breakdowns of advanced &lt;a href="https://claudelog.com/mechanics/you-are-the-main-thread/"&gt;mechanics&lt;/a&gt; including &lt;a href="https://claudelog.com/mechanics/claude-md-supremacy"&gt;CLAUDE.md best practices&lt;/a&gt;, practical technique guides like &lt;a href="https://claudelog.com/mechanics/plan-mode"&gt;plan mode&lt;/a&gt;, &lt;a href="https://claudelog.com/faqs/what-is-ultrathink/"&gt;ultrathink&lt;/a&gt;, &lt;a href="https://claudelog.com/mechanics/task-agent-tools/"&gt;sub-agents&lt;/a&gt;, &lt;a href="https://claudelog.com/mechanics/agent-first-design/"&gt;agent-first design&lt;/a&gt; and &lt;a href="https://claudelog.com/configuration"&gt;configuration guides&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/tree/main/.claude/commands"&gt;&lt;code&gt;Context Priming&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br /&gt; Provides a systematic approach to priming Claude Code with comprehensive project context through specialized commands for different project scenarios and development contexts.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/tree/main/.claude/commands"&gt;&lt;code&gt;n8n_agent&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br /&gt; Amazing comprehensive set of comments for code analysis, QA, design, documentation, project structure, project management, optimization, and many more.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/tree/main/.claude/commands"&gt;&lt;code&gt;Project Bootstrapping and Task Management&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br /&gt; Provides a structured set of commands for bootstrapping and managing a new project, including meta-commands for creating and editing custom slash-commands.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/scopecraft/command/tree/main/.claude/commands"&gt;&lt;code&gt;Project Management, Implementation, Planning, and Release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br /&gt; Really comprehensive set of commands for all aspects of SDLC.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/harperreed/dotfiles/tree/master/.claude/commands"&gt;&lt;code&gt;Project Workflow System&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/harperreed"&gt;harperreed&lt;/a&gt;&lt;br /&gt; A set of commands that provide a comprehensive workflow system for managing projects, including task management, code review, and deployment processes.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://diwank.space/field-notes-from-shipping-real-code-with-claude"&gt;&lt;code&gt;Shipping Real Code w/ Claude&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/creatorrr"&gt;Diwank&lt;/a&gt;&lt;br /&gt; A detailed blog post explaining the author's process for shipping a product with Claude Code, including CLAUDE.md files and other interesting resources.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Helmi/claude-simone"&gt;&lt;code&gt;Simone&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Helmi"&gt;Helmi&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A broader project management workflow for Claude Code that encompasses not just a set of commands, but a system of documents, guidelines, and processes to facilitate project planning and execution.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/wcygan/dotfiles/tree/d8ab6b9f5a7a81007b7f5fa3025d4f83ce12cc02/claude/commands"&gt;&lt;code&gt;Slash-commands megalist&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/wcygan"&gt;wcygan&lt;/a&gt;&lt;br /&gt; A pretty stunning list (88 at the time of this post!) of slash-commands ranging from agent orchestration, code review, project management, security, documentation, self-assessment, almost anything you can dream of.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Tooling üß∞&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tooling&lt;/strong&gt; denotes applications that are built on top of Claude Code and consist of more components than slash-commands and &lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/ryoppippi/ccusage"&gt;&lt;code&gt;CC Usage&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ryoppippi"&gt;ryoppippi&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Handy CLI tool for managing and analyzing Claude Code usage, based on analyzing local Claude Code logs. Presents a nice dashboard regarding cost information, token consumption, etc.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/nyatinte/ccexp"&gt;&lt;code&gt;ccexp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nyatinte"&gt;nyatinte&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Interactive CLI tool for discovering and managing Claude Code configuration files and slash commands with a beautiful terminal UI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Brads3290/cclogviewer"&gt;&lt;code&gt;cclogviewer&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Brads3290"&gt;Brad S.&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A humble but handy utility for viewing Claude Code &lt;code&gt;.jsonl&lt;/code&gt; conversation files in a pretty HTML UI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ruvnet/claude-code-flow"&gt;&lt;code&gt;Claude Code Flow&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ruvnet"&gt;ruvnet&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; This mode serves as a code-first orchestration layer, enabling Claude to write, edit, test, and optimize code autonomously across recursive agent cycles.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor"&gt;&lt;code&gt;Claude Code Usage Monitor&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Maciek-roboblog"&gt;Maciek-roboblog&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A real-time terminal-based tool for monitoring Claude Code token usage. It shows live token consumption, burn rate, and predictions for token depletion. Features include visual progress bars, session-aware analytics, and support for multiple subscription plans.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/possibilities/claude-composer"&gt;&lt;code&gt;Claude Composer&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/possibilities"&gt;Mike Bannister&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Unlicense&lt;br /&gt; A tool that adds small enhancements to Claude Code.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/claude-did-this/claude-hub"&gt;&lt;code&gt;Claude Hub&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/claude-did-this"&gt;Claude Did This&lt;/a&gt;&lt;br /&gt; A webhook service that connects Claude Code to GitHub repositories, enabling AI-powered code assistance directly through pull requests and issues. This integration allows Claude to analyze repositories, answer technical questions, and help developers understand and improve their codebase through simple @mentions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/smtg-ai/claude-squad"&gt;&lt;code&gt;Claude Squad&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/smtg-ai"&gt;smtg-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br /&gt; Claude Squad is a terminal app that manages multiple Claude Code, Codex (and other local agents including Aider) in separate workspaces, allowing you to work on multiple tasks simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/parruda/claude-swarm"&gt;&lt;code&gt;Claude Swarm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/parruda"&gt;parruda&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Launch Claude Code session that is connected to a swarm of Claude Code Agents.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/eyaltoledano/claude-task-master"&gt;&lt;code&gt;Claude Task Master&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eyaltoledano"&gt;eyaltoledano&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; A task management system for AI-driven development with Claude, designed to work seamlessly with Cursor AI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-task-runner"&gt;&lt;code&gt;Claude Task Runner&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt;&lt;br /&gt; A specialized tool to manage context isolation and focused task execution with Claude Code, solving the critical challenge of context length limitations and task focus when working with Claude on complex, multi-step projects.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/pchalasani/claude-code-tools"&gt;&lt;code&gt;claude-code-tools&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/pchalasani"&gt;Prasad Chalasani&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A collection of awesome tools, including tmux integrations, better session management, hooks that enhance security - a really well-done set of Claude Code enhancers, especially for tmux users.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/dagger/container-use"&gt;&lt;code&gt;Container Use&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/dagger"&gt;dagger&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Development environments for coding agents. Enable multiple agents to work safely and independently with your preferred stack.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/dtormoen/tsk"&gt;&lt;code&gt;TSK - AI Agent Task Manager and Sandbox&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/dtormoen"&gt;dtormoen&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A Rust CLI tool that lets you delegate development tasks to AI agents running in sandboxed Docker environments. Multiple agents work in parallel, returning git branches for human review.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Piebald-AI/tweakcc"&gt;&lt;code&gt;tweakcc&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Piebald-AI"&gt;Piebald-AI&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Command-line tool to customize your Claude Code styling.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sculptdotfun/viberank"&gt;&lt;code&gt;viberank&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nikshepsvn"&gt;nikshepsvn&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A community-driven leaderboard tool that enables developers to visualize, track, and compete based on their Claude Code usage statistics. It features robust data analytics, GitHub OAuth, data validation, and user-friendly CLI/web submission methods.&lt;/p&gt; 
&lt;h3&gt;IDE Integrations&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=AndrePimenta.claude-code-chat"&gt;&lt;code&gt;Claude Code Chat&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/andrepimenta"&gt;andrepimenta&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;¬©&lt;br /&gt; An elegant and user-friendly Claude Code chat interface for VS Code.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/manzaltu/claude-code-ide.el"&gt;&lt;code&gt;claude-code-ide.el&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/manzaltu"&gt;manzaltu&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br /&gt; claude-code-ide.el integrates Claude Code with Emacs, like Anthropic‚Äôs VS Code/IntelliJ extensions. It shows ediff-based code suggestions, pulls LSP/flymake/flycheck diagnostics, and tracks buffer context. It adds an extensible MCP tool support for symbol refs/defs, project metadata, and tree-sitter AST queries.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/stevemolitor/claude-code.el"&gt;&lt;code&gt;claude-code.el&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stevemolitor"&gt;stevemolitor&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; An Emacs interface for Claude Code CLI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/greggh/claude-code.nvim"&gt;&lt;code&gt;claude-code.nvim&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/greggh"&gt;greggh&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A seamless integration between Claude Code AI assistant and Neovim.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/stravu/crystal"&gt;&lt;code&gt;crystal&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stravu"&gt;stravu&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A full-fledged desktop application for orchestrating, monitoring, and interacting with Claude Code agents.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Hooks ü™ù&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Hooks&lt;/strong&gt; are a brand new API for Claude Code that allows users to activate commands and run scripts at different points in Claude's agentic lifecycle.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;[Experimental]&lt;/strong&gt; - The resources listed in this section have not been fully vetted and may not work as expected, given the bleeding-edge nature of Claude Code hooks. Nevertheless, I wished to include them at least as a source of inspiration and to explore this unknown terrain. YMMV!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/dazuiba/CCNotify"&gt;&lt;code&gt;CC Notify&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/dazuiba"&gt;dazuiba&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; CCNotify provides desktop notifications for Claude Code, alerting you to input needs or task completion, with one-click jumps back to VS Code and task duration display.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/GowayLee/cchooks"&gt;&lt;code&gt;cchooks&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/GowayLee"&gt;GowayLee&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A lightweight Python SDK with a clean API and good documentation; simplifies the process of writing hooks and integrating them into your codebase, providing a nice abstraction over the JSON configuration files.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/beyondcode/claude-hooks-sdk"&gt;&lt;code&gt;claude-code-hooks-sdk&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/beyondcode"&gt;beyondcode&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A Laravel-inspired PHP SDK for building Claude Code hook responses with a clean, fluent API. This SDK makes it easy to create structured JSON responses for Claude Code hooks using an expressive, chainable interface.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/johnlindquist/claude-hooks"&gt;&lt;code&gt;claude-hooks&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/johnlindquist"&gt;John Lindquist&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A TypeScript-based system for configuring and customizing Claude Code hooks with a powerful and flexible interface.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Veraticus/nix-config/tree/main/home-manager/claude-code/hooks"&gt;&lt;code&gt;Linting, testing, and notifications (in go)&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Veraticus"&gt;Josh Symonds&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Nice set of hooks for enforcing code quality (linting, testing, notifications), with a nice configuration setup as well.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/nizos/tdd-guard"&gt;&lt;code&gt;TDD Guard&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nizos"&gt;Nizar Selander&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A hooks-driven system that monitors file operations in real-time and blocks changes that violate TDD principles.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Slash-Commands üî™&lt;/h2&gt; 
&lt;h3&gt;Version Control &amp;amp; Git&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/danielscholl/mvn-mcp-server/raw/main/.claude/commands/bug-fix.md"&gt;&lt;code&gt;/bug-fix&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/danielscholl"&gt;danielscholl&lt;/a&gt;&lt;br /&gt; Streamlines bug fixing by creating a GitHub issue first, then a feature branch for implementing and thoroughly testing the solution before merging.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/commit.md"&gt;&lt;code&gt;/commit&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Creates git commits using conventional commit format with appropriate emojis, following project standards and creating descriptive messages that explain the purpose of changes.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/.claude/commands/2-commit-fast.md"&gt;&lt;code&gt;/commit-fast&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br /&gt; Automates git commit process by selecting the first suggested message, generating structured commits with consistent formatting while skipping manual confirmation and removing Claude co-Contributorship footer&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/toyamarinyon/giselle/raw/main/.claude/commands/create-pr.md"&gt;&lt;code&gt;/create-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/toyamarinyon"&gt;toyamarinyon&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Streamlines pull request creation by handling the entire workflow: creating a new branch, committing changes, formatting modified files with Biome, and submitting the PR.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/liam-hq/liam/raw/main/.claude/commands/create-pull-request.md"&gt;&lt;code&gt;/create-pull-request&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/liam-hq"&gt;liam-hq&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Provides comprehensive PR creation guidance with GitHub CLI, enforcing title conventions, following template structure, and offering concrete command examples with best practices.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/create-worktrees.md"&gt;&lt;code&gt;/create-worktrees&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Creates git worktrees for all open PRs or specific branches, handling branches with slashes, cleaning up stale worktrees, and supporting custom branch creation for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/jeremymailen/kotlinter-gradle/raw/master/.claude/commands/fix-github-issue.md"&gt;&lt;code&gt;/fix-github-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jeremymailen"&gt;jeremymailen&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Analyzes and fixes GitHub issues using a structured approach with GitHub CLI for issue details, implementing necessary code changes, running tests, and creating proper commit messages.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-issue.md"&gt;&lt;code&gt;/fix-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Addresses GitHub issues by taking issue number as parameter, analyzing context, implementing solution, and testing/validating the fix for proper integration.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-pr.md"&gt;&lt;code&gt;/fix-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Fetches and fixes unresolved PR comments by automatically retrieving feedback, addressing reviewer concerns, making targeted code improvements, and streamlining the review process.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/husky.md"&gt;&lt;code&gt;/husky&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Sets up and manages Husky Git hooks by configuring pre-commit hooks, establishing commit message standards, integrating with linting tools, and ensuring code quality on commits.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/arkavo-org/opentdf-rs/raw/main/.claude/commands/pr-review.md"&gt;&lt;code&gt;/pr-review&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/arkavo-org"&gt;arkavo-org&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Reviews pull request changes to provide feedback, check for issues, and suggest improvements before merging into the main codebase.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/.claude/commands/update-branch-name.md"&gt;&lt;code&gt;/update-branch-name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Updates branch names with proper prefixes and formats, enforcing naming conventions, supporting semantic prefixes, and managing remote branch updates.&lt;/p&gt; 
&lt;h3&gt;Code Analysis &amp;amp; Testing&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/rygwdn/slack-tools/raw/main/.claude/commands/check.md"&gt;&lt;code&gt;/check&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rygwdn"&gt;rygwdn&lt;/a&gt;&lt;br /&gt; Performs comprehensive code quality and security checks, featuring static analysis integration, security vulnerability scanning, code style enforcement, and detailed reporting.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Graphlet-AI/eridu/raw/main/.claude/commands/clean.md"&gt;&lt;code&gt;/clean&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Graphlet-AI"&gt;Graphlet-AI&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Addresses code formatting and quality issues by fixing black formatting problems, organizing imports with isort, resolving flake8 linting issues, and correcting mypy type errors.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/raw/main/.claude/commands/code_analysis.md"&gt;&lt;code&gt;/code_analysis&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br /&gt; Provides a menu of advanced code analysis commands for deep inspection, including knowledge graph generation, optimization suggestions, and quality evaluation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/to4iki/ai-project-rules/raw/main/.claude/commands/optimize.md"&gt;&lt;code&gt;/optimize&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/to4iki"&gt;to4iki&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Analyzes code performance to identify bottlenecks, proposing concrete optimizations with implementation guidance for improved application performance.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rzykov/metabase/raw/master/.claude/commands/repro-issue.md"&gt;&lt;code&gt;/repro-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rzykov"&gt;rzykov&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Creates reproducible test cases for GitHub issues, ensuring tests fail reliably and documenting clear reproduction steps for developers.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/zscott/pane/raw/main/.claude/commands/tdd.md"&gt;&lt;code&gt;/tdd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zscott"&gt;zscott&lt;/a&gt;&lt;br /&gt; Guides development using Test-Driven Development principles, enforcing Red-Green-Refactor discipline, integrating with git workflow, and managing PR creation.&lt;/p&gt; 
&lt;h3&gt;Context Loading &amp;amp; Priming&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/elizaOS/elizaos.github.io/raw/main/.claude/commands/context-prime.md"&gt;&lt;code&gt;/context-prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/elizaOS"&gt;elizaOS&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Primes Claude with comprehensive project understanding by loading repository structure, setting development context, establishing project goals, and defining collaboration parameters.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/okuvshynov/cubestat/raw/main/.claude/commands/initref.md"&gt;&lt;code&gt;/initref&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/okuvshynov"&gt;okuvshynov&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Initializes reference documentation structure with standard doc templates, API reference setup, documentation conventions, and placeholder content generation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ethpandaops/xatu-data/raw/master/.claude/commands/load-llms-txt.md"&gt;&lt;code&gt;/load-llms-txt&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ethpandaops"&gt;ethpandaops&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Loads LLM configuration files to context, importing specific terminology, model configurations, and establishing baseline terminology for AI discussions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_coo_context.md"&gt;&lt;code&gt;/load_coo_context&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br /&gt; References specific files for sparse matrix operations, explains transform usage, compares with previous approaches, and sets data formatting context for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_dango_pipeline.md"&gt;&lt;code&gt;/load_dango_pipeline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br /&gt; Sets context for model training by referencing pipeline files, establishing working context, and preparing for pipeline work with relevant documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/yzyydev/AI-Engineering-Structure/raw/main/.claude/commands/prime.md"&gt;&lt;code&gt;/prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/yzyydev"&gt;yzyydev&lt;/a&gt;&lt;br /&gt; Sets up initial project context by viewing directory structure and reading key files, creating standardized context with directory visualization and key documentation focus.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ddisisto/si/raw/main/.claude/commands/rsi.md"&gt;&lt;code&gt;/rsi&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ddisisto"&gt;ddisisto&lt;/a&gt;&lt;br /&gt; Reads all commands and key project files to optimize AI-assisted development by streamlining the process, loading command context, and setting up for better development workflow.&lt;/p&gt; 
&lt;h3&gt;Documentation &amp;amp; Changelogs&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/berrydev-ai/blockdoc-python/raw/main/.claude/commands/add-to-changelog.md"&gt;&lt;code&gt;/add-to-changelog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/berrydev-ai"&gt;berrydev-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Adds new entries to changelog files while maintaining format consistency, properly documenting changes, and following established project standards for version tracking.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/tree/feature/issue-227-ai-suggestions/.claude/commands/analyze-issue.md"&gt;&lt;code&gt;/create-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Analyzes code structure and purpose to create comprehensive documentation detailing inputs/outputs, behavior, user interaction flows, and edge cases with error handling.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/slunsford/coffee-analytics/raw/main/.claude/commands/docs.md"&gt;&lt;code&gt;/docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/slunsford"&gt;slunsford&lt;/a&gt;&lt;br /&gt; Generates comprehensive documentation that follows project structure, documenting APIs and usage patterns with consistent formatting for better user understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/explain-issue-fix.md"&gt;&lt;code&gt;/explain-issue-fix&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br /&gt; Documents solution approaches for GitHub issues, explaining technical decisions, detailing challenges overcome, and providing implementation context for better understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Consiliency/Flutter-Structurizr/raw/main/.claude/commands/update-docs.md"&gt;&lt;code&gt;/update-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Consiliency"&gt;Consiliency&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Reviews current documentation status, updates implementation progress, reviews phase documents, and maintains documentation consistency across the project.&lt;/p&gt; 
&lt;h3&gt;CI / Deployment&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/kelp/webdown/raw/main/.claude/commands/release.md"&gt;&lt;code&gt;/release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kelp"&gt;kelp&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Manages software releases by updating changelogs, reviewing README changes, evaluating version increments, and documenting release changes for better version tracking.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/run-ci.md"&gt;&lt;code&gt;/run-ci&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br /&gt; Activates virtual environments, runs CI-compatible check scripts, iteratively fixes errors, and ensures all tests pass before completion.&lt;/p&gt; 
&lt;h3&gt;Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/scopecraft/command/raw/main/.claude/commands/create-command.md"&gt;&lt;code&gt;/create-command&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br /&gt; Guides Claude through creating new custom commands with proper structure by analyzing requirements, templating commands by category, enforcing command standards, and creating supporting documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-jtbd.md"&gt;&lt;code&gt;/create-jtbd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br /&gt; Creates Jobs-to-be-Done frameworks that outline user needs with structured format, focusing on specific user problems and organizing by job categories for product development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-prd.md"&gt;&lt;code&gt;/create-prd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br /&gt; Generates comprehensive product requirement documents outlining detailed specifications, requirements, and features following standardized document structure and format.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Wirasm/claudecode-utils/raw/main/.claude/commands/create-prp.md"&gt;&lt;code&gt;/create-prp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Wirasm"&gt;Wirasm&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Creates product requirement plans by reading PRP methodology, following template structure, creating comprehensive requirements, and structuring product definitions for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/raw/main/.claude/commands/project_hello_w_name.md"&gt;&lt;code&gt;/project_hello_w_name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br /&gt; Creates customizable greeting components with name input, demonstrating argument passing, component reusability, state management, and user input handling.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/chrisleyva/todo-slash-command/raw/main/todo.md"&gt;&lt;code&gt;/todo&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/chrisleyva"&gt;chrisleyva&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; A convenient command to quickly manage project todo items without leaving the Claude Code interface, featuring due dates, sorting, task prioritization, and comprehensive todo list management.&lt;/p&gt; 
&lt;h3&gt;Miscellaneous&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/TuckerTucker/tkr-portfolio/raw/main/.claude/commands/five.md"&gt;&lt;code&gt;/five&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/TuckerTucker"&gt;TuckerTucker&lt;/a&gt;&lt;br /&gt; Applies the "five whys" methodology to perform root cause analysis, identify underlying issues, and create solution approaches for complex problems.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/fixing_go_in_graph.md"&gt;&lt;code&gt;/fixing_go_in_graph&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br /&gt; Focuses on Gene Ontology annotation integration in graph databases, handling multiple data sources, addressing graph representation issues, and ensuring correct data incorporation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/GaloyMoney/lana-bank/raw/main/.claude/commands/mermaid.md"&gt;&lt;code&gt;/mermaid&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/GaloyMoney"&gt;GaloyMoney&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Generates Mermaid diagrams from SQL schema files, creating entity relationship diagrams with table properties, validating diagram compilation, and ensuring complete entity coverage.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/review_dcell_model.md"&gt;&lt;code&gt;/review_dcell_model&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br /&gt; Reviews old Dcell implementation files, comparing with newer Dango model, noting changes over time, and analyzing refactoring approaches for better code organization.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/zuplo/docs/raw/main/.claude/commands/use-stepper.md"&gt;&lt;code&gt;/use-stepper&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zuplo"&gt;zuplo&lt;/a&gt;&lt;br /&gt; Reformats documentation to use React Stepper component, transforming heading formats, applying proper indentation, and maintaining markdown compatibility with admonition formatting.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;CLAUDE.md Files üìÇ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/strong&gt; are files that contain important guidelines and context-specfic information or instructions that help Claude Code to better understand your project and your coding standards&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Language-Specific&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/didalgolab/ai-intellij-plugin/raw/main/CLAUDE.md"&gt;&lt;code&gt;AI IntelliJ Plugin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/didalgolab"&gt;didalgolab&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Provides comprehensive Gradle commands for IntelliJ plugin development with platform-specific coding patterns, detailed package structure guidelines, and clear internationalization standards.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/alexei-led/aws-mcp-server/raw/main/CLAUDE.md"&gt;&lt;code&gt;AWS MCP Server&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/alexei-led"&gt;alexei-led&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Features multiple Python environment setup options with detailed code style guidelines, comprehensive error handling recommendations, and security considerations for AWS CLI interactions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/touchlab/DroidconKotlin/raw/main/CLAUDE.md"&gt;&lt;code&gt;DroidconKotlin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/touchlab"&gt;touchlab&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Delivers comprehensive Gradle commands for cross-platform Kotlin Multiplatform development with clear module structure and practical guidance for dependency injection.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/expectedparrot/edsl/raw/main/CLAUDE.md"&gt;&lt;code&gt;EDSL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/expectedparrot"&gt;expectedparrot&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Offers detailed build and test commands with strict code style enforcement, comprehensive testing requirements, and standardized development workflow using Black and mypy.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/CLAUDE.md"&gt;&lt;code&gt;Giselle&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Provides detailed build and test commands using pnpm and Vitest with strict code formatting requirements and comprehensive naming conventions for code consistency.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hashintel/hash/raw/main/CLAUDE.md"&gt;&lt;code&gt;HASH&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hashintel"&gt;hashintel&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Features comprehensive repository structure breakdown with strong emphasis on coding standards, detailed Rust documentation guidelines, and systematic PR review process.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/inkline/inkline/raw/main/CLAUDE.md"&gt;&lt;code&gt;Inkline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/inkline"&gt;inkline&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Structures development workflow using pnpm with emphasis on TypeScript and Vue 3 Composition API, detailed component creation process, and comprehensive testing recommendations.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/mattgodbolt/jsbeeb/raw/main/CLAUDE.md"&gt;&lt;code&gt;JSBeeb&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/mattgodbolt"&gt;mattgodbolt&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br /&gt; Provides development guide for JavaScript BBC Micro emulator with build and testing instructions, architecture documentation, and debugging workflows.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/LamoomAI/lamoom-python/raw/main/CLAUDE.md"&gt;&lt;code&gt;Lamoom Python&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/LamoomAI"&gt;LamoomAI&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br /&gt; Serves as reference for production prompt engineering library with load balancing of AI Models, API documentation, and usage patterns with examples.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/langchain-ai/langgraphjs/raw/main/CLAUDE.md"&gt;&lt;code&gt;LangGraphJS&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/langchain-ai"&gt;langchain-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Offers comprehensive build and test commands with detailed TypeScript style guidelines, layered library architecture, and monorepo structure using yarn workspaces.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/CLAUDE.md"&gt;&lt;code&gt;Metabase&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br /&gt; Details workflow for REPL-driven development in Clojure/ClojureScript with emphasis on incremental development, testing, and step-by-step approach for feature implementation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgcarstrends/backend/raw/main/CLAUDE.md"&gt;&lt;code&gt;SG Cars Trends Backend&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/sgcarstrends"&gt;sgcarstrends&lt;/a&gt;&lt;br /&gt; Provides comprehensive structure for TypeScript monorepo projects with detailed commands for development, testing, deployment, and AWS/Cloudflare integration.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/spylang/spy/raw/main/CLAUDE.md"&gt;&lt;code&gt;SPy&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/spylang"&gt;spylang&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Enforces strict coding conventions with comprehensive testing guidelines, multiple code compilation options, and backend-specific test decorators for targeted filtering.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KarpelesLab/tpl/raw/master/CLAUDE.md"&gt;&lt;code&gt;TPL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/KarpelesLab"&gt;KarpelesLab&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Details Go project conventions with comprehensive error handling recommendations, table-driven testing approach guidelines, and modernization suggestions for latest Go features.&lt;/p&gt; 
&lt;h3&gt;Domain-Specific&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/Layr-Labs/avs-vibe-developer-guide/raw/master/CLAUDE.md"&gt;&lt;code&gt;AVS Vibe Developer Guide&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Layr-Labs"&gt;Layr-Labs&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Structures AI-assisted EigenLayer AVS development workflow with consistent naming conventions for prompt files and established terminology standards for blockchain concepts.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/CommE2E/comm/raw/master/CLAUDE.md"&gt;&lt;code&gt;Comm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/CommE2E"&gt;CommE2E&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;BSD-3-Clause&lt;br /&gt; Serves as a development reference for E2E-encrypted messaging applications with code organization architecture, security implementation details, and testing procedures.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/badass-courses/course-builder/raw/main/CLAUDE.md"&gt;&lt;code&gt;Course Builder&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/badass-courses"&gt;badass-courses&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Enables real-time multiplayer capabilities for collaborative course creation with diverse tech stack integration and monorepo architecture using Turborepo.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/eastlondoner/cursor-tools/raw/main/CLAUDE.md"&gt;&lt;code&gt;Cursor Tools&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eastlondoner"&gt;eastlondoner&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Creates a versatile AI command interface supporting multiple providers and models with flexible command options and browser automation through "Stagehand" feature.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/soramimi/Guitar/raw/master/CLAUDE.md"&gt;&lt;code&gt;Guitar&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/soramimi"&gt;soramimi&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;GPL-2.0&lt;br /&gt; Serves as development guide for Guitar Git GUI Client with build commands for various platforms, code style guidelines for contributing, and project structure explanation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Fimeg/NetworkChronicles/raw/legacy-v1/CLAUDE.md"&gt;&lt;code&gt;Network Chronicles&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Fimeg"&gt;Fimeg&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Presents detailed implementation plan for AI-driven game characters with technical specifications for LLM integration, character guidelines, and service discovery mechanics.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/different-ai/note-companion/raw/master/CLAUDE.md"&gt;&lt;code&gt;Note Companion&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/different-ai"&gt;different-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Provides detailed styling isolation techniques for Obsidian plugins using Tailwind with custom prefix to prevent style conflicts and practical troubleshooting steps.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ParetoSecurity/pareto-mac/raw/main/CLAUDE.md"&gt;&lt;code&gt;Pareto Mac&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ParetoSecurity"&gt;ParetoSecurity&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br /&gt; Serves as development guide for Mac security audit tool with build instructions, contribution guidelines, testing procedures, and workflow documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/CLAUDE.md"&gt;&lt;code&gt;SteadyStart&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br /&gt; Clear and direct instructives about style, permissions, Claude's "role", communications, and documentation of Claude Code sessions for other team members to stay abreast.&lt;/p&gt; 
&lt;h3&gt;Project Scaffolding &amp;amp; MCP&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/basicmachines-co/basic-memory/raw/main/CLAUDE.md"&gt;&lt;code&gt;Basic Memory&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/basicmachines-co"&gt;basicmachines-co&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br /&gt; Presents an innovative AI-human collaboration framework with Model Context Protocol for bidirectional LLM-markdown communication and flexible knowledge structure for complex projects.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-code-mcp-enhanced/raw/main/CLAUDE.md"&gt;&lt;code&gt;claude-code-mcp-enhanced&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Provides detailed and emphatic instructions for Claude to follow as a coding agent, with testing guidance, code examples, and compliance checks.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Family-IT-Guy/perplexity-mcp/raw/main/CLAUDE.md"&gt;&lt;code&gt;Perplexity MCP&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Family-IT-Guy"&gt;Family-IT-Guy&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;ISC&lt;br /&gt; Offers clear step-by-step installation instructions with multiple configuration options, detailed troubleshooting guidance, and concise architecture overview of the MCP protocol.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Official Documentation üèõÔ∏è&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Links to some of Anthropic's terrific documentation and resources regarding Claude Code&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;p&gt;&lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;&lt;code&gt;Anthropic Documentation&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;¬©&lt;br /&gt; The official documentation for Claude Code, including installation instructions, usage guidelines, API references, tutorials, examples, loads of information that I won't list individually. Like Claude Code, the documentation is frequently updated.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/anthropics/anthropic-quickstarts/raw/main/CLAUDE.md"&gt;&lt;code&gt;Anthropic Quickstarts&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Offers comprehensive development guides for three distinct AI-powered demo projects with standardized workflows, strict code style guidelines, and containerization instructions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/anthropics/claude-code-action/tree/main/examples"&gt;&lt;code&gt;Claude Code GitHub Actions&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT&lt;br /&gt; Official GitHub Actions integration for Claude Code with examples and documentation for automating AI-powered workflows in CI/CD pipelines.&lt;/p&gt; 
&lt;h2&gt;Contributing üåª&lt;/h2&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code/issues/new?template=submit-resource.yml"&gt;Submit a new resource here!&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;It's easy! Just click the link above and fill out the form. No Git knowledge required - our automated system handles everything for you.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;We especially welcome:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Proven, effective resources that follow best practices and may even be in use in production&lt;/li&gt; 
 &lt;li&gt;Innovative, creative, or experimental workflows that push the boundaries of Claude Code's capabilities&lt;/li&gt; 
 &lt;li&gt;Additional libraries and tooling that are built on top of Claude Code&lt;/li&gt; 
 &lt;li&gt;Applications of Claude Code outside of the traditional "coding assistant" context (CI/CD, testing, documentation, dev-ops, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the complete submission guide and review process.&lt;/p&gt; 
&lt;p&gt;For suggestions about the repository itself, please &lt;a href="https://github.com/hesreallyhim/awesome-claude-code/issues/new"&gt;open a general issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is released with a &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/code-of-conduct.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to abide by its terms.&lt;/p&gt; 
&lt;h3&gt;A note about licenses&lt;/h3&gt; 
&lt;p&gt;Because simply listing a hyperlink does not qualify as redistribution, the license of the original source is not relevant to its inclusion. However, for posterity and convenience, we do host copies of all resources whose license permits it. Therefore, please include information about the resource's license. Additionally, take note: &lt;em&gt;if you do not include a LICENSE in your GitHub repo, then by default it is fully copyrighted and redistribution is not allowed&lt;/em&gt;. So, if you are intending to make an open source project, it's critical to pick from one of the many available open source licenses. This is just a reminder that without a LICENSE, your project is not open source (it's merely source-code-available) - it may of course still be included on this list, but this notice is to inform readers about the default rules regarding GitHub and LICENSE files. See &lt;a href="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository"&gt;here&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>